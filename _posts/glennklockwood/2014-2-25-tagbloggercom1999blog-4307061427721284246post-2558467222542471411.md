---
author: Glenn K. Lockwood's Blog
author_tag: glennklockwood
blog_subtitle: Personal thoughts and opinions of a supercomputing enthusiast
blog_title: Glenn K. Lockwood
blog_url: https://glennklockwood.blogspot.com/search/label/hpc
category: glennklockwood
date: '2014-02-25 00:59:00'
layout: post
original_url: https://glennklockwood.blogspot.com/2014/02/quantum-espresso-compiling-and-choice.html
slug: quantum-espresso-compiling-and-choice-of-libraries
title: Quantum ESPRESSO- Compiling and Choice of Libraries
---

<div class="p1">We recently upgraded our two big machines at work, and as a result of that upgrade, a number of our users had to rebuild their installation of Quantum ESPRESSO. &nbsp;As it turns out, little quirks in our system conflicted with little quirks in Quantum ESPRESSO after the upgrade and resulted in the regular process of just doing <span style="font-family: Courier New, Courier, monospace;">./configure</span> and make not working out of the box.</div>
<div class="p1"><br /></div>
<div class="p1">Since I had been playing with Quantum ESPRESSO for the purpose of <a href="http://glennklockwood.blogspot.com/2013/12/high-performance-virtualization-sr-iov_14.html">benchmarking QDR InfiniBand virtualized with SR-IOV</a>, I also took it upon myself to iron out exactly how to squeeze the best performance out of QE with respect to compilers, MPI stacks, and choice of linear algebra libraries. &nbsp;For the sake of posterity (or at least until a new version of QE comes out that makes this all irrelevant), here are my notes.<br /><br />I also wrapped all of these build options into <a href="https://github.com/sdsc/sdsc-user/blob/master/makefiles/espresso/build-espresso.sh">a script that will configure and build optimized versions of Quantum ESPRESSO</a> for various compiler and MPI combinations on the two machines I support at work.</div>
<div class="p1"><br /></div>
<h2>BLAS, LAPACK, and ScaLAPACK</h2><div class="p1">Quantum ESPRESSO, like a multitude of other scientific codes, does a lot of linear algebra and uses the BLAS, LAPACK, and ScaLAPACK libraries to this end. &nbsp;I have to shamefully admit that I never fully understood the relationship between these libraries before[1], but figuring out how to build Quantum ESPRESSO to deliver the best performance was a great excuse to sit down and get it straightened out.</div>
<div class="p1"><br /></div>
<div class="p1">BLAS, LAPACK, and ScaLAPACK are all libraries (and <i>de facto</i> standard APIs) that provide increasing levels of abstraction to glue applications to underlying hardware. &nbsp;This is the way I see this layering taking place:</div>
<div class="p1"><br /></div>
<div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-AMFxB0saoqI/UsfIweiCCPI/AAAAAAAAKLg/qWV-_P_d-V8/s1600/LAPACK+Stack.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="218" src="http://4.bp.blogspot.com/-AMFxB0saoqI/UsfIweiCCPI/AAAAAAAAKLg/qWV-_P_d-V8/s400/LAPACK+Stack.png" width="400" /></a></div>
<div class="p1"><br /></div>
<div class="p1"><b>BLAS</b> is the lowest-level library and provides subroutines that do basic vector operations. &nbsp;Netlib provides a <a href="http://www.netlib.org/blas/">reference implementation of BLAS written in Fortran</a>, but the big idea behind BLAS is to allow hardware vendors to provide <a href="http://www.netlib.org/blas/faq.html#5">highly tuned versions of the BLAS subroutines</a> that obviate the need for application developers to worry about optimizing their linear algebra for every possible computer architecture on which the application might run. &nbsp;This motivation is also what gave rise to the MPI standard, but unlike MPI, BLAS is not an actual standard.</div>
<div class="p1"><br /></div>
<div class="p1"><b>LAPACK</b> builds upon BLAS and provides higher-level matrix operations such as diagonalization (i.e., solving for eigenvectors and eigenvalues) and inversion. &nbsp;BLAS and LAPACK seem to be bundled together when actually implemented (e.g., IBM ESSL and Intel MKL both provide both optimized BLAS and LAPACK), but they provide two distinct layers of abstracting the mathematical complexity away from application developers.</div>
<div class="p1"><br /></div>
<div class="p1"><b>ScaLAPACK</b> builds upon LAPACK and provides a set of subroutines (prefixed with the letter P) that are analogous to the subroutines provided by LAPACK. &nbsp;The big difference is that ScaLAPACK uses MPI to parallelize these LAPACK routines, whereas LAPACK itself (and the underlying BLAS) are completely serial (e.g., Netlib's reference distribution) or rely on shared memory for parallelization (e.g., multithreaded).</div>
<div class="p1"><br /></div>
<div class="p1">ScaLAPACK is where things get a little hairy because it not only relies on BLAS as an abstraction layer for doing computations, but it relies on the <b>BLACS</b> library to abstract away the inter-node communications. &nbsp;The MPI standard is supposed to do much of the same thing though, and in fact BLACS now only supports MPI, making it somewhat of an antiquated layer of abstraction. &nbsp;It follows that most vendors seem to optimize their MPI libraries and leave BLACS unchanged relative to the reference distribution.</div>
<div class="p1"><br /></div>
<div class="p1">As I'll mention below, BLACS is a growing source of problems with ScaLAPACK. &nbsp;BLACS is <a href="http://mailman.cse.ohio-state.edu/pipermail/mvapich-discuss/2013-May/004434.html">known to have non-deterministic behavior</a> which renders it sensitive to the MPI implementation upon which is layered, causing ScaLAPACK to not work under similarly non-deterministic conditions.</div>
<div class="p1"><br /></div>
<div class="p1"><span style="font-size: xx-small;">[1] I have a compelling excuse though! &nbsp;I got my start in scientific computing doing molecular dynamics simulations, and there just isn't a great deal of linear algebra required to calculate most models. &nbsp;I did work on <a href="http://dx.doi.org/10.1021/jp207181s">an electronegativity-based model that required solving big systems of equations</a>, but we found that there were more efficient ways to tackle the underlying physical problem like <a href="http://dx.doi.org/10.1063/1.2206578">using a clever extended Lagrangian methods</a>.</span></div>
<div class="p1"><br /></div>
<h2>Building Quantum ESPRESSO</h2><div class="p1">Customizing a build of Quantum ESPRESSO isn't completely standard compared to most non-scientific Linux packages, but it's miles ahead of most scientific packages in that it uses autoconf instead of a home-cooked build process.<br /><br /><h3>Choice of Libraries</h3>There are a few key factors to define when building Quantum ESPRESSO. &nbsp;As you may have guessed from the previous section, they are (in no particular order):<br /><ul><li>choice of compiler</li><li>choice of MPI implementation</li><li>choice of BLAS library</li><li>choice of LAPACK library</li><li>choice of ScaLAPACK library</li><li>choice of FFT library</li></ul></div>
<div class="p1">On most academic systems like SDSC's Gordon and Trestles, there are several options available for each one of these parameters, and figuring out (1) how to actually define your choice for each, and (2) determine which provides the best performance can be a bear. &nbsp;What's worse is that these choices are often tied together; for example, the best ScaLAPACK implementation might not be compatible with the best FFT library.<br /><br />Gordon and Trestles provide the following options:<br /><br /><br /><table style="margin: 0 auto;"><thead><tr><th>Compiler</th><th>Options</th></tr></thead><tbody><tr><td>MPI</td><td>Intel and PGI</td></tr><tr><td>BLAS</td><td>MVAPICH2 and OpenMPI</td></tr><tr><td>LAPACK</td><td>MKL, ACML, and Netlib Reference</td></tr><tr><td>ScaLAPACK</td><td>MKL and Netlib Reference</td></tr><tr><td>FFTs</td><td>MKL, ACML, or FFTW3</td></tr></tbody></table><br />There are actually more than this (e.g., GNU compilers and the MPICH implementation), but I did not test them.<br /><div><br /></div>
<h3>Passing Library Choices to the Build Process</h3><div>As of Quantum ESPRESSO 5.0.3, which is what I used here, you can't specify libraries in the autoconf-standard way (e.g., <span style="font-family: Courier New, Courier, monospace;">--with-lapack=/opt/lapack/...</span>). &nbsp;I suspect this is because the actual implementations these libraries don't follow a standard convention (e.g., LAPACK calls aren't necessarily in a shared object called&nbsp;<span style="font-family: Courier New, Courier, monospace;">liblapack.so</span>), but the QE build process <i>does</i> honor certain environment variables.</div>
<div><br /></div>
<div><b>To specify compiler</b>, you can simply set the <span style="font-family: Courier New, Courier, monospace;">CC</span>, <span style="font-family: Courier New, Courier, monospace;">FC</span>, and <span style="font-family: Courier New, Courier, monospace;">F77</span> environment variables as with any other application that uses autoconf, e.g.,</div>
<blockquote style="font-family: Courier New, Courier, monospace; font-size: smaller; text-align: left;">export CC=icc<br />export FC=ifort<br />export F77=ifort</blockquote><div>QE will actually pick up any proprietary compiler in your <span style="font-family: Courier New, Courier, monospace;">$PATH</span> before it reverts to the GNU compilers, which is a surprisingly sensible approach. &nbsp;On SDSC's machines, as long as you have the intel or pgi modules loaded, just plain old <span style="font-family: Courier New, Courier, monospace;">./configure</span> will pick it up.<br /><br /><b>The MPI stack</b> will be automatically detected based on whatever <span style="font-family: Courier New, Courier, monospace;">mpif90</span> is in your path. &nbsp;Again, as long as you have a valid MPI module loaded (<span style="font-family: Courier New, Courier, monospace;">openmpi_ib</span> or <span style="font-family: Courier New, Courier, monospace;">mvapich2_ib</span> on Gordon/Trestles), you don't have to do anything special.<br /><br /><b>The BLAS implementation</b> is selected by setting the <span style="font-family: Courier New, Courier, monospace;">BLAS_LIBS</span> environment variable to the appropriate link-time options. &nbsp;For example, the Netlib reference BLAS compiled with the Intel compiler is installed in /opt/lapack/intel/lib on SDSC's machines; thus, your <span style="font-family: Courier New, Courier, monospace;">BLAS_LIBS</span> should be passed to configure as<br /><blockquote style="font-family: Courier New, Courier, monospace; font-size: smaller; text-align: left;">export BLAS_LIBS="-L/opt/lapack/intel/lib -lblas"</blockquote>Similarly, <b>the LAPACK implementation</b> can be specified using the <span style="font-family: Courier New, Courier, monospace;">LAPACK_LIBS</span> environment variable. &nbsp;At SDSC, we install the Netlib BLAS and LAPACK in the same directory, so your <span style="font-family: Courier New, Courier, monospace;">LAPACK_LIBS</span> should actually contain the same library path as <span style="font-family: Courier New, Courier, monospace;">BLAS_LIBS</span>:<br /><blockquote style="font-family: Courier New, Courier, monospace; font-size: smaller; text-align: left;">export LAPACK_LIBS="-L/opt/lapack/intel/lib -llapack"</blockquote>We (and many other supercomputing sites) provide a handy dandy environment variable when you load this <span style="font-family: Courier New, Courier, monospace;">lapack</span> module called <span style="font-family: Courier New, Courier, monospace;">$LAPACKHOME</span>. &nbsp;With this environment variable, you can specify the generic (non-compiler-specific) line to configure:<br /><blockquote style="font-family: Courier New, Courier, monospace; font-size: smaller; text-align: left;">export BLAS_LIBS="-L$LAPACKHOME/lib -lblas" <br />export LAPACK_LIBS="-L$LAPACKHOME/lib -llapack"</blockquote>for convenience.<br /><br /><b>The ScaLAPACK libraries</b> are much the same and are passed to autoconf via the SCALAPACK_LIBS environment variable. &nbsp;To use the Netlib reference on Gordon/Trestles, you can load the <span style="font-family: Courier New, Courier, monospace;">scalapack</span> module and to configure:<br /><blockquote style="font-family: Courier New, Courier, monospace; font-size: smaller; text-align: left;">export SCALAPACK_LIBS="-L$SCALAPACKHOME/lib -lscalapack"</blockquote>Finally, <b>the&nbsp;FFT libraries</b> are defined via the <span style="font-family: Courier New, Courier, monospace;">FFT_LIBS</span> environment variable. &nbsp;To use our fftw installation, <span style="font-family: Courier New, Courier, monospace;">module load fftw</span> and configure:<br /><blockquote style="font-family: Courier New, Courier, monospace; font-size: smaller; text-align: left;">export FFT_LIBS="-L$FFTWHOME/lib -lfftw3"</blockquote>This is all well and good, but using the reference implementations for BLAS and LAPACK, as I will show, will result in very poor performance.<br /><br /><h3>Using Vendor-Optimized Libraries</h3><div><br /></div>
<h4>Intel</h4>Since none of these libraries are really standardized, vendors are free to bury their API wrappers in whatever libraries they want and support them to whatever extent they want. &nbsp;Intel's compilers come bundled with their Math Kernel Library (MKL) which provides bindings for<br /><ul><li><b>BLAS:</b><br /><span style="font-family: Courier New, Courier, monospace; font-size: smaller;">BLAS_LIBS="-lmkl_intel_lp64 -lmkl_sequential -lmkl_core"</span></li><li><b>LAPACK:</b><br /><span style="font-family: Courier New, Courier, monospace; font-size: smaller;">LAPACK_LIBS</span> can be left as the default since BLAS and LAPACK are buried in the same libraries</li><li><b>ScaLAPACK/BLACS:</b><br /><span style="font-family: Courier New, Courier, monospace; font-size: smaller;">SCALAPACK_LIBS="-lmkl_scalapack_lp64 -lmkl_blacs_openmpi_lp64"</span> for OpenMPI <b>OR</b><br /><span style="font-family: Courier New, Courier, monospace; font-size: smaller;">SCALAPACK_LIBS="-lmkl_scalapack_lp64 -lmkl_blacs_intelmpi_lp64"</span> for MVAPICH2</li><li><b>FFTW</b>:<br /><span style="font-family: Courier New, Courier, monospace; font-size: smaller;">FFT_LIBS="-lmkl_intel_lp64 -lmkl_sequential -lmkl_core"</span> for modern versions of MKL; older versions had the FFTW3 bindings in a separate library</li></ul><div>so your final configure command should look something like<br /><blockquote style="font-family: 'Courier New', Courier, monospace; font-size: smaller;">./configure \<br />&nbsp; CC=icc \<br />&nbsp; CXX=icpc \<br />&nbsp; FC=ifort \<br />&nbsp; F77=ifort \<br />&nbsp; BLAS_LIBS="-lmkl_intel_lp64 -lmkl_sequential -lmkl_core" \<br />&nbsp; SCALAPACK_LIBS="-lmkl_scalapack_lp64 -lmkl_blacs_openmpi_lp64" \<br />&nbsp; FFT_LIBS="-lmkl_intel_lp64 -lmkl_sequential -lmkl_core"</blockquote>when compiling with OpenMPI, or with a slightly modified <span style="font-family: Courier New, Courier, monospace;">SCALAPACK_LIBS</span> line (<span style="font-family: Courier New, Courier, monospace;">-lmkl_blacs_intelmpi_lp64</span>) when compiling with MVAPICH2.<br /><br /><h4>PGI/AMD</h4>PGI's compilers come bundled with the AMD Core Math Library (ACML), which provides bindings for BLAS, LAPACK, and FFTW, but its lack of ScaLAPACK means we still must use Netlib's ScaLAPACK and BLACS libraries. &nbsp;Be sure to load the pgi module, your preferred MPI module, and the scalapack module first!<br /><ul><li><b>BLAS:</b><br /><span style="font-family: Courier New, Courier, monospace; font-size: smaller;">BLAS_LIBS="-L$PGIHOME/libso -lacml"</span></li><li><b>LAPACK:</b><br /><span style="font-family: Courier New, Courier, monospace; font-size: smaller;">LAPACK_LIBS</span>&nbsp;can be left as the default since BLAS and LAPACK are buried in the same ACML library</li><li><b>ScaLAPACK/BLACS:</b><br /><span style="font-family: Courier New, Courier, monospace; font-size: smaller;">SCALAPACK_LIBS="-L$SCALAPACKHOME/lib -lscalapack"</span></li><li><b>FFTW</b>:<br /><span style="font-family: Courier New, Courier, monospace; font-size: smaller;">FFT_LIBS="-L$PGIHOME/libso -lacml" </span>even though ACML is included in the <span style="font-family: Courier New, Courier, monospace;">$BLAS_LIBS</span> variable--this is because autoconf may pick up a system fftw library which needs to be superceded by the FFTW bindings in ACML.</li></ul><div>so your final configure command should look something like<br /><blockquote style="font-family: 'Courier New', Courier, monospace; font-size: smaller;">./configure \<br />&nbsp; CC=pgcc \<br />&nbsp; CXX=pgCC \<br />&nbsp; FC=pgf90 \<br />&nbsp; F77=pgf77 \<br />&nbsp; BLAS_LIBS="-L$PGIHOME/libso -lacml" \<br />&nbsp; SCALAPACK_LIBS="-L$SCALAPACKHOME/lib -lscalapack" \<br />&nbsp; FFT_LIBS="-L$PGIHOME/libso -lacml"</blockquote>After doing this, there is one additional bit of manual hacking that must be done! &nbsp;PGI is known to trigger problems in Quantum ESPRESSO's IO library, IOTK, and you will need to compile with the <span style="font-family: Courier New, Courier, monospace;">-D__IOTK_WORKAROUND1</span> switch enabled. &nbsp;This command will hack the necessary line in <span style="font-family: Courier New, Courier, monospace;">make.sys</span>:<br /><blockquote><span style="font-family: Courier New, Courier, monospace; font-size: x-small;">sed -i 's/^DFLAGS\(.*\)$/DFLAGS\1 -D__IOTK_WORKAROUND1/' make.sys</span></blockquote>I owe a lot of gratitude to <a href="http://filippospiga.me/">Filippo Spiga</a> of Cambridge/the Quantum ESPRESSO Foundation for helping me quickly work through some of the issues I encountered in getting all of these builds to work correctly.<br /><br />In my next post, I will show what effect all of these options has on actual application performance.</div>
</div>
</div>
</div>