---
author: Glenn K. Lockwood's Blog
author_tag: glennklockwood
blog_subtitle: Personal thoughts and opinions of a supercomputing enthusiast
blog_title: Glenn K. Lockwood
blog_url: https://glennklockwood.blogspot.com/search/label/hpc
category: glennklockwood
date: '2022-05-27 06:42:00'
layout: post
original_url: https://glennklockwood.blogspot.com/2022/05/life-and-leaving-nersc.html
slug: life-and-leaving-nersc
title: Life and leaving NERSC
---

<bound method Tag.renderContents of <p>When word started to spread that I was leaving my job at NERSC for Microsoft, a lot of people either directly or indirectly attributed my decision to being one motivated by money.  Rationalizing my decision to leave is certainly a lot easier with this "Glenn was lured away with bags of cash" narrative, but that wasn't really a factor when I chose to move on.  Rather, my decision is a reflection of where I see the world of HPC going in the coming decade and where I personally wanted to position myself.  For my own therapeutic reasons (and perhaps the benefit of anyone interested in what it's like to work within, and subsequently leave, the DOE HPC complex), I'll try to write it all out here.<span></span></p>
<p></p>
<h2 style="text-align: left;">Working at NERSC</h2><p>First things first: NERSC has been a wonderful place to work.</p>
<div class="separator" style="clear: both; text-align: center;"></div>
<b><div style="text-align: center;"><b><span style="font-size: x-small;">A typical view from outside NERSC's facility in Berkeley after work during the winter months.  Yes, it really does look like this.</span></b></div>
</b><p>When I started in mid-2015, I came in with about three years of prior work experience (two at SDSC doing user support and one at a biotech startup) and knew a little bit about a lot of things in HPC.  But I didn't really know the basics of I/O or storage--I couldn't tell you what "POSIX I/O" really meant or how GPFS worked.  The fact that I got to help author <a href="https://www.nersc.gov/news-publications/nersc-news/nersc-center-news/2017/new-storage-2020-report-outlines-future-hpc-storage-vision/">NERSC's ten-year strategy around storage</a> in just two years, was invited to present <a href="https://insidehpc.com/2019/08/designing-future-flash-storage-systems-for-hpc-and-beyond/">my view on how to bridge the gap between HPC and enterprise storage</a> at Samsung's North American headquarters a year later, and was trusted to oversee <a href="https://www.nextplatform.com/2021/06/07/a-35-petabyte-all-flash-balancing-act/">the design and execution of the world's first 35 petabyte all-flash Lustre file system</a> through my first four years is a testament to how much opportunity is available to learn and grow at NERSC.</p>
<p>There are a couple of reasons for this.</p>
<h3 style="text-align: left;">Stable funding</h3><p>Perhaps foremost, NERSC (and DOE's Leadership Computing Facilities, ALCF and OLCF) enjoy healthy budgets and financial stability since worldwide leadership in scientific advancement is generally a national priority by both major political parties in the US.  This means that, regardless of who is president and which party holds majorities in Congress, the DOE HPC facilities can pay their employees and deploy new supercomputers.  This solid funding makes it much easier to invest in staff development and long-term planning; I was able to become a resident I/O expert at NERSC because I was never forced to chase after the funding du jour to make ends meet.  Congress trusts NERSC to allocate its funding responsibly, and NERSC prioritized letting me learn as much as I could without distraction.</p>
<h3 style="text-align: left;">Instant credibility and access</h3><p>Second, <a href="https://twitter.com/hpcprogrammer/status/1061278775353196544?s=20&amp;t=_YGQXWvykuCElqltJ-x09Q">having a NERSC affiliation gives you instant credibility and access</a> in many cases.  It's not necessarily fair, but it's definitely true.  Within my first year at NERSC, I was invited to give <a href="https://archive.siam.org/meetings/pp16/pp16_program.pdf">a presentation about I/O performance monitoring in Paris</a> because the organizer wanted a lineup of speakers from all the big players in HPC.  I had never been to Europe at that point in my life, but being the I/O guy from NERSC (and being able to present well!) was enough to get me there.  And it was during that trip to Paris that I got to meet--and literally have conversation over dinner with--<a href="https://www.linkedin.com/in/larry-kaplan-b101936">more</a> <a href="https://people.llnl.gov/tgamblin">industry</a> <a href="https://en.wikipedia.org/wiki/David_E._Keyes">bigshots</a> that I can remember.  And that trip to Paris was not an outlier; pandemic aside, NERSC let me go to Europe at least once or twice every year I've worked there.</p>
<div class="separator" style="clear: both; text-align: center;"></div>
<b><div style="text-align: center;"><b><span style="font-size: x-small;">The first photo I ever took of Notre Dame on the first day I'd ever set foot in Europe.  NERSC sent me there less than a year after I started.</span></b></div>
</b><p>Of course, this is not to say that every employee at a DOE HPC facility is wining and dining in Paris every summer.  Many of these opportunities are earned by showing the value of the work you're doing, just like at any job.  But owing to healthy budgets, travel expenses are rarely the limiting factor in chasing after these opportunities.  In addition, going out into the world and talking about what you do is part of the job at a DOE facility; being a leader in the field of HPC is part of the mission of NERSC, ALCF, and OLCF, so doing high-risk, first-of-a-kind work <i>and telling the world about it</i> is uniquely valued within DOE in a way that it is not in industry.</p>
<h3 style="text-align: left;">Smart people</h3><p>A product of these two factors (stable budget and instant credibility) results in coworkers and colleagues who are generally very experienced and capable.  There's an interesting mix of laissez-faire management and rigorous process-driven management as a result.</p>
<p>Staff are generally given the freedom to choose their own destiny and focus on work that they enjoy much like in any academic environment; it's not hard to pick up passion projects or even move between groups if things get stale on a day-to-day basis.  Since everyone is working on their own slices of HPC, there's also easy access to world experts in different areas of technology if you need one.  For example, I recall once reviewing a storage system that appeared to rely on multiplexing two 12G SAS links over a single 24G SAS.  After one email and a few hours, a coworker confirmed, complete with a citation to the SCSI standards, that this was totally possible.  Even if someone in-house didn't know the answer, I had direct access to an engineering manager at a leading storage vendor who owed me a favor and definitely would've known the answer.  It's really, really hard to find as many smart people in arm's reach in most other HPC centers. </p>
<p>At the same time, there is rigorous federal oversight on major projects and procurements to ensure that taxpayer dollars are responsibly spent.  This is a double-edged sword because all of the reporting and reviews that go into <a href="https://www.energy.gov/articles/doe-build-next-generation-supercomputer-lawrence-berkeley-national-laboratory">massive</a> <a href="https://www.ornl.gov/news/us-department-energy-and-cray-deliver-record-setting-frontier-supercomputer-ornl">capital</a> <a href="https://www.energy.gov/articles/us-department-energy-and-intel-build-first-exascale-supercomputer">projects</a> make forward progress very slow at times.  All DOE HPC facilities review and re-review everything about these giant supercomputers before making a decision, so by the time the public sees a press release about a new supercomputer, lab staff have spent literal years going over every detail and risk.  It sometimes may not seem that way (how many problems has Aurora had?), but rest assured that every schedule slip or technology change the public hears was preceded by countless hours of meetings about risk and cost minimization.  On the flip-side though, you have the opportunity to learn every gory detail about the system directly from the people who designed it.</p>
<h3 style="text-align: left;">Pay</h3><p>In <a href="https://www.bankrate.com/banking/federal-reserve/younger-workers-sharing-salaries/">true millennial fashion</a>, I think it's important to have an open discussion about the pay.  DOE labs pay more than any other HPC facility in the world as far as I am aware, and even in the San Francisco Bay Area, salary at NERSC is comparable to the base salaries offered by all the big tech companies.  You can get an idea of what entry-level salaries (think: first job after postdoc or a few years out of undergrad) by searching <a href="https://h1bdata.info/">H1B Visa postings</a>, and anecdotally, I'd wager that a typical HPC job at NERSC pays about 2x that of the same job at a typical US university and 3x-4x that of the same job at a British or European university.  All the labs pay about the same to boot, so an HPC job at somewhere like Oak Ridge can afford you a relatively luxurious lifestyle.</p>
<p>Don't get me wrong though; affording to buy a Bay Area house on a single NERSC salary alone would be tough in the same way that buying a Bay Area house on any single salary would be.  And while NERSC's compensation is comparable to the <i>base</i> salary of the big tech companies, that base is about all you can get since DOE labs cannot offer equity or substantial bonuses.  This is less of a gap if you're just starting out, but anyone who's <a href="https://www.levels.fyi/">looked at compensation structures in tech</a> knows that stock-based compensation, not base salary, dominates total compensation as you move up.</p>
<p>So, if money wasn't an issue for me and NERSC is such a great place to work, why would I ever leave?</p>
<h2 style="text-align: left;">The road ahead for HPC</h2><p>On one hand, HPC's future has never been brighter thanks to how much life (and money!) the AI industry is bringing to the development of HPC technologies.  We have new <a href="https://vastdata.com/">all-flash</a> <a href="https://www.weka.io/">file systems</a>, <a href="https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/">gigantic GPUs</a>, awesome <a href="https://www.tomshardware.com/news/intels-sapphire-rapids-to-have-64-gigabytes-of-hbm2e-memory">CPU memory technologies</a>, and <a href="https://arxiv.org/abs/2205.12182">mixed-precision techniques</a> in the HPC space that were all directly driven by developments primarily intended for AI workloads.  On the other hand, leadership HPC appears to be engaging in unsustainable brinkmanship while midrange HPC is having its value completely undercut by cloud vendors.  I've <a href="https://glennklockwood.blogspot.com/2020/05/exascales-long-shadow-and-hpc-being.html">not been shy about my overall anxiety about where HPC is going</a> because of this, but I'll elaborate now that the exascale race has been won.</p>
<h3 style="text-align: left;">The future of leadership HPC</h3><p>Without some monumental breakthrough in transistor technology, there is only one path forward in continuing to build faster and faster supercomputers in the next decade: pour more and more energy (and dissipate more and more heat) into larger and larger (and more and more) GPUs.</p>
<p>The goal post for exascale power keeps moving because that's been the easiest way to hit the mythical exaflop milestone; while the original goal was 20 MW, <a href="https://www.nextplatform.com/2021/10/04/first-look-at-oak-ridges-frontier-exascaler-contrasted-to-argonnes-aurora/">Frontier is coming in at 29 MW</a> and <a href="https://www.tomshardware.com/news/nvidia-amd-polaris-supercomputer-department-of-energy">Aurora at "under 60 MW."</a>  Not only is this just a lot of power to feed into a single room, but the <a href="https://www.olcf.ornl.gov/2020/09/23/powering-frontier/">cost and effort</a> of actually <a href="https://www.llnl.gov/news/powering-llnl-prepares-exascale-massive-energy-and-water-upgrade">building this infrastructure</a> is <a href="https://www.lanl.gov/asc/fous/sixty-megawatts-power-available-2025.php">newsworthy</a> in and of itself these days.  At the current trajectory, the cost of building a new data center and extensive power and cooling infrastructure for every new leadership supercomputer is going to become prohibitive very soon.</p>
<p>HPC data centers situated in places where the cost of electricity and real estate (stacked atop the risk of earthquake or wildfire) further skew the economics of just adding more power are going to run up against this first.  It used to be easy to dismiss these practicality concerns by arguing that colocating scientists with supercomputers created immeasurable synergy and exchange of ideas, but the fact that science never stopped during the work-from-home days of the pandemic have taken a lot of air out of that argument.</p>
<p>My guess is that all the 50-60 MW data centers being built for the exascale supercomputers will be the last of their kind, and that there will be no public appetite to keep doubling down.</p>
<p>Given this, DOE's leadership computing facilities are facing an existential threat: how do you define leadership computing after exascale if you can't just add another 50% more power into your facility?  How do you justify spending another $600 million for a supercomputer that uses the same power but only delivers 15% more performance?  You can pour similarly huge amounts of money into application modernization to accelerate science, but at the end of the day, you'd still be buying a lot of hardware that's not a lot faster.</p>
<h3 style="text-align: left;">The future of places like NERSC</h3><p>NERSC is probably a little better off since its lack of an exascale machine today gives it at least one more turn of the crank before it hits a hard power limit in its data center.  That gives it the ability to deploy at least one more system after Perlmutter that is significantly (at least 2x) more capable but draws significantly more power.  However, compared to Frontier and Aurora, such a system may still look rather silly when it lands in the same way that Perlmutter looks a bit silly compared Summit, which was funded by the same agency but deployed years earlier.</p>
<p>And therein lies the dilemma of centers like NERSC--how do you position yourself now so that by the time you deploy an HPC system that is close to maxing out on power, it is sufficiently different from a pure-FLOPS leadership system that it can solve problems that the leadership systems cannot?</p>
<p>The easy go-to solution is to craft a story around "data-centric" supercomputing.  We did this when I was at the San Diego Supercomputer Center when we were budget-limited and had to differentiate our $12 million Comet supercomputer from TACC's $30 million Stampede.  You invest more in the file system than you would for a pure-FLOPS play, you provide low-cost but high-value onramps like Jupyter and science gateways to enable new science communities that have modest computing needs, and you fiddle with policies like allocations and queue priority to better suit interactive and urgent computing workloads.  From a productivity standpoint, this is can be a great story since users will always respond well to lower queue wait times and less frustrations with the file system.  From a system architect's standpoint, though, this is really boring.  The innovation happens in policies and software, not clever hardware or design, so there's very little that's new for a system designer to think about in this case.</p>
<p>A more innovative approach is to start thinking about how to build a system that does more than just run batch jobs.  Perhaps it gives you a private, fast file system where you can store all your data in a way indistinguishable from your personal laptop.  Perhaps it gives you a convenient place to run a Jupyter notebook that has immediate access to a powerful GPU.  Or perhaps it gives you all the tools to set up an automated process where all you have to do is upload a file to trigger an automatic data analysis and reduction pipeline that returns its output to a shiny HTTP interface.  Such a system may not be able to crank out an exaflop using HPL, but does that matter if it's the only system in the country that supports such automation?</p>
<p>There <i>are</i> interesting system architecture questions in the latter case, so as a system designer, I much prefer it over the "data-centric" angle to non-exaflop supercomputing strategies.  But there remains a problem.</p>
<h3 style="text-align: left;">The problem: cloud</h3><p>Such a "more than just batch jobs" supercomputer actually already exists.  It's called the cloud, and it's far, far ahead of where state-of-the-art large-scale HPC is today--it pioneered the idea of providing an integrated platform where you can twist the infrastructure and its services to exactly fit what you want to get done.  Triggering data analysis based on the arrival of new data has been around for the better part of a decade in the form of serverless computing frameworks like <a href="https://docs.microsoft.com/en-us/learn/modules/execute-azure-function-with-triggers/2-determine-best-trigger">Azure Functions</a>.  If you need to run a Jupyter notebook on a server that has a beefy GPU on it, just pop a few quarters into your favorite cloud provider.  And if you don't even want to worry about what infrastructure you need to make your Jupyter-based machine learning workload go fast, the cloud providers all have <a href="https://docs.microsoft.com/en-us/azure/machine-learning/overview-what-is-machine-learning-studio">integrated machine learning development environments</a> that hide all of the underlying infrastructure.</p>
<p>And therein lies the problem: the definition of "innovation" as non-exaflop HPC runs up against this power wall might actually mean "catching up to the cloud."</p>
<p>This is not to say that NERSC-like HPC centers are entirely behind the cloud; all the DOE HPC facilities have bigger, faster, and more convenient parallel file systems that are generally always on and where data is always somewhere "fast."  They also provide familiar, managed software environments and more egalitarian support to small- to mid-scale science projects.  DOE HPC also takes the most risk in deploying unproven technologies to shake them out before they become available to the wide market.</p>
<p>However, those gaps are beginning to close.  You can stick <a href="https://azure.microsoft.com/en-us/solutions/high-performance-computing/cray/">a full Cray EX system, identical to what you might find at NERSC or OLCF, inside Azure</a> nowadays and avoid that whole burdensome mess of building out a 50 MW data center.  You can also integrate such a system with all the rich infrastructure features the cloud has to offer like triggered functions.  And when it comes to being first to market for risky HPC hardware, the cloud has already caught up in many ways--<a href="https://azure.microsoft.com/en-us/blog/azure-hbv3-virtual-machines-for-hpc-now-up-to-80-percent-faster-with-amd-milanx-cpus/">Microsoft deployed AMD Milan-X CPUs in their data centers</a> before any HPC shop did, and more recently, <a href="https://www.theregister.com/2022/05/26/amd_azure_microsoft/">Microsoft invested in AMD MI-200 GPUs</a> before Frontier had a chance to shake them out.</p>
<p>Given this steep trajectory, I see only two scenarios for large-scale, non-exaflop HPC facilities in the 10+ year horizon:</p>
<p></p>
<ol style="text-align: left;"><li>They develop, adopt, steal, or squish cloud technologies into their supercomputers to make them functionally equivalent to cloud HPC deployments.  They may be a little friendlier to scientific users since cloud functionality wasn't designed for scientific computing alone, but they also may not be as stable, mature, or feature-rich as their cloud cousins.</li><li>They find better overall economics in eventually moving to <a href="https://www.hpcwire.com/2021/05/13/behind-the-met-offices-procurement-of-a-billion-dollar-microsoft-system/">massive, long-term, billion-dollar deals</a> where flagship HPC systems and their "more than just batch jobs" features are colocated inside cloud datacenters sited at economically advantageous (that is, cheap power, cooling, and labor) locations in the country.</li></ol><p>There's also grey area in between where national HPC facilities consolidate their physical infrastructure in cheap areas to manage costs but still self-manage their infrastructure rather than fully outsource to a commercial cloud.  <a href="https://ethz.ch/en/news-and-events/eth-news/news/2021/03/we-dont-just-procure-a-new-computer.html">CSCS has hinted at this model as their future plan</a> since they cannot build 100 MW datacenters in Switzerland, and this is proof that leading HPC facilities around the world see the writing on the wall and need to maneuver now to ensure they remain relevant beyond the next decade.  Unfortunately, the politics of consolidating the physical infrastructure across the DOE HPC sites would likely be mired in Congressional politics and take at least a decade to work out.  Since serious work towards this hasn't started yet, I don't envision such a grey-area solution emerging before all the DOE facilities hit their power limit.</p>
<p>Hopefully I've painted a picture of how I perceive the road ahead for large-scale HPC facilities and you can guess which one I think will win out.</p>
<h2 style="text-align: left;">Final thoughts</h2><p>I have every confidence that there will still be DOE HPC facilities in ten years and that they will still be staffed by some of the brightest minds in HPC.  And even if a cloud-based HPC facility ultimately consumes centers like NERSC, I don't think many people would be out of work.  The vast majority of what DOE's HPC people do is think carefully about technology trends, maintain a deep understanding of user requirements, provide excellent support to its thousands of users, and keep complex supercomputers running well.  Those jobs don't go away if the supercomputer is in the cloud; it's just the physical location, the hands doing physical hardware swaps, and the breadth of vendor interactions that may change.</p>
<p>For me as a system architect though, it's become too hard for me to catch up to all the new technologies and techniques HPC needs for the future while also building up other staff to be masters of today's I/O challenges.  I found myself at a fork in the road.  One path would mean catching up on a technical level and then getting in front of where the future of HPC lies before it gets there.  The other path would mean trying to steer the entire DOE HPC ship in the right direction, as long as that may take, and have faith that the people I bring along can race far enough ahead to tell me if we're still going where we need to go.  Perhaps a bit selfishly, I chose the former.  I'm just not ready to give up on racing ahead myself yet, and the only way I could hope to catch up was to make it a full-time job.</p>
<p>I don't claim to know the future, and a lot of what I've laid out is all speculative at best.  NERSC, ALCF, or OLCF very well may build another round of data centers to keep the DOE HPC party going for another decade.  However, there's no denying that the stakes keep getting higher with every passing year.</p>
<p>That all said, DOE has pulled off stranger things in the past, and it still has a bunch of talented people to make the best of whatever the future holds.</p>
<p></p>
>