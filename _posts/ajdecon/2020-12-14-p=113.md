---
author: Thinking Out Loud
author_tag: ajdecon
blog_subtitle: works in progress and scattered thoughts, often about computers
blog_title: Computing – thinking out loud
blog_url: https://thinking.ajdecon.org
category: ajdecon
date: '2020-12-14 01:03:23'
layout: post
original_url: https://thinking.ajdecon.org/2020/12/14/sketching-out-hpc-clusters-at-different-scales/
slug: sketching-out-hpc-clusters-at-different-scales
title: Sketching out HPC clusters at different scales
---

<p>High-performance computing (HPC) clusters come in a variety of shapes and sizes, depending on the scale of the problems you’re working on, the number of different people using the cluster, and what kinds of resources they need to use. </p>




<p>However, it’s often not clear what kinds of differences separate the kind of cluster you might build for your small research team:</p>




<figure class="wp-block-image size-large"><img alt="" class="wp-image-139" height="1339" src="https://thinking.ajdecon.org/wp-content/uploads/2020/12/image-9.jpg" width="1601" /><figcaption>Note: do not use in production</figcaption></figure>



<p>From the kind of cluster that might serve a large laboratory with many different researchers:</p>




<figure class="wp-block-image size-large"><img alt="" class="wp-image-140" height="470" src="https://thinking.ajdecon.org/wp-content/uploads/2020/12/img_0143.jpg" width="892" /><figcaption>The Trinity supercomputer at Los Alamos National Lab, also known as “that goddamn machine” when I used to get paged at 3am</figcaption></figure>



<p>There are lots of differences between a supercomputer and my toy Raspberry Pi cluster, but also a lot in common. From a management perspective, a big part of the difference is how many different specialized node types you might find in the larger system.</p>




<span id="more-113"></span>



<p><em>Just a note: in this post I’m assuming we’re talking about compute clusters of the type that might be used to run simulations or data analysis jobs. This probably won’t help if you’re designing a database cluster, a Kubernetes cluster to serve a web infrastructure, etc.</em></p>




<p>Let’s start with one of the simplest ways you can build a cluster: a collection of<strong> compute nodes</strong>, all connected to a network, with a single <strong>“head node</strong>” that coordinates work between them:</p>




<figure class="wp-block-image size-large"><img alt="Diagram showing a single head node connected to five compute  nodes with a single network" class="wp-image-118" height="524" src="https://thinking.ajdecon.org/wp-content/uploads/2020/12/image.jpg" width="910" /></figure>



<p>With this design, the head node performs most of the functions that coordinate work or provide shared services on the cluster. The compute nodes are then free for the actual compute jobs on the cluster, like simulating the weather or analyzing telescope data!</p>




<p>Some of the shared services that most clusters provide from the head node include:</p>




<ul><li>Running a <strong>job scheduler</strong> that accepts requests from the users and queues them up to run on the compute nodes</li><li>Exporting a <strong>shared filesystem </strong>to the other machines, so they can all access the same storage space</li><li>Accepting <strong>user logins</strong> so that the people who want to run on the cluster have an access point to the cluster</li><li>Acting as a <strong>management node</strong> that the cluster sysadmins can use to help maintain the rest of the cluster</li></ul>



<p>This kind of design can scale remarkably well, and it’s probably the most common kind of cluster out there. But at some point, you might find that the head node is doing too much, and you need to split its functions across multiple machines.</p>




<p>The first thing you’ll often see is moving user logins onto their own dedicated <strong>login node</strong>:</p>




<figure class="wp-block-image size-large"><img alt="Diagram showing a login node, a management node, and five compute nodes connected on the same network" class="wp-image-120" height="643" src="https://thinking.ajdecon.org/wp-content/uploads/2020/12/image-1.jpg" width="1317" /></figure>



<p>All the other functions are still on the head node (which is often explicitly called a <strong>management node</strong> at this point). But by moving user logins to their own node, it becomes easier to do maintenance or make changes to the larger system without disturbing your users. </p>




<p>(It also means that if your users accidentally crash the login node, they’re less likely to take down all those shared services on the management node&#8230;)</p>




<p>If you have lots of users, you can also easily add more login nodes! These scale pretty well because the shared services are all still on the management node, but your users get more interactive nodes for their development work</p>




<figure class="wp-block-image size-large"><img alt="Diagram showing three login nodes, a management node, and five compute nodes on the same network" class="wp-image-122" height="631" src="https://thinking.ajdecon.org/wp-content/uploads/2020/12/image-2.jpg" width="1269" /></figure>



<p>At this point, you might also set up a second management node in order to provide redundancy or failover in case your primary management node fails:</p>




<figure class="wp-block-image size-large"><img alt="Diagram showing three login nodes, two management nodes, and five compute nodes" class="wp-image-124" height="703" src="https://thinking.ajdecon.org/wp-content/uploads/2020/12/image-3.jpg" width="1291" /></figure>



<p>At this point we have a lot of compute nodes, redundant management nodes, and a nice collection of login nodes for the users to use for their work. What else might we need as we scale up?</p>




<p>Well, for one thing, the shared filesystem is still on the management node. We might want to split it off onto its own machine to provide better performance:</p>




<figure class="wp-block-image size-large"><img alt="Diagram showing three login nodes, two management nodes, a storage node, and five compute nodes on the same network" class="wp-image-126" height="777" src="https://thinking.ajdecon.org/wp-content/uploads/2020/12/image-4.jpg" width="1300" /><figcaption>Following tradition, storage is represented as a poorly-drawn cylinder to match the shape of a hard drive platter ?</figcaption></figure>



<p>Or if we want to scale our performance higher than a single storage server can provide, we might want to use a <strong>distributed filesystem</strong> like Lustre, BeeGFS, or GPFS and provide a whole tier of dedicated storage machines:</p>




<figure class="wp-block-image size-large"><img alt="Replace single storage node with three storage nodes in a cluster" class="wp-image-128" height="741" src="https://thinking.ajdecon.org/wp-content/uploads/2020/12/image-5.jpg" width="1341" /></figure>



<p>You might also notice that we’re using the same network for everything! Communication between compute nodes, access to storage, and management services are all competing to send messages over the same network. This could be a problem if, for example, the application wants to simultaneously read lots of data from storage and exchange messages with neighboring compute nodes. </p>




<p>At this point we may want to split these different types of traffic onto their own networks:</p>




<figure class="wp-block-image size-large"><img alt="Same diagram, but add a separate application network connecting only the compute nodes , and a separate storage network connecting storage and compute only" class="wp-image-130" height="1044" src="https://thinking.ajdecon.org/wp-content/uploads/2020/12/image-6.jpg" width="1590" /></figure>



<p>Depending on how much you need to optimize (or how much you want to spend!), you may have several different networks connecting all the machines in the cluster, separated by function. You may have dedicated networks for functions like:</p>




<ul><li><strong>High-speed network</strong> (or <strong>application network</strong>): This is a dedicated network for user applications to communicate between compute nodes, and is often built using specialized hardware like Infiniband or a vendor-proprietary technology. This is especially important if you use technologies like MPI in your applications, which rely heavily on inter-node communication.</li><li><strong>Storage network</strong>: This is a dedicated network for access to storage. If you rely on especially fast network storage, you might use Infiniband or another very fast network here too.</li><li><strong>Management network</strong>: This is often the “everything else” network, used for job scheduling, SSH, and other miscellaneous traffic. This is often a less-performant network, using 1Gb or 10Gb Ethernet, because we expect the heavier usage to be on the application or storage networks.</li><li><strong>Out-of-band management network</strong>: Many datacenter environments have methods for managing individual servers outside their operating systems, such as accessing the baseboard management controllers. However, this kind of access can be a security risk, and it’s often put on its own network to restrict access.</li></ul>



<p>All these different networks may be on their own hardware, for the best performance; or they may be virtual networks (VLANs) sharing the same physical connections. </p>




<p>Once you get past this point, there are many different ways to continue splitting off or adding special-purpose functions, but these are less common outside of very large sites.</p>




<p>For example, you may have multiple independent storage systems you want to access:</p>




<figure class="wp-block-image size-large"><img alt="Add a second storage cluster, separate from the first, on the storage network" class="wp-image-132" height="1293" src="https://thinking.ajdecon.org/wp-content/uploads/2020/12/image-7.jpg" width="1600" /></figure>



<p>Or your cluster may depend on fast access to an external resource, and you want to attach a dedicated tier of network routers:</p>




<figure class="wp-block-image size-large"><img alt="Add a pair of router nodes on the management node. The router nodes also have connections to the internet " class="wp-image-134" height="1398" src="https://thinking.ajdecon.org/wp-content/uploads/2020/12/image-8.jpg" width="1492" /></figure>



<p>Or you may even have some slower tier of storage that you need to move data in and out of, such as S3 or a tape system, and build a set of dedicated machines for data movement:</p>




<figure class="wp-block-image"><img alt="Add a pair of data movement nodes connected to the management nodes. The data movement nodes also have a connection to an external storage system" class="wp-image-135" height="1600" src="https://thinking.ajdecon.org/wp-content/uploads/2020/12/53d60f83-a8c7-447a-b528-43e61ff5e300-3966-00000359711b33bb_file.jpg" width="1600" /></figure>



<p>In other words, you can add as much complexity as you like! ? Or, as much as your users and workloads require. Very complex environments serving many researchers may have many different tiers of dedicated machines, for data movement, network routing, managing software licenses, and more. But not every environment will need this type of complexity.</p>




<p>In all cases, the general strategy is the same: if your work is being bottlenecked by some you special-purpose function, you may consider moving that work to dedicated machines to get better performance. </p>




<p>This needs to be balanced, though, against the costs of doing so, in money, power, rack space, or other constraints. Frequently, there’s a trade-off between adding special-purpose machines and adding more compute machines, and your users might prefer to just have more compute!</p>