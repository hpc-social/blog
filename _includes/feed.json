{
    "version": "https://jsonfeed.org/version/1",
    "title": "hpc.social - Aggregated Personal Blog",
    "home_page_url": "https://hpc.social/personal-blog/",
    "feed_url": "https://hpc.social/personal-blog/feed.json",
    "description": "Shared personal experiences and stories",
    "icon": "https://hpc.social/personal-blog/assets/images/apple-touch-icon.png",
    "favicon": "https://hpc.social/personal-blog/assets/images/favicon.png",
    "expired": false,
    
    "author":  {
        "name": "hpc.social",
        "url": null,
        "avatar": null
    },
    
"items": [
    
        {
            "id": "https://hpc.social/personal-blog/2024/profiling-the-xrootd-monitoring-collector/",
            "title": "Profiling the XRootD Monitoring Collector",
            "summary": null,
            "content_text": "The XRootD Monitoring Collector (collector) receives file transfer accounting messages from XRootD servers.This transfer information is parsed by the collector and sent to the GRACC accounting database for visualization.Each transfer will generate multiple messages:  Connection message with client information  Token information  File open with file name  Transfer updates (potentially multiple)  File close with statistics about bytes read and written  DisconnectionWe can see 1000+ messages a second from XRootD servers across the OSG.  But, recently the collector has not been able to keep up.  Below is the traffic of messages to the collector from the OSG’s Message Bus:        Message bus traffic before optimization    The graph is from the message bus’s perspective, so publish is incoming to the message bus, and deliver is sending to consumers (the Collector).  We are receiving (Publish) ~1550 messages a second, while the collector is only able to process (Deliver) ~500 messages a second.  1550 messages a second is higher than our average, but we need to be able to process data as fast as it comes.  Messages that are not processed will wait on the queue.  If the queue gets too large (maximum is set to 1 Million messages) then the messages will be deleted, losing valuable transfer accounting data.  At a defecit 1000 messages a second, it would only take ~16 minutes to fill the queue.  It is clear that we missed data for a significant amount of time.ProfilingThe first step to optimizing the XRootD Monitoring Collector is to profile the current process.  Profiling is the process of measuring the performance of the collector to identify bottlenecks and areas for improvement.For profiling, I created a development environment on the National Research Platform (NRP) to host the collector.  I started a jupyter notebook on the NRP, and used VSCode to edit the collector code and a Jupyter notebook to process the data.  I used the cProfile package built into python to perform the profiling.I modified the collector to output a profile update every 10 seconds so I could see the progress of the collector.After profiling, I used snakeviz to visualize the profile.  Below is a visualization of the profile before any optimization.  The largest consumer of processing time was DNS resoluiton, highlighted in the below image in purple.        Snakeviz profile.  Purple is the DNS resolution function    The collector uses DNS to resolve the hostnames for all IPs it receives in order to provide a human friendly name for clients and servers.  Significant DNS resolution is expected as the collector is receiving messages from many different hosts.  However, the DNS resolution is taking up a significant amount of time and is a bottleneck for the collector.ImprovementAfter reviewing the profile, I added a cache to the DNS resolution so that the collecotr only needs to resolve the host once every 24 hours.  When I profiled after making the change, I saw a significant improvement in DNS resolution time.  Below is another visualization of the profile after the DNS caching, purple is the DNS resolution.        Snakeviz profile.  Purple is the DNS resolution function    Notice that the DNS resolution is a much smaller portion of the overall running time when compared to the previous profile.In the following graph, I show the time spent on DNS resolution over time for both before and after the optimization.  I would expect DNS resolution to increase for both, but as you can see, the increase after adding DNS caching is much slower.        Growth of DNS resolution time    ProductionWhen we applied the changes into production, we saw a significant improvement in the collector’s ability to process messages.  Below is the graph of the OSG’s Message Bus after the change:        RabbitMQ Message Parsing    The incoming messages decreased, but the collector is now able to process messages as fast as they are received.  This is a significant improvement over the previous state.  I suspect that the decrease in incoming messages is due to server load of sending more outgoing messages to the improved collector.  The message bus can slow down the incoming messages under heavier load.Conclusions and Future WorkSince we implemented the cache for DNS resolution, the collector has been able to keep up with the incoming messages.  This is a significant improvement over the previous state.  Over time, we expect the DNS cache to capture nearly all of the hosts, and the DNS resolution time to decrease even further.We continue to look for optimizations to the collector.  When looking at the output from the most recent profile, we noticed the collector is spending a significant amount of time in the logging functions.  By default, we have debug logging turned on.  We will look at turning off debug logging in the future.Additionally, the collector is spending a lot of time polling for messages.  In fact, the message bus is receiving ~1500 messages a second, which is increasing the load on the message bus.  After reading through optimizations for RabbitMQ, it appears that less but larger messages are better for the message bus.  We will look at batching messages in the future.",
            "content_html": "<p>The <a href=\"https://github.com/opensciencegrid/xrootd-monitoring-collector\">XRootD Monitoring Collector</a> (collector) receives file transfer accounting messages from <a href=\"https://xrootd.slac.stanford.edu/\">XRootD</a> servers.This transfer information is parsed by the collector and sent to the GRACC accounting database for visualization.Each transfer will generate multiple messages:</p><ol>  <li>Connection message with client information</li>  <li>Token information</li>  <li>File open with file name</li>  <li>Transfer updates (potentially multiple)</li>  <li>File close with statistics about bytes read and written</li>  <li>Disconnection</li></ol><p>We can see 1000+ messages a second from XRootD servers across the OSG.  But, recently the collector has not been able to keep up.  Below is the traffic of messages to the collector from the OSG’s Message Bus:</p><figure class=\"\">  <img alt=\"this is a placeholder image\" src=\"https://derekweitzel.com/images/posts/profiling-xrootd-collector/before-optimization-mq.png\" /><figcaption>      Message bus traffic before optimization    </figcaption></figure><p>The graph is from the message bus’s perspective, so publish is incoming to the message bus, and deliver is sending to consumers (the Collector).  We are receiving (Publish) ~1550 messages a second, while the collector is only able to process (Deliver) ~500 messages a second.  1550 messages a second is higher than our average, but we need to be able to process data as fast as it comes.  Messages that are not processed will wait on the queue.  If the queue gets too large (maximum is set to 1 Million messages) then the messages will be deleted, losing valuable transfer accounting data.  At a defecit 1000 messages a second, it would only take ~16 minutes to fill the queue.  It is clear that we missed data for a significant amount of time.</p><h2 id=\"profiling\">Profiling</h2><p>The first step to optimizing the XRootD Monitoring Collector is to profile the current process.  Profiling is the process of measuring the performance of the collector to identify bottlenecks and areas for improvement.</p><p>For profiling, I created a development environment on the <a href=\"https://nationalresearchplatform.org/\">National Research Platform (NRP)</a> to host the collector.  I started a <a href=\"https://docs.nationalresearchplatform.org/userdocs/jupyter/jupyterhub-service/\">jupyter notebook on the NRP</a>, and used VSCode to edit the collector code and a Jupyter notebook to process the data.  I used the <a href=\"https://docs.python.org/3/library/profile.html\">cProfile</a> package built into python to perform the profiling.I modified the collector to output a profile update every 10 seconds so I could see the progress of the collector.</p><p>After profiling, I used <a href=\"https://jiffyclub.github.io/snakeviz/\">snakeviz</a> to visualize the profile.  Below is a visualization of the profile before any optimization.  The largest consumer of processing time was DNS resoluiton, highlighted in the below image in purple.</p><figure class=\"\">  <img alt=\"this is a placeholder image\" src=\"https://derekweitzel.com/images/posts/profiling-xrootd-collector/before-optimization-profile.png\" /><figcaption>      Snakeviz profile.  Purple is the DNS resolution function    </figcaption></figure><p>The collector uses DNS to resolve the hostnames for all IPs it receives in order to provide a human friendly name for clients and servers.  Significant DNS resolution is expected as the collector is receiving messages from many different hosts.  However, the DNS resolution is taking up a significant amount of time and is a bottleneck for the collector.</p><h2 id=\"improvement\">Improvement</h2><p>After reviewing the profile, <a href=\"https://github.com/opensciencegrid/xrootd-monitoring-collector/pull/43\">I added a cache to the DNS resolution</a> so that the collecotr only needs to resolve the host once every 24 hours.  When I profiled after making the change, I saw a significant improvement in DNS resolution time.  Below is another visualization of the profile after the DNS caching, purple is the DNS resolution.</p><figure class=\"\">  <img alt=\"this is a placeholder image\" src=\"https://derekweitzel.com/images/posts/profiling-xrootd-collector/after-optimization-profile.png\" /><figcaption>      Snakeviz profile.  Purple is the DNS resolution function    </figcaption></figure><p>Notice that the DNS resolution is a much smaller portion of the overall running time when compared to the previous profile.</p><p>In the following graph, I show the time spent on DNS resolution over time for both before and after the optimization.  I would expect DNS resolution to increase for both, but as you can see, the increase after adding DNS caching is much slower.</p><figure class=\"\">  <img alt=\"this is a placeholder image\" src=\"https://derekweitzel.com/images/posts/profiling-xrootd-collector/dns-resolution.png\" /><figcaption>      Growth of DNS resolution time    </figcaption></figure><h2 id=\"production\">Production</h2><p>When we applied the changes into production, we saw a significant improvement in the collector’s ability to process messages.  Below is the graph of the OSG’s Message Bus after the change:</p><figure class=\"\">  <img alt=\"this is a placeholder image\" src=\"https://derekweitzel.com/images/posts/profiling-xrootd-collector/edited-production-mq.png\" /><figcaption>      RabbitMQ Message Parsing    </figcaption></figure><p>The incoming messages decreased, but the collector is now able to process messages as fast as they are received.  This is a significant improvement over the previous state.  I suspect that the decrease in incoming messages is due to server load of sending more outgoing messages to the improved collector.  The message bus can slow down the incoming messages under heavier load.</p><h2 id=\"conclusions-and-future-work\">Conclusions and Future Work</h2><p>Since we implemented the cache for DNS resolution, the collector has been able to keep up with the incoming messages.  This is a significant improvement over the previous state.  Over time, we expect the DNS cache to capture nearly all of the hosts, and the DNS resolution time to decrease even further.</p><p>We continue to look for optimizations to the collector.  When looking at the output from the most recent profile, we noticed the collector is spending a significant amount of time in the logging functions.  By default, we have debug logging turned on.  We will look at turning off debug logging in the future.</p><p>Additionally, the collector is spending a lot of time polling for messages.  In fact, the message bus is receiving ~1500 messages a second, which is increasing the load on the message bus.  After reading through optimizations for RabbitMQ, it appears that less but larger messages are better for the message bus.  We will look at batching messages in the future.</p>",
            "url": "https://hpc.social/personal-blog/2024/profiling-the-xrootd-monitoring-collector/",
            
            
            
            
            
            "date_published": "2024-01-31T05:00:00-07:00",
            "date_modified": "2024-01-31T05:00:00-07:00",
            
                "author": "Derek Weitzel's Blog"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2023/sc-23-recap/",
            "title": "SC'23 Recap",
            "summary": null,
            "content_text": "The largest high-performance computing industry conference of the year, SC23, was held in Denver last week. This year's conference attracted over 14,000 attendees and 438 exhibitors, finally breaking pre-pandemic records, and it solidly felt like the old days of the conference in terms of breadth of attendees, the technical program, and overall engagement and interaction across the community.This was the second time I've attended the conference as a vendor instead of a customer, and this meant I spent a fair amount of time running to and from meetings instead of walking the show floor or attending technical sessions. I'm sure I missed some major announcements and themes as a result, but I thought it still might be valuable to contribute my observations based on this narrow lens of an AI-minded storage product manager for a major cloud service provider. If you're interested in a more well-rounded perspective, check out the HPC Social Supercomputing 2023 Summary and contribute your own thoughts!I don't know the best way to organize the notes that I took, so I grouped them into a few broad categories:Big news on the Top500What's new in storage for HPC and AIThe emergence of pure-play GPU cloudsOther technological dribs and drabsPersonal thoughts and reflections on the conference and communityI must also disclose that I am employed by Microsoft and I attended SC23 in that capacity. However, everything in this post is my own personal viewpoint, and my employer had no say in what I did or didn't write here. Everything below is written from my perspective as an enthusiast, not an employee, although my day job probably colors my outlook on the HPC industry.With all that being said, let's dive into the big news of the week!Big news on the Top500Unveiling the new Top500 list is the tentpole event of SC every year regardless of how much people (including myself!) deride HPL, and unlike the lists over the past year, this newest listing had two big surprises. Many of us went into the SC23 season wondering if the Aurora system, whose hardware was delivered this past June, would be far enough in installation and shakeout to unseat Frontier as the second listed exascale system. At the same time, nobody had expected another &gt;500 PF supercomputer to appear on the list, much less one operated privately and for-profit. But both systems made big debuts in the top 5, carrying with them interesting implications.The new #2: Argonne's AuroraThe Aurora exascale system has a storied history going back to 2015; first conceived of as a 180 PF supercomputer to be delivered in 2018, it evolved into a GPU-based exascale supercomputer that was supposed to land in 2021. Now two years late and a few executives short, Intel and Argonne were stuck between a rock and a hard place in choosing whether to list their HPL results at SC23:If Aurora wasn't listed on SC23's Top500 list, it risked going up against El Capitan at ISC'24 and being completely overshadowed by the simultaneous launch of a newer, bigger exascale system.If Aurora was listed at SC23's Top500 list but in an incomplete form, it would fall short of its long-awaited debut as the #1 system and would require a careful narrative to avoid being seen as a failed system.Intel and Argonne ultimately chose option #2 and listed an HPL run that used only 5,439 of Aurora's 10,624 nodes (51.1% of the total machine), and as expected, people generally understood that this sub-exaflop score was not an indictment of the whole system underdelivering, but more a reflection that the system was still not stable at its full scale. Still, headlines in trade press were dour, and there was general confusion about how to extrapolate Aurora's HPL submission to the full system.  Does the half-system listing of 585.34 PF Rmax at 24.7 MW power mean that the full system will require 50 MW to achieve an Rmax that's still lower than Frontier? Why is the efficiency (Rmax/Rpeak = 55%) so low?Interestingly, about half the people I talked to thought that Argonne should've waited until ISC'24 to list the full system, and the other half agreed that listing half of Aurora at SC'23 was the better option. Clearly there was no clearly right answer here, and I don't think anyone can fault Argonne for doing the best they could given the Top500 submission deadline and the state of the supercomputer. In talking to a couple folks from ALCF, I got the impression that there's still plenty of room to improve the score since their HPL run was performed under a time crunch, and there were known issues affecting performance that couldn't have been repaired in time. With any luck, Aurora will be ready to go at full scale for ISC'24 and have its moment in the sun in Hamburg.The new #3: Microsoft's EagleThe other new Top500 entry near the top of the list was Eagle, Microsoft's surprise 561 PF supercomputer. Like Aurora, it is composed of GPU-heavy nodes, and like Aurora, the HPL run utilized only part (1,800 nodes) of the full system. Unlike Aurora though, the full size of Eagle is not publicly disclosed by Microsoft, and its GPU-heavy node architecture was designed for one specific workload: training large language models for generative AI.At the Top500 BOF, Prabhat Ram gave a brief talk about Eagle where he emphasized that the system wasn't a custom-built, one-off stunt machine. Rather, it was built from publicly available ND H100 v5 virtual machines on a single 400G NDR InfiniBand fat tree fabric, and Microsoft had one of the physical ND H100 v5 nodes at its booth.  Here's the back side of it:From top to bottom, you can see it has eight E1.S NVMe drives, 4x OSFP ports which support 2x 400G NDR InfiniBand each, a Microsoft SmartNIC, and a ton of power.  A view from the top shows the HGX baseboard and fans:&lt;p&gt;Logically, this node (and the ND H100 v5 VM that runs on it) looks a lot like the NVIDIA DGX reference architecture. Physically, it is an air-cooled, Microsoft-designed OCP server, and Eagle’s Top500 run used 1,800 of these servers.&lt;/p&gt;Big HPL number aside, the appearance of Eagle towards the top of Top500 has powerful implications on the supercomputing industry at large.  Consider the following.Microsoft is a for-profit, public enterprise whose success is ultimately determined by how much money it makes for its shareholders. Unlike government agencies who have historically dominated the top of the list to show their supremacy in advancing science, the Eagle submission shows that there is now a huge financial incentive to build giant supercomputers to train large language models. This is a major milestone in supercomputing; up to this point, the largest systems built by private industry have come from the oil &amp; gas industry, and they have typically deployed at scales below the top 10.Eagle is also built on the latest and greatest technology--NVIDIA's H100 and NDR InfiniBand--rather than previous-generation technology that's already been proven out by the national labs.  SC23 was the first time Hopper GPUs have appeared anywhere on the Top500 list, and Eagle is likely the single largest installation of both H100 and NDR InfiniBand on the planet. Not only does this signal that it's financially viable to stand up a leadership supercomputer for profit-generating R&amp;D, but industry is now willing to take on the high risk of deploying systems using untested technology if it can give them a first-mover advantage.Eagle also shows us that the potential upside of bringing a massive new AI model to market is worth both the buying all the infrastructure required to build a half-exaflop system and hiring the talent required to shake out what is literally a world-class supercomputer. And while the US government can always obtain a DPAS rating to ensure it gets dibs on GPUs before AI companies can, there is no DPAS rating for hiring skilled individuals to stand up gigantic systems. This all makes me wonder: if Aurora was a machine sitting in some cloud data center instead of Argonne, and its commissioning was blocking the development of the next GPT model, would it have been able to take the #1 spot from Frontier this year?The appearance of such a gigantic system on Top500, motivated by and paid for as part of the AI land grab, also raises some existential questions for the US government. What role should the government have in the supercomputing industry if private industry now has a strong financial driver to invest in the development of leadership supercomputing technologies? Historically, government has always incubated cutting-edge HPC technologies so that they could stabilize enough to be palatable to commercial buyers. Today's leadership supercomputers in the national labs have always wound up as tomorrow's midrange clusters that would be deployed for profit-generating activities like seismic imaging or computer-aided engineering. If the AI industry is now taking on that mantle of incubating and de-risking new HPC technologies, perhaps government now needs to focus on ensuring that the technologies developed and matured for AI can still be used to solve scientific problems.What's new in storage for HPC and AI?Since I spent much of my career working in HPC storage, and I now focus largely on AI, it should be no surprise that I heard a lot about the intersection of AI and storage.  AI remains high in the hype cycle, so it's natural that just about every storage vendor and discussion had some talk of AI forced into it regardless of it was really relevant or not. However, there were a few places where AI and storage topics intersect that I found noteworthy.The AI-storage echo chamber&lt;p&gt;I was asked a lot of questions about storage from journalists, VCs, and even trusted colleagues that followed a common theme: What storage technologies for AI excite me the most? What’s the future of storage for AI?&lt;/p&gt;I don't fault people for asking such a broad question because the HPC/AI storage industry is full of bombastic claims. For example, two prominent storage vendors emblazoned their booths with claims of what their products could do for AI:These photos illustrate the reality that, although there is general agreement that good storage is needed for GPUs and AI, what constitutes \"good storage\" is muddy and confusing. Assuming the above approach to marketing (10x faster! 20x faster!) is effective for someone out there, there appears to be a market opportunity in just capitalizing on this general confusion by (1) asserting what the I/O problem that's jamming up all AI workloads is, and (2) showing that your storage product does a great job at solving that specific problem.For example, the MLPerf Storage working group recently announced the first MLPerf Storage benchmark, and Huiho Zheng from Argonne (co-author of the underlying DLIO tool on which MLPerf Storage was built) described how the MLPerf Storage benchmark reproduces the I/O characteristics of model training at the Workshop on Software and Hardware Co-Design of Deep Learning Systems in Accelerators:When I saw this premise, I was scratching my head--my day job is to develop new storage products to meet the demands of large-scale AI model training and inferencing, and I have never had a customer come to me claiming that they need support for small and sparse I/O or random access. In fact, write-intensive checkpointing and fine-tuning, not read-intensive data loading, is the biggest challenge faced by those training large language models in my experience. It wasn't until a few slides later did I realize where these requirements may be coming from:Storage and accelerator vendors are both defining and solving the I/O problems of the AI community which seems counterproductive--shouldn't a benchmark be set by the practitioners and not the solution providers?What I learned from talking to attendees, visiting storage vendor booths, and viewing talks like Dr. Zheng's underscores a reality that I've faced on my own work with production AI workloads: AI doesn't actually have an I/O performance problem, so storage vendors are struggling to define ways in which they're relevant in the AI market.I outlined the ways in which LLM training uses storage in my HDF5 BOF talk, and their needs are easy to meet with some local storage and basic programming. So easy, in fact, that a reasonably sophisticated AI practitioner can duct tape their way around I/O problems very quickly and move on to harder problems. There's no reason for them to buy into a sophisticated Rube Goldberg storage system, because it still won't fundamentally get them away from having to resort to local disk to achieve the scalability needed to train massive LLMs.So yes, I've got no doubt that there are storage products that can deliver 10x or 20x higher performance for some specific AI workload. And MLPerf Storage is probably an excellent way to measure that 20x performance boost. But the reality I've experienced is that a half a day of coding will deliver 19x higher performance when compared to the most naive approach, and every AI practitioner knows and does this already. That's why there are a lot of storage vendors fishing in this AI storage pond, but none of them seem to be reeling in any whoppers.This isn't to say that there's nothing interesting going on in high-performance storage though. If the most common question I was asked was \"what's the future of storage for AI,\" the second most common question was \"what do you think about VAST and WEKA?\"VAST &amp; WEKABoth companies seem to be doing something right since they were top of mind for a lot of conference attendees, and it probably grinds their respective gears that the field still groups them together in the same bucket of \"interesting parallel storage systems that we should try out.\" Rather than throw my own opinion in the pot though (I work with and value both companies and their technologies!), I'll note the general sentiments I observed.WEKA came into the week riding high on their big win as U2's official technology partner in September. Their big booth attraction was a popular Guitar Hero game and leaderboard, and an oversized Bono, presumably rocking out to how much he loves WEKA, presided over one of their seating areas:Much of their marketing centered around accelerating AI and other GPU workloads, and the feedback I heard from the WEKA customers I bumped into during the week backed this up. One person shared that the WEKA client does a great job with otherwise difficult small-file workloads, particularly common in life sciences workloads, and this anecdote is supported by the appearance of a very fast WEKA cluster owned by MSK Cancer Center on the IO500 Production list. People also remarked about WEKA's need for dedicated CPU cores and local storage to deliver the highest performance; this, combined with its client scalability, lends itself well to smaller clusters of fat GPU nodes. I didn't run into anyone using WEKA in the cloud though, so I assume the feedback I gathered had a bias towards more conventional, on-prem styles of architecting storage for traditional HPC.Whereas WEKA leaned into its rock 'n' roll theme this year, VAST doubled down on handing out the irresistibly tacky light-up cowboy hats they introduced last year (which I'm sure their neighbors at the DDN booth absolutely loved). They were all-in on promoting their new identity as a \"data platform\" this year, and although I didn't hear anyone refer to VAST as anything but a file system, I couldn't throw a rock without hitting someone who either recently bought a VAST system or tried one out.Unlike last year though, customer sentiment around VAST wasn't all sunshine and rainbows, and I ran into a few customers who described their presales engagements as more formulaic than the white-glove treatment everyone seemed to be getting a year ago. This isn't surprising; there's no way to give all customers the same royal treatment as a business scales. But it does mean that the honeymoon period between VAST and the HPC industry is probably at an end, and they will have to spend the time between now and SC24 focusing on consistent execution to maintain the momentum they've gotten from the light-up cowboy hats.The good news for VAST is that they've landed some major deals this past year, and they came to SC with customers and partners in-hand. They co-hosted a standing-room-only party with CoreWeave early in the week and shared a stage with Lambda at a customer breakfast, but they also highlighted two traditional, on-prem HPC customers (TACC and NREL) at the latter event.VAST clearly isn't letting go of the on-prem HPC market as it also pursues partnerships with emerging GPU cloud service providers; this contrasted with WEKA's apparent focus on AI, GPUs, and the cloud. Time will tell which strategy (if either, or both) proves to be the better approach.DAOSThough commercial buyers were definitely most interested in VAST and WEKA, folks from the more sophisticated HPC shops around the world also tossed a few questions about DAOS my way this year.I usually make it a point to attend the annual DAOS User Group meeting since it is always attended by all the top minds in high-performance I/O research, but I had to miss it this year on account of it running at the same time as my I/O tutorial. Fortunately, DAOS was pervasive throughout the conference, and there was no shortage of opportunity to find out what the latest news in the DAOS was. For example, check out the lineup for PDSW 2023 this year:Three out of thirteen talks were about DAOS which is more than any other single storage product or project. DAOS also won big at this year's IO500, taking the top two spots in the production storage system list:&lt;div class=\"separator\" style=\"clear: both; text-align: center;\"&gt;&lt;/div&gt;In fact, DAOS underpinned every single new awardee this year, and DAOS is now the second most represented storage system on the list behind Lustre:Why is DAOS at the top of so many people's minds this year? Well, DAOS reached a few major milestones in the past few months which has thrust it into the public eye.  First, Aurora is finally online and running jobs, and while the compute system is only running at half its capability, the full DAOS system (all 220 petabytes of it, all of which is TLC NVMe) is up and running--a testament to the scalability of DAOS that many parallel storage systems--including VAST and WEKA--have not publicly demonstrated. Because DAOS is open-source software and Aurora is an open-science system, all of DAOS' at-scale warts are also on full display to the community in a way that no competitive storage system besides of Lustre is.Second, Google Cloud cast a bold vote of confidence in DAOS by launching Parallelstore, its high-performance parallel file service based on DAOS, in August. Whereas AWS and Azure have bet on Lustre to fill the high-performance file gap (via FSx Lustre and Azure Managed Lustre), GCP has planted a stake in the ground by betting that DAOS will be the better foundation for a high-performance file service for HPC and AI workloads.Parallelstore is still in private preview and details are scant, but GCP had DAOS and Parallelstore dignitaries at all the major storage sessions in the technical program to fill in the gaps. From what I gathered, Parallelstore is still in its early stages and is intended to be a fast scratch tier; it's using DRAM for metadata which means it relies on erasure coding across servers to avoid data loss on a single server reboot, and there's no way to recover data if the whole cluster goes down at once. This lack of durability makes it ineligible for the IO500 list right now, but the upcoming metadata-on-NVMe feature (which previews in upstream DAOS in 1H2024) will be the long-term solution to that limitation.Finally, the third major bit of DAOS news was about the formation of the DAOS Foundation. First announced earlier this month, this initiative lives under the umbrella of the Linux Foundation and is led by its five founding members:Argonne National Laboratory, who has a vested interest in seeing DAOS endure given its massive investment in it,Enakta Labs, a company spun out of Croit, a German storage services company that was contributing feature development to DAOS,Google Cloud, who has made a big bet on DAOS as the underpinnings for its Parallelstore service,HPE, who has a shared fate with the DAOS installation at Argonne and who has also been contributing feature development, andIntel, whose engineers largely developed DAOS as part of the Aurora program.I see this handoff of DAOS from Intel to this new foundation as a positive change that makes DAOS a more stable long-term bet; should Intel choose to divest itself of DAOS once its obligations to the Aurora program end, DAOS now can live on without the community having to fork it. The DAOS Foundation is somewhat analogous to OpenSFS (one of the nonprofits backing Lustre) in that it is a vendor-neutral organization around which the DAOS community can gather.But unlike OpenSFS, the DAOS Foundation will also assume the responsibility of releasing new versions of DAOS after Intel releases its final version (2.6) in March 2024. The DAOS Foundation will also steer feature prioritization, but seeing as how the DAOS Foundation doesn't fund developers directly, it's not clear that contributors like Intel or GCP are actually at the mercy of the foundation's decisions. It's more likely that the DAOS Foundation will just have authority to decide what features will roll up into the next formal DAOS release, and developers contributing code to DAOS will still prioritize whatever features their employers tell them to.So, DAOS was the talk of the town at SC23. Does this all mean that DAOS is ready for prime time?While Intel and Argonne may say yes, the community seems to have mixed feelings.  Consider this slide presented by László Szűcs from LRZ at the DAOS Storage Community BOF:DAOS is clearly crazy fast and scales to hundreds of petabytes in production--Aurora's IO500 listing proves that. However, that performance comes with a lot of complexity that is currently being foisted on application developers, end-users, and system administrators. The \"opportunities\" listed in László's slide are choices that people running at leadership HPC scale may be comfortable making, but the average HPC user is not equipped to make many of these decisions and make thoughtful choices about container types and library interfaces.The fact that DAOS was featured so prominently at PDSW--a research workshop--probably underscores this as well. This slide presented by Adrian Jackson's lighting talk sums up the complexity along two different dimensions:His results showed that your choice of DAOS object class and I/O library atop the DAOS POSIX interface can result in wildly different checkpoint bandwidth. It's hard enough to teach HPC users about getting optimal performance out of a parallel file system like Lustre; I can't imagine those same users will embrace the idea that they should be mindful of which object class they use as they generate data.The other DAOS-related research talk, presented by Greg Eisenhauer, was a full-length paper that caught me by surprise and exposed how much performance varies when using different APIs into DAOS. This slide is one of many that highlighted this:I naively thought that the choice of native userspace API (key-value or array) would have negligible effects on performance, but Eisenhauer's talk showed that this isn't true. The reality appears to be that, although DAOS is capable of handling unaligned writes better than Lustre, aligning arrays on large, power-of-two boundaries still has a significant performance benefit.Based on these sorts of technical talks about DAOS presented this year, the original question--is DAOS ready for prime time--can't be answered with a simple yes or no yet.  The performance it offers is truly best in class, but achieving that performance doesn't come easy right now. Teams who are already putting heroic effort into solving a high-value problems will probably leap at the opportunity to realize the I/O performance that DAOS can deliver. Such high value problems include things like training the next generation of foundational LLMs, and GCP's bet on DAOS probably adds differentiable value to their platform as a place to train such models as efficiently as possible. But the complexity of DAOS at present probably limits its appeal to the highest echelons of leadership HPC and AI, and I think it'll be a while before DAOS is in a place where a typical summer intern will be able to appreciate its full value.InfiniaIt would be unfair of me to give all this regard to WEKA, VAST, and DAOS without also mentioning DDN's brand new Infinia product, launched right before SC23. Those in the HPC storage industry have been awaiting its launch for years now, but despite the anticipation, it really didn't come up in any conversations in which I was involved. I did learn that the engineering team developing Infinia inside DDN is completely separate from the Whamcloud team who is developing Lustre, but this could be a double-edged sword. On the good side, it means that open-source Lustre development effort isn't competing with DDN's proprietary product in engineering priorities on a day-to-day basis. On the bad side though, I still struggle to see how Infinia and Lustre can avoid eventually competing for the same business.For the time being, Infinia does seem to prioritize more enterprisey features like multitenancy and hands-free operation while Lustre is squarely aimed at delivering maximum performance to a broadening range of workloads. Their paths may eventually cross, but that day is probably a long way off, and Lustre has the benefit of being deeply entrenched across the HPC industry.The emergence of pure-play GPU cloudsIn addition to chatting with people about what's new in storage, I also went into SC23 wanting to understand how other cloud service providers are structuring end-to-end solutions for large-scale AI workloads. What I didn't anticipate was how many smaller cloud service providers (CSPs) showed up to SC for the first time this year, all waving the banner of offering NVIDIA H100 GPUs. These are predominantly companies that either didn't exist a few years ago or have historically focused on commodity cloud services like virtual private servers and managed WordPress sites, so it was jarring to suddenly see them at an HPC conference. How did so many of these smaller CSPs suddenly become experts in deploying GPU-based supercomputers in the time between SC22 and SC23? I got to talking to a few folks at these smaller CSPs to figure out exactly what they were offering to customers, and their approach is quite different from how AWS, Azure, and GCP operate. Rather than defining a standard cluster architecture and deploying copies of it all over to be consumed by whoever is willing to pay, these smaller CSPs deploy clusters of whitebox GPU nodes to customer specification and sell them as dedicated resources for fixed terms. If a customer wants a bunch of HGX H100s interconnected with InfiniBand, that's what they get. If they want RoCE, the CSP will deploy that instead. And the same is true with storage: if a customer wants EXAScaler or Weka, they'll deploy that too.While this is much closer to a traditional on-prem cluster deployment than a typical elastic, pay-as-you-go infrastructure-as-a-service offering, this is different from being a fancy colo. The end customer still consumes those GPUs as a cloud resource and never has to worry about the infrastructure that has to be deployed behind the curtain, and when the customer's contract term is up, their cluster is still owned by the CSP. As a result, the CSP can either resell that same infrastructure via pay-as-you-go or repurpose it for another dedicated customer. By owning the GPUs and selling them as a service, these CSPs can also do weird stuff like take out giant loans to build more data centers using GPUs as collateral. Meanwhile, NVIDIA can sell GPUs wholesale to these CSPs, book the revenue en masse, and let the CSPs deal with making sure they're maintained in production and well utilized.It also seems like the services that customers of these smaller CSPs get is often more barebones than what they'd get from a Big 3 CSP (AWS, Azure, and GCP). They get big GPU nodes and an RDMA fabric, but managed services beyond that are hit and miss.For example, one of these smaller CSPs told me that most of their storage is built on hundreds of petabytes of open-source Ceph. Ceph fulfills the minimum required storage services that any cloud must provide (object, block, and file), but it's generally insufficient for large-scale model training. As a result, all the smaller CSPs with whom I spoke said they are also actively exploring VAST and Weka as options for their growing GPU-based workloads. Since both VAST and Weka offer solid S3 and file interfaces, either could conceivably act as the underpinnings of these GPU clouds' first-party storage services as well.As I said above though, it seems like the predominant model is for these CSPs to just ship whatever dedicated parallel storage the customer wants if something like Ceph isn't good enough. This, and the growing interest in storage from companies like VAST and Weka, suggest a few things:Some of these CSPs have been obtaining and deploying GPUs faster than they've had time to think about the end-to-end experience, and customers have so much pent-up demand for GPUs that they're willing to either work with whatever third-party storage vendor is brought to the table or take on the responsibility of choosing their preferred storage vendor themselves.Having giant piles of GPUs is necessary, but not sufficient, to have a competitive offering in the GPU cloud services landscape. A credible platform for AI training must also have an integrated high-performance storage service.It is looking like many pure-play GPU clouds are finding it more cost-effective to buy their way out of high-performance storage problems through partnerships than build and manage their own services atop open-source software like Lustre or DAOS.None of these observations are terribly surprising; at the price these smaller CSPs are offering GPUs compared to the Big 3 CSPs, their gross margin (and therefore their ability to invest in developing services on top of their IaaS offerings) has got to be pretty low. In the short term, it's cheaper and easier to deploy one-off high-performance storage systems alongside dedicated GPU clusters based on customer demand than develop and support a standard solution across all customers.Of course, building a low-cost GPU service opens the doors for other companies to develop their own AI services on top of inexpensive GPU IaaS that is cost-competitive with the Big 3's native AI platforms (AWS SageMaker, Azure Machine Learning, and Google AI Platform). For example, I chatted with some folks at together.ai, a startup whose booth caught my eye with its bold claim of being \"the fastest cloud for [generative] AI:\"Contrary to their banner, they aren't a cloud; rather, they provide AI services--think inferencing and fine-tuning--that are accessible through an API much like OpenAI's API. They've engineered their backend stack to be rapidly deployable on any cloud that provides basic IaaS like GPU-equipped VMs, and this allows them to actually run their computational backend on whatever cloud can offer the lowest-cost, no-frills GPU VMs. In a sense, companies like together.ai develop and sell the frills that these new GPU CSPs lack, establishing a symbiotic alternative to the vertically integrated AI platforms on bigger clouds.I did ask a few of these smaller CSPs what their overall pitch was. Why I would choose GPU cloud X over their direct competitor GPU cloud Y? The answers went in two directions:They offer lower cost per GPU hour than their competitionThey are faster to get GPUs off a truck and into production than their competitionThere's a big caveat here: I didn't talk to many representatives at these CSPs, so my sample size was small and not authoritative. However, taking these value propositions at face value struck me as being quite precarious since their value is really a byproduct of severe GPU shortages driven by the hyped-up AI industry. What happens to these CSPs (and the symbionts whose businesses depend on them) when AMD GPUs appear on the market in volume? What happens if NVIDIA changes course and, instead of peanut-buttering its GPUs across CSPs of all sizes, it focuses its attention on prioritizing deliveries to just a few blessed CSPs?There is no moat around generative AI, and I left SC23 feeling like there's a dearth of long-term value being generated by some of these smaller GPU CSPs. For those CSPs whose primary focus is buying and deploying as many GPUs in as short a time as possible, not everyone can survive. They'll either come out of this GPU shortage having lost a lot of money building data centers that will go unused, or they'll be sold for parts.More importantly to me though, I learned that I should give less credence to the splashy press events of hot AI-adjacent startups if their successes lie exclusively with smaller GPU CSPs. Some of these CSPs are paying to make their problems go away in an effort to keep their focus on racking and stacking GPUs in the short term, and I worry that there's a lack of long-term vision and strong opinions in some of these companies. Some of these smaller CSPs seem much more like coin-operated GPU cluster vending machines than platform providers, and that business model doesn't lend itself to making big bets and changing the industry.Put another way, my job--both previous and current--has always been to think beyond short-term band aids and make sure that my employer has a clear and opinionated view of the technical approach that will be needed to address the challenges of HPC ten years in the future. I know who my peers are at the other Big 3 CSPs and leadership computing facilities across the world, and I know they're thinking hard about the same problems that I am. What worries me is that I do not know who my peers are at these smaller CSPs, and given their speed of growth and smaller margins, I worry that they aren't as prepared for the future as they will need to be. The AI industry as a whole will be better off when GPUs are no longer in such short supply, but the ecosystem surrounding some of these smaller GPU CSPs is going to take some damage when that day comes.Other dribs and drabsI also had a lot of interesting conversations and noticed a few subtle themes last week that don't neatly fit into any other category, but I'd love to hear more from others if they noticed the same or have more informed opinions.APUs and superchips - are they really that useful?Because I spent my booth duty standing next to one of Eagle's 8-way HGX H100 nodes, a lot of people asked me if I thought the Grace Hopper superchip would be interesting. I'm not an expert in either GPUs or AI, but I did catch up with a few colleagues who are smarter than me in this space last week, and here's the story as I understand it:The Grace Hopper superchip (let's just call it GH100) is an evolution of the architecture developed for Summit, where V100 GPUs were cache-coherent with the CPUs through a special widget that converted NVLink to the on-chip coherence protocol for Power9. With GH100, the protocol used to maintain coherence across the CPU is directly compatible with the ARM AMBA coherence protocol, eliminating one bump in the path that Power9+V100 had. Grace also has a much more capable memory subsystem and NOC that makes accessing host memory from the GPU more beneficial.Now, do AI workloads really need 72 cores per H100 GPU? Probably not.What AI (and HPC) will need are some high-performance cores to handle all the parts of application execution that GPUs are bad at--divergent code paths, pointer chasing, and I/O. Putting capable CPU cores (Neoverse V2, not the N2 used in CPUs like new Microsoft's Cobalt 100) on a capable NOC that is connected to the GPU memory subsystem at 900 GB/s opens doors for using hierarchical memory to train LLMs in clever ways.For example, naively training an LLM whose weights and activations are evenly scattered across both host memory and GPU memory won't go well since that 900 GB/s of NVLink C2C would be on the critical path of many computations. However, techniques like activation checkpointing could become a lot more versatile when the cost of offloading certain tensors from GPU memory is so much lower. In essence, the presence of easily accessible host memory will likely allow GPU memory to be used more efficiently since the time required to transfer tensors into and out of HBM is easier to hide underneath other computational steps during training.Pairing an over-specified Grace CPU with a Hopper GPU also allows the rate of GPU development to proceed independently of CPU development. Even if workloads that saturate an H100 GPU might not also need all 72 cores of the Grace CPU, H200 or other future-generation GPUs can grow into the capabilities of Grace without having to rev the entire superchip.I didn't get a chance to talk to any of my colleagues at AMD to get their perspective on the MI300 APU, but I'd imagine their story is a bit simpler since their memory space is flatter than NVIDIA's superchip design. This will make training some models undoubtedly more straightforward but perhaps leave less room for sophisticated optimizations that can otherwise cram more of a model into a given capacity of HBM. I'm no expert though, and I'd be happy to reference any explanations that real experts can offer! What about quantum?Quantum computing has been a hot topic for many years of SC now, but it feels like a topic that is finally making its way out of pure CS research and into the minds of the everyday HPC facility leaders. I talked to several people last week who asked me for my opinion on quantum computing because they have come to the realization that they need to know more about it than they do, and I have to confess, I'm in the same boat as they are. I don't follow quantum computing advancements very closely, but I know an increasing number of people who do--and they're the sort who work in CTOs' offices and have to worry about risks and opportunities more than intellectual curiosities.It's hard to say there've been any seismic shifts in the state of the art in quantum computing at SC23; as best I can tell, there's still a rich ecosystem of venture capital-backed startups who keep cranking out more qubits. But this year felt like the first year where HPC facilities who haven't yet started thinking about their position on quantum computing are now behind. Not everyone needs a quantum computer, and not everyone even needs a quantum computing researcher on staff. But everyone should be prepared with a strong point of view if they are asked \"what will you be doing with quantum computing?\" by a funding agency or chief executive.NextSiliconOne of the least-stealthy stealth-mode startups in the HPC industry has been NextSilicon, a company who debuted from stealth mode at SC23, launched their new Maverick accelerator, and announced their first big win with Sandia National Lab's Vanguard II project. What's notable about NextSilicon is that, unlike just about every other accelerator startup out there, they are not trying to go head-to-head with NVIDIA in the AI acceleration market. Rather, they've created a dataflow accelerator that aims to accelerate challenging HPC workloads that GPUs are particularly bad at--things like irregular algorithms and sparse data structures. They've paired this hardware with a magical runtime that continually optimizes the way the computational kernel is mapped to the accelerator's reconfigurable units to progressively improve the throughput of the accelerator as the application is running.The concept of dataflow accelerators has always been intriguing since they're the only alternative to improving computational throughput besides making larger and larger vectors. The challenge has always been that these accelerators are more like FPGAs than general-purpose processors, and they require similar amounts of hardcore CS expertise to use well. NextSilicon claims to have cracked that nut with their runtime, and it seems like they're hiring the rights sorts of people--real HPC with respectable pedigrees--to make sure their accelerator can really deliver value to HPC workloads.I/O benchmarking developmentsAt the IO500 BOF, there was rich discussion about adding new benchmarking modes to IOR and IO500 to represent a wider range of patterns.More specifically, there's been an ongoing conversation about including a 4K random read test, and it sounds like the most outspoken critics against it have finally softened their stance. I've not been shy about why I think using IOPS as a measure of file system performance is dumb, but 4K random IOPS do establish a lower bound of performance for what a real application might experience. Seeing as how IO500 has always been problematic as any representation of how a file system will perform in real-world environments, adding the option to run a completely synthetic, worst-case workload will give IO500 the ability to define a complete bounding box around the lower and upper limits of I/O performance for a file system.Hendrik Nolte from GWDG also proposed a few new and appealing IOR modes that approach more realistic workload scenarios.  The first was a new locally random mode where data is randomized within IOR segments but segments are repeated:Compared to globally randomized reads (which is what IOR normally does), this is much closer representation of parallel workloads that are not bulk-synchronous; for example, NCBI BLAST uses thread pools and work sharing to walk through files, and the resulting I/O pattern is similar to this new mode.He also described a proposal to run concurrent, mixed workloads in a fashion similar to how fio currently works.  Instead of performing a bulk-synchronous parallel write followed by a bulk-synchronous parallel read, his proposal would allow IOR to perform reads and writes concurrently, more accurately reflecting the state of multitenant storage systems. I actually wrote a framework to do exactly this and quantify the effects of contention using IOR and elbencho, but I left the world of research before I could get it published. I'm glad to see others seeing value in pursuing this idea.The other noteworthy development in I/O benchmarking was presented by Sven Breuner at the Analyzing Parallel I/O BOF where he described a new netbench mode for his excellent elbencho benchmark tool. This netbench mode behaves similarly to iperf in that it is a network-level throughput test, but because it is part of elbencho, it can generate the high-bandwidth incasts and broadcasts that are typically encountered between clients and servers of parallel storage systems:This is an amazing development because it makes elbencho a one-stop shop for debugging the entire data path of a parallel storage system. For example, if you're trying to figure out why the end-to-end performance of a file system is below expectation, you can use elbencho to test the network layer, the object or file layer, the block layer, and the overall end-to-end path separately to find out which layer is underperforming. Some file systems have specialized included tools to perform the same network tests (e.g., nsdperf for IBM Spectrum Scale), but elbencho now has a nice generic way to generate these network patterns for any parallel storage system.Some personal thoughtsAs with last year, I couldn't attend most of the technical program due to a packed schedule of customer briefings and partner meetings, but the SC23 Digital Experience was excellently done, and I wound up watching a lot of the content I missed during the mornings and after the conference (at 2x speed!). In that sense, the hybrid nature of the conference is making it easier to attend as someone who has to juggle business interests with technical interests; while I can't jump into public arguments about the definition of storage \"QOS\", I can still tell that my old friends and colleagues are still fighting the good fight and challenging conventional thinking across the technical program.My Parallel I/O in Practice tutorialThis was the sixth year that I co-presented the Parallel I/O in Practice tutorial with my colleagues Rob Latham, Rob Ross, and Brent Welch. A conference photographer got this great photo of me in the act:Presenting this tutorial is always an incredibly gratifying experience; I've found that sharing what I know is one of the most fulfilling ways I can spend my time, and being able to start my week in such an energizing way is what sustains the sleep deprivation that always follows. Giving the tutorial is also an interesting window into what the next generation of I/O experts is worrying about; for example, we got a lot of questions and engagement around the low-level hardware content in our morning half, and the I/O benchmarking material in the late afternoon seemed particularly well received. The majority of attendees came from the systems side rather than the user/dev side as well, perhaps suggesting that the growth in demand for parallel storage systems (and experts to run them) is outstripping the demand for new ways to perform parallel I/O. Guessing wildly, perhaps this means new developers are coming into the field higher up the stack, using frameworks like fsspec that abstract away low-level I/O.Since I've jumped over to working in industry, it's been hard to find the business justification to keep putting work hours into the tutorial despite how much I enjoy it.  I have to confess that I didn't have time to update any of the slides I presented this year even though the world of parallel I/O has not remained the same, and I am going to have to figure out how to better balance these sorts of community contributions with the demands of a day job in the coming years.An aside on COVID safetyAt SC22, I fastidiously wore a KN95 mask while indoors and avoided all after-hours events and indoor dining to minimize my risk of catching COVID. At that time, neither my wife nor I had ever gotten COVID before, and I had no desire to bring it home to my family since my father died of COVID-related respiratory failure two years prior. Staying fully masked at SC22 turned out to be a great decision at the time since a significant number of other attendees, including many I spoke with, contracted COVID at SC22. By comparison, I maintained my COVID-free streak through 2022.This year I took a more risk-tolerant approach for two reasons:My wife and I both broke our streaks this past summer and contracted COVID while on vacation, so if I got sick, we knew what to expect, andI got my gazillionth COVID and flu shots in October in anticipation of attending SC.Part of my approach to managing risk was bringing my trusty Aranet4 CO2 sensor with me so that I could be aware of areas where there was air circulation and the risk of contracting an airborne illness would be higher. I only wore a KN95 at the airport gates and while on the airplane at SC23, and despite going in all-in on after-hours events, indoor dining, and copious meetings and tours of booth duty, I'm happy to report that I made it through the conference without getting sick.I have no doubt that being vaccinated helped, as I've had several people tell me they tested positive for COVID after we had dinner together in Denver. But it's also notable that the Denver Convention Center had much better ventilation than Kay Bailey Hutchison Convention Center in Dallas where SC22 was held last year. To show this quantitatively, let's compare air quality measurements from SC22 to SC23.My schedule for the day on which I give my tutorial is always the same: the tutorial runs from 8:30am to 5:00pm with breaks at 10:00, 12:00, and 3:00. Because of this consistent schedule, comparing the CO2 readings (which are a proxy for re-breathed air) for my tutorial day at SC22 versus SC23 shows how different the air quality was in the two conference centers. Here's what that comparison looks like:What the plot shows is that CO2 (re-breathed air) steadily increased at the start of the tutorial at both SC22 and SC23, but Denver's convention center kicked on fresh air ventilation after an hour while Dallas simply didn't. Air quality remained poor (over 1,000) throughout the day in Dallas, whereas Denver was pretty fresh (below 700) even during the breaks and the indoor luncheon. This relatively good air circulation inside the convention center at SC23 made me much more comfortable about going maskless throughout the week.This isn't to say that I felt there was no risk of getting sick this year; there was at least one busy, upscale restaurant/bar in which I dined where the air circulation was no better than in a car or airplane. For folks who just don't want to risk being sick over Thanksgiving, wearing a mask and avoiding crowded bars was probably still the best option this year. And fortunately, Denver's weather was gorgeous, so outdoor dining was completely viable during the week.AI's effects on the HPC communityAlthough AI has played a prominent role in previous SC conferences, this was the first year where I noticed that the AI industry is bleeding into the HPC community in weird ways.For example, I had a bunch of journalists and media types accost me and start asking rather pointed questions while I was on booth duty. Talking to journalists isn't entirely unusual since I've always been supportive of industry press, but the social contract between practitioners like me and journalists has always been pretty formal--scheduling a call in advance, being invited to speak at an event, and things like that have long been the norm. If I was being interviewed on the record, I knew it.This year though, it seemed like there was a new generation of younger journalists who approached me no differently than a casual booth visitor. Some did introduce themselves as members of the press after we got chatting (good), but others did not (not good) which led me to take away a learning: check names and affiliations before chatting with strangers, because the days where I could assume that all booth visitors would act in good faith are gone.Now, why the sudden change?  I can think of three possible reasons:I'm getting older, and there are now tech industry journalists who are younger than me and think I am worth talking to since I've always been around. Maybe the old-school HPC folks that predate me have always had to deal with this.The proliferation of platforms like Substack make it financially viable to be an independent journalist, and conversely, anyone can be a journalist without editorial oversight.The spotlight on the massive AI industry is also illuminating the HPC industry. HPC and AI are both built on the same foundational technologies (GPUs, RDMA fabrics, HBM, and the like) so AI journalists now have a reason to start showing up at HPC community events.It'd be fair to argue that #3 is a stretch and that this isn't an AI phenomenon if not for the fact that I was also accosted by a few venture capitalists for the first time this year. HPC has never been an industry that attracted the attention of venture capital in the way that AI does, so I have to assume being asked specific questions about the viability of some startup's technology is a direct result of the AI market opportunity.While it's nice to have a broader community of attendees and more media coverage, the increasing presence of AI-focused media and VC types in the SC community means I can't be as open and honest as I once was. Working for a corporation (with secrets of its own to protect) doesn't help there either, so maybe getting cagier when talking to strangers is just a part of growing up.SC23 as a milestone yearAttending SC23 this year coincided with two personal milestones for me as well.This is the tenth year I've been in the HPC business, and the first SC I ever attended was SC13.  I can't say that this is my eleventh SC because I didn't attend in 2014 (on account of working at a biotech startup), but I've been to SC13, SC15 through SC19, SC20 and SC21 virtually, and SC22 and SC23 in-person.  At SC13 ten years ago, the weather was a lot colder:But I still have the fondest memories of that conference because it that was the week where I felt like I had finally found my community after having spent a decade as an unhappy materials science student.SC23 is also a milestone year because it may be the last SC I attend as a storage and I/O guy. I recently signed on for a new position within Microsoft to help architect the next generation of supercomputers for AI, and I'll probably have to trade in the time I used to spend at workshops like PDSW for opportunities to follow the latest advancements in large-scale model training, RDMA fabrics, and accelerators. But I think I am OK with that.I never intended to become an I/O or storage expert when I first showed up at SC13; it wasn't until I joined NERSC that I found that I could learn and contribute the most by focusing on storage problems. The world has changed since then, and now that I'm at Microsoft, it seems like the problems faced at the cutting edge of large language models, generative AI, and the pursuit of AGI are where the greatest need lies. As I said earlier in this post, AI has bigger problems to deal with than storage and I/O, and those bigger problems are what I'll be chasing. With any luck, I'll be able to say I had a hand in designing the supercomputers that Microsoft builds after Eagle. And as has been true for my last ten years in this business, I'll keep sharing whatever I learn with whoever wants to know.",
            "content_html": "<p>The largest high-performance computing industry conference of the year, SC23, was held in Denver last week. This year's conference <a href=\"https://twitter.com/thedeadline/status/1724949606188847381?s=61&amp;t=5LlVTsVajaU1kTzzuGQL7Q\">attracted over 14,000 attendees</a> and <a href=\"https://hallerickson.ungerboeck.com/prod/app85.cshtml?aat=Z8CDp%2bb4HWU7dw3dA3PesG2LIb9lCzjs2VEXLZZxGP4%3d\">438 exhibitors</a>, finally breaking pre-pandemic records, and it solidly felt like the old days of the conference in terms of breadth of attendees, the technical program, and overall engagement and interaction across the community.</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>This was the second time I've attended the conference as a vendor instead of a customer, and this meant I spent a fair amount of time running to and from meetings instead of walking the show floor or attending technical sessions. I'm sure I missed some major announcements and themes as a result, but I thought it still might be valuable to contribute my observations based on this narrow lens of an AI-minded storage product manager for a major cloud service provider. If you're interested in a more well-rounded perspective, check out the <a href=\"https://hpc.social/news/2023/supercomputing-23-summary/\">HPC Social Supercomputing 2023 Summary</a> and contribute your own thoughts!</p><p><span></span></p><p></p><p>I don't know the best way to organize the notes that I took, so I grouped them into a few broad categories:</p><p></p><ol><li>Big news on the Top500</li><li>What's new in storage for HPC and AI</li><li>The emergence of pure-play GPU clouds</li><li>Other technological dribs and drabs</li><li>Personal thoughts and reflections on the conference and community</li></ol><p>I must also disclose that I am employed by Microsoft and I attended SC23 in that capacity. However, everything in this post is my own personal viewpoint, and my employer had no say in what I did or didn't write here. Everything below is written from my perspective as an enthusiast, not an employee, although my day job probably colors my outlook on the HPC industry.</p><p>With all that being said, let's dive into the big news of the week!</p><p></p><h2>Big news on the Top500</h2><div>Unveiling the new Top500 list is the tentpole event of SC every year regardless of how much people (including myself!) deride HPL, and unlike the lists over the past year, this newest listing had two big surprises. Many of us went into the SC23 season wondering if the Aurora system, whose <a href=\"https://www.intc.com/news-events/press-releases/detail/1631/aurora-supercomputer-blade-installation-complete\">hardware was delivered this past June</a>, would be far enough in installation and shakeout to unseat Frontier as the second listed exascale system. At the same time, nobody had expected another &gt;500 PF supercomputer to appear on the list, much less one operated privately and for-profit. But both systems made big debuts in the top 5, carrying with them interesting implications.</div><h3>The new #2: Argonne's Aurora</h3><p>The Aurora exascale system has a storied history going back to 2015; first conceived of as a 180 PF supercomputer to be delivered in 2018, it evolved into a GPU-based exascale supercomputer that was supposed to land in 2021. Now two years late and a few executives short, Intel and Argonne were stuck between a rock and a hard place in choosing whether to list their HPL results at SC23:</p><p></p><ol><li>If Aurora <u>wasn't</u> listed on SC23's Top500 list, it risked <a href=\"https://www.llnl.gov/article/46161/llnl-hpe-partner-amd-el-capitan-projected-worlds-fastest-supercomputer\">going up against El Capitan</a> at ISC'24 and being completely overshadowed by the simultaneous launch of a newer, bigger exascale system.</li><li>If Aurora <u>was</u> listed at SC23's Top500 list but in an incomplete form, it would fall short of its long-awaited debut as the #1 system and would require a careful narrative to avoid being seen as a failed system.</li></ol><p>Intel and Argonne ultimately chose option #2 and listed an HPL run that used only 5,439 of Aurora's 10,624 nodes (51.1% of the total machine), and as expected, people generally understood that this sub-exaflop score was not an indictment of the whole system underdelivering, but more a reflection that the system was still not stable at its full scale. Still, <a href=\"https://www.theregister.com/2023/11/13/aurora_top500_no2/\">headlines in trade press were dour</a>, and there was general confusion about how to extrapolate Aurora's HPL submission to the full system.  Does the half-system listing of 585.34 PF Rmax at 24.7 MW power mean that the full system will require 50 MW to achieve an Rmax that's still lower than Frontier? Why is the efficiency (Rmax/Rpeak = 55%) so low?</p><p>Interestingly, about half the people I talked to thought that Argonne should've waited until ISC'24 to list the full system, and the other half agreed that listing half of Aurora at SC'23 was the better option. Clearly there was no clearly right answer here, and I don't think anyone can fault Argonne for doing the best they could given the Top500 submission deadline and the state of the supercomputer. In talking to a couple folks from ALCF, I got the impression that there's still plenty of room to improve the score since their HPL run was performed under a time crunch, and there were known issues affecting performance that couldn't have been repaired in time. With any luck, Aurora will be ready to go at full scale for ISC'24 and have its moment in the sun in Hamburg.</p><p></p><h3>The new #3: Microsoft's Eagle</h3><p>The other new Top500 entry near the top of the list was Eagle, Microsoft's surprise 561 PF supercomputer. Like Aurora, it is composed of GPU-heavy nodes, and like Aurora, the HPL run utilized only part (1,800 nodes) of the full system. Unlike Aurora though, the full size of Eagle is not publicly disclosed by Microsoft, and its GPU-heavy node architecture was designed for one specific workload: training large language models for generative AI.</p><p>At the <a href=\"https://sc23.conference-program.com/presentation/?id=bof155&amp;sess=sess314\">Top500 BOF</a>, Prabhat Ram gave a brief talk about Eagle where he emphasized that the system wasn't a custom-built, one-off stunt machine. Rather, it was built from publicly available <a href=\"https://learn.microsoft.com/en-us/azure/virtual-machines/nd-h100-v5-series\">ND H100 v5 virtual machines</a> on a single 400G NDR InfiniBand fat tree fabric, and Microsoft had one of the physical ND H100 v5 nodes at its booth.  Here's the back side of it:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>From top to bottom, you can see it has eight E1.S NVMe drives, 4x OSFP ports which support 2x 400G NDR InfiniBand each, a Microsoft SmartNIC, and a ton of power.  A view from the top shows the HGX baseboard and fans:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p><br />&lt;p&gt;Logically, this node (and the ND H100 v5 VM that runs on it) looks a lot like the NVIDIA DGX reference architecture. Physically, it is an air-cooled, Microsoft-designed OCP server, and Eagle’s Top500 run used 1,800 of these servers.&lt;/p&gt;</p><p>Big HPL number aside, the appearance of Eagle towards the top of Top500 has powerful implications on the supercomputing industry at large.  Consider the following.</p><p>Microsoft is a for-profit, public enterprise whose success is ultimately determined by how much money it makes for its shareholders. Unlike government agencies who have historically dominated the top of the list to show their supremacy in advancing science, the Eagle submission shows that there is now a huge financial incentive to build giant supercomputers to train large language models. This is a major milestone in supercomputing; up to this point, the largest systems built by private industry have come from the oil &amp; gas industry, and they have typically deployed at scales below the top 10.</p><p>Eagle is also built on the latest and greatest technology--NVIDIA's H100 and NDR InfiniBand--rather than previous-generation technology that's already been proven out by the national labs.  SC23 was the first time Hopper GPUs have appeared anywhere on the Top500 list, and Eagle is likely the single largest installation of both H100 and NDR InfiniBand on the planet. Not only does this signal that it's financially viable to stand up a leadership supercomputer for profit-generating R&amp;D, but industry is now willing to take on the high risk of deploying systems using untested technology if it can give them a first-mover advantage.</p><p>Eagle also shows us that the potential upside of bringing a massive new AI model to market is worth both the buying all the infrastructure required to build a half-exaflop system <i>and</i> hiring the talent required to shake out what is literally a world-class supercomputer. And while the US government can always obtain a <a href=\"https://www.bis.doc.gov/index.php/other-areas/strategic-industries-and-economic-security-sies/defense-priorities-a-allocations-system-program-dpas\">DPAS rating</a> to ensure it gets dibs on GPUs before AI companies can, there is no DPAS rating for hiring skilled individuals to stand up gigantic systems. This all makes me wonder: if Aurora was a machine sitting in some cloud data center instead of Argonne, and its commissioning was blocking the development of the next GPT model, would it have been able to take the #1 spot from Frontier this year?</p><p>The appearance of such a gigantic system on Top500, motivated by and paid for as part of the AI land grab, also raises some existential questions for the US government. What role should the government have in the supercomputing industry if private industry now has a strong financial driver to invest in the development of leadership supercomputing technologies? Historically, government has always incubated cutting-edge HPC technologies so that they could stabilize enough to be palatable to commercial buyers. Today's leadership supercomputers in the national labs have always wound up as tomorrow's midrange clusters that would be deployed for profit-generating activities like seismic imaging or computer-aided engineering. If the AI industry is now taking on that mantle of incubating and de-risking new HPC technologies, perhaps government now needs to focus on ensuring that the technologies developed and matured for AI can still be used to solve scientific problems.</p><p></p><h2>What's new in storage for HPC and AI?</h2><div>Since I spent much of my career working in HPC storage, and I now focus largely on AI, it should be no surprise that I heard a lot about the intersection of AI and storage.  AI remains high in the hype cycle, so it's natural that just about every storage vendor and discussion had some talk of AI forced into it regardless of it was really relevant or not. However, there were a few places where AI and storage topics intersect that I found noteworthy.</div><h3>The AI-storage echo chamber</h3><p><span></span>&lt;p&gt;I was asked a lot of questions about storage from journalists, VCs, and even trusted colleagues that followed a common theme: What storage technologies for AI excite me the most? What’s the future of storage for AI?&lt;/p&gt;</p><p>I don't fault people for asking such a broad question because the HPC/AI storage industry is full of bombastic claims. For example, two prominent storage vendors emblazoned their booths with claims of what their products could do for AI:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>These photos illustrate the reality that, although there is general agreement that good storage is needed for GPUs and AI, what constitutes \"good storage\" is muddy and confusing. Assuming the above approach to marketing (10x faster! 20x faster!) is effective for someone out there, there appears to be a market opportunity in just capitalizing on this general confusion by (1) asserting what the I/O problem that's jamming up all AI workloads is, and (2) showing that your storage product does a great job at solving that specific problem.<br /></p><p>For example, the MLPerf Storage working group recently announced the first <a href=\"https://mlcommons.org/2023/06/introducing-the-mlperf-storage-benchmark-suite/\">MLPerf Storage benchmark</a>, and Huiho Zheng from Argonne (co-author of the underlying <a href=\"https://ieeexplore.ieee.org/document/9499416\">DLIO tool</a> on which MLPerf Storage was built) described how the MLPerf Storage benchmark reproduces the I/O characteristics of model training at the <a href=\"https://sc23.conference-program.com/presentation/?id=misc289&amp;sess=sess437\">Workshop on Software and Hardware Co-Design of Deep Learning Systems in Accelerators</a>:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>When I saw this premise, I was scratching my head--my day job is to develop new storage products to meet the demands of large-scale AI model training and inferencing, and I have never had a customer come to me claiming that they need support for small and sparse I/O or random access. In fact, write-intensive checkpointing and fine-tuning, not read-intensive data loading, is the biggest challenge faced by those training large language models in my experience. It wasn't until a few slides later did I realize where these requirements may be coming from:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>Storage and accelerator vendors are both defining and solving the I/O problems of the AI community which seems counterproductive--shouldn't a benchmark be set by the practitioners and not the solution providers?</p><p>What I learned from talking to attendees, visiting storage vendor booths, and viewing talks like Dr. Zheng's underscores a reality that I've faced on my own work with production AI workloads: <b>AI doesn't actually have an I/O performance problem, so storage vendors are struggling to define ways in which they're relevant in the AI market</b>.</p><p>I outlined the ways in which LLM training uses storage in <a href=\"https://sc23.conference-program.com/presentation/?id=bof212&amp;sess=sess354\">my HDF5 BOF talk</a>, and their needs are easy to meet with some local storage and basic programming. So easy, in fact, that a reasonably sophisticated AI practitioner can duct tape their way around I/O problems very quickly and move on to harder problems. There's no reason for them to buy into a sophisticated Rube Goldberg storage system, because it still won't fundamentally get them away from having to resort to local disk to achieve the scalability needed to train massive LLMs.</p><p>So yes, I've got no doubt that there are storage products that can deliver 10x or 20x higher performance for some specific AI workload. And MLPerf Storage is probably an excellent way to measure that 20x performance boost. But the reality I've experienced is that a half a day of coding will deliver 19x higher performance when compared to the most naive approach, and every AI practitioner knows and does this already. That's why there are a lot of storage vendors fishing in this AI storage pond, but none of them seem to be reeling in any whoppers.</p><p>This isn't to say that there's nothing interesting going on in high-performance storage though. If the most common question I was asked was \"what's the future of storage for AI,\" the second most common question was \"what do you think about VAST and WEKA?\"</p><h3>VAST &amp; WEKA</h3><p>Both companies seem to be doing something right since they were top of mind for a lot of conference attendees, and it probably grinds their respective gears that the field still groups them together in the same bucket of \"interesting parallel storage systems that we should try out.\" Rather than throw my own opinion in the pot though (I work with and value both companies and their technologies!), I'll note the general sentiments I observed.</p><p>WEKA came into the week riding high on their big win as <a href=\"https://www.weka.io/company/weka-newsroom/press-releases/weka-named-u2s-official-technology-partner-ahead-of-achtung-baby-shows/\">U2's official technology partner</a> in September. Their big booth attraction was a popular Guitar Hero game and leaderboard, and an oversized Bono, presumably rocking out to how much he loves WEKA, presided over one of their seating areas:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>Much of their marketing centered around accelerating AI and other GPU workloads, and the feedback I heard from the WEKA customers I bumped into during the week backed this up. One person shared that the WEKA client does a great job with otherwise difficult small-file workloads, particularly common in life sciences workloads, and this anecdote is supported by the appearance of a <a href=\"https://www.weka.io/blog/hpc/hot-take-real-customers-real-benchmarks/\">very fast WEKA cluster owned by MSK Cancer Center on the IO500 Production list</a>. People also remarked about WEKA's need for dedicated CPU cores and local storage to deliver the highest performance; this, combined with its client scalability, lends itself well to smaller clusters of fat GPU nodes. I didn't run into anyone using WEKA in the cloud though, so I assume the feedback I gathered had a bias towards more conventional, on-prem styles of architecting storage for traditional HPC.</p><p>Whereas WEKA leaned into its rock 'n' roll theme this year, VAST doubled down on handing out the irresistibly tacky light-up cowboy hats they introduced last year (which I'm sure their neighbors at the DDN booth absolutely loved). They were all-in on promoting their new identity as a \"data platform\" this year, and although I didn't hear anyone refer to VAST as anything but a file system, I couldn't throw a rock without hitting someone who either recently bought a VAST system or tried one out.</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>Unlike last year though, customer sentiment around VAST wasn't all sunshine and rainbows, and I ran into a few customers who described their presales engagements as more formulaic than the white-glove treatment everyone seemed to be getting a year ago. This isn't surprising; there's no way to give all customers the same royal treatment as a business scales. But it does mean that the honeymoon period between VAST and the HPC industry is probably at an end, and they will have to spend the time between now and SC24 focusing on consistent execution to maintain the momentum they've gotten from the light-up cowboy hats.</p><p>The good news for VAST is that they've landed some major deals this past year, and they came to SC with customers and partners in-hand. They co-hosted a standing-room-only party with CoreWeave early in the week and shared a stage with Lambda at a customer breakfast, but they also highlighted two traditional, on-prem HPC customers (TACC and NREL) at the latter event.</p><p>VAST clearly isn't letting go of the on-prem HPC market as it also pursues partnerships with emerging GPU cloud service providers; this contrasted with WEKA's apparent focus on AI, GPUs, and the cloud. Time will tell which strategy (if either, or both) proves to be the better approach.</p><h3>DAOS</h3><div>Though commercial buyers were definitely most interested in VAST and WEKA, folks from the more sophisticated HPC shops around the world also tossed a few questions about DAOS my way this year.</div><p>I usually make it a point to attend the annual DAOS User Group meeting since it is always attended by all the top minds in high-performance I/O research, but I had to miss it this year on account of it running at the same time as my I/O tutorial. Fortunately, DAOS was pervasive throughout the conference, and there was no shortage of opportunity to find out what the latest news in the DAOS was. For example, check out the lineup for <a href=\"https://www.pdsw.org/index.shtml\">PDSW 2023</a> this year:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>Three out of thirteen talks were about DAOS which is more than any other single storage product or project. DAOS also won big at this year's IO500, taking the top two spots in the production storage system list:</p><p><br />&lt;div class=\"separator\" style=\"clear: both; text-align: center;\"&gt;&lt;/div&gt;</p><p>In fact, DAOS underpinned every single new awardee this year, and DAOS is now the second most represented storage system on the list behind Lustre:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>Why is DAOS at the top of so many people's minds this year? Well, DAOS reached a few major milestones in the past few months which has thrust it into the public eye.  </p><p>First, Aurora is finally online and running jobs, and while the compute system is only running at half its capability, the full DAOS system (<a href=\"https://io500.org/submissions/configuration/688\">all 220 petabytes of it</a>, all of which is TLC NVMe) is up and running--a testament to the scalability of DAOS that many parallel storage systems--including VAST and WEKA--have not publicly demonstrated. Because DAOS is open-source software and Aurora is an open-science system, all of DAOS' at-scale warts are also on full display to the community in a way that no competitive storage system besides of Lustre is.</p><p>Second, Google Cloud cast a bold vote of confidence in DAOS by launching <a href=\"https://cloud.google.com/parallelstore\">Parallelstore, its high-performance parallel file service based on DAOS</a>, in August. Whereas AWS and Azure have bet on Lustre to fill the high-performance file gap (via FSx Lustre and Azure Managed Lustre), GCP has planted a stake in the ground by betting that DAOS will be the better foundation for a high-performance file service for HPC and AI workloads.</p><p>Parallelstore is still in private preview and details are scant, but GCP had DAOS and Parallelstore dignitaries at all the major storage sessions in the technical program to fill in the gaps. From what I gathered, Parallelstore is still in its early stages and is intended to be a fast scratch tier; it's using DRAM for metadata which means it relies on erasure coding across servers to avoid data loss on a single server reboot, and there's no way to recover data if the whole cluster goes down at once. This lack of durability makes it ineligible for the IO500 list right now, but the upcoming metadata-on-NVMe feature (which previews in upstream DAOS in 1H2024) will be the long-term solution to that limitation.</p><p>Finally, the third major bit of DAOS news was about the formation of the <a href=\"https://foundation.daos.io/\">DAOS Foundation</a>. First announced earlier this month, this initiative lives under the umbrella of the Linux Foundation and is led by its five founding members:</p><p></p><ul><li><b>Argonne National Laboratory</b>, who has a vested interest in seeing DAOS endure given its massive investment in it,</li><li><b>Enakta Labs</b>, a company spun out of <a href=\"https://croit.io/\">Croit</a>, a German storage services company that was contributing feature development to DAOS,</li><li><b>Google Cloud</b>, who has made a big bet on DAOS as the underpinnings for its Parallelstore service,</li><li><b>HPE</b>, who has a shared fate with the DAOS installation at Argonne and who has also been contributing feature development, and</li><li><b>Intel</b>, whose engineers largely developed DAOS as part of the Aurora program.</li></ul><p></p><p>I see this handoff of DAOS from Intel to this new foundation as a positive change that makes DAOS a more stable long-term bet; should Intel choose to divest itself of DAOS once its obligations to the Aurora program end, DAOS now can live on without the community having to fork it. The DAOS Foundation is somewhat analogous to OpenSFS (one of the nonprofits backing Lustre) in that it is a vendor-neutral organization around which the DAOS community can gather.</p><p>But unlike OpenSFS, the DAOS Foundation will also assume the responsibility of releasing new versions of DAOS after Intel releases its final version (2.6) in March 2024. The DAOS Foundation will also steer feature prioritization, but seeing as how the DAOS Foundation doesn't fund developers directly, it's not clear that contributors like Intel or GCP are actually at the mercy of the foundation's decisions. It's more likely that the DAOS Foundation will just have authority to decide what features will roll up into the next formal DAOS release, and developers contributing code to DAOS will still prioritize whatever features their employers tell them to.</p><div><p>So, DAOS was the talk of the town at SC23. Does this all mean that DAOS is ready for prime time?</p><p>While Intel and Argonne may say yes, the community seems to have mixed feelings.  Consider this slide presented by László Szűcs from LRZ at the <a href=\"https://sc23.conference-program.com/presentation/?id=bof131&amp;sess=sess392\">DAOS Storage Community BOF</a>:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>DAOS is clearly crazy fast and scales to hundreds of petabytes in production--Aurora's IO500 listing proves that. However, that performance comes with a lot of complexity that is currently being foisted on application developers, end-users, and system administrators. The \"opportunities\" listed in László's slide are choices that people running at leadership HPC scale may be comfortable making, but the average HPC user is not equipped to make many of these decisions and make thoughtful choices about container types and library interfaces.</p><p>The fact that DAOS was featured so prominently at PDSW--a research workshop--probably underscores this as well. This <a href=\"https://sc23.conference-program.com/presentation/?id=ws_pdswwip104&amp;sess=sess435\">slide presented by Adrian Jackson's lighting talk</a> sums up the complexity along two different dimensions:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>His results showed that your choice of DAOS object class and I/O library atop the DAOS POSIX interface can result in wildly different checkpoint bandwidth. It's hard enough to teach HPC users about getting optimal performance out of a parallel file system like Lustre; I can't imagine those same users will embrace the idea that they should be mindful of which object class they use as they generate data.</p><p>The other <a href=\"https://sc23.conference-program.com/presentation/?id=ws_pdsw111&amp;sess=sess435\">DAOS-related research talk, presented by Greg Eisenhauer</a>, was a full-length paper that caught me by surprise and exposed how much performance varies when using different APIs into DAOS. This slide is one of many that highlighted this:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>I naively thought that the choice of native userspace API (key-value or array) would have negligible effects on performance, but Eisenhauer's talk showed that this isn't true. The reality appears to be that, although DAOS is capable of handling unaligned writes better than Lustre, aligning arrays on large, power-of-two boundaries still has a significant performance benefit.</p><p>Based on these sorts of technical talks about DAOS presented this year, the original question--is DAOS ready for prime time--can't be answered with a simple yes or no yet.  The performance it offers is truly best in class, but achieving that performance doesn't come easy right now. Teams who are already putting heroic effort into solving a high-value problems will probably leap at the opportunity to realize the I/O performance that DAOS can deliver. Such high value problems include things like training the next generation of foundational LLMs, and GCP's bet on DAOS probably adds differentiable value to their platform as a place to train such models as efficiently as possible. But the complexity of DAOS at present probably limits its appeal to the highest echelons of leadership HPC and AI, and I think it'll be a while before DAOS is in a place where a typical summer intern will be able to appreciate its full value.</p><h3>Infinia</h3></div><p>It would be unfair of me to give all this regard to WEKA, VAST, and DAOS without also mentioning <a href=\"https://www.ddn.com/press-releases/ddn-launches-infinia-next-generation-software-defined-storage-for-enterprise-ai-and-cloud-needs-everywhere-and-all-at-once/\">DDN's brand new Infinia product, launched right before SC23</a>. Those in the HPC storage industry have been awaiting its launch for years now, but despite the anticipation, it really didn't come up in any conversations in which I was involved. I did learn that the engineering team developing Infinia inside DDN is completely separate from the Whamcloud team who is developing Lustre, but this could be a double-edged sword. On the good side, it means that open-source Lustre development effort isn't competing with DDN's proprietary product in engineering priorities on a day-to-day basis. On the bad side though, I still struggle to see how Infinia and Lustre can avoid eventually competing for the same business.</p><p>For the time being, Infinia does seem to prioritize more enterprisey features like multitenancy and hands-free operation while Lustre is squarely aimed at delivering maximum performance to a broadening range of workloads. Their paths may eventually cross, but that day is probably a long way off, and Lustre has the benefit of being deeply entrenched across the HPC industry.</p><h2>The emergence of pure-play GPU clouds</h2><p>In addition to chatting with people about what's new in storage, I also went into SC23 wanting to understand how other cloud service providers are structuring end-to-end solutions for large-scale AI workloads. What I didn't anticipate was how many smaller cloud service providers (CSPs) showed up to SC for the first time this year, all waving the banner of offering NVIDIA H100 GPUs. These are predominantly companies that either didn't exist a few years ago or have historically focused on commodity cloud services like virtual private servers and managed WordPress sites, so it was jarring to suddenly see them at an HPC conference. How did so many of these smaller CSPs suddenly become experts in deploying GPU-based supercomputers in the time between SC22 and SC23? </p><p>I got to talking to a few folks at these smaller CSPs to figure out exactly what they were offering to customers, and their approach is quite different from how AWS, Azure, and GCP operate. Rather than defining a standard cluster architecture and deploying copies of it all over to be consumed by whoever is willing to pay, these smaller CSPs deploy clusters of whitebox GPU nodes to customer specification and sell them as dedicated resources for fixed terms. If a customer wants a bunch of HGX H100s interconnected with InfiniBand, that's what they get. If they want RoCE, the CSP will deploy that instead. And the same is true with storage: if a customer wants EXAScaler or Weka, they'll deploy that too.</p><p>While this is much closer to a traditional on-prem cluster deployment than a typical elastic, pay-as-you-go infrastructure-as-a-service offering, this is different from being a fancy colo. The end customer still consumes those GPUs as a cloud resource and never has to worry about the infrastructure that has to be deployed behind the curtain, and when the customer's contract term is up, their cluster is still owned by the CSP. As a result, the CSP can either resell that same infrastructure via pay-as-you-go or repurpose it for another dedicated customer. By owning the GPUs and selling them as a service, these CSPs can also do weird stuff like take out giant loans to build more data centers using GPUs as collateral. Meanwhile, NVIDIA can sell GPUs wholesale to these CSPs, book the revenue en masse, and let the CSPs deal with making sure they're maintained in production and well utilized.</p><p>It also seems like the services that customers of these smaller CSPs get is often more barebones than what they'd get from a Big 3 CSP (AWS, Azure, and GCP). They get big GPU nodes and an RDMA fabric, but managed services beyond that are hit and miss.</p><p>For example, one of these smaller CSPs told me that most of their storage is built on hundreds of petabytes of open-source Ceph. Ceph fulfills the minimum required storage services that any cloud must provide (object, block, and file), but it's generally insufficient for large-scale model training. As a result, all the smaller CSPs with whom I spoke said they are also actively exploring VAST and Weka as options for their growing GPU-based workloads. Since both VAST and Weka offer solid S3 and file interfaces, either could conceivably act as the underpinnings of these GPU clouds' first-party storage services as well.</p><p>As I said above though, it seems like the predominant model is for these CSPs to just ship whatever dedicated parallel storage the customer wants if something like Ceph isn't good enough. This, and the growing interest in storage from companies like VAST and Weka, suggest a few things:</p><p></p><ul><li>Some of these CSPs have been obtaining and deploying GPUs faster than they've had time to think about the end-to-end experience, and customers have so much pent-up demand for GPUs that they're willing to either work with whatever third-party storage vendor is brought to the table or take on the responsibility of choosing their preferred storage vendor themselves.</li><li>Having giant piles of GPUs is necessary, but not sufficient, to have a competitive offering in the GPU cloud services landscape. A credible platform for AI training must also have an integrated high-performance storage service.</li><li>It is looking like many pure-play GPU clouds are finding it more cost-effective to buy their way out of high-performance storage problems through partnerships than build and manage their own services atop open-source software like Lustre or DAOS.</li></ul><p>None of these observations are terribly surprising; at the price these smaller CSPs are offering GPUs compared to the Big 3 CSPs, their gross margin (and therefore their ability to invest in developing services on top of their IaaS offerings) has got to be pretty low. In the short term, it's cheaper and easier to deploy one-off high-performance storage systems alongside dedicated GPU clusters based on customer demand than develop and support a standard solution across all customers.</p><p>Of course, building a low-cost GPU service opens the doors for other companies to develop their own AI services on top of inexpensive GPU IaaS that is cost-competitive with the Big 3's native AI platforms (AWS SageMaker, Azure Machine Learning, and Google AI Platform). For example, I chatted with some folks at <a href=\"https://www.together.ai/\">together.ai</a>, a startup whose booth caught my eye with its bold claim of being \"the fastest cloud for [generative] AI:\"</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>Contrary to their banner, they aren't a cloud; rather, they provide AI services--think inferencing and fine-tuning--that are accessible through an API much like <a href=\"https://platform.openai.com/docs/guides/fine-tuning\">OpenAI's API</a>. They've engineered their backend stack to be rapidly deployable on any cloud that provides basic IaaS like GPU-equipped VMs, and this allows them to actually run their computational backend on whatever cloud can offer the lowest-cost, no-frills GPU VMs. In a sense, companies like together.ai develop and sell the frills that these new GPU CSPs lack, establishing a symbiotic alternative to the vertically integrated AI platforms on bigger clouds.</p><p>I did ask a few of these smaller CSPs what their overall pitch was. Why I would choose GPU cloud X over their direct competitor GPU cloud Y? The answers went in two directions:</p><p></p><ol><li>They offer lower cost per GPU hour than their competition</li><li>They are faster to get GPUs off a truck and into production than their competition</li></ol><p>There's a big caveat here: I didn't talk to many representatives at these CSPs, so my sample size was small and not authoritative. However, taking these value propositions at face value struck me as being quite precarious since their value is really a byproduct of severe GPU shortages driven by the hyped-up AI industry. What happens to these CSPs (and the symbionts whose businesses depend on them) when AMD GPUs appear on the market in volume? What happens if NVIDIA changes course and, instead of peanut-buttering its GPUs across CSPs of all sizes, it focuses its attention on prioritizing deliveries to just a few blessed CSPs?</p><p>There is <a href=\"https://a16z.com/who-owns-the-generative-ai-platform/\">no moat around generative AI</a>, and I left SC23 feeling like there's a dearth of long-term value being generated by some of these smaller GPU CSPs. For those CSPs whose primary focus is buying and deploying as many GPUs in as short a time as possible, not everyone can survive. They'll either come out of this GPU shortage having lost a lot of money building data centers that will go unused, or they'll be sold for parts.</p><p>More importantly to me though, I learned that I should give less credence to the splashy press events of hot AI-adjacent startups if their successes lie exclusively with smaller GPU CSPs. Some of these CSPs are paying to make their problems go away in an effort to keep their focus on racking and stacking GPUs in the short term, and I worry that there's a lack of long-term vision and strong opinions in some of these companies. Some of these smaller CSPs seem much more like coin-operated GPU cluster vending machines than platform providers, and that business model doesn't lend itself to making big bets and changing the industry.</p><p>Put another way, my job--both previous and current--has always been to think beyond short-term band aids and make sure that my employer has a clear and opinionated view of the technical approach that will be needed to address the challenges of HPC ten years in the future. I know who my peers are at the other Big 3 CSPs and leadership computing facilities across the world, and I know they're thinking hard about the same problems that I am. What worries me is that I do <u>not</u> know who my peers are at these smaller CSPs, and given their speed of growth and smaller margins, I worry that they aren't as prepared for the future as they will need to be. The AI industry as a whole will be better off when GPUs are no longer in such short supply, but the ecosystem surrounding some of these smaller GPU CSPs is going to take some damage when that day comes.</p><p></p><p></p><h2>Other dribs and drabs</h2><div>I also had a lot of interesting conversations and noticed a few subtle themes last week that don't neatly fit into any other category, but I'd love to hear more from others if they noticed the same or have more informed opinions.</div><h3>APUs and superchips - are they really that useful?</h3><p>Because I spent my booth duty standing next to one of Eagle's 8-way HGX H100 nodes, a lot of people asked me if I thought the Grace Hopper superchip would be interesting. I'm not an expert in either GPUs or AI, but I did catch up with a few colleagues who are smarter than me in this space last week, and here's the story as I understand it:</p><p>The Grace Hopper superchip (let's just call it GH100) is an evolution of the architecture developed for Summit, where V100 GPUs were cache-coherent with the CPUs through a special widget that converted NVLink to the on-chip coherence protocol for Power9. With GH100, the protocol used to maintain coherence across the CPU is directly compatible with the ARM AMBA coherence protocol, eliminating one bump in the path that Power9+V100 had. Grace also has a much more capable memory subsystem and NOC that makes accessing host memory from the GPU more beneficial.</p><p>Now, do AI workloads really need 72 cores per H100 GPU? Probably not.</p><p>What AI (and HPC) will need are some high-performance cores to handle all the parts of application execution that GPUs are bad at--divergent code paths, pointer chasing, and I/O. Putting capable CPU cores (Neoverse V2, not the N2 used in CPUs like new <a href=\"https://www.tomshardware.com/news/microsoft-azure-maia-ai-accelerator-cobalt-cpu-custom\">Microsoft's Cobalt 100</a>) on a capable NOC that is connected to the GPU memory subsystem at 900 GB/s opens doors for using hierarchical memory to train LLMs in clever ways.</p><p>For example, naively training an LLM whose weights and activations are evenly scattered across both host memory and GPU memory won't go well since that 900 GB/s of NVLink C2C would be on the critical path of many computations. However, techniques like <a href=\"https://lightning.ai/docs/pytorch/1.4.4/advanced/advanced_gpu.html#fairscale-activation-checkpointing\">activation checkpointing</a> could become a lot more versatile when the cost of offloading certain tensors from GPU memory is so much lower. In essence, the presence of easily accessible host memory will likely allow GPU memory to be used more efficiently since the time required to transfer tensors into and out of HBM is easier to hide underneath other computational steps during training.</p><p>Pairing an over-specified Grace CPU with a Hopper GPU also allows the rate of GPU development to proceed independently of CPU development. Even if workloads that saturate an H100 GPU might not also need all 72 cores of the Grace CPU, H200 or other future-generation GPUs can grow into the capabilities of Grace without having to rev the entire superchip.</p><p>I didn't get a chance to talk to any of my colleagues at AMD to get their perspective on the MI300 APU, but I'd imagine their story is a bit simpler since their memory space is flatter than NVIDIA's superchip design. This will make training some models undoubtedly more straightforward but perhaps leave less room for sophisticated optimizations that can otherwise cram more of a model into a given capacity of HBM. I'm no expert though, and I'd be happy to reference any explanations that real experts can offer! </p><h3>What about quantum?</h3><p>Quantum computing has been a hot topic for many years of SC now, but it feels like a topic that is finally making its way out of pure CS research and into the minds of the everyday HPC facility leaders. I talked to several people last week who asked me for my opinion on quantum computing because they have come to the realization that they need to know more about it than they do, and I have to confess, I'm in the same boat as they are. I don't follow quantum computing advancements very closely, but I know an increasing number of people who do--and they're the sort who work in CTOs' offices and have to worry about risks and opportunities more than intellectual curiosities.</p><p>It's hard to say there've been any seismic shifts in the state of the art in quantum computing at SC23; as best I can tell, there's still a rich ecosystem of venture capital-backed startups who keep cranking out more qubits. But this year felt like the first year where HPC facilities who haven't yet started thinking about their position on quantum computing are now behind. Not everyone needs a quantum computer, and not everyone even needs a quantum computing researcher on staff. But everyone should be prepared with a strong point of view if they are asked \"what will you be doing with quantum computing?\" by a funding agency or chief executive.</p><h3>NextSilicon</h3><p>One of the least-stealthy stealth-mode startups in the HPC industry has been NextSilicon, a company who debuted from stealth mode at SC23, launched their new Maverick accelerator, and announced their first big win with <a href=\"https://www.sandia.gov/research/2023/11/09/sandia-partners-with-nextsilicon-and-penguin-solutions-to-deliver-first-of-its-kind-runtime-reconfigurable-accelerator-technology/\">Sandia National Lab's Vanguard II project</a>. </p><p>What's notable about NextSilicon is that, unlike just about every other accelerator startup out there, they are not trying to go head-to-head with NVIDIA in the AI acceleration market. Rather, they've created a dataflow accelerator that aims to accelerate challenging HPC workloads that GPUs are particularly bad at--things like irregular algorithms and sparse data structures. They've paired this hardware with a magical runtime that continually optimizes the way the computational kernel is mapped to the accelerator's reconfigurable units to progressively improve the throughput of the accelerator as the application is running.</p><p>The concept of dataflow accelerators has always been intriguing since they're the only alternative to improving computational throughput besides making larger and larger vectors. The challenge has always been that these accelerators are more like FPGAs than general-purpose processors, and they require similar amounts of hardcore CS expertise to use well. NextSilicon claims to have cracked that nut with their runtime, and it seems like they're hiring the rights sorts of people--real HPC with respectable pedigrees--to make sure their accelerator can really deliver value to HPC workloads.</p><h3>I/O benchmarking developments</h3><p>At the <a href=\"https://sc23.conference-program.com/presentation/?id=bof144&amp;sess=sess363\">IO500 BOF</a>, there was rich discussion about adding new benchmarking modes to IOR and IO500 to represent a wider range of patterns.</p><p>More specifically, there's been an ongoing conversation about including a 4K random read test, and it sounds like the most outspoken critics against it have finally softened their stance. I've not been shy about why I think using <a href=\"https://glennklockwood.blogspot.com/2021/10/iops-are-dumb.html\">IOPS as a measure of file system performance is dumb</a>, but 4K random IOPS do establish a lower bound of performance for what a real application might experience. Seeing as how <a href=\"https://www.glennklockwood.com/benchmarks/io500.html#interpreting-results\">IO500 has always been problematic as any representation of how a file system will perform in real-world environments</a>, adding the option to run a completely synthetic, worst-case workload will give IO500 the ability to define a complete bounding box around the lower and upper limits of I/O performance for a file system.</p><p>Hendrik Nolte from GWDG also proposed a few new and appealing IOR modes that approach more realistic workload scenarios.  The first was a new locally random mode where data is randomized within IOR segments but segments are repeated:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>Compared to globally randomized reads (which is what IOR normally does), this is much closer representation of parallel workloads that are not bulk-synchronous; for example, NCBI BLAST uses thread pools and work sharing to walk through files, and the resulting I/O pattern is similar to this new mode.</p><p>He also described a proposal to run concurrent, mixed workloads in a fashion similar to how fio currently works.  Instead of performing a bulk-synchronous parallel write followed by a bulk-synchronous parallel read, his proposal would allow IOR to perform reads and writes concurrently, more accurately reflecting the state of multitenant storage systems. I actually wrote <a href=\"https://github.com/glennklockwood/iopup\">a framework to do exactly this and quantify the effects of contention using IOR and elbencho</a>, but I left the world of research before I could get it published. I'm glad to see others seeing value in pursuing this idea.</p><p>The other noteworthy development in I/O benchmarking was presented by <a href=\"https://sc23.conference-program.com/presentation/?id=bof108&amp;sess=sess411\">Sven Breuner at the Analyzing Parallel I/O BOF</a> where he described a new netbench mode for his excellent <a href=\"https://github.com/breuner/elbencho\">elbencho benchmark tool</a>. This netbench mode behaves similarly to iperf in that it is a network-level throughput test, but because it is part of elbencho, it can generate the high-bandwidth incasts and broadcasts that are typically encountered between clients and servers of parallel storage systems:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>This is an amazing development because it makes elbencho a one-stop shop for debugging the entire data path of a parallel storage system. For example, if you're trying to figure out why the end-to-end performance of a file system is below expectation, you can use elbencho to test the network layer, the object or file layer, the block layer, and the overall end-to-end path separately to find out which layer is underperforming. Some file systems have specialized included tools to perform the same network tests (e.g., <a href=\"https://github.com/IBM/SpectrumScale_NETWORK_READINESS/blob/master/nsdperf.C\">nsdperf for IBM Spectrum Scale</a>), but elbencho now has a nice generic way to generate these network patterns for any parallel storage system.</p><h2>Some personal thoughts</h2><p>As with last year, I couldn't attend most of the technical program due to a packed schedule of customer briefings and partner meetings, but the <a href=\"https://sc23.supercomputing.org/attend/digital-experience/\">SC23 Digital Experience</a> was excellently done, and I wound up watching a lot of the content I missed during the mornings and after the conference (at 2x speed!). In that sense, the hybrid nature of the conference is making it easier to attend as someone who has to juggle business interests with technical interests; while I can't jump into <a href=\"https://sc23.conference-program.com/presentation/?id=bof131&amp;sess=sess392\">public arguments about the definition of storage \"QOS\"</a>, I can still tell that my old friends and colleagues are still fighting the good fight and challenging conventional thinking across the technical program.</p><h3>My Parallel I/O in Practice tutorial</h3><p>This was the sixth year that I co-presented the <a href=\"https://sc23.conference-program.com/presentation/?id=tut134&amp;sess=sess243\">Parallel I/O in Practice tutorial</a> with my colleagues Rob Latham, Rob Ross, and Brent Welch. A <a href=\"https://scphoto.passgallery.com/-sc23/gallery\">conference photographer got this great photo</a> of me in the act:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>Presenting this tutorial is always an incredibly gratifying experience; I've found that sharing what I know is one of the most fulfilling ways I can spend my time, and being able to start my week in such an energizing way is what sustains the sleep deprivation that always follows. Giving the tutorial is also an interesting window into what the next generation of I/O experts is worrying about; for example, we got a lot of questions and engagement around the low-level hardware content in our morning half, and the I/O benchmarking material in the late afternoon seemed particularly well received. The majority of attendees came from the systems side rather than the user/dev side as well, perhaps suggesting that the growth in demand for parallel storage systems (and experts to run them) is outstripping the demand for new ways to perform parallel I/O. Guessing wildly, perhaps this means new developers are coming into the field higher up the stack, using frameworks like <a href=\"https://pypi.org/project/fsspec/\">fsspec</a> that abstract away low-level I/O.</p><p>Since I've jumped over to working in industry, it's been hard to find the business justification to keep putting work hours into the tutorial despite how much I enjoy it.  I have to confess that I didn't have time to update any of the slides I presented this year even though the world of parallel I/O has not remained the same, and I am going to have to figure out how to better balance these sorts of community contributions with the demands of a day job in the coming years.</p><h3>An aside on COVID safety</h3><p>At SC22, I fastidiously wore a KN95 mask while indoors and avoided all after-hours events and indoor dining to minimize my risk of catching COVID. At that time, neither my wife nor I had ever gotten COVID before, and I had no desire to bring it home to my family since my father died of COVID-related respiratory failure two years prior. Staying fully masked at SC22 turned out to be a great decision at the time since a significant number of other attendees, including many I spoke with, contracted COVID at SC22. By comparison, I maintained my COVID-free streak through 2022.</p><p>This year I took a more risk-tolerant approach for two reasons:</p><p></p><ol><li>My wife and I both broke our streaks this past summer and contracted COVID while on vacation, so if I got sick, we knew what to expect, and</li><li>I got my gazillionth COVID and flu shots in October in anticipation of attending SC.</li></ol><p>Part of my approach to managing risk was bringing my trusty <a href=\"https://aranet.com/products/aranet4/\">Aranet4 CO2 sensor</a> with me so that I could be aware of areas where there was air circulation and the risk of contracting an airborne illness would be higher. I only wore a KN95 at the airport gates and while on the airplane at SC23, and despite going in all-in on after-hours events, indoor dining, and copious meetings and tours of booth duty, I'm happy to report that I made it through the conference without getting sick.</p><p>I have no doubt that being vaccinated helped, as I've had several people tell me they tested positive for COVID after we had dinner together in Denver. But it's also notable that the Denver Convention Center had <u>much</u> better ventilation than Kay Bailey Hutchison Convention Center in Dallas where SC22 was held last year. To show this quantitatively, let's compare air quality measurements from SC22 to SC23.</p><p>My schedule for the day on which I give my tutorial is always the same: the tutorial runs from 8:30am to 5:00pm with breaks at 10:00, 12:00, and 3:00. Because of this consistent schedule, comparing the CO2 readings (which are a proxy for re-breathed air) for my tutorial day at SC22 versus SC23 shows how different the air quality was in the two conference centers. Here's what that comparison looks like:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>What the plot shows is that CO2 (re-breathed air) steadily increased at the start of the tutorial at both SC22 and SC23, but Denver's convention center kicked on fresh air ventilation after an hour while Dallas simply didn't. Air quality remained poor (over 1,000) throughout the day in Dallas, whereas Denver was pretty fresh (below 700) even during the breaks and the indoor luncheon. This relatively good air circulation inside the convention center at SC23 made me much more comfortable about going maskless throughout the week.</p><p>This isn't to say that I felt there was no risk of getting sick this year; there was at least one busy, upscale restaurant/bar in which I dined where the air circulation was no better than in a car or airplane. For folks who just don't want to risk being sick over Thanksgiving, wearing a mask and avoiding crowded bars was probably still the best option this year. And fortunately, Denver's weather was gorgeous, so outdoor dining was completely viable during the week.</p><h3>AI's effects on the HPC community</h3><p>Although AI has played a prominent role in previous SC conferences, this was the first year where I noticed that the AI industry is bleeding into the HPC community in weird ways.</p><p>For example, I had a bunch of journalists and media types accost me and start asking rather pointed questions while I was on booth duty. Talking to journalists isn't entirely unusual since I've always been supportive of industry press, but the social contract between practitioners like me and journalists has always been pretty formal--scheduling a call in advance, being invited to speak at an event, and things like that have long been the norm. If I was being interviewed on the record, I knew it.</p><p>This year though, it seemed like there was a new generation of younger journalists who approached me no differently than a casual booth visitor. Some did introduce themselves as members of the press after we got chatting (good), but others did not (not good) which led me to take away a learning: check names and affiliations before chatting with strangers, because the days where I could assume that all booth visitors would act in good faith are gone.</p><p>Now, why the sudden change?  I can think of three possible reasons:</p><p></p><ol><li>I'm getting older, and there are now tech industry journalists who are younger than me and think I am worth talking to since I've always been around. Maybe the old-school HPC folks that predate me have always had to deal with this.</li><li>The proliferation of platforms like Substack make it financially viable to be an independent journalist, and conversely, anyone can be a journalist without editorial oversight.</li><li>The spotlight on the massive AI industry is also illuminating the HPC industry. HPC and AI are both built on the same foundational technologies (GPUs, RDMA fabrics, HBM, and the like) so AI journalists now have a reason to start showing up at HPC community events.</li></ol><p>It'd be fair to argue that #3 is a stretch and that this isn't an AI phenomenon if not for the fact that I was also accosted by a few venture capitalists for the first time this year. HPC has never been an industry that attracted the attention of venture capital in the way that AI does, so I have to assume being asked specific questions about the viability of some startup's technology is a direct result of the AI market opportunity.</p><p>While it's nice to have a broader community of attendees and more media coverage, the increasing presence of AI-focused media and VC types in the SC community means I can't be as open and honest as I once was. Working for a corporation (with secrets of its own to protect) doesn't help there either, so maybe getting cagier when talking to strangers is just a part of growing up.</p><p></p><h3>SC23 as a milestone year</h3><p>Attending SC23 this year coincided with two personal milestones for me as well.</p><p>This is the tenth year I've been in the HPC business, and the first SC I ever attended was SC13.  I can't say that this is my eleventh SC because I didn't attend in 2014 (on account of working at a biotech startup), but I've been to SC13, SC15 through SC19, SC20 and SC21 virtually, and SC22 and SC23 in-person.  At SC13 ten years ago, the weather was a lot colder:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>But I still have the fondest memories of that conference because it that was the week where I felt like I had finally found my community after having spent a decade as an unhappy materials science student.</p><p>SC23 is also a milestone year because it may be the last SC I attend as a storage and I/O guy. I recently signed on for a new position within Microsoft to help architect the next generation of supercomputers for AI, and I'll probably have to trade in the time I used to spend at workshops like PDSW for opportunities to follow the latest advancements in large-scale model training, RDMA fabrics, and accelerators. But I think I am OK with that.</p><p>I never intended to become an I/O or storage expert when I first showed up at SC13; it wasn't until I joined NERSC that I found that I could learn and contribute the most by focusing on storage problems. The world has changed since then, and now that I'm at Microsoft, it seems like the problems faced at the cutting edge of large language models, generative AI, and the pursuit of AGI are where the greatest need lies. As I said earlier in this post, AI has bigger problems to deal with than storage and I/O, and those bigger problems are what I'll be chasing. With any luck, I'll be able to say I had a hand in designing the supercomputers that Microsoft builds after Eagle. And as has been true for my last ten years in this business, I'll keep sharing whatever I learn with whoever wants to know.</p>",
            "url": "https://hpc.social/personal-blog/2023/sc-23-recap/",
            
            
            
            
            
            "date_published": "2023-11-23T08:05:00-07:00",
            "date_modified": "2023-11-23T08:05:00-07:00",
            
                "author": "Glenn K. Lockwood's Blog"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2023/advanced-lsf-resource-connector-configuration-on-ibm-cloud-part-i/",
            "title": "Advanced LSF resource connector configuration on IBM Cloud - part I",
            "summary": null,
            "content_text": "OverviewThis is the first in a series of blogs that discusses some advancedconfiguration of the IBM LSF resource connector. LSF resource connector enablesLSF clusters to borrow resources from supported resource providers in thecloud. LSF includes resource connectors for the following resource providers:IBM CloudAWSGoogle Cloud PlatformMicrosoft AzureMicrosoft Azure CycleCloudRed Hat OpenShiftOpenStackThe resource connector plug-ins for LSF are available under an open sourcelicense (the Apache License 2.0) on the public IBM Spectrum Computing githubhere.LSF resource connector works in conjunction with the LSF multiclustercapability to create a flexible and dynamic hybrid HPC cloud. LSF multiclusterenables organizations to have multiple LSF clusters connect with one anotherand to define queues which can forward to remote clusters and receive jobsfrom remote clusters. Historically LSF multicluster was used by clients whohave multiple, geographically dispersed compute centres and it allowed them toconnect these environments and have work forwarded between them. Naturallythis can also be used to setup an LSF cluster in the cloud and tie it in toyour existing on-premises LSF cluster. According to a recent Hyperion Researchwhitepaper, the most widely adopted framework for leveraging HPC resources inthe cloud is in a hybrid environment where a user runs their HPC workloadsboth on-premises and in the cloud.1Cloud templatesAs part of the configuration of the LSF resource connector, it’s necessary todefine templates which are used to specify a specific cloud instance type. Inother words, the template is used to define a set of hosts with commonattributes including memory and number of cores and operating system image.These templates are used by LSF when requesting instances from a particularcloud to satisfy the workload demands.In this post, we’ll take a closer look at configuring multiple LSF resourceconnector templates for IBM Cloud as the resource provider. By default, whenmultiple templates are configured, LSF will sort the candidate templateservers alphabetically by template name. However, administrators may wish tosort the templates according to a specific priority. For example, anorganization may want to assign a high priority to a template corresponding toa less costly instance type. When priorities are specified for templates, LSFwill use high priority templates first.The environment used in this example was deployed using the IBM Cloudautomation for LSF. Using Terraform/IBM Cloud Schematics, it’s possible toautomatically deploy a fully functioning LSF cluster in about 10 minutes time.This includes a login node, NFS storage node, LSF manager node(s) andLSF Application Center. The automation also configures the LSF resourceconnector for the instance type specified at deployment time. More detailedinformation can be found about deploying LSF on IBM Cloud here.The following steps assume that LSF has been deployed on IBM Cloud. Note thatthis is a single LSF cluster and not a hybrid cloud that has been configured.Now, we’ll look at the following configuration examples:Specifying multiple templates for different cloud instance types with different prioritiesUpdate LSF resource connector user script to load required OS packages on compute nodesMultiple templatesThe LSF deployment automation on IBM Cloud configures a single template forthe compute VSI type specified at deployment time. In the example below, wesee a single template configured for VSI profile type bx2-4x16. A completelist of available IBM Cloud VPC VSI types can be found here.ibmcloudgen2_templates.json (obfuscated){    \"templates\": [        {            \"templateId\": \"Template-1\",            \"maxNumber\": 2,            \"attributes\": {                \"type\": [\"String\", \"X86_64\"],                \"ncores\": [\"Numeric\", \"2\"],                \"ncpus\": [\"Numeric\", \"4\"],                \"mem\": [\"Numeric\", \"16384\"],                \"icgen2host\": [\"Boolean\", \"1\"]            },            \"imageId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"subnetId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"vpcId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"vmType\": \"bx2-4x16\",            \"securityGroupIds\": [\"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\"],            \"resourceGroupId\": \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\",            \"sshkey_id\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"region\": \"us-east\",            \"zone\": \"us-east-1\"        }    ]}Defining prioritiesYou’ll note that there is no priority specified for the template. When nopriority is defined, LSF sorts the templates according to template name. Next,we’ll define a second template in the configuration for the VSI instance typemx2-16x128. At the same time, we’ll introduce the priority parameter andspecify a priority of 10 (higher) for bx2-4x16, and 5 (lower) formx2-16x128. More details regarding the parameters can be found here. With this configuration, LSF will favour and use the higher prioritytemplate first, which in this case will be for VSI instance type bx2-4x16.ibmcloudgen2_templates.json (obfuscated, with priorities configured){    \"templates\": [        {            \"templateId\": \"Template-1\",            \"maxNumber\": 2,            \"attributes\": {                \"type\": [\"String\", \"X86_64\"],                \"ncores\": [\"Numeric\", \"2\"],                \"ncpus\": [\"Numeric\", \"4\"],                \"mem\": [\"Numeric\", \"16384\"],                \"icgen2host\": [\"Boolean\", \"1\"]             },            \"imageId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"subnetId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"vpcId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"vmType\": \"bx2-4x16\",            \"securityGroupIds\": [\"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\"],            \"resourceGroupId\": \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\",            \"sshkey_id\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            **\"priority\": \"10\",**             \"region\": \"us-east\",            \"zone\": \"us-east-1\"         },       {            \"templateId\": \"Template-2\",            \"maxNumber\": 2,            \"attributes\": {                \"type\": [\"String\", \"X86_64\"],                \"ncores\": [\"Numeric\", \"8\"],                \"ncpus\": [\"Numeric\", \"16\"],                \"mem\": [\"Numeric\", \"131072\"],                \"icgen2host\": [\"Boolean\", \"1\"]            },            \"imageId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"subnetId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"vpcId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"vmType\": \"mx2-16x128\",            \"userData\":\"profile=mx2_16x128\",            \"\"securityGroupIds\": [\"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\"],            \"resourceGroupId\": \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\",            \"sshkey_id\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            **\"priority\": \"5\",**            \"region\": \"us-east\",            \"zone\": \"us-east-1\"        }    ]}Modifying compute server optionsNext, we’ll submit some example jobs to the LSF cluster and observe how theLSF resource connector template priority influences the startup of resourcesby the LSF resource connector. The example job we wish to run is the OSsupplied stress command. stress is not installed in the default computeimages and can be added as part of the startup of the compute servers via theLSF resource connector user_data.sh script. This script is used to setenvironment variables, and control the startup of LSF on the compute servers.It can also be used to perform customization of the environment, and pass LSFresources to the compute servers. The user_data.sh script is located in:/opt/ibm/lsf/conf/resource_connector/ibmcloudgen2.Modifying the user_data.sh script, I’ve inserted the line to install the OSstress package before the LSF daemons startup.user_data.sh......**# Install stress utility****dnf install stress -y**cat $LSF_CONF_FILE  &gt;&gt; $logfilesleep 5lsf_daemons start &amp;sleep 5lsf_daemons status &gt;&gt; $logfileecho END `date '+%Y-%m-%d %H:%M:%S'` &gt;&gt; $logfile# Allow login as lsfadminnfs_mount_dir=\"data\"mkdir -p /home/lsfadmin/.sshcp /mnt/data/ssh/authorized_keys /home/lsfadmin/.ssh/authorized_keyscat /mnt/data/ssh/id_rsa.pub &gt;&gt; /root/.ssh/authorized_keyschmod 600 /home/lsfadmin/.ssh/authorized_keyschmod 700 /home/lsfadmin/.sshchown -R lsfadmin:lsfadmin /home/lsfadmin/.sshecho \"MTU=9000\" &gt;&gt; \"/etc/sysconfig/network-scripts/ifcfg-eth0\"systemctl restart NetworkManager......With the updates made to the user_data.sh script, we’re now ready to submitjobs to LSF. We submit two stress jobs as follows:[lsfadmin@icgen2host-10-241-0-37 ~]$ bsub -q normal /usr/bin/stress --cpu 1 --vm-bytes 8192MB --timeout 60sJob &lt;2836&gt; is submitted to queue &lt;normal&gt;.[lsfadmin@icgen2host-10-241-0-37 ~]$ bsub -q normal /usr/bin/stress --cpu 1 --vm-bytes 8192MB --timeout 60sJob &lt;2837&gt; is submitted to queue &lt;normal&gt;.After a few moments, the LSF resource connector automatically starts up asingle compute server on the IBM Cloud with hostname icgen2host-10-241-0-42to satisfy the pending workload requirements. Note that the hostname prefixicgen2host stands for IBM Cloud Generation 2 VPC. The numeric portion of thehostname represents the IP address of the server that was automaticallystarted by the LSF resource connector. Therefore, this hostname may differin your environment. Compute server icgen2host-10-241-0-42 is equipped with4 cores and 16 GB RAM, matching the higher priority template bx2-4x16.[lsfadmin@icgen2host-10-241-0-37 ~]$ lshosts -wHOST_NAME                       type       model  cpuf ncpus maxmem maxswp server RESOURCESicgen2host-10-241-0-37        X86_64    Intel_E5  12.5     4  15.4G      -    Yes (mg)icgen2host-10-241-0-42        X86_64    Intel_E5  12.5     4  15.5G      -    Dyn (icgen2host)[lsfadmin@icgen2host-10-241-0-37 ~]$ bhosts -rc -wHOST_NAME          STATUS          JL/U    MAX  NJOBS    RUN  SSUSP  USUSP    RSV RC_STATUS             PROV_STATUS           UPDATED_AT             INSTANCE_ID               icgen2host-10-241-0-37 closed_Full     -      0      0      0      0      0      0           -                     -                     -                      -               icgen2host-10-241-0-42 ok              -      4      2      2      0      0      0 Allocated             running               2023-11-06T23:02:26UTC 0757_4f5295bd-a265-4fda-840c-6f89e326ca1f As there is no other work that has been submitted to the LSF cluster, once thestress jobs have completed, the LSF resource connector will automaticallyshut down the compute servers according to the LSB_RC_EXTERNAL_HOST_IDLE_TIMEparameter in lsf.conf. This defines the time interval after which the LSFresource connector will relinquish the cloud instances if no jobs are running.ConclusionWe’ve just scratched the surface in terms of the configuration possibilitieswith LSF resource connector. In the next article we’ll look at how LSFresources can be assigned to servers which are dynamically started by the LSFresource connector, as well as configuring Docker to point to a localrepository.The Evolution of HPC Includes Strong Use of Hybrid Cloud&#160;&#x21a9;&#xfe0e;",
            "content_html": "<p><strong>Overview</strong></p><p>This is the first in a series of blogs that discusses some advancedconfiguration of the IBM LSF resource connector. LSF resource connector enablesLSF clusters to borrow resources from supported resource providers in thecloud. LSF includes resource connectors for the following resource providers:</p><ul><li>IBM Cloud</li><li>AWS</li><li>Google Cloud Platform</li><li>Microsoft Azure</li><li>Microsoft Azure CycleCloud</li><li>Red Hat OpenShift</li><li>OpenStack</li></ul><p>The resource connector plug-ins for LSF are available under an open sourcelicense (the Apache License 2.0) on the public IBM Spectrum Computing github<a href=\"https://github.com/IBMSpectrumComputing/cloud-provider-plugins\">here</a>.</p><p>LSF resource connector works in conjunction with the LSF multiclustercapability to create a flexible and dynamic hybrid HPC cloud. LSF multiclusterenables organizations to have multiple LSF clusters connect with one anotherand to define queues which can forward to remote clusters and receive jobsfrom remote clusters. Historically LSF multicluster was used by clients whohave multiple, geographically dispersed compute centres and it allowed them toconnect these environments and have work forwarded between them. Naturallythis can also be used to setup an LSF cluster in the cloud and tie it in toyour existing on-premises LSF cluster. According to a recent Hyperion Researchwhitepaper, the most widely adopted framework for leveraging HPC resources inthe cloud is in a hybrid environment where a user runs their HPC workloadsboth on-premises and in the cloud.<sup id=\"fnref:1\"><a class=\"footnote-ref\" href=\"https://www.gaborsamu.com/blog/index.xml#fn:1\">1</a></sup></p><p><strong>Cloud templates</strong></p><p>As part of the configuration of the LSF resource connector, it’s necessary todefine templates which are used to specify a specific cloud instance type. Inother words, the template is used to define a set of hosts with commonattributes including memory and number of cores and operating system image.These templates are used by LSF when requesting instances from a particularcloud to satisfy the workload demands.</p><p>In this post, we’ll take a closer look at configuring multiple LSF resourceconnector templates for IBM Cloud as the resource provider. By default, whenmultiple templates are configured, LSF will sort the candidate templateservers alphabetically by template name. However, administrators may wish tosort the templates according to a specific priority. For example, anorganization may want to assign a high priority to a template corresponding toa less costly instance type. When priorities are specified for templates, LSFwill use high priority templates first.</p><p>The environment used in this example was deployed using the IBM Cloudautomation for LSF. Using Terraform/IBM Cloud Schematics, it’s possible toautomatically deploy a fully functioning LSF cluster in about 10 minutes time.This includes a login node, NFS storage node, LSF manager node(s) andLSF Application Center. The automation also configures the LSF resourceconnector for the instance type specified at deployment time. More detailedinformation can be found about deploying LSF on IBM Cloud <a href=\"https://cloud.ibm.com/docs/ibm-spectrum-lsf?topic=ibm-spectrum-lsf-getting-started-tutorial\">here</a>.</p><p>The following steps assume that LSF has been deployed on IBM Cloud. Note thatthis is a single LSF cluster and not a hybrid cloud that has been configured.Now, we’ll look at the following configuration examples:</p><ul><li>Specifying multiple templates for different cloud instance types with different priorities</li><li>Update LSF resource connector user script to load required OS packages on compute nodes</li></ul><p><strong>Multiple templates</strong></p><p>The LSF deployment automation on IBM Cloud configures a single template forthe compute VSI type specified at deployment time. In the example below, wesee a single template configured for VSI profile type <em>bx2-4x16</em>. A completelist of available IBM Cloud VPC VSI types can be found <a href=\"https://cloud.ibm.com/docs/vpc?topic=vpc-profiles&amp;interface=ui\">here</a>.</p><p><strong>ibmcloudgen2_templates.json (obfuscated)</strong><div class=\"highlight\"><pre><code class=\"language-plaintext\">{    \"templates\": [        {            \"templateId\": \"Template-1\",            \"maxNumber\": 2,            \"attributes\": {                \"type\": [\"String\", \"X86_64\"],                \"ncores\": [\"Numeric\", \"2\"],                \"ncpus\": [\"Numeric\", \"4\"],                \"mem\": [\"Numeric\", \"16384\"],                \"icgen2host\": [\"Boolean\", \"1\"]            },            \"imageId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"subnetId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"vpcId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"vmType\": \"bx2-4x16\",            \"securityGroupIds\": [\"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\"],            \"resourceGroupId\": \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\",            \"sshkey_id\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"region\": \"us-east\",            \"zone\": \"us-east-1\"        }    ]}</code></pre></div></p><p><strong>Defining priorities</strong></p><p>You’ll note that there is no priority specified for the template. When nopriority is defined, LSF sorts the templates according to template name. Next,we’ll define a second template in the configuration for the VSI instance type<em>mx2-16x128</em>. At the same time, we’ll introduce the priority parameter andspecify a priority of 10 (higher) for <em>bx2-4x16</em>, and 5 (lower) for<em>mx2-16x128</em>. More details regarding the parameters can be found <a href=\"https://www.ibm.com/docs/en/spectrum-lsf/10.1.0?topic=reference-ibmcloudgen2-templatesjson\">here</a>. With this configuration, LSF will favour and use the higher prioritytemplate first, which in this case will be for VSI instance type <em>bx2-4x16</em>.</p><p><strong>ibmcloudgen2_templates.json (obfuscated, with priorities configured)</strong></p><div class=\"highlight\"><pre><code class=\"language-plaintext\">{    \"templates\": [        {            \"templateId\": \"Template-1\",            \"maxNumber\": 2,            \"attributes\": {                \"type\": [\"String\", \"X86_64\"],                \"ncores\": [\"Numeric\", \"2\"],                \"ncpus\": [\"Numeric\", \"4\"],                \"mem\": [\"Numeric\", \"16384\"],                \"icgen2host\": [\"Boolean\", \"1\"]             },            \"imageId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"subnetId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"vpcId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"vmType\": \"bx2-4x16\",            \"securityGroupIds\": [\"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\"],            \"resourceGroupId\": \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\",            \"sshkey_id\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            **\"priority\": \"10\",**             \"region\": \"us-east\",            \"zone\": \"us-east-1\"         },       {            \"templateId\": \"Template-2\",            \"maxNumber\": 2,            \"attributes\": {                \"type\": [\"String\", \"X86_64\"],                \"ncores\": [\"Numeric\", \"8\"],                \"ncpus\": [\"Numeric\", \"16\"],                \"mem\": [\"Numeric\", \"131072\"],                \"icgen2host\": [\"Boolean\", \"1\"]            },            \"imageId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"subnetId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"vpcId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"vmType\": \"mx2-16x128\",            \"userData\":\"profile=mx2_16x128\",            \"\"securityGroupIds\": [\"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\"],            \"resourceGroupId\": \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\",            \"sshkey_id\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            **\"priority\": \"5\",**            \"region\": \"us-east\",            \"zone\": \"us-east-1\"        }    ]}</code></pre></div><p><strong>Modifying compute server options</strong></p><p>Next, we’ll submit some example jobs to the LSF cluster and observe how theLSF resource connector template priority influences the startup of resourcesby the LSF resource connector. The example job we wish to run is the OSsupplied <em>stress</em> command. <em>stress</em> is not installed in the default computeimages and can be added as part of the startup of the compute servers via theLSF resource connector user_data.sh script. This script is used to setenvironment variables, and control the startup of LSF on the compute servers.It can also be used to perform customization of the environment, and pass LSFresources to the compute servers. The user_data.sh script is located in:<em>/opt/ibm/lsf/conf/resource_connector/ibmcloudgen2</em>.</p><p>Modifying the <em>user_data.sh</em> script, I’ve inserted the line to install the OS<em>stress</em> package before the LSF daemons startup.</p><p><strong>user_data.sh</strong></p><div class=\"highlight\"><pre><code class=\"language-plaintext\">......**# Install stress utility****dnf install stress -y**cat $LSF_CONF_FILE  &gt;&gt; $logfilesleep 5lsf_daemons start &amp;sleep 5lsf_daemons status &gt;&gt; $logfileecho END `date '+%Y-%m-%d %H:%M:%S'` &gt;&gt; $logfile# Allow login as lsfadminnfs_mount_dir=\"data\"mkdir -p /home/lsfadmin/.sshcp /mnt/data/ssh/authorized_keys /home/lsfadmin/.ssh/authorized_keyscat /mnt/data/ssh/id_rsa.pub &gt;&gt; /root/.ssh/authorized_keyschmod 600 /home/lsfadmin/.ssh/authorized_keyschmod 700 /home/lsfadmin/.sshchown -R lsfadmin:lsfadmin /home/lsfadmin/.sshecho \"MTU=9000\" &gt;&gt; \"/etc/sysconfig/network-scripts/ifcfg-eth0\"systemctl restart NetworkManager......</code></pre></div><p>With the updates made to the user_data.sh script, we’re now ready to submitjobs to LSF. We submit two <em>stress</em> jobs as follows:</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">[lsfadmin@icgen2host-10-241-0-37 ~]$ bsub -q normal /usr/bin/stress --cpu 1 --vm-bytes 8192MB --timeout 60sJob &lt;2836&gt; is submitted to queue &lt;normal&gt;.[lsfadmin@icgen2host-10-241-0-37 ~]$ bsub -q normal /usr/bin/stress --cpu 1 --vm-bytes 8192MB --timeout 60sJob &lt;2837&gt; is submitted to queue &lt;normal&gt;.</code></pre></div><p>After a few moments, the LSF resource connector automatically starts up asingle compute server on the IBM Cloud with hostname <em>icgen2host-10-241-0-42</em>to satisfy the pending workload requirements. Note that the hostname prefixicgen2host stands for IBM Cloud Generation 2 VPC. The numeric portion of thehostname represents the IP address of the server that was automaticallystarted by the LSF resource connector. Therefore, this hostname may differin your environment. Compute server <em>icgen2host-10-241-0-42</em> is equipped with4 cores and 16 GB RAM, matching the higher priority template <em>bx2-4x16</em>.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">[lsfadmin@icgen2host-10-241-0-37 ~]$ lshosts -wHOST_NAME                       type       model  cpuf ncpus maxmem maxswp server RESOURCESicgen2host-10-241-0-37        X86_64    Intel_E5  12.5     4  15.4G      -    Yes (mg)icgen2host-10-241-0-42        X86_64    Intel_E5  12.5     4  15.5G      -    Dyn (icgen2host)[lsfadmin@icgen2host-10-241-0-37 ~]$ bhosts -rc -wHOST_NAME          STATUS          JL/U    MAX  NJOBS    RUN  SSUSP  USUSP    RSV RC_STATUS             PROV_STATUS           UPDATED_AT             INSTANCE_ID               icgen2host-10-241-0-37 closed_Full     -      0      0      0      0      0      0           -                     -                     -                      -               icgen2host-10-241-0-42 ok              -      4      2      2      0      0      0 Allocated             running               2023-11-06T23:02:26UTC 0757_4f5295bd-a265-4fda-840c-6f89e326ca1f </code></pre></div><p>As there is no other work that has been submitted to the LSF cluster, once the<em>stress</em> jobs have completed, the LSF resource connector will automaticallyshut down the compute servers according to the <strong>LSB_RC_EXTERNAL_HOST_IDLE_TIME</strong>parameter in <em>lsf.conf</em>. This defines the time interval after which the LSFresource connector will relinquish the cloud instances if no jobs are running.</p><p><strong>Conclusion</strong></p><p>We’ve just scratched the surface in terms of the configuration possibilitieswith LSF resource connector. In the next article we’ll look at how LSFresources can be assigned to servers which are dynamically started by the LSFresource connector, as well as configuring Docker to point to a localrepository.</p><section class=\"footnotes\"><hr /><ol><li id=\"fn:1\"><p><a href=\"https://www.ibm.com/downloads/cas/RGKYOOKB\">The Evolution of HPC Includes Strong Use of Hybrid Cloud</a>&#160;<a class=\"footnote-backref\" href=\"https://www.gaborsamu.com/blog/index.xml#fnref:1\">&#x21a9;&#xfe0e;</a></p></li></ol></section>",
            "url": "https://hpc.social/personal-blog/2023/advanced-lsf-resource-connector-configuration-on-ibm-cloud-part-i/",
            
            
            
            
            
            "date_published": "2023-11-08T02:21:04-07:00",
            "date_modified": "2023-11-08T02:21:04-07:00",
            
                "author": "Ramblings of a supercomputing enthusiast."
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2023/lsf-client-on-macos-submitting-from-your-laptop/",
            "title": "LSF client on macOS - submitting from your laptop",
            "summary": null,
            "content_text": "In traditional HPC environments, login nodes are typically used as an access point for users to submitand manage jobs. Although login nodes are still used today, HPC environments areincreasingly being used by a broad class of users with domain expertise and not necessarily IT experts.In other words, such users may be more comfortable using their native desktopenvironment rather than the CLI. Given the factors, in the commercial HPC space, organizations are always lookingfor ways to lower the barto access and interact with HPC environments.Spectrum LSF provides many ways to submit and manage jobs in an HPC cluster. For power users, the richCLI functionality exists. There is also an available web-based interface for jobsubmission and management which provides customizable application templates to greatly simplify job submission, while hiding complexity of the underlying infrastructure. A RESTful APIis also available to users of IBM Spectrum LSF Application Center or IBM Spectrum LSF Suites, which enables organizations to access the HPC environment via web services.I&rsquo;ve written previously in detail about the the LSF web-based interface in the blogThe Easy HPC Button. Here, we&rsquo;ll take a closer look at theavailable LSF client for macOS that uses the RESTful API. First, a bit about LSF clients. LSF clientscan access resources on LSF server hosts without running the LSF daemons. LSF clients don&rsquo;t require a softwarelicense and from clients, users can run all of the familiar LSF commands. Additionally, LSF clients aresubmit only, and don&rsquo;t execute jobs.Note: The macOS LSF client uses the LSF RESTful API. This means that it will function in environmentsrunning LSF Standard Edition with LSF Application Center or LSF Suites.ConfigurationThe configuration used for the example below is as follows:HostnameOSDetailkilencCentOS Stream 8.4LSF Suite for HPC v10.2.0.13My-Macbook-AirmacOS Ventura 13.2.1 (Apple M1)LSF clientOn the Spectrum LSF Suite for HPC management host (kilenc), add the following variables to the Parametersection in the file lsf.cluster.name. The FLOAT_CLIENTS variable determines how many floating clients canjoin the LSF cluster, The FLOAT_CLIENTS_ADDR_RANGE specifies the allowable IP addresses. In this case, theclient system is on a 192.168.x.x network.Begin ParametersFLOAT_CLIENTS=2FLOAT_CLIENTS_ADDR_RANGE=192.*End ParametersTo make the changes take effect, issue the following commands as the LSF administrator:lsadmin reconfigbadmin reconfigObtain the tarball pacdesktop_client10.2.0.13_macos-x86_64.tar. For users with an LSF entitlement this package is available onIBM Fix Central. Note that this package will work on systems with Apple M1 silicon through emulation.Open a Terminal on the macOS client system, copy the tarball to the $HOME/Desktop directory of user lsfuser and uncompress the tarball.lsfuser@My-MacBook-Air Desktop % pwd/Users/lsfuser/Desktoplsfuser@My-MacBook-Air Desktop % ls -la pacdesktop_client10.2.0.13_macos-x86_64.tar-rw-r--r--@ 1 lsfuser  staff  18452480 27 Feb 17:12 pacdesktop_client10.2.0.13_macos-x86_64.tarlsfuser@My-MacBook-Air Desktop % tar -xvf pacdesktop_client10.2.0.13_macos-x86_64.tarx LSF_Desktop_Client/x LSF_Desktop_Client/bappx LSF_Desktop_Client/btopx LSF_Desktop_Client/bwaitx LSF_Desktop_Client/lseligiblex LSF_Desktop_Client/bslax LSF_Desktop_Client/blparamsx LSF_Desktop_Client/bhpartx LSF_Desktop_Client/bclustersx LSF_Desktop_Client/blstartupx LSF_Desktop_Client/lsacctx LSF_Desktop_Client/bsubx LSF_Desktop_Client/bugroupx LSF_Desktop_Client/bpeekx LSF_Desktop_Client/bacctx LSF_Desktop_Client/brequeuex LSF_Desktop_Client/bjgroupx LSF_Desktop_Client/bslotsx LSF_Desktop_Client/lsrunx LSF_Desktop_Client/bjobsx LSF_Desktop_Client/lshostsx LSF_Desktop_Client/lsloadx LSF_Desktop_Client/brlainfox LSF_Desktop_Client/bresourcesx LSF_Desktop_Client/bladminx LSF_Desktop_Client/bstatusx LSF_Desktop_Client/bmodx LSF_Desktop_Client/bpostx LSF_Desktop_Client/lsidx LSF_Desktop_Client/bentagsx LSF_Desktop_Client/chx LSF_Desktop_Client/bchkpntx LSF_Desktop_Client/bparamsx LSF_Desktop_Client/bjdepinfox LSF_Desktop_Client/bgmodx LSF_Desktop_Client/brestartx LSF_Desktop_Client/lsltasksx LSF_Desktop_Client/blusersx LSF_Desktop_Client/paclogonx LSF_Desktop_Client/regnotifyx LSF_Desktop_Client/cacert.pemx LSF_Desktop_Client/bresumex LSF_Desktop_Client/blstatx LSF_Desktop_Client/bhistx LSF_Desktop_Client/bqueuesx LSF_Desktop_Client/bltasksx LSF_Desktop_Client/bresizex LSF_Desktop_Client/blcollectx LSF_Desktop_Client/lsacctmrgx LSF_Desktop_Client/bgaddx LSF_Desktop_Client/bmigx LSF_Desktop_Client/bstopx LSF_Desktop_Client/bswitchx LSF_Desktop_Client/blhostsx LSF_Desktop_Client/blcstatx LSF_Desktop_Client/brsvsx LSF_Desktop_Client/brunx LSF_Desktop_Client/blinfox LSF_Desktop_Client/lsgrunx LSF_Desktop_Client/busersx LSF_Desktop_Client/lsloadadjx LSF_Desktop_Client/blkillx LSF_Desktop_Client/bbotx LSF_Desktop_Client/lsclustersx LSF_Desktop_Client/bconfx LSF_Desktop_Client/lsinfox LSF_Desktop_Client/lsmakex LSF_Desktop_Client/blimitsx LSF_Desktop_Client/bmgroupx LSF_Desktop_Client/breadx LSF_Desktop_Client/bkillx LSF_Desktop_Client/lstcshx LSF_Desktop_Client/lsrtasksx LSF_Desktop_Client/README.TXTx LSF_Desktop_Client/lsplacex LSF_Desktop_Client/bhostsx LSF_Desktop_Client/paclogoutx LSF_Desktop_Client/bgdelFollowing the directions in the file README.TXT, set the environment variable LSF_DESKTOP_CLIENT=yes, and set the PATH variable accordingly.lsfuser@My-MacBook-Air LSF_Desktop_Client % export LSF_DESKTOP_CLIENT=yeslsfuser@My-MacBook-Air LSF_Desktop_Client % export PATH=`pwd`:$PATHNext, it&rsquo;s necessary to run the paclogon command to connect to the LSF Application Center (or LSF Suite installation). Here we point to the LSF server kilenc on port 8080.lsfuser@My-MacBook-Air LSF_Desktop_Client % paclogonLog on to IBM Spectrum LSF Application CenterUser account: lsfuserEnter password: Specify the URL to connect to IBM Spectrum LSF Application Center. Format: http://host_name:port_number/platform or https://host_name:port_number/platformURL: http://kilenc:8080/platformYou have successfully logged on to IBM Spectrum LSF Application Center.After successfully logging in using the paclogon command, it should be possible to run LSF &ldquo;base&rdquo; commands from the macOS terminal including lsid, lsload, lshosts.lsfuser@My-MacBook-Air LSF_Desktop_Client % lsidIBM Spectrum LSF 10.1.0.13, Apr 15 2022Suite Edition: IBM Spectrum LSF Suite for HPC 10.2.0.13Copyright International Business Machines Corp. 1992, 2016.US Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM Corp.My cluster name is KlaszterMy master name is kilenclsfuser@My-MacBook-Air LSF_Desktop_Client % lshosts -wHOST_NAME                       type       model  cpuf ncpus maxmem maxswp server RESOURCESkilenc                    LINUXPPC64LE      POWER9  25.0    32  30.7G  15.8G    Yes (mg docker)lsfuser@My-MacBook-Air LSF_Desktop_Client % lsload -wHOST_NAME               status  r15s   r1m  r15m   ut    pg  ls    it   tmp   swp   memkilenc                      ok   0.8   2.1   2.4   7%   0.0   0  1156  551M 15.6G   10GNext, run the LSF batch commands bqueues and bhosts.lsfuser@My-MacBook-Air LSF_Desktop_Client % bqueuesQUEUE_NAME      PRIO STATUS          MAX JL/U JL/P JL/H NJOBS  PEND   RUN  SUSP admin            50  Open:Active       -    -    -    -     0     0     0     0owners           43  Open:Active       -    -    -    -     0     0     0     0priority         43  Open:Active       -    -    -    - 75835 75803    32     0night            40  Open:Inact        -    -    -    -     0     0     0     0short            35  Open:Active       -    -    -    -     0     0     0     0dataq            33  Open:Active       -    -    -    -     0     0     0     0normal           30  Open:Active       -    -    -    -     0     0     0     0interactive      30  Open:Active       -    -    -    -     0     0     0     0sendq            30  Open:Active       -    -    -    -     0     0     0     0idle             20  Open:Active       -    -    -    -     0     0     0     0lsfuser@My-MacBook-Air LSF_Desktop_Client % bhostsHOST_NAME          STATUS       JL/U    MAX  NJOBS    RUN  SSUSP  USUSP    RSV kilenc             ok              -     32     19     19      0      0      0Running the bjobs will result in a warning message appearing on macOS stating: &ldquo;bjobs&rdquo; cannot be opened because the developer cannot be verified.To remedy the issue observed in step 9, click cancel on the warning message and browse to System Settings -&gt; Privacy &amp; Security -&gt; Security Settings. In the Security Settings view,you&rsquo;ll see the message: &ldquo;bjobs&rdquo; was blocked from use because it is not from an identified developer. To allow the bjobs command to execute, click on the Allow Anyway button. You willthen be promped to authenticate to make the change take effect.Run the LSF bjobs command again. You will now receive a new warning error popup indicating: macOS cannot verify the developer of &ldquo;bjobs&rdquo;. Are you sure you want to open it?. Toproceed, click on the Open button.The bjobs command will then run to completion as expected.  Subsequent executions of bjobs will run without any system warnings. Finally, to submita job, run the bsub command. Here we try to submit a simple sleep job (i.e. bsub -q normal sleep 3600). As was the case with the bjobs command, the bsub command is also blocked. Here,repeat the steps 10, 11 as described above but for the bsub command. Once the steps have been completed, repeat the bsub job submission command.Finally, to submit a job, run the bsub command. Here we try to submit a simple sleep job (i.e. bsub -q normal sleep 3600). As was the case with the bjobs command, the bsubcommand is also blocked. Here, repeat the steps 10, 11 as described above but for the bsub command. Once the steps have been completed, repeat the bsub job submission command.lsfuser@My-MacBook-Air LSF_Desktop_Client % bsub -q normal sleep 3600Job &lt;617551&gt; is submitted to queue &lt;normal&gt;.",
            "content_html": "<p>In traditional HPC environments, login nodes are typically used as an access point for users to submitand manage jobs. Although login nodes are still used today, HPC environments areincreasingly being used by a broad class of users with domain expertise and not necessarily IT experts.In other words, such users may be more comfortable using their native desktopenvironment rather than the CLI. Given the factors, in the commercial HPC space, organizations are always lookingfor ways to lower the barto access and interact with HPC environments.</p><p>Spectrum LSF provides many ways to submit and manage jobs in an HPC cluster. For power users, the richCLI functionality exists. There is also an available web-based interface for jobsubmission and management which provides customizable application templates to greatly simplify job submission, while hiding complexity of the underlying infrastructure. A RESTful APIis also available to users of IBM Spectrum LSF Application Center or IBM Spectrum LSF Suites, which enables organizations to access the HPC environment via web services.</p><p>I&rsquo;ve written previously in detail about the the LSF web-based interface in the blog<a href=\"https://www.gaborsamu.com/blog/easy_hpc/\">The Easy HPC Button</a>. Here, we&rsquo;ll take a closer look at theavailable LSF client for macOS that uses the RESTful API. First, a bit about LSF clients. LSF clientscan access resources on LSF server hosts without running the LSF daemons. LSF clients don&rsquo;t require a softwarelicense and from clients, users can run all of the familiar LSF commands. Additionally, LSF clients aresubmit only, and don&rsquo;t execute jobs.</p><p><strong>Note:</strong> The macOS LSF client uses the LSF RESTful API. This means that it will function in environmentsrunning LSF Standard Edition with LSF Application Center or LSF Suites.</p><p><strong>Configuration</strong></p><p>The configuration used for the example below is as follows:</p><table><thead><tr><th style=\"text-align: left;\">Hostname</th><th>OS</th><th>Detail</th></tr></thead><tbody><tr><td style=\"text-align: left;\"><em>kilenc</em></td><td>CentOS Stream 8.4</td><td>LSF Suite for HPC v10.2.0.13</td></tr><tr><td style=\"text-align: left;\"><em>My-Macbook-Air</em></td><td>macOS Ventura 13.2.1 (Apple M1)</td><td>LSF client</td></tr></tbody></table><ol><li>On the Spectrum LSF Suite for HPC management host (<em>kilenc</em>), add the following variables to the Parametersection in the file lsf.cluster.<em>name</em>. The FLOAT_CLIENTS variable determines how many floating clients canjoin the LSF cluster, The FLOAT_CLIENTS_ADDR_RANGE specifies the allowable IP addresses. In this case, theclient system is on a 192.168.x.x network.</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\">Begin ParametersFLOAT_CLIENTS=2FLOAT_CLIENTS_ADDR_RANGE=192.*End Parameters</code></pre></div><ol start=\"2\"><li>To make the changes take effect, issue the following commands as the LSF administrator:</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\">lsadmin reconfigbadmin reconfig</code></pre></div><ol start=\"3\"><li><p>Obtain the tarball <em>pacdesktop_client10.2.0.13_macos-x86_64.tar</em>. For users with an LSF entitlement this package is available on<a href=\"https://www.ibm.com/support/fixcentral/\">IBM Fix Central</a>. Note that this package will work on systems with Apple M1 silicon through emulation.</p></li><li><p>Open a Terminal on the macOS client system, copy the tarball to the $HOME/Desktop directory of user lsfuser and uncompress the tarball.</p></li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\">lsfuser@My-MacBook-Air Desktop % pwd/Users/lsfuser/Desktoplsfuser@My-MacBook-Air Desktop % ls -la pacdesktop_client10.2.0.13_macos-x86_64.tar-rw-r--r--@ 1 lsfuser  staff  18452480 27 Feb 17:12 pacdesktop_client10.2.0.13_macos-x86_64.tarlsfuser@My-MacBook-Air Desktop % tar -xvf pacdesktop_client10.2.0.13_macos-x86_64.tarx LSF_Desktop_Client/x LSF_Desktop_Client/bappx LSF_Desktop_Client/btopx LSF_Desktop_Client/bwaitx LSF_Desktop_Client/lseligiblex LSF_Desktop_Client/bslax LSF_Desktop_Client/blparamsx LSF_Desktop_Client/bhpartx LSF_Desktop_Client/bclustersx LSF_Desktop_Client/blstartupx LSF_Desktop_Client/lsacctx LSF_Desktop_Client/bsubx LSF_Desktop_Client/bugroupx LSF_Desktop_Client/bpeekx LSF_Desktop_Client/bacctx LSF_Desktop_Client/brequeuex LSF_Desktop_Client/bjgroupx LSF_Desktop_Client/bslotsx LSF_Desktop_Client/lsrunx LSF_Desktop_Client/bjobsx LSF_Desktop_Client/lshostsx LSF_Desktop_Client/lsloadx LSF_Desktop_Client/brlainfox LSF_Desktop_Client/bresourcesx LSF_Desktop_Client/bladminx LSF_Desktop_Client/bstatusx LSF_Desktop_Client/bmodx LSF_Desktop_Client/bpostx LSF_Desktop_Client/lsidx LSF_Desktop_Client/bentagsx LSF_Desktop_Client/chx LSF_Desktop_Client/bchkpntx LSF_Desktop_Client/bparamsx LSF_Desktop_Client/bjdepinfox LSF_Desktop_Client/bgmodx LSF_Desktop_Client/brestartx LSF_Desktop_Client/lsltasksx LSF_Desktop_Client/blusersx LSF_Desktop_Client/paclogonx LSF_Desktop_Client/regnotifyx LSF_Desktop_Client/cacert.pemx LSF_Desktop_Client/bresumex LSF_Desktop_Client/blstatx LSF_Desktop_Client/bhistx LSF_Desktop_Client/bqueuesx LSF_Desktop_Client/bltasksx LSF_Desktop_Client/bresizex LSF_Desktop_Client/blcollectx LSF_Desktop_Client/lsacctmrgx LSF_Desktop_Client/bgaddx LSF_Desktop_Client/bmigx LSF_Desktop_Client/bstopx LSF_Desktop_Client/bswitchx LSF_Desktop_Client/blhostsx LSF_Desktop_Client/blcstatx LSF_Desktop_Client/brsvsx LSF_Desktop_Client/brunx LSF_Desktop_Client/blinfox LSF_Desktop_Client/lsgrunx LSF_Desktop_Client/busersx LSF_Desktop_Client/lsloadadjx LSF_Desktop_Client/blkillx LSF_Desktop_Client/bbotx LSF_Desktop_Client/lsclustersx LSF_Desktop_Client/bconfx LSF_Desktop_Client/lsinfox LSF_Desktop_Client/lsmakex LSF_Desktop_Client/blimitsx LSF_Desktop_Client/bmgroupx LSF_Desktop_Client/breadx LSF_Desktop_Client/bkillx LSF_Desktop_Client/lstcshx LSF_Desktop_Client/lsrtasksx LSF_Desktop_Client/README.TXTx LSF_Desktop_Client/lsplacex LSF_Desktop_Client/bhostsx LSF_Desktop_Client/paclogoutx LSF_Desktop_Client/bgdel</code></pre></div><ol start=\"5\"><li>Following the directions in the file README.TXT, set the environment variable LSF_DESKTOP_CLIENT=yes, and set the PATH variable accordingly.</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\">lsfuser@My-MacBook-Air LSF_Desktop_Client % export LSF_DESKTOP_CLIENT=yeslsfuser@My-MacBook-Air LSF_Desktop_Client % export PATH=`pwd`:$PATH</code></pre></div><ol start=\"6\"><li>Next, it&rsquo;s necessary to run the <em>paclogon</em> command to connect to the LSF Application Center (or LSF Suite installation). Here we point to the LSF server kilenc on port 8080.</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\">lsfuser@My-MacBook-Air LSF_Desktop_Client % paclogonLog on to IBM Spectrum LSF Application CenterUser account: lsfuserEnter password: Specify the URL to connect to IBM Spectrum LSF Application Center. Format: http://host_name:port_number/platform or https://host_name:port_number/platformURL: http://kilenc:8080/platformYou have successfully logged on to IBM Spectrum LSF Application Center.</code></pre></div><ol start=\"7\"><li>After successfully logging in using the paclogon command, it should be possible to run LSF &ldquo;base&rdquo; commands from the macOS terminal including <em>lsid</em>, <em>lsload</em>, <em>lshosts</em>.</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\">lsfuser@My-MacBook-Air LSF_Desktop_Client % lsidIBM Spectrum LSF 10.1.0.13, Apr 15 2022Suite Edition: IBM Spectrum LSF Suite for HPC 10.2.0.13Copyright International Business Machines Corp. 1992, 2016.US Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM Corp.My cluster name is KlaszterMy master name is kilenclsfuser@My-MacBook-Air LSF_Desktop_Client % lshosts -wHOST_NAME                       type       model  cpuf ncpus maxmem maxswp server RESOURCESkilenc                    LINUXPPC64LE      POWER9  25.0    32  30.7G  15.8G    Yes (mg docker)lsfuser@My-MacBook-Air LSF_Desktop_Client % lsload -wHOST_NAME               status  r15s   r1m  r15m   ut    pg  ls    it   tmp   swp   memkilenc                      ok   0.8   2.1   2.4   7%   0.0   0  1156  551M 15.6G   10G</code></pre></div><ol start=\"8\"><li>Next, run the LSF batch commands <em>bqueues</em> and <em>bhosts</em>.</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\">lsfuser@My-MacBook-Air LSF_Desktop_Client % bqueuesQUEUE_NAME      PRIO STATUS          MAX JL/U JL/P JL/H NJOBS  PEND   RUN  SUSP admin            50  Open:Active       -    -    -    -     0     0     0     0owners           43  Open:Active       -    -    -    -     0     0     0     0priority         43  Open:Active       -    -    -    - 75835 75803    32     0night            40  Open:Inact        -    -    -    -     0     0     0     0short            35  Open:Active       -    -    -    -     0     0     0     0dataq            33  Open:Active       -    -    -    -     0     0     0     0normal           30  Open:Active       -    -    -    -     0     0     0     0interactive      30  Open:Active       -    -    -    -     0     0     0     0sendq            30  Open:Active       -    -    -    -     0     0     0     0idle             20  Open:Active       -    -    -    -     0     0     0     0lsfuser@My-MacBook-Air LSF_Desktop_Client % bhostsHOST_NAME          STATUS       JL/U    MAX  NJOBS    RUN  SSUSP  USUSP    RSV kilenc             ok              -     32     19     19      0      0      0</code></pre></div><ol start=\"9\"><li>Running the bjobs will result in a warning message appearing on macOS stating: <em>&ldquo;bjobs&rdquo; cannot be opened because the developer cannot be verified.</em></li></ol><figure><img src=\"https://www.gaborsamu.com/images/bjobs_unverified.png\" /></figure><ol start=\"10\"><li>To remedy the issue observed in step 9, click cancel on the warning message and browse to <strong>System Settings -&gt; Privacy &amp; Security -&gt; Security Settings</strong>. In the Security Settings view,you&rsquo;ll see the message: <em>&ldquo;bjobs&rdquo; was blocked from use because it is not from an identified developer.</em> To allow the bjobs command to execute, click on the <strong>Allow Anyway</strong> button. You willthen be promped to authenticate to make the change take effect.</li></ol><p><figure><img src=\"https://www.gaborsamu.com/images/bjobs_allow.png\" /></figure><figure><img src=\"https://www.gaborsamu.com/images/bjobs_authenticate.png\" /></figure></p><ol start=\"11\"><li>Run the LSF <em>bjobs</em> command again. You will now receive a new warning error popup indicating: <em>macOS cannot verify the developer of &ldquo;bjobs&rdquo;. Are you sure you want to open it?</em>. Toproceed, click on the Open button.The bjobs command will then run to completion as expected.  Subsequent executions of bjobs will run without any system warnings. Finally, to submita job, run the bsub command. Here we try to submit a simple sleep job (i.e. bsub -q normal sleep 3600). As was the case with the bjobs command, the bsub command is also blocked. Here,repeat the steps 10, 11 as described above but for the bsub command. Once the steps have been completed, repeat the bsub job submission command.</li></ol><figure><img src=\"https://www.gaborsamu.com/images/bjobs_open.png\" /></figure><ol start=\"12\"><li>Finally, to submit a job, run the <em>bsub</em> command. Here we try to submit a simple sleep job (i.e. <em>bsub -q normal sleep 3600</em>). As was the case with the <em>bjobs</em> command, the <em>bsub</em>command is also blocked. Here, repeat the steps 10, 11 as described above but for the <em>bsub</em> command. Once the steps have been completed, repeat the <em>bsub</em> job submission command.</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\">lsfuser@My-MacBook-Air LSF_Desktop_Client % bsub -q normal sleep 3600Job &lt;617551&gt; is submitted to queue &lt;normal&gt;.</code></pre></div>",
            "url": "https://hpc.social/personal-blog/2023/lsf-client-on-macos-submitting-from-your-laptop/",
            
            
            
            
            
            "date_published": "2023-03-01T19:10:58-07:00",
            "date_modified": "2023-03-01T19:10:58-07:00",
            
                "author": "Ramblings of a supercomputing enthusiast."
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2023/monitoring-ibm-spectrum-lsf-with-the-tig-stack/",
            "title": "Monitoring .-.. ... ..-. (IBM Spectrum LSF) with the TIG stack",
            "summary": null,
            "content_text": "Much like dashboards in automobiles, dashboards in the context of HPC infrastructure are crucial to get an understanding of what&rsquo;s happening under the hood of your HPC cluster - ata glance. During my IT career, I&rsquo;ve used a myriad of monitoring solutions ranging from SNMP and Ganglia, to the ELK (Elasticsearch, Logstash, Kibana) stack. For example, I&rsquo;ve recentlywritten an overview on how it is possible to visualize IBM Spectrum LSF (LSF) data in Grafana. LSF is an HPC job scheduler which brings to the table three decades of experience inworkload and resource management.For this blog, I decided to take this to the next level by monitoring IBM Spectrum LSF with the well known TIG (Telegraf, InfluxDB, Grafana) stack. This article is not meant to be adebate on the advantages of one monitoring stack over another. Rather, the focus is to demonstrate what is feasible in terms of monitoring Spectrum LSF clusters with the TIG stack,given the many available ways to query LSF for key information using CLI commands.The JourneyThere already exists many write-ups on how to deploy the TIG stack to monitor systems. This isn&rsquo;t meant to be a guide on setting up the TIG stack. Rather, it&rsquo;s assumed that the readeralready has some familiarity with the TIG stack. If not, then [insert your favourite search engine] is your friend.On my home network, I decided to setup a VM running on my trusty Traverse Ten64 running Fedora where InfluxDB was installed. The idea was to run InfluxDB on a system that is guaranteedto be always on in my home environment and that is energy efficient. Installing telegraf on all of the LSF cluster servers (x3) proved to be straight forward. Note that in all cases, I used the OSsupplied versions of InfluxDB, Telegraf. Finally, I already had a Grafana server running on a server in my network.Out of the box, Telegraf has the ability to monitor numerous system metrics. Furthermore, there exists literally hundreds of plugins for Telegraf to monitor a wide variety of devices,services and software. A search however, didn&rsquo;t reveal the existence of any plugin to monitor LSF. So it was time to get creative.What to monitor?A bit of research revealed that InfluxDB supports what is known as &ldquo;line protocol&rdquo;. This is a well defined text-based format for writing data to InfluxDB. I used the followingreference on &ldquo;line protocol&rdquo; to guide me. Using line protocol it would be ultimately possible towrite a plugin for Telegraf to effecively scrape information from Spectrum LSF and output in line protocol format for writing to InfluxDB.Before I could begin writing the plugin, the key was to determine what information from Spectrum LSF would be useful to display in the dashboard, and how that information could beextracted. For this I followed the KISS principle to keep things as simple as possible. The key metrics I decided to report on were servers, queues and jobs (oh my!), as well as processinformation for the LSF scheduler daemons. Refer to the following table for details:Metric(s)CommandLSF scheduler performance metricsbadmin perfmon view -jsonLSF available servers, CPUs, cores, slotsbadmin showstatusLSF server by status (total number Ok, closed, unreachable, unavailable)badmin showstatusLSF job statistics (total number running, suspended, pending)badmin showstatusLSF queue statistics (per queue, total number of jobs running, suspended, pending)bqueues -json -o queue_name:12 njobs pend run susp rsv ususp ssuspLSF mbatchd process metrics(Telegraf - inputs.procstat)LSF mbschd process metrics(Telegraf - inputs.procstat)LSF management lim process metrics(Telegraf - inputs.procstat)Scrapin' funThese above metrics would give a good idea of the state of the Spectrum LSF cluster at a glance. With the list of metrics prepared, the next step was to create a plugin script which wouldscrape data from the noted commands. Both bqueues and badmin perfmon view support output in JSON format with the appropriate flags specified. However, badmin showstatus does not supportoutput in JSON format. This meant that for badmin showstatus it was necessary to scrape data assuming hard coded field positions in the output.A copy of the Telegraf plugin for Spectrum LSF is provided below. This is just an example and is provided &ldquo;as is&rdquo; for testing purposes. Your mileage may vary.  Example lsf_telegraf_agent.py script. Click to expand!  #!/usr/bin/python3.8# # v0.9 # Sample inputs.exec script for Telegraf which outputs metrics from an IBM Spectrum LSF management server# in InfluxDB Line Protocol input format.## NOTE: It is required to set the lsf_envfile variable to point to the LSF profile.lsf file# for the LSF installation. ## Gabor Samu# January 4, 2023# import osimport jsonimport timeimport subprocessimport sysfrom pathlib import Path## Variable declarations# **NOTE: lsf_envfile needs to be set to point to the profile.lsf file for the LSF installation. #lsf_envfile = \"/opt/ibm/lsfsuite/lsf/conf/profile.lsf\"## Source the Spectrum LSF profile.  # Check for existing of lsf_envfile (profile.lsf) and source the environment. # If the specified file does not exist, then exit.  #path = Path(lsf_envfile)if path.is_file():     lsf_env = (f'env -i sh -c \"source {lsf_envfile} &amp;&amp; env\"')    for line in subprocess.getoutput(lsf_env).split(\"\\n\"):        key, value = line.split(\"=\")        os.environ[key]= valueelse:    sys.exit(f'The file {lsf_envfile} does not exist.')    # # Get the time in nanoseconds since the epoch. # This is required as part of the InfluxDB line protocol reference. # Only supported on Python 3.7+#time_nanosec = time.time_ns()## Here we set the LSF environment variable LSB_NTRIES. This will be used to determine the # number of retries before failure of a LSF batch command. This is used to cover the case # when the LSF mbatchd is not running. #os.environ[\"LSB_NTRIES\"] = \"2\"## Check if LSF performance metric monitoring is enabled. This is done by running# 'badmin perfmon view'. If badmin is not found, then exit. ## Check the return status from 'badmin perfmon view' and take the appropriate action:#  - If return status is 7, it means that performance monitoring is not enabled. The script#    will enable LSF performance metric monitoring by running 'badmin perfmon start'.#    Note that a 70 second sleep is required before LSF metrics will be available.  #  - If return status is 65, it means that the badmin command reported that the#    LSF batch system is down. This is a fatal error which will cause the script#    to exit. #lsf_path = os.environ['LSF_BINDIR']badmin_path = lsf_path + \"/badmin\"bqueues_path = lsf_path + \"/bqueues\"path = Path(badmin_path)if path.is_file():    cmd = [badmin_path, 'perfmon', 'view']    p = subprocess.Popen(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)    while p.poll() is None:        time.sleep(0.1)    return_code = p.returncode    if return_code == 7:        cmd = [badmin_path, 'perfmon', 'start']        p = subprocess.Popen(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)        while p.poll() is None:            time.sleep(0.1)        return_code = p.returncode        time.sleep(70)    elif return_code == 65:        sys.exit(f'The LSF batch system is down.')else:    sys.exit(f'{badmin_path} does not exist.')## Run badmin with the \"perfmon view\" keywords and the -json option to product JSON output# We assume here that the LSF batch system is responsive (a check was done above); if# the mbatchd is very busy there is a possiblity that it may not be responsive here. This# case is not considered; LSB_NTRIES setting will determine how many tries are made before# badmin gives up the ghost.  # # Note: We previously checked for the existence of the 'badmin' binary. #cmd = [badmin_path, 'perfmon', 'view', '-json'] p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL, text=True) stdout, stderr = p.communicate()## Guard for the case that the performance monitor has just been enabled, but is not# producing any data as the first sample period has not elapsed. #if stdout == \"\":    sys.exit(f'Output from badmin perfmon view -json is empty.')else:     data = json.loads(stdout)# # Run badmin showstatus# Next, run the command 'badmin showstatus' and capture the output. Note that badmin showstatus# does not produce JSON output. So here we must do some scraping of the output. # The output from 'badmin showstatus' it placed into the array 'showstatus'. The hard coded# positions in the output of 'badmin showstatus' are assumed when building the output # strings below. Should the format of the output of 'badmin showstatus' change, this will# need to be updated. cmd = [badmin_path, 'showstatus']p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL, text=True)stdout, stderr = p.communicate()# Convert badmin showstatus output into an arrayshowstatus = stdout.split()## Run bqueues#cmd = [bqueues_path, '-json', '-o', 'queue_name:12 njobs pend run susp rsv ususp ssusp']p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL, text=True)stdout, stderr = p.communicate()data_queues = json.loads(stdout)## At this stage, we've captured the output from 'badmin perfmon view -json' and # 'badmin showstatus'. We're now ready to print to standard output the metric# strings in InfluxDB line procotol format. ## Details about the line protocol format can be found here:# https://docs.influxdata.com/influxdb/v2.6/reference/syntax/line-protocol/# # ## LSF server status#print(\"lsf_servers,\",\"status=total\",\" value=\",showstatus[21],\"i \",time_nanosec,sep='')print(\"lsf_servers,\",\"status=ok\",\" value=\",showstatus[23],\"i \",time_nanosec,sep='')print(\"lsf_servers,\",\"status=closed\",\" value=\",showstatus[25],\"i \",time_nanosec,sep='')print(\"lsf_servers,\",\"status=unreachable\",\" value=\",showstatus[27],\"i \",time_nanosec,sep='')print(\"lsf_servers,\",\"status=unavailable\",\" value=\",showstatus[29],\"i \",time_nanosec,sep='')## LSF job status#print(\"lsf_jobs,\",\"state=total\",\" value=\",showstatus[33],\"i \",time_nanosec,sep='')print(\"lsf_jobs,\",\"state=running\",\" value=\",showstatus[35],\"i \",time_nanosec,sep='')print(\"lsf_jobs,\",\"state=suspended\",\" value=\",showstatus[37],\"i \",time_nanosec,sep='')print(\"lsf_jobs,\",\"state=pending\",\" value=\",showstatus[39],\"i \",time_nanosec,sep='')print(\"lsf_jobs,\",\"state=finished\",\" value=\",showstatus[41],\"i \",time_nanosec,sep='')## LSF user stats#print(\"lsf_users,\",\"state=numusers\",\" value=\",showstatus[45],\"i \",time_nanosec,sep='')print(\"lsf_users,\",\"state=numgroups\",\" value=\",showstatus[50],\"i \",time_nanosec,sep='')print(\"lsf_users,\",\"state=numactive\",\" value=\",showstatus[55],\"i \",time_nanosec,sep='')## LSF hosts stats# First we split out the current and peak values for clients, servers, cpus, cores, and slots.# The current and peak values are separated by the \"/\" delimiter.# clientssplit = showstatus[9].split(\"/\")serverssplit = showstatus[11].split(\"/\")cpussplit = showstatus[13].split(\"/\")coressplit = showstatus[15].split(\"/\")slotssplit = showstatus[17].split(\"/\")print(\"lsf_hosts,\",\"state=clients\",\" current=\",clientssplit[0],\"i,\",\"peak=\",clientssplit[1],\"i \",time_nanosec,sep='')print(\"lsf_hosts,\",\"state=servers\",\" current=\",serverssplit[0],\"i,\",\"peak=\",serverssplit[1],\"i \",time_nanosec,sep='')print(\"lsf_hosts,\",\"state=cpus\",\" current=\",cpussplit[0],\"i,\",\"peak=\",cpussplit[1],\"i \",time_nanosec,sep='')print(\"lsf_hosts,\",\"state=cores\",\" current=\",coressplit[0],\"i,\",\"peak=\",coressplit[1],\"i \",time_nanosec,sep='')print(\"lsf_hosts,\",\"state=slots\",\" current=\",slotssplit[0],\"i,\",\"peak=\",slotssplit[1],\"i \",time_nanosec,sep='')## Print mbatchd query metrics#print(\"lsf_mbatchd,\",\"query=job\",\" value=\",data['record'][1]['current'],\"i \",time_nanosec,sep='')print(\"lsf_mbatchd,\",\"query=host\",\" value=\",data['record'][2]['current'],\"i \",time_nanosec,sep='')print(\"lsf_mbatchd,\",\"query=queue\",\" value=\",data['record'][3]['current'],\"i \",time_nanosec,sep='')## Print mbatchd job metrics#print(\"lsf_mbatchd,\",\"jobs=submitreqs\",\" value=\",data['record'][4]['current'],\"i \",time_nanosec,sep='')print(\"lsf_mbatchd,\",\"jobs=submitted\",\" value=\",data['record'][5]['current'],\"i \",time_nanosec,sep='')print(\"lsf_mbatchd,\",\"jobs=dispatched\",\" value=\",data['record'][6]['current'],\"i \",time_nanosec,sep='')print(\"lsf_mbatchd,\",\"jobs=completed\",\" value=\",data['record'][7]['current'],\"i \",time_nanosec,sep='')print(\"lsf_mbatchd,\",\"jobs=sentremote\",\" value=\",data['record'][8]['current'],\"i \",time_nanosec,sep='')print(\"lsf_mbatchd,\",\"jobs=acceptremote\",\" value=\",data['record'][9]['current'],\"i \",time_nanosec,sep='')print(\"lsf_mbatchd,\",\"sched=interval\",\" value=\",data['record'][10]['current'],\"i \",time_nanosec,sep='')print(\"lsf_mbatchd,\",\"sched=matchhost\",\" value=\",data['record'][11]['current'],\"i \",time_nanosec,sep='')print(\"lsf_mbatchd,\",\"sched=buckets\",\" value=\",data['record'][12]['current'],\"i \",time_nanosec,sep='')print(\"lsf_mbatchd,\",\"sched=reordered\",\" value=\",data['record'][13]['current'],\"i \",time_nanosec,sep='')## Print mbatchd efficiency metrics. Here check if the efficiency metric indicated is \"-\". If so, # then assume a zero value. The trailing \"%\" sign on the metrics (percentages) is also stripped here. #slots = (data['record'][14]['current'])slots_percent = slotsif slots_percent == \"-\":    slots_percent = \"0\"elif slots_percent != \"0\":    # Strip % sign and decimal. This is to work around issue inserting float to InfluxDB    # \"type float, already exists as type integer dropped ...\"    slots_percent = slots[:-4]memory = (data['record'][15]['current'])memory_percent = memoryif memory_percent == \"-\":    memory_percent = \"0\"elif memory_percent != \"0\":    # Strip % sign and decimal. This is to work around issue inserting float to InfluxDB    # \"type float, already exists as type integer dropped ...\"    memory_percent = memory[:-4]print(\"lsf_mbatchd,\",\"utilization=slots\",\" value=\",slots_percent,\"i \",time_nanosec,sep='')print(\"lsf_mbatchd,\",\"utilization=memory\",\" value=\",memory_percent,\"i \",time_nanosec,sep='')## Print mbatchd file descriptor usage#print(\"lsf_mbatchd,\",\"fd=free\",\" value=\",data['fd']['free'],\"i \",time_nanosec,sep='')print(\"lsf_mbatchd,\",\"fd=used\",\" value=\",data['fd']['used'],\"i \",time_nanosec,sep='')print(\"lsf_mbatchd,\",\"fd=total\",\" value=\",data['fd']['total'],\"i \",time_nanosec,sep='')## Print LSF queue status (njobs)#iterations = data_queues[\"QUEUES\"]for n in range(iterations):    print(\"lsf_queues,\",\"name=\", data_queues['RECORDS'][n]['QUEUE_NAME'], \" njobs=\", data_queues['RECORDS'][n]['NJOBS'],\"i,\",          \"pend=\", data_queues['RECORDS'][n]['PEND'],\"i,\",          \"run=\", data_queues['RECORDS'][n]['RUN'],\"i,\",          \"susp=\", data_queues['RECORDS'][n]['SUSP'],\"i,\",          \"rsv=\", data_queues['RECORDS'][n]['RSV'],\"i,\",          \"ususp=\", data_queues['RECORDS'][n]['USUSP'],\"i,\",          \"ssusp=\", data_queues['RECORDS'][n]['SSUSP'],\"i \",          time_nanosec, sep='')exit()    Bringing it all togetherFor completeness, below is the detail regarding the configuration of the environment. It should be noted that the simple test environment consists of a single server running IBMSpectrum LSF Suite for HPC and a separate server which runs the InfluxDB instance.HostnameComponentVersionkilencOS (LSF mgmt server)CentOS Stream release 8 (ppc64le)kilencSpectrum LSF Suite for HPCv10.2.0.13adatbazisOS (InfluxDB server)Fedora release 36 (aarch64)adatbazisInfluxDBv1.8.10kilencTelegrafv1.24.3kilencGrafanav9.1.6The follwing steps assume that IBM Spectrum LSF Suite for HPC, InfluxDB and Telegraf have been installed.Start InfluxDB on the host adatbazisOn the LSF management server kilenc, configure telegraf to connect to the influxDB instance on host adatbazis. Edit the configuration /etc/telegraf/telegraf.conf and specifythe correct URL in the outputs.influxdb section as follows:# # Configuration for sending metrics to InfluxDB[[outputs.influxdb]]#   ## The full HTTP or UDP URL for your InfluxDB instance.#   ###   ## Multiple URLs can be specified for a single cluster, only ONE of the#   ## urls will be written to each interval.#   # urls = [\"unix:///var/run/influxdb.sock\"]#   # urls = [\"udp://127.0.0.1:8089\"]#   # urls = [\"http://127.0.0.1:8086\"]# Added gsamu Jan 04 2023urls = [\"http://adatbazis:8086\"]On the LSF management server kilenc, configure telegraf with the custom plugin script lsf_telegraf_agent_0.9.py to collect and log metrics from IBM Spectrum LSF Suite for HPC.Edit the configuration /etc/telegraf/telegraf.conf and specify the correct command path in the section inputs.exec. Additionally, set data_format equal to influx.Note that thescript lsf_telegraf_agent_0.9.py was copied to the directory /etc/telegraf/telegraf.d/scripts with permissions octal 755 and owner set to user telegraf.Note: User telegraf was automatically created during the installation of telegraf. # ## Gather LSF metrics[[inputs.exec]]  ## Commands array   commands = [  \"/etc/telegraf/telegraf.d/scripts/lsf_telegraf_agent_0.9.py\" ]   timeout = \"30s\"   interval = \"30s\"   data_format = \"influx\" # ## End LSF metricsTelegraf provides the ability to collect metrics on processes. Here we&rsquo;ll use the telegraf procstat facility to monitor the LSF mbatchd and mbschd processes. These are the keydaemons involved in handling query requests and making scheduling decisions for jobs in the environment. Edit the configuration /etc/telegraf/telegraf.conf and configure the twofollowing inputs.procstat sections.# ## Monitor CPU and memory utilization for LSF processes# ## mbatchd, mbschd, lim (manager)[[inputs.procstat]]exe = \"lim\"pattern = \"lim\"pid_finder = \"pgrep\"[[inputs.procstat]]exe = \"mbschd\"pattern = \"mbschd\"pid_finder = \"pgrep\"[[inputs.procstat]]exe = \"mbatchd\"pattern = \"mbatchd\"pid_finder = \"pgrep\"With the configuration to telegraf complete, it&rsquo;s now time to test if the configuration and custom LSF agent is functioning as expected. Note that the following operation is performedon the LSF management candidate host kilenc and assumes that the LSF daemons are up and running. This is achieve by running the command:telegraf &ndash;config /etc/telegraf/telegraf.conf &ndash;test. Note: Any errors in the configuration file /etc/telegraf/telegraf.conf will result in errors in the output.  Output of telegraf &ndash;config /etc/telegraf/telegraf.conf &ndash;test. Click to expand!  [root@kilenc telegraf]# pwd/etc/telegraf[root@kilenc telegraf]# telegraf --config /etc/telegraf/telegraf.conf --test&gt; mem,host=kilenc active=1938817024i,available=6820003840i,available_percent=20.653390597462806,buffered=4849664i,cached=6317735936i,commit_limit=33560395776i,committed_as=18635292672i,dirty=4128768i,free=2623799296i,high_free=0i,high_total=0i,huge_page_size=2097152i,huge_pages_free=0i,huge_pages_total=0i,inactive=13852016640i,low_free=0i,low_total=0i,mapped=1007353856i,page_tables=22478848i,shared=259063808i,slab=4946919424i,sreclaimable=902234112i,sunreclaim=4044685312i,swap_cached=3866624i,swap_free=16994729984i,swap_total=17049780224i,total=33021231104i,used=24074846208i,used_percent=72.90717336424115,vmalloc_chunk=0i,vmalloc_total=562949953421312i,vmalloc_used=0i,write_back=0i,write_back_tmp=0i 1674246976000000000&gt; kernel,host=kilenc boot_time=1673790850i,context_switches=1943864437i,entropy_avail=4037i,interrupts=1294179599i,processes_forked=4255316i 1674246976000000000&gt; swap,host=kilenc free=16994729984i,total=17049780224i,used=55050240i,used_percent=0.3228794698626609 1674246976000000000&gt; swap,host=kilenc in=172032i,out=851968i 1674246976000000000&gt; net,host=kilenc,interface=lo bytes_recv=90039931116i,bytes_sent=90039931116i,drop_in=0i,drop_out=0i,err_in=0i,err_out=0i,packets_recv=17245997i,packets_sent=17245997i 1674246976000000000&gt; net,host=kilenc,interface=enP4p1s0f0 bytes_recv=0i,bytes_sent=0i,drop_in=0i,drop_out=0i,err_in=0i,err_out=0i,packets_recv=0i,packets_sent=0i 1674246976000000000&gt; net,host=kilenc,interface=enP4p1s0f1 bytes_recv=11791041280i,bytes_sent=1701152001i,drop_in=0i,drop_out=0i,err_in=0i,err_out=0i,packets_recv=10322276i,packets_sent=4594948i 1674246976000000000&gt; net,host=kilenc,interface=all icmp_inaddrmaskreps=0i,icmp_inaddrmasks=0i,icmp_incsumerrors=0i,icmp_indestunreachs=8609i,icmp_inechoreps=20i,icmp_inechos=11i,icmp_inerrors=1084i,icmp_inmsgs=8640i,icmp_inparmprobs=0i,icmp_inredirects=0i,icmp_insrcquenchs=0i,icmp_intimeexcds=0i,icmp_intimestampreps=0i,icmp_intimestamps=0i,icmp_outaddrmaskreps=0i,icmp_outaddrmasks=0i,icmp_outdestunreachs=4805i,icmp_outechoreps=11i,icmp_outechos=94i,icmp_outerrors=0i,icmp_outmsgs=4910i,icmp_outparmprobs=0i,icmp_outredirects=0i,icmp_outsrcquenchs=0i,icmp_outtimeexcds=0i,icmp_outtimestampreps=0i,icmp_outtimestamps=0i,icmpmsg_intype0=20i,icmpmsg_intype3=8609i,icmpmsg_intype8=11i,icmpmsg_outtype0=11i,icmpmsg_outtype3=4805i,icmpmsg_outtype8=94i,ip_defaultttl=64i,ip_forwarding=1i,ip_forwdatagrams=0i,ip_fragcreates=62958i,ip_fragfails=0i,ip_fragoks=12611i,ip_inaddrerrors=1i,ip_indelivers=21324370i,ip_indiscards=0i,ip_inhdrerrors=0i,ip_inreceives=21324371i,ip_inunknownprotos=0i,ip_outdiscards=0i,ip_outnoroutes=30i,ip_outrequests=21248264i,ip_reasmfails=0i,ip_reasmoks=0i,ip_reasmreqds=0i,ip_reasmtimeout=0i,tcp_activeopens=763497i,tcp_attemptfails=96617i,tcp_currestab=118i,tcp_estabresets=1917i,tcp_incsumerrors=0i,tcp_inerrs=0i,tcp_insegs=19488475i,tcp_maxconn=-1i,tcp_outrsts=137188i,tcp_outsegs=20220038i,tcp_passiveopens=675805i,tcp_retranssegs=9827i,tcp_rtoalgorithm=1i,tcp_rtomax=120000i,tcp_rtomin=200i,udp_ignoredmulti=10509i,udp_incsumerrors=0i,udp_indatagrams=1816997i,udp_inerrors=0i,udp_memerrors=0i,udp_noports=264i,udp_outdatagrams=1506724i,udp_rcvbuferrors=0i,udp_sndbuferrors=0i,udplite_ignoredmulti=0i,udplite_incsumerrors=0i,udplite_indatagrams=0i,udplite_inerrors=0i,udplite_memerrors=0i,udplite_noports=0i,udplite_outdatagrams=0i,udplite_rcvbuferrors=0i,udplite_sndbuferrors=0i 1674246976000000000&gt; diskio,host=kilenc,name=dm-2 io_time=9739370i,iops_in_progress=0i,merged_reads=0i,merged_writes=0i,read_bytes=4015612416i,read_time=604060i,reads=40592i,weighted_io_time=60563370i,write_bytes=47025459712i,write_time=59959310i,writes=1079691i 1674246976000000000&gt; diskio,host=kilenc,name=sda1 io_time=1460i,iops_in_progress=0i,merged_reads=0i,merged_writes=0i,read_bytes=4849664i,read_time=1304i,reads=67i,weighted_io_time=1304i,write_bytes=0i,write_time=0i,writes=0i 1674246976000000000&gt; diskio,host=kilenc,name=sda3 io_time=45872430i,iops_in_progress=0i,merged_reads=623i,merged_writes=1061314i,read_bytes=16398521856i,read_time=3371612i,reads=139298i,weighted_io_time=311521720i,write_bytes=133715422208i,write_time=308150107i,writes=7031512i 1674246976000000000&gt; diskio,host=kilenc,name=dm-1 io_time=5780i,iops_in_progress=0i,merged_reads=0i,merged_writes=0i,read_bytes=5636096i,read_time=3030i,reads=81i,weighted_io_time=26500i,write_bytes=13631488i,write_time=23470i,writes=208i 1674246976000000000&gt; disk,device=dm-0,fstype=xfs,host=kilenc,mode=rw,path=/ free=9315028992i,inodes_free=18214222i,inodes_total=19822888i,inodes_used=1608666i,total=53660876800i,used=44345847808i,used_percent=82.64093032486566 1674246976000000000&gt; disk,device=sda2,fstype=ext4,host=kilenc,mode=rw,path=/boot free=309653504i,inodes_free=65264i,inodes_total=65536i,inodes_used=272i,total=1020702720i,used=640585728i,used_percent=67.41310045173972 1674246976000000000&gt; disk,device=dm-2,fstype=xfs,host=kilenc,mode=rw,path=/home free=856442515456i,inodes_free=452529686i,inodes_total=453312512i,inodes_used=782826i,total=927930712064i,used=71488196608i,used_percent=7.704044674735306 1674246976000000000&gt; disk,device=dm-2,fstype=xfs,host=kilenc,mode=rw,path=/home/opt/at13.0/lib free=856442515456i,inodes_free=452529686i,inodes_total=453312512i,inodes_used=782826i,total=927930712064i,used=71488196608i,used_percent=7.704044674735306 1674246976000000000&gt; disk,device=dm-2,fstype=xfs,host=kilenc,mode=rw,path=/home/opt/at13.0/lib64 free=856442515456i,inodes_free=452529686i,inodes_total=453312512i,inodes_used=782826i,total=927930712064i,used=71488196608i,used_percent=7.704044674735306 1674246976000000000&gt; disk,device=ST31000524AS/raktar,fstype=zfs,host=kilenc,mode=rw,path=/mnt/ST31000524AS free=210837438464i,inodes_free=411792117i,inodes_total=412304487i,inodes_used=512370i,total=965496143872i,used=754658705408i,used_percent=78.16278813725106 1674246976000000000&gt; diskio,host=kilenc,name=sda io_time=45899860i,iops_in_progress=0i,merged_reads=650i,merged_writes=1061332i,read_bytes=16495536128i,read_time=3440899i,reads=141325i,weighted_io_time=311596362i,write_bytes=133715696640i,write_time=308155462i,writes=7031531i 1674246976000000000&gt; disk,device=ST31000524AS,fstype=zfs,host=kilenc,mode=rw,path=/ST31000524AS free=210837438464i,inodes_free=411792117i,inodes_total=411792123i,inodes_used=6i,total=210837569536i,used=131072i,used_percent=0.00006216728844316324 1674246976000000000&gt; diskio,host=kilenc,name=sda2 io_time=18060i,iops_in_progress=0i,merged_reads=27i,merged_writes=18i,read_bytes=88372224i,read_time=31224i,reads=436i,weighted_io_time=36579i,write_bytes=274432i,write_time=5355i,writes=19i 1674246976000000000&gt; diskio,host=kilenc,name=dm-0 io_time=38788720i,iops_in_progress=0i,merged_reads=0i,merged_writes=0i,read_bytes=12341294080i,read_time=1143210i,reads=51814i,weighted_io_time=303329620i,write_bytes=86676331008i,write_time=302186410i,writes=6798400i 1674246976000000000&gt; diskio,host=kilenc,name=sdb io_time=668810i,iops_in_progress=0i,merged_reads=9i,merged_writes=58i,read_bytes=104550912i,read_time=746540i,reads=31054i,weighted_io_time=1445858i,write_bytes=10845920256i,write_time=699318i,writes=124780i 1674246976000000000&gt; diskio,host=kilenc,name=sdb1 io_time=341330i,iops_in_progress=0i,merged_reads=0i,merged_writes=58i,read_bytes=95562240i,read_time=383066i,reads=25026i,weighted_io_time=1082385i,write_bytes=10845920256i,write_time=699318i,writes=124780i 1674246976000000000&gt; diskio,host=kilenc,name=sdb9 io_time=190i,iops_in_progress=0i,merged_reads=0i,merged_writes=0i,read_bytes=4980736i,read_time=37i,reads=69i,weighted_io_time=37i,write_bytes=0i,write_time=0i,writes=0i 1674246976000000000&gt; system,host=kilenc load1=2.06,load15=2.12,load5=2.12,n_cpus=32i,n_users=0i 1674246976000000000&gt; system,host=kilenc uptime=456127i 1674246976000000000&gt; system,host=kilenc uptime_format=\"5 days,  6:42\" 1674246976000000000&gt; processes,host=kilenc blocked=1i,dead=0i,idle=569i,paging=0i,parked=1i,running=0i,sleeping=412i,stopped=0i,total=1366i,total_threads=2683i,unknown=0i,zombies=0i 1674246976000000000&gt; lsf_servers,host=kilenc,status=total value=1i 1674246976000000000&gt; lsf_servers,host=kilenc,status=ok value=1i 1674246976000000000&gt; lsf_servers,host=kilenc,status=closed value=0i 1674246976000000000&gt; lsf_servers,host=kilenc,status=unreachable value=0i 1674246976000000000&gt; lsf_servers,host=kilenc,status=unavailable value=0i 1674246976000000000&gt; lsf_jobs,host=kilenc,state=total value=121776i 1674246976000000000&gt; lsf_jobs,host=kilenc,state=running value=32i 1674246976000000000&gt; lsf_jobs,host=kilenc,state=suspended value=0i 1674246976000000000&gt; lsf_jobs,host=kilenc,state=pending value=120771i 1674246976000000000&gt; lsf_jobs,host=kilenc,state=finished value=973i 1674246976000000000&gt; lsf_users,host=kilenc,state=numusers value=4i 1674246976000000000&gt; lsf_users,host=kilenc,state=numgroups value=1i 1674246976000000000&gt; lsf_users,host=kilenc,state=numactive value=1i 1674246976000000000&gt; lsf_hosts,host=kilenc,state=clients current=0i,peak=0i 1674246976000000000&gt; lsf_hosts,host=kilenc,state=servers current=1i,peak=1i 1674246976000000000&gt; lsf_hosts,host=kilenc,state=cpus current=2i,peak=2i 1674246976000000000&gt; lsf_hosts,host=kilenc,state=cores current=32i,peak=32i 1674246976000000000&gt; lsf_hosts,host=kilenc,state=slots current=32i,peak=32i 1674246976000000000&gt; lsf_mbatchd,host=kilenc,query=job value=0i 1674246976000000000&gt; lsf_mbatchd,host=kilenc,query=host value=0i 1674246976000000000&gt; lsf_mbatchd,host=kilenc,query=queue value=2i 1674246976000000000&gt; lsf_mbatchd,host=kilenc,jobs=submitreqs value=0i 1674246976000000000&gt; lsf_mbatchd,host=kilenc,jobs=submitted value=0i 1674246976000000000&gt; lsf_mbatchd,host=kilenc,jobs=dispatched value=19i 1674246976000000000&gt; lsf_mbatchd,host=kilenc,jobs=completed value=12i 1674246976000000000&gt; lsf_mbatchd,host=kilenc,jobs=sentremote value=0i 1674246976000000000&gt; lsf_mbatchd,host=kilenc,jobs=acceptremote value=0i 1674246976000000000&gt; lsf_mbatchd,host=kilenc,sched=interval value=1i 1674246976000000000&gt; lsf_mbatchd,host=kilenc,sched=matchhost value=5i 1674246976000000000&gt; lsf_mbatchd,host=kilenc,sched=buckets value=5i 1674246976000000000&gt; lsf_mbatchd,host=kilenc,sched=reordered value=7i 1674246976000000000&gt; lsf_mbatchd,host=kilenc,utilization=slots value=100i 1674246976000000000&gt; lsf_mbatchd,host=kilenc,utilization=memory value=0i 1674246976000000000&gt; lsf_mbatchd,fd=free,host=kilenc value=65509i 1674246976000000000&gt; lsf_mbatchd,fd=used,host=kilenc value=26i 1674246976000000000&gt; lsf_mbatchd,fd=total,host=kilenc value=65535i 1674246976000000000&gt; lsf_queues,host=kilenc,name=admin njobs=0i,pend=0i,rsv=0i,run=0i,ssusp=0i,susp=0i,ususp=0i 1674246976000000000&gt; lsf_queues,host=kilenc,name=owners njobs=0i,pend=0i,rsv=0i,run=0i,ssusp=0i,susp=0i,ususp=0i 1674246976000000000&gt; lsf_queues,host=kilenc,name=priority njobs=93951i,pend=93923i,rsv=0i,run=28i,ssusp=0i,susp=0i,ususp=0i 1674246976000000000&gt; lsf_queues,host=kilenc,name=night njobs=0i,pend=0i,rsv=0i,run=0i,ssusp=0i,susp=0i,ususp=0i 1674246976000000000&gt; lsf_queues,host=kilenc,name=short njobs=2504i,pend=2504i,rsv=0i,run=0i,ssusp=0i,susp=0i,ususp=0i 1674246976000000000&gt; lsf_queues,host=kilenc,name=dataq njobs=0i,pend=0i,rsv=0i,run=0i,ssusp=0i,susp=0i,ususp=0i 1674246976000000000&gt; lsf_queues,host=kilenc,name=normal njobs=1750i,pend=1750i,rsv=0i,run=0i,ssusp=0i,susp=0i,ususp=0i 1674246976000000000&gt; lsf_queues,host=kilenc,name=interactive njobs=0i,pend=0i,rsv=0i,run=0i,ssusp=0i,susp=0i,ususp=0i 1674246976000000000&gt; lsf_queues,host=kilenc,name=sendq njobs=22598i,pend=22594i,rsv=0i,run=4i,ssusp=0i,susp=0i,ususp=0i 1674246976000000000&gt; lsf_queues,host=kilenc,name=idle njobs=0i,pend=0i,rsv=0i,run=0i,ssusp=0i,susp=0i,ususp=0i 1674246976000000000&gt; cpu,cpu=cpu0,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=100,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu4,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=100,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu8,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=100,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu12,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=100,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu16,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=98.03921568448419,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=1.9607843137324836 1674246977000000000&gt; cpu,cpu=cpu20,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=100,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu24,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=100,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu28,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=100,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu32,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=100,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu36,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=100,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu40,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=98.03921568448419,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=1.9607843136879006,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu44,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=100,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu48,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=100,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu52,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=0,usage_iowait=100,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu56,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=100,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu60,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=100,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu64,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=87.99999999906868,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=10.000000001155058,usage_user=2.0000000002764864 1674246977000000000&gt; cpu,cpu=cpu68,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=100,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu72,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=86.27450980280263,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=11.764705882127403,usage_user=1.9607843137324836 1674246977000000000&gt; cpu,cpu=cpu76,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=100,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu80,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=92.30769231113655,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=3.8461538464431086,usage_user=3.84615384653056 1674246977000000000&gt; cpu,cpu=cpu84,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=94.11764706486585,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=5.882352941197451 1674246977000000000&gt; cpu,cpu=cpu88,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=100,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu92,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=70.58823529344627,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=29.411764701983955,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu96,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=96.15384615040192,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=3.8461538460125784,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu100,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=97.99999999813735,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=1.999999999998181,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu104,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=96.07843137993407,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=3.92156862782338,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu108,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=96.07843136896838,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=1.9607843136879006,usage_user=1.9607843137324836 1674246977000000000&gt; cpu,cpu=cpu112,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=100,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu116,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=95.91836734305988,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=4.08163265313509,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu120,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=84.61538461280144,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=3.8461538460344413,usage_user=11.53846153830009 1674246977000000000&gt; cpu,cpu=cpu124,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=100,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu-total,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=93.47826086554115,usage_iowait=3.1055900618243673,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=2.484472049468532,usage_user=0.9316770186919254 1674246977000000000&gt; procstat,exe=mbatchd,host=kilenc,process_name=mbatchd,user=root child_major_faults=0i,child_minor_faults=0i,cpu_time=0i,cpu_time_guest=0,cpu_time_guest_nice=0,cpu_time_idle=0,cpu_time_iowait=0,cpu_time_irq=0,cpu_time_nice=0,cpu_time_soft_irq=0,cpu_time_steal=0,cpu_time_system=0.03,cpu_time_user=0.05,cpu_usage=0,created_at=1674246974000000000i,involuntary_context_switches=1i,major_faults=0i,memory_data=834994176i,memory_locked=0i,memory_rss=815595520i,memory_stack=327680i,memory_swap=0i,memory_usage=2.469912528991699,memory_vms=1091108864i,minor_faults=726i,nice_priority=20i,num_fds=10i,num_threads=2i,pid=62056i,ppid=4103699i,read_bytes=0i,read_count=27i,realtime_priority=0i,rlimit_cpu_time_hard=9223372036854775807i,rlimit_cpu_time_soft=9223372036854775807i,rlimit_file_locks_hard=9223372036854775807i,rlimit_file_locks_soft=9223372036854775807i,rlimit_memory_data_hard=9223372036854775807i,rlimit_memory_data_soft=9223372036854775807i,rlimit_memory_locked_hard=67108864i,rlimit_memory_locked_soft=67108864i,rlimit_memory_rss_hard=9223372036854775807i,rlimit_memory_rss_soft=9223372036854775807i,rlimit_memory_stack_hard=9223372036854775807i,rlimit_memory_stack_soft=8388608i,rlimit_memory_vms_hard=9223372036854775807i,rlimit_memory_vms_soft=9223372036854775807i,rlimit_nice_priority_hard=0i,rlimit_nice_priority_soft=0i,rlimit_num_fds_hard=262144i,rlimit_num_fds_soft=65535i,rlimit_realtime_priority_hard=0i,rlimit_realtime_priority_soft=0i,rlimit_signals_pending_hard=118856i,rlimit_signals_pending_soft=118856i,signals_pending=0i,voluntary_context_switches=5i,write_bytes=0i,write_count=16i 1674246977000000000&gt; procstat,exe=mbschd,host=kilenc,process_name=mbschd,user=lsfadmin child_major_faults=0i,child_minor_faults=2457641i,cpu_time=320i,cpu_time_guest=0,cpu_time_guest_nice=0,cpu_time_idle=0,cpu_time_iowait=0.02,cpu_time_irq=0,cpu_time_nice=0,cpu_time_soft_irq=0,cpu_time_steal=0,cpu_time_system=8.4,cpu_time_user=312.14,cpu_usage=1.836645120693344,created_at=1674227581000000000i,involuntary_context_switches=3553i,major_faults=1i,memory_data=228851712i,memory_locked=0i,memory_rss=236847104i,memory_stack=196608i,memory_swap=0i,memory_usage=0.717257022857666,memory_vms=246808576i,minor_faults=2137969i,nice_priority=20i,num_fds=3i,num_threads=1i,pid=4103740i,ppid=4103699i,read_bytes=1552384i,read_count=936861i,realtime_priority=0i,rlimit_cpu_time_hard=9223372036854775807i,rlimit_cpu_time_soft=9223372036854775807i,rlimit_file_locks_hard=9223372036854775807i,rlimit_file_locks_soft=9223372036854775807i,rlimit_memory_data_hard=9223372036854775807i,rlimit_memory_data_soft=9223372036854775807i,rlimit_memory_locked_hard=67108864i,rlimit_memory_locked_soft=67108864i,rlimit_memory_rss_hard=9223372036854775807i,rlimit_memory_rss_soft=9223372036854775807i,rlimit_memory_stack_hard=9223372036854775807i,rlimit_memory_stack_soft=8388608i,rlimit_memory_vms_hard=9223372036854775807i,rlimit_memory_vms_soft=9223372036854775807i,rlimit_nice_priority_hard=0i,rlimit_nice_priority_soft=0i,rlimit_num_fds_hard=262144i,rlimit_num_fds_soft=65535i,rlimit_realtime_priority_hard=0i,rlimit_realtime_priority_soft=0i,rlimit_signals_pending_hard=118856i,rlimit_signals_pending_soft=118856i,signals_pending=0i,voluntary_context_switches=43952i,write_bytes=0i,write_count=42311i 1674246977000000000&gt; procstat_lookup,exe=mbschd,host=kilenc,pid_finder=pgrep,result=success pid_count=1i,result_code=0i,running=1i 1674246977000000000&gt; procstat,exe=mbatchd,host=kilenc,process_name=mbatchd,user=root child_major_faults=2i,child_minor_faults=4476280i,cpu_time=177i,cpu_time_guest=0,cpu_time_guest_nice=0,cpu_time_idle=0,cpu_time_iowait=6.68,cpu_time_irq=0,cpu_time_nice=0,cpu_time_soft_irq=0,cpu_time_steal=0,cpu_time_system=51.01,cpu_time_user=126.42,cpu_usage=0,created_at=1674227573000000000i,involuntary_context_switches=4993i,major_faults=3i,memory_data=834994176i,memory_locked=0i,memory_rss=827785216i,memory_stack=327680i,memory_swap=0i,memory_usage=2.5068273544311523,memory_vms=1091108864i,minor_faults=2406945i,nice_priority=20i,num_fds=26i,num_threads=3i,pid=4103699i,ppid=4103684i,read_bytes=21008384i,read_count=364726i,realtime_priority=0i,rlimit_cpu_time_hard=9223372036854775807i,rlimit_cpu_time_soft=9223372036854775807i,rlimit_file_locks_hard=9223372036854775807i,rlimit_file_locks_soft=9223372036854775807i,rlimit_memory_data_hard=9223372036854775807i,rlimit_memory_data_soft=9223372036854775807i,rlimit_memory_locked_hard=67108864i,rlimit_memory_locked_soft=67108864i,rlimit_memory_rss_hard=9223372036854775807i,rlimit_memory_rss_soft=9223372036854775807i,rlimit_memory_stack_hard=9223372036854775807i,rlimit_memory_stack_soft=8388608i,rlimit_memory_vms_hard=9223372036854775807i,rlimit_memory_vms_soft=9223372036854775807i,rlimit_nice_priority_hard=0i,rlimit_nice_priority_soft=0i,rlimit_num_fds_hard=262144i,rlimit_num_fds_soft=65535i,rlimit_realtime_priority_hard=0i,rlimit_realtime_priority_soft=0i,rlimit_signals_pending_hard=118856i,rlimit_signals_pending_soft=118856i,signals_pending=0i,voluntary_context_switches=172583i,write_bytes=1562181632i,write_count=12164760i 1674246977000000000&gt; procstat_lookup,exe=mbatchd,host=kilenc,pid_finder=pgrep,result=success pid_count=2i,result_code=0i,running=2i 1674246977000000000Assuming there were no errors in the previous step with telegraf, proceed to start the telegraf process via systemd.[root@kilenc telegraf]# systemctl start telegraf[root@kilenc telegraf]# systemctl status telegraf● telegraf.service - Telegraf   Loaded: loaded (/usr/lib/systemd/system/telegraf.service; enabled; vendor preset: disabled)   Active: active (running) since Thu 2023-01-19 14:13:51 EST; 1 day 1h ago     Docs: https://github.com/influxdata/telegraf Main PID: 3225959 (telegraf)    Tasks: 35 (limit: 190169)   Memory: 192.6M   CGroup: /system.slice/telegraf.service           └─3225959 /usr/bin/telegraf -config /etc/telegraf/telegraf.conf -config-directory /etc/tele&gt;Jan 19 14:13:51 kilenc systemd[1]: Starting Telegraf...Jan 19 14:13:51 kilenc systemd[1]: Started Telegraf.On the host running the database instance, adatbazis, perform queries to check whether the database telegraf exists, as well as checking if LSF related data is being logged.This is confirmed in the output below.  Output from InfluxDB queries. Click to expand!  [root@adatbazis fedora]# influxConnected to https://localhost:8086 version 1.8.10InfluxDB shell version: 1.8.10&gt; authusername: influxpassword: &gt; show databasesname: databasesname----_internaltelegraf&gt; use telegrafUsing database telegraf&gt; show field keysname: cpufieldKey         fieldType--------         ---------usage_guest      floatusage_guest_nice floatusage_idle       floatusage_iowait     floatusage_irq        floatusage_nice       floatusage_softirq    floatusage_steal      floatusage_system     floatusage_user       floatname: diskfieldKey     fieldType--------     ---------free         integerinodes_free  integerinodes_total integerinodes_used  integertotal        integerused         integerused_percent floatname: diskiofieldKey         fieldType--------         ---------io_time          integeriops_in_progress integermerged_reads     integermerged_writes    integerread_bytes       integerread_time        integerreads            integerweighted_io_time integerwrite_bytes      integerwrite_time       integerwrites           integername: kernelfieldKey         fieldType--------         ---------boot_time        integercontext_switches integerentropy_avail    integerinterrupts       integerprocesses_forked integername: lsf_hostsfieldKey fieldType-------- ---------current  integerpeak     integername: lsf_jobsfieldKey fieldType-------- ---------value    integername: lsf_mbatchdfieldKey fieldType-------- ---------value    integername: lsf_queuesfieldKey fieldType-------- ---------njobs    integerpend     integerrsv      integerrun      integerssusp    integersusp     integerususp    integername: lsf_serversfieldKey fieldType-------- ---------value    integername: lsf_usersfieldKey fieldType-------- ---------value    integername: memfieldKey          fieldType--------          ---------active            integeravailable         integeravailable_percent floatbuffered          integercached            integercommit_limit      integercommitted_as      integerdirty             integerfree              integerhigh_free         integerhigh_total        integerhuge_page_size    integerhuge_pages_free   integerhuge_pages_total  integerinactive          integerlow_free          integerlow_total         integermapped            integerpage_tables       integershared            integerslab              integersreclaimable      integersunreclaim        integerswap_cached       integerswap_free         integerswap_total        integertotal             integerused              integerused_percent      floatvmalloc_chunk     integervmalloc_total     integervmalloc_used      integerwrite_back        integerwrite_back_tmp    integername: netfieldKey              fieldType--------              ---------bytes_recv            integerbytes_sent            integerdrop_in               integerdrop_out              integererr_in                integererr_out               integericmp_inaddrmaskreps   integericmp_inaddrmasks      integericmp_incsumerrors     integericmp_indestunreachs   integericmp_inechoreps       integericmp_inechos          integericmp_inerrors         integericmp_inmsgs           integericmp_inparmprobs      integericmp_inredirects      integericmp_insrcquenchs     integericmp_intimeexcds      integericmp_intimestampreps  integericmp_intimestamps     integericmp_outaddrmaskreps  integericmp_outaddrmasks     integericmp_outdestunreachs  integericmp_outechoreps      integericmp_outechos         integericmp_outerrors        integericmp_outmsgs          integericmp_outparmprobs     integericmp_outredirects     integericmp_outsrcquenchs    integericmp_outtimeexcds     integericmp_outtimestampreps integericmp_outtimestamps    integericmpmsg_intype0       integericmpmsg_intype3       integericmpmsg_intype8       integericmpmsg_outtype0      integericmpmsg_outtype3      integericmpmsg_outtype8      integerip_defaultttl         integerip_forwarding         integerip_forwdatagrams      integerip_fragcreates        integerip_fragfails          integerip_fragoks            integerip_inaddrerrors       integerip_indelivers         integerip_indiscards         integerip_inhdrerrors        integerip_inreceives         integerip_inunknownprotos    integerip_outdiscards        integerip_outnoroutes        integerip_outrequests        integerip_reasmfails         integerip_reasmoks           integerip_reasmreqds         integerip_reasmtimeout       integerpackets_recv          integerpackets_sent          integertcp_activeopens       integertcp_attemptfails      integertcp_currestab         integertcp_estabresets       integertcp_incsumerrors      integertcp_inerrs            integertcp_insegs            integertcp_maxconn           integertcp_outrsts           integertcp_outsegs           integertcp_passiveopens      integertcp_retranssegs       integertcp_rtoalgorithm      integertcp_rtomax            integertcp_rtomin            integerudp_ignoredmulti      integerudp_incsumerrors      integerudp_indatagrams       integerudp_inerrors          integerudp_memerrors         integerudp_noports           integerudp_outdatagrams      integerudp_rcvbuferrors      integerudp_sndbuferrors      integerudplite_ignoredmulti  integerudplite_incsumerrors  integerudplite_indatagrams   integerudplite_inerrors      integerudplite_memerrors     integerudplite_noports       integerudplite_outdatagrams  integerudplite_rcvbuferrors  integerudplite_sndbuferrors  integername: processesfieldKey      fieldType--------      ---------blocked       integerdead          integeridle          integerpaging        integerparked        integerrunning       integersleeping      integerstopped       integertotal         integertotal_threads integerunknown       integerzombies       integername: procstatfieldKey                     fieldType--------                     ---------child_major_faults           integerchild_minor_faults           integercpu_time_guest               floatcpu_time_guest_nice          floatcpu_time_idle                floatcpu_time_iowait              floatcpu_time_irq                 floatcpu_time_nice                floatcpu_time_soft_irq            floatcpu_time_steal               floatcpu_time_system              floatcpu_time_user                floatcpu_usage                    floatcreated_at                   integerinvoluntary_context_switches integermajor_faults                 integermemory_data                  integermemory_locked                integermemory_rss                   integermemory_stack                 integermemory_swap                  integermemory_usage                 floatmemory_vms                   integerminor_faults                 integernum_threads                  integerpid                          integerppid                         integervoluntary_context_switches   integername: procstat_lookupfieldKey    fieldType--------    ---------pid_count   integerresult_code integerrunning     integername: swapfieldKey     fieldType--------     ---------free         integerin           integerout          integertotal        integerused         integerused_percent floatname: systemfieldKey       fieldType--------       ---------load1          floatload15         floatload5          floatn_cpus         integern_unique_users integern_users        integeruptime         integeruptime_format  string&gt; select * from metrics&gt; SELECT * FROM \"lsf_hosts\";name: lsf_hoststime                current host   peak state----                ------- ----   ---- -----1674493170000000000 0       kilenc 0    clients1674493170000000000 32      kilenc 32   slots1674493170000000000 32      kilenc 32   cores1674493170000000000 1       kilenc 1    servers1674493170000000000 2       kilenc 2    cpus1674493200000000000 1       kilenc 1    servers1674493200000000000 2       kilenc 2    cpus1674493200000000000 32      kilenc 32   slots1674493200000000000 0       kilenc 0    clients1674493200000000000 32      kilenc 32   cores1674493230000000000 0       kilenc 0    clients1674493230000000000 32      kilenc 32   cores1674493230000000000 2       kilenc 2    cpus1674493230000000000 1       kilenc 1    servers1674493230000000000 32      kilenc 32   slots1674493260000000000 1       kilenc 1    servers1674493260000000000 32      kilenc 32   slots1674493260000000000 0       kilenc 0    clients1674493260000000000 2       kilenc 2    cpus1674493260000000000 32      kilenc 32   cores&gt; quitWith telegraf successfully logging data to the InfluxDB instance, it will now be possible to create a data source in Grafana in order to create a dashboard containing LSF metrics.As noted at the outset, this article is not meant to be an extensive guide to the creation of dashoards in Grafana. In the Grafana navigation select Configuration &gt; Data sources.Select the Add data source button, followed by InfluxDB, which is listed under Time series databases. On the settings page specify following values:VariableValueURLhttp://adatbazis:8086DatabasetelegrafBasic auth(enable)User&lt;influxdb_username&gt;Password&lt;influxdb_passwordNext, click on Save &amp; test. If all variables and settings were properly specified, the message datasource is working. 17 measurements found.With the datasource configured in Grafana, the final step is to create a dashboard. Creating a dashboard requires creating panels which display data pulled from the confiugred datasource using targeted queries. With a bit of effort, I was able to piece together the following dashboard which includes both metrics from LSF, as well as metrics from Telegrafinput.procstat for the LSF processes mbatchd, mbschd and the management lim.  Example dashboard definition (JSON). Click to expand!  {  \"annotations\": {    \"list\": [      {        \"builtIn\": 1,        \"datasource\": {          \"type\": \"datasource\",          \"uid\": \"grafana\"        },        \"enable\": true,        \"hide\": true,        \"iconColor\": \"rgba(0, 211, 255, 1)\",        \"name\": \"Annotations &amp; Alerts\",        \"target\": {          \"limit\": 100,          \"matchAny\": false,          \"tags\": [],          \"type\": \"dashboard\"        },        \"type\": \"dashboard\"      }    ]  },  \"editable\": true,  \"fiscalYearStartMonth\": 0,  \"graphTooltip\": 0,  \"id\": 17,  \"links\": [],  \"liveNow\": false,  \"panels\": [    {      \"collapsed\": false,      \"gridPos\": {        \"h\": 1,        \"w\": 24,        \"x\": 0,        \"y\": 0      },      \"id\": 35,      \"panels\": [],      \"title\": \"Cluster aggregate current statistics\",      \"type\": \"row\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"A view of the current status of the LSF servers in the cluster. Servers can be in one of four states: Ok, Unavailable, Closed and Unreachable. \",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"palette-classic\"          },          \"custom\": {            \"hideFrom\": {              \"legend\": false,              \"tooltip\": false,              \"viz\": false            }          },          \"decimals\": 2,          \"mappings\": []        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 8,        \"w\": 9,        \"x\": 0,        \"y\": 1      },      \"id\": 32,      \"options\": {        \"displayLabels\": [          \"name\",          \"value\"        ],        \"legend\": {          \"displayMode\": \"table\",          \"placement\": \"right\",          \"showLegend\": true,          \"sortBy\": \"Value\",          \"sortDesc\": true,          \"values\": [            \"value\",            \"percent\"          ]        },        \"pieType\": \"donut\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"\",          \"values\": false        },        \"tooltip\": {          \"mode\": \"multi\",          \"sort\": \"none\"        }      },      \"targets\": [        {          \"alias\": \"Ok\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_servers\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ]          ],          \"tags\": [            {              \"key\": \"status\",              \"operator\": \"=\",              \"value\": \"ok\"            }          ]        },        {          \"alias\": \"Closed\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_servers\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"B\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ]          ],          \"tags\": [            {              \"key\": \"status\",              \"operator\": \"=\",              \"value\": \"closed\"            }          ]        },        {          \"alias\": \"Unreachable\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_servers\",          \"orderByTime\": \"ASC\",          \"policy\": \"default\",          \"refId\": \"C\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ]          ],          \"tags\": [            {              \"key\": \"status\",              \"operator\": \"=\",              \"value\": \"unreachable\"            }          ]        },        {          \"alias\": \"Unavailable\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_servers\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"D\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"status\",              \"operator\": \"=\",              \"value\": \"unavailable\"            }          ]        }      ],      \"title\": \"Current aggregate LSF server statistics\",      \"type\": \"piechart\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"green\",                \"value\": null              }            ]          }        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 9,        \"y\": 1      },      \"id\": 43,      \"options\": {        \"colorMode\": \"value\",        \"graphMode\": \"none\",        \"justifyMode\": \"auto\",        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"\",          \"values\": false        },        \"text\": {},        \"textMode\": \"auto\"      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_jobs\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"distinct\"              }            ]          ],          \"tags\": [            {              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"running\"            }          ]        }      ],      \"title\": \"Currently running\",      \"type\": \"stat\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"light-red\",                \"value\": null              }            ]          }        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 12,        \"y\": 1      },      \"id\": 45,      \"options\": {        \"colorMode\": \"value\",        \"graphMode\": \"none\",        \"justifyMode\": \"auto\",        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"\",          \"values\": false        },        \"text\": {},        \"textMode\": \"auto\"      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"measurement\": \"lsf_jobs\",          \"orderByTime\": \"ASC\",          \"policy\": \"default\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"suspended\"            }          ]        }      ],      \"title\": \"Currently suspended\",      \"type\": \"stat\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"palette-classic\"          },          \"custom\": {            \"hideFrom\": {              \"legend\": false,              \"tooltip\": false,              \"viz\": false            }          },          \"decimals\": 2,          \"mappings\": []        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 8,        \"w\": 9,        \"x\": 15,        \"y\": 1      },      \"id\": 33,      \"options\": {        \"displayLabels\": [          \"name\",          \"value\"        ],        \"legend\": {          \"displayMode\": \"table\",          \"placement\": \"right\",          \"showLegend\": true,          \"sortBy\": \"Value\",          \"sortDesc\": true,          \"values\": [            \"value\",            \"percent\"          ]        },        \"pieType\": \"donut\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"\",          \"values\": false        },        \"tooltip\": {          \"mode\": \"multi\",          \"sort\": \"none\"        }      },      \"targets\": [        {          \"alias\": \"Running\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_jobs\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ]          ],          \"tags\": [            {              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"running\"            }          ]        },        {          \"alias\": \"Pending\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_jobs\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"B\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ]          ],          \"tags\": [            {              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"pending\"            }          ]        },        {          \"alias\": \"Suspended\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_jobs\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"C\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ]          ],          \"tags\": [            {              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"suspended\"            }          ]        }      ],      \"title\": \"Current aggregate LSF job statistics\",      \"type\": \"piechart\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"yellow\",                \"value\": null              }            ]          }        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 9,        \"y\": 5      },      \"id\": 44,      \"options\": {        \"colorMode\": \"value\",        \"graphMode\": \"none\",        \"justifyMode\": \"auto\",        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"\",          \"values\": false        },        \"text\": {},        \"textMode\": \"auto\"      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"measurement\": \"lsf_jobs\",          \"orderByTime\": \"ASC\",          \"policy\": \"default\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"pending\"            }          ]        }      ],      \"title\": \"Currently pending \",      \"type\": \"stat\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"blue\",                \"value\": null              }            ]          }        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 12,        \"y\": 5      },      \"id\": 46,      \"options\": {        \"colorMode\": \"value\",        \"graphMode\": \"none\",        \"justifyMode\": \"auto\",        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"\",          \"values\": false        },        \"text\": {},        \"textMode\": \"auto\"      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"measurement\": \"lsf_jobs\",          \"orderByTime\": \"ASC\",          \"policy\": \"default\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"finished\"            }          ]        }      ],      \"title\": \"Finished (past hour)\",      \"type\": \"stat\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"Spectrum LSF queue statistics. Here we show jobs in running, pending and suspended jobs. \",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"palette-classic\"          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"green\",                \"value\": null              },              {                \"color\": \"red\",                \"value\": 80              }            ]          }        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 8,        \"w\": 9,        \"x\": 0,        \"y\": 9      },      \"id\": 41,      \"options\": {        \"displayMode\": \"lcd\",        \"minVizHeight\": 10,        \"minVizWidth\": 0,        \"orientation\": \"horizontal\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"\",          \"values\": false        },        \"showUnfilled\": true      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"alias\": \"Running\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"measurement\": \"lsf_queues\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"run\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ]          ],          \"tags\": [            {              \"key\": \"name\",              \"operator\": \"=~\",              \"value\": \"/^$Queue$/\"            }          ]        },        {          \"alias\": \"Pending\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_queues\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"B\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"pend\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ]          ],          \"tags\": [            {              \"key\": \"name\",              \"operator\": \"=~\",              \"value\": \"/^$Queue$/\"            }          ]        },        {          \"alias\": \"Suspended\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_queues\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"C\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"susp\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ]          ],          \"tags\": [            {              \"key\": \"name\",              \"operator\": \"=~\",              \"value\": \"/^$Queue$/\"            }          ]        }      ],      \"title\": \"Current queue statistics ($Queue)\",      \"type\": \"bargauge\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"min\": 0,          \"thresholds\": {            \"mode\": \"percentage\",            \"steps\": [              {                \"color\": \"green\",                \"value\": null              }            ]          },          \"unit\": \"none\"        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 9,        \"y\": 9      },      \"id\": 53,      \"options\": {        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"/^lsf_hosts\\\\.last$/\",          \"values\": false        },        \"showThresholdLabels\": false,        \"showThresholdMarkers\": true      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_hosts\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"current\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ],            [              {                \"params\": [                  \"peak\"                ],                \"type\": \"field\"              }            ]          ],          \"tags\": [            {              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            },            {              \"condition\": \"AND\",              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"servers\"            }          ]        }      ],      \"title\": \"Servers\",      \"type\": \"gauge\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"min\": 0,          \"thresholds\": {            \"mode\": \"percentage\",            \"steps\": [              {                \"color\": \"yellow\",                \"value\": null              }            ]          },          \"unit\": \"none\"        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 12,        \"y\": 9      },      \"id\": 54,      \"options\": {        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"/^lsf_hosts\\\\.last$/\",          \"values\": false        },        \"showThresholdLabels\": false,        \"showThresholdMarkers\": true      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_hosts\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"current\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ],            [              {                \"params\": [                  \"peak\"                ],                \"type\": \"field\"              }            ]          ],          \"tags\": [            {              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            },            {              \"condition\": \"AND\",              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"cpus\"            }          ]        }      ],      \"title\": \"CPUs\",      \"type\": \"gauge\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"palette-classic\"          },          \"custom\": {            \"axisCenteredZero\": false,            \"axisColorMode\": \"text\",            \"axisLabel\": \"\",            \"axisPlacement\": \"auto\",            \"barAlignment\": 0,            \"drawStyle\": \"line\",            \"fillOpacity\": 0,            \"gradientMode\": \"none\",            \"hideFrom\": {              \"legend\": false,              \"tooltip\": false,              \"viz\": false            },            \"lineInterpolation\": \"stepBefore\",            \"lineWidth\": 1,            \"pointSize\": 5,            \"scaleDistribution\": {              \"log\": 2,              \"type\": \"log\"            },            \"showPoints\": \"auto\",            \"spanNulls\": true,            \"stacking\": {              \"group\": \"A\",              \"mode\": \"none\"            },            \"thresholdsStyle\": {              \"mode\": \"off\"            }          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"green\",                \"value\": null              },              {                \"color\": \"red\",                \"value\": 80              }            ]          }        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 8,        \"w\": 9,        \"x\": 15,        \"y\": 9      },      \"id\": 42,      \"options\": {        \"legend\": {          \"calcs\": [],          \"displayMode\": \"list\",          \"placement\": \"bottom\",          \"showLegend\": true        },        \"tooltip\": {          \"mode\": \"single\",          \"sort\": \"none\"        }      },      \"targets\": [        {          \"alias\": \"Running\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_jobs\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ]          ],          \"tags\": [            {              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"running\"            }          ]        },        {          \"alias\": \"Pending\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_jobs\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"B\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ]          ],          \"tags\": [            {              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"pending\"            }          ]        },        {          \"alias\": \"Suspended\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_jobs\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"C\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ]          ],          \"tags\": [            {              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"suspended\"            }          ]        }      ],      \"title\": \"Aggregate LSF job statistics\",      \"type\": \"timeseries\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"min\": 0,          \"thresholds\": {            \"mode\": \"percentage\",            \"steps\": [              {                \"color\": \"light-red\",                \"value\": null              }            ]          },          \"unit\": \"none\"        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 9,        \"y\": 13      },      \"id\": 55,      \"options\": {        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"/^lsf_hosts\\\\.last$/\",          \"values\": false        },        \"showThresholdLabels\": false,        \"showThresholdMarkers\": true      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_hosts\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"current\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ],            [              {                \"params\": [                  \"peak\"                ],                \"type\": \"field\"              }            ]          ],          \"tags\": [            {              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            },            {              \"condition\": \"AND\",              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"cores\"            }          ]        }      ],      \"title\": \"Cores\",      \"type\": \"gauge\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"min\": 0,          \"thresholds\": {            \"mode\": \"percentage\",            \"steps\": [              {                \"color\": \"blue\",                \"value\": null              }            ]          },          \"unit\": \"none\"        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 12,        \"y\": 13      },      \"id\": 56,      \"options\": {        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"/^lsf_hosts\\\\.last$/\",          \"values\": false        },        \"showThresholdLabels\": false,        \"showThresholdMarkers\": true      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_hosts\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"current\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ],            [              {                \"params\": [                  \"peak\"                ],                \"type\": \"field\"              }            ]          ],          \"tags\": [            {              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            },            {              \"condition\": \"AND\",              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"slots\"            }          ]        }      ],      \"title\": \"Slots\",      \"type\": \"gauge\"    },    {      \"collapsed\": false,      \"gridPos\": {        \"h\": 1,        \"w\": 24,        \"x\": 0,        \"y\": 17      },      \"id\": 37,      \"panels\": [],      \"title\": \"LSF scheduler statistics\",      \"type\": \"row\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"palette-classic\"          },          \"custom\": {            \"axisCenteredZero\": false,            \"axisColorMode\": \"text\",            \"axisLabel\": \"\",            \"axisPlacement\": \"auto\",            \"barAlignment\": 0,            \"drawStyle\": \"line\",            \"fillOpacity\": 10,            \"gradientMode\": \"none\",            \"hideFrom\": {              \"graph\": false,              \"legend\": false,              \"tooltip\": false,              \"viz\": false            },            \"lineInterpolation\": \"linear\",            \"lineWidth\": 1,            \"pointSize\": 5,            \"scaleDistribution\": {              \"type\": \"linear\"            },            \"showPoints\": \"never\",            \"spanNulls\": true,            \"stacking\": {              \"group\": \"A\",              \"mode\": \"none\"            },            \"thresholdsStyle\": {              \"mode\": \"off\"            }          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"green\",                \"value\": null              },              {                \"color\": \"red\",                \"value\": 80              }            ]          },          \"unit\": \"short\"        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 8,        \"w\": 12,        \"x\": 0,        \"y\": 18      },      \"id\": 20,      \"options\": {        \"graph\": {},        \"legend\": {          \"calcs\": [],          \"displayMode\": \"list\",          \"placement\": \"right\",          \"showLegend\": true        },        \"tooltip\": {          \"mode\": \"single\",          \"sort\": \"none\"        }      },      \"pluginVersion\": \"7.5.15\",      \"targets\": [        {          \"alias\": \"CPU utilization (%)\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"measurement\": \"procstat\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"cpu_usage\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"exe\",              \"operator\": \"=\",              \"value\": \"mbatchd\"            },            {              \"condition\": \"AND\",              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            }          ]        },        {          \"alias\": \"Memory utilization (%)\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"procstat\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"B\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"memory_usage\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"exe\",              \"operator\": \"=\",              \"value\": \"mbatchd\"            },            {              \"condition\": \"AND\",              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            }          ]        },        {          \"alias\": \"Number of threads\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"procstat\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"C\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"num_threads\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"exe\",              \"operator\": \"=\",              \"value\": \"mbatchd\"            },            {              \"condition\": \"AND\",              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            }          ]        },        {          \"alias\": \"File descriptors\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_mbatchd\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"D\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"fd\",              \"operator\": \"=\",              \"value\": \"used\"            },            {              \"condition\": \"AND\",              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            }          ]        }      ],      \"title\": \"LSF mbatchd process metrics\",      \"type\": \"timeseries\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"palette-classic\"          },          \"custom\": {            \"axisCenteredZero\": false,            \"axisColorMode\": \"text\",            \"axisLabel\": \"\",            \"axisPlacement\": \"auto\",            \"barAlignment\": 0,            \"drawStyle\": \"line\",            \"fillOpacity\": 10,            \"gradientMode\": \"none\",            \"hideFrom\": {              \"graph\": false,              \"legend\": false,              \"tooltip\": false,              \"viz\": false            },            \"lineInterpolation\": \"linear\",            \"lineWidth\": 1,            \"pointSize\": 5,            \"scaleDistribution\": {              \"type\": \"linear\"            },            \"showPoints\": \"never\",            \"spanNulls\": true,            \"stacking\": {              \"group\": \"A\",              \"mode\": \"none\"            },            \"thresholdsStyle\": {              \"mode\": \"off\"            }          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"green\",                \"value\": null              },              {                \"color\": \"red\",                \"value\": 80              }            ]          },          \"unit\": \"short\"        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 8,        \"w\": 12,        \"x\": 12,        \"y\": 18      },      \"id\": 57,      \"options\": {        \"graph\": {},        \"legend\": {          \"calcs\": [],          \"displayMode\": \"list\",          \"placement\": \"right\",          \"showLegend\": true        },        \"tooltip\": {          \"mode\": \"single\",          \"sort\": \"none\"        }      },      \"pluginVersion\": \"7.5.15\",      \"targets\": [        {          \"alias\": \"CPU utilization (%)\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"measurement\": \"procstat\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"cpu_usage\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"exe\",              \"operator\": \"=\",              \"value\": \"lim\"            },            {              \"condition\": \"AND\",              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            }          ]        },        {          \"alias\": \"Memory utilization (%)\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"procstat\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"B\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"memory_usage\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"exe\",              \"operator\": \"=\",              \"value\": \"lim\"            },            {              \"condition\": \"AND\",              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            }          ]        },        {          \"alias\": \"Number of threads\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"procstat\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"C\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"num_threads\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"exe\",              \"operator\": \"=\",              \"value\": \"lim\"            },            {              \"condition\": \"AND\",              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            }          ]        }      ],      \"title\": \"LSF management lim process metrics\",      \"type\": \"timeseries\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"palette-classic\"          },          \"custom\": {            \"axisCenteredZero\": false,            \"axisColorMode\": \"text\",            \"axisLabel\": \"\",            \"axisPlacement\": \"auto\",            \"barAlignment\": 0,            \"drawStyle\": \"line\",            \"fillOpacity\": 10,            \"gradientMode\": \"none\",            \"hideFrom\": {              \"graph\": false,              \"legend\": false,              \"tooltip\": false,              \"viz\": false            },            \"lineInterpolation\": \"linear\",            \"lineWidth\": 1,            \"pointSize\": 5,            \"scaleDistribution\": {              \"type\": \"linear\"            },            \"showPoints\": \"never\",            \"spanNulls\": true,            \"stacking\": {              \"group\": \"A\",              \"mode\": \"none\"            },            \"thresholdsStyle\": {              \"mode\": \"off\"            }          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"green\",                \"value\": null              },              {                \"color\": \"red\",                \"value\": 80              }            ]          },          \"unit\": \"short\"        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 8,        \"w\": 12,        \"x\": 0,        \"y\": 26      },      \"id\": 27,      \"options\": {        \"graph\": {},        \"legend\": {          \"calcs\": [],          \"displayMode\": \"list\",          \"placement\": \"right\",          \"showLegend\": true        },        \"tooltip\": {          \"mode\": \"single\",          \"sort\": \"none\"        }      },      \"pluginVersion\": \"7.5.15\",      \"targets\": [        {          \"alias\": \"Job buckets\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"measurement\": \"lsf_mbatchd\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"sched\",              \"operator\": \"=\",              \"value\": \"buckets\"            },            {              \"condition\": \"AND\",              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            }          ]        },        {          \"alias\": \"Matching host criteria\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_mbatchd\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"B\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"sched\",              \"operator\": \"=\",              \"value\": \"matchhost\"            },            {              \"condition\": \"AND\",              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            }          ]        },        {          \"alias\": \"Scheduling interval (seconds)\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_mbatchd\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"C\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"sched\",              \"operator\": \"=\",              \"value\": \"interval\"            },            {              \"condition\": \"AND\",              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            }          ]        }      ],      \"title\": \"LSF scheduler metrics\",      \"type\": \"timeseries\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"palette-classic\"          },          \"custom\": {            \"axisCenteredZero\": false,            \"axisColorMode\": \"text\",            \"axisLabel\": \"\",            \"axisPlacement\": \"auto\",            \"barAlignment\": 0,            \"drawStyle\": \"line\",            \"fillOpacity\": 10,            \"gradientMode\": \"none\",            \"hideFrom\": {              \"graph\": false,              \"legend\": false,              \"tooltip\": false,              \"viz\": false            },            \"lineInterpolation\": \"linear\",            \"lineWidth\": 1,            \"pointSize\": 5,            \"scaleDistribution\": {              \"type\": \"linear\"            },            \"showPoints\": \"never\",            \"spanNulls\": true,            \"stacking\": {              \"group\": \"A\",              \"mode\": \"none\"            },            \"thresholdsStyle\": {              \"mode\": \"off\"            }          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"green\",                \"value\": null              },              {                \"color\": \"red\",                \"value\": 80              }            ]          },          \"unit\": \"short\"        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 8,        \"w\": 12,        \"x\": 12,        \"y\": 26      },      \"id\": 58,      \"options\": {        \"graph\": {},        \"legend\": {          \"calcs\": [],          \"displayMode\": \"list\",          \"placement\": \"right\",          \"showLegend\": true        },        \"tooltip\": {          \"mode\": \"single\",          \"sort\": \"none\"        }      },      \"pluginVersion\": \"7.5.15\",      \"targets\": [        {          \"alias\": \"CPU utilization (%)\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"measurement\": \"procstat\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"cpu_usage\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"exe\",              \"operator\": \"=\",              \"value\": \"mbschd\"            },            {              \"condition\": \"AND\",              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            }          ]        },        {          \"alias\": \"Memory utilization (%)\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"procstat\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"B\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"memory_usage\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"exe\",              \"operator\": \"=\",              \"value\": \"mbatchd\"            },            {              \"condition\": \"AND\",              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            }          ]        },        {          \"alias\": \"Number of threads\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"procstat\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"C\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"num_threads\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"exe\",              \"operator\": \"=\",              \"value\": \"mbatchd\"            },            {              \"condition\": \"AND\",              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            }          ]        }      ],      \"title\": \"LSF mbschd process metrics\",      \"type\": \"timeseries\"    },    {      \"collapsed\": false,      \"gridPos\": {        \"h\": 1,        \"w\": 24,        \"x\": 0,        \"y\": 34      },      \"id\": 39,      \"panels\": [],      \"title\": \"Additional metrics (scratch)\",      \"type\": \"row\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"green\",                \"value\": null              }            ]          }        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 0,        \"y\": 35      },      \"id\": 2,      \"options\": {        \"colorMode\": \"value\",        \"graphMode\": \"none\",        \"justifyMode\": \"auto\",        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"\",          \"values\": false        },        \"text\": {},        \"textMode\": \"auto\"      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_jobs\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"distinct\"              }            ]          ],          \"tags\": [            {              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"running\"            }          ]        }      ],      \"title\": \"Running\",      \"type\": \"stat\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"yellow\",                \"value\": null              }            ]          }        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 3,        \"y\": 35      },      \"id\": 5,      \"options\": {        \"colorMode\": \"value\",        \"graphMode\": \"none\",        \"justifyMode\": \"auto\",        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"\",          \"values\": false        },        \"text\": {},        \"textMode\": \"auto\"      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"measurement\": \"lsf_jobs\",          \"orderByTime\": \"ASC\",          \"policy\": \"default\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"pending\"            }          ]        }      ],      \"title\": \"Pending\",      \"type\": \"stat\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"red\",                \"value\": null              }            ]          }        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 6,        \"y\": 35      },      \"id\": 6,      \"options\": {        \"colorMode\": \"value\",        \"graphMode\": \"none\",        \"justifyMode\": \"auto\",        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"\",          \"values\": false        },        \"text\": {},        \"textMode\": \"auto\"      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"measurement\": \"lsf_jobs\",          \"orderByTime\": \"ASC\",          \"policy\": \"default\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"suspended\"            }          ]        }      ],      \"title\": \"Suspended\",      \"type\": \"stat\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"blue\",                \"value\": null              }            ]          }        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 9,        \"y\": 35      },      \"id\": 7,      \"options\": {        \"colorMode\": \"value\",        \"graphMode\": \"none\",        \"justifyMode\": \"auto\",        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"\",          \"values\": false        },        \"text\": {},        \"textMode\": \"auto\"      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"measurement\": \"lsf_jobs\",          \"orderByTime\": \"ASC\",          \"policy\": \"default\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"finished\"            }          ]        }      ],      \"title\": \"Finished\",      \"type\": \"stat\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"green\",                \"value\": null              }            ]          }        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 12,        \"y\": 35      },      \"id\": 15,      \"options\": {        \"colorMode\": \"value\",        \"graphMode\": \"none\",        \"justifyMode\": \"auto\",        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"\",          \"values\": false        },        \"text\": {},        \"textMode\": \"auto\"      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"alias\": \"Ok\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_servers\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"status\",              \"operator\": \"=\",              \"value\": \"ok\"            }          ]        }      ],      \"title\": \"Ok\",      \"type\": \"stat\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"blue\",                \"value\": null              }            ]          }        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 15,        \"y\": 35      },      \"id\": 16,      \"options\": {        \"colorMode\": \"value\",        \"graphMode\": \"none\",        \"justifyMode\": \"auto\",        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"\",          \"values\": false        },        \"text\": {},        \"textMode\": \"auto\"      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"alias\": \"Closed\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_servers\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"status\",              \"operator\": \"=\",              \"value\": \"closed\"            }          ]        }      ],      \"title\": \"Closed\",      \"type\": \"stat\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"yellow\",                \"value\": null              }            ]          }        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 18,        \"y\": 35      },      \"id\": 17,      \"options\": {        \"colorMode\": \"value\",        \"graphMode\": \"none\",        \"justifyMode\": \"auto\",        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"\",          \"values\": false        },        \"text\": {},        \"textMode\": \"auto\"      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"alias\": \"Unreachable\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_servers\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"status\",              \"operator\": \"=\",              \"value\": \"unreachable\"            }          ]        }      ],      \"title\": \"Unreachable\",      \"type\": \"stat\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"red\",                \"value\": null              }            ]          }        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 21,        \"y\": 35      },      \"id\": 18,      \"options\": {        \"colorMode\": \"value\",        \"graphMode\": \"none\",        \"justifyMode\": \"auto\",        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"\",          \"values\": false        },        \"text\": {},        \"textMode\": \"auto\"      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"alias\": \"Unavailable\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_servers\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"status\",              \"operator\": \"=\",              \"value\": \"unavailable\"            }          ]        }      ],      \"title\": \"Unavailable\",      \"type\": \"stat\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"green\",                \"value\": null              }            ]          }        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 0,        \"y\": 39      },      \"id\": 21,      \"options\": {        \"colorMode\": \"value\",        \"graphMode\": \"none\",        \"justifyMode\": \"auto\",        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"\",          \"values\": false        },        \"text\": {},        \"textMode\": \"auto\"      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"alias\": \"Clients\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_hosts\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"current\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ]          ],          \"tags\": [            {              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            },            {              \"condition\": \"AND\",              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"clients\"            }          ]        }      ],      \"title\": \"Clients\",      \"type\": \"stat\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"green\",                \"value\": null              }            ]          }        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 3,        \"y\": 39      },      \"id\": 22,      \"options\": {        \"colorMode\": \"value\",        \"graphMode\": \"none\",        \"justifyMode\": \"auto\",        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"\",          \"values\": false        },        \"text\": {},        \"textMode\": \"auto\"      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"alias\": \"Servers\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_hosts\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"current\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ]          ],          \"tags\": [            {              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            },            {              \"condition\": \"AND\",              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"servers\"            }          ]        }      ],      \"title\": \"Servers\",      \"type\": \"stat\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"green\",                \"value\": null              }            ]          }        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 6,        \"y\": 39      },      \"id\": 23,      \"options\": {        \"colorMode\": \"value\",        \"graphMode\": \"none\",        \"justifyMode\": \"auto\",        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"\",          \"values\": false        },        \"text\": {},        \"textMode\": \"auto\"      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"alias\": \"Servers\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_hosts\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"current\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ]          ],          \"tags\": [            {              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            },            {              \"condition\": \"AND\",              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"cpus\"            }          ]        }      ],      \"title\": \"CPUs\",      \"type\": \"stat\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"green\",                \"value\": null              }            ]          }        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 9,        \"y\": 39      },      \"id\": 24,      \"options\": {        \"colorMode\": \"value\",        \"graphMode\": \"none\",        \"justifyMode\": \"auto\",        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"\",          \"values\": false        },        \"text\": {},        \"textMode\": \"auto\"      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"alias\": \"Cores\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_hosts\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"current\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ]          ],          \"tags\": [            {              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            },            {              \"condition\": \"AND\",              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"cores\"            }          ]        }      ],      \"title\": \"Cores\",      \"type\": \"stat\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"green\",                \"value\": null              }            ]          }        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 12,        \"y\": 39      },      \"id\": 25,      \"options\": {        \"colorMode\": \"value\",        \"graphMode\": \"none\",        \"justifyMode\": \"auto\",        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"\",          \"values\": false        },        \"text\": {},        \"textMode\": \"auto\"      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"alias\": \"Slots\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_hosts\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"current\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ]          ],          \"tags\": [            {              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            },            {              \"condition\": \"AND\",              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"slots\"            }          ]        }      ],      \"title\": \"Slots\",      \"type\": \"stat\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"min\": 0,          \"thresholds\": {            \"mode\": \"percentage\",            \"steps\": [              {                \"color\": \"green\",                \"value\": null              }            ]          },          \"unit\": \"none\"        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 3,        \"y\": 43      },      \"id\": 52,      \"options\": {        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"/^lsf_hosts\\\\.last$/\",          \"values\": false        },        \"showThresholdLabels\": false,        \"showThresholdMarkers\": true      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_hosts\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"current\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ],            [              {                \"params\": [                  \"peak\"                ],                \"type\": \"field\"              }            ]          ],          \"tags\": [            {              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            },            {              \"condition\": \"AND\",              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"servers\"            }          ]        }      ],      \"title\": \"Servers\",      \"type\": \"gauge\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"min\": 0,          \"thresholds\": {            \"mode\": \"percentage\",            \"steps\": [              {                \"color\": \"yellow\",                \"value\": null              }            ]          },          \"unit\": \"none\"        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 6,        \"y\": 43      },      \"id\": 51,      \"options\": {        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"/^lsf_hosts\\\\.last$/\",          \"values\": false        },        \"showThresholdLabels\": false,        \"showThresholdMarkers\": true      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_hosts\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"current\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ],            [              {                \"params\": [                  \"peak\"                ],                \"type\": \"field\"              }            ]          ],          \"tags\": [            {              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            },            {              \"condition\": \"AND\",              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"cpus\"            }          ]        }      ],      \"title\": \"CPUs\",      \"type\": \"gauge\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"min\": 0,          \"thresholds\": {            \"mode\": \"percentage\",            \"steps\": [              {                \"color\": \"light-red\",                \"value\": null              }            ]          },          \"unit\": \"none\"        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 9,        \"y\": 43      },      \"id\": 50,      \"options\": {        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"/^lsf_hosts\\\\.last$/\",          \"values\": false        },        \"showThresholdLabels\": false,        \"showThresholdMarkers\": true      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_hosts\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"current\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ],            [              {                \"params\": [                  \"peak\"                ],                \"type\": \"field\"              }            ]          ],          \"tags\": [            {              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            },            {              \"condition\": \"AND\",              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"cores\"            }          ]        }      ],      \"title\": \"Cores\",      \"type\": \"gauge\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"min\": 0,          \"thresholds\": {            \"mode\": \"percentage\",            \"steps\": [              {                \"color\": \"blue\",                \"value\": null              }            ]          },          \"unit\": \"none\"        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 12,        \"y\": 43      },      \"id\": 49,      \"options\": {        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"/^lsf_hosts\\\\.last$/\",          \"values\": false        },        \"showThresholdLabels\": false,        \"showThresholdMarkers\": true      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_hosts\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"current\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ],            [              {                \"params\": [                  \"peak\"                ],                \"type\": \"field\"              }            ]          ],          \"tags\": [            {              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            },            {              \"condition\": \"AND\",              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"slots\"            }          ]        }      ],      \"title\": \"Slots\",      \"type\": \"gauge\"    }  ],  \"refresh\": \"30s\",  \"schemaVersion\": 37,  \"style\": \"dark\",  \"tags\": [],  \"templating\": {    \"list\": [      {        \"current\": {          \"selected\": true,          \"text\": [            \"priority\"          ],          \"value\": [            \"priority\"          ]        },        \"datasource\": {          \"type\": \"influxdb\",          \"uid\": \"oSnSlVc4k\"        },        \"definition\": \"show tag values from \\\"lsf_queues\\\" with key=\\\"name\\\"\",        \"hide\": 0,        \"includeAll\": false,        \"multi\": false,        \"name\": \"Queue\",        \"options\": [],        \"query\": \"show tag values from \\\"lsf_queues\\\" with key=\\\"name\\\"\",        \"refresh\": 1,        \"regex\": \"\",        \"skipUrlSync\": false,        \"sort\": 0,        \"tagValuesQuery\": \"\",        \"tagsQuery\": \"\",        \"type\": \"query\",        \"useTags\": false      }    ]  },  \"time\": {    \"from\": \"now-1h\",    \"to\": \"now\"  },  \"timepicker\": {},  \"timezone\": \"\",  \"title\": \"LSF cluster status\",  \"uid\": \"ORojp8cVz\",  \"version\": 160,  \"weekStart\": \"\"}As you can see, with a short plugin script to collect information from LSF, it&rsquo;s possible to monitor your LSF cluster using the TIG stack. It&rsquo;s important to note that there are powerfulmonitoring and reporting tools available from IBM as add-ons to LSF; IBM Spectrum LSF RTM and IBM Spectrum LSF Explorer. You can find more details about the add-on capabilities for LSFhere.",
            "content_html": "<p>Much like dashboards in automobiles, dashboards in the context of HPC infrastructure are crucial to get an understanding of what&rsquo;s happening under the hood of your HPC cluster - ata glance. During my IT career, I&rsquo;ve used a myriad of monitoring solutions ranging from SNMP and Ganglia, to the ELK (Elasticsearch, Logstash, Kibana) stack. For example, I&rsquo;ve recentlywritten an overview on how it is possible to visualize <a href=\"https://www.ibm.com/products/hpc-workload-management\">IBM Spectrum LSF</a> (LSF) data in Grafana. LSF is an HPC job scheduler which brings to the table three decades of experience inworkload and resource management.</p><p>For this blog, I decided to take this to the next level by monitoring IBM Spectrum LSF with the well known TIG (Telegraf, InfluxDB, Grafana) stack. This article is not meant to be adebate on the advantages of one monitoring stack over another. Rather, the focus is to demonstrate what is feasible in terms of monitoring Spectrum LSF clusters with the TIG stack,given the many available ways to query LSF for key information using CLI commands.</p><hr /><p><strong>The Journey</strong></p><p>There already exists many write-ups on how to deploy the TIG stack to monitor systems. This isn&rsquo;t meant to be a guide on setting up the TIG stack. Rather, it&rsquo;s assumed that the readeralready has some familiarity with the TIG stack. If not, then [<em>insert your favourite search engine</em>] is your friend.</p><p>On my home network, I decided to setup a VM running on my trusty <a href=\"https://traverse.com.au/products/ten64-networking-platform/\">Traverse Ten64</a> running Fedora where InfluxDB was installed. The idea was to run InfluxDB on a system that is guaranteedto be always on in my home environment and that is energy efficient. Installing telegraf on all of the LSF cluster servers (x3) proved to be straight forward. Note that in all cases, I used the OSsupplied versions of InfluxDB, Telegraf. Finally, I already had a Grafana server running on a server in my network.</p><p>Out of the box, Telegraf has the ability to monitor numerous system metrics. Furthermore, there exists literally hundreds of plugins for Telegraf to monitor a wide variety of devices,services and software. A search however, didn&rsquo;t reveal the existence of any plugin to monitor LSF. So it was time to get creative.</p><hr /><p><strong>What to monitor?</strong></p><p>A bit of research revealed that InfluxDB supports what is known as &ldquo;line protocol&rdquo;. This is a well defined text-based format for writing data to InfluxDB. I used the following<a href=\"https://docs.influxdata.com/influxdb/v1.8/write_protocols/line_protocol_tutorial/\">reference</a> on &ldquo;line protocol&rdquo; to guide me. Using line protocol it would be ultimately possible towrite a plugin for Telegraf to effecively scrape information from Spectrum LSF and output in line protocol format for writing to InfluxDB.</p><p>Before I could begin writing the plugin, the key was to determine what information from Spectrum LSF would be useful to display in the dashboard, and how that information could beextracted. For this I followed the KISS principle to keep things as simple as possible. The key metrics I decided to report on were servers, queues and jobs (oh my!), as well as processinformation for the LSF scheduler daemons. Refer to the following table for details:</p><hr /><table><thead><tr><th>Metric(s)</th><th>Command</th></tr></thead><tbody><tr><td>LSF scheduler performance metrics</td><td><em>badmin perfmon view -json</em></td></tr><tr><td>LSF available servers, CPUs, cores, slots</td><td><em>badmin showstatus</em></td></tr><tr><td>LSF server by status (total number Ok, closed, unreachable, unavailable)</td><td><em>badmin showstatus</em></td></tr><tr><td>LSF job statistics (total number running, suspended, pending)</td><td><em>badmin showstatus</em></td></tr><tr><td>LSF queue statistics (per queue, total number of jobs running, suspended, pending)</td><td><em>bqueues -json -o queue_name:12 njobs pend run susp rsv ususp ssusp</em></td></tr><tr><td>LSF mbatchd process metrics</td><td>(Telegraf - inputs.procstat)</td></tr><tr><td>LSF mbschd process metrics</td><td>(Telegraf - inputs.procstat)</td></tr><tr><td>LSF management lim process metrics</td><td>(Telegraf - inputs.procstat)</td></tr></tbody></table><hr /><p><strong>Scrapin' fun</strong></p><p>These above metrics would give a good idea of the state of the Spectrum LSF cluster at a glance. With the list of metrics prepared, the next step was to create a plugin script which wouldscrape data from the noted commands. Both <em>bqueues</em> and <em>badmin perfmon view</em> support output in JSON format with the appropriate flags specified. However, <em>badmin showstatus</em> does not supportoutput in JSON format. This meant that for <em>badmin showstatus</em> it was necessary to scrape data assuming hard coded field positions in the output.</p><p>A copy of the Telegraf plugin for Spectrum LSF is provided below. This is just an example and is provided &ldquo;as is&rdquo; for testing purposes. Your mileage may vary.</p><hr /><details>  <strong>Example lsf_telegraf_agent.py script. Click to expand!</strong>  <div class=\"highlight\"><pre><code class=\"language-python\"><span style=\"color: #75715e;\">#!/usr/bin/python3.8</span><span style=\"color: #75715e;\"># </span><span style=\"color: #75715e;\"># v0.9 </span><span style=\"color: #75715e;\"># Sample inputs.exec script for Telegraf which outputs metrics from an IBM Spectrum LSF management server</span><span style=\"color: #75715e;\"># in InfluxDB Line Protocol input format.</span><span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># NOTE: It is required to set the lsf_envfile variable to point to the LSF profile.lsf file</span><span style=\"color: #75715e;\"># for the LSF installation. </span><span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># Gabor Samu</span><span style=\"color: #75715e;\"># January 4, 2023</span><span style=\"color: #75715e;\">#</span> <span style=\"color: #f92672;\">import</span> os<span style=\"color: #f92672;\">import</span> json<span style=\"color: #f92672;\">import</span> time<span style=\"color: #f92672;\">import</span> subprocess<span style=\"color: #f92672;\">import</span> sys<span style=\"color: #f92672;\">from</span> pathlib <span style=\"color: #f92672;\">import</span> Path<span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># Variable declarations</span><span style=\"color: #75715e;\"># **NOTE: lsf_envfile needs to be set to point to the profile.lsf file for the LSF installation. </span><span style=\"color: #75715e;\">#</span>lsf_envfile <span style=\"color: #f92672;\">=</span> <span style=\"color: #e6db74;\">\"/opt/ibm/lsfsuite/lsf/conf/profile.lsf\"</span><span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># Source the Spectrum LSF profile.  </span><span style=\"color: #75715e;\"># Check for existing of lsf_envfile (profile.lsf) and source the environment. </span><span style=\"color: #75715e;\"># If the specified file does not exist, then exit.  </span><span style=\"color: #75715e;\">#</span>path <span style=\"color: #f92672;\">=</span> Path(lsf_envfile)<span style=\"color: #66d9ef;\">if</span> path<span style=\"color: #f92672;\">.</span>is_file():     lsf_env <span style=\"color: #f92672;\">=</span> (<span style=\"color: #e6db74;\">f</span><span style=\"color: #e6db74;\">'env -i sh -c \"source </span><span style=\"color: #e6db74;\">{</span>lsf_envfile<span style=\"color: #e6db74;\">}</span><span style=\"color: #e6db74;\"> &amp;&amp; env\"'</span>)    <span style=\"color: #66d9ef;\">for</span> line <span style=\"color: #f92672;\">in</span> subprocess<span style=\"color: #f92672;\">.</span>getoutput(lsf_env)<span style=\"color: #f92672;\">.</span>split(<span style=\"color: #e6db74;\">\"</span><span style=\"color: #ae81ff;\">\\n</span><span style=\"color: #e6db74;\">\"</span>):        key, value <span style=\"color: #f92672;\">=</span> line<span style=\"color: #f92672;\">.</span>split(<span style=\"color: #e6db74;\">\"=\"</span>)        os<span style=\"color: #f92672;\">.</span>environ[key]<span style=\"color: #f92672;\">=</span> value<span style=\"color: #66d9ef;\">else</span>:    sys<span style=\"color: #f92672;\">.</span>exit(<span style=\"color: #e6db74;\">f</span><span style=\"color: #e6db74;\">'The file </span><span style=\"color: #e6db74;\">{</span>lsf_envfile<span style=\"color: #e6db74;\">}</span><span style=\"color: #e6db74;\"> does not exist.'</span>)    <span style=\"color: #75715e;\"># </span><span style=\"color: #75715e;\"># Get the time in nanoseconds since the epoch. </span><span style=\"color: #75715e;\"># This is required as part of the InfluxDB line protocol reference. </span><span style=\"color: #75715e;\"># Only supported on Python 3.7+</span><span style=\"color: #75715e;\">#</span>time_nanosec <span style=\"color: #f92672;\">=</span> time<span style=\"color: #f92672;\">.</span>time_ns()<span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># Here we set the LSF environment variable LSB_NTRIES. This will be used to determine the </span><span style=\"color: #75715e;\"># number of retries before failure of a LSF batch command. This is used to cover the case </span><span style=\"color: #75715e;\"># when the LSF mbatchd is not running. </span><span style=\"color: #75715e;\">#</span>os<span style=\"color: #f92672;\">.</span>environ[<span style=\"color: #e6db74;\">\"LSB_NTRIES\"</span>] <span style=\"color: #f92672;\">=</span> <span style=\"color: #e6db74;\">\"2\"</span><span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># Check if LSF performance metric monitoring is enabled. This is done by running</span><span style=\"color: #75715e;\"># 'badmin perfmon view'. If badmin is not found, then exit. </span><span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># Check the return status from 'badmin perfmon view' and take the appropriate action:</span><span style=\"color: #75715e;\">#  - If return status is 7, it means that performance monitoring is not enabled. The script</span><span style=\"color: #75715e;\">#    will enable LSF performance metric monitoring by running 'badmin perfmon start'.</span><span style=\"color: #75715e;\">#    Note that a 70 second sleep is required before LSF metrics will be available.  </span><span style=\"color: #75715e;\">#  - If return status is 65, it means that the badmin command reported that the</span><span style=\"color: #75715e;\">#    LSF batch system is down. This is a fatal error which will cause the script</span><span style=\"color: #75715e;\">#    to exit. </span><span style=\"color: #75715e;\">#</span>lsf_path <span style=\"color: #f92672;\">=</span> os<span style=\"color: #f92672;\">.</span>environ[<span style=\"color: #e6db74;\">'LSF_BINDIR'</span>]badmin_path <span style=\"color: #f92672;\">=</span> lsf_path <span style=\"color: #f92672;\">+</span> <span style=\"color: #e6db74;\">\"/badmin\"</span>bqueues_path <span style=\"color: #f92672;\">=</span> lsf_path <span style=\"color: #f92672;\">+</span> <span style=\"color: #e6db74;\">\"/bqueues\"</span>path <span style=\"color: #f92672;\">=</span> Path(badmin_path)<span style=\"color: #66d9ef;\">if</span> path<span style=\"color: #f92672;\">.</span>is_file():    cmd <span style=\"color: #f92672;\">=</span> [badmin_path, <span style=\"color: #e6db74;\">'perfmon'</span>, <span style=\"color: #e6db74;\">'view'</span>]    p <span style=\"color: #f92672;\">=</span> subprocess<span style=\"color: #f92672;\">.</span>Popen(cmd, stdout<span style=\"color: #f92672;\">=</span>subprocess<span style=\"color: #f92672;\">.</span>DEVNULL, stderr<span style=\"color: #f92672;\">=</span>subprocess<span style=\"color: #f92672;\">.</span>DEVNULL)    <span style=\"color: #66d9ef;\">while</span> p<span style=\"color: #f92672;\">.</span>poll() <span style=\"color: #f92672;\">is</span> <span style=\"color: #66d9ef;\">None</span>:        time<span style=\"color: #f92672;\">.</span>sleep(<span style=\"color: #ae81ff;\">0.1</span>)    return_code <span style=\"color: #f92672;\">=</span> p<span style=\"color: #f92672;\">.</span>returncode    <span style=\"color: #66d9ef;\">if</span> return_code <span style=\"color: #f92672;\">==</span> <span style=\"color: #ae81ff;\">7</span>:        cmd <span style=\"color: #f92672;\">=</span> [badmin_path, <span style=\"color: #e6db74;\">'perfmon'</span>, <span style=\"color: #e6db74;\">'start'</span>]        p <span style=\"color: #f92672;\">=</span> subprocess<span style=\"color: #f92672;\">.</span>Popen(cmd, stdout<span style=\"color: #f92672;\">=</span>subprocess<span style=\"color: #f92672;\">.</span>DEVNULL, stderr<span style=\"color: #f92672;\">=</span>subprocess<span style=\"color: #f92672;\">.</span>DEVNULL)        <span style=\"color: #66d9ef;\">while</span> p<span style=\"color: #f92672;\">.</span>poll() <span style=\"color: #f92672;\">is</span> <span style=\"color: #66d9ef;\">None</span>:            time<span style=\"color: #f92672;\">.</span>sleep(<span style=\"color: #ae81ff;\">0.1</span>)        return_code <span style=\"color: #f92672;\">=</span> p<span style=\"color: #f92672;\">.</span>returncode        time<span style=\"color: #f92672;\">.</span>sleep(<span style=\"color: #ae81ff;\">70</span>)    <span style=\"color: #66d9ef;\">elif</span> return_code <span style=\"color: #f92672;\">==</span> <span style=\"color: #ae81ff;\">65</span>:        sys<span style=\"color: #f92672;\">.</span>exit(<span style=\"color: #e6db74;\">f</span><span style=\"color: #e6db74;\">'The LSF batch system is down.'</span>)<span style=\"color: #66d9ef;\">else</span>:    sys<span style=\"color: #f92672;\">.</span>exit(<span style=\"color: #e6db74;\">f</span><span style=\"color: #e6db74;\">'</span><span style=\"color: #e6db74;\">{</span>badmin_path<span style=\"color: #e6db74;\">}</span><span style=\"color: #e6db74;\"> does not exist.'</span>)<span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># Run badmin with the \"perfmon view\" keywords and the -json option to product JSON output</span><span style=\"color: #75715e;\"># We assume here that the LSF batch system is responsive (a check was done above); if</span><span style=\"color: #75715e;\"># the mbatchd is very busy there is a possiblity that it may not be responsive here. This</span><span style=\"color: #75715e;\"># case is not considered; LSB_NTRIES setting will determine how many tries are made before</span><span style=\"color: #75715e;\"># badmin gives up the ghost.  </span><span style=\"color: #75715e;\"># </span><span style=\"color: #75715e;\"># Note: We previously checked for the existence of the 'badmin' binary. </span><span style=\"color: #75715e;\">#</span>cmd <span style=\"color: #f92672;\">=</span> [badmin_path, <span style=\"color: #e6db74;\">'perfmon'</span>, <span style=\"color: #e6db74;\">'view'</span>, <span style=\"color: #e6db74;\">'-json'</span>] p <span style=\"color: #f92672;\">=</span> subprocess<span style=\"color: #f92672;\">.</span>Popen(cmd, stdout<span style=\"color: #f92672;\">=</span>subprocess<span style=\"color: #f92672;\">.</span>PIPE, stderr<span style=\"color: #f92672;\">=</span>subprocess<span style=\"color: #f92672;\">.</span>DEVNULL, text<span style=\"color: #f92672;\">=</span><span style=\"color: #66d9ef;\">True</span>) stdout, stderr <span style=\"color: #f92672;\">=</span> p<span style=\"color: #f92672;\">.</span>communicate()<span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># Guard for the case that the performance monitor has just been enabled, but is not</span><span style=\"color: #75715e;\"># producing any data as the first sample period has not elapsed. </span><span style=\"color: #75715e;\">#</span><span style=\"color: #66d9ef;\">if</span> stdout <span style=\"color: #f92672;\">==</span> <span style=\"color: #e6db74;\">\"\"</span>:    sys<span style=\"color: #f92672;\">.</span>exit(<span style=\"color: #e6db74;\">f</span><span style=\"color: #e6db74;\">'Output from badmin perfmon view -json is empty.'</span>)<span style=\"color: #66d9ef;\">else</span>:     data <span style=\"color: #f92672;\">=</span> json<span style=\"color: #f92672;\">.</span>loads(stdout)<span style=\"color: #75715e;\"># </span><span style=\"color: #75715e;\"># Run badmin showstatus</span><span style=\"color: #75715e;\"># Next, run the command 'badmin showstatus' and capture the output. Note that badmin showstatus</span><span style=\"color: #75715e;\"># does not produce JSON output. So here we must do some scraping of the output. </span><span style=\"color: #75715e;\"># The output from 'badmin showstatus' it placed into the array 'showstatus'. The hard coded</span><span style=\"color: #75715e;\"># positions in the output of 'badmin showstatus' are assumed when building the output </span><span style=\"color: #75715e;\"># strings below. Should the format of the output of 'badmin showstatus' change, this will</span><span style=\"color: #75715e;\"># need to be updated. </span>cmd <span style=\"color: #f92672;\">=</span> [badmin_path, <span style=\"color: #e6db74;\">'showstatus'</span>]p <span style=\"color: #f92672;\">=</span> subprocess<span style=\"color: #f92672;\">.</span>Popen(cmd, stdout<span style=\"color: #f92672;\">=</span>subprocess<span style=\"color: #f92672;\">.</span>PIPE, stderr<span style=\"color: #f92672;\">=</span>subprocess<span style=\"color: #f92672;\">.</span>DEVNULL, text<span style=\"color: #f92672;\">=</span><span style=\"color: #66d9ef;\">True</span>)stdout, stderr <span style=\"color: #f92672;\">=</span> p<span style=\"color: #f92672;\">.</span>communicate()<span style=\"color: #75715e;\"># Convert badmin showstatus output into an array</span>showstatus <span style=\"color: #f92672;\">=</span> stdout<span style=\"color: #f92672;\">.</span>split()<span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># Run bqueues</span><span style=\"color: #75715e;\">#</span>cmd <span style=\"color: #f92672;\">=</span> [bqueues_path, <span style=\"color: #e6db74;\">'-json'</span>, <span style=\"color: #e6db74;\">'-o'</span>, <span style=\"color: #e6db74;\">'queue_name:12 njobs pend run susp rsv ususp ssusp'</span>]p <span style=\"color: #f92672;\">=</span> subprocess<span style=\"color: #f92672;\">.</span>Popen(cmd, stdout<span style=\"color: #f92672;\">=</span>subprocess<span style=\"color: #f92672;\">.</span>PIPE, stderr<span style=\"color: #f92672;\">=</span>subprocess<span style=\"color: #f92672;\">.</span>DEVNULL, text<span style=\"color: #f92672;\">=</span><span style=\"color: #66d9ef;\">True</span>)stdout, stderr <span style=\"color: #f92672;\">=</span> p<span style=\"color: #f92672;\">.</span>communicate()data_queues <span style=\"color: #f92672;\">=</span> json<span style=\"color: #f92672;\">.</span>loads(stdout)<span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># At this stage, we've captured the output from 'badmin perfmon view -json' and </span><span style=\"color: #75715e;\"># 'badmin showstatus'. We're now ready to print to standard output the metric</span><span style=\"color: #75715e;\"># strings in InfluxDB line procotol format. </span><span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># Details about the line protocol format can be found here:</span><span style=\"color: #75715e;\"># https://docs.influxdata.com/influxdb/v2.6/reference/syntax/line-protocol/</span><span style=\"color: #75715e;\"># </span><span style=\"color: #75715e;\"># </span><span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># LSF server status</span><span style=\"color: #75715e;\">#</span>print(<span style=\"color: #e6db74;\">\"lsf_servers,\"</span>,<span style=\"color: #e6db74;\">\"status=total\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,showstatus[<span style=\"color: #ae81ff;\">21</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_servers,\"</span>,<span style=\"color: #e6db74;\">\"status=ok\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,showstatus[<span style=\"color: #ae81ff;\">23</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_servers,\"</span>,<span style=\"color: #e6db74;\">\"status=closed\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,showstatus[<span style=\"color: #ae81ff;\">25</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_servers,\"</span>,<span style=\"color: #e6db74;\">\"status=unreachable\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,showstatus[<span style=\"color: #ae81ff;\">27</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_servers,\"</span>,<span style=\"color: #e6db74;\">\"status=unavailable\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,showstatus[<span style=\"color: #ae81ff;\">29</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)<span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># LSF job status</span><span style=\"color: #75715e;\">#</span>print(<span style=\"color: #e6db74;\">\"lsf_jobs,\"</span>,<span style=\"color: #e6db74;\">\"state=total\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,showstatus[<span style=\"color: #ae81ff;\">33</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_jobs,\"</span>,<span style=\"color: #e6db74;\">\"state=running\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,showstatus[<span style=\"color: #ae81ff;\">35</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_jobs,\"</span>,<span style=\"color: #e6db74;\">\"state=suspended\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,showstatus[<span style=\"color: #ae81ff;\">37</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_jobs,\"</span>,<span style=\"color: #e6db74;\">\"state=pending\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,showstatus[<span style=\"color: #ae81ff;\">39</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_jobs,\"</span>,<span style=\"color: #e6db74;\">\"state=finished\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,showstatus[<span style=\"color: #ae81ff;\">41</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)<span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># LSF user stats</span><span style=\"color: #75715e;\">#</span>print(<span style=\"color: #e6db74;\">\"lsf_users,\"</span>,<span style=\"color: #e6db74;\">\"state=numusers\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,showstatus[<span style=\"color: #ae81ff;\">45</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_users,\"</span>,<span style=\"color: #e6db74;\">\"state=numgroups\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,showstatus[<span style=\"color: #ae81ff;\">50</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_users,\"</span>,<span style=\"color: #e6db74;\">\"state=numactive\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,showstatus[<span style=\"color: #ae81ff;\">55</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)<span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># LSF hosts stats</span><span style=\"color: #75715e;\"># First we split out the current and peak values for clients, servers, cpus, cores, and slots.</span><span style=\"color: #75715e;\"># The current and peak values are separated by the \"/\" delimiter.</span><span style=\"color: #75715e;\"># </span>clientssplit <span style=\"color: #f92672;\">=</span> showstatus[<span style=\"color: #ae81ff;\">9</span>]<span style=\"color: #f92672;\">.</span>split(<span style=\"color: #e6db74;\">\"/\"</span>)serverssplit <span style=\"color: #f92672;\">=</span> showstatus[<span style=\"color: #ae81ff;\">11</span>]<span style=\"color: #f92672;\">.</span>split(<span style=\"color: #e6db74;\">\"/\"</span>)cpussplit <span style=\"color: #f92672;\">=</span> showstatus[<span style=\"color: #ae81ff;\">13</span>]<span style=\"color: #f92672;\">.</span>split(<span style=\"color: #e6db74;\">\"/\"</span>)coressplit <span style=\"color: #f92672;\">=</span> showstatus[<span style=\"color: #ae81ff;\">15</span>]<span style=\"color: #f92672;\">.</span>split(<span style=\"color: #e6db74;\">\"/\"</span>)slotssplit <span style=\"color: #f92672;\">=</span> showstatus[<span style=\"color: #ae81ff;\">17</span>]<span style=\"color: #f92672;\">.</span>split(<span style=\"color: #e6db74;\">\"/\"</span>)print(<span style=\"color: #e6db74;\">\"lsf_hosts,\"</span>,<span style=\"color: #e6db74;\">\"state=clients\"</span>,<span style=\"color: #e6db74;\">\" current=\"</span>,clientssplit[<span style=\"color: #ae81ff;\">0</span>],<span style=\"color: #e6db74;\">\"i,\"</span>,<span style=\"color: #e6db74;\">\"peak=\"</span>,clientssplit[<span style=\"color: #ae81ff;\">1</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_hosts,\"</span>,<span style=\"color: #e6db74;\">\"state=servers\"</span>,<span style=\"color: #e6db74;\">\" current=\"</span>,serverssplit[<span style=\"color: #ae81ff;\">0</span>],<span style=\"color: #e6db74;\">\"i,\"</span>,<span style=\"color: #e6db74;\">\"peak=\"</span>,serverssplit[<span style=\"color: #ae81ff;\">1</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_hosts,\"</span>,<span style=\"color: #e6db74;\">\"state=cpus\"</span>,<span style=\"color: #e6db74;\">\" current=\"</span>,cpussplit[<span style=\"color: #ae81ff;\">0</span>],<span style=\"color: #e6db74;\">\"i,\"</span>,<span style=\"color: #e6db74;\">\"peak=\"</span>,cpussplit[<span style=\"color: #ae81ff;\">1</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_hosts,\"</span>,<span style=\"color: #e6db74;\">\"state=cores\"</span>,<span style=\"color: #e6db74;\">\" current=\"</span>,coressplit[<span style=\"color: #ae81ff;\">0</span>],<span style=\"color: #e6db74;\">\"i,\"</span>,<span style=\"color: #e6db74;\">\"peak=\"</span>,coressplit[<span style=\"color: #ae81ff;\">1</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_hosts,\"</span>,<span style=\"color: #e6db74;\">\"state=slots\"</span>,<span style=\"color: #e6db74;\">\" current=\"</span>,slotssplit[<span style=\"color: #ae81ff;\">0</span>],<span style=\"color: #e6db74;\">\"i,\"</span>,<span style=\"color: #e6db74;\">\"peak=\"</span>,slotssplit[<span style=\"color: #ae81ff;\">1</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)<span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># Print mbatchd query metrics</span><span style=\"color: #75715e;\">#</span>print(<span style=\"color: #e6db74;\">\"lsf_mbatchd,\"</span>,<span style=\"color: #e6db74;\">\"query=job\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,data[<span style=\"color: #e6db74;\">'record'</span>][<span style=\"color: #ae81ff;\">1</span>][<span style=\"color: #e6db74;\">'current'</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_mbatchd,\"</span>,<span style=\"color: #e6db74;\">\"query=host\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,data[<span style=\"color: #e6db74;\">'record'</span>][<span style=\"color: #ae81ff;\">2</span>][<span style=\"color: #e6db74;\">'current'</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_mbatchd,\"</span>,<span style=\"color: #e6db74;\">\"query=queue\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,data[<span style=\"color: #e6db74;\">'record'</span>][<span style=\"color: #ae81ff;\">3</span>][<span style=\"color: #e6db74;\">'current'</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)<span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># Print mbatchd job metrics</span><span style=\"color: #75715e;\">#</span>print(<span style=\"color: #e6db74;\">\"lsf_mbatchd,\"</span>,<span style=\"color: #e6db74;\">\"jobs=submitreqs\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,data[<span style=\"color: #e6db74;\">'record'</span>][<span style=\"color: #ae81ff;\">4</span>][<span style=\"color: #e6db74;\">'current'</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_mbatchd,\"</span>,<span style=\"color: #e6db74;\">\"jobs=submitted\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,data[<span style=\"color: #e6db74;\">'record'</span>][<span style=\"color: #ae81ff;\">5</span>][<span style=\"color: #e6db74;\">'current'</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_mbatchd,\"</span>,<span style=\"color: #e6db74;\">\"jobs=dispatched\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,data[<span style=\"color: #e6db74;\">'record'</span>][<span style=\"color: #ae81ff;\">6</span>][<span style=\"color: #e6db74;\">'current'</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_mbatchd,\"</span>,<span style=\"color: #e6db74;\">\"jobs=completed\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,data[<span style=\"color: #e6db74;\">'record'</span>][<span style=\"color: #ae81ff;\">7</span>][<span style=\"color: #e6db74;\">'current'</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_mbatchd,\"</span>,<span style=\"color: #e6db74;\">\"jobs=sentremote\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,data[<span style=\"color: #e6db74;\">'record'</span>][<span style=\"color: #ae81ff;\">8</span>][<span style=\"color: #e6db74;\">'current'</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_mbatchd,\"</span>,<span style=\"color: #e6db74;\">\"jobs=acceptremote\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,data[<span style=\"color: #e6db74;\">'record'</span>][<span style=\"color: #ae81ff;\">9</span>][<span style=\"color: #e6db74;\">'current'</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">'</span><span style=\"color: #e6db74;\">')</span>print(<span style=\"color: #e6db74;\">\"lsf_mbatchd,\"</span>,<span style=\"color: #e6db74;\">\"sched=interval\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,data[<span style=\"color: #e6db74;\">'record'</span>][<span style=\"color: #ae81ff;\">10</span>][<span style=\"color: #e6db74;\">'current'</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_mbatchd,\"</span>,<span style=\"color: #e6db74;\">\"sched=matchhost\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,data[<span style=\"color: #e6db74;\">'record'</span>][<span style=\"color: #ae81ff;\">11</span>][<span style=\"color: #e6db74;\">'current'</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_mbatchd,\"</span>,<span style=\"color: #e6db74;\">\"sched=buckets\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,data[<span style=\"color: #e6db74;\">'record'</span>][<span style=\"color: #ae81ff;\">12</span>][<span style=\"color: #e6db74;\">'current'</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_mbatchd,\"</span>,<span style=\"color: #e6db74;\">\"sched=reordered\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,data[<span style=\"color: #e6db74;\">'record'</span>][<span style=\"color: #ae81ff;\">13</span>][<span style=\"color: #e6db74;\">'current'</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)<span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># Print mbatchd efficiency metrics. Here check if the efficiency metric indicated is \"-\". If so, </span><span style=\"color: #75715e;\"># then assume a zero value. The trailing \"%\" sign on the metrics (percentages) is also stripped here. </span><span style=\"color: #75715e;\">#</span>slots <span style=\"color: #f92672;\">=</span> (data[<span style=\"color: #e6db74;\">'record'</span>][<span style=\"color: #ae81ff;\">14</span>][<span style=\"color: #e6db74;\">'current'</span>])slots_percent <span style=\"color: #f92672;\">=</span> slots<span style=\"color: #66d9ef;\">if</span> slots_percent <span style=\"color: #f92672;\">==</span> <span style=\"color: #e6db74;\">\"-\"</span>:    slots_percent <span style=\"color: #f92672;\">=</span> <span style=\"color: #e6db74;\">\"0\"</span><span style=\"color: #66d9ef;\">elif</span> slots_percent <span style=\"color: #f92672;\">!=</span> <span style=\"color: #e6db74;\">\"0\"</span>:    <span style=\"color: #75715e;\"># Strip % sign and decimal. This is to work around issue inserting float to InfluxDB</span>    <span style=\"color: #75715e;\"># \"type float, already exists as type integer dropped ...\"</span>    slots_percent <span style=\"color: #f92672;\">=</span> slots[:<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">4</span>]memory <span style=\"color: #f92672;\">=</span> (data[<span style=\"color: #e6db74;\">'record'</span>][<span style=\"color: #ae81ff;\">15</span>][<span style=\"color: #e6db74;\">'current'</span>])memory_percent <span style=\"color: #f92672;\">=</span> memory<span style=\"color: #66d9ef;\">if</span> memory_percent <span style=\"color: #f92672;\">==</span> <span style=\"color: #e6db74;\">\"-\"</span>:    memory_percent <span style=\"color: #f92672;\">=</span> <span style=\"color: #e6db74;\">\"0\"</span><span style=\"color: #66d9ef;\">elif</span> memory_percent <span style=\"color: #f92672;\">!=</span> <span style=\"color: #e6db74;\">\"0\"</span>:    <span style=\"color: #75715e;\"># Strip % sign and decimal. This is to work around issue inserting float to InfluxDB</span>    <span style=\"color: #75715e;\"># \"type float, already exists as type integer dropped ...\"</span>    memory_percent <span style=\"color: #f92672;\">=</span> memory[:<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">4</span>]print(<span style=\"color: #e6db74;\">\"lsf_mbatchd,\"</span>,<span style=\"color: #e6db74;\">\"utilization=slots\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,slots_percent,<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_mbatchd,\"</span>,<span style=\"color: #e6db74;\">\"utilization=memory\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,memory_percent,<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)<span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># Print mbatchd file descriptor usage</span><span style=\"color: #75715e;\">#</span>print(<span style=\"color: #e6db74;\">\"lsf_mbatchd,\"</span>,<span style=\"color: #e6db74;\">\"fd=free\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,data[<span style=\"color: #e6db74;\">'fd'</span>][<span style=\"color: #e6db74;\">'free'</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_mbatchd,\"</span>,<span style=\"color: #e6db74;\">\"fd=used\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,data[<span style=\"color: #e6db74;\">'fd'</span>][<span style=\"color: #e6db74;\">'used'</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_mbatchd,\"</span>,<span style=\"color: #e6db74;\">\"fd=total\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,data[<span style=\"color: #e6db74;\">'fd'</span>][<span style=\"color: #e6db74;\">'total'</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)<span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># Print LSF queue status (njobs)</span><span style=\"color: #75715e;\">#</span>iterations <span style=\"color: #f92672;\">=</span> data_queues[<span style=\"color: #e6db74;\">\"QUEUES\"</span>]<span style=\"color: #66d9ef;\">for</span> n <span style=\"color: #f92672;\">in</span> range(iterations):    print(<span style=\"color: #e6db74;\">\"lsf_queues,\"</span>,<span style=\"color: #e6db74;\">\"name=\"</span>, data_queues[<span style=\"color: #e6db74;\">'RECORDS'</span>][n][<span style=\"color: #e6db74;\">'QUEUE_NAME'</span>], <span style=\"color: #e6db74;\">\" njobs=\"</span>, data_queues[<span style=\"color: #e6db74;\">'RECOR</span>DS<span style=\"color: #e6db74;\">'][n]['</span>NJOBS<span style=\"color: #e6db74;\">'],\"i,\",</span>          <span style=\"color: #e6db74;\">\"pend=\"</span>, data_queues[<span style=\"color: #e6db74;\">'RECORDS'</span>][n][<span style=\"color: #e6db74;\">'PEND'</span>],<span style=\"color: #e6db74;\">\"i,\"</span>,          <span style=\"color: #e6db74;\">\"run=\"</span>, data_queues[<span style=\"color: #e6db74;\">'RECORDS'</span>][n][<span style=\"color: #e6db74;\">'RUN'</span>],<span style=\"color: #e6db74;\">\"i,\"</span>,          <span style=\"color: #e6db74;\">\"susp=\"</span>, data_queues[<span style=\"color: #e6db74;\">'RECORDS'</span>][n][<span style=\"color: #e6db74;\">'SUSP'</span>],<span style=\"color: #e6db74;\">\"i,\"</span>,          <span style=\"color: #e6db74;\">\"rsv=\"</span>, data_queues[<span style=\"color: #e6db74;\">'RECORDS'</span>][n][<span style=\"color: #e6db74;\">'RSV'</span>],<span style=\"color: #e6db74;\">\"i,\"</span>,          <span style=\"color: #e6db74;\">\"ususp=\"</span>, data_queues[<span style=\"color: #e6db74;\">'RECORDS'</span>][n][<span style=\"color: #e6db74;\">'USUSP'</span>],<span style=\"color: #e6db74;\">\"i,\"</span>,          <span style=\"color: #e6db74;\">\"ssusp=\"</span>, data_queues[<span style=\"color: #e6db74;\">'RECORDS'</span>][n][<span style=\"color: #e6db74;\">'SSUSP'</span>],<span style=\"color: #e6db74;\">\"i \"</span>,          time_nanosec, sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)exit()    </code></pre></div></details><hr /><p><strong>Bringing it all together</strong></p><p>For completeness, below is the detail regarding the configuration of the environment. It should be noted that the simple test environment consists of a single server running IBMSpectrum LSF Suite for HPC and a separate server which runs the InfluxDB instance.</p><hr /><table><thead><tr><th style=\"text-align: left;\">Hostname</th><th>Component</th><th>Version</th></tr></thead><tbody><tr><td style=\"text-align: left;\"><em>kilenc</em></td><td>OS (LSF mgmt server)</td><td><em>CentOS Stream release 8 (ppc64le)</em></td></tr><tr><td style=\"text-align: left;\"><em>kilenc</em></td><td>Spectrum LSF Suite for HPC</td><td><em>v10.2.0.13</em></td></tr><tr><td style=\"text-align: left;\"><em>adatbazis</em></td><td>OS (InfluxDB server)</td><td><em>Fedora release 36 (aarch64)</em></td></tr><tr><td style=\"text-align: left;\"><em>adatbazis</em></td><td>InfluxDB</td><td><em>v1.8.10</em></td></tr><tr><td style=\"text-align: left;\"><em>kilenc</em></td><td>Telegraf</td><td><em>v1.24.3</em></td></tr><tr><td style=\"text-align: left;\"><em>kilenc</em></td><td>Grafana</td><td><em>v9.1.6</em></td></tr></tbody></table><hr /><p>The follwing steps assume that IBM Spectrum LSF Suite for HPC, InfluxDB and Telegraf have been installed.</p><ol><li><p>Start InfluxDB on the host <em>adatbazis</em></p></li><li><p>On the LSF management server <em>kilenc</em>, configure telegraf to connect to the influxDB instance on host <em>adatbazis</em>. Edit the configuration <em>/etc/telegraf/telegraf.conf</em> and specifythe correct URL in the <em>outputs.influxdb</em> section as follows:</p></li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\"># # Configuration for sending metrics to InfluxDB[[outputs.influxdb]]#   ## The full HTTP or UDP URL for your InfluxDB instance.#   ###   ## Multiple URLs can be specified for a single cluster, only ONE of the#   ## urls will be written to each interval.#   # urls = [\"unix:///var/run/influxdb.sock\"]#   # urls = [\"udp://127.0.0.1:8089\"]#   # urls = [\"http://127.0.0.1:8086\"]# Added gsamu Jan 04 2023urls = [\"http://adatbazis:8086\"]</code></pre></div><ol start=\"3\"><li>On the LSF management server <em>kilenc</em>, configure telegraf with the custom plugin script <em>lsf_telegraf_agent_0.9.py</em> to collect and log metrics from IBM Spectrum LSF Suite for HPC.Edit the configuration <em>/etc/telegraf/telegraf.conf</em> and specify the correct command path in the section <em>inputs.exec</em>. Additionally, set <em>data_format</em> equal to <em>influx</em>.Note that thescript <em>lsf_telegraf_agent_0.9.py</em> was copied to the directory <em>/etc/telegraf/telegraf.d/scripts</em> with permissions octal 755 and owner set to user <em>telegraf</em>.<strong>Note:</strong> User <em>telegraf</em> was automatically created during the installation of telegraf.</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\"> # ## Gather LSF metrics[[inputs.exec]]  ## Commands array   commands = [  \"/etc/telegraf/telegraf.d/scripts/lsf_telegraf_agent_0.9.py\" ]   timeout = \"30s\"   interval = \"30s\"   data_format = \"influx\" # ## End LSF metrics</code></pre></div><ol start=\"4\"><li>Telegraf provides the ability to collect metrics on processes. Here we&rsquo;ll use the telegraf <em>procstat</em> facility to monitor the LSF mbatchd and mbschd processes. These are the keydaemons involved in handling query requests and making scheduling decisions for jobs in the environment. Edit the configuration <em>/etc/telegraf/telegraf.conf</em> and configure the twofollowing <em>inputs.procstat</em> sections.</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\"># ## Monitor CPU and memory utilization for LSF processes# ## mbatchd, mbschd, lim (manager)[[inputs.procstat]]exe = \"lim\"pattern = \"lim\"pid_finder = \"pgrep\"[[inputs.procstat]]exe = \"mbschd\"pattern = \"mbschd\"pid_finder = \"pgrep\"[[inputs.procstat]]exe = \"mbatchd\"pattern = \"mbatchd\"pid_finder = \"pgrep\"</code></pre></div><ol start=\"5\"><li>With the configuration to telegraf complete, it&rsquo;s now time to test if the configuration and custom LSF agent is functioning as expected. Note that the following operation is performedon the LSF management candidate host <em>kilenc</em> and assumes that the LSF daemons are up and running. This is achieve by running the command:<em>telegraf &ndash;config /etc/telegraf/telegraf.conf &ndash;test</em>. <strong>Note:</strong> Any errors in the configuration file <em>/etc/telegraf/telegraf.conf</em> will result in errors in the output.</li></ol><hr /><details>  <strong>Output of <em>telegraf &ndash;config /etc/telegraf/telegraf.conf &ndash;test</em>. Click to expand!</strong>  <div class=\"highlight\"><pre><code class=\"language-python\">[root<span style=\"color: #a6e22e;\">@kilenc</span> telegraf]<span style=\"color: #75715e;\"># pwd</span><span style=\"color: #f92672;\">/</span>etc<span style=\"color: #f92672;\">/</span>telegraf[root<span style=\"color: #a6e22e;\">@kilenc</span> telegraf]<span style=\"color: #75715e;\"># telegraf --config /etc/telegraf/telegraf.conf --test</span><span style=\"color: #f92672;\">&gt;</span> mem,host<span style=\"color: #f92672;\">=</span>kilenc active<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1938817024</span>i,available<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">6820003840</span>i,available_percent<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">20.653390597462806</span>,buffered<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4849664</span>i,cached<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">6317735936</span>i,commit_limit<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">33560395776</span>i,committed_as<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">18635292672</span>i,dirty<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4128768</span>i,free<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2623799296</span>i,high_free<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,high_total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,huge_page_size<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2097152</span>i,huge_pages_free<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,huge_pages_total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,inactive<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">13852016640</span>i,low_free<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,low_total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,mapped<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1007353856</span>i,page_tables<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">22478848</span>i,shared<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">259063808</span>i,slab<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4946919424</span>i,sreclaimable<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">902234112</span>i,sunreclaim<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4044685312</span>i,swap_cached<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">3866624</span>i,swap_free<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">16994729984</span>i,swap_total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">17049780224</span>i,total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">33021231104</span>i,used<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">24074846208</span>i,used_percent<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">72.90717336424115</span>,vmalloc_chunk<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,vmalloc_total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">562949953421312</span>i,vmalloc_used<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,write_back<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,write_back_tmp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> kernel,host<span style=\"color: #f92672;\">=</span>kilenc boot_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1673790850</span>i,context_switches<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1943864437</span>i,entropy_avail<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4037</span>i,interrupts<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1294179599</span>i,processes_forked<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4255316</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> swap,host<span style=\"color: #f92672;\">=</span>kilenc free<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">16994729984</span>i,total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">17049780224</span>i,used<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">55050240</span>i,used_percent<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0.3228794698626609</span> <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> swap,host<span style=\"color: #f92672;\">=</span>kilenc <span style=\"color: #f92672;\">in</span><span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">172032</span>i,out<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">851968</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> net,host<span style=\"color: #f92672;\">=</span>kilenc,interface<span style=\"color: #f92672;\">=</span>lo bytes_recv<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">90039931116</span>i,bytes_sent<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">90039931116</span>i,drop_in<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,drop_out<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,err_in<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,err_out<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,packets_recv<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">17245997</span>i,packets_sent<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">17245997</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> net,host<span style=\"color: #f92672;\">=</span>kilenc,interface<span style=\"color: #f92672;\">=</span>enP4p1s0f0 bytes_recv<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,bytes_sent<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,drop_in<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,drop_out<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,err_in<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,err_out<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,packets_recv<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,packets_sent<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> net,host<span style=\"color: #f92672;\">=</span>kilenc,interface<span style=\"color: #f92672;\">=</span>enP4p1s0f1 bytes_recv<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">11791041280</span>i,bytes_sent<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1701152001</span>i,drop_in<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,drop_out<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,err_in<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,err_out<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,packets_recv<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">10322276</span>i,packets_sent<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4594948</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> net,host<span style=\"color: #f92672;\">=</span>kilenc,interface<span style=\"color: #f92672;\">=</span>all icmp_inaddrmaskreps<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,icmp_inaddrmasks<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,icmp_incsumerrors<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,icmp_indestunreachs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">8609</span>i,icmp_inechoreps<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">20</span>i,icmp_inechos<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">11</span>i,icmp_inerrors<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1084</span>i,icmp_inmsgs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">8640</span>i,icmp_inparmprobs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,icmp_inredirects<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,icmp_insrcquenchs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,icmp_intimeexcds<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,icmp_intimestampreps<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,icmp_intimestamps<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,icmp_outaddrmaskreps<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,icmp_outaddrmasks<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,icmp_outdestunreachs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4805</span>i,icmp_outechoreps<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">11</span>i,icmp_outechos<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">94</span>i,icmp_outerrors<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,icmp_outmsgs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4910</span>i,icmp_outparmprobs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,icmp_outredirects<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,icmp_outsrcquenchs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,icmp_outtimeexcds<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,icmp_outtimestampreps<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,icmp_outtimestamps<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,icmpmsg_intype0<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">20</span>i,icmpmsg_intype3<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">8609</span>i,icmpmsg_intype8<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">11</span>i,icmpmsg_outtype0<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">11</span>i,icmpmsg_outtype3<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4805</span>i,icmpmsg_outtype8<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">94</span>i,ip_defaultttl<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">64</span>i,ip_forwarding<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1</span>i,ip_forwdatagrams<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ip_fragcreates<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">62958</span>i,ip_fragfails<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ip_fragoks<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">12611</span>i,ip_inaddrerrors<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1</span>i,ip_indelivers<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">21324370</span>i,ip_indiscards<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ip_inhdrerrors<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ip_inreceives<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">21324371</span>i,ip_inunknownprotos<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ip_outdiscards<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ip_outnoroutes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">30</span>i,ip_outrequests<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">21248264</span>i,ip_reasmfails<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ip_reasmoks<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ip_reasmreqds<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ip_reasmtimeout<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,tcp_activeopens<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">763497</span>i,tcp_attemptfails<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">96617</span>i,tcp_currestab<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">118</span>i,tcp_estabresets<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1917</span>i,tcp_incsumerrors<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,tcp_inerrs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,tcp_insegs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">19488475</span>i,tcp_maxconn<span style=\"color: #f92672;\">=-</span><span style=\"color: #ae81ff;\">1</span>i,tcp_outrsts<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">137188</span>i,tcp_outsegs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">20220038</span>i,tcp_passiveopens<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">675805</span>i,tcp_retranssegs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9827</span>i,tcp_rtoalgorithm<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1</span>i,tcp_rtomax<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">120000</span>i,tcp_rtomin<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">200</span>i,udp_ignoredmulti<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">10509</span>i,udp_incsumerrors<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,udp_indatagrams<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1816997</span>i,udp_inerrors<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,udp_memerrors<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,udp_noports<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">264</span>i,udp_outdatagrams<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1506724</span>i,udp_rcvbuferrors<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,udp_sndbuferrors<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,udplite_ignoredmulti<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,udplite_incsumerrors<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,udplite_indatagrams<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,udplite_inerrors<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,udplite_memerrors<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,udplite_noports<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,udplite_outdatagrams<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,udplite_rcvbuferrors<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,udplite_sndbuferrors<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> diskio,host<span style=\"color: #f92672;\">=</span>kilenc,name<span style=\"color: #f92672;\">=</span>dm<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span> io_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9739370</span>i,iops_in_progress<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,merged_reads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,merged_writes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,read_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4015612416</span>i,read_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">604060</span>i,reads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">40592</span>i,weighted_io_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">60563370</span>i,write_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">47025459712</span>i,write_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">59959310</span>i,writes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1079691</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> diskio,host<span style=\"color: #f92672;\">=</span>kilenc,name<span style=\"color: #f92672;\">=</span>sda1 io_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1460</span>i,iops_in_progress<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,merged_reads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,merged_writes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,read_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4849664</span>i,read_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1304</span>i,reads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">67</span>i,weighted_io_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1304</span>i,write_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,write_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,writes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> diskio,host<span style=\"color: #f92672;\">=</span>kilenc,name<span style=\"color: #f92672;\">=</span>sda3 io_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">45872430</span>i,iops_in_progress<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,merged_reads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">623</span>i,merged_writes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1061314</span>i,read_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">16398521856</span>i,read_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">3371612</span>i,reads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">139298</span>i,weighted_io_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">311521720</span>i,write_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">133715422208</span>i,write_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">308150107</span>i,writes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">7031512</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> diskio,host<span style=\"color: #f92672;\">=</span>kilenc,name<span style=\"color: #f92672;\">=</span>dm<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span> io_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">5780</span>i,iops_in_progress<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,merged_reads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,merged_writes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,read_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">5636096</span>i,read_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">3030</span>i,reads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">81</span>i,weighted_io_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">26500</span>i,write_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">13631488</span>i,write_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">23470</span>i,writes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">208</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> disk,device<span style=\"color: #f92672;\">=</span>dm<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">0</span>,fstype<span style=\"color: #f92672;\">=</span>xfs,host<span style=\"color: #f92672;\">=</span>kilenc,mode<span style=\"color: #f92672;\">=</span>rw,path<span style=\"color: #f92672;\">=/</span> free<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9315028992</span>i,inodes_free<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">18214222</span>i,inodes_total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">19822888</span>i,inodes_used<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1608666</span>i,total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">53660876800</span>i,used<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">44345847808</span>i,used_percent<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">82.64093032486566</span> <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> disk,device<span style=\"color: #f92672;\">=</span>sda2,fstype<span style=\"color: #f92672;\">=</span>ext4,host<span style=\"color: #f92672;\">=</span>kilenc,mode<span style=\"color: #f92672;\">=</span>rw,path<span style=\"color: #f92672;\">=/</span>boot free<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">309653504</span>i,inodes_free<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">65264</span>i,inodes_total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">65536</span>i,inodes_used<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">272</span>i,total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1020702720</span>i,used<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">640585728</span>i,used_percent<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">67.41310045173972</span> <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> disk,device<span style=\"color: #f92672;\">=</span>dm<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span>,fstype<span style=\"color: #f92672;\">=</span>xfs,host<span style=\"color: #f92672;\">=</span>kilenc,mode<span style=\"color: #f92672;\">=</span>rw,path<span style=\"color: #f92672;\">=/</span>home free<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">856442515456</span>i,inodes_free<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">452529686</span>i,inodes_total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">453312512</span>i,inodes_used<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">782826</span>i,total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">927930712064</span>i,used<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">71488196608</span>i,used_percent<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">7.704044674735306</span> <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> disk,device<span style=\"color: #f92672;\">=</span>dm<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span>,fstype<span style=\"color: #f92672;\">=</span>xfs,host<span style=\"color: #f92672;\">=</span>kilenc,mode<span style=\"color: #f92672;\">=</span>rw,path<span style=\"color: #f92672;\">=/</span>home<span style=\"color: #f92672;\">/</span>opt<span style=\"color: #f92672;\">/</span>at13<span style=\"color: #ae81ff;\">.0</span><span style=\"color: #f92672;\">/</span>lib free<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">856442515456</span>i,inodes_free<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">452529686</span>i,inodes_total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">453312512</span>i,inodes_used<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">782826</span>i,total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">927930712064</span>i,used<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">71488196608</span>i,used_percent<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">7.704044674735306</span> <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> disk,device<span style=\"color: #f92672;\">=</span>dm<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span>,fstype<span style=\"color: #f92672;\">=</span>xfs,host<span style=\"color: #f92672;\">=</span>kilenc,mode<span style=\"color: #f92672;\">=</span>rw,path<span style=\"color: #f92672;\">=/</span>home<span style=\"color: #f92672;\">/</span>opt<span style=\"color: #f92672;\">/</span>at13<span style=\"color: #ae81ff;\">.0</span><span style=\"color: #f92672;\">/</span>lib64 free<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">856442515456</span>i,inodes_free<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">452529686</span>i,inodes_total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">453312512</span>i,inodes_used<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">782826</span>i,total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">927930712064</span>i,used<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">71488196608</span>i,used_percent<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">7.704044674735306</span> <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> disk,device<span style=\"color: #f92672;\">=</span>ST31000524AS<span style=\"color: #f92672;\">/</span>raktar,fstype<span style=\"color: #f92672;\">=</span>zfs,host<span style=\"color: #f92672;\">=</span>kilenc,mode<span style=\"color: #f92672;\">=</span>rw,path<span style=\"color: #f92672;\">=/</span>mnt<span style=\"color: #f92672;\">/</span>ST31000524AS free<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">210837438464</span>i,inodes_free<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">411792117</span>i,inodes_total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">412304487</span>i,inodes_used<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">512370</span>i,total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">965496143872</span>i,used<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">754658705408</span>i,used_percent<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">78.16278813725106</span> <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> diskio,host<span style=\"color: #f92672;\">=</span>kilenc,name<span style=\"color: #f92672;\">=</span>sda io_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">45899860</span>i,iops_in_progress<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,merged_reads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">650</span>i,merged_writes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1061332</span>i,read_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">16495536128</span>i,read_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">3440899</span>i,reads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">141325</span>i,weighted_io_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">311596362</span>i,write_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">133715696640</span>i,write_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">308155462</span>i,writes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">7031531</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> disk,device<span style=\"color: #f92672;\">=</span>ST31000524AS,fstype<span style=\"color: #f92672;\">=</span>zfs,host<span style=\"color: #f92672;\">=</span>kilenc,mode<span style=\"color: #f92672;\">=</span>rw,path<span style=\"color: #f92672;\">=/</span>ST31000524AS free<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">210837438464</span>i,inodes_free<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">411792117</span>i,inodes_total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">411792123</span>i,inodes_used<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">6</span>i,total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">210837569536</span>i,used<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">131072</span>i,used_percent<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0.00006216728844316324</span> <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> diskio,host<span style=\"color: #f92672;\">=</span>kilenc,name<span style=\"color: #f92672;\">=</span>sda2 io_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">18060</span>i,iops_in_progress<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,merged_reads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">27</span>i,merged_writes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">18</span>i,read_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">88372224</span>i,read_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">31224</span>i,reads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">436</span>i,weighted_io_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">36579</span>i,write_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">274432</span>i,write_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">5355</span>i,writes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">19</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> diskio,host<span style=\"color: #f92672;\">=</span>kilenc,name<span style=\"color: #f92672;\">=</span>dm<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">0</span> io_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">38788720</span>i,iops_in_progress<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,merged_reads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,merged_writes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,read_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">12341294080</span>i,read_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1143210</span>i,reads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">51814</span>i,weighted_io_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">303329620</span>i,write_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">86676331008</span>i,write_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">302186410</span>i,writes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">6798400</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> diskio,host<span style=\"color: #f92672;\">=</span>kilenc,name<span style=\"color: #f92672;\">=</span>sdb io_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">668810</span>i,iops_in_progress<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,merged_reads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9</span>i,merged_writes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">58</span>i,read_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">104550912</span>i,read_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">746540</span>i,reads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">31054</span>i,weighted_io_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1445858</span>i,write_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">10845920256</span>i,write_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">699318</span>i,writes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">124780</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> diskio,host<span style=\"color: #f92672;\">=</span>kilenc,name<span style=\"color: #f92672;\">=</span>sdb1 io_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">341330</span>i,iops_in_progress<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,merged_reads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,merged_writes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">58</span>i,read_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">95562240</span>i,read_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">383066</span>i,reads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">25026</span>i,weighted_io_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1082385</span>i,write_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">10845920256</span>i,write_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">699318</span>i,writes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">124780</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> diskio,host<span style=\"color: #f92672;\">=</span>kilenc,name<span style=\"color: #f92672;\">=</span>sdb9 io_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">190</span>i,iops_in_progress<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,merged_reads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,merged_writes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,read_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4980736</span>i,read_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">37</span>i,reads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">69</span>i,weighted_io_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">37</span>i,write_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,write_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,writes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> system,host<span style=\"color: #f92672;\">=</span>kilenc load1<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2.06</span>,load15<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2.12</span>,load5<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2.12</span>,n_cpus<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">32</span>i,n_users<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> system,host<span style=\"color: #f92672;\">=</span>kilenc uptime<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">456127</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> system,host<span style=\"color: #f92672;\">=</span>kilenc uptime_format<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">\"5 days,  6:42\"</span> <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> processes,host<span style=\"color: #f92672;\">=</span>kilenc blocked<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1</span>i,dead<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">569</span>i,paging<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,parked<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1</span>i,running<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,sleeping<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">412</span>i,stopped<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1366</span>i,total_threads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2683</span>i,unknown<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,zombies<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_servers,host<span style=\"color: #f92672;\">=</span>kilenc,status<span style=\"color: #f92672;\">=</span>total value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_servers,host<span style=\"color: #f92672;\">=</span>kilenc,status<span style=\"color: #f92672;\">=</span>ok value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_servers,host<span style=\"color: #f92672;\">=</span>kilenc,status<span style=\"color: #f92672;\">=</span>closed value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_servers,host<span style=\"color: #f92672;\">=</span>kilenc,status<span style=\"color: #f92672;\">=</span>unreachable value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_servers,host<span style=\"color: #f92672;\">=</span>kilenc,status<span style=\"color: #f92672;\">=</span>unavailable value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_jobs,host<span style=\"color: #f92672;\">=</span>kilenc,state<span style=\"color: #f92672;\">=</span>total value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">121776</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_jobs,host<span style=\"color: #f92672;\">=</span>kilenc,state<span style=\"color: #f92672;\">=</span>running value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">32</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_jobs,host<span style=\"color: #f92672;\">=</span>kilenc,state<span style=\"color: #f92672;\">=</span>suspended value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_jobs,host<span style=\"color: #f92672;\">=</span>kilenc,state<span style=\"color: #f92672;\">=</span>pending value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">120771</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_jobs,host<span style=\"color: #f92672;\">=</span>kilenc,state<span style=\"color: #f92672;\">=</span>finished value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">973</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_users,host<span style=\"color: #f92672;\">=</span>kilenc,state<span style=\"color: #f92672;\">=</span>numusers value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_users,host<span style=\"color: #f92672;\">=</span>kilenc,state<span style=\"color: #f92672;\">=</span>numgroups value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_users,host<span style=\"color: #f92672;\">=</span>kilenc,state<span style=\"color: #f92672;\">=</span>numactive value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_hosts,host<span style=\"color: #f92672;\">=</span>kilenc,state<span style=\"color: #f92672;\">=</span>clients current<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,peak<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_hosts,host<span style=\"color: #f92672;\">=</span>kilenc,state<span style=\"color: #f92672;\">=</span>servers current<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1</span>i,peak<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_hosts,host<span style=\"color: #f92672;\">=</span>kilenc,state<span style=\"color: #f92672;\">=</span>cpus current<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2</span>i,peak<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_hosts,host<span style=\"color: #f92672;\">=</span>kilenc,state<span style=\"color: #f92672;\">=</span>cores current<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">32</span>i,peak<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">32</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_hosts,host<span style=\"color: #f92672;\">=</span>kilenc,state<span style=\"color: #f92672;\">=</span>slots current<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">32</span>i,peak<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">32</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_mbatchd,host<span style=\"color: #f92672;\">=</span>kilenc,query<span style=\"color: #f92672;\">=</span>job value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_mbatchd,host<span style=\"color: #f92672;\">=</span>kilenc,query<span style=\"color: #f92672;\">=</span>host value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_mbatchd,host<span style=\"color: #f92672;\">=</span>kilenc,query<span style=\"color: #f92672;\">=</span>queue value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_mbatchd,host<span style=\"color: #f92672;\">=</span>kilenc,jobs<span style=\"color: #f92672;\">=</span>submitreqs value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_mbatchd,host<span style=\"color: #f92672;\">=</span>kilenc,jobs<span style=\"color: #f92672;\">=</span>submitted value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_mbatchd,host<span style=\"color: #f92672;\">=</span>kilenc,jobs<span style=\"color: #f92672;\">=</span>dispatched value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">19</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_mbatchd,host<span style=\"color: #f92672;\">=</span>kilenc,jobs<span style=\"color: #f92672;\">=</span>completed value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">12</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_mbatchd,host<span style=\"color: #f92672;\">=</span>kilenc,jobs<span style=\"color: #f92672;\">=</span>sentremote value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_mbatchd,host<span style=\"color: #f92672;\">=</span>kilenc,jobs<span style=\"color: #f92672;\">=</span>acceptremote value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_mbatchd,host<span style=\"color: #f92672;\">=</span>kilenc,sched<span style=\"color: #f92672;\">=</span>interval value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_mbatchd,host<span style=\"color: #f92672;\">=</span>kilenc,sched<span style=\"color: #f92672;\">=</span>matchhost value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">5</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_mbatchd,host<span style=\"color: #f92672;\">=</span>kilenc,sched<span style=\"color: #f92672;\">=</span>buckets value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">5</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_mbatchd,host<span style=\"color: #f92672;\">=</span>kilenc,sched<span style=\"color: #f92672;\">=</span>reordered value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">7</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_mbatchd,host<span style=\"color: #f92672;\">=</span>kilenc,utilization<span style=\"color: #f92672;\">=</span>slots value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">100</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_mbatchd,host<span style=\"color: #f92672;\">=</span>kilenc,utilization<span style=\"color: #f92672;\">=</span>memory value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_mbatchd,fd<span style=\"color: #f92672;\">=</span>free,host<span style=\"color: #f92672;\">=</span>kilenc value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">65509</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_mbatchd,fd<span style=\"color: #f92672;\">=</span>used,host<span style=\"color: #f92672;\">=</span>kilenc value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">26</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_mbatchd,fd<span style=\"color: #f92672;\">=</span>total,host<span style=\"color: #f92672;\">=</span>kilenc value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">65535</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_queues,host<span style=\"color: #f92672;\">=</span>kilenc,name<span style=\"color: #f92672;\">=</span>admin njobs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,pend<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rsv<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,run<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ssusp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,susp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ususp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_queues,host<span style=\"color: #f92672;\">=</span>kilenc,name<span style=\"color: #f92672;\">=</span>owners njobs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,pend<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rsv<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,run<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ssusp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,susp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ususp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_queues,host<span style=\"color: #f92672;\">=</span>kilenc,name<span style=\"color: #f92672;\">=</span>priority njobs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">93951</span>i,pend<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">93923</span>i,rsv<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,run<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">28</span>i,ssusp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,susp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ususp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_queues,host<span style=\"color: #f92672;\">=</span>kilenc,name<span style=\"color: #f92672;\">=</span>night njobs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,pend<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rsv<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,run<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ssusp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,susp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ususp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_queues,host<span style=\"color: #f92672;\">=</span>kilenc,name<span style=\"color: #f92672;\">=</span>short njobs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2504</span>i,pend<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2504</span>i,rsv<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,run<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ssusp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,susp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ususp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_queues,host<span style=\"color: #f92672;\">=</span>kilenc,name<span style=\"color: #f92672;\">=</span>dataq njobs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,pend<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rsv<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,run<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ssusp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,susp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ususp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_queues,host<span style=\"color: #f92672;\">=</span>kilenc,name<span style=\"color: #f92672;\">=</span>normal njobs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1750</span>i,pend<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1750</span>i,rsv<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,run<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ssusp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,susp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ususp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_queues,host<span style=\"color: #f92672;\">=</span>kilenc,name<span style=\"color: #f92672;\">=</span>interactive njobs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,pend<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rsv<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,run<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ssusp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,susp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ususp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_queues,host<span style=\"color: #f92672;\">=</span>kilenc,name<span style=\"color: #f92672;\">=</span>sendq njobs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">22598</span>i,pend<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">22594</span>i,rsv<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,run<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4</span>i,ssusp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,susp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ususp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_queues,host<span style=\"color: #f92672;\">=</span>kilenc,name<span style=\"color: #f92672;\">=</span>idle njobs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,pend<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rsv<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,run<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ssusp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,susp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ususp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu0,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">100</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu4,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">100</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu8,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">100</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu12,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">100</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu16,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">98.03921568448419</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1.9607843137324836</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu20,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">100</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu24,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">100</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu28,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">100</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu32,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">100</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu36,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">100</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu40,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">98.03921568448419</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1.9607843136879006</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu44,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">100</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu48,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">100</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu52,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">100</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu56,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">100</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu60,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">100</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu64,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">87.99999999906868</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">10.000000001155058</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2.0000000002764864</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu68,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">100</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu72,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">86.27450980280263</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">11.764705882127403</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1.9607843137324836</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu76,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">100</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu80,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">92.30769231113655</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">3.8461538464431086</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">3.84615384653056</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu84,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">94.11764706486585</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">5.882352941197451</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu88,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">100</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu92,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">70.58823529344627</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">29.411764701983955</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu96,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">96.15384615040192</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">3.8461538460125784</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu100,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">97.99999999813735</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1.999999999998181</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu104,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">96.07843137993407</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">3.92156862782338</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu108,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">96.07843136896838</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1.9607843136879006</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1.9607843137324836</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu112,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">100</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu116,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">95.91836734305988</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4.08163265313509</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu120,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">84.61538461280144</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">3.8461538460344413</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">11.53846153830009</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu124,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">100</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu<span style=\"color: #f92672;\">-</span>total,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">93.47826086554115</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">3.1055900618243673</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2.484472049468532</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0.9316770186919254</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> procstat,exe<span style=\"color: #f92672;\">=</span>mbatchd,host<span style=\"color: #f92672;\">=</span>kilenc,process_name<span style=\"color: #f92672;\">=</span>mbatchd,user<span style=\"color: #f92672;\">=</span>root child_major_faults<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,child_minor_faults<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,cpu_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,cpu_time_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_soft_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0.03</span>,cpu_time_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0.05</span>,cpu_usage<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,created_at<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1674246974000000000</span>i,involuntary_context_switches<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1</span>i,major_faults<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,memory_data<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">834994176</span>i,memory_locked<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,memory_rss<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">815595520</span>i,memory_stack<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">327680</span>i,memory_swap<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,memory_usage<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2.469912528991699</span>,memory_vms<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1091108864</span>i,minor_faults<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">726</span>i,nice_priority<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">20</span>i,num_fds<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">10</span>i,num_threads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2</span>i,pid<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">62056</span>i,ppid<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4103699</span>i,read_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,read_count<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">27</span>i,realtime_priority<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rlimit_cpu_time_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_cpu_time_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_file_locks_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_file_locks_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_data_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_data_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_locked_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">67108864</span>i,rlimit_memory_locked_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">67108864</span>i,rlimit_memory_rss_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_rss_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_stack_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_stack_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">8388608</span>i,rlimit_memory_vms_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_vms_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_nice_priority_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rlimit_nice_priority_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rlimit_num_fds_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">262144</span>i,rlimit_num_fds_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">65535</span>i,rlimit_realtime_priority_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rlimit_realtime_priority_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rlimit_signals_pending_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">118856</span>i,rlimit_signals_pending_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">118856</span>i,signals_pending<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,voluntary_context_switches<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">5</span>i,write_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,write_count<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">16</span>i <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> procstat,exe<span style=\"color: #f92672;\">=</span>mbschd,host<span style=\"color: #f92672;\">=</span>kilenc,process_name<span style=\"color: #f92672;\">=</span>mbschd,user<span style=\"color: #f92672;\">=</span>lsfadmin child_major_faults<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,child_minor_faults<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2457641</span>i,cpu_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">320</span>i,cpu_time_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0.02</span>,cpu_time_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_soft_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">8.4</span>,cpu_time_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">312.14</span>,cpu_usage<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1.836645120693344</span>,created_at<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1674227581000000000</span>i,involuntary_context_switches<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">3553</span>i,major_faults<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1</span>i,memory_data<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">228851712</span>i,memory_locked<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,memory_rss<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">236847104</span>i,memory_stack<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">196608</span>i,memory_swap<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,memory_usage<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0.717257022857666</span>,memory_vms<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">246808576</span>i,minor_faults<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2137969</span>i,nice_priority<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">20</span>i,num_fds<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">3</span>i,num_threads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1</span>i,pid<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4103740</span>i,ppid<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4103699</span>i,read_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1552384</span>i,read_count<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">936861</span>i,realtime_priority<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rlimit_cpu_time_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_cpu_time_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_file_locks_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_file_locks_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_data_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_data_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_locked_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">67108864</span>i,rlimit_memory_locked_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">67108864</span>i,rlimit_memory_rss_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_rss_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_stack_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_stack_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">8388608</span>i,rlimit_memory_vms_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_vms_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_nice_priority_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rlimit_nice_priority_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rlimit_num_fds_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">262144</span>i,rlimit_num_fds_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">65535</span>i,rlimit_realtime_priority_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rlimit_realtime_priority_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rlimit_signals_pending_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">118856</span>i,rlimit_signals_pending_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">118856</span>i,signals_pending<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,voluntary_context_switches<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">43952</span>i,write_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,write_count<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">42311</span>i <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> procstat_lookup,exe<span style=\"color: #f92672;\">=</span>mbschd,host<span style=\"color: #f92672;\">=</span>kilenc,pid_finder<span style=\"color: #f92672;\">=</span>pgrep,result<span style=\"color: #f92672;\">=</span>success pid_count<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1</span>i,result_code<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,running<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1</span>i <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> procstat,exe<span style=\"color: #f92672;\">=</span>mbatchd,host<span style=\"color: #f92672;\">=</span>kilenc,process_name<span style=\"color: #f92672;\">=</span>mbatchd,user<span style=\"color: #f92672;\">=</span>root child_major_faults<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2</span>i,child_minor_faults<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4476280</span>i,cpu_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">177</span>i,cpu_time_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">6.68</span>,cpu_time_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_soft_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">51.01</span>,cpu_time_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">126.42</span>,cpu_usage<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,created_at<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1674227573000000000</span>i,involuntary_context_switches<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4993</span>i,major_faults<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">3</span>i,memory_data<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">834994176</span>i,memory_locked<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,memory_rss<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">827785216</span>i,memory_stack<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">327680</span>i,memory_swap<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,memory_usage<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2.5068273544311523</span>,memory_vms<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1091108864</span>i,minor_faults<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2406945</span>i,nice_priority<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">20</span>i,num_fds<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">26</span>i,num_threads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">3</span>i,pid<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4103699</span>i,ppid<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4103684</span>i,read_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">21008384</span>i,read_count<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">364726</span>i,realtime_priority<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rlimit_cpu_time_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_cpu_time_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_file_locks_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_file_locks_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_data_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_data_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_locked_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">67108864</span>i,rlimit_memory_locked_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">67108864</span>i,rlimit_memory_rss_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_rss_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_stack_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_stack_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">8388608</span>i,rlimit_memory_vms_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_vms_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_nice_priority_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rlimit_nice_priority_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rlimit_num_fds_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">262144</span>i,rlimit_num_fds_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">65535</span>i,rlimit_realtime_priority_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rlimit_realtime_priority_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rlimit_signals_pending_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">118856</span>i,rlimit_signals_pending_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">118856</span>i,signals_pending<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,voluntary_context_switches<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">172583</span>i,write_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1562181632</span>i,write_count<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">12164760</span>i <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> procstat_lookup,exe<span style=\"color: #f92672;\">=</span>mbatchd,host<span style=\"color: #f92672;\">=</span>kilenc,pid_finder<span style=\"color: #f92672;\">=</span>pgrep,result<span style=\"color: #f92672;\">=</span>success pid_count<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2</span>i,result_code<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,running<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2</span>i <span style=\"color: #ae81ff;\">1674246977000000000</span></code></pre></div></details><hr /><ol start=\"6\"><li>Assuming there were no errors in the previous step with telegraf, proceed to start the telegraf process via systemd.</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\">[root@kilenc telegraf]# systemctl start telegraf[root@kilenc telegraf]# systemctl status telegraf● telegraf.service - Telegraf   Loaded: loaded (/usr/lib/systemd/system/telegraf.service; enabled; vendor preset: disabled)   Active: active (running) since Thu 2023-01-19 14:13:51 EST; 1 day 1h ago     Docs: https://github.com/influxdata/telegraf Main PID: 3225959 (telegraf)    Tasks: 35 (limit: 190169)   Memory: 192.6M   CGroup: /system.slice/telegraf.service           └─3225959 /usr/bin/telegraf -config /etc/telegraf/telegraf.conf -config-directory /etc/tele&gt;Jan 19 14:13:51 kilenc systemd[1]: Starting Telegraf...Jan 19 14:13:51 kilenc systemd[1]: Started Telegraf.</code></pre></div><ol start=\"7\"><li>On the host running the database instance, <em>adatbazis</em>, perform queries to check whether the database <em>telegraf</em> exists, as well as checking if LSF related data is being logged.This is confirmed in the output below.</li></ol><hr /><details>  <strong>Output from InfluxDB queries. Click to expand!</strong>  <div class=\"highlight\"><pre><code class=\"language-js\">[<span style=\"color: #a6e22e;\">root</span><span style=\"color: #960050; background-color: #1e0010;\">@</span><span style=\"color: #a6e22e;\">adatbazis</span> <span style=\"color: #a6e22e;\">fedora</span>]<span style=\"color: #960050; background-color: #1e0010;\">#</span> <span style=\"color: #a6e22e;\">influx</span><span style=\"color: #a6e22e;\">Connected</span> <span style=\"color: #a6e22e;\">to</span> <span style=\"color: #a6e22e;\">https</span><span style=\"color: #f92672;\">:</span><span style=\"color: #75715e;\">//localhost:8086 version 1.8.10</span><span style=\"color: #75715e;\"></span><span style=\"color: #a6e22e;\">InfluxDB</span> <span style=\"color: #a6e22e;\">shell</span> <span style=\"color: #a6e22e;\">version</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #ae81ff;\">1.8</span>.<span style=\"color: #ae81ff;\">10</span><span style=\"color: #f92672;\">&gt;</span> <span style=\"color: #a6e22e;\">auth</span><span style=\"color: #a6e22e;\">username</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #a6e22e;\">influx</span><span style=\"color: #a6e22e;\">password</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #f92672;\">&gt;</span> <span style=\"color: #a6e22e;\">show</span> <span style=\"color: #a6e22e;\">databases</span><span style=\"color: #a6e22e;\">name</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #a6e22e;\">databases</span><span style=\"color: #a6e22e;\">name</span><span style=\"color: #f92672;\">----</span><span style=\"color: #ae81ff;\">_</span><span style=\"color: #a6e22e;\">internal</span><span style=\"color: #a6e22e;\">telegraf</span><span style=\"color: #f92672;\">&gt;</span> <span style=\"color: #a6e22e;\">use</span> <span style=\"color: #a6e22e;\">telegraf</span><span style=\"color: #a6e22e;\">Using</span> <span style=\"color: #a6e22e;\">database</span> <span style=\"color: #a6e22e;\">telegraf</span><span style=\"color: #f92672;\">&gt;</span> <span style=\"color: #a6e22e;\">show</span> <span style=\"color: #a6e22e;\">field</span> <span style=\"color: #a6e22e;\">keys</span><span style=\"color: #a6e22e;\">name</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #a6e22e;\">cpu</span><span style=\"color: #a6e22e;\">fieldKey</span>         <span style=\"color: #a6e22e;\">fieldType</span><span style=\"color: #f92672;\">--------</span>         <span style=\"color: #f92672;\">---------</span><span style=\"color: #a6e22e;\">usage_guest</span>      <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">usage_guest_nice</span> <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">usage_idle</span>       <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">usage_iowait</span>     <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">usage_irq</span>        <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">usage_nice</span>       <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">usage_softirq</span>    <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">usage_steal</span>      <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">usage_system</span>     <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">usage_user</span>       <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">name</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #a6e22e;\">disk</span><span style=\"color: #a6e22e;\">fieldKey</span>     <span style=\"color: #a6e22e;\">fieldType</span><span style=\"color: #f92672;\">--------</span>     <span style=\"color: #f92672;\">---------</span><span style=\"color: #a6e22e;\">free</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">inodes_free</span>  <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">inodes_total</span> <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">inodes_used</span>  <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">total</span>        <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">used</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">used_percent</span> <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">name</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #a6e22e;\">diskio</span><span style=\"color: #a6e22e;\">fieldKey</span>         <span style=\"color: #a6e22e;\">fieldType</span><span style=\"color: #f92672;\">--------</span>         <span style=\"color: #f92672;\">---------</span><span style=\"color: #a6e22e;\">io_time</span>          <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">iops_in_progress</span> <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">merged_reads</span>     <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">merged_writes</span>    <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">read_bytes</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">read_time</span>        <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">reads</span>            <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">weighted_io_time</span> <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">write_bytes</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">write_time</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">writes</span>           <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">name</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #a6e22e;\">kernel</span><span style=\"color: #a6e22e;\">fieldKey</span>         <span style=\"color: #a6e22e;\">fieldType</span><span style=\"color: #f92672;\">--------</span>         <span style=\"color: #f92672;\">---------</span><span style=\"color: #a6e22e;\">boot_time</span>        <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">context_switches</span> <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">entropy_avail</span>    <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">interrupts</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">processes_forked</span> <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">name</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #a6e22e;\">lsf_hosts</span><span style=\"color: #a6e22e;\">fieldKey</span> <span style=\"color: #a6e22e;\">fieldType</span><span style=\"color: #f92672;\">--------</span> <span style=\"color: #f92672;\">---------</span><span style=\"color: #a6e22e;\">current</span>  <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">peak</span>     <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">name</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #a6e22e;\">lsf_jobs</span><span style=\"color: #a6e22e;\">fieldKey</span> <span style=\"color: #a6e22e;\">fieldType</span><span style=\"color: #f92672;\">--------</span> <span style=\"color: #f92672;\">---------</span><span style=\"color: #a6e22e;\">value</span>    <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">name</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #a6e22e;\">lsf_mbatchd</span><span style=\"color: #a6e22e;\">fieldKey</span> <span style=\"color: #a6e22e;\">fieldType</span><span style=\"color: #f92672;\">--------</span> <span style=\"color: #f92672;\">---------</span><span style=\"color: #a6e22e;\">value</span>    <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">name</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #a6e22e;\">lsf_queues</span><span style=\"color: #a6e22e;\">fieldKey</span> <span style=\"color: #a6e22e;\">fieldType</span><span style=\"color: #f92672;\">--------</span> <span style=\"color: #f92672;\">---------</span><span style=\"color: #a6e22e;\">njobs</span>    <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">pend</span>     <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">rsv</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">run</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ssusp</span>    <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">susp</span>     <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ususp</span>    <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">name</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #a6e22e;\">lsf_servers</span><span style=\"color: #a6e22e;\">fieldKey</span> <span style=\"color: #a6e22e;\">fieldType</span><span style=\"color: #f92672;\">--------</span> <span style=\"color: #f92672;\">---------</span><span style=\"color: #a6e22e;\">value</span>    <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">name</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #a6e22e;\">lsf_users</span><span style=\"color: #a6e22e;\">fieldKey</span> <span style=\"color: #a6e22e;\">fieldType</span><span style=\"color: #f92672;\">--------</span> <span style=\"color: #f92672;\">---------</span><span style=\"color: #a6e22e;\">value</span>    <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">name</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #a6e22e;\">mem</span><span style=\"color: #a6e22e;\">fieldKey</span>          <span style=\"color: #a6e22e;\">fieldType</span><span style=\"color: #f92672;\">--------</span>          <span style=\"color: #f92672;\">---------</span><span style=\"color: #a6e22e;\">active</span>            <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">available</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">available_percent</span> <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">buffered</span>          <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">cached</span>            <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">commit_limit</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">committed_as</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">dirty</span>             <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">free</span>              <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">high_free</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">high_total</span>        <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">huge_page_size</span>    <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">huge_pages_free</span>   <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">huge_pages_total</span>  <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">inactive</span>          <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">low_free</span>          <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">low_total</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">mapped</span>            <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">page_tables</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">shared</span>            <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">slab</span>              <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">sreclaimable</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">sunreclaim</span>        <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">swap_cached</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">swap_free</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">swap_total</span>        <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">total</span>             <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">used</span>              <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">used_percent</span>      <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">vmalloc_chunk</span>     <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">vmalloc_total</span>     <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">vmalloc_used</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">write_back</span>        <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">write_back_tmp</span>    <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">name</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #a6e22e;\">net</span><span style=\"color: #a6e22e;\">fieldKey</span>              <span style=\"color: #a6e22e;\">fieldType</span><span style=\"color: #f92672;\">--------</span>              <span style=\"color: #f92672;\">---------</span><span style=\"color: #a6e22e;\">bytes_recv</span>            <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">bytes_sent</span>            <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">drop_in</span>               <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">drop_out</span>              <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">err_in</span>                <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">err_out</span>               <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_inaddrmaskreps</span>   <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_inaddrmasks</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_incsumerrors</span>     <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_indestunreachs</span>   <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_inechoreps</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_inechos</span>          <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_inerrors</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_inmsgs</span>           <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_inparmprobs</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_inredirects</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_insrcquenchs</span>     <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_intimeexcds</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_intimestampreps</span>  <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_intimestamps</span>     <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_outaddrmaskreps</span>  <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_outaddrmasks</span>     <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_outdestunreachs</span>  <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_outechoreps</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_outechos</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_outerrors</span>        <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_outmsgs</span>          <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_outparmprobs</span>     <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_outredirects</span>     <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_outsrcquenchs</span>    <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_outtimeexcds</span>     <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_outtimestampreps</span> <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_outtimestamps</span>    <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmpmsg_intype0</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmpmsg_intype3</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmpmsg_intype8</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmpmsg_outtype0</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmpmsg_outtype3</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmpmsg_outtype8</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ip_defaultttl</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ip_forwarding</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ip_forwdatagrams</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ip_fragcreates</span>        <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ip_fragfails</span>          <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ip_fragoks</span>            <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ip_inaddrerrors</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ip_indelivers</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ip_indiscards</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ip_inhdrerrors</span>        <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ip_inreceives</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ip_inunknownprotos</span>    <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ip_outdiscards</span>        <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ip_outnoroutes</span>        <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ip_outrequests</span>        <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ip_reasmfails</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ip_reasmoks</span>           <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ip_reasmreqds</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ip_reasmtimeout</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">packets_recv</span>          <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">packets_sent</span>          <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">tcp_activeopens</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">tcp_attemptfails</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">tcp_currestab</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">tcp_estabresets</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">tcp_incsumerrors</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">tcp_inerrs</span>            <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">tcp_insegs</span>            <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">tcp_maxconn</span>           <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">tcp_outrsts</span>           <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">tcp_outsegs</span>           <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">tcp_passiveopens</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">tcp_retranssegs</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">tcp_rtoalgorithm</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">tcp_rtomax</span>            <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">tcp_rtomin</span>            <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">udp_ignoredmulti</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">udp_incsumerrors</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">udp_indatagrams</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">udp_inerrors</span>          <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">udp_memerrors</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">udp_noports</span>           <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">udp_outdatagrams</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">udp_rcvbuferrors</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">udp_sndbuferrors</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">udplite_ignoredmulti</span>  <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">udplite_incsumerrors</span>  <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">udplite_indatagrams</span>   <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">udplite_inerrors</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">udplite_memerrors</span>     <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">udplite_noports</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">udplite_outdatagrams</span>  <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">udplite_rcvbuferrors</span>  <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">udplite_sndbuferrors</span>  <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">name</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #a6e22e;\">processes</span><span style=\"color: #a6e22e;\">fieldKey</span>      <span style=\"color: #a6e22e;\">fieldType</span><span style=\"color: #f92672;\">--------</span>      <span style=\"color: #f92672;\">---------</span><span style=\"color: #a6e22e;\">blocked</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">dead</span>          <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">idle</span>          <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">paging</span>        <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">parked</span>        <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">running</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">sleeping</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">stopped</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">total</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">total_threads</span> <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">unknown</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">zombies</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">name</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #a6e22e;\">procstat</span><span style=\"color: #a6e22e;\">fieldKey</span>                     <span style=\"color: #a6e22e;\">fieldType</span><span style=\"color: #f92672;\">--------</span>                     <span style=\"color: #f92672;\">---------</span><span style=\"color: #a6e22e;\">child_major_faults</span>           <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">child_minor_faults</span>           <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">cpu_time_guest</span>               <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">cpu_time_guest_nice</span>          <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">cpu_time_idle</span>                <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">cpu_time_iowait</span>              <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">cpu_time_irq</span>                 <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">cpu_time_nice</span>                <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">cpu_time_soft_irq</span>            <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">cpu_time_steal</span>               <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">cpu_time_system</span>              <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">cpu_time_user</span>                <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">cpu_usage</span>                    <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">created_at</span>                   <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">involuntary_context_switches</span> <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">major_faults</span>                 <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">memory_data</span>                  <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">memory_locked</span>                <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">memory_rss</span>                   <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">memory_stack</span>                 <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">memory_swap</span>                  <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">memory_usage</span>                 <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">memory_vms</span>                   <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">minor_faults</span>                 <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">num_threads</span>                  <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">pid</span>                          <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ppid</span>                         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">voluntary_context_switches</span>   <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">name</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #a6e22e;\">procstat_lookup</span><span style=\"color: #a6e22e;\">fieldKey</span>    <span style=\"color: #a6e22e;\">fieldType</span><span style=\"color: #f92672;\">--------</span>    <span style=\"color: #f92672;\">---------</span><span style=\"color: #a6e22e;\">pid_count</span>   <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">result_code</span> <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">running</span>     <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">name</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #a6e22e;\">swap</span><span style=\"color: #a6e22e;\">fieldKey</span>     <span style=\"color: #a6e22e;\">fieldType</span><span style=\"color: #f92672;\">--------</span>     <span style=\"color: #f92672;\">---------</span><span style=\"color: #a6e22e;\">free</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #66d9ef;\">in</span>           <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">out</span>          <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">total</span>        <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">used</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">used_percent</span> <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">name</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #a6e22e;\">system</span><span style=\"color: #a6e22e;\">fieldKey</span>       <span style=\"color: #a6e22e;\">fieldType</span><span style=\"color: #f92672;\">--------</span>       <span style=\"color: #f92672;\">---------</span><span style=\"color: #a6e22e;\">load1</span>          <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">load15</span>         <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">load5</span>          <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">n_cpus</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">n_unique_users</span> <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">n_users</span>        <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">uptime</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">uptime_format</span>  <span style=\"color: #a6e22e;\">string</span><span style=\"color: #f92672;\">&gt;</span> <span style=\"color: #a6e22e;\">select</span> <span style=\"color: #f92672;\">*</span> <span style=\"color: #a6e22e;\">from</span> <span style=\"color: #a6e22e;\">metrics</span><span style=\"color: #f92672;\">&gt;</span> <span style=\"color: #a6e22e;\">SELECT</span> <span style=\"color: #f92672;\">*</span> <span style=\"color: #a6e22e;\">FROM</span> <span style=\"color: #e6db74;\">\"lsf_hosts\"</span>;<span style=\"color: #a6e22e;\">name</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #a6e22e;\">lsf_hosts</span><span style=\"color: #a6e22e;\">time</span>                <span style=\"color: #a6e22e;\">current</span> <span style=\"color: #a6e22e;\">host</span>   <span style=\"color: #a6e22e;\">peak</span> <span style=\"color: #a6e22e;\">state</span><span style=\"color: #f92672;\">----</span>                <span style=\"color: #f92672;\">-------</span> <span style=\"color: #f92672;\">----</span>   <span style=\"color: #f92672;\">----</span> <span style=\"color: #f92672;\">-----</span><span style=\"color: #ae81ff;\">1674493170000000000</span> <span style=\"color: #ae81ff;\">0</span>       <span style=\"color: #a6e22e;\">kilenc</span> <span style=\"color: #ae81ff;\">0</span>    <span style=\"color: #a6e22e;\">clients</span><span style=\"color: #ae81ff;\">1674493170000000000</span> <span style=\"color: #ae81ff;\">32</span>      <span style=\"color: #a6e22e;\">kilenc</span> <span style=\"color: #ae81ff;\">32</span>   <span style=\"color: #a6e22e;\">slots</span><span style=\"color: #ae81ff;\">1674493170000000000</span> <span style=\"color: #ae81ff;\">32</span>      <span style=\"color: #a6e22e;\">kilenc</span> <span style=\"color: #ae81ff;\">32</span>   <span style=\"color: #a6e22e;\">cores</span><span style=\"color: #ae81ff;\">1674493170000000000</span> <span style=\"color: #ae81ff;\">1</span>       <span style=\"color: #a6e22e;\">kilenc</span> <span style=\"color: #ae81ff;\">1</span>    <span style=\"color: #a6e22e;\">servers</span><span style=\"color: #ae81ff;\">1674493170000000000</span> <span style=\"color: #ae81ff;\">2</span>       <span style=\"color: #a6e22e;\">kilenc</span> <span style=\"color: #ae81ff;\">2</span>    <span style=\"color: #a6e22e;\">cpus</span><span style=\"color: #ae81ff;\">1674493200000000000</span> <span style=\"color: #ae81ff;\">1</span>       <span style=\"color: #a6e22e;\">kilenc</span> <span style=\"color: #ae81ff;\">1</span>    <span style=\"color: #a6e22e;\">servers</span><span style=\"color: #ae81ff;\">1674493200000000000</span> <span style=\"color: #ae81ff;\">2</span>       <span style=\"color: #a6e22e;\">kilenc</span> <span style=\"color: #ae81ff;\">2</span>    <span style=\"color: #a6e22e;\">cpus</span><span style=\"color: #ae81ff;\">1674493200000000000</span> <span style=\"color: #ae81ff;\">32</span>      <span style=\"color: #a6e22e;\">kilenc</span> <span style=\"color: #ae81ff;\">32</span>   <span style=\"color: #a6e22e;\">slots</span><span style=\"color: #ae81ff;\">1674493200000000000</span> <span style=\"color: #ae81ff;\">0</span>       <span style=\"color: #a6e22e;\">kilenc</span> <span style=\"color: #ae81ff;\">0</span>    <span style=\"color: #a6e22e;\">clients</span><span style=\"color: #ae81ff;\">1674493200000000000</span> <span style=\"color: #ae81ff;\">32</span>      <span style=\"color: #a6e22e;\">kilenc</span> <span style=\"color: #ae81ff;\">32</span>   <span style=\"color: #a6e22e;\">cores</span><span style=\"color: #ae81ff;\">1674493230000000000</span> <span style=\"color: #ae81ff;\">0</span>       <span style=\"color: #a6e22e;\">kilenc</span> <span style=\"color: #ae81ff;\">0</span>    <span style=\"color: #a6e22e;\">clients</span><span style=\"color: #ae81ff;\">1674493230000000000</span> <span style=\"color: #ae81ff;\">32</span>      <span style=\"color: #a6e22e;\">kilenc</span> <span style=\"color: #ae81ff;\">32</span>   <span style=\"color: #a6e22e;\">cores</span><span style=\"color: #ae81ff;\">1674493230000000000</span> <span style=\"color: #ae81ff;\">2</span>       <span style=\"color: #a6e22e;\">kilenc</span> <span style=\"color: #ae81ff;\">2</span>    <span style=\"color: #a6e22e;\">cpus</span><span style=\"color: #ae81ff;\">1674493230000000000</span> <span style=\"color: #ae81ff;\">1</span>       <span style=\"color: #a6e22e;\">kilenc</span> <span style=\"color: #ae81ff;\">1</span>    <span style=\"color: #a6e22e;\">servers</span><span style=\"color: #ae81ff;\">1674493230000000000</span> <span style=\"color: #ae81ff;\">32</span>      <span style=\"color: #a6e22e;\">kilenc</span> <span style=\"color: #ae81ff;\">32</span>   <span style=\"color: #a6e22e;\">slots</span><span style=\"color: #ae81ff;\">1674493260000000000</span> <span style=\"color: #ae81ff;\">1</span>       <span style=\"color: #a6e22e;\">kilenc</span> <span style=\"color: #ae81ff;\">1</span>    <span style=\"color: #a6e22e;\">servers</span><span style=\"color: #ae81ff;\">1674493260000000000</span> <span style=\"color: #ae81ff;\">32</span>      <span style=\"color: #a6e22e;\">kilenc</span> <span style=\"color: #ae81ff;\">32</span>   <span style=\"color: #a6e22e;\">slots</span><span style=\"color: #ae81ff;\">1674493260000000000</span> <span style=\"color: #ae81ff;\">0</span>       <span style=\"color: #a6e22e;\">kilenc</span> <span style=\"color: #ae81ff;\">0</span>    <span style=\"color: #a6e22e;\">clients</span><span style=\"color: #ae81ff;\">1674493260000000000</span> <span style=\"color: #ae81ff;\">2</span>       <span style=\"color: #a6e22e;\">kilenc</span> <span style=\"color: #ae81ff;\">2</span>    <span style=\"color: #a6e22e;\">cpus</span><span style=\"color: #ae81ff;\">1674493260000000000</span> <span style=\"color: #ae81ff;\">32</span>      <span style=\"color: #a6e22e;\">kilenc</span> <span style=\"color: #ae81ff;\">32</span>   <span style=\"color: #a6e22e;\">cores</span><span style=\"color: #f92672;\">&gt;</span> <span style=\"color: #a6e22e;\">quit</span></code></pre></div></details><hr /><ol start=\"8\"><li>With telegraf successfully logging data to the InfluxDB instance, it will now be possible to create a data source in Grafana in order to create a dashboard containing LSF metrics.As noted at the outset, this article is not meant to be an extensive guide to the creation of dashoards in Grafana. In the Grafana navigation select <em>Configuration</em> &gt; <em>Data sources</em>.</li></ol><figure><img src=\"https://www.gaborsamu.com/images/configure_datasource.png\" /></figure><ol start=\"9\"><li>Select the <em>Add data source</em> button, followed by InfluxDB, which is listed under <em>Time series databases</em>. On the settings page specify following values:</li></ol><hr /><table><thead><tr><th>Variable</th><th>Value</th></tr></thead><tbody><tr><td>URL</td><td><em>http://adatbazis:8086</em></td></tr><tr><td>Database</td><td><em>telegraf</em></td></tr><tr><td>Basic auth</td><td>(enable)</td></tr><tr><td>User</td><td>&lt;influxdb_username&gt;</td></tr><tr><td>Password</td><td>&lt;influxdb_password</td></tr></tbody></table><hr /><p>Next, click on <em>Save &amp; test</em>. If all variables and settings were properly specified, the message <em>datasource is working. 17 measurements found</em>.</p><figure><img src=\"https://www.gaborsamu.com/images/test_datasource.png\" /></figure><ol start=\"10\"><li>With the datasource configured in Grafana, the final step is to create a dashboard. Creating a dashboard requires creating panels which display data pulled from the confiugred datasource using targeted queries. With a bit of effort, I was able to piece together the following dashboard which includes both metrics from LSF, as well as metrics from Telegraf<em>input.procstat</em> for the LSF processes <em>mbatchd</em>, <em>mbschd</em> and the management <em>lim</em>.</li></ol><figure><img src=\"https://www.gaborsamu.com/images/lsf_dashboard3.jpg\" /></figure><hr /><details>  <strong>Example dashboard definition (JSON). Click to expand!</strong>  <div class=\"highlight\"><pre><code class=\"language-json\">{  <span style=\"color: #f92672;\">\"annotations\"</span>: {    <span style=\"color: #f92672;\">\"list\"</span>: [      {        <span style=\"color: #f92672;\">\"builtIn\"</span>: <span style=\"color: #ae81ff;\">1</span>,        <span style=\"color: #f92672;\">\"datasource\"</span>: {          <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"datasource\"</span>,          <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"grafana\"</span>        },        <span style=\"color: #f92672;\">\"enable\"</span>: <span style=\"color: #66d9ef;\">true</span>,        <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">true</span>,        <span style=\"color: #f92672;\">\"iconColor\"</span>: <span style=\"color: #e6db74;\">\"rgba(0, 211, 255, 1)\"</span>,        <span style=\"color: #f92672;\">\"name\"</span>: <span style=\"color: #e6db74;\">\"Annotations &amp; Alerts\"</span>,        <span style=\"color: #f92672;\">\"target\"</span>: {          <span style=\"color: #f92672;\">\"limit\"</span>: <span style=\"color: #ae81ff;\">100</span>,          <span style=\"color: #f92672;\">\"matchAny\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"tags\"</span>: [],          <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"dashboard\"</span>        },        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"dashboard\"</span>      }    ]  },  <span style=\"color: #f92672;\">\"editable\"</span>: <span style=\"color: #66d9ef;\">true</span>,  <span style=\"color: #f92672;\">\"fiscalYearStartMonth\"</span>: <span style=\"color: #ae81ff;\">0</span>,  <span style=\"color: #f92672;\">\"graphTooltip\"</span>: <span style=\"color: #ae81ff;\">0</span>,  <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">17</span>,  <span style=\"color: #f92672;\">\"links\"</span>: [],  <span style=\"color: #f92672;\">\"liveNow\"</span>: <span style=\"color: #66d9ef;\">false</span>,  <span style=\"color: #f92672;\">\"panels\"</span>: [    {      <span style=\"color: #f92672;\">\"collapsed\"</span>: <span style=\"color: #66d9ef;\">false</span>,      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">1</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">24</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">0</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">0</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">35</span>,      <span style=\"color: #f92672;\">\"panels\"</span>: [],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Cluster aggregate current statistics\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"row\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"A view of the current status of the LSF servers in the cluster. Servers can be in one of four states: Ok, Unavailable, Closed and Unreachable. \"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"palette-classic\"</span>          },          <span style=\"color: #f92672;\">\"custom\"</span>: {            <span style=\"color: #f92672;\">\"hideFrom\"</span>: {              <span style=\"color: #f92672;\">\"legend\"</span>: <span style=\"color: #66d9ef;\">false</span>,              <span style=\"color: #f92672;\">\"tooltip\"</span>: <span style=\"color: #66d9ef;\">false</span>,              <span style=\"color: #f92672;\">\"viz\"</span>: <span style=\"color: #66d9ef;\">false</span>            }          },          <span style=\"color: #f92672;\">\"decimals\"</span>: <span style=\"color: #ae81ff;\">2</span>,          <span style=\"color: #f92672;\">\"mappings\"</span>: []        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">8</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">9</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">0</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">1</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">32</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"displayLabels\"</span>: [          <span style=\"color: #e6db74;\">\"name\"</span>,          <span style=\"color: #e6db74;\">\"value\"</span>        ],        <span style=\"color: #f92672;\">\"legend\"</span>: {          <span style=\"color: #f92672;\">\"displayMode\"</span>: <span style=\"color: #e6db74;\">\"table\"</span>,          <span style=\"color: #f92672;\">\"placement\"</span>: <span style=\"color: #e6db74;\">\"right\"</span>,          <span style=\"color: #f92672;\">\"showLegend\"</span>: <span style=\"color: #66d9ef;\">true</span>,          <span style=\"color: #f92672;\">\"sortBy\"</span>: <span style=\"color: #e6db74;\">\"Value\"</span>,          <span style=\"color: #f92672;\">\"sortDesc\"</span>: <span style=\"color: #66d9ef;\">true</span>,          <span style=\"color: #f92672;\">\"values\"</span>: [            <span style=\"color: #e6db74;\">\"value\"</span>,            <span style=\"color: #e6db74;\">\"percent\"</span>          ]        },        <span style=\"color: #f92672;\">\"pieType\"</span>: <span style=\"color: #e6db74;\">\"donut\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"tooltip\"</span>: {          <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"multi\"</span>,          <span style=\"color: #f92672;\">\"sort\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>        }      },      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Ok\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_servers\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"status\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"ok\"</span>            }          ]        },        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Closed\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_servers\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"B\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"status\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"closed\"</span>            }          ]        },        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Unreachable\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_servers\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"default\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"C\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"status\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"unreachable\"</span>            }          ]        },        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Unavailable\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_servers\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"D\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"status\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"unavailable\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Current aggregate LSF server statistics\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"piechart\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"green\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          }        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">9</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">1</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">43</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"colorMode\"</span>: <span style=\"color: #e6db74;\">\"value\"</span>,        <span style=\"color: #f92672;\">\"graphMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,        <span style=\"color: #f92672;\">\"justifyMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"text\"</span>: {},        <span style=\"color: #f92672;\">\"textMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_jobs\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"distinct\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"running\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Currently running\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"stat\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"light-red\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          }        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">12</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">1</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">45</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"colorMode\"</span>: <span style=\"color: #e6db74;\">\"value\"</span>,        <span style=\"color: #f92672;\">\"graphMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,        <span style=\"color: #f92672;\">\"justifyMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"text\"</span>: {},        <span style=\"color: #f92672;\">\"textMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_jobs\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"default\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"suspended\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Currently suspended\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"stat\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"palette-classic\"</span>          },          <span style=\"color: #f92672;\">\"custom\"</span>: {            <span style=\"color: #f92672;\">\"hideFrom\"</span>: {              <span style=\"color: #f92672;\">\"legend\"</span>: <span style=\"color: #66d9ef;\">false</span>,              <span style=\"color: #f92672;\">\"tooltip\"</span>: <span style=\"color: #66d9ef;\">false</span>,              <span style=\"color: #f92672;\">\"viz\"</span>: <span style=\"color: #66d9ef;\">false</span>            }          },          <span style=\"color: #f92672;\">\"decimals\"</span>: <span style=\"color: #ae81ff;\">2</span>,          <span style=\"color: #f92672;\">\"mappings\"</span>: []        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">8</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">9</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">15</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">1</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">33</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"displayLabels\"</span>: [          <span style=\"color: #e6db74;\">\"name\"</span>,          <span style=\"color: #e6db74;\">\"value\"</span>        ],        <span style=\"color: #f92672;\">\"legend\"</span>: {          <span style=\"color: #f92672;\">\"displayMode\"</span>: <span style=\"color: #e6db74;\">\"table\"</span>,          <span style=\"color: #f92672;\">\"placement\"</span>: <span style=\"color: #e6db74;\">\"right\"</span>,          <span style=\"color: #f92672;\">\"showLegend\"</span>: <span style=\"color: #66d9ef;\">true</span>,          <span style=\"color: #f92672;\">\"sortBy\"</span>: <span style=\"color: #e6db74;\">\"Value\"</span>,          <span style=\"color: #f92672;\">\"sortDesc\"</span>: <span style=\"color: #66d9ef;\">true</span>,          <span style=\"color: #f92672;\">\"values\"</span>: [            <span style=\"color: #e6db74;\">\"value\"</span>,            <span style=\"color: #e6db74;\">\"percent\"</span>          ]        },        <span style=\"color: #f92672;\">\"pieType\"</span>: <span style=\"color: #e6db74;\">\"donut\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"tooltip\"</span>: {          <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"multi\"</span>,          <span style=\"color: #f92672;\">\"sort\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>        }      },      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Running\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_jobs\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"running\"</span>            }          ]        },        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Pending\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_jobs\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"B\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"pending\"</span>            }          ]        },        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Suspended\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_jobs\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"C\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"suspended\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Current aggregate LSF job statistics\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"piechart\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"yellow\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          }        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">9</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">5</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">44</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"colorMode\"</span>: <span style=\"color: #e6db74;\">\"value\"</span>,        <span style=\"color: #f92672;\">\"graphMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,        <span style=\"color: #f92672;\">\"justifyMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"text\"</span>: {},        <span style=\"color: #f92672;\">\"textMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_jobs\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"default\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"pending\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Currently pending \"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"stat\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"blue\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          }        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">12</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">5</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">46</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"colorMode\"</span>: <span style=\"color: #e6db74;\">\"value\"</span>,        <span style=\"color: #f92672;\">\"graphMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,        <span style=\"color: #f92672;\">\"justifyMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"text\"</span>: {},        <span style=\"color: #f92672;\">\"textMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_jobs\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"default\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"finished\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Finished (past hour)\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"stat\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"Spectrum LSF queue statistics. Here we show jobs in running, pending and suspended jobs. \"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"palette-classic\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"green\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              },              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"red\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #ae81ff;\">80</span>              }            ]          }        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">8</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">9</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">0</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">9</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">41</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"displayMode\"</span>: <span style=\"color: #e6db74;\">\"lcd\"</span>,        <span style=\"color: #f92672;\">\"minVizHeight\"</span>: <span style=\"color: #ae81ff;\">10</span>,        <span style=\"color: #f92672;\">\"minVizWidth\"</span>: <span style=\"color: #ae81ff;\">0</span>,        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"horizontal\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"showUnfilled\"</span>: <span style=\"color: #66d9ef;\">true</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Running\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_queues\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"run\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"name\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=~\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"/^$Queue$/\"</span>            }          ]        },        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Pending\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_queues\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"B\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"pend\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"name\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=~\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"/^$Queue$/\"</span>            }          ]        },        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Suspended\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_queues\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"C\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"susp\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"name\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=~\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"/^$Queue$/\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Current queue statistics ($Queue)\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"bargauge\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"min\"</span>: <span style=\"color: #ae81ff;\">0</span>,          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"percentage\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"green\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          },          <span style=\"color: #f92672;\">\"unit\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">9</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">9</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">53</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"/^lsf_hosts\\\\.last$/\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"showThresholdLabels\"</span>: <span style=\"color: #66d9ef;\">false</span>,        <span style=\"color: #f92672;\">\"showThresholdMarkers\"</span>: <span style=\"color: #66d9ef;\">true</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_hosts\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"current\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ],            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"peak\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"servers\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Servers\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"gauge\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"min\"</span>: <span style=\"color: #ae81ff;\">0</span>,          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"percentage\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"yellow\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          },          <span style=\"color: #f92672;\">\"unit\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">12</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">9</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">54</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"/^lsf_hosts\\\\.last$/\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"showThresholdLabels\"</span>: <span style=\"color: #66d9ef;\">false</span>,        <span style=\"color: #f92672;\">\"showThresholdMarkers\"</span>: <span style=\"color: #66d9ef;\">true</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_hosts\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"current\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ],            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"peak\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"cpus\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"CPUs\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"gauge\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"palette-classic\"</span>          },          <span style=\"color: #f92672;\">\"custom\"</span>: {            <span style=\"color: #f92672;\">\"axisCenteredZero\"</span>: <span style=\"color: #66d9ef;\">false</span>,            <span style=\"color: #f92672;\">\"axisColorMode\"</span>: <span style=\"color: #e6db74;\">\"text\"</span>,            <span style=\"color: #f92672;\">\"axisLabel\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,            <span style=\"color: #f92672;\">\"axisPlacement\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,            <span style=\"color: #f92672;\">\"barAlignment\"</span>: <span style=\"color: #ae81ff;\">0</span>,            <span style=\"color: #f92672;\">\"drawStyle\"</span>: <span style=\"color: #e6db74;\">\"line\"</span>,            <span style=\"color: #f92672;\">\"fillOpacity\"</span>: <span style=\"color: #ae81ff;\">0</span>,            <span style=\"color: #f92672;\">\"gradientMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,            <span style=\"color: #f92672;\">\"hideFrom\"</span>: {              <span style=\"color: #f92672;\">\"legend\"</span>: <span style=\"color: #66d9ef;\">false</span>,              <span style=\"color: #f92672;\">\"tooltip\"</span>: <span style=\"color: #66d9ef;\">false</span>,              <span style=\"color: #f92672;\">\"viz\"</span>: <span style=\"color: #66d9ef;\">false</span>            },            <span style=\"color: #f92672;\">\"lineInterpolation\"</span>: <span style=\"color: #e6db74;\">\"stepBefore\"</span>,            <span style=\"color: #f92672;\">\"lineWidth\"</span>: <span style=\"color: #ae81ff;\">1</span>,            <span style=\"color: #f92672;\">\"pointSize\"</span>: <span style=\"color: #ae81ff;\">5</span>,            <span style=\"color: #f92672;\">\"scaleDistribution\"</span>: {              <span style=\"color: #f92672;\">\"log\"</span>: <span style=\"color: #ae81ff;\">2</span>,              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"log\"</span>            },            <span style=\"color: #f92672;\">\"showPoints\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,            <span style=\"color: #f92672;\">\"spanNulls\"</span>: <span style=\"color: #66d9ef;\">true</span>,            <span style=\"color: #f92672;\">\"stacking\"</span>: {              <span style=\"color: #f92672;\">\"group\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,              <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>            },            <span style=\"color: #f92672;\">\"thresholdsStyle\"</span>: {              <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"off\"</span>            }          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"green\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              },              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"red\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #ae81ff;\">80</span>              }            ]          }        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">8</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">9</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">15</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">9</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">42</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"legend\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [],          <span style=\"color: #f92672;\">\"displayMode\"</span>: <span style=\"color: #e6db74;\">\"list\"</span>,          <span style=\"color: #f92672;\">\"placement\"</span>: <span style=\"color: #e6db74;\">\"bottom\"</span>,          <span style=\"color: #f92672;\">\"showLegend\"</span>: <span style=\"color: #66d9ef;\">true</span>        },        <span style=\"color: #f92672;\">\"tooltip\"</span>: {          <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"single\"</span>,          <span style=\"color: #f92672;\">\"sort\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>        }      },      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Running\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_jobs\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"running\"</span>            }          ]        },        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Pending\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_jobs\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"B\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"pending\"</span>            }          ]        },        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Suspended\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_jobs\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"C\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"suspended\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Aggregate LSF job statistics\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"timeseries\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"min\"</span>: <span style=\"color: #ae81ff;\">0</span>,          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"percentage\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"light-red\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          },          <span style=\"color: #f92672;\">\"unit\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">9</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">13</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">55</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"/^lsf_hosts\\\\.last$/\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"showThresholdLabels\"</span>: <span style=\"color: #66d9ef;\">false</span>,        <span style=\"color: #f92672;\">\"showThresholdMarkers\"</span>: <span style=\"color: #66d9ef;\">true</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_hosts\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"current\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ],            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"peak\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"cores\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Cores\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"gauge\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"min\"</span>: <span style=\"color: #ae81ff;\">0</span>,          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"percentage\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"blue\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          },          <span style=\"color: #f92672;\">\"unit\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">12</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">13</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">56</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"/^lsf_hosts\\\\.last$/\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"showThresholdLabels\"</span>: <span style=\"color: #66d9ef;\">false</span>,        <span style=\"color: #f92672;\">\"showThresholdMarkers\"</span>: <span style=\"color: #66d9ef;\">true</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_hosts\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"current\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ],            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"peak\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"slots\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Slots\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"gauge\"</span>    },    {      <span style=\"color: #f92672;\">\"collapsed\"</span>: <span style=\"color: #66d9ef;\">false</span>,      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">1</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">24</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">0</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">17</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">37</span>,      <span style=\"color: #f92672;\">\"panels\"</span>: [],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"LSF scheduler statistics\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"row\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"palette-classic\"</span>          },          <span style=\"color: #f92672;\">\"custom\"</span>: {            <span style=\"color: #f92672;\">\"axisCenteredZero\"</span>: <span style=\"color: #66d9ef;\">false</span>,            <span style=\"color: #f92672;\">\"axisColorMode\"</span>: <span style=\"color: #e6db74;\">\"text\"</span>,            <span style=\"color: #f92672;\">\"axisLabel\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,            <span style=\"color: #f92672;\">\"axisPlacement\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,            <span style=\"color: #f92672;\">\"barAlignment\"</span>: <span style=\"color: #ae81ff;\">0</span>,            <span style=\"color: #f92672;\">\"drawStyle\"</span>: <span style=\"color: #e6db74;\">\"line\"</span>,            <span style=\"color: #f92672;\">\"fillOpacity\"</span>: <span style=\"color: #ae81ff;\">10</span>,            <span style=\"color: #f92672;\">\"gradientMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,            <span style=\"color: #f92672;\">\"hideFrom\"</span>: {              <span style=\"color: #f92672;\">\"graph\"</span>: <span style=\"color: #66d9ef;\">false</span>,              <span style=\"color: #f92672;\">\"legend\"</span>: <span style=\"color: #66d9ef;\">false</span>,              <span style=\"color: #f92672;\">\"tooltip\"</span>: <span style=\"color: #66d9ef;\">false</span>,              <span style=\"color: #f92672;\">\"viz\"</span>: <span style=\"color: #66d9ef;\">false</span>            },            <span style=\"color: #f92672;\">\"lineInterpolation\"</span>: <span style=\"color: #e6db74;\">\"linear\"</span>,            <span style=\"color: #f92672;\">\"lineWidth\"</span>: <span style=\"color: #ae81ff;\">1</span>,            <span style=\"color: #f92672;\">\"pointSize\"</span>: <span style=\"color: #ae81ff;\">5</span>,            <span style=\"color: #f92672;\">\"scaleDistribution\"</span>: {              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"linear\"</span>            },            <span style=\"color: #f92672;\">\"showPoints\"</span>: <span style=\"color: #e6db74;\">\"never\"</span>,            <span style=\"color: #f92672;\">\"spanNulls\"</span>: <span style=\"color: #66d9ef;\">true</span>,            <span style=\"color: #f92672;\">\"stacking\"</span>: {              <span style=\"color: #f92672;\">\"group\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,              <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>            },            <span style=\"color: #f92672;\">\"thresholdsStyle\"</span>: {              <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"off\"</span>            }          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"green\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              },              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"red\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #ae81ff;\">80</span>              }            ]          },          <span style=\"color: #f92672;\">\"unit\"</span>: <span style=\"color: #e6db74;\">\"short\"</span>        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">8</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">12</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">0</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">18</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">20</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"graph\"</span>: {},        <span style=\"color: #f92672;\">\"legend\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [],          <span style=\"color: #f92672;\">\"displayMode\"</span>: <span style=\"color: #e6db74;\">\"list\"</span>,          <span style=\"color: #f92672;\">\"placement\"</span>: <span style=\"color: #e6db74;\">\"right\"</span>,          <span style=\"color: #f92672;\">\"showLegend\"</span>: <span style=\"color: #66d9ef;\">true</span>        },        <span style=\"color: #f92672;\">\"tooltip\"</span>: {          <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"single\"</span>,          <span style=\"color: #f92672;\">\"sort\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>        }      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"7.5.15\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"CPU utilization (%)\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"procstat\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"cpu_usage\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"exe\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"mbatchd\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            }          ]        },        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Memory utilization (%)\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"procstat\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"B\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"memory_usage\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"exe\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"mbatchd\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            }          ]        },        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Number of threads\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"procstat\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"C\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"num_threads\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"exe\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"mbatchd\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            }          ]        },        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"File descriptors\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_mbatchd\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"D\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"fd\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"used\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"LSF mbatchd process metrics\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"timeseries\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"palette-classic\"</span>          },          <span style=\"color: #f92672;\">\"custom\"</span>: {            <span style=\"color: #f92672;\">\"axisCenteredZero\"</span>: <span style=\"color: #66d9ef;\">false</span>,            <span style=\"color: #f92672;\">\"axisColorMode\"</span>: <span style=\"color: #e6db74;\">\"text\"</span>,            <span style=\"color: #f92672;\">\"axisLabel\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,            <span style=\"color: #f92672;\">\"axisPlacement\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,            <span style=\"color: #f92672;\">\"barAlignment\"</span>: <span style=\"color: #ae81ff;\">0</span>,            <span style=\"color: #f92672;\">\"drawStyle\"</span>: <span style=\"color: #e6db74;\">\"line\"</span>,            <span style=\"color: #f92672;\">\"fillOpacity\"</span>: <span style=\"color: #ae81ff;\">10</span>,            <span style=\"color: #f92672;\">\"gradientMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,            <span style=\"color: #f92672;\">\"hideFrom\"</span>: {              <span style=\"color: #f92672;\">\"graph\"</span>: <span style=\"color: #66d9ef;\">false</span>,              <span style=\"color: #f92672;\">\"legend\"</span>: <span style=\"color: #66d9ef;\">false</span>,              <span style=\"color: #f92672;\">\"tooltip\"</span>: <span style=\"color: #66d9ef;\">false</span>,              <span style=\"color: #f92672;\">\"viz\"</span>: <span style=\"color: #66d9ef;\">false</span>            },            <span style=\"color: #f92672;\">\"lineInterpolation\"</span>: <span style=\"color: #e6db74;\">\"linear\"</span>,            <span style=\"color: #f92672;\">\"lineWidth\"</span>: <span style=\"color: #ae81ff;\">1</span>,            <span style=\"color: #f92672;\">\"pointSize\"</span>: <span style=\"color: #ae81ff;\">5</span>,            <span style=\"color: #f92672;\">\"scaleDistribution\"</span>: {              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"linear\"</span>            },            <span style=\"color: #f92672;\">\"showPoints\"</span>: <span style=\"color: #e6db74;\">\"never\"</span>,            <span style=\"color: #f92672;\">\"spanNulls\"</span>: <span style=\"color: #66d9ef;\">true</span>,            <span style=\"color: #f92672;\">\"stacking\"</span>: {              <span style=\"color: #f92672;\">\"group\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,              <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>            },            <span style=\"color: #f92672;\">\"thresholdsStyle\"</span>: {              <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"off\"</span>            }          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"green\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              },              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"red\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #ae81ff;\">80</span>              }            ]          },          <span style=\"color: #f92672;\">\"unit\"</span>: <span style=\"color: #e6db74;\">\"short\"</span>        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">8</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">12</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">12</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">18</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">57</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"graph\"</span>: {},        <span style=\"color: #f92672;\">\"legend\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [],          <span style=\"color: #f92672;\">\"displayMode\"</span>: <span style=\"color: #e6db74;\">\"list\"</span>,          <span style=\"color: #f92672;\">\"placement\"</span>: <span style=\"color: #e6db74;\">\"right\"</span>,          <span style=\"color: #f92672;\">\"showLegend\"</span>: <span style=\"color: #66d9ef;\">true</span>        },        <span style=\"color: #f92672;\">\"tooltip\"</span>: {          <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"single\"</span>,          <span style=\"color: #f92672;\">\"sort\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>        }      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"7.5.15\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"CPU utilization (%)\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"procstat\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"cpu_usage\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"exe\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"lim\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            }          ]        },        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Memory utilization (%)\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"procstat\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"B\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"memory_usage\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"exe\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"lim\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            }          ]        },        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Number of threads\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"procstat\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"C\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"num_threads\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"exe\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"lim\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"LSF management lim process metrics\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"timeseries\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"palette-classic\"</span>          },          <span style=\"color: #f92672;\">\"custom\"</span>: {            <span style=\"color: #f92672;\">\"axisCenteredZero\"</span>: <span style=\"color: #66d9ef;\">false</span>,            <span style=\"color: #f92672;\">\"axisColorMode\"</span>: <span style=\"color: #e6db74;\">\"text\"</span>,            <span style=\"color: #f92672;\">\"axisLabel\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,            <span style=\"color: #f92672;\">\"axisPlacement\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,            <span style=\"color: #f92672;\">\"barAlignment\"</span>: <span style=\"color: #ae81ff;\">0</span>,            <span style=\"color: #f92672;\">\"drawStyle\"</span>: <span style=\"color: #e6db74;\">\"line\"</span>,            <span style=\"color: #f92672;\">\"fillOpacity\"</span>: <span style=\"color: #ae81ff;\">10</span>,            <span style=\"color: #f92672;\">\"gradientMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,            <span style=\"color: #f92672;\">\"hideFrom\"</span>: {              <span style=\"color: #f92672;\">\"graph\"</span>: <span style=\"color: #66d9ef;\">false</span>,              <span style=\"color: #f92672;\">\"legend\"</span>: <span style=\"color: #66d9ef;\">false</span>,              <span style=\"color: #f92672;\">\"tooltip\"</span>: <span style=\"color: #66d9ef;\">false</span>,              <span style=\"color: #f92672;\">\"viz\"</span>: <span style=\"color: #66d9ef;\">false</span>            },            <span style=\"color: #f92672;\">\"lineInterpolation\"</span>: <span style=\"color: #e6db74;\">\"linear\"</span>,            <span style=\"color: #f92672;\">\"lineWidth\"</span>: <span style=\"color: #ae81ff;\">1</span>,            <span style=\"color: #f92672;\">\"pointSize\"</span>: <span style=\"color: #ae81ff;\">5</span>,            <span style=\"color: #f92672;\">\"scaleDistribution\"</span>: {              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"linear\"</span>            },            <span style=\"color: #f92672;\">\"showPoints\"</span>: <span style=\"color: #e6db74;\">\"never\"</span>,            <span style=\"color: #f92672;\">\"spanNulls\"</span>: <span style=\"color: #66d9ef;\">true</span>,            <span style=\"color: #f92672;\">\"stacking\"</span>: {              <span style=\"color: #f92672;\">\"group\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,              <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>            },            <span style=\"color: #f92672;\">\"thresholdsStyle\"</span>: {              <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"off\"</span>            }          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"green\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              },              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"red\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #ae81ff;\">80</span>              }            ]          },          <span style=\"color: #f92672;\">\"unit\"</span>: <span style=\"color: #e6db74;\">\"short\"</span>        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">8</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">12</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">0</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">26</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">27</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"graph\"</span>: {},        <span style=\"color: #f92672;\">\"legend\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [],          <span style=\"color: #f92672;\">\"displayMode\"</span>: <span style=\"color: #e6db74;\">\"list\"</span>,          <span style=\"color: #f92672;\">\"placement\"</span>: <span style=\"color: #e6db74;\">\"right\"</span>,          <span style=\"color: #f92672;\">\"showLegend\"</span>: <span style=\"color: #66d9ef;\">true</span>        },        <span style=\"color: #f92672;\">\"tooltip\"</span>: {          <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"single\"</span>,          <span style=\"color: #f92672;\">\"sort\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>        }      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"7.5.15\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Job buckets\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_mbatchd\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"sched\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"buckets\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            }          ]        },        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Matching host criteria\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_mbatchd\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"B\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"sched\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"matchhost\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            }          ]        },        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Scheduling interval (seconds)\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_mbatchd\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"C\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"sched\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"interval\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"LSF scheduler metrics\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"timeseries\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"palette-classic\"</span>          },          <span style=\"color: #f92672;\">\"custom\"</span>: {            <span style=\"color: #f92672;\">\"axisCenteredZero\"</span>: <span style=\"color: #66d9ef;\">false</span>,            <span style=\"color: #f92672;\">\"axisColorMode\"</span>: <span style=\"color: #e6db74;\">\"text\"</span>,            <span style=\"color: #f92672;\">\"axisLabel\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,            <span style=\"color: #f92672;\">\"axisPlacement\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,            <span style=\"color: #f92672;\">\"barAlignment\"</span>: <span style=\"color: #ae81ff;\">0</span>,            <span style=\"color: #f92672;\">\"drawStyle\"</span>: <span style=\"color: #e6db74;\">\"line\"</span>,            <span style=\"color: #f92672;\">\"fillOpacity\"</span>: <span style=\"color: #ae81ff;\">10</span>,            <span style=\"color: #f92672;\">\"gradientMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,            <span style=\"color: #f92672;\">\"hideFrom\"</span>: {              <span style=\"color: #f92672;\">\"graph\"</span>: <span style=\"color: #66d9ef;\">false</span>,              <span style=\"color: #f92672;\">\"legend\"</span>: <span style=\"color: #66d9ef;\">false</span>,              <span style=\"color: #f92672;\">\"tooltip\"</span>: <span style=\"color: #66d9ef;\">false</span>,              <span style=\"color: #f92672;\">\"viz\"</span>: <span style=\"color: #66d9ef;\">false</span>            },            <span style=\"color: #f92672;\">\"lineInterpolation\"</span>: <span style=\"color: #e6db74;\">\"linear\"</span>,            <span style=\"color: #f92672;\">\"lineWidth\"</span>: <span style=\"color: #ae81ff;\">1</span>,            <span style=\"color: #f92672;\">\"pointSize\"</span>: <span style=\"color: #ae81ff;\">5</span>,            <span style=\"color: #f92672;\">\"scaleDistribution\"</span>: {              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"linear\"</span>            },            <span style=\"color: #f92672;\">\"showPoints\"</span>: <span style=\"color: #e6db74;\">\"never\"</span>,            <span style=\"color: #f92672;\">\"spanNulls\"</span>: <span style=\"color: #66d9ef;\">true</span>,            <span style=\"color: #f92672;\">\"stacking\"</span>: {              <span style=\"color: #f92672;\">\"group\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,              <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>            },            <span style=\"color: #f92672;\">\"thresholdsStyle\"</span>: {              <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"off\"</span>            }          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"green\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              },              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"red\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #ae81ff;\">80</span>              }            ]          },          <span style=\"color: #f92672;\">\"unit\"</span>: <span style=\"color: #e6db74;\">\"short\"</span>        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">8</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">12</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">12</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">26</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">58</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"graph\"</span>: {},        <span style=\"color: #f92672;\">\"legend\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [],          <span style=\"color: #f92672;\">\"displayMode\"</span>: <span style=\"color: #e6db74;\">\"list\"</span>,          <span style=\"color: #f92672;\">\"placement\"</span>: <span style=\"color: #e6db74;\">\"right\"</span>,          <span style=\"color: #f92672;\">\"showLegend\"</span>: <span style=\"color: #66d9ef;\">true</span>        },        <span style=\"color: #f92672;\">\"tooltip\"</span>: {          <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"single\"</span>,          <span style=\"color: #f92672;\">\"sort\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>        }      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"7.5.15\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"CPU utilization (%)\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"procstat\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"cpu_usage\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"exe\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"mbschd\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            }          ]        },        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Memory utilization (%)\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"procstat\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"B\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"memory_usage\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"exe\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"mbatchd\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            }          ]        },        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Number of threads\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"procstat\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"C\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"num_threads\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"exe\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"mbatchd\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"LSF mbschd process metrics\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"timeseries\"</span>    },    {      <span style=\"color: #f92672;\">\"collapsed\"</span>: <span style=\"color: #66d9ef;\">false</span>,      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">1</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">24</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">0</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">34</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">39</span>,      <span style=\"color: #f92672;\">\"panels\"</span>: [],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Additional metrics (scratch)\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"row\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"green\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          }        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">0</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">35</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">2</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"colorMode\"</span>: <span style=\"color: #e6db74;\">\"value\"</span>,        <span style=\"color: #f92672;\">\"graphMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,        <span style=\"color: #f92672;\">\"justifyMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"text\"</span>: {},        <span style=\"color: #f92672;\">\"textMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_jobs\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"distinct\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"running\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Running\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"stat\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"yellow\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          }        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">35</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">5</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"colorMode\"</span>: <span style=\"color: #e6db74;\">\"value\"</span>,        <span style=\"color: #f92672;\">\"graphMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,        <span style=\"color: #f92672;\">\"justifyMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"text\"</span>: {},        <span style=\"color: #f92672;\">\"textMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_jobs\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"default\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"pending\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Pending\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"stat\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"red\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          }        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">6</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">35</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">6</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"colorMode\"</span>: <span style=\"color: #e6db74;\">\"value\"</span>,        <span style=\"color: #f92672;\">\"graphMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,        <span style=\"color: #f92672;\">\"justifyMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"text\"</span>: {},        <span style=\"color: #f92672;\">\"textMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_jobs\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"default\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"suspended\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Suspended\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"stat\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"blue\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          }        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">9</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">35</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">7</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"colorMode\"</span>: <span style=\"color: #e6db74;\">\"value\"</span>,        <span style=\"color: #f92672;\">\"graphMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,        <span style=\"color: #f92672;\">\"justifyMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"text\"</span>: {},        <span style=\"color: #f92672;\">\"textMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_jobs\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"default\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"finished\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Finished\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"stat\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"green\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          }        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">12</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">35</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">15</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"colorMode\"</span>: <span style=\"color: #e6db74;\">\"value\"</span>,        <span style=\"color: #f92672;\">\"graphMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,        <span style=\"color: #f92672;\">\"justifyMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"text\"</span>: {},        <span style=\"color: #f92672;\">\"textMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Ok\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_servers\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"status\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"ok\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Ok\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"stat\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"blue\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          }        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">15</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">35</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">16</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"colorMode\"</span>: <span style=\"color: #e6db74;\">\"value\"</span>,        <span style=\"color: #f92672;\">\"graphMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,        <span style=\"color: #f92672;\">\"justifyMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"text\"</span>: {},        <span style=\"color: #f92672;\">\"textMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Closed\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_servers\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"status\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"closed\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Closed\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"stat\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"yellow\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          }        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">18</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">35</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">17</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"colorMode\"</span>: <span style=\"color: #e6db74;\">\"value\"</span>,        <span style=\"color: #f92672;\">\"graphMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,        <span style=\"color: #f92672;\">\"justifyMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"text\"</span>: {},        <span style=\"color: #f92672;\">\"textMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Unreachable\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_servers\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"status\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"unreachable\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Unreachable\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"stat\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"red\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          }        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">21</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">35</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">18</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"colorMode\"</span>: <span style=\"color: #e6db74;\">\"value\"</span>,        <span style=\"color: #f92672;\">\"graphMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,        <span style=\"color: #f92672;\">\"justifyMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"text\"</span>: {},        <span style=\"color: #f92672;\">\"textMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Unavailable\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_servers\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"status\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"unavailable\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Unavailable\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"stat\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"green\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          }        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">0</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">39</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">21</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"colorMode\"</span>: <span style=\"color: #e6db74;\">\"value\"</span>,        <span style=\"color: #f92672;\">\"graphMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,        <span style=\"color: #f92672;\">\"justifyMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"text\"</span>: {},        <span style=\"color: #f92672;\">\"textMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Clients\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_hosts\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"current\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"clients\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Clients\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"stat\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"green\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          }        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">39</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">22</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"colorMode\"</span>: <span style=\"color: #e6db74;\">\"value\"</span>,        <span style=\"color: #f92672;\">\"graphMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,        <span style=\"color: #f92672;\">\"justifyMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"text\"</span>: {},        <span style=\"color: #f92672;\">\"textMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Servers\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_hosts\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"current\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"servers\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Servers\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"stat\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"green\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          }        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">6</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">39</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">23</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"colorMode\"</span>: <span style=\"color: #e6db74;\">\"value\"</span>,        <span style=\"color: #f92672;\">\"graphMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,        <span style=\"color: #f92672;\">\"justifyMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"text\"</span>: {},        <span style=\"color: #f92672;\">\"textMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Servers\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_hosts\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"current\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"cpus\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"CPUs\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"stat\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"green\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          }        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">9</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">39</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">24</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"colorMode\"</span>: <span style=\"color: #e6db74;\">\"value\"</span>,        <span style=\"color: #f92672;\">\"graphMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,        <span style=\"color: #f92672;\">\"justifyMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"text\"</span>: {},        <span style=\"color: #f92672;\">\"textMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Cores\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_hosts\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"current\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"cores\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Cores\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"stat\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"green\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          }        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">12</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">39</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">25</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"colorMode\"</span>: <span style=\"color: #e6db74;\">\"value\"</span>,        <span style=\"color: #f92672;\">\"graphMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,        <span style=\"color: #f92672;\">\"justifyMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"text\"</span>: {},        <span style=\"color: #f92672;\">\"textMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Slots\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_hosts\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"current\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"slots\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Slots\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"stat\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"min\"</span>: <span style=\"color: #ae81ff;\">0</span>,          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"percentage\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"green\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          },          <span style=\"color: #f92672;\">\"unit\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">43</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">52</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"/^lsf_hosts\\\\.last$/\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"showThresholdLabels\"</span>: <span style=\"color: #66d9ef;\">false</span>,        <span style=\"color: #f92672;\">\"showThresholdMarkers\"</span>: <span style=\"color: #66d9ef;\">true</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_hosts\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"current\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ],            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"peak\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"servers\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Servers\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"gauge\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"min\"</span>: <span style=\"color: #ae81ff;\">0</span>,          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"percentage\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"yellow\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          },          <span style=\"color: #f92672;\">\"unit\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">6</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">43</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">51</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"/^lsf_hosts\\\\.last$/\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"showThresholdLabels\"</span>: <span style=\"color: #66d9ef;\">false</span>,        <span style=\"color: #f92672;\">\"showThresholdMarkers\"</span>: <span style=\"color: #66d9ef;\">true</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_hosts\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"current\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ],            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"peak\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"cpus\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"CPUs\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"gauge\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"min\"</span>: <span style=\"color: #ae81ff;\">0</span>,          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"percentage\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"light-red\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          },          <span style=\"color: #f92672;\">\"unit\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">9</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">43</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">50</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"/^lsf_hosts\\\\.last$/\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"showThresholdLabels\"</span>: <span style=\"color: #66d9ef;\">false</span>,        <span style=\"color: #f92672;\">\"showThresholdMarkers\"</span>: <span style=\"color: #66d9ef;\">true</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_hosts\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"current\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ],            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"peak\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"cores\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Cores\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"gauge\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"min\"</span>: <span style=\"color: #ae81ff;\">0</span>,          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"percentage\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"blue\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          },          <span style=\"color: #f92672;\">\"unit\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">12</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">43</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">49</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"/^lsf_hosts\\\\.last$/\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"showThresholdLabels\"</span>: <span style=\"color: #66d9ef;\">false</span>,        <span style=\"color: #f92672;\">\"showThresholdMarkers\"</span>: <span style=\"color: #66d9ef;\">true</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_hosts\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"current\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ],            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"peak\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"slots\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Slots\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"gauge\"</span>    }  ],  <span style=\"color: #f92672;\">\"refresh\"</span>: <span style=\"color: #e6db74;\">\"30s\"</span>,  <span style=\"color: #f92672;\">\"schemaVersion\"</span>: <span style=\"color: #ae81ff;\">37</span>,  <span style=\"color: #f92672;\">\"style\"</span>: <span style=\"color: #e6db74;\">\"dark\"</span>,  <span style=\"color: #f92672;\">\"tags\"</span>: [],  <span style=\"color: #f92672;\">\"templating\"</span>: {    <span style=\"color: #f92672;\">\"list\"</span>: [      {        <span style=\"color: #f92672;\">\"current\"</span>: {          <span style=\"color: #f92672;\">\"selected\"</span>: <span style=\"color: #66d9ef;\">true</span>,          <span style=\"color: #f92672;\">\"text\"</span>: [            <span style=\"color: #e6db74;\">\"priority\"</span>          ],          <span style=\"color: #f92672;\">\"value\"</span>: [            <span style=\"color: #e6db74;\">\"priority\"</span>          ]        },        <span style=\"color: #f92672;\">\"datasource\"</span>: {          <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,          <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"oSnSlVc4k\"</span>        },        <span style=\"color: #f92672;\">\"definition\"</span>: <span style=\"color: #e6db74;\">\"show tag values from \\\"lsf_queues\\\" with key=\\\"name\\\"\"</span>,        <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #ae81ff;\">0</span>,        <span style=\"color: #f92672;\">\"includeAll\"</span>: <span style=\"color: #66d9ef;\">false</span>,        <span style=\"color: #f92672;\">\"multi\"</span>: <span style=\"color: #66d9ef;\">false</span>,        <span style=\"color: #f92672;\">\"name\"</span>: <span style=\"color: #e6db74;\">\"Queue\"</span>,        <span style=\"color: #f92672;\">\"options\"</span>: [],        <span style=\"color: #f92672;\">\"query\"</span>: <span style=\"color: #e6db74;\">\"show tag values from \\\"lsf_queues\\\" with key=\\\"name\\\"\"</span>,        <span style=\"color: #f92672;\">\"refresh\"</span>: <span style=\"color: #ae81ff;\">1</span>,        <span style=\"color: #f92672;\">\"regex\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,        <span style=\"color: #f92672;\">\"skipUrlSync\"</span>: <span style=\"color: #66d9ef;\">false</span>,        <span style=\"color: #f92672;\">\"sort\"</span>: <span style=\"color: #ae81ff;\">0</span>,        <span style=\"color: #f92672;\">\"tagValuesQuery\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,        <span style=\"color: #f92672;\">\"tagsQuery\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"query\"</span>,        <span style=\"color: #f92672;\">\"useTags\"</span>: <span style=\"color: #66d9ef;\">false</span>      }    ]  },  <span style=\"color: #f92672;\">\"time\"</span>: {    <span style=\"color: #f92672;\">\"from\"</span>: <span style=\"color: #e6db74;\">\"now-1h\"</span>,    <span style=\"color: #f92672;\">\"to\"</span>: <span style=\"color: #e6db74;\">\"now\"</span>  },  <span style=\"color: #f92672;\">\"timepicker\"</span>: {},  <span style=\"color: #f92672;\">\"timezone\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,  <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"LSF cluster status\"</span>,  <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"ORojp8cVz\"</span>,  <span style=\"color: #f92672;\">\"version\"</span>: <span style=\"color: #ae81ff;\">160</span>,  <span style=\"color: #f92672;\">\"weekStart\"</span>: <span style=\"color: #e6db74;\">\"\"</span>}</code></pre></div></details><hr /><p>As you can see, with a short plugin script to collect information from LSF, it&rsquo;s possible to monitor your LSF cluster using the TIG stack. It&rsquo;s important to note that there are powerfulmonitoring and reporting tools available from IBM as add-ons to LSF; IBM Spectrum LSF RTM and IBM Spectrum LSF Explorer. You can find more details about the add-on capabilities for LSF<a href=\"https://www.ibm.com/products/hpc-workload-management/resources\">here</a>.</p>",
            "url": "https://hpc.social/personal-blog/2023/monitoring-ibm-spectrum-lsf-with-the-tig-stack/",
            
            
            
            
            
            "date_published": "2023-01-24T19:48:44-07:00",
            "date_modified": "2023-01-24T19:48:44-07:00",
            
                "author": "Ramblings of a supercomputing enthusiast."
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/adam-s-weekly-ish-update-2022-12-20/",
            "title": "Adam’s weekly (-ish) update, 2022-12-20",
            "summary": null,
            "content_text": "What&#8217;s newThe past few weeks have been on the intense side at work, so I completely lost track of the blog and haven&#8217;t had a chance to write much in that time. However, I&#8217;m now on a holiday break, and finally have time to sit down at a keyboard to write more than code and Slack messages.One of the highlights of the past few weeks was a trip to San Jose, and the NVIDIA headquarters. I changed teams at work back in July, transferring from a group that was closely integrated with product management, to a more straightforward engineering team which designs and builds new high-performance computing systems. This was the first chance I&#8217;ve had to meet up with other members of my new team in person, and it was a really wonderful experience to be in the same physical space as folks who were previously just images on my screen. I love working remotely, but it&#8217;s also great to be able to stand in front of a white board with someone and brainstorm, or get coffee and just have a chat with a coworker outside of a video call with an agenda.(Plus, we were all careful and managed to avoid catching COVID from each other! Which was a win on its own.)Now, for the next two weeks I&#8217;m off work, and planning to take some time to relax and spend time on projects that are harder to focus on during busy work weeks. Expect (maybe) less about computers in my blog and social feeds, and more about D&amp;D, baking, and tasty cocktails.What I&#8217;m reading, watching, and listening toI&#8217;ve been a bit too scattered to focus on actual books the past few weeks, but I did find time for a few interesting articles and podcasts. In particular,&#8220;Why Roman Egypt was such a strange province&#8221;, from Bret Devereaux: As usual from Devereaux, an accessible but extremely detailed discussion of why so much of what we know about the Roman empire is from Egyptian records, but why that also might not be representative of the broader empire.&#8220;Emoji as incident resolution tools&#8221;, from Will Gallego: A fun discussion of how using emoji as part of a team&#8217;s communication can add nuance and shared understanding during incident management, along with a discussion of the disadvantages and costs associated with the practice.&#8220;What does modern software architecture look like in 2022?&#8221;, from Bartosz Mikulski: A nice  article which discusses how service-oriented software architecture can often include an explicit expectation of change. For example, the architecture might include notes on an ongoing deprecation of a library, or might signpost the need to factor a new microservice out when overall system load gets high enough.The Brady Heywood podcast: Found via the Oxide and Friends podcast, the Brady Heywood podcast is a series on engineering disasters and their consequences from a forensic engineering firm. It&#8217;s mostly not being updated any more (with the podcasters moving on to a separate series on complexity science), but it has a deep back catalog of good episodes, and includes thoughtful discussions of human factors, safety engineering, and how organizational pressures become manifest in engineering artifacts.Recent recipesSmitten Kitchen&#8217;s Homemade Irish Cream: This is a recipe I make every year, and I often give away small bottles of it as holiday gifts. It&#8217;s really ridiculously tasty, much better than Baileys  or similar, and good either on its own or in hot chocolate.Smitten Kitchen&#8217;s Fairytale of New York: This is a really tasty whiskey cocktail, and the star of the show is a &#8220;winter warmth syrup&#8221; that substitutes in for simple syrup. The syrup is simply very tasty, and turns what&#8217;s effectively an OId Fashioned variant into a lovely holiday cocktail.Sparkling gingerbread from Yossy Arefi&#8217;s Snaking Cakes: This recipe takes a little more prep than most of Arefi&#8217;s &#8220;snacking cakes&#8221;, as it includes ginger three ways (ground, fresh, and crystallized), but it&#8217;s worth the few minutes of extra work.Pet photosI&#8217;m pretty sure these two want me to turn the fireplace on.Just Percy bullying the dog by stealing his bed.",
            "content_html": "<h2>What&#8217;s new</h2><p>The past few weeks have been on the intense side at work, so I completely lost track of the blog and haven&#8217;t had a chance to write much in that time. However, I&#8217;m now on a holiday break, and finally have time to sit down at a keyboard to write more than code and Slack messages.</p><p><span id=\"more-289\"></span></p><p>One of the highlights of the past few weeks was a trip to San Jose, and the NVIDIA headquarters. I changed teams at work back in July, transferring from a group that was closely integrated with product management, to a more straightforward engineering team which <a href=\"https://blogs.nvidia.com/blog/2020/08/14/making-selene-pandemic-ai/\">designs and builds new high-performance computing systems</a>. </p><p>This was the first chance I&#8217;ve had to meet up with other members of my new team in person, and it was a really wonderful experience to be in the same physical space as folks who were previously just images on my screen. I love working remotely, but it&#8217;s also great to be able to stand in front of a white board with someone and brainstorm, or get coffee and just have a chat with a coworker outside of a video call with an agenda.</p><p>(Plus, we were all careful and managed to avoid catching COVID from each other! Which was a win on its own.)</p><p>Now, for the next two weeks I&#8217;m off work, and planning to take some time to relax and spend time on projects that are harder to focus on during busy work weeks. Expect (maybe) less about computers in my blog and social feeds, and more about D&amp;D, baking, and tasty cocktails.</p><h2>What I&#8217;m reading, watching, and listening to</h2><p>I&#8217;ve been a bit too scattered to focus on actual books the past few weeks, but I did find time for a few interesting articles and podcasts. In particular,</p><ul><li><a href=\"https://acoup.blog/2022/12/02/collections-why-roman-egypt-was-such-a-strange-province/\">&#8220;Why Roman Egypt was such a strange province&#8221;</a>, from Bret Devereaux: As usual from Devereaux, an accessible but extremely detailed discussion of why so much of what we know about the Roman empire is from Egyptian records, but why that also might not be representative of the broader empire.</li><li><a href=\"https://willgallego.com/2022/12/18/emoji-as-incident-resolution-tools/\">&#8220;Emoji as incident resolution tools&#8221;</a>, from Will Gallego: A fun discussion of how using emoji as part of a team&#8217;s communication can add nuance and shared understanding during incident management, along with a discussion of the disadvantages and costs associated with the practice.</li><li><a href=\"https://www.mikulskibartosz.name/modern-software-architecture-in-2022/\">&#8220;What does modern software architecture look like in 2022?&#8221;</a>, from Bartosz Mikulski: A nice  article which discusses how service-oriented software architecture can often include an explicit expectation of change. For example, the architecture might include notes on an ongoing deprecation of a library, or might signpost the need to factor a new microservice out when overall system load gets high enough.</li><li><a href=\"https://www.bradyheywood.com.au/podcasts/\">The Brady Heywood podcast</a>: Found via the <a href=\"https://oxide.computer/podcasts/oxide-and-friends/1137359\">Oxide and Friends podcast</a>, the Brady Heywood podcast is a series on engineering disasters and their consequences from a forensic engineering firm. It&#8217;s mostly not being updated any more (with the podcasters moving on to a separate series on complexity science), but it has a deep back catalog of good episodes, and includes thoughtful discussions of human factors, safety engineering, and how organizational pressures become manifest in engineering artifacts.</li></ul><h2>Recent recipes</h2><ul><li><a href=\"https://smittenkitchen.com/2016/12/homemade-irish-cream/\">Smitten Kitchen&#8217;s Homemade Irish Cream</a>: This is a recipe I make every year, and I often give away small bottles of it as holiday gifts. It&#8217;s really ridiculously tasty, much better than Baileys  or similar, and good either on its own or in hot chocolate.</li><li><a href=\"https://smittenkitchen.com/2014/12/fairytale-of-new-york/\">Smitten Kitchen&#8217;s Fairytale of New York</a>: This is a really tasty whiskey cocktail, and the star of the show is a &#8220;winter warmth syrup&#8221; that substitutes in for simple syrup. The syrup is simply very tasty, and turns what&#8217;s effectively an OId Fashioned variant into a lovely holiday cocktail.</li><li>Sparkling gingerbread from <a href=\"http://www.apt2bbakingco.com/snacking-cakes\">Yossy Arefi&#8217;s Snaking Cakes</a>: This recipe takes a little more prep than most of Arefi&#8217;s &#8220;snacking cakes&#8221;, as it includes ginger three ways (ground, fresh, and crystallized), but it&#8217;s worth the few minutes of extra work.</li></ul><h2>Pet photos</h2><figure class=\"wp-block-image size-large is-resized\"><img alt=\"A white calico cat and a gray tabby cat lounging on a large brown pet bed in front of a gas fireplace.\" class=\"wp-image-295\" height=\"512\" src=\"https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_7207-768x1024.jpeg\" width=\"384\" /><figcaption class=\"wp-element-caption\">I&#8217;m pretty sure these two want me to turn the fireplace on.</figcaption></figure><figure class=\"wp-block-image size-large is-resized\"><img alt=\"A gray tabby cat lounges on a dog bed, while a golden doodle lays on the floor nearby and looks forlornly at the bed.\" class=\"wp-image-294\" height=\"512\" src=\"https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_1725-1024x1024.jpeg\" width=\"512\" /><figcaption class=\"wp-element-caption\">Just Percy bullying the dog by stealing his bed.</figcaption></figure>",
            "url": "https://hpc.social/personal-blog/2022/adam-s-weekly-ish-update-2022-12-20/",
            
            
            
            
            
            "date_published": "2022-12-20T18:14:52-07:00",
            "date_modified": "2022-12-20T18:14:52-07:00",
            
                "author": "Thinking Out Loud"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/visualizing-spectrum-lsf-data-with-grafana/",
            "title": "Visualizing Spectrum LSF data with Grafana",
            "summary": null,
            "content_text": "OverviewSystem monitoring is a fundamental part of IT best practices. High performance computing (HPC) environments are no exception to this. At the high-end, HPC clusters can consist ofthousands of servers, processing millions of jobs per day. HPC admins need ways to monitor the overall cluster to determine system status and availability through to the efficiencyof workloads. Servers today produce a wide array of metrics which can be monitored for example to check for various conditions. Additionally, workload schedulers also produce a wealthof data about jobs. Having a single dashboard to show this type of detail can be of great benefit.IBM Spectrum LSF Suites provide a complete solution for HPC workload management. This includes reporting capabilities out of the box. Spectrum LSF Suite features an integrated webinterface for job management and reporting. The reporting capabilities include a number of reports out of the box, with the ability to customize and add new reports. The reportingcapability in Spectrum LSF Suite and IBM Spectrum LSF Explorer is underpinned by Elasticsearch, which is used to store, index and query data. With LSF data in Elasticsearch, it’salso possible to configure LSF command-line interface (CLI) tools to query information from Elasticsearch rather than flat files – for greater performance. This is controlled viathe LSF_QUERY_ES_FUNCTIONS parameter of Spectrum LSF. More details about the LSF_QUERY_ES_FUNCTIONS can be found in the LSF documentation here.(1) Here is a look at the indices that are created by LSF in Elasticsearch. Note that the status shows as yellow because I only have a single Elasticsearch node.# curl -XGET localhost:9200/_cat/indicesyellow open lsf_events-202205             tejh7jsMSwSeQUJzYM7cww 5 1    1137     0 808.1kb 808.1kbyellow open lsf_jobs_pendingreason-202204 4wi7Ta8uQPSXlFBqPh4kOQ 5 1   90531     0   8.6mb   8.6mbyellow open lsf_events-202204             tWYvW_w8TVyU1deRFOEoZg 5 1  116957 32691  59.1mb  59.1mbyellow open lsf_jobs_active-202212        Q0pStQxvTgaeL7R-f02XWA 5 1  210052     0  50.6mb  50.6mbyellow open lsf_jobs_pendingreason-202206 ENWIwfGrSqCHvi53aUQXJQ 5 1   44991     0   4.5mb   4.5mbyellow open host_booleanres_latest        RE8thZCgTGeMBGodeMfXEQ 5 1       5     0  23.3kb  23.3kbyellow open lsf_jobs_pendingreason-202205 yo0iZH_4TvOqq6kQgBluvA 5 1     111     0 181.4kb 181.4kbyellow open lsf_jobs_pend-202212          9ViIS3nDRFewrqtILEbKTQ 5 1     707     0 446.9kb 446.9kbyellow open lsf_hostconf_latest           9N1Y8ML4TiyaamCPEDRQog 5 1       2     0  10.6kb  10.6kbyellow open lsf_events-202209             rtKQ8F4bSleHl8EbAQez8A 5 1    8200   955   4.4mb   4.4mbyellow open lsf_events-202206             UUKPWfN7SZ-dzVs5NAkjUg 5 1   79503 23452  36.8mb  36.8mbyellow open lsf_hostmetrics-202209        7FUNFCWPQtuGyx5jTJLb1A 5 1    4701     0   2.2mb   2.2mbyellow open lsf_hostmetrics-202208        52xef_3hQWK-jVuJqyUpHA 5 1    3823     0   1.9mb   1.9mbyellow open lsf_hostmetrics-202207        IqZYhU0RQNGIFWSRH-Ym8Q 5 1    6316     0   2.9mb   2.9mbyellow open lsf_job_acct-202209           h1ZgCSB8RwCBxwIUUzDHEQ 5 1    2050   438   1.9mb   1.9mbyellow open lsf_jobs_active-202209        iBfnf07CTcS7Gb6TxwomRA 5 1    2658     0     1mb     1mbyellow open lsf_hostmetrics-202206        0PXSYBOgTA2Qa_zzaafUPg 5 1    4301     0   2.1mb   2.1mbyellow open model                         xSqB_T_VSByOzYavEcEVyQ 1 1      55     0   257kb   257kbyellow open lsf_job_acct-202206           C639GnzBSjCEVczfh5u23g 5 1   16719   353   8.9mb   8.9mbyellow open lsf_jobs_active-202204        8gN_ENkQRTSfnmxrtMcOlA 5 1   33286     0   9.8mb   9.8mbyellow open lsf_job_acct-202205           LOxmhm_8RxaCuTd7YWYbLw 5 1     274     0 439.4kb 439.4kbyellow open lsf_jobs_active-202205        61u2RlXgR_SXagmZfrmttQ 5 1    1880     0   1.1mb   1.1mbyellow open lsf_jobs_pend-202209          eTgqPp9nQOScNiwyUWXmHA 5 1       9     0 106.2kb 106.2kbyellow open lsf_job_acct-202204           dDDegS6RQSWtWN99eklexg 5 1   28902  2177  17.4mb  17.4mbyellow open lsf_jobs_active-202206        8ivkjWSNR1Sh_BxWACP0ZA 5 1   16921     0   4.6mb   4.6mbyellow open lsf_current_status            92KE3V4YSJ-RtRp_kepxYg 5 1  115450     0     9mb     9mbyellow open lsf_hostmetrics-202210        vbuK2wW3RRmXuY07tDPUNQ 5 1     785     0 942.1kb 942.1kbyellow open lsf_jobs_pend-202206          OhSwn-b0SiSj8mCW5tcNIA 5 1      22     0 244.6kb 244.6kbyellow open lsf_jobs_pend-202205          OfBtWklETYK9cRx000aNPw 5 1       1     0  12.7kb  12.7kbyellow open lsf_events-202212             WUC5KJWmS-2WIN8XCQpSuw 5 1  712399 74728   337mb   337mbyellow open lsf_jobs_pend-202204          OhUsXqohSciZTPZlTryMyA 5 1      50     0 275.3kb 275.3kbyellow open resource_attributes_latest    R9bk_WIPTU62dVg3O1LDBA 5 1       5     0  24.4kb  24.4kbyellow open lsf_jobs_pendingreason-202212 55iwDC5mRI-eRbzQLwWP6Q 5 1 3314828     0 288.7mb 288.7mbyellow open pa-lite-log                   o8-jaNoGTsSVcjJW5Ufs0w 5 1    1549     0 547.2kb 547.2kbyellow open lsf_job_acct-202212           4HXvAD02Sxq0tgp2fS2cfQ 5 1  161502     0  73.6mb  73.6mbyellow open lsf_hostmetrics-202212        Tki6OJ41R363u9Tx02N4zw 5 1    2548     0   1.7mb   1.7mbyellow open lsf_jobs_pendingreason-202209 D3TOZY2ORiK9PppGVt10Fg 5 1    2511     0 381.4kb 381.4kb(2) With the LSF data stored in Elasticsearch, the next step is to connect to the Grafana server. Here we point our browser to the Grafana server on the default port: http://lsf_manager:3000 and login to Grafana. This step assumes an account has already been setup on Grafana. Here we are using the default admin account.(3) In Grafana, navigate to Configuration -&gt; Data sources. It’s here that it will be possible to add an Elasticsearch data source(4) Next, click the Add data source button.(5 In the list of data sources, filter by name for Elasticsearch and click the Select button on the Elasticsearch entry.(6) When configuring the data source, it’s necessary to specify an index name. This is where the list of indices in Elasticsearch that we generated earlier will come in handy. For this example, we wish to display the total number of pending jobs in the Spectrum LSF cluster over time. This data is stored in the lsf_jobs_pend* indices in Elasticsearch. To configure the data source appropriately, we specify the following values:Name:\t“LSF pending jobs”URL: http://localhost:9200Index name: “lsf_jobs_pend*”Time field name: “time_stamp”Version: 7.0+Note that the URL needs to point to the Elasticsearch server. In this case, both the Elasticsearch server and Grafana server are running on the same host.Next click on the Save &amp; Test button. It should return the message Index OK. Time field name OK..Assuming that no errors were found, click on the Back button.(7) Now you should see LSF pending jobs listed as a Data Source.(8) With the data source configured, we’re now ready to configure a dashboard to display the LSF pending job information. Navigate to Create -&gt; Dashboard.(9) Click on Add an empty panel. This is used to create a new panel where the LSF pending job information will be plotted.(10) In the panel editor, specify the following options:Panel title: “LSF pending jobs”Specify the data source “LSF pending jobs” which was created previouslySpecify a suitable time range (2 days)Line width (5 points)You should immediately see in the panel editor the plot of the hourly pending jobs.  Click on the Apply button to save the changes.(11) After clicking Apply, you will be returned to the Dashboard screen. The Dashboard should now display the new LSF pending jobs panel that was created above. This Dashboard could also include panels for system metrics collected by Prometheus for example.(12) Next, click on the diskette icon in the upper right to save the Dashboard with the LSF pending jobs panel.  We’ll name it Spectrum LSF cluster status.Additional panels can be added to the Spectrum LSF cluster status based on the data logged by Spectrum LSF to Elasticsearch.That concludes the simple example of plotting Spectrum LSF cluster data from Elasticsearch in Grafana.  As mentioned, the IBM Spectrum LSF Suites integrated web interface also provides reporting capabilities, with several built-in reports provided out of the box. Below, we’ve included a screenshot of the pending job analysis report included with Spectrum LSF Suites.SummarySpectrum LSF provides many hooks and integration points enabling administrators to change things ranging from scheduling behavior and the output of query commands through to job information being logged to Elasticsearch. Spectrum LSF is highly customizable by organizations to suit specific needs and requirements. We’ve demonstrated this using Grafana to visualize data from the LSF scheduler in a simple example. Following the above example, administrators can combine existing HPC cluster system level reporting in Grafana with job information from Spectrum LSF for a better overall view and understanding of the infrastructure.",
            "content_html": "<p><strong>Overview</strong></p><p>System monitoring is a fundamental part of IT best practices. High performance computing (HPC) environments are no exception to this. At the high-end, HPC clusters can consist ofthousands of servers, processing millions of jobs per day. HPC admins need ways to monitor the overall cluster to determine system status and availability through to the efficiencyof workloads. Servers today produce a wide array of metrics which can be monitored for example to check for various conditions. Additionally, workload schedulers also produce a wealthof data about jobs. Having a single dashboard to show this type of detail can be of great benefit.</p><p><a href=\"https://www.ibm.com/products/hpc-workload-management\">IBM Spectrum LSF Suites</a> provide a complete solution for HPC workload management. This includes reporting capabilities out of the box. Spectrum LSF Suite features an integrated webinterface for job management and reporting. The reporting capabilities include a number of reports out of the box, with the ability to customize and add new reports. The reportingcapability in Spectrum LSF Suite and IBM Spectrum LSF Explorer is underpinned by Elasticsearch, which is used to store, index and query data. With LSF data in Elasticsearch, it’salso possible to configure LSF command-line interface (CLI) tools to query information from Elasticsearch rather than flat files – for greater performance. This is controlled viathe <strong>LSF_QUERY_ES_FUNCTIONS</strong> parameter of Spectrum LSF. More details about the <strong>LSF_QUERY_ES_FUNCTIONS</strong> can be found in the LSF documentation <a href=\"https://www.ibm.com/docs/en/spectrum-lsf/10.1.0?topic=lsfconf-lsf-query-es-functions\">here</a>.</p><p>(1) Here is a look at the indices that are created by LSF in Elasticsearch. Note that the status shows as yellow because I only have a single Elasticsearch node.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\"># curl -XGET localhost:9200/_cat/indicesyellow open lsf_events-202205             tejh7jsMSwSeQUJzYM7cww 5 1    1137     0 808.1kb 808.1kbyellow open lsf_jobs_pendingreason-202204 4wi7Ta8uQPSXlFBqPh4kOQ 5 1   90531     0   8.6mb   8.6mbyellow open lsf_events-202204             tWYvW_w8TVyU1deRFOEoZg 5 1  116957 32691  59.1mb  59.1mbyellow open lsf_jobs_active-202212        Q0pStQxvTgaeL7R-f02XWA 5 1  210052     0  50.6mb  50.6mbyellow open lsf_jobs_pendingreason-202206 ENWIwfGrSqCHvi53aUQXJQ 5 1   44991     0   4.5mb   4.5mbyellow open host_booleanres_latest        RE8thZCgTGeMBGodeMfXEQ 5 1       5     0  23.3kb  23.3kbyellow open lsf_jobs_pendingreason-202205 yo0iZH_4TvOqq6kQgBluvA 5 1     111     0 181.4kb 181.4kbyellow open lsf_jobs_pend-202212          9ViIS3nDRFewrqtILEbKTQ 5 1     707     0 446.9kb 446.9kbyellow open lsf_hostconf_latest           9N1Y8ML4TiyaamCPEDRQog 5 1       2     0  10.6kb  10.6kbyellow open lsf_events-202209             rtKQ8F4bSleHl8EbAQez8A 5 1    8200   955   4.4mb   4.4mbyellow open lsf_events-202206             UUKPWfN7SZ-dzVs5NAkjUg 5 1   79503 23452  36.8mb  36.8mbyellow open lsf_hostmetrics-202209        7FUNFCWPQtuGyx5jTJLb1A 5 1    4701     0   2.2mb   2.2mbyellow open lsf_hostmetrics-202208        52xef_3hQWK-jVuJqyUpHA 5 1    3823     0   1.9mb   1.9mbyellow open lsf_hostmetrics-202207        IqZYhU0RQNGIFWSRH-Ym8Q 5 1    6316     0   2.9mb   2.9mbyellow open lsf_job_acct-202209           h1ZgCSB8RwCBxwIUUzDHEQ 5 1    2050   438   1.9mb   1.9mbyellow open lsf_jobs_active-202209        iBfnf07CTcS7Gb6TxwomRA 5 1    2658     0     1mb     1mbyellow open lsf_hostmetrics-202206        0PXSYBOgTA2Qa_zzaafUPg 5 1    4301     0   2.1mb   2.1mbyellow open model                         xSqB_T_VSByOzYavEcEVyQ 1 1      55     0   257kb   257kbyellow open lsf_job_acct-202206           C639GnzBSjCEVczfh5u23g 5 1   16719   353   8.9mb   8.9mbyellow open lsf_jobs_active-202204        8gN_ENkQRTSfnmxrtMcOlA 5 1   33286     0   9.8mb   9.8mbyellow open lsf_job_acct-202205           LOxmhm_8RxaCuTd7YWYbLw 5 1     274     0 439.4kb 439.4kbyellow open lsf_jobs_active-202205        61u2RlXgR_SXagmZfrmttQ 5 1    1880     0   1.1mb   1.1mbyellow open lsf_jobs_pend-202209          eTgqPp9nQOScNiwyUWXmHA 5 1       9     0 106.2kb 106.2kbyellow open lsf_job_acct-202204           dDDegS6RQSWtWN99eklexg 5 1   28902  2177  17.4mb  17.4mbyellow open lsf_jobs_active-202206        8ivkjWSNR1Sh_BxWACP0ZA 5 1   16921     0   4.6mb   4.6mbyellow open lsf_current_status            92KE3V4YSJ-RtRp_kepxYg 5 1  115450     0     9mb     9mbyellow open lsf_hostmetrics-202210        vbuK2wW3RRmXuY07tDPUNQ 5 1     785     0 942.1kb 942.1kbyellow open lsf_jobs_pend-202206          OhSwn-b0SiSj8mCW5tcNIA 5 1      22     0 244.6kb 244.6kbyellow open lsf_jobs_pend-202205          OfBtWklETYK9cRx000aNPw 5 1       1     0  12.7kb  12.7kbyellow open lsf_events-202212             WUC5KJWmS-2WIN8XCQpSuw 5 1  712399 74728   337mb   337mbyellow open lsf_jobs_pend-202204          OhUsXqohSciZTPZlTryMyA 5 1      50     0 275.3kb 275.3kbyellow open resource_attributes_latest    R9bk_WIPTU62dVg3O1LDBA 5 1       5     0  24.4kb  24.4kbyellow open lsf_jobs_pendingreason-202212 55iwDC5mRI-eRbzQLwWP6Q 5 1 3314828     0 288.7mb 288.7mbyellow open pa-lite-log                   o8-jaNoGTsSVcjJW5Ufs0w 5 1    1549     0 547.2kb 547.2kbyellow open lsf_job_acct-202212           4HXvAD02Sxq0tgp2fS2cfQ 5 1  161502     0  73.6mb  73.6mbyellow open lsf_hostmetrics-202212        Tki6OJ41R363u9Tx02N4zw 5 1    2548     0   1.7mb   1.7mbyellow open lsf_jobs_pendingreason-202209 D3TOZY2ORiK9PppGVt10Fg 5 1    2511     0 381.4kb 381.4kb</code></pre></div><p>(2) With the LSF data stored in Elasticsearch, the next step is to connect to the Grafana server. Here we point our browser to the Grafana server on the default port: <em>http://lsf_manager:3000</em> and login to Grafana. This step assumes an account has already been setup on Grafana. Here we are using the default admin account.</p><p>(3) In Grafana, navigate to <strong>Configuration</strong> -&gt; <strong>Data sources</strong>. It’s here that it will be possible to add an Elasticsearch data source</p><figure><img src=\"https://www.gaborsamu.com/images/grafana_3.png\" /></figure><p>(4) Next, click the <strong>Add data source</strong> button.</p><figure><img src=\"https://www.gaborsamu.com/images/grafana_4.png\" /></figure><p>(5 In the list of data sources, filter by name for <em>Elasticsearch</em> and click the Select button on the Elasticsearch entry.</p><figure><img src=\"https://www.gaborsamu.com/images/grafana_5.png\" /></figure><p>(6) When configuring the data source, it’s necessary to specify an index name. This is where the list of indices in Elasticsearch that we generated earlier will come in handy. For this example, we wish to display the total number of pending jobs in the Spectrum LSF cluster over time. This data is stored in the <em>lsf_jobs_pend*</em> indices in Elasticsearch. To configure the data source appropriately, we specify the following values:</p><ul><li>Name:\t“LSF pending jobs”</li><li>URL: http://localhost:9200</li><li>Index name: “lsf_jobs_pend*”</li><li>Time field name: “time_stamp”</li><li>Version: 7.0+Note that the URL needs to point to the Elasticsearch server. In this case, both the Elasticsearch server and Grafana server are running on the same host.</li></ul><p>Next click on the <strong>Save &amp; Test button</strong>. It should return the message <em>Index OK. Time field name OK.</em>.</p><p>Assuming that no errors were found, click on the <strong>Back</strong> button.</p><figure><img src=\"https://www.gaborsamu.com/images/grafana_6.png\" /></figure><p>(7) Now you should see <em>LSF pending jobs</em> listed as a Data Source.</p><figure><img src=\"https://www.gaborsamu.com/images/grafana_7.png\" /></figure><p>(8) With the data source configured, we’re now ready to configure a dashboard to display the LSF pending job information. Navigate to <strong>Create</strong> -&gt; <strong>Dashboard</strong>.</p><figure><img src=\"https://www.gaborsamu.com/images/grafana_8.png\" /></figure><p>(9) Click on <strong>Add an empty panel</strong>. This is used to create a new panel where the LSF pending job information will be plotted.</p><figure><img src=\"https://www.gaborsamu.com/images/grafana_9.png\" /></figure><p>(10) In the panel editor, specify the following options:</p><ul><li>Panel title: “LSF pending jobs”</li><li>Specify the data source “LSF pending jobs” which was created previously</li><li>Specify a suitable time range (2 days)</li><li>Line width (5 points)</li></ul><p>You should immediately see in the panel editor the plot of the hourly pending jobs.  Click on the <strong>Apply</strong> button to save the changes.</p><figure><img src=\"https://www.gaborsamu.com/images/grafana_10.png\" /></figure><p>(11) After clicking Apply, you will be returned to the Dashboard screen. The Dashboard should now display the new LSF pending jobs panel that was created above. This Dashboard could also include panels for system metrics collected by Prometheus for example.</p><figure><img src=\"https://www.gaborsamu.com/images/grafana_11.png\" /></figure><p>(12) Next, click on the diskette icon in the upper right to save the Dashboard with the LSF pending jobs panel.  We’ll name it <em>Spectrum LSF cluster status</em>.</p><figure><img src=\"https://www.gaborsamu.com/images/grafana_12.png\" /></figure><p>Additional panels can be added to the <em>Spectrum LSF cluster status</em> based on the data logged by Spectrum LSF to Elasticsearch.</p><p>That concludes the simple example of plotting Spectrum LSF cluster data from Elasticsearch in Grafana.  As mentioned, the IBM Spectrum LSF Suites integrated web interface also provides reporting capabilities, with several built-in reports provided out of the box. Below, we’ve included a screenshot of the <em>pending job analysis</em> report included with Spectrum LSF Suites.</p><figure><img src=\"https://www.gaborsamu.com/images/lsf_pending.png\" /></figure><p><strong>Summary</strong></p><p>Spectrum LSF provides many hooks and integration points enabling administrators to change things ranging from scheduling behavior and the output of query commands through to job information being logged to Elasticsearch. Spectrum LSF is highly customizable by organizations to suit specific needs and requirements. We’ve demonstrated this using Grafana to visualize data from the LSF scheduler in a simple example. Following the above example, administrators can combine existing HPC cluster system level reporting in Grafana with job information from Spectrum LSF for a better overall view and understanding of the infrastructure.</p>",
            "url": "https://hpc.social/personal-blog/2022/visualizing-spectrum-lsf-data-with-grafana/",
            
            
            
            
            
            "date_published": "2022-12-13T00:06:51-07:00",
            "date_modified": "2022-12-13T00:06:51-07:00",
            
                "author": "Ramblings of a supercomputing enthusiast."
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/adam-s-weekly-update-2022-12-04/",
            "title": "Adam’s weekly update, 2022-12-04",
            "summary": null,
            "content_text": "What&#8217;s newThis week was really intense from a work perspective. Not &#8220;bad intense&#8221;, but the kind of week where every day was spent with such a level of focus, that at 5 PM or so I found myself staring off into space and forgetting words. I think I got some good things accomplished, but my brain also felt like mush by the time the weekend came.This week I&#8217;m traveling to San Jose for work (I just checked into my hotel a little while ago!), so I fully expect this week to also be eaten by work. So I don&#8217;t promise anything terribly interesting for next week&#8217;s post&#8230;However, I did take advantage of a Sunday in San Jose to visit the Computer History Museum in Mountain View! I try to visit the museum every few years, and while a lot of the exhibits are the same, enough things change that I always get something new from the visit. Also, I&#8217;ve been doing a lot of reading about hardware development and the history thereof lately, so it was interesting to examine the museum through that new lens.I may write more about my visit later this week &#8212; it definitely sparked some thoughts &#8212; but in the mean time, here are a few photos I took while wandering around the museum.The Babbage Difference Engine, and other mechanical computers, have always fascinated me.Can&#8217;t visit the museum without visiting the Cray-1.I would have loved to have seen a CM-1 in operation, with its red LEDs showing the operation of its many single-bit CPUs.Having recently read Charles Petzold&#8217;s &#8220;Code&#8221;, I was struck by how closely the front panel of the Altair 8800 resembles the fictional front panel of the computer that Petzold constructs from logic gates up.The CHM Learning Lab now includes a back room with a couple of Dell PowerEdge R710 servers, complete with instructions for how to disassemble and reassemble them. Anyone who wants can wander in and take them apart. It was great fun watching a 5-year-old kid pulling components out of one of these&#8230; As well as feeling a little weird, as I think I&#8217;ve run these in production!What I&#8217;m readingI don&#8217;t have a ton to share this week &#8212; honestly, the whole week feels like a blur &#8212; but here are two books that I recommend.The Red Scholar&#8217;s Wake, by Aliette de Bodard: As the blurb says, &#8220;Lesbian space pirates!&#8221; Also, a really wonderful novella about building a new relationship amidst grief, power differentials, politics, and space battles. I think I basically recommend everything that de Bodard writes, but especially this. And it basically stands alone! So you can read this first, without going back to the other stories in the same world.Dealers of Lightning: XEROX PARC and the Dawn of the Computer Age, by Michael Hiltzik: I&#8217;ve just started this, but it&#8217;s already a really interesting snapshot of a key period in the development of the personal computer.Recent recipesSmitten Kitchen&#8217;s Unfussy Sugar Cookies: These cookies did, indeed, prove to be both tasty and easy to make. If you just want some easy cookies to snack on, I absolutely recommend this recipe.Pet photos",
            "content_html": "<h2>What&#8217;s new</h2><p>This week was really intense from a work perspective. Not &#8220;bad intense&#8221;, but the kind of week where every day was spent with such a level of focus, that at 5 PM or so I found myself staring off into space and forgetting words. I think I got some good things accomplished, but my brain also felt like mush by the time the weekend came.</p><p><span id=\"more-268\"></span></p><p>This week I&#8217;m traveling to San Jose for work (I just checked into my hotel a little while ago!), so I fully expect this week to also be eaten by work. So I don&#8217;t promise anything terribly interesting for next week&#8217;s post&#8230;</p><p>However, I did take advantage of a Sunday in San Jose to visit the <a href=\"https://computerhistory.org/\">Computer History Museum</a> in Mountain View! I try to visit the museum every few years, and while a lot of the exhibits are the same, enough things change that I always get something new from the visit. Also, I&#8217;ve been doing a lot of reading about hardware development and the history thereof lately, so it was interesting to examine the museum through that new lens.</p><p>I may write more about my visit later this week &#8212; it definitely sparked some thoughts &#8212; but in the mean time, here are a few photos I took while wandering around the museum.</p><figure class=\"wp-block-image size-large is-resized\"><img alt=\"A mechanical computer built mostly of brass, with various numerical dials. A small placard labels this as a replica of the Babbage Difference Engine No. 1 Demonstration Piece.\" class=\"wp-image-282\" height=\"800\" src=\"https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_6894-768x1024.jpg\" width=\"600\" /><figcaption class=\"wp-element-caption\">The Babbage Difference Engine, and other mechanical computers, have always fascinated me.</figcaption></figure><figure class=\"wp-block-image size-large is-resized\"><img alt=\"The Cray-1, a round computer with its own built-in seating attached.\" class=\"wp-image-283\" height=\"446\" src=\"https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_6965-1024x768.jpg\" width=\"595\" /><figcaption class=\"wp-element-caption\">Can&#8217;t visit the museum without visiting the Cray-1.</figcaption></figure><figure class=\"wp-block-image size-large is-resized\"><img alt=\"The Connection Machine 1, a large black cube divided in eight sections.\" class=\"wp-image-284\" height=\"768\" src=\"https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_6973-768x1024.jpg\" width=\"576\" /><figcaption class=\"wp-element-caption\">I would have loved to have seen a CM-1 in operation, with its red LEDs showing the operation of its many single-bit CPUs.</figcaption></figure><figure class=\"wp-block-image size-large is-resized\"><img alt=\"The front panel of an Altair 8800 computer, with an array of LEDs and switches controlling the state of individual bits.\" class=\"wp-image-285\" height=\"449\" src=\"https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_7037-1024x768.jpg\" width=\"598\" /><figcaption class=\"wp-element-caption\">Having recently read Charles Petzold&#8217;s &#8220;Code&#8221;, I was struck by how closely the front panel of the Altair 8800 resembles the fictional front panel of the computer that Petzold constructs from logic gates up.</figcaption></figure><figure class=\"wp-block-image size-large is-resized\"><img alt=\"A Dell PowerEdge R710 lays on a white plastic table, top cover off, surrounded by instructions on how to disassemble it.\" class=\"wp-image-286\" height=\"467\" src=\"https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_7073-1024x768.jpg\" width=\"623\" /><figcaption class=\"wp-element-caption\">The CHM Learning Lab now includes a back room with a couple of Dell PowerEdge R710 servers, complete with instructions for how to disassemble and reassemble them. Anyone who wants can wander in and take them apart. It was great fun watching a 5-year-old kid pulling components out of one of these&#8230; As well as feeling a little weird, as I think I&#8217;ve run these in production!</figcaption></figure><h2>What I&#8217;m reading</h2><p>I don&#8217;t have a ton to share this week &#8212; honestly, the whole week feels like a blur &#8212; but here are two books that I recommend.</p><ul><li><a href=\"https://www.aliettedebodard.com/bibliography/novels/the-universe-of-xuya/the-red-scholars-wake/\">The Red Scholar&#8217;s Wake, by Aliette de Bodard</a>: As the blurb says, &#8220;Lesbian space pirates!&#8221; Also, a really wonderful novella about building a new relationship amidst grief, power differentials, politics, and space battles. I think I basically recommend everything that de Bodard writes, but especially this. And it basically stands alone! So you can read this first, without going back to the other stories in the same world.</li><li><a href=\"https://www.harpercollins.com/products/dealers-of-lightning-michael-a-hiltzik?variant=40824247779362\">Dealers of Lightning: XEROX PARC and the Dawn of the Computer Age, by Michael Hiltzik</a>: I&#8217;ve just started this, but it&#8217;s already a really interesting snapshot of a key period in the development of the personal computer.</li></ul><h2>Recent recipes</h2><ul><li><a href=\"https://smittenkitchen.com/2019/12/unfussy-sugar-cookies/\">Smitten Kitchen&#8217;s Unfussy Sugar Cookies</a>: These cookies did, indeed, prove to be both tasty and easy to make. If you just want some easy cookies to snack on, I absolutely recommend this recipe.</li></ul><h2>Pet photos</h2><figure class=\"wp-block-image size-large is-resized\"><img alt=\"Phyrne the calico cat stares down into the camera from a stairway\" class=\"wp-image-279\" height=\"414\" src=\"https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_6881-768x1024.jpg\" width=\"310\" /></figure><figure class=\"wp-block-image size-large is-resized\"><img alt=\"Close-up on the face of Percy the gray tabby cat\" class=\"wp-image-280\" height=\"420\" src=\"https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_6879-768x1024.jpg\" width=\"314\" /></figure><figure class=\"wp-block-image size-large is-resized\"><img alt=\"Benny the golden doodle curled up on a dog bed\" class=\"wp-image-281\" height=\"238\" src=\"https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_6876-1024x768.jpg\" width=\"317\" /></figure>",
            "url": "https://hpc.social/personal-blog/2022/adam-s-weekly-update-2022-12-04/",
            
            
            
            
            
            "date_published": "2022-12-05T05:49:35-07:00",
            "date_modified": "2022-12-05T05:49:35-07:00",
            
                "author": "Thinking Out Loud"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/an-initial-look-at-deep-learning-io-performance/",
            "title": "An Initial Look at Deep Learning IO Performance",
            "summary": null,
            "content_text": "AbstractThis blog post describes an investigation of IO behavior of TensorFlow and PyTorch during resnet50 training running on Lambda Lab’s 8x V100 GPU instances.  Both ephemeral local NVMe storage and network attached persistent storage was tested.  The local NVMe storage was fast enough to achieve a throughput rate required to hit synthetic test targets.  The network attached persistent storage may not be able to fully saturate 8 V100 GPUs during training, though can achieve nearly the same level of performance as the local storage so long as TFRecords are utilized.  Further, there are specific behaviors and bottlenecks in TensorFlow and PyTorch that can reduce training performance when using real data from ImageNet.AcknowledgementsThank you to Michael Balaban at Lambda Labs for providing access to their GPU cloud for this testing.  Thank you to Chuan Li for the creation of his TensorFlow benchmarking tools.  Thank you also to Andrej Karpathy, Toby Boyd, Yanan Cao, Sanjoy Das, Thomas Joerg, and Justin Lebar for their excellent blog posts on deep learning and XLA performance that helped inform this article.  I hope that this post will be useful for others as your work and writing was useful for me.Introduction  …just because you can formulate your problem as RL doesn’t mean you should. If you insist on using the technology without understanding how it works you are likely to fail.          Andrej Karpathy, A Recipe for Training Neural Networks, 2019That was the phrase that stuck in my head when I first started this project.   What project you may ask?  I want to understand how deep learning experiments utilize fast storage devices.  Not just any experiments either: real ones, preferably big.  That’s how I happened upon Andrej Karpathy’s blog.  He is the former Sr. Director of AI at Tesla and knows a thing or two about training big neural networks.  I’ve spent the last decade working on Ceph and have worked on distributed systems and distributed storage for nearly 2 decades at this point.  But training neural nets?  The closest I’ve come was back in the early 2000s when I tried to build a tool to predict video game framerates.  I scraped benchmark numbers from review websites and built M5 decision trees based on hardware and video card settings.  It sort of worked, but was terribly overtrained on a small (~4000 sample) dataset.  Training with petabytes of data to teach an AI how to responsibly drive a car?  I can already feel a bit of imposter syndrome setting in.Thankfully my goal is comparatively modest.  I don’t need to build a cutting edge classifier or explore the intricacies of manually implementing back-propagation.  I simply want to understand the IO patterns that are involved when training big datasets with fast GPUs so I can help researchers speed up their work.  Up until now, my ability to do this was fairly limited.  At the day job I’ve had access to a small group of nodes with extremely modest GPUs.  I set up runs with MLPerf but the datasets (WMT G-E and CoCo) easily fit into memory. Other than a short burst of read traffic at the very beginning of training there was very little IO.  Recently I had the opportunity to meet Michael Balaban, Co-Founder of Lambda Labs.  I told him what I wanted to do and he gave me access to Lambda’s GPU cloud and beta persistent storage to give it a try.  I was able to grab one of Lambda’s 8x Tesla V100 instances (These things are incredibly popular so it’s best to grab one early in the morning!).  Not all of Lambda’s instance types currently have access to the persistent storage but the V100 instances in the Texas zone do.  Once secured, I got to work.TensorFlow - SyntheticBefore even attempting to run tests with real data, I realized I needed a baseline to start with.  Luckily, Chuan Li, Lambda’s Chief Scientific Officer, wrote a tool for running TensorFlow benchmarks and made it available on github here. One of the advantages of Lambda’s cloud is that they’ve already bundled up many popular tools for running deep-learning workloads into one package called Lambda Stack which comes pre-installed when you start an instance.  This made it fast to get started, though I did run into one issue.  Lambda Stack comes standard with TensorFlow 2, but Chuan Li’s tool relies on a TensorFlow benchmark submodule that is designed to work with TensorFlow 1.  Luckily, the parent repository was unofficially updated to work with Tensorflow 2 (with a warning that it is no longer being maintained).  A quick “git checkout master” in the “benchmarks” submodule directory got everything working.  Chuan Li’s tool makes it simple to run tests with several preconfigured templates already included.  I chose the fp16 resnet50 configuration as it should be fast at processing images and is fairly standard.TF_XLA_FLAGS=--tf_xla_auto_jit=2 ./batch_benchmark.sh X X 1 100 2 config/config_resnet50_replicated_fp16_train_synUsing the invocation provided in the benchmark README.md file, I was able to quickly run benchmarks with synthetic data on up to 8 V100 GPUs in the node.  At one point I got stuck, hitting what appeared at first to be an unexplainable 25% performance loss. I reran the tests multiple times and even monitored GPU clockspeeds/temperatures in nvidia-smi with no luck.  Ultimately I discovered my error.  In the slow cases, I had inadvertently left out the “TF_XLA_FLAGS=–tf_xla_auto_jit=2” environment variable.  It turns out that setting this allows Tensorflow compile and execute functions with XLA (Accelerated Linear Algebra) support which is a pretty big win for these tests.At this point I decided that I needed to understand how Chuan Li’s tool works.  It turns out that he is using the same base tf_cnn_benchmarks.py benchmark code that companies like Nvidia and Dell also use for benchmarking their GPU solutions.  I spent some time running it directly with Dell’s settings from their deep learning overview here.  Unfortunately those tests had mixed results, even after various tweaks.  While researching the XLA issues I mentioned earlier however, I made an even better discovery on the TensorFlow website.  I found an excellent blog post with performance data written by some of the core Tensorflow developers.  It’s now 4 years old, but still appears to be quite valid.  The tuning options used were both simpler and resulted in higher performance versus other configurations that I’ve come across.Training with synthetic data in Lambda’s cloud resulted in similar performance to what the Tensorflow developer’s reported.  In fact, using their own settings yielded slightly faster results when running on Lambda’s 8xV100 instance!  It was incredibly encouraging to me that even in Lambda’s cloud environment with virtual machine instances I could achieve performance that was as fast or faster than what the Tensorflow developers were reporting.Choosing a Real Data Set  The first step to training a neural net is to not touch any neural net code at all and instead begin by thoroughly inspecting your data.          Andrej Karpathy, A Recipe for Training Neural Networks, 2019Having convinced myself that I had Tensorflow operating reasonably efficiently in synthetic tests, it was time to start thinking about what dataset to use for “real” training.  The largest and most obvious choice is ImageNet.  ImageNet is composed of over 1.2 million categorized images that form a roughly 160GB training dataset.  It is also the largest dataset I could find that was publicly accessible. Downloading it isn’t so easy however. The only version that I could access is the ImageNet Object Localization Challenge dataset hosted on kaggle.After finally figuring out how to download the data, it was time to follow Andrej’s advice and try to learn something about it.  While ImageNet is curated and annotated, it has many images of different sizes, dimensions, and pixel counts.  Images also come from many sources with different levels of quality.  Through the power of stack-exchange I was able to find a bash one-liner script to generate a histogram of image sizes:find . -type f -print0 | xargs -0 ls -l | awk '{size[int(log($5)/log(2))]++}END{for (i in size) printf(\"%10d %3d\\n\", 2^i, size[i])}' | sort -nRoughly 80% of the images are in the 64KB or 128KB size bins. Almost all of the remaining images are smaller.  That gives us a pretty good idea of what kind of IOs to expect during classification.  Or at least…it does for frameworks that read those images directly.  In Tensorflow’s case, there’s an alternative format called TFRecord.  TFRecords are basically collections of image data sequentially laid out in much larger files.  Instead of iterating over thousands or millions of individual image files, TFRecords allow Tensorflow to instead stream fewer, larger files that each house multiple images.  It’s a one time cost to pre-process the data so Tensorflow has less work to do during training.  After I downloaded the ImageNet data I took a shot at converting the ImageNet LOC data into TensorFlow records.  Luckily, the TensorFlow tpu github repository already has a tool that can do this.  I had to manipulate the dataset slightly, but ultimately this process worked (at least for the training data):pip install gcloud google-cloud-storagepip install protobuf==3.20.1mkdir ~/data/ImageNetFooln -s ~/data/ImageNet/ILSVRC/Data/CLS-LOC/train ~/data/ImageNetFoo/trainln -s ~/data/ImageNet/ILSVRC/Data/CLS-LOC/val ~/data/ImageNetFoo/valln -s ~/data/ImageNet/ILSVRC/Data/CLS-LOC/test ~/data/ImageNetFoo/testln -s ~/data/ImageNet/LOC_synset_mapping.txt ~/data/ImageNetFoo/synset_labels.txtpython imagenet_to_gcs.py --raw_data_dir=/home/ubuntu/data/ImageNetFoo --local_scratch_dir=/home/ubuntu/ExaltedOrbs/ImageNet/tf_records --nogcs_uploadPerhaps I should say that this worked so long as the original dataset was located on the local NVMe drive.  The persistent storage didn’t fare as well.  Attempting to decompress ImageNet on the persistent storage resulted in blowing past the max number of open files allowed with errors like:OSError: [Errno 24] Too many open files.Unfortunately this couldn’t be fixed on the instance.  It appeared to be passed through from the host and the persistent storage was completely unusable until the instance was rebooted.  Recently I spoke to one of Lambda’s engineers and they are working on a fix. (It may already be implemented by the time you read this!)  I also want to note that the persistent storage is still in beta so issues like this are not entirely unexpected.  Having said that, before hitting the error it was significantly slower extracting ImageNet on the persistent storage vs on the local NVMe storage.  It’s probably best to extract ImageNet locally and then write the large TFRecords to the persistent storage during the conversion process.  Luckily extracting ImageNet to local storage was fine, and storing the original archive and the resulting TFRecords on the persistent storage worked perfectly fine as well.FIO - Baseline IO ResultsNext, I turned my attention to running baseline tests on Lambda’s local and persistent storage using fio.  Fio is a highly configurable and well respected benchmark in the storage community and perfect for generating baseline results.  I decided to use a dataset size that is roughly similar to ImageNet (200GB), the libaio engine in fio with direct IO, and an appropriately high IO depth to let the NVMe drives stretch their legs a bit.Throughput with the local NVMe drive(s) is surprisingly good.  The persistent storage is slower, but still might be fast enough at a little over 1GB/s for large reads.  16K IOPS was somewhat slower in both cases.  I chose 16K so that I could quickly compare to tests I ran in my Ceph QEMU/KVM performance blog post here.  Without getting into the details, I suspect there’s still some room for improved IOPS with Lambda’s setup.  Luckily though, converting into TFRecords should make Tensorflow throughput bound instead of latency bound.  What about PyTorch or other tools that want to read images directly though?  Fio gives us the ability to simulate it by using its ‘bssplit’ feature.  We can take the size ranges and percentiles generated when examining ImageNet and give fio a similar distribution:fio --ioengine=libaio --direct=1 --bssplit=2K/1:4K/2:8K/4:16K/8:32K/13:64K/38:128K/33:256K/1 --iodepth=128 --rw=randread --norandommap --size=200G --numjobs=1 --runtime=300 --time_based --name=fooThis isn’t exactly right as we are not reading data spread across millions of files, but it should provide something of an upper bound on what to expect.  It looks like the persistent storage can do approximately 10K reads/second at a throughput rate of around 750MB/s.  The local storage is about 3-4 times faster.  Local storage should be fast enough to support the kind of images/second throughput rates we want to hit in Tensorflow on 8 V100 GPUs, but the jury is still out for the persistent storage.Tensorflow - ImageNetRunning benchmarks with real data rather than synthetic data is fairly straightforward in Tensorflow.  You simply append data_dir and data_name flags to the CLI invocation to let it know where the TFRecords are located:sync; echo 3 | sudo tee /proc/sys/vm/drop_cachespython ./tf_cnn_benchmarks.py --batch_size=256 --num_batches=100 --model=resnet50 --optimizer=momentum --variable_update=replicated --all_reduce_spec=nccl --use_fp16=True --nodistortions --gradient_repacking=2 --compute_lr_on_cpu=True --single_l2_loss_op=True --xla_compile=True --num_gpus=8 --loss_type_to_report=base_loss --data_dir=/home/ubuntu/ImageNet-TF/train --data_name=imagenetOuch.  Much lower performance with the ImageNet data vs synthetic!  This is especially unfortunate given that 4 years ago the Tensorflow developers reported much better results.  I spent some time reading and experimenting with different settings.  Ultimately the one setting that made a substantial difference was “datasets_num_private_threads”.  In the Tensorflow benchmark source code, this setting is described as: “[The] number of threads for a private threadpool created for all datasets computation.”  I’ll go into more detail what these threads are doing in a bit. For now, let’s see how increasing the number of threads affects the results:Increasing the number of private threads has a dramatic effect on performance, though I was unable to fully match the performance achieved in the synthetic tests on either the local or persistent storage.  The local storage fared better at high thread counts gradually topping out at around 8600 images/second.  At high private thread counts the persistent storage topped out between 7000-8000 images/second with a higher degree of variability between runs.  I suspect that in this case the persistent storage has likely hit its (per instance) limit.In addition to having a dramatic effect on performance, changing the private thread count also had a large effect on the CPU consumption of the TensorFlow process.  CPU usage increases almost linearly with additional private threads up to around 30 cores.  What exactly are these private threads doing?  To answer that question, I utilized two tools that I often deploy when diagnosing CPU usage in Ceph.  When testing with a lower number of private threads, I used linux’s perf tool to look at where cycles are being consumed when the private threads are fully saturated.  At higher levels of private threads, I used my wallclock profiler uwpmp to look at how private threads spend their time when increasing the thread count no longer improves performance.In the first case with perf, we can get a good view of the work that these private threads are doing:--77.31%--tensorflow::ThreadPoolDevice::Compute          |                    |--51.19%--0x7f511a00c7d8          |          |                    |           --51.18%--tensorflow::jpeg::Uncompress          |--14.48%--tensorflow::ResizeBilinearOp&lt;Eigen::ThreadPoolDevice, unsigned char&gt;::Compute          |--5.47%--tensorflow::CastOpBase::Compute          |--2.66%--tensorflow::ReverseV2Op&lt;Eigen::ThreadPoolDevice, unsigned char, int&gt;::ComputeThe majority of the cycles consumed is in jpeg decompression and resize operations, along with a smattering of other stuff.  What happens if we look at a case with a higher private thread count but now look at wallclock time instead of cycles?  I ended up having some trouble getting the profiler to work properly and consistently get clean callgraphs, but I was able to get at least one run in that revealed some interesting information.  First, I saw time spent in the same functions that perf told us we were spending cycles in:+ 100.00% Eigen::ThreadPoolTempl&lt;tensorflow::thread::EigenEnvironment&gt;::WorkerLoop(int) + 99.90% ??? |+ 97.30% ??? ||+ 92.40% ??? |||+ 77.10% _PyEval_EvalFrameDefault ||||+ 47.20% ??? |||||+ 38.10% tensorflow::jpeg::Uncompress(void const*, int, tensorflow::jpeg::UncompressFlags const&amp;, long*, std::function&lt;unsigned char* (int, int, int)&gt;) ||||+ 12.20% tensorflow::ResizeBilinearOp&lt;Eigen::ThreadPoolDevice, unsigned char&gt;::Compute(tensorflow::OpKernelContext*) ||||+ 4.40% tensorflow::CastOpBase::Compute(tensorflow::OpKernelContext*) ||||+ 1.70% tensorflow::ReverseV2Op&lt;Eigen::ThreadPoolDevice, unsigned char, int&gt;::Compute(tensorflow::OpKernelContext*)But the wallclock profile also exposed that there may be contention in multiple areas in the private threads around some of the nsync synchronization primitives being used: |||||||    |  + 4.50% nsync::nsync_mu_semaphore_p(nsync::nsync_semaphore_s_*) |||||||    |   + 4.50% syscallThis almost always appeared nested deep inside:tensorflow::BFCAllocator::AllocateRaw(unsigned long, unsigned long, tensorflow::AllocationAttributes const&amp;)Sadly I was missing a number of debug symbols and don’t 100% trust the wallclock trace.  For now I’ll just say that the private threads are doing a significant amount of work decompressing and manipulating the image data to keep the GPUs fed.  I suspect that with newer and faster GPUs the image retrieval pipeline could become an even bigger issue when training with real image data.  The mystery for me is how The TensorFlow developers achieved such good results 4 years ago without using dedicated private threads at all.  Perhaps they had a significantly faster jpeg decompression mechanism that I am unaware of?PyTorch - ImageNetAfter running Tensorflow, I also ran some benchmarks in PyTorch using Nvidia’s “DeepLearningExamples” github repo.  First, I installed the prereqs and setup the repository:pip install 'git+https://github.com/NVIDIA/dllogger'pip install --extra-index-url https://developer.download.nvidia.com/compute/redist --upgrade nvidia-dali-cuda110git clone https://github.com/NVIDIA/DeepLearningExamplesThen, prepared ImageNet for usage in PyTorch:cd ~/data/ImageNet/ILSVRC/Data/CLS-LOC/valwget -qO- https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh | bashAnd finally ran a test:cd DeepLearningExamples/PyTorch/Classification/ConvNetssync; echo 3 | sudo tee /proc/sys/vm/drop_cachespython ./multiproc.py --nproc_per_node 1 ./main.py --arch resnet50 --label-smoothing 0.1 --run-epoch 1 --amp --static-loss-scale 256 --workspace /home/ubuntu/data/ImageNet-Scratch /home/ubuntu/data/ImageNet-Orig/ILSVRC/Data/CLS-LOC/There are a couple of differences here versus the TensorFlow tests.  First, I’m using the raw ImageNet archive instead of a preprocessed TFRecord dataset, so the read behavior is different.  Because I was unable to extract or copy the raw ImageNet archive onto the persistent storage, I’m also only testing the local NVMe drive.  Finally, I didn’t see any specific examples for running with fp16 in nVidia’s documentation, so I’m using amp (automatic mixed precision) which may be slightly slower.Given the number of differences it’s tough to draw direct comparisons with Tensorflow.  Amp is one difference, but it’s quite possible that there are tuning options that could improve performance here that I don’t know about.  I did notice that PyTorch, like Tensorflow, is using quite a bit of CPU to keep the GPUs working.  I suspect that there are ways to tweak the IO pipeline that could improve performance.  For now though, let’s compare the IO patterns on the local NVMe drive during the Tensorflow and PyTorch runs.  I was hoping to be able to use blktrace to do this, but unfortunately was unable to get any data from the virtual devices in the instance.  I was able to collect more general statistics using collectl however.Disk Read Statistics During PyTorch 8 GPU run:            Time      Name      KBytes      Merged      IOs      Size      Wait      QLen      SvcTim                  00:29:18      vda      761136      0      6746      113      58      431      0              00:29:19      vda      752172      0      6648      113      112      810      0              00:29:20      vda      747824      0      6595      113      84      604      0              00:29:21      vda      735964      0      6583      112      73      551      0              00:29:22      vda      695636      0      6237      112      102      760      0      Disk Read Statistics During TensorFlow 8 GPU run:            Time      Name      KBytes      Merged      IOs      Size      Wait      QLen      SvcTim                  00:38:45      vda      1081324      0      8440      128      0      7      0              00:38:46      vda      927512      0      7241      128      0      7      0              00:38:47      vda      913512      0      7130      128      0      7      0              00:38:48      vda      1047444      0      8186      128      0      6      0              00:38:49      vda      968776      0      7560      128      0      6      0      When just looking at the IO sizes, both runs appear similar, but that doesn’t tell the whole story.  It is likely that Tensorflow is doing much larger reads that are broken up into contiguous 128KB chunks by the block layer based on the underlying device’s max_sectors_kb setting.  The tells here are the very low queue length and wait times for the TensorFlow run versus the PyTorch run.  In both case the device service times are low (0), but in the TensorFlow case IOs are still backing up in the device queue.Interestingly, it appears that it may be possible to use nVidia’s DALI (Data Loading Library) package to read TFRecords into PyTorch.  I didn’t have time to attempt it, but potentially that could have a big effect on IO behavior and performance as well.ConclusionAs I’ve been writing this post, I realize just how complicated it is to understand the performance characteristics of training of neural networks.  Even as we talk about metrics like images/second, the options that are used (batch size for instance) can also affect convergence.  It’s very difficult to come up with a common methodology that is always better than others.  I wonder if another metric, like reaching a desired level of convergence, would be better in the end.  Having said that, I am glad for having done this exercise as I learned some valuable things:      Pre-processing data into a format like TFRecords on fast local storage is a big win from an IO perspective.  It lets storage systems that have slow metadata performance succeed so long as they have enough sequential read throughput to keep the machine learning framework busy.  This is a big win for many distributed file systems that may have substandard metadata performance (and even the good ones may still benefit).        To train on a dataset like ImageNet, you need somewhere around 1-1.3GB/s of raw disk throughput to keep 8 V100 GPUs busy when training in fp16.  For amp or fp32 the requirements are likely lower since the GPUs can’t work quite as fast.  With modern GPUs that are faster than the V100, the disk throughput requirements could be significantly higher.        Lambda’s local NVMe storage is likely fast enough to saturate 8 GPUs, even newer ones, so long as the rest of the IO path can keep up.  The persistent storage appears to become a bottleneck with sufficient GPUs and TensorFlow private threads, though can still function fairly well so long as TFRecords are used.  A concern going forward is how to ensure that the data pipeline in TensorFlow and PyTorch are fast enough to keep the GPUs fed.  The Tensorflow benchmark required a large number of private threads and showed potential evidence of contention at high thread counts.  PyTorch did not appear to natively support TFRecords, but NVidia DALI or other 3rd party code might help improve the IO path.        If it’s necessary to train directly with images rather than TFRecords, it may not make sense to host them on shared file systems.  It appears that Tensorflow and possibly PyTorch give users the ability to specify a separate training data and work directory.  If all operations against the training data are reads, it may be better to host datasets on read-only block device snapshots. For instance with Ceph, perhaps you could create a read/write RBD volume where you put a certain dataset, take a snapshot, and then map that snapshot as read only on multiple instances that all need access to the same image set.        Even with a training set as large as ImageNet, Lambda’s instances have so much memory that eventually the entire dataset becomes cached.  It was necessary to sync and drop caches before each test and keep tests short enough that they didn’t re-read the same data from buffer cache.  I was able to watch as long running tests eventually stopped performing reads and got faster as time went on.  This could make apples-to-apples comparison between different storage vendors difficult if not carefully controlled.        I’m almost certainly missing additional tweaks that can help speed up both Tensorflow and PyTorch.  This post shouldn’t be seen as the be-all/end-all for how to achieve high performance with these frameworks, but I hope it may at least help showcase some of the areas that are valuable to investigate when trying to train with real data and achieve high performance.  This wraps up my initial work looking at Deep Learning IO behavior.  I hope that next time I can come armed with a bit more knowledge about the internals of how PyTorch and Tensorflow work, focus a bit more on the quality of the training, find even larger datasets to work with, and maybe actually accomplish something useful rather than just play with ImageNet.Thanks for reading!",
            "content_html": "<h2 id=\"abstract\">Abstract</h2><p>This blog post describes an investigation of IO behavior of TensorFlow and PyTorch during resnet50 training running on Lambda Lab’s 8x V100 GPU instances.  Both ephemeral local NVMe storage and network attached persistent storage was tested.  The local NVMe storage was fast enough to achieve a throughput rate required to hit synthetic test targets.  The network attached persistent storage may not be able to fully saturate 8 V100 GPUs during training, though can achieve nearly the same level of performance as the local storage so long as TFRecords are utilized.  Further, there are specific behaviors and bottlenecks in TensorFlow and PyTorch that can reduce training performance when using real data from ImageNet.</p><h2 id=\"acknowledgements\">Acknowledgements</h2><p>Thank you to Michael Balaban at Lambda Labs for providing access to their GPU cloud for this testing.  Thank you to Chuan Li for the creation of his TensorFlow benchmarking tools.  Thank you also to Andrej Karpathy, Toby Boyd, Yanan Cao, Sanjoy Das, Thomas Joerg, and Justin Lebar for their excellent blog posts on deep learning and XLA performance that helped inform this article.  I hope that this post will be useful for others as your work and writing was useful for me.</p><h2 id=\"introduction\">Introduction</h2><blockquote>  <p><em>…just because you can formulate your problem as RL doesn’t mean you should. If you insist on using the technology without understanding how it works you are likely to fail.</em></p>  <p>        Andrej Karpathy, <a href=\"https://karpathy.github.io/2019/04/25/recipe/\">A Recipe for Training Neural Networks</a>, 2019</p></blockquote><p>That was the phrase that stuck in my head when I first started this project.   What project you may ask?  I want to understand how deep learning experiments utilize fast storage devices.  Not just any experiments either: <em>real</em> ones, preferably big.  That’s how I happened upon Andrej Karpathy’s blog.  He is the former Sr. Director of AI at Tesla and knows a thing or two about training big neural networks.  I’ve spent the last decade working on Ceph and have worked on distributed systems and distributed storage for nearly 2 decades at this point.  But training neural nets?  The closest I’ve come was back in the early 2000s when I tried to build a tool to predict video game framerates.  I scraped benchmark numbers from review websites and built M5 decision trees based on hardware and video card settings.  It sort of worked, but was terribly overtrained on a small (~4000 sample) dataset.  Training with petabytes of data to teach an AI how to responsibly drive a car?  I can already feel a bit of imposter syndrome setting in.</p><p>Thankfully my goal is comparatively modest.  I don’t need to build a cutting edge classifier or explore the intricacies of manually implementing back-propagation.  I simply want to understand the IO patterns that are involved when training big datasets with fast GPUs so I can help researchers speed up their work.  Up until now, my ability to do this was fairly limited.  At the day job I’ve had access to a small group of nodes with extremely modest GPUs.  I set up runs with MLPerf but the datasets (WMT G-E and CoCo) easily fit into memory. Other than a short burst of read traffic at the very beginning of training there was very little IO.  Recently I had the opportunity to meet Michael Balaban, Co-Founder of <a href=\"https://lambdalabs.com/\">Lambda Labs</a>.  I told him what I wanted to do and he gave me access to Lambda’s GPU cloud and beta persistent storage to give it a try.  I was able to grab one of Lambda’s 8x Tesla V100 instances (These things are incredibly popular so it’s best to grab one early in the morning!).  Not all of Lambda’s instance types currently have access to the persistent storage but the V100 instances in the Texas zone do.  Once secured, I got to work.</p><h2 id=\"tensorflow---synthetic\">TensorFlow - Synthetic</h2><p>Before even attempting to run tests with real data, I realized I needed a baseline to start with.  Luckily, Chuan Li, Lambda’s Chief Scientific Officer, wrote a tool for running TensorFlow benchmarks and made it available on github <a href=\"https://github.com/lambdal/lambda-tensorflow-benchmark\">here</a>. One of the advantages of Lambda’s cloud is that they’ve already bundled up many popular tools for running deep-learning workloads into one package called <a href=\"https://lambdalabs.com/lambda-stack-deep-learning-software\">Lambda Stack</a> which comes pre-installed when you start an instance.  This made it fast to get started, though I did run into one issue.  Lambda Stack comes standard with TensorFlow 2, but Chuan Li’s tool relies on a TensorFlow benchmark submodule that is designed to work with TensorFlow 1.  Luckily, the parent repository was unofficially updated to work with Tensorflow 2 (with a warning that it is no longer being maintained).  A quick “git checkout master” in the “benchmarks” submodule directory got everything working.  Chuan Li’s tool makes it simple to run tests with several preconfigured templates already included.  I chose the fp16 resnet50 configuration as it should be fast at processing images and is fairly standard.</p><pre><code>TF_XLA_FLAGS=--tf_xla_auto_jit=2 ./batch_benchmark.sh X X 1 100 2 config/config_resnet50_replicated_fp16_train_syn</code></pre><p>Using the invocation provided in the benchmark README.md file, I was able to quickly run benchmarks with synthetic data on up to 8 V100 GPUs in the node.  At one point I got stuck, hitting what appeared at first to be an unexplainable 25% performance loss. I reran the tests multiple times and even monitored GPU clockspeeds/temperatures in nvidia-smi with no luck.  Ultimately I discovered my error.  In the slow cases, I had inadvertently left out the “TF_XLA_FLAGS=–tf_xla_auto_jit=2” environment variable.  It turns out that setting this allows Tensorflow compile and execute functions with XLA (Accelerated Linear Algebra) support which is a pretty big win for these tests.</p><p><img alt=\"\" src=\"https://markhpc.github.io/images/2022-11-28-Lambda/Tensorflow_-_ResNet50_Synthetic_Training_fp16.svg\" /></p><p>At this point I decided that I needed to understand how Chuan Li’s tool works.  It turns out that he is using the same base tf_cnn_benchmarks.py benchmark code that companies like Nvidia and Dell also use for benchmarking their GPU solutions.  I spent some time running it directly with Dell’s settings from their deep learning overview <a href=\"https://infohub.delltechnologies.com/l/high-speed-object-storage-for-deep-learning/overview-3284\">here</a>.  Unfortunately those tests had mixed results, even after various tweaks.  While researching the XLA issues I mentioned earlier however, I made an even better <a href=\"https://blog.tensorflow.org/2018/11/pushing-limits-of-gpu-performance-with-xla.html\">discovery</a> on the TensorFlow website.  I found an excellent blog post with performance data written by some of the core Tensorflow developers.  It’s now 4 years old, but still appears to be quite valid.  The tuning options used were both simpler and resulted in higher performance versus other configurations that I’ve come across.</p><p><img alt=\"\" src=\"https://markhpc.github.io/images/2022-11-28-Lambda/Tensorflow_-_ResNet50_Synthetic_Training_fp16_blog_compare.svg\" /></p><p>Training with synthetic data in Lambda’s cloud resulted in similar performance to what the Tensorflow developer’s reported.  In fact, using their own settings yielded slightly faster results when running on Lambda’s 8xV100 instance!  It was incredibly encouraging to me that even in Lambda’s cloud environment with virtual machine instances I could achieve performance that was as fast or faster than what the Tensorflow developers were reporting.</p><h1 id=\"choosing-a-real-data-set\">Choosing a Real Data Set</h1><blockquote>  <p><em>The first step to training a neural net is to not touch any neural net code at all and instead begin by thoroughly inspecting your data.</em></p>  <p>        Andrej Karpathy, <a href=\"https://karpathy.github.io/2019/04/25/recipe/\">A Recipe for Training Neural Networks</a>, 2019</p></blockquote><p>Having convinced myself that I had Tensorflow operating reasonably efficiently in synthetic tests, it was time to start thinking about what dataset to use for “real” training.  The largest and most obvious choice is ImageNet.  ImageNet is composed of over 1.2 million categorized images that form a roughly 160GB training dataset.  It is also the largest dataset I could find that was publicly accessible. Downloading it isn’t so easy however. The only version that I could access is the ImageNet Object Localization Challenge dataset hosted on <a href=\"https://www.kaggle.com/c/imagenet-object-localization-challenge\">kaggle</a>.</p><p>After finally figuring out how to download the data, it was time to follow Andrej’s advice and try to learn something about it.  While ImageNet is curated and annotated, it has many images of different sizes, dimensions, and pixel counts.  Images also come from many sources with different levels of quality.  Through the power of stack-exchange I was able to find a bash one-liner script to generate a histogram of image sizes:</p><pre><code>find . -type f -print0 | xargs -0 ls -l | awk '{size[int(log($5)/log(2))]++}END{for (i in size) printf(\"%10d %3d\\n\", 2^i, size[i])}' | sort -n</code></pre><p><img alt=\"\" src=\"https://markhpc.github.io/images/2022-11-28-Lambda/ImageNet_-_Image_Distribution_by_Approximate_Size.svg\" /></p><p>Roughly 80% of the images are in the 64KB or 128KB size bins. Almost all of the remaining images are smaller.  That gives us a pretty good idea of what kind of IOs to expect during classification.  Or at least…it does for frameworks that read those images directly.  In Tensorflow’s case, there’s an alternative format called TFRecord.  TFRecords are basically collections of image data sequentially laid out in much larger files.  Instead of iterating over thousands or millions of individual image files, TFRecords allow Tensorflow to instead stream fewer, larger files that each house multiple images.  It’s a one time cost to pre-process the data so Tensorflow has less work to do during training.  After I downloaded the ImageNet data I took a shot at converting the ImageNet LOC data into TensorFlow records.  Luckily, the TensorFlow tpu github repository already has a <a href=\"https://github.com/tensorflow/tpu/blob/master/tools/datasets/README.md\">tool</a> that can do this.  I had to manipulate the dataset slightly, but ultimately this process worked (at least for the training data):</p><pre><code>pip install gcloud google-cloud-storagepip install protobuf==3.20.1mkdir ~/data/ImageNetFooln -s ~/data/ImageNet/ILSVRC/Data/CLS-LOC/train ~/data/ImageNetFoo/trainln -s ~/data/ImageNet/ILSVRC/Data/CLS-LOC/val ~/data/ImageNetFoo/valln -s ~/data/ImageNet/ILSVRC/Data/CLS-LOC/test ~/data/ImageNetFoo/testln -s ~/data/ImageNet/LOC_synset_mapping.txt ~/data/ImageNetFoo/synset_labels.txtpython imagenet_to_gcs.py --raw_data_dir=/home/ubuntu/data/ImageNetFoo --local_scratch_dir=/home/ubuntu/ExaltedOrbs/ImageNet/tf_records --nogcs_upload</code></pre><p>Perhaps I should say that this worked so long as the original dataset was located on the local NVMe drive.  The persistent storage didn’t fare as well.  Attempting to decompress ImageNet on the persistent storage resulted in blowing past the max number of open files allowed with errors like:</p><pre><code>OSError: [Errno 24] Too many open files.</code></pre><p>Unfortunately this couldn’t be fixed on the instance.  It appeared to be passed through from the host and the persistent storage was completely unusable until the instance was rebooted.  Recently I spoke to one of Lambda’s engineers and they are working on a fix. (It may already be implemented by the time you read this!)  I also want to note that the persistent storage is still in beta so issues like this are not entirely unexpected.  Having said that, before hitting the error it was significantly slower extracting ImageNet on the persistent storage vs on the local NVMe storage.  It’s probably best to extract ImageNet locally and then write the large TFRecords to the persistent storage during the conversion process.  Luckily extracting ImageNet to local storage was fine, and storing the original archive and the resulting TFRecords on the persistent storage worked perfectly fine as well.</p><h2 id=\"fio---baseline-io-results\">FIO - Baseline IO Results</h2><p>Next, I turned my attention to running baseline tests on Lambda’s local and persistent storage using fio.  Fio is a highly configurable and well respected benchmark in the storage community and perfect for generating baseline results.  I decided to use a dataset size that is roughly similar to ImageNet (200GB), the libaio engine in fio with direct IO, and an appropriately high IO depth to let the NVMe drives stretch their legs a bit.</p><p><img alt=\"\" src=\"https://markhpc.github.io/images/2022-11-28-Lambda/Lambda_Labs_8xv100_Storage.svg\" /></p><p>Throughput with the local NVMe drive(s) is surprisingly good.  The persistent storage is slower, but still might be fast enough at a little over 1GB/s for large reads.  16K IOPS was somewhat slower in both cases.  I chose 16K so that I could quickly compare to tests I ran in my Ceph QEMU/KVM performance blog post <a href=\"https://ceph.io/en/news/blog/2022/qemu-kvm-tuning/\">here</a>.  Without getting into the details, I suspect there’s still some room for improved IOPS with Lambda’s setup.  Luckily though, converting into TFRecords should make Tensorflow throughput bound instead of latency bound.  What about PyTorch or other tools that want to read images directly though?  Fio gives us the ability to simulate it by using its ‘bssplit’ feature.  We can take the size ranges and percentiles generated when examining ImageNet and give fio a similar distribution:</p><pre><code>fio --ioengine=libaio --direct=1 --bssplit=2K/1:4K/2:8K/4:16K/8:32K/13:64K/38:128K/33:256K/1 --iodepth=128 --rw=randread --norandommap --size=200G --numjobs=1 --runtime=300 --time_based --name=foo</code></pre><p><img alt=\"\" src=\"https://markhpc.github.io/images/2022-11-28-Lambda/Lambda_Labs_8xV100_Storage_Reads_Second_Bssplit.svg\" /></p><p>This isn’t exactly right as we are not reading data spread across millions of files, but it should provide something of an upper bound on what to expect.  It looks like the persistent storage can do approximately 10K reads/second at a throughput rate of around 750MB/s.  The local storage is about 3-4 times faster.  Local storage should be fast enough to support the kind of images/second throughput rates we want to hit in Tensorflow on 8 V100 GPUs, but the jury is still out for the persistent storage.</p><h2 id=\"tensorflow---imagenet\">Tensorflow - ImageNet</h2><p>Running benchmarks with real data rather than synthetic data is fairly straightforward in Tensorflow.  You simply append data_dir and data_name flags to the CLI invocation to let it know where the TFRecords are located:</p><pre><code>sync; echo 3 | sudo tee /proc/sys/vm/drop_cachespython ./tf_cnn_benchmarks.py --batch_size=256 --num_batches=100 --model=resnet50 --optimizer=momentum --variable_update=replicated --all_reduce_spec=nccl --use_fp16=True --nodistortions --gradient_repacking=2 --compute_lr_on_cpu=True --single_l2_loss_op=True --xla_compile=True --num_gpus=8 --loss_type_to_report=base_loss --data_dir=/home/ubuntu/ImageNet-TF/train --data_name=imagenet</code></pre><p><img alt=\"\" src=\"https://markhpc.github.io/images/2022-11-28-Lambda/Tensorflow_-_ResNet50_Real_Training_First_Attempt_fp16.svg\" /></p><p>Ouch.  Much lower performance with the ImageNet data vs synthetic!  This is especially unfortunate given that 4 years ago the Tensorflow developers reported much better results.  I spent some time reading and experimenting with different settings.  Ultimately the one setting that made a substantial difference was “datasets_num_private_threads”.  In the Tensorflow benchmark source code, this setting is described as: “[The] number of threads for a private threadpool created for all datasets computation.”  I’ll go into more detail what these threads are doing in a bit. For now, let’s see how increasing the number of threads affects the results:</p><p><img alt=\"\" src=\"https://markhpc.github.io/images/2022-11-28-Lambda/Tensorflow_-_ResNet50_ImageNet_Training_fp16_private_threads.svg\" /></p><p>Increasing the number of private threads has a dramatic effect on performance, though I was unable to fully match the performance achieved in the synthetic tests on either the local or persistent storage.  The local storage fared better at high thread counts gradually topping out at around 8600 images/second.  At high private thread counts the persistent storage topped out between 7000-8000 images/second with a higher degree of variability between runs.  I suspect that in this case the persistent storage has likely hit its (per instance) limit.</p><p>In addition to having a dramatic effect on performance, changing the private thread count also had a large effect on the CPU consumption of the TensorFlow process.  CPU usage increases almost linearly with additional private threads up to around 30 cores.  What exactly are these private threads doing?  To answer that question, I utilized two tools that I often deploy when diagnosing CPU usage in Ceph.  When testing with a lower number of private threads, I used linux’s perf tool to look at where cycles are being consumed when the private threads are fully saturated.  At higher levels of private threads, I used my wallclock profiler <a href=\"https://github.com/markhpc/uwpmp\">uwpmp</a> to look at how private threads spend their time when increasing the thread count no longer improves performance.</p><p>In the first case with perf, we can get a good view of the work that these private threads are doing:</p><pre><code>--77.31%--tensorflow::ThreadPoolDevice::Compute          |                    |--51.19%--0x7f511a00c7d8          |          |                    |           --51.18%--tensorflow::jpeg::Uncompress          |--14.48%--tensorflow::ResizeBilinearOp&lt;Eigen::ThreadPoolDevice, unsigned char&gt;::Compute          |--5.47%--tensorflow::CastOpBase::Compute          |--2.66%--tensorflow::ReverseV2Op&lt;Eigen::ThreadPoolDevice, unsigned char, int&gt;::Compute</code></pre><p>The majority of the cycles consumed is in jpeg decompression and resize operations, along with a smattering of other stuff.  What happens if we look at a case with a higher private thread count but now look at wallclock time instead of cycles?  I ended up having some trouble getting the profiler to work properly and consistently get clean callgraphs, but I was able to get at least one run in that revealed some interesting information.  First, I saw time spent in the same functions that perf told us we were spending cycles in:</p><pre><code>+ 100.00% Eigen::ThreadPoolTempl&lt;tensorflow::thread::EigenEnvironment&gt;::WorkerLoop(int) + 99.90% ??? |+ 97.30% ??? ||+ 92.40% ??? |||+ 77.10% _PyEval_EvalFrameDefault ||||+ 47.20% ??? |||||+ 38.10% tensorflow::jpeg::Uncompress(void const*, int, tensorflow::jpeg::UncompressFlags const&amp;, long*, std::function&lt;unsigned char* (int, int, int)&gt;) ||||+ 12.20% tensorflow::ResizeBilinearOp&lt;Eigen::ThreadPoolDevice, unsigned char&gt;::Compute(tensorflow::OpKernelContext*) ||||+ 4.40% tensorflow::CastOpBase::Compute(tensorflow::OpKernelContext*) ||||+ 1.70% tensorflow::ReverseV2Op&lt;Eigen::ThreadPoolDevice, unsigned char, int&gt;::Compute(tensorflow::OpKernelContext*)</code></pre><p>But the wallclock profile also exposed that there may be contention in multiple areas in the private threads around some of the nsync synchronization primitives being used:</p><pre><code> |||||||    |  + 4.50% nsync::nsync_mu_semaphore_p(nsync::nsync_semaphore_s_*) |||||||    |   + 4.50% syscall</code></pre><p>This almost always appeared nested deep inside:</p><pre><code>tensorflow::BFCAllocator::AllocateRaw(unsigned long, unsigned long, tensorflow::AllocationAttributes const&amp;)</code></pre><p>Sadly I was missing a number of debug symbols and don’t 100% trust the wallclock trace.  For now I’ll just say that the private threads are doing a significant amount of work decompressing and manipulating the image data to keep the GPUs fed.  I suspect that with newer and faster GPUs the image retrieval pipeline could become an even bigger issue when training with real image data.  The mystery for me is how The TensorFlow developers achieved such good results 4 years ago without using dedicated private threads at all.  Perhaps they had a significantly faster jpeg decompression mechanism that I am unaware of?</p><h2 id=\"pytorch---imagenet\">PyTorch - ImageNet</h2><p>After running Tensorflow, I also ran some benchmarks in PyTorch using Nvidia’s “DeepLearningExamples” github <a href=\"https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Classification/ConvNets/resnet50v1.5\">repo</a>.  First, I installed the prereqs and setup the repository:</p><pre><code>pip install 'git+https://github.com/NVIDIA/dllogger'pip install --extra-index-url https://developer.download.nvidia.com/compute/redist --upgrade nvidia-dali-cuda110git clone https://github.com/NVIDIA/DeepLearningExamples</code></pre><p>Then, prepared ImageNet for usage in PyTorch:</p><pre><code>cd ~/data/ImageNet/ILSVRC/Data/CLS-LOC/valwget -qO- https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh | bash</code></pre><p>And finally ran a test:</p><pre><code>cd DeepLearningExamples/PyTorch/Classification/ConvNetssync; echo 3 | sudo tee /proc/sys/vm/drop_cachespython ./multiproc.py --nproc_per_node 1 ./main.py --arch resnet50 --label-smoothing 0.1 --run-epoch 1 --amp --static-loss-scale 256 --workspace /home/ubuntu/data/ImageNet-Scratch /home/ubuntu/data/ImageNet-Orig/ILSVRC/Data/CLS-LOC/</code></pre><p>There are a couple of differences here versus the TensorFlow tests.  First, I’m using the raw ImageNet archive instead of a preprocessed TFRecord dataset, so the read behavior is different.  Because I was unable to extract or copy the raw ImageNet archive onto the persistent storage, I’m also only testing the local NVMe drive.  Finally, I didn’t see any specific examples for running with fp16 in nVidia’s documentation, so I’m using amp (automatic mixed precision) which may be slightly slower.</p><p><img alt=\"\" src=\"https://markhpc.github.io/images/2022-11-28-Lambda/Pytorch_-_ResNet50v15_ImageNet_Training_AMP.svg\" /></p><p>Given the number of differences it’s tough to draw direct comparisons with Tensorflow.  Amp is one difference, but it’s quite possible that there are tuning options that could improve performance here that I don’t know about.  I did notice that PyTorch, like Tensorflow, is using quite a bit of CPU to keep the GPUs working.  I suspect that there are ways to tweak the IO pipeline that could improve performance.  For now though, let’s compare the IO patterns on the local NVMe drive during the Tensorflow and PyTorch runs.  I was hoping to be able to use blktrace to do this, but unfortunately was unable to get any data from the virtual devices in the instance.  I was able to collect more general statistics using collectl however.</p><h5 id=\"disk-read-statistics-during-pytorch-8-gpu-run\">Disk Read Statistics During PyTorch 8 GPU run:</h5><table>  <thead>    <tr>      <th>Time</th>      <th>Name</th>      <th>KBytes</th>      <th>Merged</th>      <th>IOs</th>      <th>Size</th>      <th>Wait</th>      <th>QLen</th>      <th>SvcTim</th>    </tr>  </thead>  <tbody>    <tr>      <td>00:29:18</td>      <td>vda</td>      <td>761136</td>      <td>0</td>      <td>6746</td>      <td>113</td>      <td>58</td>      <td>431</td>      <td>0</td>    </tr>    <tr>      <td>00:29:19</td>      <td>vda</td>      <td>752172</td>      <td>0</td>      <td>6648</td>      <td>113</td>      <td>112</td>      <td>810</td>      <td>0</td>    </tr>    <tr>      <td>00:29:20</td>      <td>vda</td>      <td>747824</td>      <td>0</td>      <td>6595</td>      <td>113</td>      <td>84</td>      <td>604</td>      <td>0</td>    </tr>    <tr>      <td>00:29:21</td>      <td>vda</td>      <td>735964</td>      <td>0</td>      <td>6583</td>      <td>112</td>      <td>73</td>      <td>551</td>      <td>0</td>    </tr>    <tr>      <td>00:29:22</td>      <td>vda</td>      <td>695636</td>      <td>0</td>      <td>6237</td>      <td>112</td>      <td>102</td>      <td>760</td>      <td>0</td>    </tr>  </tbody></table><h5 id=\"disk-read-statistics-during-tensorflow-8-gpu-run\">Disk Read Statistics During TensorFlow 8 GPU run:</h5><table>  <thead>    <tr>      <th>Time</th>      <th>Name</th>      <th>KBytes</th>      <th>Merged</th>      <th>IOs</th>      <th>Size</th>      <th>Wait</th>      <th>QLen</th>      <th>SvcTim</th>    </tr>  </thead>  <tbody>    <tr>      <td>00:38:45</td>      <td>vda</td>      <td>1081324</td>      <td>0</td>      <td>8440</td>      <td>128</td>      <td>0</td>      <td>7</td>      <td>0</td>    </tr>    <tr>      <td>00:38:46</td>      <td>vda</td>      <td>927512</td>      <td>0</td>      <td>7241</td>      <td>128</td>      <td>0</td>      <td>7</td>      <td>0</td>    </tr>    <tr>      <td>00:38:47</td>      <td>vda</td>      <td>913512</td>      <td>0</td>      <td>7130</td>      <td>128</td>      <td>0</td>      <td>7</td>      <td>0</td>    </tr>    <tr>      <td>00:38:48</td>      <td>vda</td>      <td>1047444</td>      <td>0</td>      <td>8186</td>      <td>128</td>      <td>0</td>      <td>6</td>      <td>0</td>    </tr>    <tr>      <td>00:38:49</td>      <td>vda</td>      <td>968776</td>      <td>0</td>      <td>7560</td>      <td>128</td>      <td>0</td>      <td>6</td>      <td>0</td>    </tr>  </tbody></table><p><br />When just looking at the IO sizes, both runs appear similar, but that doesn’t tell the whole story.  It is likely that Tensorflow is doing much larger reads that are broken up into contiguous 128KB chunks by the block layer based on the underlying device’s max_sectors_kb setting.  The tells here are the very low queue length and wait times for the TensorFlow run versus the PyTorch run.  In both case the device service times are low (0), but in the TensorFlow case IOs are still backing up in the device queue.</p><p>Interestingly, it appears that it may be possible to use nVidia’s DALI (Data Loading Library) package to <a href=\"https://docs.nvidia.com/deeplearning/dali/archives/dali_170/user-guide/docs/examples/frameworks/pytorch/pytorch-various-readers.html\">read TFRecords into PyTorch</a>.  I didn’t have time to attempt it, but potentially that could have a big effect on IO behavior and performance as well.</p><h2 id=\"conclusion\">Conclusion</h2><p>As I’ve been writing this post, I realize just how complicated it is to understand the performance characteristics of training of neural networks.  Even as we talk about metrics like images/second, the options that are used (batch size for instance) can also affect convergence.  It’s very difficult to come up with a common methodology that is always better than others.  I wonder if another metric, like reaching a desired level of convergence, would be better in the end.  Having said that, I am glad for having done this exercise as I learned some valuable things:</p><ol>  <li>    <p>Pre-processing data into a format like TFRecords on fast local storage is a big win from an IO perspective.  It lets storage systems that have slow metadata performance succeed so long as they have enough sequential read throughput to keep the machine learning framework busy.  This is a big win for many distributed file systems that may have substandard metadata performance (and even the good ones may still benefit).</p>  </li>  <li>    <p>To train on a dataset like ImageNet, you need somewhere around 1-1.3GB/s of raw disk throughput to keep 8 V100 GPUs busy when training in fp16.  For amp or fp32 the requirements are likely lower since the GPUs can’t work quite as fast.  With modern GPUs that are faster than the V100, the disk throughput requirements could be significantly higher.</p>  </li>  <li>    <p>Lambda’s local NVMe storage is likely fast enough to saturate 8 GPUs, even newer ones, so long as the rest of the IO path can keep up.  The persistent storage appears to become a bottleneck with sufficient GPUs and TensorFlow private threads, though can still function fairly well so long as TFRecords are used.  A concern going forward is how to ensure that the data pipeline in TensorFlow and PyTorch are fast enough to keep the GPUs fed.  The Tensorflow benchmark required a large number of private threads and showed potential evidence of contention at high thread counts.  PyTorch did not appear to natively support TFRecords, but NVidia DALI or other 3rd party code might help improve the IO path.</p>  </li>  <li>    <p>If it’s necessary to train directly with images rather than TFRecords, it may not make sense to host them on shared file systems.  It appears that Tensorflow and possibly PyTorch give users the ability to specify a separate training data and work directory.  If all operations against the training data are reads, it may be better to host datasets on read-only block device snapshots. For instance with Ceph, perhaps you could create a read/write RBD volume where you put a certain dataset, take a snapshot, and then map that snapshot as read only on multiple instances that all need access to the same image set.</p>  </li>  <li>    <p>Even with a training set as large as ImageNet, Lambda’s instances have so much memory that eventually the entire dataset becomes cached.  It was necessary to sync and drop caches before each test and keep tests short enough that they didn’t re-read the same data from buffer cache.  I was able to watch as long running tests eventually stopped performing reads and got faster as time went on.  This could make apples-to-apples comparison between different storage vendors difficult if not carefully controlled.</p>  </li>  <li>    <p>I’m almost certainly missing additional tweaks that can help speed up both Tensorflow and PyTorch.  This post shouldn’t be seen as the be-all/end-all for how to achieve high performance with these frameworks, but I hope it may at least help showcase some of the areas that are valuable to investigate when trying to train with real data and achieve high performance.</p>  </li></ol><p>This wraps up my initial work looking at Deep Learning IO behavior.  I hope that next time I can come armed with a bit more knowledge about the internals of how PyTorch and Tensorflow work, focus a bit more on the quality of the training, find even larger datasets to work with, and maybe actually accomplish something useful rather than just play with ImageNet.</p><p>Thanks for reading!</p>",
            "url": "https://hpc.social/personal-blog/2022/an-initial-look-at-deep-learning-io-performance/",
            
            
            
            
            
            "date_published": "2022-11-28T00:00:00-07:00",
            "date_modified": "2022-11-28T00:00:00-07:00",
            
                "author": "Mark Nelson's Blog"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/adam-s-weekly-update-2022-11-27/",
            "title": "Adam’s weekly update, 2022-11-27",
            "summary": null,
            "content_text": "What&#8217;s newThe first thing that&#8217;s new is&#8230; this post! I&#8217;m going to try to do at least a weekly post on the blog now, just a general update and some links. This will hopefully help me get back into the habit of writing on the blog regularly, and maybe inspire me to write a bit more in general.I was off work this week for the Thanksgiving holiday, and traveled Michigan to visit my parents and my brother&#8217;s family. My mom has been struggling with some pretty major health issues this year, so it was really wonderful and reassuring to get to spend some time with her and my dad. I also finally got to meet my brother&#8217;s three-year-old son, who was born right before the pandemic started, and who I hadn&#8217;t managed to meet up until now.On the tech-related front, I used this week to take a break from Twitter (mostly), and to be honest&#8230; it was kinda refreshing! I had developed a pretty bad Twitter habit this year, doomscrolling for more time than I like to admit. While I really like Twitter and I&#8217;ve had some nice career boosts from it, it was also a time sink that was not entirely healthy.Admittedly, that time was somewhat replaced by playing around on the Fediverse / Mastodon. But with the lack of algorithmic suggestions, quote tweets, and other means of virality, that network so far feels a lot quieter and less time-consuming than Twitter. Tim Bray has a good post up which discusses some of the advantages and pitfalls of federated social media, and I can highly recommend reading that. I&#8217;m still a bit skeptical that it will be a practical &#8220;Twitter replacement&#8221; for most people, but so far I&#8217;m finding it pleasant.What I&#8217;m readingNonfiction book: Code, Second Edition, by Charles Petzold. This book walks through the process of building a working computer, starting with ideas like Morse code, then working up from logic gates on up. This is technically a re-read, as I read the first edition&#8230; 10+ years ago? But I&#8217;m getting a lot more out of it this time around, and really enjoying it.Fiction book: The Spare Man, by Mary Robinette Kowal. A cozy murder mystery on a luxury cruise to Mars. I&#8217;m only a few chapters in, but already greatly enjoying myself.&#8220;Hiding theory in practice&#8221;, by Fred Hebert. I&#8217;ve been reading a lot about safety engineering and its application to computing lately, but that community can sometimes get off into the weeds about points of theory that don&#8217;t have consensus in the broader computing community. This post has a good discussion of how to use the theory of safety engineering to guide decisions, without requiring that everyone working with you be handed a reading list.&#8220;Paper: Repentance as Rebuke: Betrayal and Moral Injury in Safety Engineering&#8221;, also by Fred Hebert. A discussion of a paper by Dekker et al which looks at the aftermath of the 737 MAX air disasters, and the public repentance of some of the engineers who were involved. Go read the post, it&#8217;s great. And I&#8217;m planning to read the original paper this week.&#8220;Cannon Lake: Intel&#8217;s Forgotten Generation&#8221;, from Chips and Cheese. Really I&#8217;ve been reading a bunch of the technical posts from Chips and Cheese lately, and they&#8217;re doing pretty good analyses of recent hardware. They&#8217;ve definitely earned that spot in my RSS reader.Glenn K Lockwood&#8217;s &#8220;SC&#8217;22 Recap&#8221;. I was sad to miss Supercomputing this year, though enough folks have come down with COVID that I don&#8217;t really regret the decision. But Glenn wrote up a really interesting recap post, with an interesting new viewpoint now that he&#8217;s working at Microsoft Azure. Among other things, he included a whole section titled The underwhelming, with the opening line &#8220;The biggest deal appears to be that exascale is here, and it turns out that it&#8217;s not that big of a deal.&#8221;Recent recipesBecause it was Thanksgiving, I did a lot of cooking this week! I&#8217;m not going to list everything I made, but a few of my favorites were:Cheesy Garlic Butter Rolls from Delish: Nothing special, but really tasty.Challah Stuffing from Smitten Kitchen: This recipe was a huge winner, with most of the family coming back for seconds, and then having more the next day for leftovers. It was really good, and is probably what I&#8217;ll make if I ever do stuffing again.Best Challah from Smitten Kitchen: I baked the bread that went into the stuffing, and it was really tasty on its own! This recipe makes two loaves, and I only needed one for the stuffing. So I also made french toast with it, which worked really nicely.Pet photosGotta have those pet photos.",
            "content_html": "<h2>What&#8217;s new</h2><p>The first thing that&#8217;s new is&#8230; this post! I&#8217;m going to try to do at least a weekly post on the blog now, just a general update and some links. This will <em>hopefully</em> help me get back into the habit of writing on the blog regularly, and maybe inspire me to write a bit more in general.</p><p><span id=\"more-264\"></span></p><p>I was off work this week for the Thanksgiving holiday, and traveled Michigan to visit my parents and my brother&#8217;s family. My mom has been struggling with some pretty major health issues this year, so it was really wonderful and reassuring to get to spend some time with her and my dad. I also finally got to meet my brother&#8217;s three-year-old son, who was born <em>right</em> before the pandemic started, and who I hadn&#8217;t managed to meet up until now.</p><p>On the tech-related front, I used this week to take a break from Twitter (mostly), and to be honest&#8230; it was kinda refreshing! I had developed a pretty bad Twitter habit this year, doomscrolling for more time than I like to admit. While I really like Twitter and I&#8217;ve had some nice career boosts from it, it was also a time sink that was not entirely healthy.</p><p>Admittedly, that time was somewhat replaced by playing around on the <a href=\"https://calico.social/ajdecon\">Fediverse / Mastodon</a>. But with the lack of algorithmic suggestions, quote tweets, and other means of virality, that network so far feels a lot quieter and less time-consuming than Twitter. Tim Bray has a <a href=\"https://www.tbray.org/ongoing/When/202x/2022/11/26/Bye-Twitter\">good post</a> up which discusses some of the advantages and pitfalls of federated social media, and I can highly recommend reading that. I&#8217;m still a bit skeptical that it will be a practical &#8220;Twitter replacement&#8221; for most people, but so far I&#8217;m finding it pleasant.</p><h2>What I&#8217;m reading</h2><ul><li><strong>Nonfiction book: </strong><a href=\"https://bookshop.org/p/books/code-the-hidden-language-of-computer-hardware-and-software-charles-petzold/18465738\">Code, Second Edition, by Charles Petzold</a>. This book walks through the process of building a working computer, starting with ideas like Morse code, then working up from logic gates on up. This is technically a re-read, as I read the first edition&#8230; 10+ years ago? But I&#8217;m getting a lot more out of it this time around, and really enjoying it.</li><li><strong>Fiction book: </strong><a href=\"https://bookshop.org/p/books/the-spare-man-mary-robinette-kowal/18834426\">The Spare Man, by Mary Robinette Kowal</a>. A cozy murder mystery on a luxury cruise to Mars. I&#8217;m only a few chapters in, but already greatly enjoying myself.</li><li><a href=\"https://ferd.ca/hiding-theory-in-practice.html\">&#8220;Hiding theory in practice&#8221;, by Fred Hebert</a>. I&#8217;ve been reading a lot about safety engineering and its application to computing lately, but that community can sometimes get off into the weeds about points of theory that don&#8217;t have consensus in the broader computing community. This post has a good discussion of how to use the theory of safety engineering to guide decisions, without requiring that everyone working with you be handed a reading list.</li><li><a href=\"https://cohost.org/mononcqc/post/385225-paper-repentance-as\">&#8220;Paper: Repentance as Rebuke: Betrayal and Moral Injury in Safety Engineering&#8221;, also by Fred Hebert</a>. A discussion of <a href=\"https://link.springer.com/article/10.1007/s11948-022-00412-2\">a paper by Dekker <em>et al</em></a> which looks at the aftermath of the 737 MAX air disasters, and the public repentance of some of the engineers who were involved. Go read the post, it&#8217;s great. And I&#8217;m planning to read the original paper this week.</li><li><a href=\"https://chipsandcheese.com/2022/11/15/cannon-lake-intels-forgotten-generation/\">&#8220;Cannon Lake: Intel&#8217;s Forgotten Generation&#8221;, from <em>Chips and Cheese</em></a>. Really I&#8217;ve been reading a bunch of the technical posts from <em>Chips and Cheese</em> lately, and they&#8217;re doing pretty good analyses of recent hardware. They&#8217;ve definitely earned that spot in my RSS reader.</li><li><a href=\"https://glennklockwood.blogspot.com/2022/11/sc22-recap.html\">Glenn K Lockwood&#8217;s &#8220;SC&#8217;22 Recap&#8221;</a>. I was sad to miss Supercomputing this year, though enough folks have come down with COVID that I don&#8217;t really regret the decision. But Glenn wrote up a really interesting recap post, with an interesting new viewpoint now that he&#8217;s working at Microsoft Azure. Among other things, he included a whole section titled <em>The underwhelming</em>, with the opening line &#8220;The biggest deal appears to be that exascale is here, and it turns out that it&#8217;s not that big of a deal.&#8221;</li></ul><h2>Recent recipes</h2><p>Because it was Thanksgiving, I did a lot of cooking this week! I&#8217;m not going to list everything I made, but a few of my favorites were:</p><ul><li><a href=\"https://www.delish.com/cooking/recipe-ideas/a23340027/cheesy-garlic-butter-rolls-recipe/\">Cheesy Garlic Butter Rolls from Delish</a>: Nothing special, but really tasty.</li><li><a href=\"https://smittenkitchen.com/2019/11/challah-stuffing/\">Challah Stuffing from Smitten Kitchen</a>: This recipe was a huge winner, with most of the family coming back for seconds, and then having more the next day for leftovers. It was really good, and is probably what I&#8217;ll make if I ever do stuffing again.</li><li><a href=\"https://smittenkitchen.com/2008/09/best-challah-egg-bread/\">Best Challah from Smitten Kitchen</a>: I baked the bread that went into the stuffing, and it was really tasty on its own! This recipe makes two loaves, and I only needed one for the stuffing. So I also made french toast with it, which worked really nicely.</li></ul><h2>Pet photos</h2><p>Gotta have those pet photos.</p><figure class=\"wp-block-image size-large is-resized\"><img alt=\"A blond golden doodle in a red harness and a blue bandanna lays on sandy dirt and looks into the camera\" class=\"wp-image-271\" height=\"233\" src=\"https://thinking.ajdecon.org/wp-content/uploads/2022/11/IMG_6863-1024x768.jpeg\" width=\"311\" /></figure><figure class=\"wp-block-image size-large is-resized\"><img alt=\"A white calico cat sits on a blanket and washes her front paw\" class=\"wp-image-272\" height=\"410\" src=\"https://thinking.ajdecon.org/wp-content/uploads/2022/11/69075713241__19379770-6B0C-4780-8DD0-30C62A033C88-768x1024.jpeg\" width=\"308\" /></figure><figure class=\"wp-block-image size-large is-resized\"><img alt=\"A gray-brown tabby cat wearing a green collar sitting on a wall, looking vaguely toward the camera\" class=\"wp-image-273\" height=\"405\" src=\"https://thinking.ajdecon.org/wp-content/uploads/2022/11/69073206299__DB9CA33B-0EB5-4681-96DA-8368554B6B8A-768x1024.jpeg\" width=\"304\" /></figure>",
            "url": "https://hpc.social/personal-blog/2022/adam-s-weekly-update-2022-11-27/",
            
            
            
            
            
            "date_published": "2022-11-27T15:28:16-07:00",
            "date_modified": "2022-11-27T15:28:16-07:00",
            
                "author": "Thinking Out Loud"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/sc-22-recap/",
            "title": "SC'22 Recap",
            "summary": null,
            "content_text": "The biggest annual conference in HPC, the SC conference, was recently held in Dallas, Texas in its second hybrid incarnation since being all-remote for the pandemic. This year attracted over 11,000 attendees which is much closer to the pre-pandemic high of 14,000 than last year's 7,000, and judging from the crushed conference rooms and busy expo floor, it looks like SC is not that much worse for wear.This year's conference quite different for me since I attended for my first time as a vendor, not a researcher or practitioner, and I spent most of my days behind closed doors talking to customers. I didn't get to attend any of the keynotes, BOFs, or panels to which I wasn't invited as a result, so I'm not really qualified to give an erudite summary of the conference or expo this year.So instead, I'm just writing down what I remember in order that I remember it and not necessarily in a coherent narrative form. I'm sure I missed a lot (for example, mixed precision seemed big this year, and I heard Jack Dongarra gave a fantastic Turing Award talk) so I encourage others to write their own recaps and share with the community!High-level themesI actually started writing an SC'21 recap last year which I never posted, and re-reading the intro was funny--you'd think nothing has changed in the last year.The underwhelmingThe biggest deal appears to be that exascale is here, and it turns out that it's not that big of a deal. China let the air out of the tires by debuting their exascale systems at SC'21, and not only did they thumb their nose at Top500 by not submitting, they debuted by winning a Gordon Bell prize instead. The first US exascale system, Frontier, was debuted at ISC this year leaving its showing at SC a bit deflated too. Frontier was featured in the Gordon Bell prize-winning paper this year, but that work required the use of four Top-10 systems, not just Frontier, painting the reality that one giant computer rarely stands on its own when it comes to advancing science.This isn't to say that deploying exascale systems isn't a noteworthy feat and worth commendation, but I felt like the hype over the last five years treated the achievement like an end state instead of a milestone. And now that we've passed the milestone, the community is grasping to figure out what comes next. So what is next?Quantum had a strong and growing presence at SC, as it has for the last few years. But the conclusion of the panel \"Quantum Computing: A Future for HPC Acceleration\" was that no, it's not close to being ready.Disaggregation and composability was another theme with growing momentum. And like quantum, there was a panel asking the same question: \"Does HPC need composability now?\" The answer, again, was no, not yet. More on that below.What about RISC-V? Surely that will revolutionize the field. As it turns out, the answer there is also that RISC-V is not ready to do anything useful for HPC yet.The list goes on of technologies and trends that people are trying to boost now that exascale is \"solved.\" The reality, I think, is that \"exascale\" will take years to actually mature since it appears to have a ton of technical debt that accumulated during the race to be first. US Exascale rests on the shoulders of AMD and Intel, two companies whose software stacks have not caught up to the market leader, so there will be a lot of thrashing around as development practices and optimization settle out around these systems.Struggling with code porting is not very exciting to computer science Ph.D.s, so I expect future SCs to mirror this one and bifurcate into two distinct tracks: those struggling to identify the next big thing in the research space, and those struggling to use the systems that were rushed to deployment.The unexpectedMy SC experience was very biased since I didn't get out much, but two related themes kept popping up across different meetings and the sessions I did attend.Power efficiency is serious business now. It used to seem like people talked about the need for energy-efficient HPC in an abstract sense while continuing to jam more power into every rack without changing their approach to system design, facilities, and deployment models. That has hit a hard wall with energy prices soaring in Europe, though. The financial impacts of power-inefficient supercomputing have gone from a one-time capex cost to an ongoing opex cost that is putting many HPC facilities on an unsustainable cost trajectory. Even sites that aren't doing new deployments are facing sudden, sharp increases in their costs, and nobody has good answers about how they will keep the lights on.Cloud HPC is confusing. With only 15% of total HPC dollars winding up in the cloud, it's little surprise that most HPC folks are only peripherally aware of what HPC in the cloud really means. Worse yet, a subset of those folks are actively hostile towards the idea of running HPC workloads in the cloud. I spoke with my colleagues from all three major cloud service providers as well as my colleagues in DOE, NSF, and education throughout the week, and everyone painted this same general picture.There seems to be a mismatch between the expectations of on-prem HPC folks and cloud HPC folks. For example, I was asked why Windows doesn't support OpenMP very well, and after a bit of digging, I realized that the question really wasn't about using OpenMP on Windows as much as it was about using OpenMP in the cloud. There was a latent assumption that \"HPC in Microsoft's cloud\" must mean \"HPC on Windows\" which, for the record, is false--I don't even know how to use Windows anymore. Similarly, people decried the performance impacts of sharing HPC nodes with others in the cloud (they are not shared), overheads of virtualizing InfiniBand or GPUs (everyone uses PCIe passthrough or SR-IOV for HPC nodes), and other misconceptions.This isn't to say that cloud people aren't confused too; I heard stories about conversations that went sideways because a cloud folks (not from my employer, thankfully!) didn’t realize that the requirements of a traditional gov/edu HPC facility couldn’t be neatly wrapped up into a single workload with a single solution, contrary to the case across many commercial AI shops. And both sides are struggling to find models for partnership and engagement that mirror the traditional relationship between places like a DOE or NSF facility and a company like Cray. HPC departments are used to buying supercomputers and parallel file systems, while cloud providers sell computing and storage as a service. The distinction may seem trivial at the surface, but there's a large divide that becomes evident once both sides start trying to drill into the details of what a partnership would look like.Parallel I/O in Practice TutorialThis was my fifth year contributing to the Parallel I/O in Practice Tutorial with my colleagues at Argonne and Google, and it was our first time doing it in-person since 2019. It felt really good to be back in front of people to opine about the perils of POSIX and the greatness of the Darshan I/O profiling tool, and this year I retired out the material I used to present on burst buffers (since DataWarp and Infinite Memory Engine have lost relevance in HPC) and the TOKIO holistic I/O analysis framework (since it is no longer funded/maintained). In their stead, I presented material on benchmarking with IOR and mdtest I debuted at LUG 2022 this year.I haven't gotten feedback yet on whether this change was a net positive one, but I think it went over well. Benchmarking I/O is really challenging if you don't understand how things like page cache really work in distributed systems, and walking through some benchmark examples concretizes a lot of abstract parallel file system concepts like locking and striping. And since benchmarking is a rabbit hole of arbitrary complexity, ending the tutorial with advanced benchmarking topics turned out to be a nice way to add buffer to the end of an eight-hour stretch of carefully timed presentations. It's very easy to skip over the nuances of analyzing mdtest outputs if attendees have a lot of questions about more important things at the end of the day.The most surprising observation of the tutorial is how many attendees aren't using MPI anymore. We got a lot of questions last year about task-oriented I/O, and this year had some great questions about trying to understand or tune the I/O performed by Python-based analytics frameworks. We decided to add support for Darshan to profile non-MPI applications back in 2019 which is now paying dividends by ensuring it is a relevant tool for these new analytics and AI workloads, and we'll probably have to give more attention to optimizing these workloads' I/O in the future.DAOS User GroupMonday morning was cold and rainy--a perfect day to attend the 2022 DAOS User Group which was held off-site at the Fairmont Hotel.Whether you particularly care about DAOS or not, the cross-community HPC I/O brain trust is guaranteed to be in attendance, and this year did not disappoint. In addition to the expected stakeholders from Intel and DOE, representatives from all three big CSPs were in attendance. Google Cloud, Seagate, and HPE/Cray were all on the agenda, painting a diversifying landscape of large HPC companies investing time into DAOS and the strength and willingness of the DAOS team to partner with all comers.Life after OptaneThe question that opened up the meeting, of course, was \"what is the future of DAOS since Intel cancelled Optane?\" Kelsey Prantis had the official statement (I'll replace the grainy photo once the DUG slides are online...):The high-level project answer is that DAOS isn't going anywhere. Aurora, by virtue of still having Optane DIMMs, will not be affected, and DAOS will maintain support for Optane until Intel drops its last Optane DIMMs (Crow Pass for Sapphire Rapids) from support life sometime towards the end of this decade.For new customers who aren't going to use Optane, the answer is \"Metadata on NVMe,\" a development being codeveloped by Intel, HPE, and Google to implement a write-ahead log (WAL) and allow DAOS to use volatile DRAM instead of Optane. It will work like a file system journal in that a compact representation of writes will be committed to NVMe immediately after landing in DRAM, and then DAOS will asynchronously write back the properly serialized representation of that transaction after it is acknowledged. Johann Lombardi had a helpful cartoon that showed how this WAL will fit into DAOS:A key benefit of DAOS's implementation of this WAL is that it will be able to still service incoming writes while flushing old writes; although I don't fully grasp how this works, it is something enabled by the sophisticated I/O scheduler already implemented in DAOS.The complete implementation isn't expected to be released until Spring 2024, but it appears to touch only a few components of DAOS and doesn't affect anything above the VOS layer of the DAOS server.There was also mention of developing operability with new CXL-attached memory-semantic SSDs to keep the persistent memory capability of DAOS alive beyond Optane. I'm not sure if this would offer a performance benefit over the metadata-on-NVMe feature; early results show that metadata-on-NVMe actually delivers higher IOPS than Optane since the synchronous write path is much simpler without having to account for memory persistence. That said, I didn't really follow the full extent of options on the table for how DAOS metadata may work across different types of memory though.DAOS in the flesh at ArgonneKevin Harms presented an update on Aurora's massive 220 PB DAOS installation and laid out its configuration. There are 1,024 DAOS servers based on the Intel Coyote Pass server design, each sporting2x Intel Xeon 5320 (Ice Lake) sockets2x DAOS engines (one per socket)16x 32GB DDR4 DIMMs16x 512GB Optane DIMMs (Persistent Memory 200)16x 15.36 TB Samsung PM1733 NVMe SSDs2x 200 Gb/s Slingshot NICsThe total configuration is quoted at 220 PB usable, but Kevin pointed out that this assumes that every object is erasure coded at 16+2. Unlike virtually every other storage system out there, though, users can choose the data protection for their individual objects when they create them, meaning this 220 PB capacity is an upper limit to what users can do. Users with very hot, read-only objects may choose to replicate instead of erasure code, while others who are capacity-constrained may choose to erasure code everything at 16+2 at the cost of latency and IOPS. This flexibility is really powerful for users since they can tailor their object layout (\"object class\" in DAOS parlance) to match the needs of their workload.Argonne will be slicing up this DAOS system by giving each scientific project its own DAOS pool, and each pool will be assigned to only 80% of the available DAOS servers by default. This seems like a nice way of providing most of the storage system performance to every user, but offering more freedom to work around bad hardware, bad users, and other performance problems that plague file systems like Lustre that distribute everything across every single server equally.Finally, I noticed that Aurora will be using Samsung SSDs, not the Intel (now Solidigm) QLC NAND that appeared in all the DAOS slides floating around two years ago. I'm not sure what happened there, but the move from Solidigm QLC to Samsung TLC couldn't have been cheap.New features and contributionsDAOS is starting to pick up some truly valuable features that are being developed and contributed by third parties. Of note, croit has contributed a feature which allows DAOS to serve up NVMe over Fabrics targets, and Seagate contributed an S3 gateway for DAOS. Along with the DFS file system interface, DAOS now offers the trifecta of standard object, block, and file services just like Ceph. Unlike Ceph though, performance on DAOS is a first-class citizen. While croit made it clear that the NVMeoF support still has a ways to go to improve the way it does thread pooling and provides resilience, they showed 1.4 million IOPS from a single storage client using TCP over Ethernet with minimal client-side overhead.Intel is also developing multitenant support for DFUSE, allowing a single compute node to share a DAOS mount and let permissions be enforced through UID/GID just like a regular file system. Before this update, the FUSE-based nature of DAOS allowed any unprivileged user to mount their container (good), but only one FUSE agent could be alive on a single node at a time (not good) which prevented multiple users sharing a node from both mounting their own containers.DAOS also has some longer-term enhancements that I thought were interesting:expanding the range of POSIX calls supported by DAOS's intercept library to include metadata calls and memory-mapped I/O using userfaultfdimplementing collaborative caching - essentially reimplementing the Linux kernel page cache in userspace so that multiple processes can share cached DAOS pagessupporting a computational storage paradigm by enabling offload of userspace eBPF scripts to DAOS serversDAOS in a larger data center ecosystemDean Hildebrand from Google Cloud then gave an overview of Google's efforts in bringing DAOS into the cloud. He had some nice performance graphs and I'll link the full presentation here once it's uploaded (it's worth a watch), but the part I found the most insightful was how they are trying to decide where a technology like DAOS fits in the larger cloud storage ecosystem. He outlined two different ways DAOS could work in GCP:Caching: Google Cloud Storage (GCS) is the point of truth and DAOS is a cacheTiering: DAOS is a point of truth, and GCS is an archiveHe said they were leaning towards the caching model where data only lives ephemerally in DAOS, and personally, I think this is the right move since DAOS in the cloud is not resilient without Optane. However, this choice reflects a much larger tension in cloud storage for HPC:The centerpiece of every cloud's data story is a scalable, low-cost, low-performance object store which is analogous to what on-prem HPC would call campaign, community, or project storage.HPC demands higher performance than what these object stores can generally deliver though.To bridge the gap between these two truths, auxiliary services must bolt on to the object layer and provide higher performance, at a higher cost, for the duration of I/O-intensive HPC jobs. Some choose to provide true tiering from object into a resilient layer of flash (like FSx Lustre and Weka do), while others project the contents of the object through a high-performance caching layer (like HPC Cache and File Cache) and are never meant to persistently hold data.This isn't rocket science, but I never thought deeply about the two models since campaign/community/project storage in on-prem HPC is usually fast enough to avoid needing caches or fine-grained tiering capabilities.John Bent also had a thought-provoking presentation about how Seagate's now-\"deprioritized\" CORTX object store, which once competed with DAOS as Mero, contains ideas that can complement DAOS:Whereas DAOS delivers high performance using NVMe, CORTX delivers great economics using HDDs, and their strengths are complementary to each other. While I don't fully grasp how a tiered (or caching!) system comprised of DAOS and CORTX could be implemented, John rightly pointed out that the same level of space efficiency can deliver higher data protection if multi-level erasure coding is used to stripe across durable block storage. His specific example was erasure coding at 8+1 across servers and 10+1 within servers to deliver both high efficiency and high durability. This could map to something like running DAOS atop something like CORVAULT, but I don't think all the necessary pieces are in place to realize such a harmonious coexistence yet.Of course, completely tossing Reed-Solomon for something more sophisticated (like VAST does with its locally decodable 150+4 scheme) obviates the need for multilevel erasure entirely. But DAOS has not gone down that route yet.And as with every talk John gives, there were lots of other interesting nuggets scattered throughout his presentation. Two of my favorites were:A slide that pointed out that, when you buy something like Ceph as an appliance, you may be spending only 25% of the total cost on storage media and the rest is infrastructure, service, and support. This struck me as a bit on the low end, but some enterprisey NAS and midrange parallel file system appliances can go this low. Spending 60% to 90% on media is a lot nicer for the buyer (and companies like Seagate) if you can buy at scale or eschew the white-glove support, and John suggested that it's up to companies like Seagate to fix the software issues that require customers to pay for white-glove support in the first place.  After all, the less someone spends on support and licenses, the more they can spend on Seagate hard drives.John's final slide pointed out that object stores were originally designed to get around the limitations of POSIX file systems, but as they've evolved over the last decade, they're starting to look a lot like file systems anyway since they require strong consistency, hierarchical namespaces, and familiar file semantics. Has all the work put into developing super-fast object stores like DAOS over the last ten years really just brought us back full circle to parallel file systems?  Companies like VAST and Weka have shown that maybe POSIX isn't as bad as the research community (myself included!) have claimed it to be; it was really just low-performance implementations that nobody wanted.Once John's talk is uploaded to the DUG 2022 website, I'll link it here.  Like Dean Hildebrand's talk, it is well worth watching (but for wildly different reasons!)PDSW 2022I had to duck out of the DAOS User Group early to run (through the rain) to the 7th International Parallel Data Systems Workshop (PDSW 2022) on Monday afternoon.Much to everyone’s surprise, PDSW was only given a half day this year and everything felt a little compressed as a result. The organizers kept the work-in-progress (WIP) sessions which can often be an interesting peek into what students are pursuing, but little A/V problems and the unforgiving schedule probably did a disservice to the up-and-comers who use the WIP track to lay the groundwork for future full-length papers. Hopefully SC’23 restores PDSW to its original full-day status.&lt;p&gt;&lt;/p&gt;Splinters keynote from Arif Merchant at GoogleThe keynote presentation was given by Arif Merchant from Google about Splinters, the framework that Google Cloud uses to sample I/Os in a scalable way. The challenge they face is that it's impossible to trace and store every single I/O that hits Google's storage servers (D servers), but having an understanding of I/O patterns is essential for characterizing workload I/O behavior and planning for future infrastructure. In fact, this problem is so important that Google isn't the only cloud that's solved it!A lot of what Arif talked about is very similar to how Azure does its I/O tracing under the hood. I suppose it should not be surprise that there are only so many ways to solve the challenge of sampling individual IOPS in a way that fairly represents the aggregate workload of a huge distributed storage system. One really smart thing Splinters does that I liked was sample along two different dimensions: not only do they evenly sample across all IOPS at a fixed rate (the obvious thing), but they also sample across files at a fixed rate. In this latter case of per-file sampling, they take a tiny fraction of files and capture every I/O for that file to get a complete picture of how individual files are being accessed.This file sampling fills the huge gap that exists when randomly sampling IOPS alone. Because different I/Os have different \"costs\" (for example, reading a 1 MiB file using a single 1 MiB read op or 256x 4 KiB read ops are functionally equivalent to an application), randomly sampling ops introduces systematic biases that can be difficult to back out after the data has been sampled, subsampled, aggregated, and reduced. Splinters' approach lets you see the workload from two different angles (and biases) and answer a much larger range of questions about what's really happening across thousands of storage servers.That said, it was interesting to hear Arif describe how Splinters evolved out of a different internal Google project but wound up outliving it. Splinters is also similar to, but slightly different from, their Dapper infrastructure which also does scalable distributed system tracing. And he made overtures to F1, a scalable SQL database that is similar to (but not the same as) the SQL-like query interface that Splinters uses. I got the impression that new technologies come and go pretty quickly at Google, and there's a large appetite for creating new software systems outright rather than shoehorning an existing system into solving a new problem. I can't say one way is better than the other; I was just surprised at the contrast with my own experiences.Practical papersPDSW had a healthy combination of both very-researchy papers and applied research papers this year. I could only stick around for the applied papers, and two left an impression.In the first, Jean Luca Bez presented Drishti, a tool that lives downstream of the Darshan I/O profiling library and finally does what the Darshan community has danced around for years--turning a Darshan log into an actionable set of recommendations on how to improve I/O performance. It does this by cataloguing a bunch of heuristics and using Darshan's new Python integrations to pore through a log and identify known-problematic I/O patterns. Like Jean Luca's DXT Explorer tool, Drishti has a slick user interface and greatly extends the usability and insights that can be pulled out of a Darshan log file. It probably won't win a Turing Award, but this sort of work is probably going to benefit scores of HPC end-users by making Darshan (and troubleshooting I/O problems) much more accessible to mere mortals for years to come.Adrian Jackson also presented a very tidy apples-to-apples comparison of DAOS and Lustre on the same hardware using both a systems-level benchmark and an application-inspired, object-oriented data model benchmark. The specific bake-off of a new curiosity (DAOS) and the decades-old incumbent (Lustre) is probably interesting to storage nerds, but I think the real novelty of the work is in its exploration of some uncomfortable realities that the HPC I/O community will have to face in the coming years:Does \"slow memory\" (nonvolatile Optane or CXL-attached memory SSDs) give actual benefit to existing file systems (like Lustre), or is rethinking the entire storage stack (like DAOS did) really necessary to unlock the performance of new hardware?Do applications need to rethink their approach to I/O to make use of post-POSIX storage systems like DAOS, or is performing I/O as you would on a file system (Lustre) on a post-POSIX storage system (DAOS) good enough?My take from the work is that, for simple I/O patterns like checkpoint/restart, you can get pretty far by just treating something like DAOS the same as you would a parallel file system:Figure from Manubens et al, \"Performance Comparison of DAOS and Lustre for Object Data Storage Approaches.\"But if you want your data at rest to have the same data model as how it's handled within the application, you really ought to use a storage system that supports data models that are more expressive than a stream of bytes (which is what POSIX files are).The authors didn't do a perfect job of giving Lustre its fair shake since they chose to use (abuse) directories and files to represent their application's data model on-disk instead of developing an object-file model that file systems like Lustre handle a little better. But let's be real--HPC is full of applications that do the exact same thing and represent datasets on-disk using complex hierarchies of directories and files simply because that's the easiest way to map the application's representation of data into the standard file system model. In that sense, storage systems that represent rich data models in a high-performance way should be really valuable to naive applications that map in-memory data structures directly to files and directories.Going back to John Bent's closing slide from his DAOS User Group talk, though, does any of this even matter since all answers lead back to parallel file systems? Maybe there's something to be learned about adding better back-door APIs that support more diverse data models than what POSIX file interfaces give us.The SC22 ExpoThe expo is my favorite part of SC because it's when I get to talk to people one-on-one and learn about corners of the HPC industry that I would've never otherwise sought out. Much to my dismay, though, I had very little time to walk the floor this year--so little that I didn't get any swag. If you want to read up on what interesting technology was being showcased, I strongly recommend reading all the great content that Patrick Kennedy and his team at STH created covering the expo.That said, I did notice some curious trends about the show floor overall.The NVIDIA booth was notably absent this year (though they shared booth space with partners), and many of the usual top vendors had significantly smaller presence on the expo floor. Just for fun, I compiled the top ten(ish) vendors by booth size:Weka.io (3,200 sqft)VAST Data, Department of Energy, Penguin Computing, HPE, and Microsoft (2,500 sqft)AWS (2,000 sqft)Google and TACC (1,600 sqft)Supermicro, AMD, Intel, Dell, NASA, and Indiana University (1,500 sqft)I think it's amazing to see all-flash storage companies at the top of the list alongside all of the Big 3 cloud service providers. I may be reading too much into this, but this may mean that the money behind SC is shifting towards companies playing in the cloud-based AI space instead of traditional big iron for simulation. Or perhaps it's a sign that most of the traditional HPC players are taking a hard look at the return they get on a big booth given the current economic climate and pulled back this year.I did chat with a couple colleagues who completely opted out of a booth this year (for reference, SC'21 had 10% fewer exhibitor booths than SC'19), and the reasoning was consistent: they found more value in having staff meet with customers privately or attend the technical sessions and engage with people organically. Combined with a bit of bad taste left over from SC's high cost of hosting pandemic-era \"digital booths\" despite low return (did anyone visit digital booths at SC'20 or SC'21?), I can see why some vendors may have chosen to skip the expo this year.Whatever the reasons may be, I was a bit sad to see such a small presence from some of my favorites like IBM, Fujitsu, Atos, and NEC. Hopefully the SC Exhibits Committee (and the economy!) can find ways to bring back the pre-pandemic glory of the show floor.The expo wasn't all doom and gloom though! Even though I couldn't make my complete rounds this year, there were a couple of highlights for me.VAST's masterful marketingPerhaps the splashiest vendor at SC was VAST Data who had a brilliant marketing presence. First was the giant Vastronaut mascot that was the centerpiece of their booth:A quick search of Twitter shows just how many people seized the opportunity to take a selfie at their booth. I would love to know how they transported that thing to and from the conference, but whatever the cost, I'll bet it was worth it.At the Grand Opening Gala on Monday, they also gave out delightfully tacky light-up cowboy hats that everyone seemed to be wearing:We were there! #sc22 #sc2022 @VAST_Data pic.twitter.com/fWhuSgBfpL— ntnu-hpc (@ntnuhpc) November 15, 2022The subtle genius of this was that not only did people wear them during the gala and the Flop Gun-themed Beowulf Bash 2022 party later that night, but they had to wear them on their plane rides home since they were so inconveniently bulky. Proof in point, my wife (who doesn't work in tech) sent me this text message to confirm that she was waiting for me at the right luggage carousel at San Francisco Airport:I wonder how many innocent bystanders, traveling home for Thanksgiving on Thursday or Friday, saw the shiny cowboy hats at airports around the country and wondered what VAST was.The icing on the cake was VAST's CEO, Renen Hallak, parading around in an unmissable Chuck McGill-style space suit all week, clearly not taking himself too seriously and painting VAST as a work hard/play hard kind of company. Now, do flashy space suits and blinking cowboy hats alone mean VAST has a great product? I can't say**. But marketing is an art that I appreciate, and VAST hit some great notes this year.** (Seriously, I'm not sure I wouldn't get in trouble for opining about another company here.)The Microsoft hardware barThe only booth where I spent any appreciable time this year was my own employer's. I personally love booth duty and accosting strangers on the show floor, especially if there's something interesting at the booth to jumpstart a conversation. When I worked at SDSC it was a Raspberry Pi cluster, and at the Microsoft booth this year it was the \"hardware bar.\"In addition to the customary booth presentations with giveaways, swag desk, seating area, and a fun caricature artist, the physical servers that underpin the HPC nodes in Azure were on display. Microsoft contributes its hardware platform designs to the Open Compute Project so the physical hardware that runs in Azure data centers isn't entirely mysterious. Still, every cloud has its hardware secrets, so I was surprised to see these servers laid bare.The newest HPC node type (dubbed HBv4) on display was a node powered by AMD's Genoa processors just announced a few days earlier:This wasn't a display model, either; it had real DDR5 DRAM, a real NDR InfiniBand HCA, real PCIe Gen5, and real big OCP mezzanine card with real big aluminum heat sinks and a big Microsoft sticker on top. A couple visitors commented on the way the heat piping for those Genoa CPUs was done which I guess is unusual; rather than have a giant copper block on top of each socket, heat pipes connect the socket to massive aluminum heat sinks that are closer to the chassis inlets. In retrospect it makes sense; Genoa has a whopping twelve DDR5 DIMMs per socket which leaves little extra room for heat sinks, and these 88+ core sockets have a staggering thermal design power.Another exotic piece of hardware on display was an \"ND MI200 v4\" server:It's logically similar to Azure's \"ND A100 v4\" server platform with two CPU sockets, eight SXM4 GPU sockets, eight 200G HDR InfiniBand HCAs, and a bunch of M.2 NVMes. But this specific server has eight MI200 GPUs on a common OAM baseboard and uses Infinity Fabric for GPU-to-GPU communication. I've never seen an OAM-socketed anything in real life before, much less eight of them on a baseboard, so I thought this was pretty great to see in the flesh.The ND A100 v4 platform was also on display and looked very similar-but-different with its eight A100 GPUs and HGX baseboard:And unlike the MI200 variant, the general public can run on these nodes.I'm not sure what more I'm allowed to say, but my colleague Karl made a nice, quick video that runs through the entire Microsoft booth that's worth a watch, and more details can be had by contacting me or your favorite Microsoft account team privately.Of course, the hardware bar was just a way to lure people into the booth so I could achieve my real goal: meeting new folks. As I wrote before, one of my biggest realizations at SC this year is how generally confused people are about what HPC in the cloud really means--both people who come from traditional on-prem HPC and people who come from traditional enterprisey cloud. I found myself surprising many of the people with whom I spoke on the show floor with factoids that I have taken for granted. For example,Linux is the most common OS on these HPC node types. While you probably(?) can run Windows if you want on this stuff, I think only a few niche markets do this.The usage model for an HPC cluster in the cloud can be the same as on-prem. You can have login nodes, Slurm, home directories, parallel file systems, and all that. Jobs don't have to be containerized or turned into a VM image.The InfiniBand coming out of these nodes is real InfiniBand with real OFED that supports real mpich/mvapich/OpenMPI. It's the same stuff as in on-prem supercomputers. And nodes are assembled into full-bisection fat tree InfiniBand clusters just like normal.There's no noisy neighbor problem on compute nodes because HPC node types aren't shared between users. When you run a VM on an HPC node, you get the whole thing. Just like on large supercomputers.There's no horrible loss of performance due to running in a VM. Virtualization extensions, PCIe passthrough, and SR-IOV bypass the hypervisor for most things. Inside your VM, you see real Zen cores and real Mellanox HCAs, not virtualized devices.My takeaway impression is that a lot of traditional HPC folks looked at the cloud five or ten years ago, had a sour experience, and haven't paid attention since. In those last five years, though, AI has changed the game. Massive demand for the latest CPUs and accelerators, funded by live-fast-die-young venture capital, has given cloud vendors tremendous financial incentive to catch up to on-prem levels of performance efficiency for AI workloads. And it just so happens that infrastructure that's good for AI is also good for traditional modeling and simulation.SCinet!One of the unexpected highlights of my SC this year arose from a chance encounter with a former coworker from NERSC, Ron Kumar, who gave me a whirlwind tour of SCinet.I have to confess great ignorance around SCinet in general; I always saw it was a weird technological proof of concept that the strange networking people at work would go off and do in the weeks leading up to the actual conference. I knew they did some impressive wide-area transfer demos (like the petabyte-in-a-day demo at SC'16), but I didn't really get the significance.So what is SCinet? It's this yellow bundle of cables dangling from the ceiling.&lt;p&gt;The yellow cables are 144-core fiber trunks that bring over a terabit per second of bandwidth into the convention center from the Internet via the national research backbones like ESnet and Internet2 and distribute many terabits per second of capacity throughout the SC conference venue. For comparison, most HPC centers in the US only have a tenth of SCinet’s wide-area bandwidth at best since 400G infrastructure is still rolling out.&lt;/p&gt;Most attendees may be familiar with the row of expensive-looking networking racks behind a glass wall towards the back of the expo which is where those yellow cables dangling from the ceiling end. Here's a photo from inside that glass wall:What I didn't realize is that if you go around to the back of the giant walled area behind this glass display, there's a security checkpoint that gates entry into a massive network operations center (NOC) full of laptops, spools of fiber, meeting rooms, and busily working teams in charge of all the lower layers of the networking stack.The process to get into the NOC involves an escort and being tagged in with a tamper-proof wristband, and I learned on the tour that there's millions upon millions of dollars worth of high-end networking equipment in the racks shown above. If you look closely, you can see a security camera at the end of the aisle that speaks to this; that camera was one of many.Behind the pretty public-facing side of the SCinet racks is a mess of fiber and cables:I guess if you have to tear all this down after just a few weeks, there's no point in investing days in dressing it all up nicely! I particularly enjoyed the fiber panels in the third rack that appear to be affixed to the rack post with shoe laces.This year, SCinet did do a neat proof-of-concept where they demonstrated three 400G routers from three vendors (Juniper, Arista, and Cisco?) all talking the same protocol to handle what I assume is the core routing for everything in the convention center:I wish I remembered exactly what was going on here, but I know enough about networking to know that, despite there being standard protocols for coordinating between networking gear, each vendor does their own implementation that is rarely easy to get interoperability from. If anyone out there knows the details of this achievement, please let me know so I can explain this a little better!In addition to networking nerd-level demonstrations, SCinet also serves up all the wifi across the convention center. That is why there were tripods with access points scattered around, and why astute attendees may have noticed janky networking equipment scattered around that looked like this:Again, I get it: for a network infrastructure that's only going to last a week, I don't think it's a good use of anyone's time or money to nicely dress all the networking.One last factoid I didn't know until this year was that exhibitors can request 100 Gb/s network drops into their individual booths for demos (or downloading the latest version of a PowerPoint presentation really fast). The end result of supporting both a vast wifi network and 100G fiber across the show floor is that there was a lot of fiber going into the single row of SCinet equipment:Finally, when I posted some of these photos online during the conference, my colleague Bilel was kind enough to post a slide from the SC22 opening presentation that had the speeds and feeds of what I had toured:Candy Culhane shared Scinet facts #SC22 #HPC5.01 Tb/s of WAN capacity$70M in HW &amp; SW, &amp; services provided by 29 SCinet contrib.175 volunteers from 80 vol. organiz.&gt; 450 wireless deployed29 network research exhibition proposals11.7 miles of fiber 2384 fiber patch https://t.co/JtPhjVHZJd pic.twitter.com/kwGl5Ydqp5— Bilel Hadri (@mnoukhiya) November 16, 2022If you know anyone involved with SCinet, I highly recommend seeing if you can get a tour at the next SC. Even as a relative networking novice, I walked away with a much greater appreciation for the annual achievement of building SCinet. And who knows? Once I get bored of this whole storage thing, maybe I'll try getting into high-performance networking.Composability panelThis year I was invited to participate in a panel titled \"Smackdown! Does HPC Need Composability Now?\" moderated by Addison Snell and Dan Olds from Intersect360 Research. This panel was...different. Unlike the traditional SC panel where panelists take turns presenting slides and saying erudite things, this panel had two teams of panelists. And my team only had one slide to present:The ground rules included \"personal attacks are allowed,\" and needless to say, the panel was about equal parts entertainment and technical discourse. That's not a bad thing, though.Addison and Dan did a phenomenal job of pulling their respective teams together and leading discussion in a format that both brought forward the key pros and cons of composability in HPC while poking fun at the thinly veiled, ego-driven personalities that often make up these sorts of panels. Rather than politely dancing around issues like sacrificing memory bandwidth by putting accelerators at the far end of a PCIe bus or gaining higher utilization by allowing users to mix and match CPU, NICs, and GPUs, us panelists were free to shoot straight (or perhaps a bit hyperbolically) and call each other out on our hidden agendas.I hope it goes without saying that all us panelists were in on the format and don't actually think people on the other side are dumb. By wrapping technical arguments in snarky comments, we could keep the level of discussion accessible to a wide audience, drive home the key points from both sides, and ensure that we weren't losing audience members who don't care about the PhD-level details as much as they want to hear what their peers are thinking about this exciting new space. I got some feedback afterwards that I didn't seem to hold back, so if anyone did take anything I said seriously, I am very sorry!On a technical level, what was the outcome?It turns out that there was about a 60/40 split between people who felt composability wasn't required yet and those who felt it was after both sides argued their case. Even among panelists, many of us were a lot less convinced about our respective positions than we let on during the panel itself. I got a chuckle when I realized that I wasn't the only one who, when invited to be on the panel, asked \"what side do you want me to argue?\" I honestly could have gone either way because the dust has not yet settled. Dan Stanzione, director of TACC, gave the truest answer to the question of \"will composability help HPC\" up front--\"it depends.\" Maybe this is a growth opportunity, or maybe it's a lukewarm reception.Either way, composable technologies are hitting the market regardless of whether you think they'll be useful or not.  AMD Genoa supports CXL 1.1 with extensions for memory pooling, Samsung has memory-semantic SSDs, and everyone and their mother is working on photonics to get higher bandwidths and lower latencies over longer distances. This makes it easier for people to dip their toes in the water to see if composability makes sense, and I think that's what a lot of people will wind up doing in the coming years.Customer meetingsUnlike in years past, my SC experience this year was dominated by customer meetings. I've been on the customer side of the table plenty of times, but I was surprised to find that it was actually more fun to be on the vendor side for a change. I'm part salesman at heart, so I found it personally gratifying to end a meeting with people nodding along rather than scratching their heads. I learned as a customer that it's very easy for vendors to go way off the rails and waste everyone's time, so I was grateful to have avoided the awkward confusion that punctuates those kinds of meetings. I also went into the week worrying that I'd be sitting in the same room, hearing the same pitch and the same jokes, and answering the same questions all week. Thankfully, I work with some great field, business, and product teams who set up interesting conversations rather than rote recitations of boring roadmap slides. Approaching the same topics from different angles helped me figure out how all the pieces of what I'm working on fit together to make a complete picture too; there weren't nearly as many opportunities to do this in the DOE world since the end-users of the HPC systems on which I worked aren't told anything until all the design decisions have already been made.A few personal notesThis SC was significant to me at a variety of levels; it was the first time I'd gotten on an airplane since February 2020, the first time I'd traveled since starting a new job at a new company, and the first time I'd met any of my new coworkers outside of the structure of a Teams call. During the pandemic I realized that getting out into the world and talking to people from all corners of HPC were my favorite part of my job. Not being able to go to events like SC and maintain that a sense of community involvement dramatically impacted my level of professional satisfaction for the last two years, so I'm glad I was able to finally go this year.Though customer meetings were a lot more fun than I expected them to be, I still felt bummed that I could spend so little time walking the expo, talking to folks, and attending all the BOFs normally on my must-attend list. Compounding this was my personal choice to not dine indoors and consequently miss out on almost all other chances to catch up with old friends and colleagues. I also decided to leave SC a day earlier than I usually do to reduce my risk of getting sick which didn't help either. There's never enough time at SC, but this year was particularly pressed.I say all this not to complain, but to say how much I appreciated the people who went out of their way to come accost me during the precious few hours I actually had on the exhibit floor. Some I'd not seen since SC'19, and some I'd never actually met since we only started working together mid-pandemic. The conference is busy for everyone, so giving me a slice of your time was very meaningful. That sense of community membership is why I go to SC, it's why I still work in this business, and it's why I try to contribute whatever I can to whomever wants it whether it be a student, engineer, salesperson, or marketer.",
            "content_html": "<p>The biggest annual conference in HPC, the <a href=\"https://sc22.supercomputing.org\">SC conference</a>, was recently held in Dallas, Texas in its second hybrid incarnation since being all-remote for the pandemic. This year attracted over 11,000 attendees which is much closer to the pre-pandemic high of 14,000 than last year's 7,000, and judging from the crushed conference rooms and busy expo floor, it looks like SC is not that much worse for wear.</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>This year's conference quite different for me since I attended for my first time as a vendor, not a researcher or practitioner, and I spent most of my days behind closed doors talking to customers. I didn't get to attend any of the keynotes, BOFs, or panels to which I wasn't invited as a result, so I'm not really qualified to give an erudite summary of the conference or expo this year.</p><p>So instead, I'm just writing down what I remember in order that I remember it and not necessarily in a coherent narrative form. I'm sure I missed a lot (for example, mixed precision seemed big this year, and I heard Jack Dongarra gave a fantastic Turing Award talk) so I encourage others to write their own recaps and share with the community!<span></span></p><p></p><h2 style=\"text-align: left;\">High-level themes</h2><p>I actually started writing an SC'21 recap last year which I never posted, and re-reading the intro was funny--you'd think nothing has changed in the last year.</p><h3 style=\"text-align: left;\">The underwhelming</h3><p>The biggest deal appears to be that exascale is here, and it turns out that it's not that big of a deal. China let the air out of the tires by debuting their exascale systems at SC'21, and not only did they thumb their nose at Top500 by not submitting, they debuted by winning a Gordon Bell prize instead. The first US exascale system, Frontier, was debuted at ISC this year leaving its showing at SC a bit deflated too. <a href=\"https://www.hpcwire.com/2022/11/17/2022-gordon-bell-prize-goes-to-plasma-accelerator-research/\">Frontier was featured in the Gordon Bell prize-winning paper</a> this year, but that work required the use of four Top-10 systems, not just Frontier, painting the reality that one giant computer rarely stands on its own when it comes to advancing science.</p><p>This isn't to say that deploying exascale systems isn't a noteworthy feat and worth commendation, but I felt like the hype over the last five years treated the achievement like an end state instead of a milestone. And now that we've passed the milestone, the community is grasping to figure out what comes next. So what <i>is</i> next?</p><p><b>Quantum</b> had a strong and growing presence at SC, as it has for the last few years. But the conclusion of the panel \"<a href=\"https://www.hpcwire.com/2022/11/19/quantum-are-we-there-or-close-yet-no-says-the-panel/\">Quantum Computing: A Future for HPC Acceleration</a>\" was that no, it's not close to being ready.</p><p><b>Disaggregation and composability</b> was another theme with growing momentum. And like quantum, there was a panel asking the same question: \"<a href=\"https://www.hpcwire.com/off-the-wire/informal-poll-of-sc22-attendees-suggests-a-bright-future-for-composability/\">Does HPC need composability now?</a>\" The answer, again, was no, not yet. More on that below.</p><p>What about <b>RISC-V</b>? Surely that will revolutionize the field. As it turns out, the answer there is also that <a href=\"https://www.hpcwire.com/2022/11/18/risc-v-is-far-from-being-an-alternative-to-x86-and-arm-in-hpc/\">RISC-V is not ready to do anything useful for HPC yet</a>.</p><p>The list goes on of technologies and trends that people are trying to boost now that exascale is \"solved.\" The reality, I think, is that \"exascale\" will take years to actually mature since it appears to have a ton of technical debt that accumulated during the race to be first. US Exascale rests on the shoulders of AMD and Intel, two companies whose software stacks have not caught up to the market leader, so there will be a lot of thrashing around as development practices and optimization settle out around these systems.</p><p>Struggling with code porting is not very exciting to computer science Ph.D.s, so I expect future SCs to mirror this one and bifurcate into two distinct tracks: those struggling to identify the next big thing in the research space, and those struggling to use the systems that were rushed to deployment.</p><h3 style=\"text-align: left;\">The unexpected</h3><p>My SC experience was very biased since I didn't get out much, but two related themes kept popping up across different meetings and the sessions I did attend.</p><p><b>Power efficiency is serious business now</b>. It used to seem like people talked about the need for energy-efficient HPC in an abstract sense while continuing to jam more power into every rack without changing their approach to system design, facilities, and deployment models. That has hit a hard wall with energy prices soaring in Europe, though. The financial impacts of power-inefficient supercomputing have gone from a one-time capex cost to an ongoing opex cost that is putting many HPC facilities on an unsustainable cost trajectory. Even sites that aren't doing new deployments are facing sudden, sharp increases in their costs, and nobody has good answers about how they will keep the lights on.</p><p><b>Cloud HPC is confusing</b>. With only <a href=\"https://www.nextplatform.com/2022/11/08/hpc-follows-the-enterprise-into-the-cloud/\">15% of total HPC dollars winding up in the cloud</a>, it's little surprise that most HPC folks are only peripherally aware of what HPC in the cloud really means. Worse yet, a subset of those folks are actively hostile towards the idea of running HPC workloads in the cloud. I spoke with my colleagues from all three major cloud service providers as well as my colleagues in DOE, NSF, and education throughout the week, and everyone painted this same general picture.</p><p>There seems to be a mismatch between the expectations of on-prem HPC folks and cloud HPC folks. For example, I was asked why Windows doesn't support OpenMP very well, and after a bit of digging, I realized that the question really wasn't about using OpenMP on Windows as much as it was about using OpenMP in the cloud. There was a latent assumption that \"HPC in Microsoft's cloud\" must mean \"HPC on Windows\" which, for the record, is false--I don't even know how to use Windows anymore. Similarly, people decried the performance impacts of sharing HPC nodes with others in the cloud (they are not shared), overheads of virtualizing InfiniBand or GPUs (everyone uses PCIe passthrough or SR-IOV for HPC nodes), and other misconceptions.</p><p>This isn't to say that cloud people aren't confused too; I heard stories about conversations that went sideways because a cloud folks (not from my employer, thankfully!) didn’t realize that the requirements of a traditional gov/edu HPC facility couldn’t be neatly wrapped up into a single workload with a single solution, contrary to the case across many commercial AI shops. And both sides are struggling to find models for partnership and engagement that mirror the traditional relationship between places like a DOE or NSF facility and a company like Cray. HPC departments are used to buying supercomputers and parallel file systems, while cloud providers sell computing and storage as a <i>service</i>. The distinction may seem trivial at the surface, but there's a large divide that becomes evident once both sides start trying to drill into the details of what a partnership would look like.</p><h2 style=\"text-align: left;\">Parallel I/O in Practice Tutorial</h2><p>This was my fifth year contributing to the Parallel I/O in Practice Tutorial with my colleagues at Argonne and Google, and it was our first time doing it in-person since 2019. It felt really good to be back in front of people to opine about the perils of POSIX and the greatness of the <a href=\"https://www.mcs.anl.gov/research/projects/darshan/\">Darshan I/O profiling tool</a>, and this year I retired out the material I used to present on burst buffers (since DataWarp and Infinite Memory Engine have lost relevance in HPC) and the <a href=\"https://www.nersc.gov/tokio/\">TOKIO holistic I/O analysis framework</a> (since it is no longer funded/maintained). In their stead, I presented material on <a href=\"https://wiki.lustre.org/Lustre_User_Group_2022\">benchmarking with IOR and mdtest I debuted at LUG 2022 this year</a>.</p><p>I haven't gotten feedback yet on whether this change was a net positive one, but I think it went over well. Benchmarking I/O is really challenging if you don't understand how things like page cache really work in distributed systems, and walking through some benchmark examples concretizes a lot of abstract parallel file system concepts like locking and striping. And since benchmarking is a rabbit hole of arbitrary complexity, ending the tutorial with advanced benchmarking topics turned out to be a nice way to add buffer to the end of an eight-hour stretch of carefully timed presentations. It's very easy to skip over the nuances of analyzing mdtest outputs if attendees have a lot of questions about more important things at the end of the day.</p><p>The most surprising observation of the tutorial is how many attendees aren't using MPI anymore. We got a lot of questions last year about task-oriented I/O, and this year had some great questions about trying to understand or tune the I/O performed by Python-based analytics frameworks. We decided to add support for <a href=\"https://www.mcs.anl.gov/research/projects/darshan/2019/12/11/new-experimental-version-of-darshan-available-for-instrumenting-non-mpi-applications/\">Darshan to profile non-MPI applications back in 2019</a> which is now paying dividends by ensuring it is a relevant tool for these new analytics and AI workloads, and we'll probably have to give more attention to optimizing these workloads' I/O in the future.</p><h2 style=\"text-align: left;\">DAOS User Group</h2><p>Monday morning was cold and rainy--a perfect day to attend the <a href=\"https://daosio.atlassian.net/wiki/spaces/DC/pages/11248861216/DUG22\">2022 DAOS User Group</a> which was held off-site at the Fairmont Hotel.</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>Whether you particularly care about DAOS or not, the cross-community HPC I/O brain trust is guaranteed to be in attendance, and this year did not disappoint. In addition to the expected stakeholders from Intel and DOE, representatives from all three big CSPs were in attendance. Google Cloud, Seagate, and HPE/Cray were all on the agenda, painting a diversifying landscape of large HPC companies investing time into DAOS and the strength and willingness of the DAOS team to partner with all comers.</p><h3 style=\"text-align: left;\">Life after Optane</h3><p>The question that opened up the meeting, of course, was \"what is the future of DAOS since Intel cancelled Optane?\" Kelsey Prantis had the official statement (I'll replace the grainy photo once the DUG slides are online...):</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>The high-level project answer is that DAOS isn't going anywhere. Aurora, by virtue of still having Optane DIMMs, will not be affected, and DAOS will maintain support for Optane until Intel drops its last Optane DIMMs (Crow Pass for Sapphire Rapids) from support life sometime towards the end of this decade.</p><p>For new customers who aren't going to use Optane, the answer is \"<a href=\"https://daosio.atlassian.net/issues/?jql=labels%20%3D%20%22md_on_ssd%22\">Metadata on NVMe</a>,\" a development being codeveloped by Intel, HPE, and Google to implement a write-ahead log (WAL) and allow DAOS to use volatile DRAM instead of Optane. It will work like a file system journal in that a compact representation of writes will be committed to NVMe immediately after landing in DRAM, and then DAOS will asynchronously write back the properly serialized representation of that transaction after it is acknowledged. Johann Lombardi had a helpful cartoon that showed how this WAL will fit into DAOS:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>A key benefit of DAOS's implementation of this WAL is that it will be able to still service incoming writes while flushing old writes; although I don't fully grasp how this works, it is something enabled by the sophisticated I/O scheduler already implemented in DAOS.</p><p>The complete implementation isn't expected to be released until Spring 2024, but it appears to touch only a few components of DAOS and doesn't affect anything above the VOS layer of the DAOS server.</p><p>There was also mention of developing operability with new <a href=\"https://news.samsung.com/global/samsung-electronics-unveils-far-reaching-next-generation-memory-solutions-at-flash-memory-summit-2022\">CXL-attached memory-semantic SSDs</a> to keep the persistent memory capability of DAOS alive beyond Optane. I'm not sure if this would offer a performance benefit over the metadata-on-NVMe feature; early results show that metadata-on-NVMe actually delivers higher IOPS than Optane since the synchronous write path is much simpler without having to account for memory persistence. That said, I didn't really follow the full extent of options on the table for how DAOS metadata may work across different types of memory though.</p><h3 style=\"text-align: left;\">DAOS in the flesh at Argonne</h3><p>Kevin Harms presented an update on Aurora's massive 220 PB DAOS installation and laid out its configuration. There are 1,024 DAOS servers based on the Intel Coyote Pass server design, each sporting</p><p></p><ul style=\"text-align: left;\"><li>2x Intel Xeon 5320 (Ice Lake) sockets</li><li>2x DAOS engines (one per socket)</li><li>16x 32GB DDR4 DIMMs</li><li>16x 512GB Optane DIMMs (Persistent Memory 200)</li><li>16x 15.36 TB Samsung PM1733 NVMe SSDs</li><li>2x 200 Gb/s Slingshot NICs</li></ul><p>The total configuration is quoted at 220 PB usable, but Kevin pointed out that this assumes that every object is erasure coded at 16+2. Unlike virtually every other storage system out there, though, users can choose the data protection for their individual objects when they create them, meaning this 220 PB capacity is an upper limit to what users can do. Users with very hot, read-only objects may choose to replicate instead of erasure code, while others who are capacity-constrained may choose to erasure code everything at 16+2 at the cost of latency and IOPS. This flexibility is really powerful for users since they can tailor their object layout (\"<a href=\"https://www.intel.com/content/www/us/en/developer/articles/technical/understanding-data-redundancy-and-sharding-in-daos.html\">object class</a>\" in DAOS parlance) to match the needs of their workload.</p><p>Argonne will be slicing up this DAOS system by giving each scientific project its own DAOS pool, and each pool will be assigned to only 80% of the available DAOS servers by default. This seems like a nice way of providing most of the storage system performance to every user, but offering more freedom to work around bad hardware, bad users, and other performance problems that plague file systems like Lustre that distribute everything across every single server equally.</p><p>Finally, I noticed that Aurora will be using Samsung SSDs, not the Intel (now Solidigm) QLC NAND that appeared in all the DAOS slides floating around two years ago. I'm not sure what happened there, but the move from Solidigm QLC to Samsung TLC couldn't have been cheap.</p><h3 style=\"text-align: left;\">New features and contributions</h3><p>DAOS is starting to pick up some truly valuable features that are being developed and contributed by third parties. Of note, croit has contributed a feature which allows DAOS to serve up NVMe over Fabrics targets, and Seagate contributed an S3 gateway for DAOS. Along with the DFS file system interface, DAOS now offers the trifecta of standard object, block, and file services just like Ceph. Unlike Ceph though, performance on DAOS is a first-class citizen. While croit made it clear that the NVMeoF support still has a ways to go to improve the way it does thread pooling and provides resilience, they showed 1.4 million IOPS from a single storage client using TCP over Ethernet with minimal client-side overhead.</p><p>Intel is also developing multitenant support for DFUSE, allowing a single compute node to share a DAOS mount and let permissions be enforced through UID/GID just like a regular file system. Before this update, the FUSE-based nature of DAOS allowed any unprivileged user to mount their container (good), but only one FUSE agent could be alive on a single node at a time (not good) which prevented multiple users sharing a node from both mounting their own containers.</p><p>DAOS also has some longer-term enhancements that I thought were interesting:</p><p></p><ul style=\"text-align: left;\"><li>expanding the range of POSIX calls supported by DAOS's intercept library to include metadata calls and memory-mapped I/O using <a href=\"https://docs.kernel.org/admin-guide/mm/userfaultfd.html\">userfaultfd</a></li><li>implementing collaborative caching - essentially reimplementing the Linux kernel page cache in userspace so that multiple processes can share cached DAOS pages</li><li>supporting a computational storage paradigm by enabling offload of <a href=\"https://github.com/rlane/ubpf\">userspace eBPF scripts</a> to DAOS servers</li></ul><h3 style=\"text-align: left;\">DAOS in a larger data center ecosystem</h3><p>Dean Hildebrand from Google Cloud then gave an overview of Google's efforts in bringing DAOS into the cloud. He had some nice performance graphs and I'll link the full presentation here once it's uploaded (it's worth a watch), but the part I found the most insightful was how they are trying to decide where a technology like DAOS fits in the larger cloud storage ecosystem. He outlined two different ways DAOS could work in GCP:</p><p></p><ol style=\"text-align: left;\"><li><b>Caching</b>: Google Cloud Storage (GCS) is the point of truth and DAOS is a cache</li><li><b>Tiering</b>: DAOS is a point of truth, and GCS is an archive</li></ol><p></p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>He said they were leaning towards the caching model where data only lives ephemerally in DAOS, and personally, I think this is the right move since DAOS in the cloud is not resilient without Optane. However, this choice reflects a much larger tension in cloud storage for HPC:</p><p></p><ol style=\"text-align: left;\"><li>The centerpiece of every cloud's data story is a scalable, low-cost, low-performance object store which is analogous to what on-prem HPC would call campaign, community, or project storage.</li><li>HPC demands higher performance than what these object stores can generally deliver though.</li></ol><div>To bridge the gap between these two truths, auxiliary services must bolt on to the object layer and provide higher performance, at a higher cost, for the duration of I/O-intensive HPC jobs. Some choose to provide true tiering from object into a resilient layer of flash (like <a href=\"https://aws.amazon.com/fsx/lustre/\">FSx Lustre</a> and <a href=\"https://docs.weka.io/overview/data-storage\">Weka</a> do), while others project the contents of the object through a high-performance caching layer (like <a href=\"https://azure.microsoft.com/en-us/products/hpc-cache/#overview\">HPC Cache</a> and <a href=\"https://aws.amazon.com/blogs/aws/amazon-file-cache-a-high-performance-cache-on-aws-for-your-on-premises-file-systems/\">File Cache</a>) and are never meant to persistently hold data.</div><p></p><p>This isn't rocket science, but I never thought deeply about the two models since campaign/community/project storage in on-prem HPC is usually fast enough to avoid needing caches or fine-grained tiering capabilities.</p><p>John Bent also had a thought-provoking presentation about how Seagate's now-\"deprioritized\" CORTX object store, which once <a href=\"https://blog.seagate.com/enterprises/seagate-and-sage-project-innovate-to-boost-hpc-and-big-data-community/\">competed with DAOS as Mero</a>, contains ideas that can complement DAOS:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>Whereas DAOS delivers high performance using NVMe, CORTX delivers great economics using HDDs, and their strengths are complementary to each other. While I don't fully grasp how a tiered (or caching!) system comprised of DAOS and CORTX could be implemented, John rightly pointed out that the same level of space efficiency can deliver higher data protection if multi-level erasure coding is used to stripe across durable block storage. His specific example was erasure coding at 8+1 across servers and 10+1 within servers to deliver both high efficiency and high durability. This could map to something like running DAOS atop something like CORVAULT, but I don't think all the necessary pieces are in place to realize such a harmonious coexistence yet.</p><p>Of course, completely tossing Reed-Solomon for something more sophisticated (like VAST does with its locally decodable 150+4 scheme) obviates the need for multilevel erasure entirely. But DAOS has not gone down that route yet.</p><p>And as with every talk John gives, there were lots of other interesting nuggets scattered throughout his presentation. Two of my favorites were:</p><p></p><ul style=\"text-align: left;\"><li>A slide that pointed out that, when you buy something like Ceph as an appliance, you may be spending only 25% of the total cost on storage media and the rest is infrastructure, service, and support. This struck me as a bit on the low end, but some enterprisey NAS and midrange parallel file system appliances can go this low. Spending 60% to 90% on media is a lot nicer for the buyer (and companies like Seagate) if you can buy at scale or eschew the white-glove support, and John suggested that it's up to companies like Seagate to fix the software issues that require customers to pay for white-glove support in the first place.  After all, the less someone spends on support and licenses, the more they can spend on Seagate hard drives.</li><li>John's final slide pointed out that object stores were originally designed to get around the limitations of POSIX file systems, but as they've evolved over the last decade, they're starting to look a lot like file systems anyway since they require strong consistency, hierarchical namespaces, and familiar file semantics. Has all the work put into developing super-fast object stores like DAOS over the last ten years really just brought us back full circle to parallel file systems?  Companies like VAST and Weka have shown that <a href=\"https://www.nextplatform.com/2017/09/11/whats-bad-posix-io/\">maybe POSIX isn't as bad as the research community (myself included!) have claimed it to be</a>; it was really just low-performance implementations that nobody wanted.</li></ul><div>Once John's talk is uploaded to the DUG 2022 website, I'll link it here.  Like Dean Hildebrand's talk, it is well worth watching (but for wildly different reasons!)</div><p></p><p></p><p></p><h2 style=\"text-align: left;\">PDSW 2022</h2><p>I had to duck out of the DAOS User Group early to run (through the rain) to the 7th International Parallel Data Systems Workshop (PDSW 2022) on Monday afternoon.</p><p></p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p><br />Much to everyone’s surprise, PDSW was only given a half day this year and everything felt a little compressed as a result. The organizers kept the work-in-progress (WIP) sessions which can often be an interesting peek into what students are pursuing, but little A/V problems and the unforgiving schedule probably did a disservice to the up-and-comers who use the WIP track to lay the groundwork for future full-length papers. Hopefully SC’23 restores PDSW to its original full-day status.&lt;p&gt;&lt;/p&gt;</p><h3 style=\"text-align: left;\">Splinters keynote from Arif Merchant at Google</h3><p>The keynote presentation was given by Arif Merchant from Google about Splinters, the framework that Google Cloud uses to sample I/Os in a scalable way. The challenge they face is that it's impossible to trace and store every single I/O that hits Google's storage servers (D servers), but having an understanding of I/O patterns is essential for characterizing workload I/O behavior and planning for future infrastructure. In fact, this problem is so important that Google isn't the only cloud that's solved it!</p><p>A lot of what Arif talked about is very similar to how Azure does its I/O tracing under the hood. I suppose it should not be surprise that there are only so many ways to solve the challenge of sampling individual IOPS in a way that fairly represents the aggregate workload of a huge distributed storage system. One really smart thing Splinters does that I liked was sample along two different dimensions: not only do they evenly sample across all IOPS at a fixed rate (the obvious thing), but they also sample across files at a fixed rate. In this latter case of per-file sampling, they take a tiny fraction of files and capture every I/O for that file to get a complete picture of how individual files are being accessed.</p><p>This file sampling fills the huge gap that exists when randomly sampling IOPS alone. Because different I/Os have different \"costs\" (for example, reading a 1 MiB file using a single 1 MiB read op or 256x 4 KiB read ops are functionally equivalent to an application), randomly sampling ops introduces systematic biases that can be difficult to back out after the data has been sampled, subsampled, aggregated, and reduced. Splinters' approach lets you see the workload from two different angles (and biases) and answer a much larger range of questions about what's really happening across thousands of storage servers.</p><p>That said, it was interesting to hear Arif describe how Splinters evolved out of a different internal Google project but wound up outliving it. Splinters is also similar to, but slightly different from, their <a href=\"https://research.google/pubs/pub36356/\">Dapper</a> infrastructure which also does scalable distributed system tracing. And he made overtures to <a href=\"https://research.google/pubs/pub41344/\">F1</a>, a scalable SQL database that is similar to (but not the same as) the SQL-like query interface that Splinters uses. I got the impression that new technologies come and go pretty quickly at Google, and there's a large appetite for creating new software systems outright rather than shoehorning an existing system into solving a new problem. I can't say one way is better than the other; I was just surprised at the contrast with my own experiences.</p><h3 style=\"text-align: left;\">Practical papers</h3><p>PDSW had a healthy combination of both very-researchy papers and applied research papers this year. I could only stick around for the applied papers, and two left an impression.</p><p>In the first, <a href=\"https://jeanlucabez.io\">Jean Luca Bez</a> presented <a href=\"https://github.com/hpc-io/drishti\">Drishti</a>, a tool that lives downstream of the Darshan I/O profiling library and finally does what the Darshan community has danced around for years--turning a Darshan log into an actionable set of recommendations on how to improve I/O performance. It does this by cataloguing a bunch of heuristics and using Darshan's new Python integrations to pore through a log and identify known-problematic I/O patterns. Like Jean Luca's <a href=\"https://dxt-explorer.readthedocs.io/en/latest/\">DXT Explorer tool</a>, Drishti has a slick user interface and greatly extends the usability and insights that can be pulled out of a Darshan log file. It probably won't win a Turing Award, but this sort of work is probably going to benefit scores of HPC end-users by making Darshan (and troubleshooting I/O problems) much more accessible to mere mortals for years to come.</p><p>Adrian Jackson also presented a very tidy <a href=\"https://arxiv.org/abs/2211.09162\">apples-to-apples comparison of DAOS and Lustre on the same hardware</a> using both a systems-level benchmark and an application-inspired, object-oriented data model benchmark. The specific bake-off of a new curiosity (DAOS) and the decades-old incumbent (Lustre) is probably interesting to storage nerds, but I think the real novelty of the work is in its exploration of some uncomfortable realities that the HPC I/O community will have to face in the coming years:</p><p></p><ul style=\"text-align: left;\"><li>Does \"slow memory\" (nonvolatile Optane or CXL-attached memory SSDs) give actual benefit to existing file systems (like Lustre), or is rethinking the entire storage stack (like DAOS did) really necessary to unlock the performance of new hardware?</li><li>Do applications need to rethink their approach to I/O to make use of post-POSIX storage systems like DAOS, or is performing I/O as you would on a file system (Lustre) on a post-POSIX storage system (DAOS) good enough?</li></ul><p>My take from the work is that, for simple I/O patterns like checkpoint/restart, you can get pretty far by just treating something like DAOS the same as you would a parallel file system:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><b><span style=\"font-size: x-small;\">Figure from Manubens et al, \"<a href=\"https://arxiv.org/abs/2211.09162\">Performance Comparison of DAOS and Lustre for Object Data Storage Approaches</a>.\"</span></b></div><p>But if you want your data at rest to have the same data model as how it's handled within the application, you really ought to use a storage system that supports data models that are more expressive than a stream of bytes (which is what POSIX files are).</p><p>The authors didn't do a perfect job of giving Lustre its fair shake since they chose to use (abuse) directories and files to represent their application's data model on-disk instead of developing an object-file model that file systems like Lustre handle a little better. But let's be real--HPC is full of applications that do the exact same thing and represent datasets on-disk using complex hierarchies of directories and files simply because that's the easiest way to map the application's representation of data into the standard file system model. In that sense, storage systems that represent rich data models in a high-performance way should be really valuable to naive applications that map in-memory data structures directly to files and directories.</p><p>Going back to John Bent's closing slide from his DAOS User Group talk, though, does any of this even matter since all answers lead back to parallel file systems? Maybe there's something to be learned about adding better back-door APIs that support more diverse data models than what POSIX file interfaces give us.</p><h2 style=\"text-align: left;\">The SC22 Expo</h2><p>The expo is my favorite part of SC because it's when I get to talk to people one-on-one and learn about corners of the HPC industry that I would've never otherwise sought out. Much to my dismay, though, I had very little time to walk the floor this year--so little that I didn't get any swag. If you want to read up on what interesting technology was being showcased, I strongly recommend reading <a href=\"https://www.servethehome.com/?s=sc22\">all the great content that Patrick Kennedy and his team at STH created covering the expo</a>.</p><p>That said, I did notice some curious trends about the show floor overall.</p><p>The NVIDIA booth was notably absent this year (though they shared booth space with partners), and many of the usual top vendors had significantly smaller presence on the expo floor. Just for fun, I compiled the top ten(ish) vendors by booth size:</p><p></p><ol style=\"text-align: left;\"><li>Weka.io (3,200 sqft)</li><li>VAST Data, Department of Energy, Penguin Computing, HPE, and Microsoft (2,500 sqft)</li><li>AWS (2,000 sqft)</li><li>Google and TACC (1,600 sqft)</li><li>Supermicro, AMD, Intel, Dell, NASA, and Indiana University (1,500 sqft)</li></ol><p>I think it's amazing to see all-flash storage companies at the top of the list alongside all of the Big 3 cloud service providers. I may be reading too much into this, but this may mean that the money behind SC is shifting towards companies playing in the cloud-based AI space instead of traditional big iron for simulation. Or perhaps it's a sign that most of the traditional HPC players are taking a hard look at the return they get on a big booth given the current economic climate and pulled back this year.</p><p>I did chat with a couple colleagues who completely opted out of a booth this year (for reference, <a href=\"https://hallerickson.ungerboeck.com/prod/app85.cshtml?AppCode=VFP&amp;OrgCode=34&amp;EvtID=5025&amp;CC=SC22SM\">SC'21</a> had 10% fewer exhibitor booths than <a href=\"https://hallerickson.ungerboeck.com/prod/app85.cshtml?AppCode=VFP&amp;OrgCode=34&amp;EvtID=5020&amp;CC=SC19\">SC'19</a>), and the reasoning was consistent: they found more value in having staff meet with customers privately or attend the technical sessions and engage with people organically. Combined with a bit of bad taste left over from SC's <a href=\"https://sc21.supercomputing.org/exhibits/exhibit-at-sc/\">high cost of hosting pandemic-era \"digital booths\"</a> despite low return (did anyone visit digital booths at SC'20 or SC'21?), I can see why some vendors may have chosen to skip the expo this year.</p><p>Whatever the reasons may be, I was a bit sad to see such a small presence from some of my favorites like IBM, Fujitsu, Atos, and NEC. Hopefully the SC Exhibits Committee (and the economy!) can find ways to bring back the pre-pandemic glory of the show floor.</p><p>The expo wasn't all doom and gloom though! Even though I couldn't make my complete rounds this year, there were a couple of highlights for me.</p><p></p><h3 style=\"text-align: left;\">VAST's masterful marketing</h3><p>Perhaps the splashiest vendor at SC was VAST Data who had a brilliant marketing presence. First was the giant Vastronaut mascot that was the centerpiece of their booth:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>A <a href=\"https://twitter.com/search?q=sc22%20vast&amp;f=live\">quick search of Twitter</a> shows just how many people seized the opportunity to take a selfie at their booth. I would love to know how they transported that thing to and from the conference, but whatever the cost, I'll bet it was worth it.</p><p>At the Grand Opening Gala on Monday, they also gave out delightfully tacky light-up cowboy hats that everyone seemed to be wearing:</p><blockquote class=\"twitter-tweet\"><p dir=\"ltr\" lang=\"en\">We were there! <a href=\"https://twitter.com/hashtag/sc22?src=hash&amp;ref_src=twsrc%5Etfw\">#sc22</a> <a href=\"https://twitter.com/hashtag/sc2022?src=hash&amp;ref_src=twsrc%5Etfw\">#sc2022</a> <a href=\"https://twitter.com/VAST_Data?ref_src=twsrc%5Etfw\">@VAST_Data</a> <a href=\"https://t.co/fWhuSgBfpL\">pic.twitter.com/fWhuSgBfpL</a></p>— ntnu-hpc (@ntnuhpc) <a href=\"https://twitter.com/ntnuhpc/status/1592330266932301829?ref_src=twsrc%5Etfw\">November 15, 2022</a></blockquote><p>The subtle genius of this was that not only did people wear them during the gala and the <a href=\"https://beowulfbash.com\">Flop Gun-themed Beowulf Bash 2022 party</a> later that night, but they had to wear them on their plane rides home since they were so inconveniently bulky. Proof in point, my wife (who doesn't work in tech) sent me this text message to confirm that she was waiting for me at the right luggage carousel at San Francisco Airport:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>I wonder how many innocent bystanders, traveling home for Thanksgiving on Thursday or Friday, saw the shiny cowboy hats at airports around the country and wondered what VAST was.</p><p>The icing on the cake was VAST's CEO, Renen Hallak, parading around in an unmissable Chuck McGill-style space suit all week, clearly not taking himself too seriously and painting VAST as a work hard/play hard kind of company. Now, do flashy space suits and blinking cowboy hats alone mean VAST has a great product? I can't say<sup>**</sup>. But marketing is an art that I appreciate, and VAST hit some great notes this year.</p><p style=\"font-size: xx-small;\"><sup>**</sup> (Seriously, I'm not sure I wouldn't get in trouble for opining about another company here.)</p><h3 style=\"text-align: left;\">The Microsoft hardware bar</h3><p>The only booth where I spent any appreciable time this year was my own employer's. I personally love booth duty and accosting strangers on the show floor, especially if there's something interesting at the booth to jumpstart a conversation. When I worked at SDSC it was a <a href=\"https://www.sdsc.edu/News%20Items/PR111213_meteor.html\">Raspberry Pi cluster</a>, and at the Microsoft booth this year it was the \"hardware bar.\"</p><p>In addition to the customary booth presentations with giveaways, swag desk, seating area, and a fun caricature artist, the physical servers that underpin the HPC nodes in Azure were on display. <a href=\"https://www.opencompute.org/wiki/Server/ProjectOlympus\">Microsoft contributes its hardware platform designs to the Open Compute Project</a> so the physical hardware that runs in Azure data centers isn't entirely mysterious. Still, every cloud has its hardware secrets, so I was surprised to see these servers laid bare.</p><p>The newest HPC node type (dubbed <a href=\"https://learn.microsoft.com/en-us/azure/virtual-machines/hbv4-series\">HBv4</a>) on display was a node powered by AMD's Genoa processors just announced a few days earlier:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>This wasn't a display model, either; it had real DDR5 DRAM, a real NDR InfiniBand HCA, real PCIe Gen5, and real big OCP mezzanine card with real big aluminum heat sinks and a big Microsoft sticker on top. A couple visitors commented on the way the heat piping for those Genoa CPUs was done which I guess is unusual; rather than have a giant copper block on top of each socket, heat pipes connect the socket to massive aluminum heat sinks that are closer to the chassis inlets. In retrospect it makes sense; Genoa has a whopping twelve DDR5 DIMMs per socket which leaves little extra room for heat sinks, and these 88+ core sockets have a staggering thermal design power.</p><p>Another exotic piece of hardware on display was an \"ND MI200 v4\" server:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>It's logically similar to Azure's \"<a href=\"https://learn.microsoft.com/en-us/azure/virtual-machines/nda100-v4-series\">ND A100 v4</a>\" server platform with two CPU sockets, eight SXM4 GPU sockets, eight 200G HDR InfiniBand HCAs, and a bunch of M.2 NVMes. But this specific server has eight MI200 GPUs on a common OAM baseboard and uses Infinity Fabric for GPU-to-GPU communication. I've never seen an OAM-socketed anything in real life before, much less eight of them on a baseboard, so I thought this was pretty great to see in the flesh.</p><p>The ND A100 v4 platform was also on display and looked very similar-but-different with its eight A100 GPUs and HGX baseboard:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>And unlike the MI200 variant, the general public can run on these nodes.</p><p>I'm not sure what more I'm allowed to say, but my colleague Karl made a nice, <a href=\"https://twitter.com/KarlPodesta/status/1593627537330126851?s=20&amp;t=uthjeb7YYmTZWRVWaF4XUA\">quick video that runs through the entire Microsoft booth</a> that's worth a watch, and more details can be had by contacting me or your favorite Microsoft account team privately.</p><p>Of course, the hardware bar was just a way to lure people into the booth so I could achieve my real goal: meeting new folks. As I wrote before, one of my biggest realizations at SC this year is how generally confused people are about what HPC in the cloud really means--both people who come from traditional on-prem HPC and people who come from traditional enterprisey cloud. I found myself surprising many of the people with whom I spoke on the show floor with factoids that I have taken for granted. For example,</p><p></p><ul style=\"text-align: left;\"><li>Linux is the most common OS on these HPC node types. While you probably(?) can run Windows if you want on this stuff, I think only a few niche markets do this.</li><li>The usage model for an HPC cluster in the cloud can be the same as on-prem. You can have login nodes, Slurm, home directories, parallel file systems, and all that. Jobs don't have to be containerized or turned into a VM image.</li><li>The InfiniBand coming out of these nodes is real InfiniBand with real OFED that supports real mpich/mvapich/OpenMPI. It's the same stuff as in on-prem supercomputers. And nodes are assembled into <a href=\"https://learn.microsoft.com/en-us/azure/virtual-machines/sizes-hpc\">full-bisection fat tree InfiniBand</a> clusters just like normal.</li><li>There's no noisy neighbor problem on compute nodes because HPC node types aren't shared between users. When you run a VM on an HPC node, you get the whole thing. Just like on large supercomputers.</li><li>There's no horrible loss of performance due to running in a VM. Virtualization extensions, PCIe passthrough, and SR-IOV bypass the hypervisor for most things. Inside your VM, you see real Zen cores and real Mellanox HCAs, not virtualized devices.</li></ul><p>My takeaway impression is that a lot of traditional HPC folks looked at the cloud five or ten years ago, had a sour experience, and haven't paid attention since. In those last five years, though, AI has changed the game. Massive demand for the latest CPUs and accelerators, funded by live-fast-die-young venture capital, has given cloud vendors tremendous financial incentive to catch up to on-prem levels of performance efficiency for AI workloads. And it just so happens that infrastructure that's good for AI is also good for traditional modeling and simulation.</p><h2 style=\"text-align: left;\">SCinet!</h2><p>One of the unexpected highlights of my SC this year arose from a chance encounter with a former coworker from NERSC, <a href=\"https://www.nersc.gov/about/nersc-staff/networking-security/ronal-kumar/\">Ron Kumar</a>, who gave me a whirlwind tour of SCinet.</p><p>I have to confess great ignorance around SCinet in general; I always saw it was a weird technological proof of concept that the strange networking people at work would go off and do in the weeks leading up to the actual conference. I knew they did some impressive wide-area transfer demos (like the <a href=\"https://scinet.supercomputing.org/community/documents/43/sc17-Kettimuthu-transferring_1petabyte_per_day.pdf\">petabyte-in-a-day demo at SC'16</a>), but I didn't really get the significance.</p><p>So what is SCinet? It's this yellow bundle of cables dangling from the ceiling.</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p><br />&lt;p&gt;The yellow cables are 144-core fiber trunks that bring over a terabit per second of bandwidth into the convention center from the Internet via the national research backbones like ESnet and Internet2 and distribute many terabits per second of capacity throughout the SC conference venue. For comparison, most HPC centers in the US only have a tenth of SCinet’s wide-area bandwidth at best since 400G infrastructure is still rolling out.&lt;/p&gt;</p><p>Most attendees may be familiar with the row of expensive-looking networking racks behind a glass wall towards the back of the expo which is where those yellow cables dangling from the ceiling end. Here's a photo from inside that glass wall:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>What I didn't realize is that if you go around to the back of the giant walled area behind this glass display, there's a security checkpoint that gates entry into a massive network operations center (NOC) full of laptops, spools of fiber, meeting rooms, and busily working teams in charge of all the lower layers of the networking stack.</p><p>The process to get into the NOC involves an escort and being tagged in with a tamper-proof wristband, and I learned on the tour that there's millions upon millions of dollars worth of high-end networking equipment in the racks shown above. If you look closely, you can see a security camera at the end of the aisle that speaks to this; that camera was one of many.</p><p>Behind the pretty public-facing side of the SCinet racks is a mess of fiber and cables:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>I guess if you have to tear all this down after just a few weeks, there's no point in investing days in dressing it all up nicely! I particularly enjoyed the fiber panels in the third rack that appear to be affixed to the rack post with shoe laces.</p><p>This year, SCinet did do a neat proof-of-concept where they demonstrated three 400G routers from three vendors (Juniper, Arista, and Cisco?) all talking the same protocol to handle what I assume is the core routing for everything in the convention center:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>I wish I remembered exactly what was going on here, but I know enough about networking to know that, despite there being standard protocols for coordinating between networking gear, each vendor does their own implementation that is rarely easy to get interoperability from. If anyone out there knows the details of this achievement, please let me know so I can explain this a little better!</p><p>In addition to networking nerd-level demonstrations, SCinet also serves up all the wifi across the convention center. That is why there were tripods with access points scattered around, and why astute attendees may have noticed janky networking equipment scattered around that looked like this:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>Again, I get it: for a network infrastructure that's only going to last a week, I don't think it's a good use of anyone's time or money to nicely dress all the networking.</p><p>One last factoid I didn't know until this year was that exhibitors can request 100 Gb/s network drops into their individual booths for demos (or downloading the latest version of a PowerPoint presentation <i>really fast</i>). The end result of supporting both a vast wifi network and 100G fiber across the show floor is that there was a <u>lot</u> of fiber going into the single row of SCinet equipment:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>Finally, when I <a href=\"https://twitter.com/glennklockwood/status/1592725187015114752?s=61&amp;t=1c4Kbx75SpTJhCruzuy0Ng\">posted some of these photos online</a> during the conference, my colleague Bilel was kind enough to post a slide from the SC22 opening presentation that had the speeds and feeds of what I had toured:</p><blockquote class=\"twitter-tweet\"><p dir=\"ltr\" lang=\"en\">Candy Culhane shared Scinet facts <a href=\"https://twitter.com/hashtag/SC22?src=hash&amp;ref_src=twsrc%5Etfw\">#SC22</a> <a href=\"https://twitter.com/hashtag/HPC?src=hash&amp;ref_src=twsrc%5Etfw\">#HPC</a><br /><br />5.01 Tb/s of WAN capacity<br />$70M in HW &amp; SW, &amp; services provided by 29 SCinet contrib.<br />175 volunteers from 80 vol. organiz.<br />&gt; 450 wireless deployed<br />29 network research exhibition proposals<br />11.7 miles of fiber <br />2384 fiber patch <a href=\"https://t.co/JtPhjVHZJd\">https://t.co/JtPhjVHZJd</a> <a href=\"https://t.co/kwGl5Ydqp5\">pic.twitter.com/kwGl5Ydqp5</a></p>— Bilel Hadri (@mnoukhiya) <a href=\"https://twitter.com/mnoukhiya/status/1592737463617089536?ref_src=twsrc%5Etfw\">November 16, 2022</a></blockquote><p>If you know anyone involved with SCinet, I highly recommend seeing if you can get a tour at the next SC. Even as a relative networking novice, I walked away with a much greater appreciation for the annual achievement of building SCinet. And who knows? Once I get bored of this whole storage thing, maybe I'll try getting into high-performance networking.</p><h2 style=\"text-align: left;\">Composability panel</h2><p>This year I was invited to participate in a panel titled \"Smackdown! Does HPC Need Composability Now?\" moderated by Addison Snell and Dan Olds from <a href=\"https://www.intersect360.com\">Intersect360 Research</a>. This panel was...different. Unlike the traditional SC panel where panelists take turns presenting slides and saying erudite things, this panel had two teams of panelists. And my team only had one slide to present:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>The ground rules included \"personal attacks are allowed,\" and needless to say, the panel was about equal parts entertainment and technical discourse. That's not a bad thing, though.</p><p>Addison and Dan did a phenomenal job of pulling their respective teams together and leading discussion in a format that both brought forward the key pros and cons of composability in HPC while poking fun at the thinly veiled, ego-driven personalities that often make up these sorts of panels. Rather than politely dancing around issues like sacrificing memory bandwidth by putting accelerators at the far end of a PCIe bus or gaining higher utilization by allowing users to mix and match CPU, NICs, and GPUs, us panelists were free to shoot straight (or perhaps a bit hyperbolically) and call each other out on our hidden agendas.</p><p>I hope it goes without saying that all us panelists were in on the format and don't actually think people on the other side are dumb. By wrapping technical arguments in snarky comments, we could keep the level of discussion accessible to a wide audience, drive home the key points from both sides, and ensure that we weren't losing audience members who don't care about the PhD-level details as much as they want to hear what their peers are thinking about this exciting new space. I got some feedback afterwards that I didn't seem to hold back, so if anyone did take anything I said seriously, I am very sorry!</p><p>On a technical level, what was the outcome?</p><p>It turns out that <a href=\"https://www.hpcwire.com/off-the-wire/informal-poll-of-sc22-attendees-suggests-a-bright-future-for-composability/\">there was about a 60/40 split between people who felt composability wasn't required yet and those who felt it was</a> after both sides argued their case. Even among panelists, many of us were a lot less convinced about our respective positions than we let on during the panel itself. I got a chuckle when I realized that I wasn't the only one who, when invited to be on the panel, asked \"what side do you want me to argue?\" I honestly could have gone either way because the dust has not yet settled. <a href=\"https://www.tacc.utexas.edu/about/directory/dan-stanzione\">Dan Stanzione, director of TACC</a>, gave the truest answer to the question of \"will composability help HPC\" up front--\"<a href=\"https://twitter.com/HPC_Guru/status/1592604467698241537?s=20&amp;t=tn3WQBUY9M0MWSfqx1XLKA\">it depends</a>.\" Maybe this is a growth opportunity, or maybe it's a lukewarm reception.</p><p>Either way, composable technologies are hitting the market regardless of whether you think they'll be useful or not.  <a href=\"https://www.nextplatform.com/2022/11/10/amd-genoa-epyc-server-cpus-take-the-heavyweight-title/\">AMD Genoa supports CXL 1.1 with extensions for memory pooling</a>, <a href=\"https://news.samsung.com/global/samsung-electronics-unveils-far-reaching-next-generation-memory-solutions-at-flash-memory-summit-2022\">Samsung has memory-semantic SSDs</a>, and everyone and their mother is working on photonics to get higher bandwidths and lower latencies over longer distances. This makes it easier for people to dip their toes in the water to see if composability makes sense, and I think that's what a lot of people will wind up doing in the coming years.</p><h2 style=\"text-align: left;\">Customer meetings</h2><p>Unlike in years past, my SC experience this year was dominated by customer meetings. I've been on the customer side of the table plenty of times, but I was surprised to find that it was actually more fun to be on the vendor side for a change. I'm part salesman at heart, so I found it personally gratifying to end a meeting with people nodding along rather than scratching their heads. I learned as a customer that it's very easy for vendors to go way off the rails and waste everyone's time, so I was grateful to have avoided the awkward confusion that punctuates those kinds of meetings. </p><p>I also went into the week worrying that I'd be sitting in the same room, hearing the same pitch and the same jokes, and answering the same questions all week. Thankfully, I work with some great field, business, and product teams who set up interesting conversations rather than rote recitations of boring roadmap slides. Approaching the same topics from different angles helped me figure out how all the pieces of what I'm working on fit together to make a complete picture too; there weren't nearly as many opportunities to do this in the DOE world since the end-users of the HPC systems on which I worked aren't told anything until all the design decisions have already been made.</p><h2 style=\"text-align: left;\">A few personal notes</h2><p>This SC was significant to me at a variety of levels; it was the first time I'd gotten on an airplane since February 2020, the first time I'd traveled since starting a new job at a new company, and the first time I'd met any of my new coworkers outside of the structure of a Teams call. During the pandemic I realized that getting out into the world and talking to people from all corners of HPC were my favorite part of my job. Not being able to go to events like SC and maintain that a sense of community involvement dramatically impacted my level of professional satisfaction for the last two years, so I'm glad I was able to finally go this year.</p><p>Though customer meetings were a lot more fun than I expected them to be, I still felt bummed that I could spend so little time walking the expo, talking to folks, and attending all the BOFs normally on my <a href=\"https://sc22.supercomputing.org/presentation/?id=bof124&amp;sess=sess331\">must</a>-<a href=\"https://sc22.supercomputing.org/presentation/?id=bof112&amp;sess=sess307\">attend</a> <a href=\"https://sc22.supercomputing.org/presentation/?id=bof110&amp;sess=sess369\">list</a>. Compounding this was my personal choice to not dine indoors and consequently miss out on almost all other chances to catch up with old friends and colleagues. I also decided to leave SC a day earlier than I usually do to reduce my risk of getting sick which didn't help either. There's never enough time at SC, but this year was particularly pressed.</p><p>I say all this not to complain, but to say how much I appreciated the people who went out of their way to come accost me during the precious few hours I actually had on the exhibit floor. Some I'd not seen since SC'19, and some I'd never actually met since we only started working together mid-pandemic. The conference is busy for everyone, so giving me a slice of your time was very meaningful. That sense of community membership is why I go to SC, it's why I still work in this business, and it's why I try to contribute whatever I can to whomever wants it whether it be a student, engineer, salesperson, or marketer.</p>",
            "url": "https://hpc.social/personal-blog/2022/sc-22-recap/",
            
            
            
            
            
            "date_published": "2022-11-24T02:00:00-07:00",
            "date_modified": "2022-11-24T02:00:00-07:00",
            
                "author": "Glenn K. Lockwood's Blog"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/converged-computing/",
            "title": "Converged Computing",
            "summary": null,
            "content_text": "For many years, there has been a battle between cloud and HPC. The cloud side of the equation says “micro services, cloud native!”and the HPC side says “too expensive!” Conversations often don’t progress because both sides are up-in-arms and focused on why they cannot work together. At best, we might get access to cloud from an HPC center,or an company might present a product as branded for “HPC.” But it’s not truly collaborative in the way that I’d like.I’ll also step back and comment that (I do not believe) folks (myself included) on the HPC side have done enoughto sit at the table. For example, we haven’t been a voice in the Open Containers Initiative (although I’ve tried), nor have we been present (historically) for conferences that are more focused around cloud native technologies.There is no pointing fingers or fault here - it’s just a matter of two different cultures, and it’s been challenging figuring out how to talk to one another, and how to work together. I’ve tried my best to be involved, to the best of my ability, in small ways on both sides. But I’m only one person. This isn’t to say there haven’t been small collaborations, but I believe we can do more.Change is ComingI think this is going to change. The reason is because both sides of the equation have started to realize we have similar goals,and it’s not about creating hybrid environments – having both pancakes and waffles for breakfast – but rather convergence – recognizing that pancakes and waffles are both kinds of breakfast cakes, and we can take features that we like of each to create a breakfast cake that will make everyone happy.The idea of “Converged Computing” comes from my amazing team (see Dan’s talk at KubeCon here) and is the idea that technologies from HPC can be integrated into more traditionally cloud approaches to produce a solution thatsolves problems on both sides. Explicitly for these projects, it means testing the Flux Framework scheduler alongside Kubernetes. Do we still want portable workflows that can move from an HPC environment to cloud? Of course.However, the niche or gradient that I’m interested in is the space that lives between these two worlds.While I won’t go into huge detail (this would be more appropriate for a talk) the lab openly works on Flux Framework, a resource manager that (in my opinion) is one of the coolest projects coming out of our space. I started working with these teams a few months ago, and am bringing my excitement and vision for (what I hope to be) a future where we are actively developing alongside other Kubernetes projects, and our work is well-known and established in this space.What does that mean? Let me share some cool work under development. This is all being done publicly on GitHub, so there isno issue to talk about it! My first year or so at the lab I was hired under a research project, and although I learned a lot, I haven’t felt inspired and driven until starting this work. Let’s talk about some of it! 🎉️The Flux OperatorIf you aren’t familiar with Kubernetes Operators, let’s step back and talk about a human operator. If you are a syadmin managing appswith associated services and databases on a cluster, you often had to do maintenance or update tasks like increasing a storage volume,or modifying a service to a new user need. As this pattern has emerged as a common thing, they have come up with the concept of a Kubernetes Operator - an actual controller you install to your cluster that can automate this. In simple terms, after you install an operator to your cluster,you can hand it a desired state (represented in a yaml configuration file) and the operator will do whatever it takes to reach that state. What does that means in the context of Flux? The Flux Operator is interested in creatingwhat we are calling a “Mini Cluster,” illustrated below.In Kubernetes object terms this is an Indexed Job, a few config maps, secrets, and a RESTFul API and user interface that I designed exposed as a service.  You can read more about our current design here.This Mini Cluster is generated from a “custom resource definition” or CRD (the yaml you provide), and it can take these parameters. Concetually, you as the user own the Mini Cluster and can submit jobs to it (either via the web interface or the API) until you are done. When you are done, you can bring down the cluster.We are excited for this work because in the next months (to a bit longer) we are going to be testing different kinds of workloads running using Flux alongside this Mini Cluster, but on Kubernetes! I’ve started a small repository of dummy examples that I’m extending quickly atrse-ops/flux-hpc and please open an issue there if you have a suggestion.Stay Tuned!Stay tuned for more work in this space! I’ve been doing a ton of programming in Go, Python, and workingon a wide range of technologies, and fairly quickly, and I am very much in my happy place. Please come and join us! ❤️",
            "content_html": "<p>For many years, there has been a battle between cloud and HPC. The cloud side of the equation says “micro services, cloud native!”and the HPC side says “too expensive!” Conversations often don’t progress because both sides are up-in-arms and focused on why they cannot work together. At best, we might get access to cloud from an HPC center,or an company might present a product as branded for “HPC.” But it’s not truly collaborative in the way that I’d like.</p><p>I’ll also step back and comment that (I do not believe) folks (myself included) on the HPC side have done enoughto sit at the table. For example, we haven’t been a voice in the Open Containers Initiative (<a href=\"https://supercontainers.github.io/containers-wg/\" target=\"_blank\">although I’ve tried</a>), nor have we been present (historically) for conferences that are more focused around cloud native technologies.There is no pointing fingers or fault here - it’s just a matter of two different cultures, and it’s been challenging figuring out how to talk to one another, and how to work together. I’ve tried my best to be involved, to the best of my ability, in small ways on both sides. But I’m only one person. This isn’t to say there haven’t been small collaborations, but I believe we can do more.</p><h2 id=\"change-is-coming\">Change is Coming</h2><p>I think this is going to change. The reason is because both sides of the equation have started to realize we have similar goals,and it’s not about creating hybrid environments – having both pancakes and waffles for breakfast – but rather convergence – recognizing that pancakes and waffles are both kinds of breakfast cakes, and we can take features that we like of each to create a breakfast cake that will make everyone happy.The idea of “Converged Computing” comes from my amazing team (see <a href=\"https://www.youtube.com/watch?v=9VwAcSOtph0\" target=\"_blank\">Dan’s talk at KubeCon here</a>) and is the idea that technologies from HPC can be integrated into more traditionally cloud approaches to produce a solution thatsolves problems on both sides. Explicitly for these projects, it means testing the Flux Framework scheduler alongside Kubernetes. Do we still want portable workflows that can move from an HPC environment to cloud? Of course.However, the niche or gradient that I’m interested in is the space that lives <em>between</em> these two worlds.</p><p>While I won’t go into huge detail (this would be more appropriate for a talk) the lab openly works on <a href=\"https://github.com/flux-framework\" target=\"_blank\">Flux Framework</a>, a resource manager that (in my opinion) is one of the coolest projects coming out of our space. I started working with these teams a few months ago, and am bringing my excitement and vision for (what I hope to be) a future where we are actively developing alongside other Kubernetes projects, and our work is well-known and established in this space.What does that mean? Let me share some cool work under development. This is all being done publicly on GitHub, so there isno issue to talk about it! My first year or so at the lab I was hired under a research project, and although I learned a lot, I haven’t felt inspired and driven until starting this work. Let’s talk about some of it! 🎉️</p><h3 id=\"the-flux-operator\">The Flux Operator</h3><div style=\"padding: 20px;\"><img src=\"https://flux-framework.org/flux-operator/_images/the-operator.jpg\" /></div><p>If you aren’t familiar with Kubernetes Operators, let’s step back and talk about a human operator. If you are a syadmin managing appswith associated services and databases on a cluster, you often had to do maintenance or update tasks like increasing a storage volume,or modifying a service to a new user need. As this pattern has emerged as a common thing, they have come up with the concept of a Kubernetes Operator - an actual controller you install to your cluster that can automate this. In simple terms, after you install an operator to your cluster,you can hand it a desired state (represented in a yaml configuration file) and the operator will do whatever it takes to reach that state. What does that means in the context of Flux? The Flux Operator is interested in creatingwhat we are calling a “Mini Cluster,” illustrated below.</p><div style=\"padding: 20px;\"><img src=\"https://flux-framework.org/flux-operator/_images/design-three-team1.png\" /></div><p>In Kubernetes object terms this is an <a href=\"https://kubernetes.io/docs/tasks/job/indexed-parallel-processing-static/\" target=\"_blank\">Indexed Job</a>, a few config maps, secrets, and a <a href=\"https://flux-framework.org/flux-restful-api/\" target=\"_blank\">RESTFul API</a> and user interface that I designed exposed as a service.  You can read more about our current design <a href=\"https://flux-framework.org/flux-operator/development/designs.html\" target=\"_blank\">here</a>.</p><p>This Mini Cluster is generated from a “custom resource definition” or CRD (the yaml you provide), and it can take <a href=\"https://flux-framework.org/flux-operator/getting_started/custom-resource-definition.html\" target=\"_blank\">these parameters</a>. Concetually, you as the user own the Mini Cluster and can submit jobs to it (either via the web interface or the API) until you are done. When you are done, you can bring down the cluster.</p><p>We are excited for this work because in the next months (to a bit longer) we are going to be testing different kinds of workloads running using Flux alongside this Mini Cluster, but on Kubernetes! I’ve started a small repository of dummy examples that I’m extending quickly at<a href=\"https://github.com/rse-ops/flux-hpc\" target=\"_blank\">rse-ops/flux-hpc</a> and please open an issue there if you have a suggestion.</p><h3 id=\"stay-tuned\">Stay Tuned!</h3><p>Stay tuned for more work in this space! I’ve been doing a ton of programming in Go, Python, and workingon a wide range of technologies, and fairly quickly, and I am very much in my happy place. Please come and join us! ❤️</p>",
            "url": "https://hpc.social/personal-blog/2022/converged-computing/",
            
            
            
            
            
            "date_published": "2022-11-18T08:30:00-07:00",
            "date_modified": "2022-11-18T08:30:00-07:00",
            
                "author": "Vanessasaurus"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/ceph-osd-cpu-scaling-part-1/",
            "title": "Ceph OSD CPU Scaling - Part 1",
            "summary": null,
            "content_text": "Last summer we had a user that hit some performance issues based on a recommendation to use 2 cores per OSD in their systems.  I wanted to provide some data for the community and wrote up a blog post on the ceph.io website.  Please take a look!",
            "content_html": "<p>Last summer we had a user that hit some performance issues based on a recommendation to use 2 cores per OSD in their systems.  I wanted to provide some data for the community and wrote up a blog <a href=\"https://ceph.io/en/news/blog/2022/ceph-osd-cpu-scaling/\">post</a> on the ceph.io website.  Please take a look!</p>",
            "url": "https://hpc.social/personal-blog/2022/ceph-osd-cpu-scaling-part-1/",
            
            
            
            
            
            "date_published": "2022-11-08T00:00:00-07:00",
            "date_modified": "2022-11-08T00:00:00-07:00",
            
                "author": "Mark Nelson's Blog"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/containerize-it-baby/",
            "title": "Containerize It, Baby!",
            "summary": null,
            "content_text": "I’ve just submit my entry to the HPC Guru Elevator Pitch Contest for the Supercomputing 2022 conference!I’m fairly sure (like many of these contests) it will be a politically correct winner - someone that is best appealingto the conference, but I’ll take a stand right now that I think my submission is tops in terms of creativityand excited energy! I mean, there is just no alternative when it comes to technologies I’m excited about.  Containerize it, baby!Mic Drop! 🎙️Regardless of the outcome of this contest, I feel like I’ve already won - I’ve had so much fun making this and sharing with the community! 🎉️",
            "content_html": "<p>I’ve just submit my <a href=\"https://twitter.com/vsoch/status/1588215058009464832\" target=\"_blank\">entry</a> to the HPC Guru Elevator Pitch Contest for the Supercomputing 2022 conference!</p><p>I’m fairly sure (like many of these contests) it will be a politically correct winner - someone that is best appealingto the conference, but I’ll take a stand right now that I think my submission is tops in terms of creativityand excited energy! I mean, there is just no alternative when it comes to technologies I’m excited about.</p><blockquote>  <p>Containerize it, baby!</p></blockquote><p><em>Mic Drop!</em> 🎙️</p><p>Regardless of the outcome of this contest, I feel like I’ve already won - I’ve had so much fun making this and sharing with the community! 🎉️</p>",
            "url": "https://hpc.social/personal-blog/2022/containerize-it-baby/",
            
            
            
            
            
            "date_published": "2022-11-03T09:30:00-06:00",
            "date_modified": "2022-11-03T09:30:00-06:00",
            
                "author": "Vanessasaurus"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/happy-living-close-ish-to-the-metal/",
            "title": "happy living close (-ish) to the metal",
            "summary": null,
            "content_text": "For various reasons, I’ve been doing a little bit of career introspection lately. One of the interesting realizations to come out of this is that, despite in practice doing mostly software work, I’ve been happiest when my work involved a strong awareness of the hardware I was running on.I suppose it shouldn’t be a surprise, exactly, but I hadn’t exactly thought about it in those terms before! Before I got into computing, I got a bachelors degree in physics, and got through much of a PhD in materials science. While I wasn’t building computers directly, I was definitely working regularly on hardware, building experimental apparatus involving various combinations of vacuum chambers, lasers, exotic microscopes, custom electronics, and microfluidics.In terms of my computing career, I’ve generally worked in the area of “high-performance computing”, a buzzword that means I’ve focused on building fast parallel systems aimed at researchers. It’s a sub-field that lends itself to awareness of hardware: even as a new baby sysadmin, I was staring at motherboard block diagrams and thinking about the performance differences between different PCIe topologies. And because HPC is one of the areas that took the longest to embrace cloud computing, I spent a lot of years doing work in datacenters. Most of my work would usually involve writing code, doing configuration management, and managing Linux systems… but on a regular basis I’d head into a big loud room full of air conditioners and server racks, carrying a screwdriver.Amusingly, my relatively recent stint at a hyperscaler was the first time I had worked on computers, but didn’t have my office in the same building as the computers I was running! Even there I was at least somewhat cognizant of hardware specifics, and one of my early projects was performance testing on the Bryce Canyon storage node, to see if it was ready for use in a large-scale distributed filesystem.And these days, at NVIDIA, I’m enjoying being even closer to the metal. (At least conceptually; I still work remote…) I spend my days thinking about datacenter requirements, cable lengths, firmware upgrades, hardware health checks, and application performance tests on large clusters. And I love getting to play with these shiny toys.Anyway, this is just a ramble. But a useful one. While I’d be the first to admit that cloud has its place, and I use it for some personal projects, I really enjoy understanding the hardware I run on. I have trouble thinking of computers as remote abstractions with no underlying detail. They are pleasingly physical in my mind, even if they’re thousands of miles away.",
            "content_html": "<p>For various reasons, I’ve been doing a little bit of career introspection lately. One of the interesting realizations to come out of this is that, despite in practice doing mostly software work, I’ve been happiest when my work involved a strong awareness of the hardware I was running on.</p><p><span id=\"more-247\"></span></p><p>I suppose it shouldn’t be a surprise, exactly, but I hadn’t exactly thought about it in those terms before! Before I got into computing, I got a bachelors degree in physics, and got through much of a PhD in materials science. While I wasn’t building computers directly, I was definitely working regularly on hardware, building experimental apparatus involving various combinations of vacuum chambers, lasers, exotic microscopes, custom electronics, and microfluidics.</p><p>In terms of my computing career, I’ve generally worked in the area of “high-performance computing”, a buzzword that means I’ve focused on building fast parallel systems aimed at researchers. </p><p>It’s a sub-field that lends itself to awareness of hardware: even as a new baby sysadmin, I was staring at motherboard block diagrams and thinking about the performance differences between different PCIe topologies. </p><p>And because HPC is one of the areas that took the longest to embrace cloud computing, I spent a lot of years doing work in datacenters. Most of my work would usually involve writing code, doing configuration management, and managing Linux systems… but on a regular basis I’d head into a big loud room full of air conditioners and server racks, carrying a screwdriver.</p><p>Amusingly, my relatively recent stint at a hyperscaler was the first time I had worked on computers, but didn’t have my office in the same building as the computers I was running! Even there I was at least somewhat cognizant of hardware specifics, and one of my early projects was performance testing on the B<a href=\"https://www.opencompute.org/documents/facebook-bryce-canyon-storage-system-specification\">ryce Canyon </a>storage node, to see if it was ready for use in a large-scale distributed filesystem.</p><p>And these days, at NVIDIA, I’m enjoying being even closer to the metal. (At least conceptually; I still work remote…) I spend my days thinking about datacenter requirements, cable lengths, firmware upgrades, hardware health checks, and application performance tests on large clusters. And I love getting to play with these shiny toys.</p><p>Anyway, this is just a ramble. But a useful one. While I’d be the first to admit that cloud has its place, and I use it for some personal projects, I really enjoy understanding the hardware I run on. I have trouble thinking of computers as remote abstractions with no underlying detail. They are pleasingly physical in my mind, even if they’re thousands of miles away.</p>",
            "url": "https://hpc.social/personal-blog/2022/happy-living-close-ish-to-the-metal/",
            
            
            
            
            
            "date_published": "2022-11-02T00:18:17-06:00",
            "date_modified": "2022-11-02T00:18:17-06:00",
            
                "author": "Thinking Out Loud"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/the-web-services-i-self-host/",
            "title": "The web services I self-host",
            "summary": null,
            "content_text": "Why self-host anything?In a lot of ways, self-hosting web services is signing up for extra pain. Most useful web services are available in SaaS format these days, and most people don&#8217;t want to be a sysadmin just to use chat, email, or read the news.In general, I decide to self-host a service if one of two things is true:Self-hosting is going to add a capability that&#8217;s difficult to find in a SaaS alternative. That might be privacy, or extra compute, or just an extra degree of customization that I want.I find it interesting or amusing to self-host it! I have been a professional sysadmin, and ran production web services for over a decade. So I enjoy messing around with servers, and can have a fair amount of fun with this.Infrastructure and general toolingRight now my self-hosted services are hosted on Oracle Cloud Infrastructure, for a very simple reason: OCI includes a very generous Always Free tier, which doesn&#8217;t even ask for a credit card! So I&#8217;m confident I&#8217;m not going to accidentally spend any money. I use ARM Ampere A1 Compute instances for service hosting.The individual services are mostly managed using Docker Compose files, though a few are just running bare-metal. I have so far managed to resist the urge to put everything in Kubernetes.Everything is backed up on a regular basis using Tarsnap.I also use Tailscale to provide a VPN between my cloud servers and my various client devices (phone, laptop, tablet). If a service needs to be exposed to the public Internet to function, I do that&#8230; but otherwise, everything is only exposed within the Tailscale VPN, so that only my own devices can access them. This is both a lovely convenience (not having to manage as many DNS records), and provides an extra degree of security by hiding services that no one else needs to access.Services that I self-hostRSS reader: Despite the demise of Google Reader back in the mists of time, I&#8217;ve been a consistently heavy user of RSS feed since at least 2008. At times I&#8217;ve used commercial products such as Feedly, but these days I self-host the aggregator using FreshRSS. I use FreshRSS partly because it&#8217;s pretty easy to spin up and administer, and partly because it&#8217;s compatible with Reeder, a Mac and iOS app that I generally use to actually read my feeds.Fediverse instance: I run a self-hosted instance on the Fediverse ensemble of social networking sites. The best-known tool for this is Mastodon, but I currently use the Pleroma server, mostly because it seemed less painful to set up and configure. I run my own instance partly out of curiosity, and partly because I didn&#8217;t strongly resonate with any particular topic-specific server that&#8217;s already out there.IRC bouncer: I&#8217;m not on IRC very much these days, but I do like to avoid losing messages, and sometimes want to be logged into the same channels on different physical clients. So I run a ZNC server to maintain persistence.Matrix server: Matrix is a decentralized messaging platform that supports end-to-end encryption. Think of it as being a little like the Fediverse, but for chat rather than microblogging. This falls pretty squarely in the category of &#8220;I find this amusing to run&#8221;, because I mostly chat with less-nerdy folks on other, commercial platforms.Git server: I run a Gitea server which I use to mirror my own repos, as well as a variety of other open source repos. This is mostly to ensure that I have an up-to-date backup of repos I care about, independent of Github or whatever provider.Jupyter notebooks: I keep a persistent Jupyter notebook instance running for random code experiments and as a tiny development playground. This runs on its own VM where I also do other random software development, and it&#8217;s separate from the other services mostly so I don&#8217;t take down all my personal infra with an accidental OOM from a big build.Software package repository: I run an instance of Nexus Repository OSS, mostly to cache Docker images and other content that run the rest of the services above!Services where I use managed hosting but don&#8217;t own the serverThis website! My regular website and this blog run on a shared hosting provider, mostly through inertia. (I&#8217;ve used the same hosting provider for web hosting since around 2008.)Email: In theory it&#8217;s an open, federated system similar to the Fediverse. In practice, the combination of spam and the growth of large providers makes it increasingly painful to run a server yourself. This post from Carlos Fenollosa does a good job of describing the difficulties.I do, however, run all my email through my own domain, though it&#8217;s hosted via Google Apps GSuite Google Workspace. I also back up my inbox locally on a regular basis. That means that if Google ever decides to remove my account, charge obnoxious costs, or otherwise misbehave, my email address is at least portable to other providers.",
            "content_html": "<h2>Why self-host anything?</h2><p>In a lot of ways, self-hosting web services is signing up for extra pain. Most useful web services are available in SaaS format these days, and most people don&#8217;t want to be a sysadmin just to use chat, email, or read the news.</p><p>In general, I decide to self-host a service if one of two things is true:</p><p><span id=\"more-235\"></span></p><ul><li>Self-hosting is going to add a capability that&#8217;s difficult to find in a SaaS alternative. That might be privacy, or extra compute, or just an extra degree of customization that I want.<br /></li><li>I find it interesting or amusing to self-host it! I <em>have been</em> a professional sysadmin, and ran production web services for over a decade. So I enjoy messing around with servers, and can have a fair amount of fun with this.</li></ul><h2>Infrastructure and general tooling</h2><p>Right now my self-hosted services are hosted on <a href=\"https://www.oracle.com/cloud/\">Oracle Cloud Infrastructure</a>, for a very simple reason: OCI includes a <em>very</em> generous <a href=\"https://www.oracle.com/cloud/free/\">Always Free tier</a>, which doesn&#8217;t even ask for a credit card! So I&#8217;m confident I&#8217;m not going to accidentally spend any money. I use ARM Ampere A1 Compute instances for service hosting.</p><p>The individual services are mostly managed using <a href=\"https://docs.docker.com/compose/\">Docker Compose files</a>, though a few are just running bare-metal. I have so far managed to resist the urge to put everything in Kubernetes.</p><p>Everything is backed up on a regular basis using <a href=\"https://www.tarsnap.com/\">Tarsnap</a>.</p><p>I also use <a href=\"https://tailscale.com/\">Tailscale</a> to provide a VPN between my cloud servers and my various client devices (phone, laptop, tablet). If a service needs to be exposed to the public Internet to function, I do that&#8230; but otherwise, everything is only exposed within the Tailscale VPN, so that only my own devices can access them. This is both a lovely convenience (not having to manage as many DNS records), and provides an extra degree of security by hiding services that no one else needs to access.</p><h2>Services that I self-host</h2><ul><li><strong>RSS reader: </strong>Despite the demise of Google Reader back in the mists of time, I&#8217;ve been a consistently heavy user of RSS feed since at least 2008. At times I&#8217;ve used commercial products such as <a href=\"https://feedly.com/\">Feedly</a>, but these days I self-host the aggregator using <a href=\"https://freshrss.org/\">FreshRSS</a>. I use FreshRSS partly because it&#8217;s pretty easy to spin up and administer, and partly because it&#8217;s compatible with <a href=\"https://reederapp.com/\">Reeder</a>, a Mac and iOS app that I generally use to actually read my feeds.<br /></li><li><strong>Fediverse instance: </strong>I run a <a href=\"https://calico.social/\">self-hosted instance</a> on the <a href=\"https://en.wikipedia.org/wiki/Fediverse\">Fediverse</a> ensemble of social networking sites. The best-known tool for this is <a href=\"https://joinmastodon.org/\">Mastodon</a>, but I currently use the <a href=\"https://pleroma.social/\">Pleroma server</a>, mostly because it seemed less painful to set up and configure. I run my own instance partly out of curiosity, and partly because I didn&#8217;t strongly resonate with any particular topic-specific server that&#8217;s already out there.<br /></li><li><strong>IRC bouncer: </strong>I&#8217;m not on IRC very much these days, but I do like to avoid losing messages, and sometimes want to be logged into the same channels on different physical clients. So I run a <a href=\"https://wiki.znc.in/ZNC\">ZNC</a> server to maintain persistence.<br /></li><li><strong>Matrix server: </strong><a href=\"https://matrix.org/\">Matrix</a> is a decentralized messaging platform that supports end-to-end encryption. Think of it as being a little like the Fediverse, but for chat rather than microblogging. This falls pretty squarely in the category of &#8220;I find this amusing to run&#8221;, because I mostly chat with less-nerdy folks on other, commercial platforms.<br /></li><li><strong>Git server: </strong>I run a <a href=\"https://gitea.io/en-us/\">Gitea</a> server which I use to mirror my own repos, as well as a variety of other open source repos. This is mostly to ensure that I have an up-to-date backup of repos I care about, independent of Github or whatever provider.<br /></li><li><strong>Jupyter notebooks: </strong>I keep a persistent <a href=\"https://jupyter.org/\">Jupyter</a> notebook instance running for random code experiments and as a tiny development playground. This runs on its own VM where I also do other random software development, and it&#8217;s separate from the other services mostly so I don&#8217;t take down all my personal infra with an accidental OOM from a big build.<br /></li><li><strong>Software package repository: </strong>I run an instance of <a href=\"https://www.sonatype.com/products/repository-oss-download\">Nexus Repository OSS</a>, mostly to cache Docker images and other content that run the rest of the services above!</li></ul><h2>Services where I use managed hosting but don&#8217;t own the server</h2><ul><li><strong>This website!</strong> My <a href=\"https://www.ajdecon.org\">regular website</a> and this blog run on a shared hosting provider, mostly through inertia. (I&#8217;ve used the same hosting provider for web hosting since around 2008.)<br /></li><li><strong>Email: </strong>In theory it&#8217;s an open, federated system similar to the Fediverse. In practice, the combination of spam and the growth of large providers makes it increasingly painful to run a server yourself. This <a href=\"https://cfenollosa.com/blog/after-self-hosting-my-email-for-twenty-three-years-i-have-thrown-in-the-towel-the-oligopoly-has-won.html\">post from Carlos Fenollosa</a> does a good job of describing the difficulties.<br /><br />I do, however, run all my email through my own domain, though it&#8217;s hosted via <s>Google Apps</s> <s>GSuite</s> Google Workspace. I also back up my inbox locally on a regular basis. That means that if Google ever decides to remove my account, charge obnoxious costs, or otherwise misbehave, my email address is at least portable to other providers.</li></ul><p></p>",
            "url": "https://hpc.social/personal-blog/2022/the-web-services-i-self-host/",
            
            
            
            
            
            "date_published": "2022-10-30T21:59:55-06:00",
            "date_modified": "2022-10-30T21:59:55-06:00",
            
                "author": "Thinking Out Loud"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/qemu-kvm-ceph-librbd-performance/",
            "title": "QEMU/KVM + Ceph Librbd Performance",
            "summary": null,
            "content_text": "Checkout my blog post at the ceph.io website about tuning QEMU/KVM for high performance with librbd.  We got over 123K random read IOPs with 16K IOs from a single VM!",
            "content_html": "<p>Checkout my blog <a href=\"https://ceph.io/en/news/blog/2022/qemu-kvm-tuning/\">post</a> at the ceph.io website about tuning QEMU/KVM for high performance with librbd.  We got over 123K random read IOPs with 16K IOs from a single VM!</p>",
            "url": "https://hpc.social/personal-blog/2022/qemu-kvm-ceph-librbd-performance/",
            
            
            
            
            
            "date_published": "2022-10-24T01:00:00-06:00",
            "date_modified": "2022-10-24T01:00:00-06:00",
            
                "author": "Mark Nelson's Blog"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/dashboards-for-learning-data-visualizations/",
            "title": "Dashboards for Learning Data Visualizations",
            "summary": null,
            "content_text": "Creating dashboards and data visualizations are a favorite past time of mine.  Also, I jump at any chance to learn a new technology.  That is why I have spent the last couple of months building dashboards and data visualizations for various projects while learning several web technologies.Through these dashboards, I have learned many new technologies:  React and NextJS  Mapping libraries such as Leaflet and Mapbox  CSS libraries such as TailwindCSS  Data access JS clients for Elasticsearch and Prometheus  Website hosting service Vercel  Data Visualization library D3.jsGP-ARGO DashboardThe Great Plains Augmented Regional Gateway to the Open Science Grid (GP-ARGO) is a regional collaboration of 16 campuses hosting computing that is made available to the OSG.  My goal with the GP-ARGO dashboard was to show who is using the resources, as well as give high level overview of the region and sites hosting GP-ARGO resources.The metrics are gathered from OSG’s GRACC Elasticsearch.  The list of projects are also from GRACC, and the bar graph in the bottom right are from OSG is simply an iframe to a grafana panel from GRACC.Technologies used: React, NextJS, Leaflet, ElasticsearchRepo: GP-ARGO MapOSDF WebsiteMy next website was the Open Science Data Federation landing page.  I was more bold in the design of the OSDF page.  I took heavy inspiration from other technology websites such as the Mapbox website and the Lens website.  The theme is darker and it was also my first experience with the TailwindCSS library.  Additionally, I learned the CSS flexbox layout techniques.The spinning globe is using the Globe.gl library.  The library is great to create visualizations to show distribution throughout the world.  On the globe I added “transfers” between the OSDF origins and caches.  Each origin sends transfers to every cache in the visualization, though it’s all just animation.  There is no data behind the transfers, it’s only for visual effect.  Also, on the globe, each cache location is labeled.  The globe can be rotated and zoomed with your mouse.The number of bytes read and files read is gathered using the Elasticsearch client querying GRACC, the OSG’s accounting service.  The OSG gathers statistics on every transfer a cache or origin perform.  Additionally, we calculate the rate of data transfers and rate of files being read using GRACC.One unique feature of the OSDF website is the resiliency of the bytes read and files read metrics.  We wanted to make sure that the metrics would be shown even if a data component has failed.  The metrics are gathered in 3 different ways for resiliency:  If all components are working correctly, the metrics are downloaded from the OSG’s Elasticsearch instance.  If OSG Elasticsearch has failed, the dashboard pulls saved metrics from NRP’s S3 storage.  The metrics are saved everytime they are succesfully gathered from Elasticsearch, so they should be fairly recent.  The metrics are gathered and saved on each website build.  The metrics are static and immediatly available upon website load.  If all else fails, these saved static metrics are always available, even if they may be old.Technologies used: React, NextJS, Globe.glRepo: OSDF WebsiteNRP DashboardThe National Research Platform dashboard is largely similar to the GP-ARGO dashboard.  It uses the same basic framework and technologies.  But, the data acquisition is different.The metrics shown are the number of gpus allocated, number of pod running, and the number of active research groups.  The metrics are gathered from the NRP’s prometheus server on-demand.  The graph in the background of the metric is generated with D3.js.Technologies used: React, NextJS, D3.js, Prometheus, TailwindCSSRepo: NRP Map AppPNRP WebsiteThe Prototype National Research Platform is a NSF research platform.  The dashboard is also in prototype stage as the PNRP hardware is not fully delivered and operational yet.The dashboard is my first experience with a large map from Mapbox.  I used a React binding to interface with the Mapbox service.  Also, when you click on a site, it zooms into the building where the PNRP hardware will be hosted.The transfer metrics come from the NRP’s prometheus which shows the bytes moving into and out of the node.  The transfer metrics are for cache nodes nearby the sites, but once PNRP hardware becomes operational the transfer metrics will show the site’s cache.Technologies Used: React, NextJS, Mapbox, TailwindCSS, PrometheusRepo: NRP Website",
            "content_html": "<p>Creating dashboards and data visualizations are a favorite past time of mine.  Also, I jump at any chance to learn a new technology.  That is why I have spent the last couple of months building dashboards and data visualizations for various projects while learning several web technologies.</p><p>Through these dashboards, I have learned many new technologies:</p><ul>  <li><a href=\"https://reactjs.org/\">React</a> and <a href=\"https://nextjs.org/\">NextJS</a></li>  <li>Mapping libraries such as <a href=\"https://leafletjs.com/\">Leaflet</a> and <a href=\"https://www.mapbox.com/\">Mapbox</a></li>  <li>CSS libraries such as <a href=\"https://derekweitzel.com/2022/09/14/dashboards/TailwindCSS\">TailwindCSS</a></li>  <li>Data access JS clients for <a href=\"https://derekweitzel.com/2022/09/14/dashboards/Elasticsearch\">Elasticsearch</a> and <a href=\"https://derekweitzel.com/2022/09/14/dashboards/Prometheus\">Prometheus</a></li>  <li>Website hosting service <a href=\"https://derekweitzel.com/2022/09/14/dashboards/Vercel\">Vercel</a></li>  <li>Data Visualization library <a href=\"https://derekweitzel.com/2022/09/14/dashboards/D3.js\">D3.js</a></li></ul><h2 id=\"gp-argo-dashboard\"><a href=\"https://gp-argo.greatplains.net/\">GP-ARGO Dashboard</a></h2><p><a href=\"https://gp-argo.greatplains.net/\">The Great Plains Augmented Regional Gateway to the Open Science Grid</a> (GP-ARGO) is a regional collaboration of 16 campuses hosting computing that is made available to the OSG.  My goal with the GP-ARGO dashboard was to show who is using the resources, as well as give high level overview of the region and sites hosting GP-ARGO resources.</p><p>The metrics are gathered from OSG’s <a href=\"https://gracc.opensciencegrid.org/\">GRACC Elasticsearch</a>.  The list of projects are also from GRACC, and the bar graph in the bottom right are from OSG is simply an iframe to a grafana panel from GRACC.</p><p>Technologies used: <a href=\"https://reactjs.org/\">React</a>, <a href=\"https://nextjs.org/\">NextJS</a>, <a href=\"https://leafletjs.com/\">Leaflet</a>, <a href=\"https://github.com/elastic/elasticsearch-js\">Elasticsearch</a></p><p><strong>Repo:</strong> <a href=\"https://github.com/djw8605/gp-argo-map\">GP-ARGO Map</a></p><p><a href=\"https://gp-argo.greatplains.net/\"><img alt=\"GP-ARGO\" src=\"https://derekweitzel.com/images/posts/Dashboards/gp-argo-screenshot.png\" /></a></p><h2 id=\"osdf-website\"><a href=\"https://osdf.osg-htc.org/\">OSDF Website</a></h2><p>My next website was the <a href=\"https://osdf.osg-htc.org/\">Open Science Data Federation</a> landing page.  I was more bold in the design of the OSDF page.  I took heavy inspiration from other technology websites such as the <a href=\"https://www.mapbox.com/\">Mapbox</a> website and the <a href=\"https://k8slens.dev/\">Lens</a> website.  The theme is darker and it was also my first experience with the TailwindCSS library.  Additionally, I learned the CSS <a href=\"https://en.wikipedia.org/wiki/CSS_Flexible_Box_Layout\">flexbox</a> layout techniques.</p><p>The spinning globe is using the <a href=\"https://globe.gl/\">Globe.gl</a> library.  The library is great to create visualizations to show distribution throughout the world.  On the globe I added “transfers” between the OSDF origins and caches.  Each origin sends transfers to every cache in the visualization, though it’s all just animation.  There is no data behind the transfers, it’s only for visual effect.  Also, on the globe, each cache location is labeled.  The globe can be rotated and zoomed with your mouse.</p><p>The number of bytes read and files read is gathered using the Elasticsearch client querying GRACC, the OSG’s accounting service.  The OSG gathers statistics on every transfer a cache or origin perform.  Additionally, we calculate the rate of data transfers and rate of files being read using GRACC.</p><p>One unique feature of the OSDF website is the resiliency of the bytes read and files read metrics.  We wanted to make sure that the metrics would be shown even if a data component has failed.  The metrics are gathered in 3 different ways for resiliency:</p><ol>  <li>If all components are working correctly, the metrics are downloaded from the OSG’s Elasticsearch instance.</li>  <li>If OSG Elasticsearch has failed, the dashboard pulls saved metrics from NRP’s S3 storage.  The metrics are saved everytime they are succesfully gathered from Elasticsearch, so they should be fairly recent.</li>  <li>The metrics are gathered and saved on each website build.  The metrics are static and immediatly available upon website load.  If all else fails, these saved static metrics are always available, even if they may be old.</li></ol><p>Technologies used: <a href=\"https://reactjs.org/\">React</a>, <a href=\"https://nextjs.org/\">NextJS</a>, <a href=\"https://globe.gl/\">Globe.gl</a></p><p><strong>Repo:</strong> <a href=\"https://github.com/djw8605/osdf-website\">OSDF Website</a></p><p><a href=\"https://osdf.osg-htc.org/\"><img alt=\"OSDF\" src=\"https://derekweitzel.com/images/posts/Dashboards/osdf-screenshot.png\" /></a></p><h2 id=\"nrp-dashboard\"><a href=\"https://dash.nrp-nautilus.io/\">NRP Dashboard</a></h2><p>The National Research Platform dashboard is largely similar to the <a href=\"https://derekweitzel.com/2022/09/14/dashboards/#gp-argo-dashboard\">GP-ARGO</a> dashboard.  It uses the same basic framework and technologies.  But, the data acquisition is different.</p><p>The metrics shown are the number of gpus allocated, number of pod running, and the number of active research groups.  The metrics are gathered from the NRP’s <a href=\"https://prometheus.io/\">prometheus</a> server on-demand.  The graph in the background of the metric is generated with <a href=\"https://d3js.org/\">D3.js</a>.</p><p>Technologies used: <a href=\"https://reactjs.org/\">React</a>, <a href=\"https://nextjs.org/\">NextJS</a>, <a href=\"https://d3js.org/\">D3.js</a>, <a href=\"https://github.com/siimon/prom-client\">Prometheus</a>, <a href=\"https://tailwindcss.com/\">TailwindCSS</a></p><p><strong>Repo:</strong> <a href=\"https://github.com/djw8605/nrp-map-app\">NRP Map App</a></p><p><a href=\"https://dash.nrp-nautilus.io/\"><img alt=\"NRP Dashboard\" src=\"https://derekweitzel.com/images/posts/Dashboards/nrp-dashboard-screenshot.png\" /></a></p><h2 id=\"pnrp-website\"><a href=\"https://nrp-website.vercel.app/\">PNRP Website</a></h2><p>The <a href=\"https://www.nsf.gov/awardsearch/showAward?AWD_ID=2112167&amp;HistoricalAwards=false\">Prototype National Research Platform</a> is a NSF research platform.  The dashboard is also in prototype stage as the PNRP hardware is not fully delivered and operational yet.</p><p>The dashboard is my first experience with a large map from <a href=\"https://www.mapbox.com/\">Mapbox</a>.  I used a <a href=\"https://visgl.github.io/react-map-gl/\">React binding</a> to interface with the <a href=\"https://www.mapbox.com/\">Mapbox</a> service.  Also, when you click on a site, it zooms into the building where the PNRP hardware will be hosted.</p><p>The transfer metrics come from the NRP’s prometheus which shows the bytes moving into and out of the node.  The transfer metrics are for cache nodes nearby the sites, but once PNRP hardware becomes operational the transfer metrics will show the site’s cache.</p><p>Technologies Used: <a href=\"https://reactjs.org/\">React</a>, <a href=\"https://nextjs.org/\">NextJS</a>, <a href=\"https://www.mapbox.com/\">Mapbox</a>, <a href=\"https://tailwindcss.com/\">TailwindCSS</a>, <a href=\"https://github.com/siimon/prom-client\">Prometheus</a></p><p><strong>Repo:</strong> <a href=\"https://github.com/djw8605/nrp-website\">NRP Website</a></p><p><a href=\"https://nrp-website.vercel.app/\"><img alt=\"PNRP Website\" src=\"https://derekweitzel.com/images/posts/Dashboards/nrp-website-screenshot.png\" /></a></p>",
            "url": "https://hpc.social/personal-blog/2022/dashboards-for-learning-data-visualizations/",
            
            
            
            
            
            "date_published": "2022-09-14T06:00:00-06:00",
            "date_modified": "2022-09-14T06:00:00-06:00",
            
                "author": "Derek Weitzel's Blog"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/tunel-apps-for-hpc/",
            "title": "Tunel- Apps for HPC",
            "summary": null,
            "content_text": "A few months ago I was talking about ssh tunnels. The reason was because I was looking for a solution to deploy apps (like a Jupyter notebook) onto HPC.After an adventure I got it working, and it came down a relatively simple set of commands that I needed to just write into my app logic and forget about.The reason for this was working on my new personal project, tunel.  Tunel is named for what it does. “Tunel” is an elegant derivation of “tunnel” and will do exactly that - create a tunnel between your local workstation and an HPC cluster. In short, tunel will provide a collection of “apps” that are easy to deploy to HPC. There are concepts called launchers, and examples are singularity, slurm, or htcondor. And we can add more! It’s the job of a launcher to take a an app recipe (a definition in yaml plus helper scripts that can be customized on the fly by the user) and get it running, whatever that means (run a job? a container? monitor something? something else?). For the most part, most apps that I’ve developing have web interfaces, as they have historically been the most challenging thing to get easily working on HPC. As a quick example, to run a jupyter notebook via Singularity on my login node, after I install tunel and have my ssh connection defined as “osg” I can do:$ tunel run-app osg singularity/socket/jupyter --jupyterlab=true The name “singularity/socket/jupyter” is the unique identifier (and path) to the recipe and config, and I can provide custom arguments as shown above. And although this is the “singularity” launcher, we can do the same kind of interaction with a slurm launcher, going one level deeper to run the notebook on a node after we submit a job!And in my typical way of doing things, I have automation that generates a table and documentation for each of these apps. Check them out here!. I’m mostly working on singularity an HTCondor apps at the moment because I use the open science grid (OSG) for development, as this is a personal project. Thanks to Matthew West for showing me OSG - I was pretty handicapped to develop before finding it!Django template with a socket?This kind of framework can be powerful if I develop a bunch of custom apps, but it’s much more powerful if I can enable YOU to easily do that too! Thus, I knew one of the first tasks I wanted to do is create a template, likely in each of Flask, Django, and FastAPI, that would plug immediately into Tunel. And while I have much work left to do, last night and this evening I figured out a technical issue that is going to empower us to make so many cool things and I wanted to share! Let’s talk about the problem, what I tried, and what ultimately worked.Traditional Setup with uwsgi and nginxIf you look at a family of Python + web interface apps, you’ll find this uwsgi guy in the middle (I don’t know the correct pronunciation but I say YOU-SKI). It’s a fairly rich tool, but in layman’s terms I think of it as a middleman between Python and a traditional web server. But actually, you don’t technically need the web server - and this is where things start to get interesting. For a traditonal setup, you might find a nginx (a web server) configuration file that looks like this.# the upstream component nginx needs to connect toupstream django {    server unix:///tmp/tunel-django.sock;}# configuration of the serverserver {    # the port your site will be served on    listen      8000;    charset     utf-8;    server_name           localhost;    client_max_body_size 10024M;    client_body_buffer_size 10024M;    client_body_timeout 120;    ...    location ~* \\.(php|aspx|myadmin|asp)$ {      deny all;    }    location /static/ {        autoindex on;        alias /var/www/static/;    }    # Finally, send all non-media requests to the Django server.    location / {        uwsgi_pass  django;        uwsgi_max_temp_file_size 10024m;        include /code/scripts/nginx/uwsgi_params.par;    }}I’ve made a lot of web apps, and whether I use docker-compose with separate containers or a single one, I usually have to write a nginx configuration. The above gets started in the container entrypoint with my app calling uwsgi, and defining that same socket:$ uwsgi --socket=${socket} /code/scripts/uwsgi.iniAnd of course things happen before that, but that’s the main last line. The uwsgi.ini is a configuration filethat makes it easier to define settings.[uwsgi]master = trueprocesses = 4threads = 4py-autoreload = 1#socket = :3031chdir = /code/post-buffering = truelog-date = truemax-requests = 5000http-timeout = 3600000socket-timeout = 120chmod-socket = 666wsgi-file = tuneldjango/wsgi.pyignore-sigpipe = trueignore-write-errors = truedisable-write-exception = truebuffer-size=32768Without going into huge detail, the above says that the app that I wrote (in Python) is listening on that socket, so requests to the web server will either be directed to some static file, filtered out, or sent to our application. And we typically want to use nginx because it’s really good at serving static files and handling traffic.But now let’s step back. If you look under the server in the config above, you’ll notice we are servingcontent on port 8000. This is why I can open the browser to localhost and that port and see my application.But as we know with headless HPC, there are no ports. I can’t use this. So this was my first predicament, last night. I had created this application and it ran locally, but I needed to somehow get the entire thing routed through a tunneled socket to take a next step.Uwsgi Only?I’ll skip over the many hours of things that I tried and failed. I really liked having nginx so I first wanted to somehow send it to the user via a socket, but that never worked. I had an idea to just map the original socket and then have a second container on the host for nginx, but I decided that was too complex. What would up working is realizing that uwsgi can serve http directly, and that came down to a single addition to its config:listen=200protocol=httpOnce I did that, I tried the same technique to map the socket being written to directly to a port via the ssh tunnel, and boum I saw a page! But it was really ugly, because it had no style. This is where I was like OHNO I need nginx for static. But then I found this page and it was a message from the heavens - I could define the same static and media URls using uwsgi directly! That looked like this:$ uwsgi --socket=${socket} --static-map /static=/code/static /code/scripts/uwsgi-standalone.iniAt this point I held my breath, re-ran my app, and wow! There it was - my entire app being served by a container running on a remote machine, only accessible to me through a physical socket. And guess what? I added a file browser, and it even worked to upload a dinosaur picture! Here is the entire page for the app - you can see there are many flags you can add and customize to interact. While it’s only accessible to you and there isn’t need for any kind of login, I did add the default username/password login to Django, and require it for logging in to the file browser. Of course I will eventually need this to be more formally security audited, but at least I don’t have anything interesting on my OSG home to be worried about. And is using just uwsgi a performance issue? I think probably not since the expected use case is only once person.A Future for AppsThis is just the beginning - my plan is to put together a list of use cases for a GUI on a cluster, and then just package them into the core template apps for the developer user to easilyc customize. I have big plans for working on this, and honestly I’m so excited that I find I’m staying up way too late and just egging for the work day to end so I can continue. This idea is so powerful, because it’s using existing technologies to deploy containerized apps on HPC, where you don’t need any special permission. Just to show y’all, here is what it looks like to launch my app template:$ tunel run-app osg singularity/socket/tunel-django --tag=dev --pullI added the pull flag and a custom tag because I am actively developing, and my workflow is to quickly rebuild, push, and then run that command. That then shows me the ssh tunnel command that will immediately connect me to my app on a port in my browser.$ ssh -NT -L 7789:/../tunel/singularity/singularity/socket/tunel-django/singularity-socket-tunel-django.sock sochat1@osgAnd that’s seriously it. You as the developer user are empowered to make and deploy apps, and they have interfaces, and you don’t need to do something silly like open a port or actually deploy a web server. It’s so stupidly easy - I’m looking around at all these complex web app setups that people have made for HPC over the years and I wonder why they aren’t doing something simpler. Maybe it’s just a space of development that people gave up on, or there are some security things I’m missing. Either way, I’m going to charge forward working on this! It’s too simple, and the idea is to beautiful to do anything else by this point.",
            "content_html": "<p>A few months ago I was talking about <a href=\"https://vsoch.github.io/2022/ssh-tunnels/\" target=\"_blank\">ssh tunnels</a>. The reason was because I was looking for a solution to deploy apps (like a Jupyter notebook) onto HPC.After an adventure I got it working, and it came down a relatively simple set of commands that I needed to just <a href=\"https://github.com/tunel-apps/tunel/blob/main/tunel/ssh/commands.py\">write into my app logic</a> and forget about.The reason for this was working on my new personal project, <a href=\"https://tunel-apps.github.io/tunel/\" target=\"_blank\">tunel</a>.</p><blockquote>  <p>Tunel is named for what it does. “Tunel” is an elegant derivation of “tunnel” and will do exactly that - create a tunnel between your local workstation and an HPC cluster.</p></blockquote><div style=\"padding: 20px;\"> <img src=\"https://vsoch.github.io/assets/images/posts/tunel/tunel-docs.png\" /></div><p>In short, tunel will provide a collection of “apps” that are easy to deploy to HPC. There are concepts called launchers, and examples are singularity, slurm, or htcondor. And we can add more! It’s the job of a launcher to take a an app recipe (a definition in yaml plus helper scripts that can be customized on the fly by the user) and get it running, whatever that means (run a job? a container? monitor something? something else?). For the most part, most apps that I’ve developing have web interfaces, as they have historically been the most challenging thing to get easily working on HPC. As a quick example, to run a jupyter notebook via Singularity on my login node, after I install tunel and have my ssh connection defined as “osg” I can do:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>tunel run-app osg singularity/socket/jupyter <span class=\"nt\">--jupyterlab</span><span class=\"o\">=</span><span class=\"nb\">true</span> </code></pre></div></div><p>The name “singularity/socket/jupyter” is the unique identifier (and path) to the recipe and config, and I can provide custom arguments as shown above. And although this is the “singularity” launcher, we can do the same kind of interaction with a slurm launcher, going one level deeper to run the notebook on a node after we submit a job!And in my typical way of doing things, I have automation that generates a table and documentation for each of these apps. <a href=\"https://tunel-apps.github.io/tunel/_static/apps/\" target=\"_blank\">Check them out here!</a>.</p><div style=\"padding: 20px;\"> <img src=\"https://vsoch.github.io/assets/images/posts/tunel/table.png\" /></div><p>I’m mostly working on singularity an HTCondor apps at the moment because I use the open science grid (OSG) for development, as this is a personal project. Thanks to <a href=\"https://twitter.com/westbynoreaster\" target=\"_blank\">Matthew West</a> for showing me OSG - I was pretty handicapped to develop before finding it!</p><h2 id=\"django-template-with-a-socket\">Django template with a socket?</h2><p>This kind of framework can be powerful if I develop a bunch of custom apps, but it’s much more powerful if I can enable YOU to easily do that too! Thus, I knew one of the first tasks I wanted to do is create a template, likely in each of Flask, Django, and FastAPI, that would plug immediately into Tunel. And while I have much work left to do, last night and this evening I figured out a technical issue that is going to empower us to make so many cool things and I wanted to share! Let’s talk about the problem, what I tried, and what ultimately worked.</p><h3 id=\"traditional-setup-with-uwsgi-and-nginx\">Traditional Setup with uwsgi and nginx</h3><p>If you look at a family of Python + web interface apps, you’ll find this <a href=\"https://uwsgi-docs.readthedocs.io/en/latest/\" target=\"_blank\">uwsgi</a> guy in the middle (I don’t know the correct pronunciation but I say YOU-SKI). It’s a fairly rich tool, but in layman’s terms I think of it as a middleman between Python and a traditional web server. But actually, you don’t technically need the web server - and this is where things start to get interesting. For a traditonal setup, you might find a nginx (a web server) configuration file that <a href=\"https://github.com/tunel-apps/tunel-django/blob/main/scripts/nginx/nginx.conf\" target=\"_blank\">looks like this</a>.</p><div class=\"language-yaml highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\"># the upstream component nginx needs to connect to</span><span class=\"s\">upstream django {</span>    <span class=\"s\">server unix:///tmp/tunel-django.sock;</span><span class=\"err\">}</span><span class=\"c1\"># configuration of the server</span><span class=\"s\">server {</span>    <span class=\"s\"># the port your site will be served on</span>    <span class=\"s\">listen      8000;</span>    <span class=\"s\">charset     utf-8;</span>    <span class=\"s\">server_name           localhost;</span>    <span class=\"s\">client_max_body_size 10024M;</span>    <span class=\"s\">client_body_buffer_size 10024M;</span>    <span class=\"s\">client_body_timeout 120;</span>    <span class=\"s\">...</span>    <span class=\"s\">location ~* \\.(php|aspx|myadmin|asp)$ {</span>      <span class=\"s\">deny all;</span>    <span class=\"s\">}</span>    <span class=\"s\">location /static/ {</span>        <span class=\"s\">autoindex on;</span>        <span class=\"s\">alias /var/www/static/;</span>    <span class=\"s\">}</span>    <span class=\"s\"># Finally, send all non-media requests to the Django server.</span>    <span class=\"s\">location / {</span>        <span class=\"s\">uwsgi_pass  django;</span>        <span class=\"s\">uwsgi_max_temp_file_size 10024m;</span>        <span class=\"s\">include /code/scripts/nginx/uwsgi_params.par;</span>    <span class=\"s\">}</span><span class=\"err\">}</span></code></pre></div></div><p>I’ve made a lot of web apps, and whether I use docker-compose with separate containers or a single one, I usually have to write a nginx configuration. The above gets started in the container entrypoint with my app calling uwsgi, and defining that same socket:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>uwsgi <span class=\"nt\">--socket</span><span class=\"o\">=</span><span class=\"k\">${</span><span class=\"nv\">socket</span><span class=\"k\">}</span> /code/scripts/uwsgi.ini</code></pre></div></div><p>And of course things happen before that, but that’s the main last line. The uwsgi.ini is a configuration filethat makes it easier to define settings.</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>[uwsgi]master = trueprocesses = 4threads = 4py-autoreload = 1#socket = :3031chdir = /code/post-buffering = truelog-date = truemax-requests = 5000http-timeout = 3600000socket-timeout = 120chmod-socket = 666wsgi-file = tuneldjango/wsgi.pyignore-sigpipe = trueignore-write-errors = truedisable-write-exception = truebuffer-size=32768</code></pre></div></div><p>Without going into huge detail, the above says that the app that I wrote (in Python) is listening on that socket, so requests to the web server will either be directed to some static file, filtered out, or sent to our application. And we typically want to use nginx because it’s really good at serving static files and handling traffic.</p><p>But now let’s step back. If you look under the server in the config above, you’ll notice we are servingcontent on port 8000. This is why I can open the browser to localhost and that port and see my application.But as we know with headless HPC, there are no ports. I can’t use this. So this was my first predicament, last night. I had created this application and it ran locally, but I needed to somehow get the entire thing routed through a tunneled socket to take a next step.</p><h3 id=\"uwsgi-only\">Uwsgi Only?</h3><p>I’ll skip over the many hours of things that I tried and failed. I really liked having nginx so I first wanted to somehow send it to the user via a socket, but that never worked. I had an idea to just map the original socket and then have a second container on the host for nginx, but I decided that was too complex. What would up working is realizing that uwsgi can serve http directly, and that came down to a single addition to its config:</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>listen=200protocol=http</code></pre></div></div><p>Once I did that, I tried the same technique to map the socket being written to directly to a port via the ssh tunnel, and <em>boum</em> I saw a page! But it was really ugly, because it had no style. This is where I was like OHNO I need nginx for static. But then I found <a href=\"https://uwsgi-docs.readthedocs.io/en/latest/StaticFiles.html\" target=\"_blank\">this page</a> and it was a message from the heavens - I could define the same static and media URls using uwsgi directly! That looked like this:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>uwsgi <span class=\"nt\">--socket</span><span class=\"o\">=</span><span class=\"k\">${</span><span class=\"nv\">socket</span><span class=\"k\">}</span> <span class=\"nt\">--static-map</span> /static<span class=\"o\">=</span>/code/static /code/scripts/uwsgi-standalone.ini</code></pre></div></div><p>At this point I held my breath, re-ran my app, and wow!</p><div style=\"padding: 20px;\"> <img src=\"https://vsoch.github.io/assets/images/posts/tunel/home.png\" /></div><p>There it was - my entire app being served by a container running on a remote machine, only accessible to me through a physical socket. And guess what? I added a file browser, and it even worked to upload a dinosaur picture!</p><div style=\"padding: 20px;\"> <img src=\"https://vsoch.github.io/assets/images/posts/tunel/browser.png\" /></div><p>Here is the entire page for the app - you can see there are many flags you can add and customize to interact.</p><div style=\"padding: 20px;\"> <img src=\"https://vsoch.github.io/assets/images/posts/tunel/app.png\" /></div><p>While it’s only accessible to you and there isn’t need for any kind of login, I did add the default username/password login to Django, and require it for logging in to the file browser. Of course I will eventually need this to be more formally security audited, but at least I don’t have anything interesting on my OSG home to be worried about. And is using just uwsgi a performance issue? I think probably not since the expected use case is only once person.</p><h3 id=\"a-future-for-apps\">A Future for Apps</h3><p>This is just the beginning - my plan is to put together a list of use cases for a GUI on a cluster, and then just package them into the core template apps for the developer user to easilyc customize. I have big plans for working on this, and honestly I’m so excited that I find I’m staying up way too late and just egging for the work day to end so I can continue. This idea is so powerful, because it’s using existing technologies to deploy containerized apps on HPC, where you don’t need any special permission. Just to show y’all, here is what it looks like to launch my app template:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>tunel run-app osg singularity/socket/tunel-django <span class=\"nt\">--tag</span><span class=\"o\">=</span>dev <span class=\"nt\">--pull</span></code></pre></div></div><p>I added the pull flag and a custom tag because I am actively developing, and my workflow is to quickly rebuild, push, and then run that command. That then shows me the ssh tunnel command that will immediately connect me to my app on a port in my browser.</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>ssh <span class=\"nt\">-NT</span> <span class=\"nt\">-L</span> 7789:/../tunel/singularity/singularity/socket/tunel-django/singularity-socket-tunel-django.sock sochat1@osg</code></pre></div></div><p>And that’s seriously it. You as the developer user are empowered to make and deploy apps, and they have interfaces, and you don’t need to do something silly like open a port or actually deploy a web server. It’s so stupidly easy - I’m looking around at all these complex web app setups that people have made for HPC over the years and I wonder why they aren’t doing something simpler. Maybe it’s just a space of development that people gave up on, or there are some security things I’m missing. Either way, I’m going to charge forward working on this! It’s too simple, and the idea is to beautiful to do anything else by this point.</p>",
            "url": "https://hpc.social/personal-blog/2022/tunel-apps-for-hpc/",
            
            
            
            
            
            "date_published": "2022-08-04T13:30:00-06:00",
            "date_modified": "2022-08-04T13:30:00-06:00",
            
                "author": "Vanessasaurus"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/ceph-rocksdb-tuning-deep-dive/",
            "title": "Ceph RocksDB Tuning Deep-Dive",
            "summary": null,
            "content_text": "See my post on the Ceph.io blog about tuning RocksDB in Ceph!",
            "content_html": "<p>See my <a href=\"https://ceph.io/en/news/blog/2022/rocksdb-tuning-deep-dive/\">post</a> on the Ceph.io blog about tuning RocksDB in Ceph!</p>",
            "url": "https://hpc.social/personal-blog/2022/ceph-rocksdb-tuning-deep-dive/",
            
            
            
            
            
            "date_published": "2022-07-25T01:00:00-06:00",
            "date_modified": "2022-07-25T01:00:00-06:00",
            
                "author": "Mark Nelson's Blog"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/the-utility-vs-the-professional-services-firm/",
            "title": "The Utility vs the Professional Services Firm",
            "summary": null,
            "content_text": "As research computing and data becomes more complex and diverse, we need more professional services firms and fewer utilties(Note: This post is adapted from #127 of the Research Computing Teams Newsletter)I get to talk with a lot of research computing and data teams - software, data, and systems.  Sometimes in these conversations it’s pretty clear that some teams, or the team and their funder, or a team and I, are talking a bit past each other.  And that’s usually because they or we are (currently) operating with very different mental models of how they operate.Some research computing and data teams are operating as Utilities, and see the world through that lens; a growing number are operating as Professional Services Firms.  Others are moving from one to the other, and are at different places along that very abrupt transition.  Some kinds of groups (like bioinformatics cores) are much more likely to already be operating in service mode, while others (like research compute infrastructure teams) are more likely to still think of themselves as utilities.  It varies from place to place, though, depending on local conditions.  But they’re very different models!    Utility service and professional services delivery are very different, and require different funding, management, and career development models.  Image credit: left and right.Utilities, like power companies or garbage collection or municipal potable water, were really the only sensible role models for the first decades of research computing and data teams.  Those teams were entirely about operating large equipment purchased from vendors.  Costs were mostly a big capital expense.  Everyone who needed the utility needed the same thing - undifferentiated flops and bytes, or 60Hz 120VAC.  Because everyone needed the same thing, economies of scale led to natural monopolies; the most reasonable provision model was for the local jurisdiction/institution to own or control a single operator.  Differentiation or strategy, or gaining new customers, weren’t meaningful discussion topics.  The only thing that really makes a difference is scale, which leads to mergers.  Innovation happens slowly, top-down, at the industry-wide scale and usually from the vendors (“hey, did you hear about those new gas compressors Dyneco announced?”), and diffuses outwards.  Employees take pride in and the organization values operational skill and things ticking along smoothly.  Customers value reliability.  The only thing that matters for any individual operator is to operate effectively and to provide the standard service with the right amount of cost: high enough to absorb the available subsidy, low enough to not go broke.  If a customer needs something other than what the utility provides, rather than that being a market opportunity, it’s either an inconvenience or an irrelevance.  The power company or the water utility or the old phone monopoly just doesn’t serve that need.Professional Service Firms — say engineering firms, or architects, or consultancies — are very different beasts.  They might very well have significant capital investment in specialized equipment, but their main selling point and their biggest cost is expertise.  Competing for and retaining that expertise, and developing that expertise in house and amongst their clients, are principal concerns.  As part of a “full-service” offering they they likely have some fairly standard services they offer at the low end, where operating cost and efficiency is vital.  But what the organization values, and the employees enjoy, is at the high-touch end — getting deeply involved with the client work, and being as much a collaborator or partner or “trusted advisor” as a service provider.  Different clients want very different things, and that high-touch high-expertise work is specialized and labour intensive, so the firms themselves need a clear focus; they can’t meet all needs.  Clients can go elsewhere, so there is redundancy and competition, but less than you’d think at a distance.  In civil engineering a geotechnical firm is complementary, not competing, with one that specializes in water resource engineering.As in the rest of our lives, in research computing we need to have utilities.  As research data management matures, institutional or regional data depositories become mature and “enterprise” enough to become utilities, likely run by IT or the Library.  Teaching or CI/CD or MLOps resources for data science or software development are likely best served by this model.  The closer the operations are to standard, something that can be run by IT, the more likely it is to be a utility.  But one has to be careful.  Utilies are commodoties: they tend to get merged together wherever feasible, since scale matters and it’s all undifferentiated commodity provision.As research computing becomes broader and faster changing and more diverse, we need more and more professional services firms, too; nimble groups specialized to particular needs and ready to adapt as those needs change.  As even infrastructure is becoming less one-size-fits-all, and methods for making use of computing and data for diverse fields grow more complex and expertise intensive, the preconditions for the utility model are met in fewer situations than used to be.A lot of research computing teams are interested in providing something more like professional services, but were created in the Utility model, and are stuck there by their funders.  The institutional or external funders still have this very specific (and to their mind time tested and successful) operating model in their plans.  Utilities are funded very differently than professional services firms.  At utility scale, it doesn’t make sense to outsource things, or develop non-standard services (who wants non-standard power coming into their house!)  Funders requirements on eligible expenses may focus almost entirely on the capital spend, and not on operating funding that’s needed to make effective use of the capital, or to be more agile in how services are delivered.Even those teams who aren’t being held back by funders and who want to make the switch to professional services from their original utility model find it a hard transition. There’s no obvious, incremental path to go from providing a standard, stable commodity to changing, specialized, bundles of expertise.  Utilities operate very differently from professional services firms.  They value different things. The models for staff growth are different.  So they have to be managed quiet differently, and there’s no clear path internally from A to B.Besides funding, and internal considerations, utilities and professional services firms are also percieved and valued by their clients very differently.  Utilities’ existing customers don’t want change, and new customers aren’t yet interested in getting advanced app software development suggestions from what they perceive to still be the mobile telephony provider.But research computing and data is changing, increasingly quickly, and the utility approach only meets a piece of these growing needs.  Navigating the transition isn’t going to be easy, for RCD teams, leaders, or funders; but expressing it clearly and talking about it more will maybe mean we’re not talking past each other so often.",
            "content_html": "<h2 id=\"as-research-computing-and-data-becomes-more-complex-and-diverse-we-need-more-professional-services-firms-and-fewer-utilties\">As research computing and data becomes more complex and diverse, we need more professional services firms and fewer utilties</h2><p>(Note: This post is adapted from <a href=\"https://www.researchcomputingteams.org/newsletter_issues/0127\">#127</a> of the <a href=\"https://www.researchcomputingteams.org\">Research Computing Teams Newsletter</a>)</p><p>I get to talk with a lot of research computing and data teams - software, data, and systems.  Sometimes in these conversations it’s pretty clear that some teams, or the team and their funder, or a team and I, are talking a bit past each other.  And that’s usually because they or we are (currently) operating with very different mental models of how they operate.</p><p>Some research computing and data teams are operating as Utilities, and see the world through that lens; a growing number are operating as Professional Services Firms.  Others are moving from one to the other, and are at different places along that very abrupt transition.  Some kinds of groups (like bioinformatics cores) are much more likely to already be operating in service mode, while others (like research compute infrastructure teams) are more likely to still think of themselves as utilities.  It varies from place to place, though, depending on local conditions.  But they’re very different models!</p><figure style=\"width: 45%; float: right;\">  <img alt=\"Utility vs professional services.  Image Credit: left, John Moore (@thejmoore) at Unsplash.com; right, Jason Goodman @jasongoodman_youxventures at Unsplash.com\" src=\"https://www.dursi.ca/assets/imgs/utility-vs-professional-svc.png\" />  <figcaption><i>Utility service and professional services delivery are very different, and require different funding, management, and career development models.  Image credit: <a href=\"https://unsplash.com/photos/0MKzwPmehRE\">left</a> and <a href=\"https://unsplash.com/photos/X8H8vPcelPk\">right</a>.</i></figcaption></figure><p>Utilities, like power companies or garbage collection or municipal potable water, were really the only sensible role models for the first decades of research computing and data teams.  Those teams were entirely about operating large equipment purchased from vendors.  Costs were mostly a big capital expense.  Everyone who needed the utility needed the same thing - undifferentiated flops and bytes, or 60Hz 120VAC.  Because everyone needed the same thing, economies of scale led to <a href=\"https://en.wikipedia.org/wiki/Natural_monopoly\">natural monopolies</a>; the most reasonable provision model was for the local jurisdiction/institution to own or control a single operator.  Differentiation or strategy, or gaining new customers, weren’t meaningful discussion topics.  The only thing that really makes a difference is scale, which leads to mergers.  Innovation happens slowly, top-down, at the industry-wide scale and usually from the vendors (“hey, did you hear about those new gas compressors Dyneco announced?”), and diffuses outwards.  Employees take pride in and the organization values operational skill and things ticking along smoothly.  Customers value reliability.  The only thing that matters for any individual operator is to operate effectively and to provide the standard service with the right amount of cost: high enough to absorb the available subsidy, low enough to not go broke.  If a customer needs something other than what the utility provides, rather than that being a market opportunity, it’s either an inconvenience or an irrelevance.  The power company or the water utility or the <a href=\"https://vimeo.com/355556831\">old phone monopoly</a> just doesn’t serve that need.</p><p>Professional Service Firms — say engineering firms, or architects, or consultancies — are very different beasts.  They might very well have significant capital investment in specialized equipment, but their main selling point and their biggest cost is expertise.  Competing for and retaining that expertise, and developing that expertise in house and amongst their clients, are principal concerns.  As part of a “full-service” offering they they likely have some fairly standard services they offer at the low end, where operating cost and efficiency is vital.  But what the organization values, and the employees enjoy, is at the high-touch end — getting deeply involved with the client work, and being as much a collaborator or partner or “trusted advisor” as a service provider.  Different clients want very different things, and that high-touch high-expertise work is specialized and labour intensive, so the firms themselves need a clear focus; they <em>can’t</em> meet all needs.  Clients can go elsewhere, so there is redundancy and competition, but less than you’d think at a distance.  In civil engineering a geotechnical firm is complementary, not competing, with one that specializes in water resource engineering.</p><p>As in the rest of our lives, in research computing we need to have utilities.  As research data management matures, institutional or regional data depositories become mature and “enterprise” enough to become utilities, likely run by IT or the Library.  Teaching or CI/CD or MLOps resources for data science or software development are likely best served by this model.  The closer the operations are to standard, something that can be run by IT, the more likely it is to be a utility.  But one has to be careful.  Utilies are commodoties: they tend to get merged together wherever feasible, since scale matters and it’s all undifferentiated commodity provision.</p><p>As research computing becomes broader and faster changing and more diverse, we need more and more professional services firms, too; nimble groups specialized to particular needs and ready to adapt as those needs change.  As even infrastructure is becoming less one-size-fits-all, and methods for making use of computing and data for diverse fields grow more complex and expertise intensive, the preconditions for the utility model are met in fewer situations than used to be.</p><p>A lot of research computing teams are interested in providing something more like professional services, but were created in the Utility model, and are stuck there by their funders.  The institutional or external funders still have this very specific (and to their mind time tested and successful) operating model in their plans.  Utilities are funded very differently than professional services firms.  At utility scale, it doesn’t make sense to outsource things, or develop non-standard services (who wants non-standard power coming into their house!)  Funders requirements on eligible expenses may focus almost entirely on the capital spend, and not on operating funding that’s needed to make effective use of the capital, or to be more agile in how services are delivered.</p><p>Even those teams who aren’t being held back by funders and who want to make the switch to professional services from their original utility model find it a hard transition. There’s no obvious, incremental path to go from providing a standard, stable commodity to changing, specialized, bundles of expertise.  Utilities operate very differently from professional services firms.  They value different things. The models for staff growth are different.  So they have to be managed quiet differently, and there’s no clear path internally from A to B.</p><p>Besides funding, and internal considerations, utilities and professional services firms are also percieved and valued by their clients very differently.  Utilities’ existing customers don’t want change, and new customers aren’t yet interested in getting advanced app software development suggestions from what they perceive to still be the mobile telephony provider.</p><p>But research computing and data is changing, increasingly quickly, and the utility approach only meets a piece of these growing needs.  Navigating the transition isn’t going to be easy, for RCD teams, leaders, or funders; but expressing it clearly and talking about it more will maybe mean we’re not talking past each other so often.</p>",
            "url": "https://hpc.social/personal-blog/2022/the-utility-vs-the-professional-services-firm/",
            
            
            
            
            
            "date_published": "2022-07-03T01:00:00-06:00",
            "date_modified": "2022-07-03T01:00:00-06:00",
            
                "author": "Jonathan Dursi's Blog"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/ssh-tunnels/",
            "title": "SSH Tunnels",
            "summary": null,
            "content_text": "Today I want to talk about ssh tunnels. Very abstractly, we would want to use an sshtunnel to securely send information. In the case of HPC, you are probably familiar with ssh,(Secure Shell or Secure Socket Shell) when you login to your node. You might do something like this:$ ssh dinosaur@server.address.eduOr if you have a proper setup in your ~/.ssh/config (with a named server) you might just do:$ ssh dinosaurI like to use ssh connection multiplexingso the connection is kept alive for a bit, but I won’t go into detail becausethis post isn’t specifically about the details of ssh. The use case I’m interested in (and the thingthat HPC is very bad at) is how to deploy something interactive on an HPC cluster.SSH Tunnel with PortsGiven that a cluster has exposed ports (either the login node, or both the login node and compute nodes)creating a tunnel is fairly straight forward! In the past I created a tool called forward to handle all the manual steps to get this working, meaning:  Show the user how to set up their ~/.ssh/config (once)  Define (once) parameters like a port, memory, GPUs, and if the cluster has isolated nodes  Start any number of provided apps that come with forward (e.g., jupyter, singularity, etc.)An interaction using forward might look like any of the following:# Run a Singularity container that already exists on your resource (recommended)bash start-node.sh singularity-run /scratch/users/vsochat/share/pytorch-dev.simg# Execute a custom command to the same Singularity containerbash start-node.sh singularity-exec /scratch/users/vsochat/share/pytorch-dev.simg echo \"Hello World\"# Run a Singularity container from a url, `docker://ubuntu`bash start-node.sh singularity-run docker://ubuntu# Execute a custom command to the same containerbash start-node.sh singularity-exec docker://ubuntu echo \"Hello World\"# To start a jupyter notebook in a specific directory ON the cluster resourcebash start.sh jupyter &lt;cluster-dir&gt;# To start a jupyter notebook with tensorflow in a specific directorybash start.sh py3-tensorflow &lt;cluster-dir&gt;Note that the last set of commands are pertaining to notebooks, which is where these tunnels come into play!A notebook is going to be run on a compute node that looks something like the following:$ jupyter notebook --no-browser --port=$PORTAnd if you ran this with a Singularity container, you’d also want to bind jovyan’s home to be the user’s, along with the jupyter config directory:$ singularity exec --home ${HOME} \\    --bind ${HOME}/.local:/home/jovyan/.local \\    --bind ${HOME}/.jupyter:/home/jovyan/.jupyter \\      datascience_notebook.sif jupyter notebook --no-browser --port=$PORT --ip 0.0.0.0As we described earlier here,there are subtle differences between making a tunnel (with a port) given that you have isolated nodes (or not).You can determine this based on your ability to ssh into a non-login node (meaning where your job is running) from “the outside world”that is your computer. If you cannot, your nodes are isolated, which we will discuss next.Isolated NodesLet’s say that we need to create a tunnel (using ports) to an isolated node. This means that we are basically goingto establish a tunnel to the login node, and then from the login node another one to the compute node.We might use a command that looks like this:$ ssh -L $PORT:localhost:$PORT ${RESOURCE} ssh -L $PORT:localhost:$PORT -N \"$MACHINE\" &amp;In the command above, the first half (ssh -L $PORT:localhost:$PORT ${RESOURCE}) is executed on the local machine, which establishes a port forwarding to the login node. The “-L” in the above (from the man pages) :  Specifies that connections to the given TCP port or Unix socket on the local (client) host are to be forwarded to thegiven host and port, or Unix socket, on the remote side.This works by allocating a socket to listen to either a TCPport on the local side, optionally bound to the specifiedbind_address, or to a Unix socket.  Whenever a connection ismade to the local port or socket, the connection is for‐warded over the secure channel, and a connection is made toeither host port hostport, or the Unix socket remote_socket,from the remote machine.Or in layman’s terms:  Forward whatever is running on the second port on my resource to my local machine.Since we are forwarding ports, this would require minimally the login node to expose ports.The next line ssh -L $PORT:localhost:$PORT -N \"$MACHINE\" &amp; is a second command run from the login node, and port forwards it to the compute node, since you can only access the compute node from the login nodes.You’ll notice it looks just like the first, and this works because ssh commands can be chained.The -N says “don’t execute a remote command (and just forward the port).”Finally, the last $MACHINE is the node that the jupyter notebook is running on.Not IsolatedFor HPCs where the compute node is not isolated from the outside world the ssh command for port forwarding first establishes a connection the login node, but then continues to pass on the login credentials to the compute node to establish a tunnel between the localhost and the port on the compute node. The ssh command in this case utilizes the flag -K that forwards the login credentials to the compute node:$ ssh \"$DOMAINNAME\" -l $FORWARD_USERNAME -K -L  $PORT:$MACHINE:$PORT -N  &amp;I’m not sure in practice how common this is anymore. At least at my current employer it’s not even the casethat ports are exposed on the login node! It’s probably better that way, because in cases where you do get ports it’s sort of a “pick a port above this range and hope that no other user picks the same one!” It’s messy. So let’s talk about the case of not having ports exposed next, since this was the entire reason I wanted to write this post!SSH Tunnel with SocketMore than a year ago, I had this realization that a lot of people at Stanford used the “forward” tool, and just for notebooks (and thiswas before they were available via Open OnDemand, which is what I’d recommend to a Stanford user at this point). I decided I wanted to make a new open source tool, “tunel” (an elegant derivation of “tunnel”) vsoch/tunel to make it easyto run what I call “apps” on an HPC cluster. Are there better ways of exposing user interfaces on HPC? Yes, indeed. But not everyonehas easy access. It was also a stubborn “I want this to work” proof of concept. This new tool would be like forward, but a little nicer.Because I, along with every other HPC developer and user, wishes we could have nice things 😭️.At this time I had just started a new role at a national lab, and I realized that none of my old techniques for launchingthe job worked because of the lack of exposed ports. Thinking this was impossible, I abandoned it for a year. But then this last week I found this! I was motivated! I was excited! The basic launch command of the notebook looks like this:$ jupyter notebook --sock /tmp/test.sock --no-browserAnd then with a different looking tunnel, we could forward this socket to the host, and map it to a port! My excitement was then brought downby what led to two days of struggling. I first tried my entire tunel workflow, meaning launching a job on a node,and then running that command, and providing the instruction to the user to create the tunnel as follows:$ ssh -L 8888:/tmp/test.sock -N user@this_hostThat didn’t work (and remember this socket was created on the isolated node, that’s important to remember for later). So I started looking at the socket with “nc”  - “arbitrary TCP and UDP connections and listens” from the login node. The “-U” below is for UNIX sockets:$ nc -U /tmp/test.sockAnd from the head node I saw:Ncat: Connection refused.So then I knew I needed a simpler, dummier example. I got rid of tunel and just ran the notebook command on the head node.Dear reader, it still did not work. I opened an issue and asked Twitter for help. Someone else on Twitter reported that it worked for them, and that (in my opinion) is the challenge and story of HPC - given the huge differences in setups, it’s hard to reproduce what another person does unless you scope to a very specificenvironment or technology and hugely go out of your way to do it. I’m always grateful when someone tries to help, but when the ultimate answer is just“But it works on my machine!” I (and I think all of us) are like:(╯°□°)╯︵ ┻━┻🤣️Please know that is intended to be funny, and I really am grateful for the attempt to help! Anyway, the first night I was devastated because I was so excited about the possibility of this working! But of course (as it usually does) my quasi-sadness turned again into relentless stubborn-ness, and for my SaturdayI embarked on trying everything. I call this the stubborn brute force approach, and it actually leads to some pretty good outcomes?Socket from Login NodeFirst from the login node, I started reading about flags in detail, again from the man pages. It occurred to me that the suggested command included “-L” (discussed earlier) but there were a ton of other flags to try, and maybe I need them for my setup? The command that wound up working (after much trial and error) was just:# Running on login node$ ssh -NT -L 8888:/tmp/test.sock user@serverAnd here again was the suggested command:$ ssh -L 8888:/tmp/test.sock -N user@this_hostSo they are very similar - and the main difference is the -T is to “Disable pseudo-terminal allocation.”So I suspect (also based on the version of ssl I’m using) that without the flag, you might be making a request for a pty to the server(more details here) and then it could abort. Adding the flag just skips this, because we don’t need that - we just need the simple forward. And yes, this indeed feels very specific to your ssh setup, version of ssh, and server configuration. Of course, this was only the beginning of figuring things out, because I had no idea how to get this working from one level deeper - an isolated compute node.Socket with Isolated NodesRemember that when we created the socket on the isolated node and we tried this out from the login node:$ nc -U /tmp/test.sockAnd the result was this:Ncat: Connection refused.My spidey senses were telling me that this should work. Indeed, when I ssh into the isolated node from the login node,that same command allowed me to connect (meaning it hung / there was no error output). So my first task, I decided, was to tryand “forward” this socket to the login node. Again, back to the man pages! I wound up with something like this (run from the login node):$ ssh isolated-node -NT -L /home/dinosaur/login-node.sock:/home/dinosaur/jupyter.sockThe above is again using -L but instead of a port (which aren’t exposed) we are using a socket! It’s kind of neat you can switch out those two. When I tried the same nc command from the loginnode, we had progress (no connection refused message!) 🎉️ And then I moved this up one level to see if I could make this same request from my local machine, sort of combining the first command that worked with the login node notebook with this one. That looked like this (and yes this took more trial and error):$ ssh -NT user@server ssh isolated-node -NT -L /home/dinosaur/login-node.sock:/home/dinosaur/jupyter.sockAnd to confirm it was working, I’d ssh into the server and again run that nc command to ensure that the newly forwarded socket was readable fromthe login node. After this, again with more trial and error, I tried running a second command to just forward that (now working socket) to my host.That eventually looked like this:# And another for the local socket$ ssh -NT -L 8899:/home/dinosaur/login-node.sock user@serverAnd then (all together now!) I tried putting them together.$ ssh -NT -L 8899:/home/dinosaur/login-node.sock user@server ssh isolated-node \\       -NT -L /home/dinosaur/login-node.sock:/home/dinosaur/jupyter.sockAnd then I spent some time integrating it into tunel, and surprise! the first implementation didn’t work. The first bug was that I needed to clean up old sockets each time the “same” app was run (determined by the job name and organizational namespace so the user can only run one of a particular interactive app at once, and not forget about previous runs). The second issue was about opening the tunnel - it didn’t seem to work if the process exited and/or it was run in a subshell (that also probably exits). I realized that (for the time being) running this connection step on behalf of the user, since it’s something the user should have more control over, probably wasn’t the right way to go. If the user hasn’t added something like an rsa key to ~/.ssh/authorized_keys on their clusters, it would also ask for a password interactively, making it harder for me to manage. So for simplicity sake, and assuming that we really should put the user in control of deciding when to start/stop the tunnel, I simply print the full ssh command in the terminal and let them copy paste it. A successful connection might then prompt them for their password for that second ssh, which (by default) I don’t think is carrying forward auth from the first.So that was my adventure! Mind you, this entire adventure was only about two days, and that included time to write this post, so I still have lots in front of me to work on. However, with these updated commands (and some nice tweaks from Python’s rich library) I quickly had a nice set of commands to run and stop an app with an interactive jupyter notebook, and using sockets on isolated nodes!$ tunel run-app server slurm/socket/jupyter$ tunel stop-app server slurm/socket/jupyterAs a sidenote, one thing I like about rich is that it puts the aesthetic as a first class citizen.So many tools just don’t consider this, and I love that with rich I can think about colors, presentation,and even animations like spinners!Getting a socket working  means I’ll be able to continue working on this library (hooray!) so if you have ideas or requests for appsyou’d like to run on HPC, assuming just this basic technology, please give me a ping and I’d love to chat and support them.I’m also going to be requesting an allocation on the Open Science Grid, which hopefully will give me other kinds of clustersto test on. I hope this was interesting to read, thanks for doing that!",
            "content_html": "<p>Today I want to talk about ssh tunnels. Very abstractly, we would want to use an sshtunnel to securely send information. In the case of HPC, you are probably familiar with ssh,(Secure Shell or Secure Socket Shell) when you login to your node. You might do something like this:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>ssh dinosaur@server.address.edu</code></pre></div></div><p>Or if you have a proper setup in your <code class=\"language-plaintext highlighter-rouge\">~/.ssh/config</code> (with a named server) you might just do:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>ssh dinosaur</code></pre></div></div><p>I like to use <a href=\"https://en.wikibooks.org/wiki/OpenSSH/Cookbook/Multiplexing\" target=\"_blank\">ssh connection multiplexing</a>so the connection is kept alive for a bit, but I won’t go into detail becausethis post isn’t specifically about the details of ssh. The use case I’m interested in (and the thingthat HPC is very bad at) is how to deploy something interactive on an HPC cluster.</p><h2 id=\"ssh-tunnel-with-ports\">SSH Tunnel with Ports</h2><p>Given that a cluster has exposed ports (either the login node, or both the login node and compute nodes)creating a tunnel is fairly straight forward! In the past I created a tool called <a href=\"https://github.com/vsoch/forward\" target=\"_blank\">forward</a> to handle all the manual steps to get this working, meaning:</p><ol class=\"custom-counter\">  <li>Show the user <a href=\"https://github.com/vsoch/forward#ssh-config\" target=\"_blank\">how to set up their ~/.ssh/config</a> (once)</li>  <li>Define (once) parameters like a port, memory, GPUs, and if the cluster has isolated nodes</li>  <li>Start any number of provided apps that come with forward (e.g., jupyter, singularity, etc.)</li></ol><p>An interaction using forward might look like any of the following:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\"># Run a Singularity container that already exists on your resource (recommended)</span>bash start-node.sh singularity-run /scratch/users/vsochat/share/pytorch-dev.simg<span class=\"c\"># Execute a custom command to the same Singularity container</span>bash start-node.sh singularity-exec /scratch/users/vsochat/share/pytorch-dev.simg <span class=\"nb\">echo</span> <span class=\"s2\">\"Hello World\"</span><span class=\"c\"># Run a Singularity container from a url, `docker://ubuntu`</span>bash start-node.sh singularity-run docker://ubuntu<span class=\"c\"># Execute a custom command to the same container</span>bash start-node.sh singularity-exec docker://ubuntu <span class=\"nb\">echo</span> <span class=\"s2\">\"Hello World\"</span><span class=\"c\"># To start a jupyter notebook in a specific directory ON the cluster resource</span>bash start.sh jupyter &lt;cluster-dir&gt;<span class=\"c\"># To start a jupyter notebook with tensorflow in a specific directory</span>bash start.sh py3-tensorflow &lt;cluster-dir&gt;</code></pre></div></div><p>Note that the last set of commands are pertaining to notebooks, which is where these tunnels come into play!A notebook is going to be run on a compute node that looks something like the following:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>jupyter notebook <span class=\"nt\">--no-browser</span> <span class=\"nt\">--port</span><span class=\"o\">=</span><span class=\"nv\">$PORT</span></code></pre></div></div><p>And if you ran this with a Singularity container, you’d also want to bind jovyan’s home to be the user’s, along with the jupyter config directory:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>singularity <span class=\"nb\">exec</span> <span class=\"nt\">--home</span> <span class=\"k\">${</span><span class=\"nv\">HOME</span><span class=\"k\">}</span> <span class=\"se\">\\</span>    <span class=\"nt\">--bind</span> <span class=\"k\">${</span><span class=\"nv\">HOME</span><span class=\"k\">}</span>/.local:/home/jovyan/.local <span class=\"se\">\\</span>    <span class=\"nt\">--bind</span> <span class=\"k\">${</span><span class=\"nv\">HOME</span><span class=\"k\">}</span>/.jupyter:/home/jovyan/.jupyter <span class=\"se\">\\ </span>     datascience_notebook.sif jupyter notebook <span class=\"nt\">--no-browser</span> <span class=\"nt\">--port</span><span class=\"o\">=</span><span class=\"nv\">$PORT</span> <span class=\"nt\">--ip</span> 0.0.0.0</code></pre></div></div><p>As we described earlier <a href=\"https://github.com/vsoch/forward#ssh-port-forwarding-considerations\" target=\"_blank\">here</a>,there are subtle differences between making a tunnel (with a port) given that you have isolated nodes (or not).You can determine this based on your ability to ssh into a non-login node (meaning where your job is running) from “the outside world”that is your computer. If you cannot, your nodes are isolated, which we will discuss next.</p><h3 id=\"isolated-nodes\">Isolated Nodes</h3><p>Let’s say that we need to create a tunnel (using ports) to an isolated node. This means that we are basically goingto establish a tunnel to the login node, and then from the login node another one to the compute node.We might use a command that looks like this:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>ssh <span class=\"nt\">-L</span> <span class=\"nv\">$PORT</span>:localhost:<span class=\"nv\">$PORT</span> <span class=\"k\">${</span><span class=\"nv\">RESOURCE</span><span class=\"k\">}</span> ssh <span class=\"nt\">-L</span> <span class=\"nv\">$PORT</span>:localhost:<span class=\"nv\">$PORT</span> <span class=\"nt\">-N</span> <span class=\"s2\">\"</span><span class=\"nv\">$MACHINE</span><span class=\"s2\">\"</span> &amp;</code></pre></div></div><p>In the command above, the first half (<code class=\"language-plaintext highlighter-rouge\">ssh -L $PORT:localhost:$PORT ${RESOURCE}</code>) is executed on the local machine, which establishes a port forwarding to the login node. The “-L” in the above (from the <a href=\"https://linuxcommand.org/lc3_man_pages/ssh1.html\" target=\"_blank\">man pages</a>) :</p><blockquote>  <p>Specifies that connections to the given TCP port or Unix socket on the local (client) host are to be forwarded to thegiven host and port, or Unix socket, on the remote side.This works by allocating a socket to listen to either a TCPport on the local side, optionally bound to the specifiedbind_address, or to a Unix socket.  Whenever a connection ismade to the local port or socket, the connection is for‐warded over the secure channel, and a connection is made toeither host port hostport, or the Unix socket remote_socket,from the remote machine.</p></blockquote><p>Or in layman’s terms:</p><blockquote>  <p>Forward whatever is running on the second port on my resource to my local machine.</p></blockquote><p>Since we are forwarding ports, this would require minimally the login node to expose ports.The next line <code class=\"language-plaintext highlighter-rouge\">ssh -L $PORT:localhost:$PORT -N \"$MACHINE\" &amp;</code> is a second command run from the login node, and port forwards it to the compute node, since you can only access the compute node from the login nodes.You’ll notice it looks just like the first, and this works because ssh commands can be chained.The <code class=\"language-plaintext highlighter-rouge\">-N</code> says “don’t execute a remote command (and just forward the port).”Finally, the last <code class=\"language-plaintext highlighter-rouge\">$MACHINE</code> is the node that the jupyter notebook is running on.</p><h3 id=\"not-isolated\">Not Isolated</h3><p>For HPCs where the compute node is not isolated from the outside world the ssh command for port forwarding first establishes a connection the login node, but then continues to pass on the login credentials to the compute node to establish a tunnel between the localhost and the port on the compute node. The ssh command in this case utilizes the flag <code class=\"language-plaintext highlighter-rouge\">-K</code> that forwards the login credentials to the compute node:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>ssh <span class=\"s2\">\"</span><span class=\"nv\">$DOMAINNAME</span><span class=\"s2\">\"</span> <span class=\"nt\">-l</span> <span class=\"nv\">$FORWARD_USERNAME</span> <span class=\"nt\">-K</span> <span class=\"nt\">-L</span>  <span class=\"nv\">$PORT</span>:<span class=\"nv\">$MACHINE</span>:<span class=\"nv\">$PORT</span> <span class=\"nt\">-N</span>  &amp;</code></pre></div></div><p>I’m not sure in practice how common this is anymore. At least at my current employer it’s not even the casethat ports are exposed on the login node! It’s probably better that way, because in cases where you do get ports it’s sort of a “pick a port above this range and hope that no other user picks the same one!” It’s messy. So let’s talk about the case of not having ports exposed next, since this was the entire reason I wanted to write this post!</p><h2 id=\"ssh-tunnel-with-socket\">SSH Tunnel with Socket</h2><p>More than a year ago, I had this realization that a lot of people at Stanford used the “forward” tool, and just for notebooks (and thiswas before they were available via Open OnDemand, which is what I’d recommend to a Stanford user at this point). I decided I wanted to make a new open source tool, “tunel” (an elegant derivation of “tunnel”) <a href=\"https://github.com/vsoch/tunel\" target=\"_blank\">vsoch/tunel</a> to make it easyto run what I call “apps” on an HPC cluster. Are there better ways of exposing user interfaces on HPC? Yes, indeed. But not everyonehas easy access. It was also a stubborn “I want this to work” proof of concept. This new tool would be like forward, but a little nicer.Because I, along with every other HPC developer and user, wishes we could have nice things 😭️.</p><p>At this time I had just started a new role at a national lab, and I realized that none of my old techniques for launchingthe job worked because of the lack of exposed ports. Thinking this was impossible, I abandoned it for a year. But then this last week I found <a href=\"https://github.com/jupyter/notebook/pull/4835\" target=\"_blank\">this</a>! I was motivated! I was excited! The basic launch command of the notebook looks like this:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>jupyter notebook <span class=\"nt\">--sock</span> /tmp/test.sock <span class=\"nt\">--no-browser</span></code></pre></div></div><p>And then with a different looking tunnel, we could forward this socket to the host, and map it to a port! My excitement was then brought downby what led to two days of struggling. I first tried my entire tunel workflow, meaning launching a job on a node,and then running that command, and providing the instruction to the user to create the tunnel as follows:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>ssh <span class=\"nt\">-L</span> 8888:/tmp/test.sock <span class=\"nt\">-N</span> user@this_host</code></pre></div></div><p>That didn’t work (and remember this socket was created on the isolated node, that’s important to remember for later). So I started looking at the socket with “nc”  - “arbitrary TCP and UDP connections and listens” from the login node. The “-U” below is for UNIX sockets:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>nc <span class=\"nt\">-U</span> /tmp/test.sock</code></pre></div></div><p>And from the head node I saw:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>Ncat: Connection refused.</code></pre></div></div><p>So then I knew I needed a simpler, dummier example. I got rid of tunel and just ran the notebook command on the head node.Dear reader, it still did not work. I <a href=\"https://github.com/jupyter/notebook/issues/6459\" target=\"_blank\">opened an issue</a> and asked <a href=\"https://twitter.com/vsoch/status/1540546526044250112\" target=\"_blank\">Twitter for help</a>. Someone else on Twitter reported that <a href=\"https://twitter.com/al3x609/status/1540846694262243328\" target=\"_blank\">it worked for them</a>, and that (in my opinion) is the challenge and story of HPC - given the huge differences in setups, it’s hard to reproduce what another person does unless you scope to a very specificenvironment or technology and hugely go out of your way to do it. I’m always grateful when someone tries to help, but when the ultimate answer is just“But it works on my machine!” I (and I think all of us) are like:</p><p><span style=\"font-size: 50px; color: darkorchid;\">(╯°□°)╯︵ ┻━┻</span></p><p>🤣️</p><p>Please know that is intended to be funny, and I really am grateful for the attempt to help! Anyway, the first night I was devastated because I was so excited about the possibility of this working! But of course (as it usually does) my quasi-sadness turned again into relentless stubborn-ness, and for my SaturdayI embarked on trying everything. I call this the stubborn brute force approach, and it actually leads to some pretty good outcomes?</p><h3 id=\"socket-from-login-node\">Socket from Login Node</h3><p>First from the login node, I started reading about flags in detail, again from the <a href=\"https://linuxcommand.org/lc3_man_pages/ssh1.html\" target=\"_blank\">man pages</a>. It occurred to me that the suggested command included “-L” (discussed earlier) but there were a ton of other flags to try, and maybe I need them for my setup? The command that wound up working (after much trial and error) was just:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\"># Running on login node</span><span class=\"nv\">$ </span>ssh <span class=\"nt\">-NT</span> <span class=\"nt\">-L</span> 8888:/tmp/test.sock user@server</code></pre></div></div><p>And here again was the suggested command:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>ssh <span class=\"nt\">-L</span> 8888:/tmp/test.sock <span class=\"nt\">-N</span> user@this_host</code></pre></div></div><p>So they are very similar - and the main difference is the <code class=\"language-plaintext highlighter-rouge\">-T</code> is to “Disable pseudo-terminal allocation.”So I suspect (also based on the version of ssl I’m using) that without the flag, you might be making a request for a pty to the server(<a href=\"https://stackoverflow.com/questions/10330678/gitolite-pty-allocation-request-failed-on-channel-0/10346575#10346575\" target=\"_blank\">more details here</a>) and then it could abort. Adding the flag just skips this, because we don’t need that - we just need the simple forward. And yes, this indeed feels very specific to your ssh setup, version of ssh, and server configuration. Of course, this was only the beginning of figuring things out, because I had no idea how to get this working from one level deeper - an isolated compute node.</p><h3 id=\"socket-with-isolated-nodes\">Socket with Isolated Nodes</h3><p>Remember that when we created the socket on the isolated node and we tried this out from the login node:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>nc <span class=\"nt\">-U</span> /tmp/test.sock</code></pre></div></div><p>And the result was this:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>Ncat: Connection refused.</code></pre></div></div><p>My spidey senses were telling me that this should work. Indeed, when I ssh into the isolated node from the login node,that same command allowed me to connect (meaning it hung / there was no error output). So my first task, I decided, was to tryand “forward” this socket to the login node. Again, back to the man pages! I wound up with something like this (run from the login node):</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>ssh isolated-node <span class=\"nt\">-NT</span> <span class=\"nt\">-L</span> /home/dinosaur/login-node.sock:/home/dinosaur/jupyter.sock</code></pre></div></div><p>The above is again using <code class=\"language-plaintext highlighter-rouge\">-L</code> but instead of a port (which aren’t exposed) we are using a socket! It’s kind of neat you can switch out those two. When I tried the same nc command from the loginnode, we had progress (no connection refused message!) 🎉️ And then I moved this up one level to see if I could make this same request from my local machine, sort of combining the first command that worked with the login node notebook with this one. That looked like this (and yes this took more trial and error):</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>ssh <span class=\"nt\">-NT</span> user@server ssh isolated-node <span class=\"nt\">-NT</span> <span class=\"nt\">-L</span> /home/dinosaur/login-node.sock:/home/dinosaur/jupyter.sock</code></pre></div></div><p>And to confirm it was working, I’d ssh into the server and again run that nc command to ensure that the newly forwarded socket was readable fromthe login node. After this, again with more trial and error, I tried running a second command to just forward that (now working socket) to my host.That eventually looked like this:</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\"># And another for the local socket</span><span class=\"nv\">$ </span>ssh <span class=\"nt\">-NT</span> <span class=\"nt\">-L</span> 8899:/home/dinosaur/login-node.sock user@server</code></pre></div></div><p>And then (all together now!) I tried putting them together.</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>ssh <span class=\"nt\">-NT</span> <span class=\"nt\">-L</span> 8899:/home/dinosaur/login-node.sock user@server ssh isolated-node <span class=\"se\">\\</span>       <span class=\"nt\">-NT</span> <span class=\"nt\">-L</span> /home/dinosaur/login-node.sock:/home/dinosaur/jupyter.sock</code></pre></div></div><p>And then I spent some time integrating it into tunel, and <em>surprise!</em> the first implementation didn’t work. The first bug was that I needed to clean up old sockets each time the “same” app was run (determined by the job name and organizational namespace so the user can only run one of a particular interactive app at once, and not forget about previous runs). The second issue was about opening the tunnel - it didn’t seem to work if the process exited and/or it was run in a subshell (that also probably exits). I realized that (for the time being) running this connection step on behalf of the user, since it’s something the user should have more control over, probably wasn’t the right way to go. If the user hasn’t added something like an rsa key to <code class=\"language-plaintext highlighter-rouge\">~/.ssh/authorized_keys</code> on their clusters, it would also ask for a password interactively, making it harder for me to manage. So for simplicity sake, and assuming that we really should put the user in control of deciding when to start/stop the tunnel, I simply print the full ssh command in the terminal and let them copy paste it. A successful connection might then prompt them for their password for that second ssh, which (by default) I don’t think is carrying forward auth from the first.</p><p>So that was my adventure! Mind you, this entire adventure was only about two days, and that included time to write this post, so I still have lots in front of me to work on. However, with these updated commands (and some nice tweaks from Python’s <a href=\"https://github.com/Textualize/rich\" target=\"_blank\">rich</a> library) I quickly had a nice set of commands to run and stop an app with an interactive jupyter notebook, and using sockets on isolated nodes!</p><div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>tunel run-app server slurm/socket/jupyter<span class=\"nv\">$ </span>tunel stop-app server slurm/socket/jupyter</code></pre></div></div><p>As a sidenote, one thing I like about rich is that it puts the aesthetic as a first class citizen.So many tools just don’t consider this, and I love that with rich I can think about colors, presentation,and even animations like spinners!</p><p>Getting a socket working  means I’ll be able to continue working on this library (hooray!) so if you have ideas or requests for appsyou’d like to run on HPC, assuming just this basic technology, please give me a ping and I’d love to chat and support them.I’m also going to be requesting an allocation on the Open Science Grid, which hopefully will give me other kinds of clustersto test on. I hope this was interesting to read, thanks for doing that!</p>",
            "url": "https://hpc.social/personal-blog/2022/ssh-tunnels/",
            
            
            
            
            
            "date_published": "2022-06-26T13:30:00-06:00",
            "date_modified": "2022-06-26T13:30:00-06:00",
            
                "author": "Vanessasaurus"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/research-software-registries/",
            "title": "Research Software Registries",
            "summary": null,
            "content_text": "This post spurred from some original thinking about research software registries, and my recent discovery of the SciCodes Consortium, which I’m excited to find (and a bit surprised I didn’t earlier given my experience with research software and registries)! Since I’ve developed registries and been involved extensively in communities that develop standards and tooling for them, I’ve naturally been ruminating over ideas for several months, and hoping to find others that are motivated to think about similar things. This is the motivation of this post - to ruminate, share my thinking, and think together about ideas. You can read the content, or listen to the ideas below.vsoch · Research Software RegistriesWhy do we want research software registries?Research software registries have value when they are deployed for a specific context. However,I’m not convinced that a research software registry, at the most basic form providing archives with DOIS and metadata, is a useful thing in and of itself. It’s adding complexity and redundancy to an already cluttered ecosystem. The reason is because the source of truth of software is usually the source code in version control, e.g., the GitHub repository, which often already has support for features we need to enable easy citation (CITATION.cff), tagged releases, and programmatically accessible metadata. In this context, any kind of registry that provides another identifier and points to the first is providing redundant information. The only potential benefit is grouping and curation, which I would then argue should still point to the version control and/or a specific release as a source of truth.I’m also not convinced that we have established an actual use case of “searching a registry for software.” What happens in labs and communities is that you establish communities around the software, and then there are established workflows or slack communities or GitHub organizations to join around that. Most labs already have chosen languages, and even software pipelines that new members extend or work on. I would even go as far to say that for some (myself included) I don’t find research software, but it finds me. It appears as a link in some social media or chat channel, and I click the link and then there are about 15 seconds during which I make a determination if the software can help me to solve a problem that I have, or if it looks easy, professional, and/or fun and I simply want to try it out. If the answer is “yes” then I add it to a list in a Google Document with other things to try out when I have time. If not, I close the lab and life moves on. But I want to point out that nowhere in this workflow do I explicitly go looking for software. The software often finds me, and then I keep a mental cache of “tools that I’ve seen” and go back to it when the use case arises.So being able to answer this question about wanting research software registries is especially challenging because I’m not sure I’ve ever wanted one.Unless there is a specific kind of context around a registry (e.g., search for a specific name in a package manager to use, or look for an already assembled workflow) I haven’t been able to convince myself (yet) that I would find a use for one. I could be wrong about this, however, because as we know, people (myself included) are fairly bad at predicting the future, and perhaps there could be some future where “checking a research software registry” is a part of a daily workflow. I am skeptical because I think that a context is needed. Even if some central source of software ability truth was established, would it not be the case that a graduate student or researcher needs to go there with a use case or context in mind? I can’t imagine just mindlessly browsing for the sake of browsing. It’s akin to search engines - we are usually looking for something very specific. We don’t search without a purpose. The question here then is, what is the purpose?Research Software Registries with a PurposeA very good example of purpose comes down to workflows. This is the “I need to perform this specific function and I want to use what many others have done before me and not re-invent the wheel.” The minimum example of a workflow registry would be a search interface that indexes pipelines that are perhaps stored in version control. And extended version of that includes being able to provide structured inputs, outputs, and arguments, so the registry can programmatically provide this information to tools. You can then also quickly see how changing this to be general inputs/outputs of software (and functions within) and entrypoints of containers can quickly become a more generalized registry for software that could be used by any workflow manager that knows how to consume its information. However, there is a fine line here, because when we talk about I/O we are goingsquarely into workflow management territoty, and again in my opinion, we have to be careful about that scope. The closest thing that comes to mind for providing workflows as a service is something like openneuro that has a beautiful idea of “Get your data into this standard format and we will serve it and provide other easy ways to analyze it.” This kind of success story tells me that perhaps there is something to say for developing anything related to processing or pipelines in the context of a community. You can’t create the perfect registry for every scientific discipline, or perhaps you can do a mediocre job at trying, but perhaps if you scope to a specific one you can do a very good job. I’ve found the same to be true with software - it’s often better to do one or few things very well than more things kind of mediocre.A Provider of Identifiers?I’m skeptical when I hear that people want to apply our traditional model of publication (e.g., having a DOI) to software. The reason isn’t because I don’t value means to support reproducibility (and knowing the exact version of something that was used) but rather that we already have means to tag specific versions of software, and means that fit into a well-established ecosystem: package managers, versions, and releases. To think that a single frozen version of software is “the correct unit to provide” I also disagree with. Software is a living, and changing entity, and when it truly does “freeze” and stops being worked on, unlike a DOI in the academic space, this is sort of its death. The correct entrypoint for a piece of software, in my opinion, is the current version on version control, from where you could decide to pin a particular release or install a particular version from a package manager. But to provide a single frozen DOI that is wrapping some other version / release of the software? It doesn’t make sense to me. It’s adding additional complexity that’s not needed. So my opinion (as I’ve shared before) is that we should be thinking more about preserving specific timepoints in package managers, and not adding on an artificially created layer of “DOI” that seems (in my opinion) more of a reflection of our need to shove things into an academic framework we are comfortable with than anything else.So (I hope) that the purpose of a research software registry would not just be to provide DOIs. That doesn’t help me get my work done at the end of the day. All that said, I don’t think there can be a singular answer for purpose. I think the purpose ultimately comes down to the institution (or community) and the specific goals of the registry. For this reason there is no one answer for what a registry should look like or provide, and it is (or will be) challenging to define attributes that “any registry should have.”What is my purpose?You cut butter!Just kidding :_) I’ve been ruminating on this idea for quite some time, and namely because I’m motivated to build a new kind of research software registry, but first I need to convince myself of a meaningful purpose. While I don’t have my convincing answer yet (but I do have a sense of direction) the way I’ve been thinking about this is to provide a set of questions or use cases that seem plausible. It seems like most people are asking “What kind of information should we have in a registry” but I think this isn’t exactly the question I’m interested in - I want to know:  What do you want to do next with the software you find?This is important because it’s going to drive the context and purpose of the registry. Here are a few examples:  I want to quickly try this out → a registry that can deploy a developer environment  I want to find if this is in a package manager → a reproducible install  I want to use this with a workflow manager → this is some kind of workflow hub    I want to see inputs / outputs / entrypoints → support to workflow tools    I want to install this on HPC → I want a module deployment or similar    I want to cite this → use case akin to CITATION.cff    I want to understand dependencies of an ecosystem → a registry deploying something akin to citelang    I want to see all my options to do X → a domain or categorical registry    I want to see new and noteworthy libraries → a registry with advanced filtering and ranking    I want to see change over time → a registry with a layer of analysis tools  Indeed many of the above contexts require additional information. For example, if we want to be able to ask what software is specifically used to perform X, we need a set of functions that are common to a domain, and then to annotate specific software (or even functions) that do it. If we want to then ask “Which of these is the best?” we need to then generate benchmarks to measure this functionality. E.g., how long does it take to run? What are the inputs and outputs and are they correct? What are resource needs? It would be an incredibly cool thing to be able to ask these questions, but an enormous amount of work for any particular scientific domain. As an example of thinking about functional needs, we might look to brain imaging, which is arguably a subfield of neuroinformatics. We might define custom processing functions like thresholding, registration, normalization, or creating regions of interest, tag specific functions that can do each, and then collect and share metrics about the degree to which easy is successful to do each. Arguably, if I wanted to do this I would create wrappers to workflow managers (akin to Snakemake Wrappers) that not only measure metrics, but make it easy for people to quickly use it in their work.It needs to be easyWhether I’m thinking about being a user of a research software registry or a developer, it just needs to be easy. Here are some ideas around that.Re-inventing the wheel?I come with the experience of deploying a custom container registry (Singularity Hub) years ago, and then being involved in standards committees (the Open Container Initiative) that develop generalized specifications that now drive the most common software (container) registries. I’ve also developed registry proxies that do interesting things, along with a Python OCI registry, and I’m the main developer for oras-py (ORAS == OCI Registry as Storage). So believe me when I say that in terms of storing blobs and metadata about them, I don’t think we should re-invent the wheel. Any new registry I create is going to start with these standards. You might disagree, and that’s OK. But I think people have thought long and hard about these things, and we are stronger for working together on them over always making our own new thing.As a supplement to that, I want to point out one of the biggest challenges in our community. The majority of research software, I would argue, doesn’t get used beyond the lab it’s created for. Said lab might submit or include it in a paper, and then they get their publication and move on. This is reflective of many things, and I’ll review them here. The first is our funding model - we maybe can fund working on a piece of software only up until the funding dries out, and then it becomes an abandoned repository, if it’s made publicly available. The second is our incentive model - the academic community is focused on writing papers, so once you get there, you don’t have reason to consider the long term impact of the software. The third is communication. It is actually much easier to throw together your own library than to have to search and then try contributing to someone else’s.I say this because I don’t think the way that things are are necessarily the fault of anyone - we are all agents responding to incentives and resources available.But then on the flip side - these observations beg to ask what leads to software that is successful, on a community level? I think a few things can happen. Either someone puts time and energy into establishing community, period, meaning bringing together people that are working on common goals and explicitly asking “How can we do this together,” or what I’ve seen with more commercial open source - having enough money or power that you can create strong branding and community just by way of having the funds for it.  I’ve talked about this a few times before and it’s not necessarily bad, but it’s unfair at best. Software that maybe would not be successful by its own merit rises to the top, and really great software that doesn’t have those resources does not. That said, I’ve also seen sort of mediocre software get much better and earn its reputation, so I can’t say it’s a completely wrong dynamic.Is the answer Mooooar Metadata?As we design the “perfect set of information” we want provided for any piece of software, we need to put people first.We have to ask ourselves what are people willing to do, and generally people aren’t wanting to spend inordinate amounts of extra time defining metadata or inputs/outputs for their custom scripts. This was a point also brought up by Paula in the SciCodes meeting and I am 100% behind it. If we require extensive metadata about software, it needs to be done in an automated fashion. In practice when I think of archives for software, I’m just not that motivated to provide more than the absolute minimum to click the “submit” button.Do people know what they want?One of the hardest things about this kind of problem is that people don’t often know what they want. And actually - I’d extend that to software in general. Think of common tools like git (version control) or containers.Could most people have told you in advance about the designs for these tools? I suspect likely not.This is often the game that software developers play - we imagine new ways of doing things that scratch an itchor have a problem that we have, and then hand over our duct taped laden prototype to others and we’re  likehey, is this useful to you? And often the response in radio silence, but then sometimes it’s a resounding, “WoW, yes!”So I’m going to throw out this idea that people generally don’t know what they want until they see it, touch it and try it.This is also why I want to inspire you to take some time to think about your specific needs and motivation for wanting(on a high level) to browse and interact with research software. What are the compelling reasons for this registry,for you?This is actually really fun to think about, because what even is a research software registry? Is it a place to find software to plug into workflows? Does it provide ABI or more general function signatures to help you plug into workflows? Does it provide a citation? A container? An interactive environment? Dependency graph? Something else? This is inded why this problem is so hard - there are so many ways to thinkabout this basic concept. And that’s kind of what makes it fun too? But also what makes it hard. Personally speaking sinceI’m more interested in building things I find myself ruminating about details for a specific use case. And since I’m a developer and craving better support for developer environments, this tends to be where my brain goes. And have you noticed I haven’t givena direct answer for what is a research software registry? It’s 1. because I don’t know, and 2. because we are trying to define a registry for a kind of output that we don’t even have an agreed upon definition for yet! So perhaps the definition will happen on the level of the deployment or institution? Anyway, I hope you take the opportunity to discuss with your peers, pets, and even yourself, to try and answer this question.SummaryTo summarize, I’m spending a lot of time thinking about this, and less in an “I’m an academic that wants DOIs and metadata” and more in a “I am a software engineer that wants to build something that I actually find useful.” Might I scratch itches along the way? Sure. And I do have some early ideas that I plan to hack on before sharing publicly. In the meantime, I do hope you are interested in some of these ideas and take time to write or introspect yourself.And on a higher level, I really like this format of writing and speaking, where the speaking isn’t formal enough to be a talk that you put together and practice for weeks (I put this all together in an afternoon) but it still is a media format that literally gives a voice.",
            "content_html": "<p>This post spurred from some original thinking about <a href=\"https://rse-ops.github.io/proposals/proposals/drafts/research-software-registry/\" target=\"_blank\">research software registries</a>, and my recent discovery of the <a href=\"https://scicodes.net/\" target=\"_blank\">SciCodes Consortium</a>, which I’m excited to find (and a bit surprised I didn’t earlier given my experience with research software and registries)! Since I’ve developed registries and been involved extensively in communities that develop standards and tooling for them, I’ve naturally been ruminating over ideas for several months, and hoping to find others that are motivated to think about similar things. This is the motivation of this post - to ruminate, share my thinking, and think together about ideas. You can read the content, or listen to the ideas below.</p><div style=\"font-size: 10px; color: #cccccc; overflow: hidden; white-space: nowrap; font-family: Interstate,Lucida Grande,Lucida Sans Unicode,Lucida Sans,Garuda,Verdana,Tahoma,sans-serif; font-weight: 100;\"><a href=\"https://soundcloud.com/vsoch\" style=\"color: #cccccc; text-decoration: none;\" target=\"_blank\" title=\"vsoch\">vsoch</a> · <a href=\"https://soundcloud.com/vsoch/research-software-registries\" style=\"color: #cccccc; text-decoration: none;\" target=\"_blank\" title=\"Research Software Registries\">Research Software Registries</a></div><h2 id=\"why-do-we-want-research-software-registries\">Why do we want research software registries?</h2><p>Research software registries have value when they are deployed for a specific context. However,I’m not convinced that a research software registry, at the most basic form providing archives with DOIS and metadata, is a useful thing in and of itself. It’s adding complexity and redundancy to an already cluttered ecosystem. The reason is because the source of truth of software is usually the source code in version control, e.g., the GitHub repository, which often already has support for features we need to enable easy citation (CITATION.cff), tagged releases, and programmatically accessible metadata. In this context, any kind of registry that provides another identifier and points to the first is providing redundant information. The only potential benefit is grouping and curation, which I would then argue should still point to the version control and/or a specific release as a source of truth.</p><p>I’m also not convinced that we have established an actual use case of “searching a registry for software.” What happens in labs and communities is that you establish communities around the software, and then there are established workflows or slack communities or GitHub organizations to join around that. Most labs already have chosen languages, and even software pipelines that new members extend or work on. I would even go as far to say that for some (myself included) I don’t find research software, but it finds me. It appears as a link in some social media or chat channel, and I click the link and then there are about 15 seconds during which I make a determination if the software can help me to solve a problem that I have, or if it looks easy, professional, and/or fun and I simply want to try it out. If the answer is “yes” then I add it to a list in a Google Document with other things to try out when I have time. If not, I close the lab and life moves on. But I want to point out that nowhere in this workflow do I explicitly go looking for software. The software often finds me, and then I keep a mental cache of “tools that I’ve seen” and go back to it when the use case arises.</p><p>So being able to answer this question about wanting research software registries is especially challenging because I’m not sure I’ve ever wanted one.Unless there is a specific kind of context around a registry (e.g., search for a specific name in a package manager to use, or look for an already assembled workflow) I haven’t been able to convince myself (yet) that I would find a use for one. I could be wrong about this, however, because as we know, people (myself included) are fairly bad at predicting the future, and perhaps there could be some future where “checking a research software registry” is a part of a daily workflow. I am skeptical because I think that a context is needed. Even if some central source of software ability truth was established, would it not be the case that a graduate student or researcher needs to go there with a use case or context in mind? I can’t imagine just mindlessly browsing for the sake of browsing. It’s akin to search engines - we are usually looking for something very specific. We don’t search without a purpose. The question here then is, what is the purpose?</p><h2 id=\"research-software-registries-with-a-purpose\">Research Software Registries with a Purpose</h2><p>A very good example of purpose comes down to workflows. This is the “I need to perform this specific function and I want to use what many others have done before me and not re-invent the wheel.” The minimum example of a workflow registry would be a search interface that indexes pipelines that are perhaps stored in version control. And extended version of that includes being able to provide structured inputs, outputs, and arguments, so the registry can programmatically provide this information to tools. You can then also quickly see how changing this to be general inputs/outputs of software (and functions within) and entrypoints of containers can quickly become a more generalized registry for software that could be used by any workflow manager that knows how to consume its information. However, there is a fine line here, because when we talk about I/O we are goingsquarely into workflow management territoty, and again in my opinion, we have to be careful about that scope. The closest thing that comes to mind for providing workflows as a service is something like <a href=\"https://openneuro.org/\" target=\"_blank\">openneuro</a> that has a beautiful idea of “Get your data into this standard format and we will serve it and provide other easy ways to analyze it.” This kind of success story tells me that perhaps there is something to say for developing anything related to processing or pipelines in the context of a community. You can’t create the perfect registry for every scientific discipline, or perhaps you can do a mediocre job at trying, but perhaps if you scope to a specific one you can do a very good job. I’ve found the same to be true with software - it’s often better to do one or few things very well than more things kind of mediocre.</p><h3 id=\"a-provider-of-identifiers\">A Provider of Identifiers?</h3><p>I’m skeptical when I hear that people want to apply our traditional model of publication (e.g., having a DOI) to software. The reason isn’t because I don’t value means to support reproducibility (and knowing the exact version of something that was used) but rather that we already have means to tag specific versions of software, and means that fit into a well-established ecosystem: package managers, versions, and releases. To think that a single frozen version of software is “the correct unit to provide” I also disagree with. Software is a living, and changing entity, and when it truly does “freeze” and stops being worked on, unlike a DOI in the academic space, this is sort of its death. The correct entrypoint for a piece of software, in my opinion, is the current version on version control, from where you could decide to pin a particular release or install a particular version from a package manager. But to provide a single frozen DOI that is wrapping some other version / release of the software? It doesn’t make sense to me. It’s adding additional complexity that’s not needed. So my opinion (as I’ve shared before) is that we should be thinking more about preserving specific timepoints in package managers, and not adding on an artificially created layer of “DOI” that seems (in my opinion) more of a reflection of our need to shove things into an academic framework we are comfortable with than anything else.</p><p>So (I hope) that the purpose of a research software registry would not just be to provide DOIs. That doesn’t help me get my work done at the end of the day. All that said, I don’t think there can be a singular answer for purpose. I think the purpose ultimately comes down to the institution (or community) and the specific goals of the registry. For this reason there is no one answer for what a registry should look like or provide, and it is (or will be) challenging to define attributes that “any registry should have.”</p><h3 id=\"what-is-my-purpose\">What is my purpose?</h3><p><em>You cut butter</em>!</p><p>Just kidding :_) I’ve been ruminating on this idea for quite some time, and namely because I’m motivated to build a new kind of research software registry, but first I need to convince myself of a meaningful purpose. While I don’t have my convincing answer yet (but I do have a sense of direction) the way I’ve been thinking about this is to provide a set of questions or use cases that seem plausible. It seems like most people are asking “What kind of information should we have in a registry” but I think this isn’t exactly the question I’m interested in - I want to know:</p><blockquote>  <p>What do you want to do next with the software you find?</p></blockquote><p>This is important because it’s going to drive the context and purpose of the registry. Here are a few examples:</p><ol class=\"custom-counter\">  <li><strong>I want to quickly try this out</strong> → a registry that can deploy a developer environment</li>  <li><strong>I want to find if this is in a package manager</strong> → a reproducible install</li>  <li><strong>I want to use this with a workflow manager</strong> → this is some kind of workflow hub</li>    <li><strong>I want to see inputs / outputs / entrypoints</strong> → support to workflow tools</li>    <li><strong>I want to install this on HPC</strong> → I want a module deployment or similar</li>    <li><strong>I want to cite this</strong> → use case akin to CITATION.cff</li>    <li><strong>I want to understand dependencies of an ecosystem</strong> → a registry deploying something akin to citelang</li>    <li><strong>I want to see all my options to do X</strong> → a domain or categorical registry</li>    <li><strong>I want to see new and noteworthy libraries</strong> → a registry with advanced filtering and ranking</li>    <li><strong>I want to see change over time</strong> → a registry with a layer of analysis tools</li>  </ol><p>Indeed many of the above contexts require additional information. For example, if we want to be able to ask what software is specifically used to perform X, we need a set of functions that are common to a domain, and then to annotate specific software (or even functions) that do it. If we want to then ask “Which of these is the best?” we need to then generate benchmarks to measure this functionality. E.g., how long does it take to run? What are the inputs and outputs and are they correct? What are resource needs? It would be an incredibly cool thing to be able to ask these questions, but an enormous amount of work for any particular scientific domain. As an example of thinking about functional needs, we might look to brain imaging, which is arguably a subfield of neuroinformatics. We might define custom processing functions like thresholding, registration, normalization, or creating regions of interest, tag specific functions that can do each, and then collect and share metrics about the degree to which easy is successful to do each. Arguably, if I wanted to do this I would create wrappers to workflow managers (akin to Snakemake Wrappers) that not only measure metrics, but make it easy for people to quickly use it in their work.</p><h2 id=\"it-needs-to-be-easy\">It needs to be easy</h2><p>Whether I’m thinking about being a user of a research software registry or a developer, it just needs to be easy. Here are some ideas around that.</p><h3 id=\"re-inventing-the-wheel\">Re-inventing the wheel?</h3><p>I come with the experience of deploying a custom container registry (Singularity Hub) years ago, and then being involved in standards committees (the Open Container Initiative) that develop generalized specifications that now drive the most common software (container) registries. I’ve also developed registry proxies that do interesting things, along with a Python OCI registry, and I’m the main developer for oras-py (ORAS == OCI Registry as Storage). So believe me when I say that in terms of storing blobs and metadata about them, I don’t think we should re-invent the wheel. Any new registry I create is going to start with these standards. You might disagree, and that’s OK. But I think people have thought long and hard about these things, and we are stronger for working together on them over always making our own new thing.</p><p>As a supplement to that, I want to point out one of the biggest challenges in our community. The majority of research software, I would argue, doesn’t get used beyond the lab it’s created for. Said lab might submit or include it in a paper, and then they get their publication and move on. This is reflective of many things, and I’ll review them here. The first is our funding model - we maybe can fund working on a piece of software only up until the funding dries out, and then it becomes an abandoned repository, if it’s made publicly available. The second is our incentive model - the academic community is focused on writing papers, so once you get there, you don’t have reason to consider the long term impact of the software. The third is communication. It is actually much easier to throw together your own library than to have to search and then try contributing to someone else’s.I say this because I don’t think the way that things are are necessarily the fault of anyone - we are all agents responding to incentives and resources available.</p><p>But then on the flip side - these observations beg to ask what leads to software that is successful, on a community level? I think a few things can happen. Either someone puts time and energy into establishing community, period, meaning bringing together people that are working on common goals and explicitly asking “How can we do this together,” or what I’ve seen with more commercial open source - having enough money or power that you can create strong branding and community just by way of having the funds for it.  I’ve talked about this a <a href=\"https://vsoch.github.io/2019/transparency/\" target=\"_blank\">few times before</a> and it’s not necessarily bad, but it’s unfair at best. Software that maybe would not be successful by its own merit rises to the top, and really great software that doesn’t have those resources does not. That said, I’ve also seen sort of mediocre software get much better and earn its reputation, so I can’t say it’s a completely wrong dynamic.</p><h3 id=\"is-the-answer-mooooar-metadata\">Is the answer Mooooar Metadata?</h3><p>As we design the “perfect set of information” we want provided for any piece of software, we need to put people first.We have to ask ourselves what are people willing to do, and generally people aren’t wanting to spend inordinate amounts of extra time defining metadata or inputs/outputs for their custom scripts. This was a point also brought up by <a href=\"https://twitter.com/orchid00\" target=\"_blank\">Paula</a> in the SciCodes meeting and I am 100% behind it. If we require extensive metadata about software, it needs to be done in an automated fashion. In practice when I think of archives for software, I’m just not that motivated to provide more than the absolute minimum to click the “submit” button.</p><h2 id=\"do-people-know-what-they-want\">Do people know what they want?</h2><p>One of the hardest things about this kind of problem is that people don’t often know what they want. And actually - I’d extend that to software in general. Think of common tools like git (version control) or containers.Could most people have told you in advance about the designs for these tools? I suspect likely not.This is often the game that software developers play - we imagine new ways of doing things that scratch an itchor have a problem that we have, and then hand over our duct taped laden prototype to others and we’re  likehey, is this useful to you? And often the response in radio silence, but then sometimes it’s a resounding, “WoW, yes!”So I’m going to throw out this idea that people generally don’t know what they want until they see it, touch it and try it.This is also why I want to inspire you to take some time to think about your specific needs and motivation for wanting(on a high level) to browse and interact with research software. What are the compelling reasons for this registry,for you?</p><p>This is actually really fun to think about, because what even is a research software registry? Is it a place to find software to plug into workflows? Does it provide ABI or more general function signatures to help you plug into workflows? Does it provide a citation? A container? An interactive environment? Dependency graph? Something else? This is inded why this problem is so hard - there are so many ways to thinkabout this basic concept. And that’s kind of what makes it fun too? But also what makes it hard. Personally speaking sinceI’m more interested in building things I find myself ruminating about details for a specific use case. And since I’m a developer and craving better support for developer environments, this tends to be where my brain goes. And have you noticed I haven’t givena direct answer for what is a research software registry? It’s 1. because I don’t know, and 2. because we are trying to define a registry for a kind of output that we don’t even have an agreed upon definition for yet! So perhaps the definition will happen on the level of the deployment or institution? Anyway, I hope you take the opportunity to discuss with your peers, pets, and even yourself, to try and answer this question.</p><h2 id=\"summary\">Summary</h2><p>To summarize, I’m spending a lot of time thinking about this, and less in an “I’m an academic that wants DOIs and metadata” and more in a “I am a software engineer that wants to build something that I actually find useful.” Might I scratch itches along the way? Sure. And I do have some early ideas that I plan to hack on before sharing publicly. In the meantime, I do hope you are interested in some of these ideas and take time to write or introspect yourself.</p><p>And on a higher level, I really like this format of writing and speaking, where the speaking isn’t formal enough to be a talk that you put together and practice for weeks (I put this all together in an afternoon) but it still is a media format that literally gives a voice.</p>",
            "url": "https://hpc.social/personal-blog/2022/research-software-registries/",
            
            
            
            
            
            "date_published": "2022-06-19T13:15:00-06:00",
            "date_modified": "2022-06-19T13:15:00-06:00",
            
                "author": "Vanessasaurus"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/mnt-reform-2-part-deux/",
            "title": "MNT Reform 2 - part deux",
            "summary": null,
            "content_text": "A few days back I posted some of my initial thoughts of the MNT Reform 2 laptop which justrecently arrived. I ran the usual battery of tests on the laptop including the High PerformanceLinpack (HPL) of course just for kicks.At that time, I made no attempt to optmize HPL. I simply went with the OS supplied gcc andmath libraries. My next step was to look at how I could improve my HPL result usingthe Arm compiler for Linux and the Arm performance libraries. Here I&rsquo;ll walk through thosesteps from installing the Arm tools, to compiling and running HPL - and all of the smalldetails in between.(1) To start, I downloaded the latest verion of the Arm compiler for Linux package from here.This was the package with the filename: arm-compiler-for-linux_22.0.2_Ubuntu-20.04_aarch64.tar.(2) After uncompressing arm-compiler-for-linux_22.0.2_Ubuntu-20.04_aarch64.tar, I ran theinstallation command ./arm-compiler-for-linux_22.0.2_Ubuntu-20.04.sh -a which installed thesoftware to /opt/arm on the system. Note that the Arm compilers for Linux ship withmodule files to make setting up the envionment for compiling easy. To support thisI had to install the OS environment-modules package with apt-get install environment-modules(3) In order to load the module for the Arm compiler for Linux, the following steps arenecessary. This assumes that the Arm compiler for Linux is installed in /opt/arm.    root@reform:/# module avail----------------------------------- /usr/share/modules/modulefiles ------------------------------------dot  module-git  module-info  modules  null  use.own  Key:modulepath  root@reform:/# export MODULEPATH=/opt/arm/modulefiles:$MODULEPATHroot@reform:/# module avail---------------------------------------- /opt/arm/modulefiles -----------------------------------------acfl/22.0.2  binutils/11.2.0  gnu/11.2.0  ----------------------------------- /usr/share/modules/modulefiles ------------------------------------dot  module-git  module-info  modules  null  use.own  Key:modulepath  root@reform:/# module load acfl/22.0.2Loading acfl/22.0.2  Loading requirement: binutils/11.2.0root@reform:/# echo $PATH/opt/arm/arm-linux-compiler-22.0.2_Generic-AArch64_Ubuntu-20.04_aarch64-linux/bin:/opt/arm/gcc-11.2.0_Generic-AArch64_Ubuntu-20.04_aarch64-linux/binutils_bin:/root/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/binroot@reform:/# armclang --versionArm C/C++/Fortran Compiler version 22.0.2 (build number 1776) (based on LLVM 13.0.0)Target: aarch64-unknown-linux-gnuThread model: posixInstalledDir: /opt/arm/arm-linux-compiler-22.0.2_Generic-AArch64_Ubuntu-20.04_aarch64-linux/bin(4) Now we shift our focus to Open MPI. Open MPI is an open source distribution of the messagepassing interface (MPI) library for writing parallel applications. We will compile HPL against thisOpen MPI version. For this, I downloaded the latest Open MPI version (4.1.4) from here.By default, Open MPI compiles with support for the SLURM workload manager. My Reform hasIBM Spectrum LSF installed as the workload scheduler. In order to enable LSF support in Open MPI, weneed to specify the appropriate configure flags (see below).root@reform:/opt/HPC/openmpi-4.1.4# ./configure --prefix=/opt/HPC/openmpi-4.1.4 --with-lsf=/opt/ibm/lsf/10.1 --with-lsf-libdir=/opt/ibm/lsf/10.1/linux3.12-glibc2.17-armv8/libroot@reform:/opt/HPC/openmpi-4.1.4# make -j 4......root@reform:/opt/HPC/openmpi-4.1.4# make install......(5) After completing the compilation of Open MPI, the ompi_info command is run to check ifsupport for LSF has been enabled. Note that you must ensure to source the LSF environment(i.e. . ./profile.lsf) before running ompi_info or the LSF libraries won&rsquo;t be found.root@reform:/opt/HPC/openmpi-4.1.4# ./bin/ompi_info |grep -i lsf  Configure command line: '--prefix=/opt/HPC/openmpi-4.1.4' '--with-lsf=/opt/ibm/lsf/10.1' '--with-lsf-libdir=/opt/ibm/lsf/10.1/linux3.12-glibc2.17-armv8/lib'                 MCA ess: lsf (MCA v2.1.0, API v3.0.0, Component v4.1.4)                 MCA plm: lsf (MCA v2.1.0, API v2.0.0, Component v4.1.4)                 MCA ras: lsf (MCA v2.1.0, API v2.0.0, Component v4.1.4)(6) Next, I downloaded the latest HPL package from here.I uncompressed the the package hpl-2.3.tar.gz in the /opt/HPC directory. Next, I had to createa new Makefile for HPL which would use the Arm compiler for Linux and optmized math libraries.A copy of Make.imx8qm follows below.#  #  -- High Performance Computing Linpack Benchmark (HPL)                #     HPL - 2.3 - December 2, 2018                          #     Antoine P. Petitet                                                #     University of Tennessee, Knoxville                                #     Innovative Computing Laboratory                                 #     (C) Copyright 2000-2008 All Rights Reserved                       #                                                                       #  -- Copyright notice and Licensing terms:                             #                                                                       #  Redistribution  and  use in  source and binary forms, with or without#  modification, are  permitted provided  that the following  conditions#  are met:                                                             #                                                                       #  1. Redistributions  of  source  code  must retain the above copyright#  notice, this list of conditions and the following disclaimer.        #                                                                       #  2. Redistributions in binary form must reproduce  the above copyright#  notice, this list of conditions,  and the following disclaimer in the#  documentation and/or other materials provided with the distribution. #                                                                       #  3. All  advertising  materials  mentioning  features  or  use of this#  software must display the following acknowledgement:                 #  This  product  includes  software  developed  at  the  University  of#  Tennessee, Knoxville, Innovative Computing Laboratory.             #                                                                       #  4. The name of the  University,  the name of the  Laboratory,  or the#  names  of  its  contributors  may  not  be used to endorse or promote#  products  derived   from   this  software  without  specific  written#  permission.                                                          #                                                                       #  -- Disclaimer:                                                       #                                                                       #  THIS  SOFTWARE  IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS#  ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES,  INCLUDING,  BUT NOT#  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR#  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE UNIVERSITY#  OR  CONTRIBUTORS  BE  LIABLE FOR ANY  DIRECT,  INDIRECT,  INCIDENTAL,#  SPECIAL,  EXEMPLARY,  OR  CONSEQUENTIAL DAMAGES  (INCLUDING,  BUT NOT#  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,#  DATA OR PROFITS; OR BUSINESS INTERRUPTION)  HOWEVER CAUSED AND ON ANY#  THEORY OF LIABILITY, WHETHER IN CONTRACT,  STRICT LIABILITY,  OR TORT#  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE#  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. # #######################################################################  # ----------------------------------------------------------------------# - shell --------------------------------------------------------------# ----------------------------------------------------------------------#SHELL        = /bin/sh#CD           = cdCP           = cpLN_S         = ln -sMKDIR        = mkdirRM           = /bin/rm -fTOUCH        = touch## ----------------------------------------------------------------------# - Platform identifier ------------------------------------------------# ----------------------------------------------------------------------#ARCH         = imx8qm## ----------------------------------------------------------------------# - HPL Directory Structure / HPL library ------------------------------# ----------------------------------------------------------------------#TOPdir       = /opt/HPC/hpl-2.3INCdir       = /opt/HPC/hpl-2.3/includeBINdir       = /opt/HPC/hpl-2.3/bin/$(ARCH)LIBdir       = /opt/HPC/hpl-2.3/lib/$(ARCH)#HPLlib       = /opt/HPC/hpl-2.3/lib/libhpl.a ## ----------------------------------------------------------------------# - Message Passing library (MPI) --------------------------------------# ----------------------------------------------------------------------# MPinc tells the  C  compiler where to find the Message Passing library# header files,  MPlib  is defined  to be the name of  the library to be# used. The variable MPdir is only used for defining MPinc and MPlib.#MPdir        = /opt/HPC/openmpi-4.1.4MPinc        = /opt/HPC/openmpi-4.1.4/includeMPlib        = /opt/HPC/openmpi-4.1.4/lib/libmpi.so## ----------------------------------------------------------------------# - Linear Algebra library (BLAS or VSIPL) -----------------------------# ----------------------------------------------------------------------# LAinc tells the  C  compiler where to find the Linear Algebra  library# header files,  LAlib  is defined  to be the name of  the library to be# used. The variable LAdir is only used for defining LAinc and LAlib.#LAdir        =LAinc        =# LAlib        = -lamath -lm -mcpu=nativeLAlib        = ## ----------------------------------------------------------------------# - F77 / C interface --------------------------------------------------# ----------------------------------------------------------------------# You can skip this section  if and only if  you are not planning to use# a  BLAS  library featuring a Fortran 77 interface.  Otherwise,  it  is# necessary  to  fill out the  F2CDEFS  variable  with  the  appropriate# options.  **One and only one**  option should be chosen in **each** of# the 3 following categories:## 1) name space (How C calls a Fortran 77 routine)## -DAdd_              : all lower case and a suffixed underscore  (Suns,#                       Intel, ...),                           [default]# -DNoChange          : all lower case (IBM RS6000),# -DUpCase            : all upper case (Cray),# -DAdd__             : the FORTRAN compiler in use is f2c.## 2) C and Fortran 77 integer mapping## -DF77_INTEGER=int   : Fortran 77 INTEGER is a C int,         [default]# -DF77_INTEGER=long  : Fortran 77 INTEGER is a C long,# -DF77_INTEGER=short : Fortran 77 INTEGER is a C short.## 3) Fortran 77 string handling## -DStringSunStyle    : The string address is passed at the string loca-#                       tion on the stack, and the string length is then#                       passed as  an  F77_INTEGER  after  all  explicit#                       stack arguments,                       [default]# -DStringStructPtr   : The address  of  a  structure  is  passed  by  a#                       Fortran 77  string,  and the structure is of the#                       form: struct {char *cp; F77_INTEGER len;},# -DStringStructVal   : A structure is passed by value for each  Fortran#                       77 string,  and  the  structure is  of the form:#                       struct {char *cp; F77_INTEGER len;},# -DStringCrayStyle   : Special option for  Cray  machines,  which  uses#                       Cray  fcd  (fortran  character  descriptor)  for#                       interoperation.#F2CDEFS      = ## ----------------------------------------------------------------------# - HPL includes / libraries / specifics -------------------------------# ----------------------------------------------------------------------#HPL_INCLUDES = -I$(INCdir) -I$(INCdir)/$(ARCH) $(LAinc) -I$(MPinc) -I/opt/arm/armpl-22.0.2_AArch64_Ubuntu-20.04_gcc_aarch64-linux/include/HPL_LIBS     = $(HPLlib) $(LAlib) $(MPlib)## - Compile time options -----------------------------------------------## -DHPL_COPY_L           force the copy of the panel L before bcast;# -DHPL_CALL_CBLAS       call the cblas interface;# -DHPL_CALL_VSIPL       call the vsip  library;# -DHPL_DETAILED_TIMING  enable detailed timers;## By default HPL will:#    *) not copy L before broadcast,#    *) call the BLAS Fortran 77 interface,#    *) not display detailed timing information.#HPL_OPTS     =## ----------------------------------------------------------------------#HPL_DEFS     = $(F2CDEFS) $(HPL_OPTS) $(HPL_INCLUDES)## ----------------------------------------------------------------------# - Compilers / linkers - Optimization flags ---------------------------# ----------------------------------------------------------------------#CC           = armclang CCNOOPT      = $(HPL_DEFS)CCFLAGS      = $(HPL_DEFS) -O3 -larmpl_lp64 -lamath -lm #LINKER       = armclang -O3 -armpl -lamath -lm LINKFLAGS    = $(CCFLAGS)#ARCHIVER     = arARFLAGS      = rRANLIB       = echo## ----------------------------------------------------------------------(7) To compile HPL with the above Makefile is as simple as running the appropriate make command andspecify the architecture imx8qm.root@reform:/opt/HPC/hpl-2.3# make arch=imx8qm......(8) Barring any errors, we should now have an xhpl binary in under the /opt/HPC/hpl-2.3/bin/imx8qmdirectory.root@reform:/opt/HPC/hpl-2.3/bin/imx8qm# pwd/opt/HPC/hpl-2.3/bin/imx8qmroot@reform:/opt/HPC/hpl-2.3/bin/imx8qm# ls -latotal 156drwxr-xr-x 2 root root   4096 Jun  8 13:30 .drwxr-xr-x 3 root root   4096 Jun  8 13:20 ..-rw-r--r-- 1 root root   1454 Jun  8 13:30 HPL.dat-rwxr-xr-x 1 root root 146960 Jun  8 13:24 xhplroot@reform:/opt/HPC/hpl-2.3/bin/imx8qm# ldd ./xhpl\tlinux-vdso.so.1 (0x0000007faa7b1000)\tlibamath_aarch64.so =&gt; /opt/arm/arm-linux-compiler-22.0.2_Generic-AArch64_Ubuntu-20.04_aarch64-linux/llvm-bin/../lib/libamath_aarch64.so (0x0000007faa5ef000)\tlibm.so.6 =&gt; /lib/aarch64-linux-gnu/libm.so.6 (0x0000007faa520000)\tlibarmpl_lp64.so =&gt; /opt/arm/arm-linux-compiler-22.0.2_Generic-AArch64_Ubuntu-20.04_aarch64-linux/lib/clang/13.0.0/armpl_links/lib/libarmpl_lp64.so (0x0000007fa3cd5000)\tlibmpi.so.40 =&gt; /usr/lib/aarch64-linux-gnu/libmpi.so.40 (0x0000007fa3b8f000)\tlibarmflang.so =&gt; /opt/arm/arm-linux-compiler-22.0.2_Generic-AArch64_Ubuntu-20.04_aarch64-linux/llvm-bin/../lib/libarmflang.so (0x0000007fa3728000)\tlibomp.so =&gt; /opt/arm/arm-linux-compiler-22.0.2_Generic-AArch64_Ubuntu-20.04_aarch64-linux/llvm-bin/../lib/libomp.so (0x0000007fa3649000)\tlibrt.so.1 =&gt; /lib/aarch64-linux-gnu/librt.so.1 (0x0000007fa3631000)\tlibdl.so.2 =&gt; /lib/aarch64-linux-gnu/libdl.so.2 (0x0000007fa361d000)\tlibpthread.so.0 =&gt; /lib/aarch64-linux-gnu/libpthread.so.0 (0x0000007fa35ed000)\tlibastring_aarch64.so =&gt; /opt/arm/arm-linux-compiler-22.0.2_Generic-AArch64_Ubuntu-20.04_aarch64-linux/llvm-bin/../lib/libastring_aarch64.so (0x0000007fa35da000)\tlibc.so.6 =&gt; /lib/aarch64-linux-gnu/libc.so.6 (0x0000007fa345f000)\t/lib/ld-linux-aarch64.so.1 (0x0000007faa77e000)\tlibgcc_s.so.1 =&gt; /opt/arm/gcc-11.2.0_Generic-AArch64_Ubuntu-20.04_aarch64-linux/lib64/libgcc_s.so.1 (0x0000007fa343a000)\tlibopen-rte.so.40 =&gt; /usr/lib/aarch64-linux-gnu/libopen-rte.so.40 (0x0000007fa336c000)\tlibopen-pal.so.40 =&gt; /usr/lib/aarch64-linux-gnu/libopen-pal.so.40 (0x0000007fa32aa000)\tlibhwloc.so.15 =&gt; /usr/lib/aarch64-linux-gnu/libhwloc.so.15 (0x0000007fa3245000)\tlibstdc++.so.6 =&gt; /opt/arm/gcc-11.2.0_Generic-AArch64_Ubuntu-20.04_aarch64-linux/lib64/libstdc++.so.6 (0x0000007fa3030000)\tlibz.so.1 =&gt; /lib/aarch64-linux-gnu/libz.so.1 (0x0000007fa3006000)\tlibevent_core-2.1.so.7 =&gt; /usr/lib/aarch64-linux-gnu/libevent_core-2.1.so.7 (0x0000007fa2fbf000)\tlibutil.so.1 =&gt; /lib/aarch64-linux-gnu/libutil.so.1 (0x0000007fa2fab000)\tlibevent_pthreads-2.1.so.7 =&gt; /usr/lib/aarch64-linux-gnu/libevent_pthreads-2.1.so.7 (0x0000007fa2f98000)\tlibudev.so.1 =&gt; /usr/lib/aarch64-linux-gnu/libudev.so.1 (0x0000007fa2f5e000)(9) A default HPL.dat file should ber present in the directory /opt/HPC/hpl-2.3/bin/imx8qm. The fileHPL.dat is used to tune the benchmark problem size according to the system. A copy of theHPL.dat file I created follows below. This is suitable for the 4 GB memory configuration ofReform with 4 processor cores.HPLinpack benchmark input fileInnovative Computing Laboratory, University of TennesseeHPL.out      output file name (if any) 6            device out (6=stdout,7=stderr,file)1            # of problems sizes (N)19000         Ns1            # of NBs192           NBs0            PMAP process mapping (0=Row-,1=Column-major)1            # of process grids (P x Q)2            Ps2            Qs16.0         threshold1            # of panel fact2            PFACTs (0=left, 1=Crout, 2=Right)1            # of recursive stopping criterium4            NBMINs (&gt;= 1)1            # of panels in recursion2            NDIVs1            # of recursive panel fact.1            RFACTs (0=left, 1=Crout, 2=Right)1            # of broadcast1            BCASTs (0=1rg,1=1rM,2=2rg,3=2rM,4=Lng,5=LnM)1            # of lookahead depth1            DEPTHs (&gt;=0)2            SWAP (0=bin-exch,1=long,2=mix)64           swapping threshold0            L1 in (0=transposed,1=no-transposed) form0            U  in (0=transposed,1=no-transposed) form1            Equilibration (0=no,1=yes)8            memory alignment in double (&gt; 0)##### This line (no. 32) is ignored (it serves as a separator). ######0                               Number of additional problem sizes for PTRANS1200 10000 30000                values of N0                               number of additional blocking sizes for PTRANS40 9 8 13 13 20 16 32 64        values of NB(10) Now we&rsquo;re ready to execute the appropriate mpirun command to run the xhpl executable.We specify -np 4 to run across the 4 cores of the processor. With this better optimized run we&rsquo;reseeing ~8.9 GFLOPS performance compared with ~4 GFLOPS for my previous runs where HPL was compiledwith the OS supplied GCC and Math libraries (ATLAS). Note that as this is roughly double the GFLOPSfrom my previous runs, it appears that there is an issue with double precision or perhapsvectorization with the non-optimized runs.gsamu@reform:/opt/HPC/hpl-2.3/bin/imx8qm$ mpirun -np 4 ./xhpl ================================================================================HPLinpack 2.3  --  High-Performance Linpack benchmark  --   December 2, 2018Written by A. Petitet and R. Clint Whaley,  Innovative Computing Laboratory, UTKModified by Piotr Luszczek, Innovative Computing Laboratory, UTKModified by Julien Langou, University of Colorado Denver================================================================================An explanation of the input/output parameters follows:T/V    : Wall time / encoded variant.N      : The order of the coefficient matrix A.NB     : The partitioning blocking factor.P      : The number of process rows.Q      : The number of process columns.Time   : Time in seconds to solve the linear system.Gflops : Rate of execution for solving the linear system.The following parameter values will be used:N      :   19000 NB     :     192 PMAP   : Row-major process mappingP      :       2 Q      :       2 PFACT  :   Right NBMIN  :       4 NDIV   :       2 RFACT  :   Crout BCAST  :  1ringM DEPTH  :       1 SWAP   : Mix (threshold = 64)L1     : transposed formU      : transposed formEQUIL  : yesALIGN  : 8 double precision words--------------------------------------------------------------------------------- The matrix A is randomly generated for each test.- The following scaled residual check will be computed:      ||Ax-b||_oo / ( eps * ( || x ||_oo * || A ||_oo + || b ||_oo ) * N )- The relative machine precision (eps) is taken to be               1.110223e-16- Computational tests pass if scaled residuals are less than                16.0================================================================================T/V                N    NB     P     Q               Time                 Gflops--------------------------------------------------------------------------------WR11C2R4       19000   192     2     2             513.92             8.8987e+00HPL_pdgesv() start time Wed Jun  8 21:28:07 2022HPL_pdgesv() end time   Wed Jun  8 21:36:41 2022--------------------------------------------------------------------------------||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   4.89711678e-03 ...... PASSED================================================================================Finished      1 tests with the following results:              1 tests completed and passed residual checks,              0 tests completed and failed residual checks,              0 tests skipped because of illegal input values.--------------------------------------------------------------------------------End of Tests.================================================================================(11) Finally, we submit the same run of Linpack but through Spectrum LSF. The LSF bsub commandinvocation is shown below and the resulting output.gsamu@reform:~$ bsub -n 4 -I -m reform \"cd /opt/HPC/hpl-2.3/bin/imx8qm ; mpirun ./xhpl\" Job &lt;35301&gt; is submitted to default queue &lt;interactive&gt;.&lt;&lt;Waiting for dispatch ...&gt;&gt;&lt;&lt;Starting on reform&gt;&gt;================================================================================HPLinpack 2.3  --  High-Performance Linpack benchmark  --   December 2, 2018Written by A. Petitet and R. Clint Whaley,  Innovative Computing Laboratory, UTKModified by Piotr Luszczek, Innovative Computing Laboratory, UTKModified by Julien Langou, University of Colorado Denver================================================================================An explanation of the input/output parameters follows:T/V    : Wall time / encoded variant.N      : The order of the coefficient matrix A.NB     : The partitioning blocking factor.P      : The number of process rows.Q      : The number of process columns.Time   : Time in seconds to solve the linear system.Gflops : Rate of execution for solving the linear system.The following parameter values will be used:N      :   19000 NB     :     192 PMAP   : Row-major process mappingP      :       2 Q      :       2 PFACT  :   Right NBMIN  :       4 NDIV   :       2 RFACT  :   Crout BCAST  :  1ringM DEPTH  :       1 SWAP   : Mix (threshold = 64)L1     : transposed formU      : transposed formEQUIL  : yesALIGN  : 8 double precision words--------------------------------------------------------------------------------- The matrix A is randomly generated for each test.- The following scaled residual check will be computed:      ||Ax-b||_oo / ( eps * ( || x ||_oo * || A ||_oo + || b ||_oo ) * N )- The relative machine precision (eps) is taken to be               1.110223e-16- Computational tests pass if scaled residuals are less than                16.0================================================================================T/V                N    NB     P     Q               Time                 Gflops--------------------------------------------------------------------------------WR11C2R4       19000   192     2     2             518.02             8.8283e+00HPL_pdgesv() start time Thu Jun  9 09:33:35 2022HPL_pdgesv() end time   Thu Jun  9 09:42:13 2022--------------------------------------------------------------------------------||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   4.89711678e-03 ...... PASSED================================================================================Finished      1 tests with the following results:              1 tests completed and passed residual checks,              0 tests completed and failed residual checks,              0 tests skipped because of illegal input values.--------------------------------------------------------------------------------End of Tests.================================================================================",
            "content_html": "<p>A few days back I posted some of my <a href=\"https://www.gaborsamu.com/blog/neunundneunzig_reform/\">initial thoughts</a> of the MNT Reform 2 laptop which justrecently arrived. I ran the usual battery of tests on the laptop including the High PerformanceLinpack (HPL) of course just for kicks.</p><p>At that time, I made no attempt to optmize HPL. I simply went with the OS supplied gcc andmath libraries. My next step was to look at how I could improve my HPL result usingthe Arm compiler for Linux and the Arm performance libraries. Here I&rsquo;ll walk through thosesteps from installing the Arm tools, to compiling and running HPL - and all of the smalldetails in between.</p><p>(1) To start, I downloaded the latest verion of the Arm compiler for Linux package from <a href=\"https://developer.arm.com/Tools%20and%20Software/Arm%20Compiler%20for%20Linux\">here</a>.This was the package with the filename: <em>arm-compiler-for-linux_22.0.2_Ubuntu-20.04_aarch64.tar</em>.</p><p>(2) After uncompressing <em>arm-compiler-for-linux_22.0.2_Ubuntu-20.04_aarch64.tar</em>, I ran theinstallation command <em>./arm-compiler-for-linux_22.0.2_Ubuntu-20.04.sh -a</em> which installed thesoftware to <em>/opt/arm</em> on the system. <strong>Note</strong> that the Arm compilers for Linux ship withmodule files to make setting up the envionment for compiling easy. To support thisI had to install the OS environment-modules package with <em>apt-get install environment-modules</em></p><p>(3) In order to load the module for the Arm compiler for Linux, the following steps arenecessary. This assumes that the Arm compiler for Linux is installed in <em>/opt/arm</em>.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">    root@reform:/# module avail----------------------------------- /usr/share/modules/modulefiles ------------------------------------dot  module-git  module-info  modules  null  use.own  Key:modulepath  root@reform:/# export MODULEPATH=/opt/arm/modulefiles:$MODULEPATHroot@reform:/# module avail---------------------------------------- /opt/arm/modulefiles -----------------------------------------acfl/22.0.2  binutils/11.2.0  gnu/11.2.0  ----------------------------------- /usr/share/modules/modulefiles ------------------------------------dot  module-git  module-info  modules  null  use.own  Key:modulepath  root@reform:/# module load acfl/22.0.2Loading acfl/22.0.2  Loading requirement: binutils/11.2.0root@reform:/# echo $PATH/opt/arm/arm-linux-compiler-22.0.2_Generic-AArch64_Ubuntu-20.04_aarch64-linux/bin:/opt/arm/gcc-11.2.0_Generic-AArch64_Ubuntu-20.04_aarch64-linux/binutils_bin:/root/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/binroot@reform:/# armclang --versionArm C/C++/Fortran Compiler version 22.0.2 (build number 1776) (based on LLVM 13.0.0)Target: aarch64-unknown-linux-gnuThread model: posixInstalledDir: /opt/arm/arm-linux-compiler-22.0.2_Generic-AArch64_Ubuntu-20.04_aarch64-linux/bin</code></pre></div><p>(4) Now we shift our focus to Open MPI. Open MPI is an open source distribution of the messagepassing interface (MPI) library for writing parallel applications. We will compile HPL against thisOpen MPI version. For this, I downloaded the latest Open MPI version (4.1.4) from <a href=\"https://www.netlib.org/benchmark/hpl/\">here</a>.</p><p>By default, Open MPI compiles with support for the SLURM workload manager. My Reform hasIBM Spectrum LSF installed as the workload scheduler. In order to enable LSF support in Open MPI, weneed to specify the appropriate configure flags (see below).</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">root@reform:/opt/HPC/openmpi-4.1.4# ./configure --prefix=/opt/HPC/openmpi-4.1.4 --with-lsf=/opt/ibm/lsf/10.1 --with-lsf-libdir=/opt/ibm/lsf/10.1/linux3.12-glibc2.17-armv8/libroot@reform:/opt/HPC/openmpi-4.1.4# make -j 4......root@reform:/opt/HPC/openmpi-4.1.4# make install......</code></pre></div><p>(5) After completing the compilation of Open MPI, the <em>ompi_info</em> command is run to check ifsupport for LSF has been enabled. Note that you must ensure to source the LSF environment(i.e. . ./profile.lsf) before running <em>ompi_info</em> or the LSF libraries won&rsquo;t be found.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">root@reform:/opt/HPC/openmpi-4.1.4# ./bin/ompi_info |grep -i lsf  Configure command line: '--prefix=/opt/HPC/openmpi-4.1.4' '--with-lsf=/opt/ibm/lsf/10.1' '--with-lsf-libdir=/opt/ibm/lsf/10.1/linux3.12-glibc2.17-armv8/lib'                 MCA ess: lsf (MCA v2.1.0, API v3.0.0, Component v4.1.4)                 MCA plm: lsf (MCA v2.1.0, API v2.0.0, Component v4.1.4)                 MCA ras: lsf (MCA v2.1.0, API v2.0.0, Component v4.1.4)</code></pre></div><p>(6) Next, I downloaded the latest HPL package from <a href=\"https://www.netlib.org/benchmark/hpl/\">here</a>.I uncompressed the the package <em>hpl-2.3.tar.gz</em> in the /opt/HPC directory. Next, I had to createa new Makefile for HPL which would use the Arm compiler for Linux and optmized math libraries.A copy of <em>Make.imx8qm</em> follows below.</p><!-- raw HTML omitted --><div class=\"highlight\"><pre><code class=\"language-plaintext\">#  #  -- High Performance Computing Linpack Benchmark (HPL)                #     HPL - 2.3 - December 2, 2018                          #     Antoine P. Petitet                                                #     University of Tennessee, Knoxville                                #     Innovative Computing Laboratory                                 #     (C) Copyright 2000-2008 All Rights Reserved                       #                                                                       #  -- Copyright notice and Licensing terms:                             #                                                                       #  Redistribution  and  use in  source and binary forms, with or without#  modification, are  permitted provided  that the following  conditions#  are met:                                                             #                                                                       #  1. Redistributions  of  source  code  must retain the above copyright#  notice, this list of conditions and the following disclaimer.        #                                                                       #  2. Redistributions in binary form must reproduce  the above copyright#  notice, this list of conditions,  and the following disclaimer in the#  documentation and/or other materials provided with the distribution. #                                                                       #  3. All  advertising  materials  mentioning  features  or  use of this#  software must display the following acknowledgement:                 #  This  product  includes  software  developed  at  the  University  of#  Tennessee, Knoxville, Innovative Computing Laboratory.             #                                                                       #  4. The name of the  University,  the name of the  Laboratory,  or the#  names  of  its  contributors  may  not  be used to endorse or promote#  products  derived   from   this  software  without  specific  written#  permission.                                                          #                                                                       #  -- Disclaimer:                                                       #                                                                       #  THIS  SOFTWARE  IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS#  ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES,  INCLUDING,  BUT NOT#  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR#  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE UNIVERSITY#  OR  CONTRIBUTORS  BE  LIABLE FOR ANY  DIRECT,  INDIRECT,  INCIDENTAL,#  SPECIAL,  EXEMPLARY,  OR  CONSEQUENTIAL DAMAGES  (INCLUDING,  BUT NOT#  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,#  DATA OR PROFITS; OR BUSINESS INTERRUPTION)  HOWEVER CAUSED AND ON ANY#  THEORY OF LIABILITY, WHETHER IN CONTRACT,  STRICT LIABILITY,  OR TORT#  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE#  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. # #######################################################################  # ----------------------------------------------------------------------# - shell --------------------------------------------------------------# ----------------------------------------------------------------------#SHELL        = /bin/sh#CD           = cdCP           = cpLN_S         = ln -sMKDIR        = mkdirRM           = /bin/rm -fTOUCH        = touch## ----------------------------------------------------------------------# - Platform identifier ------------------------------------------------# ----------------------------------------------------------------------#ARCH         = imx8qm## ----------------------------------------------------------------------# - HPL Directory Structure / HPL library ------------------------------# ----------------------------------------------------------------------#TOPdir       = /opt/HPC/hpl-2.3INCdir       = /opt/HPC/hpl-2.3/includeBINdir       = /opt/HPC/hpl-2.3/bin/$(ARCH)LIBdir       = /opt/HPC/hpl-2.3/lib/$(ARCH)#HPLlib       = /opt/HPC/hpl-2.3/lib/libhpl.a ## ----------------------------------------------------------------------# - Message Passing library (MPI) --------------------------------------# ----------------------------------------------------------------------# MPinc tells the  C  compiler where to find the Message Passing library# header files,  MPlib  is defined  to be the name of  the library to be# used. The variable MPdir is only used for defining MPinc and MPlib.#MPdir        = /opt/HPC/openmpi-4.1.4MPinc        = /opt/HPC/openmpi-4.1.4/includeMPlib        = /opt/HPC/openmpi-4.1.4/lib/libmpi.so## ----------------------------------------------------------------------# - Linear Algebra library (BLAS or VSIPL) -----------------------------# ----------------------------------------------------------------------# LAinc tells the  C  compiler where to find the Linear Algebra  library# header files,  LAlib  is defined  to be the name of  the library to be# used. The variable LAdir is only used for defining LAinc and LAlib.#LAdir        =LAinc        =# LAlib        = -lamath -lm -mcpu=nativeLAlib        = ## ----------------------------------------------------------------------# - F77 / C interface --------------------------------------------------# ----------------------------------------------------------------------# You can skip this section  if and only if  you are not planning to use# a  BLAS  library featuring a Fortran 77 interface.  Otherwise,  it  is# necessary  to  fill out the  F2CDEFS  variable  with  the  appropriate# options.  **One and only one**  option should be chosen in **each** of# the 3 following categories:## 1) name space (How C calls a Fortran 77 routine)## -DAdd_              : all lower case and a suffixed underscore  (Suns,#                       Intel, ...),                           [default]# -DNoChange          : all lower case (IBM RS6000),# -DUpCase            : all upper case (Cray),# -DAdd__             : the FORTRAN compiler in use is f2c.## 2) C and Fortran 77 integer mapping## -DF77_INTEGER=int   : Fortran 77 INTEGER is a C int,         [default]# -DF77_INTEGER=long  : Fortran 77 INTEGER is a C long,# -DF77_INTEGER=short : Fortran 77 INTEGER is a C short.## 3) Fortran 77 string handling## -DStringSunStyle    : The string address is passed at the string loca-#                       tion on the stack, and the string length is then#                       passed as  an  F77_INTEGER  after  all  explicit#                       stack arguments,                       [default]# -DStringStructPtr   : The address  of  a  structure  is  passed  by  a#                       Fortran 77  string,  and the structure is of the#                       form: struct {char *cp; F77_INTEGER len;},# -DStringStructVal   : A structure is passed by value for each  Fortran#                       77 string,  and  the  structure is  of the form:#                       struct {char *cp; F77_INTEGER len;},# -DStringCrayStyle   : Special option for  Cray  machines,  which  uses#                       Cray  fcd  (fortran  character  descriptor)  for#                       interoperation.#F2CDEFS      = ## ----------------------------------------------------------------------# - HPL includes / libraries / specifics -------------------------------# ----------------------------------------------------------------------#HPL_INCLUDES = -I$(INCdir) -I$(INCdir)/$(ARCH) $(LAinc) -I$(MPinc) -I/opt/arm/armpl-22.0.2_AArch64_Ubuntu-20.04_gcc_aarch64-linux/include/HPL_LIBS     = $(HPLlib) $(LAlib) $(MPlib)## - Compile time options -----------------------------------------------## -DHPL_COPY_L           force the copy of the panel L before bcast;# -DHPL_CALL_CBLAS       call the cblas interface;# -DHPL_CALL_VSIPL       call the vsip  library;# -DHPL_DETAILED_TIMING  enable detailed timers;## By default HPL will:#    *) not copy L before broadcast,#    *) call the BLAS Fortran 77 interface,#    *) not display detailed timing information.#HPL_OPTS     =## ----------------------------------------------------------------------#HPL_DEFS     = $(F2CDEFS) $(HPL_OPTS) $(HPL_INCLUDES)## ----------------------------------------------------------------------# - Compilers / linkers - Optimization flags ---------------------------# ----------------------------------------------------------------------#CC           = armclang CCNOOPT      = $(HPL_DEFS)CCFLAGS      = $(HPL_DEFS) -O3 -larmpl_lp64 -lamath -lm #LINKER       = armclang -O3 -armpl -lamath -lm LINKFLAGS    = $(CCFLAGS)#ARCHIVER     = arARFLAGS      = rRANLIB       = echo## ----------------------------------------------------------------------</code></pre></div><!-- raw HTML omitted --><p>(7) To compile HPL with the above Makefile is as simple as running the appropriate <em>make</em> command andspecify the architecture <em>imx8qm</em>.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">root@reform:/opt/HPC/hpl-2.3# make arch=imx8qm......</code></pre></div><p>(8) Barring any errors, we should now have an <em>xhpl</em> binary in under the /opt/HPC/hpl-2.3/bin/imx8qmdirectory.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">root@reform:/opt/HPC/hpl-2.3/bin/imx8qm# pwd/opt/HPC/hpl-2.3/bin/imx8qmroot@reform:/opt/HPC/hpl-2.3/bin/imx8qm# ls -latotal 156drwxr-xr-x 2 root root   4096 Jun  8 13:30 .drwxr-xr-x 3 root root   4096 Jun  8 13:20 ..-rw-r--r-- 1 root root   1454 Jun  8 13:30 HPL.dat-rwxr-xr-x 1 root root 146960 Jun  8 13:24 xhplroot@reform:/opt/HPC/hpl-2.3/bin/imx8qm# ldd ./xhpl\tlinux-vdso.so.1 (0x0000007faa7b1000)\tlibamath_aarch64.so =&gt; /opt/arm/arm-linux-compiler-22.0.2_Generic-AArch64_Ubuntu-20.04_aarch64-linux/llvm-bin/../lib/libamath_aarch64.so (0x0000007faa5ef000)\tlibm.so.6 =&gt; /lib/aarch64-linux-gnu/libm.so.6 (0x0000007faa520000)\tlibarmpl_lp64.so =&gt; /opt/arm/arm-linux-compiler-22.0.2_Generic-AArch64_Ubuntu-20.04_aarch64-linux/lib/clang/13.0.0/armpl_links/lib/libarmpl_lp64.so (0x0000007fa3cd5000)\tlibmpi.so.40 =&gt; /usr/lib/aarch64-linux-gnu/libmpi.so.40 (0x0000007fa3b8f000)\tlibarmflang.so =&gt; /opt/arm/arm-linux-compiler-22.0.2_Generic-AArch64_Ubuntu-20.04_aarch64-linux/llvm-bin/../lib/libarmflang.so (0x0000007fa3728000)\tlibomp.so =&gt; /opt/arm/arm-linux-compiler-22.0.2_Generic-AArch64_Ubuntu-20.04_aarch64-linux/llvm-bin/../lib/libomp.so (0x0000007fa3649000)\tlibrt.so.1 =&gt; /lib/aarch64-linux-gnu/librt.so.1 (0x0000007fa3631000)\tlibdl.so.2 =&gt; /lib/aarch64-linux-gnu/libdl.so.2 (0x0000007fa361d000)\tlibpthread.so.0 =&gt; /lib/aarch64-linux-gnu/libpthread.so.0 (0x0000007fa35ed000)\tlibastring_aarch64.so =&gt; /opt/arm/arm-linux-compiler-22.0.2_Generic-AArch64_Ubuntu-20.04_aarch64-linux/llvm-bin/../lib/libastring_aarch64.so (0x0000007fa35da000)\tlibc.so.6 =&gt; /lib/aarch64-linux-gnu/libc.so.6 (0x0000007fa345f000)\t/lib/ld-linux-aarch64.so.1 (0x0000007faa77e000)\tlibgcc_s.so.1 =&gt; /opt/arm/gcc-11.2.0_Generic-AArch64_Ubuntu-20.04_aarch64-linux/lib64/libgcc_s.so.1 (0x0000007fa343a000)\tlibopen-rte.so.40 =&gt; /usr/lib/aarch64-linux-gnu/libopen-rte.so.40 (0x0000007fa336c000)\tlibopen-pal.so.40 =&gt; /usr/lib/aarch64-linux-gnu/libopen-pal.so.40 (0x0000007fa32aa000)\tlibhwloc.so.15 =&gt; /usr/lib/aarch64-linux-gnu/libhwloc.so.15 (0x0000007fa3245000)\tlibstdc++.so.6 =&gt; /opt/arm/gcc-11.2.0_Generic-AArch64_Ubuntu-20.04_aarch64-linux/lib64/libstdc++.so.6 (0x0000007fa3030000)\tlibz.so.1 =&gt; /lib/aarch64-linux-gnu/libz.so.1 (0x0000007fa3006000)\tlibevent_core-2.1.so.7 =&gt; /usr/lib/aarch64-linux-gnu/libevent_core-2.1.so.7 (0x0000007fa2fbf000)\tlibutil.so.1 =&gt; /lib/aarch64-linux-gnu/libutil.so.1 (0x0000007fa2fab000)\tlibevent_pthreads-2.1.so.7 =&gt; /usr/lib/aarch64-linux-gnu/libevent_pthreads-2.1.so.7 (0x0000007fa2f98000)\tlibudev.so.1 =&gt; /usr/lib/aarch64-linux-gnu/libudev.so.1 (0x0000007fa2f5e000)</code></pre></div><p>(9) A default HPL.dat file should ber present in the directory /opt/HPC/hpl-2.3/bin/imx8qm. The fileHPL.dat is used to tune the benchmark problem size according to the system. A copy of theHPL.dat file I created follows below. This is suitable for the 4 GB memory configuration ofReform with 4 processor cores.</p><!-- raw HTML omitted --><div class=\"highlight\"><pre><code class=\"language-plaintext\">HPLinpack benchmark input fileInnovative Computing Laboratory, University of TennesseeHPL.out      output file name (if any) 6            device out (6=stdout,7=stderr,file)1            # of problems sizes (N)19000         Ns1            # of NBs192           NBs0            PMAP process mapping (0=Row-,1=Column-major)1            # of process grids (P x Q)2            Ps2            Qs16.0         threshold1            # of panel fact2            PFACTs (0=left, 1=Crout, 2=Right)1            # of recursive stopping criterium4            NBMINs (&gt;= 1)1            # of panels in recursion2            NDIVs1            # of recursive panel fact.1            RFACTs (0=left, 1=Crout, 2=Right)1            # of broadcast1            BCASTs (0=1rg,1=1rM,2=2rg,3=2rM,4=Lng,5=LnM)1            # of lookahead depth1            DEPTHs (&gt;=0)2            SWAP (0=bin-exch,1=long,2=mix)64           swapping threshold0            L1 in (0=transposed,1=no-transposed) form0            U  in (0=transposed,1=no-transposed) form1            Equilibration (0=no,1=yes)8            memory alignment in double (&gt; 0)##### This line (no. 32) is ignored (it serves as a separator). ######0                               Number of additional problem sizes for PTRANS1200 10000 30000                values of N0                               number of additional blocking sizes for PTRANS40 9 8 13 13 20 16 32 64        values of NB</code></pre></div><!-- raw HTML omitted --><p>(10) Now we&rsquo;re ready to execute the appropriate <em>mpirun</em> command to run the <em>xhpl</em> executable.We specify -np 4 to run across the 4 cores of the processor. With this better optimized run we&rsquo;reseeing ~8.9 GFLOPS performance compared with ~4 GFLOPS for my previous runs where HPL was compiledwith the OS supplied GCC and Math libraries (ATLAS). Note that as this is roughly double the GFLOPSfrom my previous runs, it <strong>appears</strong> that there is an issue with double precision or perhapsvectorization with the non-optimized runs.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">gsamu@reform:/opt/HPC/hpl-2.3/bin/imx8qm$ mpirun -np 4 ./xhpl ================================================================================HPLinpack 2.3  --  High-Performance Linpack benchmark  --   December 2, 2018Written by A. Petitet and R. Clint Whaley,  Innovative Computing Laboratory, UTKModified by Piotr Luszczek, Innovative Computing Laboratory, UTKModified by Julien Langou, University of Colorado Denver================================================================================An explanation of the input/output parameters follows:T/V    : Wall time / encoded variant.N      : The order of the coefficient matrix A.NB     : The partitioning blocking factor.P      : The number of process rows.Q      : The number of process columns.Time   : Time in seconds to solve the linear system.Gflops : Rate of execution for solving the linear system.The following parameter values will be used:N      :   19000 NB     :     192 PMAP   : Row-major process mappingP      :       2 Q      :       2 PFACT  :   Right NBMIN  :       4 NDIV   :       2 RFACT  :   Crout BCAST  :  1ringM DEPTH  :       1 SWAP   : Mix (threshold = 64)L1     : transposed formU      : transposed formEQUIL  : yesALIGN  : 8 double precision words--------------------------------------------------------------------------------- The matrix A is randomly generated for each test.- The following scaled residual check will be computed:      ||Ax-b||_oo / ( eps * ( || x ||_oo * || A ||_oo + || b ||_oo ) * N )- The relative machine precision (eps) is taken to be               1.110223e-16- Computational tests pass if scaled residuals are less than                16.0================================================================================T/V                N    NB     P     Q               Time                 Gflops--------------------------------------------------------------------------------WR11C2R4       19000   192     2     2             513.92             8.8987e+00HPL_pdgesv() start time Wed Jun  8 21:28:07 2022HPL_pdgesv() end time   Wed Jun  8 21:36:41 2022--------------------------------------------------------------------------------||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   4.89711678e-03 ...... PASSED================================================================================Finished      1 tests with the following results:              1 tests completed and passed residual checks,              0 tests completed and failed residual checks,              0 tests skipped because of illegal input values.--------------------------------------------------------------------------------End of Tests.================================================================================</code></pre></div><p>(11) Finally, we submit the same run of Linpack but through Spectrum LSF. The LSF <em>bsub</em> commandinvocation is shown below and the resulting output.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">gsamu@reform:~$ bsub -n 4 -I -m reform \"cd /opt/HPC/hpl-2.3/bin/imx8qm ; mpirun ./xhpl\" Job &lt;35301&gt; is submitted to default queue &lt;interactive&gt;.&lt;&lt;Waiting for dispatch ...&gt;&gt;&lt;&lt;Starting on reform&gt;&gt;================================================================================HPLinpack 2.3  --  High-Performance Linpack benchmark  --   December 2, 2018Written by A. Petitet and R. Clint Whaley,  Innovative Computing Laboratory, UTKModified by Piotr Luszczek, Innovative Computing Laboratory, UTKModified by Julien Langou, University of Colorado Denver================================================================================An explanation of the input/output parameters follows:T/V    : Wall time / encoded variant.N      : The order of the coefficient matrix A.NB     : The partitioning blocking factor.P      : The number of process rows.Q      : The number of process columns.Time   : Time in seconds to solve the linear system.Gflops : Rate of execution for solving the linear system.The following parameter values will be used:N      :   19000 NB     :     192 PMAP   : Row-major process mappingP      :       2 Q      :       2 PFACT  :   Right NBMIN  :       4 NDIV   :       2 RFACT  :   Crout BCAST  :  1ringM DEPTH  :       1 SWAP   : Mix (threshold = 64)L1     : transposed formU      : transposed formEQUIL  : yesALIGN  : 8 double precision words--------------------------------------------------------------------------------- The matrix A is randomly generated for each test.- The following scaled residual check will be computed:      ||Ax-b||_oo / ( eps * ( || x ||_oo * || A ||_oo + || b ||_oo ) * N )- The relative machine precision (eps) is taken to be               1.110223e-16- Computational tests pass if scaled residuals are less than                16.0================================================================================T/V                N    NB     P     Q               Time                 Gflops--------------------------------------------------------------------------------WR11C2R4       19000   192     2     2             518.02             8.8283e+00HPL_pdgesv() start time Thu Jun  9 09:33:35 2022HPL_pdgesv() end time   Thu Jun  9 09:42:13 2022--------------------------------------------------------------------------------||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   4.89711678e-03 ...... PASSED================================================================================Finished      1 tests with the following results:              1 tests completed and passed residual checks,              0 tests completed and failed residual checks,              0 tests skipped because of illegal input values.--------------------------------------------------------------------------------End of Tests.================================================================================</code></pre></div>",
            "url": "https://hpc.social/personal-blog/2022/mnt-reform-2-part-deux/",
            
            
            
            
            
            "date_published": "2022-06-09T01:06:51-06:00",
            "date_modified": "2022-06-09T01:06:51-06:00",
            
                "author": "Ramblings of a supercomputing enthusiast."
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/neunundneunzig-mnt-reform-s/",
            "title": "Neunundneunzig MNT Reform(s)",
            "summary": null,
            "content_text": "I&rsquo;ll admit it. I sat on the fence for a long time before placing an orderfor the MNT Reform 2 laptop. At the time, I was in the market for a laptopas my 2 Macbook Pro retina laptops were repurposed for online schoolingfor my children during the pandemic (and as it turns out were neverreturned to me).I have fairly extensive experience with Arm-based systems and was aware of potentialangst with custom distros when specific system support is not in the Linux mainline.Yes this has been pretty much addressed - for servers with the Arm SBSAspecifications. However the MNT Reform 2 was never marketed as SBSA.With eyes wide open, I ultimately decided to go ahead an order an MNT Reform 2.My laptop needs were really for light coding/scripting, occasional browsing,writing (blogs, etc), tinkering and as a terminal to my other systems. Sure,these requirements coud have bee met by some less expensive x86 laptops oreven Chromebooks. But those are distinctly lacking a cool factor. What really helpedto reach this decision was the following:Put together by a small, enthusiastic teamA proper keyboard and cool trackball in a laptopIntel outsideSwappable CPUs (there are some drop in replacements in the works)User replaceable, 18650 batteriesAntithesis of paper thin laptopsOf course, knowing that the Reform is based on the NXP/Freescale i.MX8MQ with4 x Arm Cortex-A53 cores (1.5 GHz), I knew it was not going to be a barn burnerin terms of performance.Late to the partyBecause of my procrastination, I only recieved my Reform this past week.Given that they&rsquo;ve been shipping for some time, I definitely had thatlate to the party feeling. In a way this was good though as all of thewrite-ups and videos that have been posted over time gave me a goodidea of what to expect. So in this blog I don&rsquo;t expect to coveranything ground breaking - just my experience so far.Much has been written about the packaging of the system. And in this senseit didn&rsquo;t disappoint. You could tell that it was packaged with great careby the team at MNT and it was frankly a very enjoyable experience tounwrap the components. I&rsquo;ve included a collage of photos at the end of this blogof the Reform.And that wasn&rsquo;t the only fun. Right away I had to remove the transparent bottomcover of the Reform in order to connect up the batteries and install the Wifi andNVMe SSD.  At this time I also replaced the plastic port covers with the optional metalversions that I ordered earlier this year. Once this was done, the system sprang tolife and I was able to very quickly get it booting from the encrypted NVMe thanksto the detailed handbook that was also included in the bundle I purchased andtips on the MNT Community site.As for the keyboard, I really enjoy the tactile feel of it. It&rsquo;s quite a refreshingexperience from the mushy keyboard on the MacBook Air M1 that I use for work. Andalthough I ordered both the trackball and trackpad for the Reform, I&rsquo;ll likelystick with the trackball for now as it&rsquo;s just a pleasure to use. Note thatmy Reform appears to have the updated trackball which has ball bearings for asmoother action.Fanless bitteOf course one of the first things an #HPC minded person like myself will do witha system is run High-performance Linpack (HPL) on it. This is a force of habit for meand thought it may also prove to be a good way to burn in the system.So I started with Open MPI. I downloaded and compiled Open MPI v4.1.4. This completedwithout a hitch. Note that I didn&rsquo;t specify any specific flags configuring Open MPIother than a prefix for the installation location (under $HOME).HPL was easy to compile as well. Note that I simply used the OS ATLAS and BLASlibraries and the OS supplied compiler(s). So we can say that this is not anoptimized build of HPL.And below we see the results of the run of xhpl below, which achieved a result of4.2 GFLOPS.$ mpirun -np 4 ./xhpl ================================================================================HPLinpack 2.3  --  High-Performance Linpack benchmark  --   December 2, 2018Written by A. Petitet and R. Clint Whaley,  Innovative Computing Laboratory, UTKModified by Piotr Luszczek, Innovative Computing Laboratory, UTKModified by Julien Langou, University of Colorado Denver================================================================================An explanation of the input/output parameters follows:T/V    : Wall time / encoded variant.N      : The order of the coefficient matrix A.NB     : The partitioning blocking factor.P      : The number of process rows.Q      : The number of process columns.Time   : Time in seconds to solve the linear system.Gflops : Rate of execution for solving the linear system.The following parameter values will be used:N      :   19000 NB     :     192 PMAP   : Row-major process mappingP      :       2 Q      :       2 PFACT  :   Right NBMIN  :       4 NDIV   :       2 RFACT  :   Crout BCAST  :  1ringM DEPTH  :       1 SWAP   : Mix (threshold = 64)L1     : transposed formU      : transposed formEQUIL  : yesALIGN  : 8 double precision words--------------------------------------------------------------------------------- The matrix A is randomly generated for each test.- The following scaled residual check will be computed:      ||Ax-b||_oo / ( eps * ( || x ||_oo * || A ||_oo + || b ||_oo ) * N )- The relative machine precision (eps) is taken to be               1.110223e-16- Computational tests pass if scaled residuals are less than                16.0================================================================================T/V                N    NB     P     Q               Time                 Gflops--------------------------------------------------------------------------------WR11C2R4       19000   192     2     2            1073.27             4.2610e+00HPL_pdgesv() start time Mon Jun  6 12:08:36 2022HPL_pdgesv() end time   Mon Jun  6 12:26:30 2022--------------------------------------------------------------------------------||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   1.18409443e-03 ...... PASSED================================================================================Finished      1 tests with the following results:              1 tests completed and passed residual checks,              0 tests completed and failed residual checks,              0 tests skipped because of illegal input values.--------------------------------------------------------------------------------End of Tests.================================================================================Just for kicks, I&rsquo;ve also included a screenshot of lstopo, which is part of thePortal Hardware Locality (hwloc) project. I am a bit confused as to why theL1 and L2 cache sizes are zero though in the output.I&rsquo;ve included the output from some system commands below including lscpu, lspci and lsusb.$ lscpuArchitecture:                    aarch64CPU op-mode(s):                  32-bit, 64-bitByte Order:                      Little EndianCPU(s):                          4On-line CPU(s) list:             0-3Thread(s) per core:              1Core(s) per socket:              4Socket(s):                       1NUMA node(s):                    1Vendor ID:                       ARMModel:                           4Model name:                      Cortex-A53Stepping:                        r0p4CPU max MHz:                     1500.0000CPU min MHz:                     1000.0000BogoMIPS:                        16.66NUMA node0 CPU(s):               0-3Vulnerability Itlb multihit:     Not affectedVulnerability L1tf:              Not affectedVulnerability Mds:               Not affectedVulnerability Meltdown:          Not affectedVulnerability Spec store bypass: Not affectedVulnerability Spectre v1:        Mitigation; \\__user pointer sanitizationVulnerability Spectre v2:        Not affectedVulnerability Srbds:             Not affectedVulnerability Tsx async abort:   Not affectedFlags:                           fp asimd evtstrm aes pmull sha1 sha2 crc32 cpuid$ lspci0000:00:00.0 PCI bridge: Synopsys, Inc. DWC_usb3 / PCIe bridge (rev 01)0000:01:00.0 Network controller: Qualcomm Atheros AR928X Wireless Network Adapter (PCI-Express) (rev 01)0001:00:00.0 PCI bridge: Synopsys, Inc. DWC_usb3 / PCIe bridge (rev 01)0001:01:00.0 Non-Volatile memory controller: Silicon Motion, Inc. SM2262/SM2262EN SSD Controller (rev 03)$ lsusbBus 004 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hubBus 003 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hubBus 002 Device 002: ID 0451:8140 Texas Instruments, Inc. TUSB8041 4-Port HubBus 002 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hubBus 001 Device 004: ID 03eb:2041 Atmel Corp. LUFA Mouse Demo ApplicationBus 001 Device 003: ID 03eb:2042 Atmel Corp. LUFA Keyboard Demo ApplicationBus 001 Device 002: ID 0451:8142 Texas Instruments, Inc. TUSB8041 4-Port HubBus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hubSo that&rsquo;s a very brief look at my initial experiences with the Reform laptop. I&rsquo;veonly scratched the surface here, but so far I&rsquo;m liking what I&rsquo;m seeing. As for theneunundneunzig title reference, well I suppose that&rsquo;s part of the vibe I got withthe laptop.A few photos for your viewing pleasure!",
            "content_html": "<p>I&rsquo;ll admit it. I sat on the fence for a long time before placing an orderfor the MNT Reform 2 laptop. At the time, I was in the market for a laptopas my 2 Macbook Pro retina laptops were repurposed for online schoolingfor my children during the pandemic (and as it turns out were neverreturned to me).</p><p>I have fairly extensive experience with Arm-based systems and was aware of potentialangst with custom distros when specific system support is not in the Linux mainline.Yes this has been pretty much addressed - for servers with the Arm SBSAspecifications. However the MNT Reform 2 was never marketed as SBSA.</p><p>With eyes wide open, I ultimately decided to go ahead an order an MNT Reform 2.My laptop needs were really for light coding/scripting, occasional browsing,writing (blogs, etc), tinkering and as a terminal to my other systems. Sure,these requirements coud have bee met by some less expensive x86 laptops oreven Chromebooks. But those are distinctly lacking a cool factor. What really helpedto reach this decision was the following:</p><ul><li>Put together by a small, enthusiastic team</li><li>A proper keyboard and cool trackball in a laptop</li><li>Intel outside</li><li>Swappable CPUs (there are some drop in replacements in the works)</li><li>User replaceable, 18650 batteries</li><li>Antithesis of paper thin laptops</li></ul><p>Of course, knowing that the Reform is based on the NXP/Freescale i.MX8MQ with4 x Arm Cortex-A53 cores (1.5 GHz), I knew it was not going to be a barn burnerin terms of performance.</p><p><strong>Late to the party</strong></p><p>Because of my procrastination, I only recieved my Reform this past week.Given that they&rsquo;ve been shipping for some time, I definitely had that<em>late to the party feeling</em>. In a way this was good though as all of thewrite-ups and videos that have been posted over time gave me a goodidea of what to expect. So in this blog I don&rsquo;t expect to coveranything ground breaking - just my experience so far.</p><p>Much has been written about the packaging of the system. And in this senseit didn&rsquo;t disappoint. You could tell that it was packaged with great careby the team at MNT and it was frankly a very enjoyable experience tounwrap the components. I&rsquo;ve included a collage of photos at the end of this blogof the Reform.</p><p>And that wasn&rsquo;t the only fun. Right away I had to remove the transparent bottomcover of the Reform in order to connect up the batteries and install the Wifi andNVMe SSD.  At this time I also replaced the plastic port covers with the optional metalversions that I ordered earlier this year. Once this was done, the system sprang tolife and I was able to very quickly get it booting from the encrypted NVMe thanksto the detailed handbook that was also included in the bundle I purchased andtips on the <a href=\"https://community.mnt.re/\">MNT Community site</a>.</p><p>As for the keyboard, I really enjoy the tactile feel of it. It&rsquo;s quite a refreshingexperience from the mushy keyboard on the MacBook Air M1 that I use for work. Andalthough I ordered both the trackball and trackpad for the Reform, I&rsquo;ll likelystick with the trackball for now as it&rsquo;s just a pleasure to use. Note thatmy Reform appears to have the updated trackball which has ball bearings for asmoother action.</p><p><strong>Fanless bitte</strong></p><p>Of course one of the first things an #HPC minded person like myself will do witha system is run <a href=\"https://www.netlib.org/benchmark/hpl/\">High-performance Linpack</a> (HPL) on it. This is a force of habit for meand thought it may also prove to be a good way to burn in the system.</p><p>So I started with <a href=\"https://www.open-mpi.org/\">Open MPI</a>. I downloaded and compiled Open MPI v4.1.4. This completedwithout a hitch. Note that I didn&rsquo;t specify any specific flags configuring Open MPIother than a prefix for the installation location (under $HOME).</p><p>HPL was easy to compile as well. Note that I simply used the OS ATLAS and BLASlibraries and the OS supplied compiler(s). So we can say that this is not anoptimized build of HPL.</p><p>And below we see the results of the run of xhpl below, which achieved a result of4.2 GFLOPS.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ mpirun -np 4 ./xhpl ================================================================================HPLinpack 2.3  --  High-Performance Linpack benchmark  --   December 2, 2018Written by A. Petitet and R. Clint Whaley,  Innovative Computing Laboratory, UTKModified by Piotr Luszczek, Innovative Computing Laboratory, UTKModified by Julien Langou, University of Colorado Denver================================================================================An explanation of the input/output parameters follows:T/V    : Wall time / encoded variant.N      : The order of the coefficient matrix A.NB     : The partitioning blocking factor.P      : The number of process rows.Q      : The number of process columns.Time   : Time in seconds to solve the linear system.Gflops : Rate of execution for solving the linear system.The following parameter values will be used:N      :   19000 NB     :     192 PMAP   : Row-major process mappingP      :       2 Q      :       2 PFACT  :   Right NBMIN  :       4 NDIV   :       2 RFACT  :   Crout BCAST  :  1ringM DEPTH  :       1 SWAP   : Mix (threshold = 64)L1     : transposed formU      : transposed formEQUIL  : yesALIGN  : 8 double precision words--------------------------------------------------------------------------------- The matrix A is randomly generated for each test.- The following scaled residual check will be computed:      ||Ax-b||_oo / ( eps * ( || x ||_oo * || A ||_oo + || b ||_oo ) * N )- The relative machine precision (eps) is taken to be               1.110223e-16- Computational tests pass if scaled residuals are less than                16.0================================================================================T/V                N    NB     P     Q               Time                 Gflops--------------------------------------------------------------------------------WR11C2R4       19000   192     2     2            1073.27             4.2610e+00HPL_pdgesv() start time Mon Jun  6 12:08:36 2022HPL_pdgesv() end time   Mon Jun  6 12:26:30 2022--------------------------------------------------------------------------------||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   1.18409443e-03 ...... PASSED================================================================================Finished      1 tests with the following results:              1 tests completed and passed residual checks,              0 tests completed and failed residual checks,              0 tests skipped because of illegal input values.--------------------------------------------------------------------------------End of Tests.================================================================================</code></pre></div><p>Just for kicks, I&rsquo;ve also included a screenshot of <em>lstopo</em>, which is part of the<a href=\"https://www.open-mpi.org/projects/hwloc/\">Portal Hardware Locality (hwloc)</a> project. I am a bit confused as to why theL1 and L2 cache sizes are zero though in the output.</p><figure><img src=\"https://www.gaborsamu.com/images/reform_lstopo.jpg\" /></figure><p>I&rsquo;ve included the output from some system commands below including <em>lscpu</em>, <em>lspci</em> and <em>lsusb</em>.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ lscpuArchitecture:                    aarch64CPU op-mode(s):                  32-bit, 64-bitByte Order:                      Little EndianCPU(s):                          4On-line CPU(s) list:             0-3Thread(s) per core:              1Core(s) per socket:              4Socket(s):                       1NUMA node(s):                    1Vendor ID:                       ARMModel:                           4Model name:                      Cortex-A53Stepping:                        r0p4CPU max MHz:                     1500.0000CPU min MHz:                     1000.0000BogoMIPS:                        16.66NUMA node0 CPU(s):               0-3Vulnerability Itlb multihit:     Not affectedVulnerability L1tf:              Not affectedVulnerability Mds:               Not affectedVulnerability Meltdown:          Not affectedVulnerability Spec store bypass: Not affectedVulnerability Spectre v1:        Mitigation; \\__user pointer sanitizationVulnerability Spectre v2:        Not affectedVulnerability Srbds:             Not affectedVulnerability Tsx async abort:   Not affectedFlags:                           fp asimd evtstrm aes pmull sha1 sha2 crc32 cpuid</code></pre></div><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ lspci0000:00:00.0 PCI bridge: Synopsys, Inc. DWC_usb3 / PCIe bridge (rev 01)0000:01:00.0 Network controller: Qualcomm Atheros AR928X Wireless Network Adapter (PCI-Express) (rev 01)0001:00:00.0 PCI bridge: Synopsys, Inc. DWC_usb3 / PCIe bridge (rev 01)0001:01:00.0 Non-Volatile memory controller: Silicon Motion, Inc. SM2262/SM2262EN SSD Controller (rev 03)</code></pre></div><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ lsusbBus 004 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hubBus 003 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hubBus 002 Device 002: ID 0451:8140 Texas Instruments, Inc. TUSB8041 4-Port HubBus 002 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hubBus 001 Device 004: ID 03eb:2041 Atmel Corp. LUFA Mouse Demo ApplicationBus 001 Device 003: ID 03eb:2042 Atmel Corp. LUFA Keyboard Demo ApplicationBus 001 Device 002: ID 0451:8142 Texas Instruments, Inc. TUSB8041 4-Port HubBus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub</code></pre></div><p>So that&rsquo;s a very brief look at my initial experiences with the Reform laptop. I&rsquo;veonly scratched the surface here, but so far I&rsquo;m liking what I&rsquo;m seeing. As for the<em>neunundneunzig</em> title reference, well I suppose that&rsquo;s part of the vibe I got withthe laptop.</p><p>A few photos for your viewing pleasure!</p><p><figure><img src=\"https://www.gaborsamu.com/images/reform_bottom.jpg\" /></figure><figure><img src=\"https://www.gaborsamu.com/images/reform_firstboot.jpg\" /></figure></p>",
            "url": "https://hpc.social/personal-blog/2022/neunundneunzig-mnt-reform-s/",
            
            
            
            
            
            "date_published": "2022-06-06T18:54:07-06:00",
            "date_modified": "2022-06-06T18:54:07-06:00",
            
                "author": "Ramblings of a supercomputing enthusiast."
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/life-and-leaving-nersc/",
            "title": "Life and leaving NERSC",
            "summary": null,
            "content_text": "When word started to spread that I was leaving my job at NERSC for Microsoft, a lot of people either directly or indirectly attributed my decision to being one motivated by money.  Rationalizing my decision to leave is certainly a lot easier with this \"Glenn was lured away with bags of cash\" narrative, but that wasn't really a factor when I chose to move on.  Rather, my decision is a reflection of where I see the world of HPC going in the coming decade and where I personally wanted to position myself.  For my own therapeutic reasons (and perhaps the benefit of anyone interested in what it's like to work within, and subsequently leave, the DOE HPC complex), I'll try to write it all out here.Working at NERSCFirst things first: NERSC has been a wonderful place to work.&lt;div style=\"text-align: center;\"&gt;A typical view from outside NERSC’s facility in Berkeley after work during the winter months.  Yes, it really does look like this.&lt;/div&gt;&lt;p&gt;When I started in mid-2015, I came in with about three years of prior work experience (two at SDSC doing user support and one at a biotech startup) and knew a little bit about a lot of things in HPC.  But I didn’t really know the basics of I/O or storage–I couldn’t tell you what “POSIX I/O” really meant or how GPFS worked.  The fact that I got to help author NERSC’s ten-year strategy around storage in just two years, was invited to present my view on how to bridge the gap between HPC and enterprise storage at Samsung’s North American headquarters a year later, and was trusted to oversee the design and execution of the world’s first 35 petabyte all-flash Lustre file system through my first four years is a testament to how much opportunity is available to learn and grow at NERSC.&lt;/p&gt;There are a couple of reasons for this.Stable fundingPerhaps foremost, NERSC (and DOE's Leadership Computing Facilities, ALCF and OLCF) enjoy healthy budgets and financial stability since worldwide leadership in scientific advancement is generally a national priority by both major political parties in the US.  This means that, regardless of who is president and which party holds majorities in Congress, the DOE HPC facilities can pay their employees and deploy new supercomputers.  This solid funding makes it much easier to invest in staff development and long-term planning; I was able to become a resident I/O expert at NERSC because I was never forced to chase after the funding du jour to make ends meet.  Congress trusts NERSC to allocate its funding responsibly, and NERSC prioritized letting me learn as much as I could without distraction.Instant credibility and accessSecond, having a NERSC affiliation gives you instant credibility and access in many cases.  It's not necessarily fair, but it's definitely true.  Within my first year at NERSC, I was invited to give a presentation about I/O performance monitoring in Paris because the organizer wanted a lineup of speakers from all the big players in HPC.  I had never been to Europe at that point in my life, but being the I/O guy from NERSC (and being able to present well!) was enough to get me there.  And it was during that trip to Paris that I got to meet--and literally have conversation over dinner with--more industry bigshots that I can remember.  And that trip to Paris was not an outlier; pandemic aside, NERSC let me go to Europe at least once or twice every year I've worked there.&lt;div style=\"text-align: center;\"&gt;The first photo I ever took of Notre Dame on the first day I’d ever set foot in Europe.  NERSC sent me there less than a year after I started.&lt;/div&gt;&lt;p&gt;Of course, this is not to say that every employee at a DOE HPC facility is wining and dining in Paris every summer.  Many of these opportunities are earned by showing the value of the work you’re doing, just like at any job.  But owing to healthy budgets, travel expenses are rarely the limiting factor in chasing after these opportunities.  In addition, going out into the world and talking about what you do is part of the job at a DOE facility; being a leader in the field of HPC is part of the mission of NERSC, ALCF, and OLCF, so doing high-risk, first-of-a-kind work and telling the world about it is uniquely valued within DOE in a way that it is not in industry.&lt;/p&gt;Smart peopleA product of these two factors (stable budget and instant credibility) results in coworkers and colleagues who are generally very experienced and capable.  There's an interesting mix of laissez-faire management and rigorous process-driven management as a result.Staff are generally given the freedom to choose their own destiny and focus on work that they enjoy much like in any academic environment; it's not hard to pick up passion projects or even move between groups if things get stale on a day-to-day basis.  Since everyone is working on their own slices of HPC, there's also easy access to world experts in different areas of technology if you need one.  For example, I recall once reviewing a storage system that appeared to rely on multiplexing two 12G SAS links over a single 24G SAS.  After one email and a few hours, a coworker confirmed, complete with a citation to the SCSI standards, that this was totally possible.  Even if someone in-house didn't know the answer, I had direct access to an engineering manager at a leading storage vendor who owed me a favor and definitely would've known the answer.  It's really, really hard to find as many smart people in arm's reach in most other HPC centers. At the same time, there is rigorous federal oversight on major projects and procurements to ensure that taxpayer dollars are responsibly spent.  This is a double-edged sword because all of the reporting and reviews that go into massive capital projects make forward progress very slow at times.  All DOE HPC facilities review and re-review everything about these giant supercomputers before making a decision, so by the time the public sees a press release about a new supercomputer, lab staff have spent literal years going over every detail and risk.  It sometimes may not seem that way (how many problems has Aurora had?), but rest assured that every schedule slip or technology change the public hears was preceded by countless hours of meetings about risk and cost minimization.  On the flip-side though, you have the opportunity to learn every gory detail about the system directly from the people who designed it.PayIn true millennial fashion, I think it's important to have an open discussion about the pay.  DOE labs pay more than any other HPC facility in the world as far as I am aware, and even in the San Francisco Bay Area, salary at NERSC is comparable to the base salaries offered by all the big tech companies.  You can get an idea of what entry-level salaries (think: first job after postdoc or a few years out of undergrad) by searching H1B Visa postings, and anecdotally, I'd wager that a typical HPC job at NERSC pays about 2x that of the same job at a typical US university and 3x-4x that of the same job at a British or European university.  All the labs pay about the same to boot, so an HPC job at somewhere like Oak Ridge can afford you a relatively luxurious lifestyle.Don't get me wrong though; affording to buy a Bay Area house on a single NERSC salary alone would be tough in the same way that buying a Bay Area house on any single salary would be.  And while NERSC's compensation is comparable to the base salary of the big tech companies, that base is about all you can get since DOE labs cannot offer equity or substantial bonuses.  This is less of a gap if you're just starting out, but anyone who's looked at compensation structures in tech knows that stock-based compensation, not base salary, dominates total compensation as you move up.So, if money wasn't an issue for me and NERSC is such a great place to work, why would I ever leave?The road ahead for HPCOn one hand, HPC's future has never been brighter thanks to how much life (and money!) the AI industry is bringing to the development of HPC technologies.  We have new all-flash file systems, gigantic GPUs, awesome CPU memory technologies, and mixed-precision techniques in the HPC space that were all directly driven by developments primarily intended for AI workloads.  On the other hand, leadership HPC appears to be engaging in unsustainable brinkmanship while midrange HPC is having its value completely undercut by cloud vendors.  I've not been shy about my overall anxiety about where HPC is going because of this, but I'll elaborate now that the exascale race has been won.The future of leadership HPCWithout some monumental breakthrough in transistor technology, there is only one path forward in continuing to build faster and faster supercomputers in the next decade: pour more and more energy (and dissipate more and more heat) into larger and larger (and more and more) GPUs.The goal post for exascale power keeps moving because that's been the easiest way to hit the mythical exaflop milestone; while the original goal was 20 MW, Frontier is coming in at 29 MW and Aurora at \"under 60 MW.\"  Not only is this just a lot of power to feed into a single room, but the cost and effort of actually building this infrastructure is newsworthy in and of itself these days.  At the current trajectory, the cost of building a new data center and extensive power and cooling infrastructure for every new leadership supercomputer is going to become prohibitive very soon.HPC data centers situated in places where the cost of electricity and real estate (stacked atop the risk of earthquake or wildfire) further skew the economics of just adding more power are going to run up against this first.  It used to be easy to dismiss these practicality concerns by arguing that colocating scientists with supercomputers created immeasurable synergy and exchange of ideas, but the fact that science never stopped during the work-from-home days of the pandemic have taken a lot of air out of that argument.My guess is that all the 50-60 MW data centers being built for the exascale supercomputers will be the last of their kind, and that there will be no public appetite to keep doubling down.Given this, DOE's leadership computing facilities are facing an existential threat: how do you define leadership computing after exascale if you can't just add another 50% more power into your facility?  How do you justify spending another $600 million for a supercomputer that uses the same power but only delivers 15% more performance?  You can pour similarly huge amounts of money into application modernization to accelerate science, but at the end of the day, you'd still be buying a lot of hardware that's not a lot faster.The future of places like NERSCNERSC is probably a little better off since its lack of an exascale machine today gives it at least one more turn of the crank before it hits a hard power limit in its data center.  That gives it the ability to deploy at least one more system after Perlmutter that is significantly (at least 2x) more capable but draws significantly more power.  However, compared to Frontier and Aurora, such a system may still look rather silly when it lands in the same way that Perlmutter looks a bit silly compared Summit, which was funded by the same agency but deployed years earlier.And therein lies the dilemma of centers like NERSC--how do you position yourself now so that by the time you deploy an HPC system that is close to maxing out on power, it is sufficiently different from a pure-FLOPS leadership system that it can solve problems that the leadership systems cannot?The easy go-to solution is to craft a story around \"data-centric\" supercomputing.  We did this when I was at the San Diego Supercomputer Center when we were budget-limited and had to differentiate our $12 million Comet supercomputer from TACC's $30 million Stampede.  You invest more in the file system than you would for a pure-FLOPS play, you provide low-cost but high-value onramps like Jupyter and science gateways to enable new science communities that have modest computing needs, and you fiddle with policies like allocations and queue priority to better suit interactive and urgent computing workloads.  From a productivity standpoint, this is can be a great story since users will always respond well to lower queue wait times and less frustrations with the file system.  From a system architect's standpoint, though, this is really boring.  The innovation happens in policies and software, not clever hardware or design, so there's very little that's new for a system designer to think about in this case.A more innovative approach is to start thinking about how to build a system that does more than just run batch jobs.  Perhaps it gives you a private, fast file system where you can store all your data in a way indistinguishable from your personal laptop.  Perhaps it gives you a convenient place to run a Jupyter notebook that has immediate access to a powerful GPU.  Or perhaps it gives you all the tools to set up an automated process where all you have to do is upload a file to trigger an automatic data analysis and reduction pipeline that returns its output to a shiny HTTP interface.  Such a system may not be able to crank out an exaflop using HPL, but does that matter if it's the only system in the country that supports such automation?There are interesting system architecture questions in the latter case, so as a system designer, I much prefer it over the \"data-centric\" angle to non-exaflop supercomputing strategies.  But there remains a problem.The problem: cloudSuch a \"more than just batch jobs\" supercomputer actually already exists.  It's called the cloud, and it's far, far ahead of where state-of-the-art large-scale HPC is today--it pioneered the idea of providing an integrated platform where you can twist the infrastructure and its services to exactly fit what you want to get done.  Triggering data analysis based on the arrival of new data has been around for the better part of a decade in the form of serverless computing frameworks like Azure Functions.  If you need to run a Jupyter notebook on a server that has a beefy GPU on it, just pop a few quarters into your favorite cloud provider.  And if you don't even want to worry about what infrastructure you need to make your Jupyter-based machine learning workload go fast, the cloud providers all have integrated machine learning development environments that hide all of the underlying infrastructure.And therein lies the problem: the definition of \"innovation\" as non-exaflop HPC runs up against this power wall might actually mean \"catching up to the cloud.\"This is not to say that NERSC-like HPC centers are entirely behind the cloud; all the DOE HPC facilities have bigger, faster, and more convenient parallel file systems that are generally always on and where data is always somewhere \"fast.\"  They also provide familiar, managed software environments and more egalitarian support to small- to mid-scale science projects.  DOE HPC also takes the most risk in deploying unproven technologies to shake them out before they become available to the wide market.However, those gaps are beginning to close.  You can stick a full Cray EX system, identical to what you might find at NERSC or OLCF, inside Azure nowadays and avoid that whole burdensome mess of building out a 50 MW data center.  You can also integrate such a system with all the rich infrastructure features the cloud has to offer like triggered functions.  And when it comes to being first to market for risky HPC hardware, the cloud has already caught up in many ways--Microsoft deployed AMD Milan-X CPUs in their data centers before any HPC shop did, and more recently, Microsoft invested in AMD MI-200 GPUs before Frontier had a chance to shake them out.Given this steep trajectory, I see only two scenarios for large-scale, non-exaflop HPC facilities in the 10+ year horizon:They develop, adopt, steal, or squish cloud technologies into their supercomputers to make them functionally equivalent to cloud HPC deployments.  They may be a little friendlier to scientific users since cloud functionality wasn't designed for scientific computing alone, but they also may not be as stable, mature, or feature-rich as their cloud cousins.They find better overall economics in eventually moving to massive, long-term, billion-dollar deals where flagship HPC systems and their \"more than just batch jobs\" features are colocated inside cloud datacenters sited at economically advantageous (that is, cheap power, cooling, and labor) locations in the country.There's also grey area in between where national HPC facilities consolidate their physical infrastructure in cheap areas to manage costs but still self-manage their infrastructure rather than fully outsource to a commercial cloud.  CSCS has hinted at this model as their future plan since they cannot build 100 MW datacenters in Switzerland, and this is proof that leading HPC facilities around the world see the writing on the wall and need to maneuver now to ensure they remain relevant beyond the next decade.  Unfortunately, the politics of consolidating the physical infrastructure across the DOE HPC sites would likely be mired in Congressional politics and take at least a decade to work out.  Since serious work towards this hasn't started yet, I don't envision such a grey-area solution emerging before all the DOE facilities hit their power limit.Hopefully I've painted a picture of how I perceive the road ahead for large-scale HPC facilities and you can guess which one I think will win out.Final thoughtsI have every confidence that there will still be DOE HPC facilities in ten years and that they will still be staffed by some of the brightest minds in HPC.  And even if a cloud-based HPC facility ultimately consumes centers like NERSC, I don't think many people would be out of work.  The vast majority of what DOE's HPC people do is think carefully about technology trends, maintain a deep understanding of user requirements, provide excellent support to its thousands of users, and keep complex supercomputers running well.  Those jobs don't go away if the supercomputer is in the cloud; it's just the physical location, the hands doing physical hardware swaps, and the breadth of vendor interactions that may change.For me as a system architect though, it's become too hard for me to catch up to all the new technologies and techniques HPC needs for the future while also building up other staff to be masters of today's I/O challenges.  I found myself at a fork in the road.  One path would mean catching up on a technical level and then getting in front of where the future of HPC lies before it gets there.  The other path would mean trying to steer the entire DOE HPC ship in the right direction, as long as that may take, and have faith that the people I bring along can race far enough ahead to tell me if we're still going where we need to go.  Perhaps a bit selfishly, I chose the former.  I'm just not ready to give up on racing ahead myself yet, and the only way I could hope to catch up was to make it a full-time job.I don't claim to know the future, and a lot of what I've laid out is all speculative at best.  NERSC, ALCF, or OLCF very well may build another round of data centers to keep the DOE HPC party going for another decade.  However, there's no denying that the stakes keep getting higher with every passing year.That all said, DOE has pulled off stranger things in the past, and it still has a bunch of talented people to make the best of whatever the future holds.",
            "content_html": "<p>When word started to spread that I was leaving my job at NERSC for Microsoft, a lot of people either directly or indirectly attributed my decision to being one motivated by money.  Rationalizing my decision to leave is certainly a lot easier with this \"Glenn was lured away with bags of cash\" narrative, but that wasn't really a factor when I chose to move on.  Rather, my decision is a reflection of where I see the world of HPC going in the coming decade and where I personally wanted to position myself.  For my own therapeutic reasons (and perhaps the benefit of anyone interested in what it's like to work within, and subsequently leave, the DOE HPC complex), I'll try to write it all out here.<span></span></p><p></p><h2 style=\"text-align: left;\">Working at NERSC</h2><p>First things first: NERSC has been a wonderful place to work.</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p><b>&lt;div style=\"text-align: center;\"&gt;<b><span style=\"font-size: x-small;\">A typical view from outside NERSC’s facility in Berkeley after work during the winter months.  Yes, it really does look like this.</span></b>&lt;/div&gt;</b>&lt;p&gt;When I started in mid-2015, I came in with about three years of prior work experience (two at SDSC doing user support and one at a biotech startup) and knew a little bit about a lot of things in HPC.  But I didn’t really know the basics of I/O or storage–I couldn’t tell you what “POSIX I/O” really meant or how GPFS worked.  The fact that I got to help author <a href=\"https://www.nersc.gov/news-publications/nersc-news/nersc-center-news/2017/new-storage-2020-report-outlines-future-hpc-storage-vision/\">NERSC’s ten-year strategy around storage</a> in just two years, was invited to present <a href=\"https://insidehpc.com/2019/08/designing-future-flash-storage-systems-for-hpc-and-beyond/\">my view on how to bridge the gap between HPC and enterprise storage</a> at Samsung’s North American headquarters a year later, and was trusted to oversee <a href=\"https://www.nextplatform.com/2021/06/07/a-35-petabyte-all-flash-balancing-act/\">the design and execution of the world’s first 35 petabyte all-flash Lustre file system</a> through my first four years is a testament to how much opportunity is available to learn and grow at NERSC.&lt;/p&gt;</p><p>There are a couple of reasons for this.</p><h3 style=\"text-align: left;\">Stable funding</h3><p>Perhaps foremost, NERSC (and DOE's Leadership Computing Facilities, ALCF and OLCF) enjoy healthy budgets and financial stability since worldwide leadership in scientific advancement is generally a national priority by both major political parties in the US.  This means that, regardless of who is president and which party holds majorities in Congress, the DOE HPC facilities can pay their employees and deploy new supercomputers.  This solid funding makes it much easier to invest in staff development and long-term planning; I was able to become a resident I/O expert at NERSC because I was never forced to chase after the funding du jour to make ends meet.  Congress trusts NERSC to allocate its funding responsibly, and NERSC prioritized letting me learn as much as I could without distraction.</p><h3 style=\"text-align: left;\">Instant credibility and access</h3><p>Second, <a href=\"https://twitter.com/hpcprogrammer/status/1061278775353196544?s=20&amp;t=_YGQXWvykuCElqltJ-x09Q\">having a NERSC affiliation gives you instant credibility and access</a> in many cases.  It's not necessarily fair, but it's definitely true.  Within my first year at NERSC, I was invited to give <a href=\"https://archive.siam.org/meetings/pp16/pp16_program.pdf\">a presentation about I/O performance monitoring in Paris</a> because the organizer wanted a lineup of speakers from all the big players in HPC.  I had never been to Europe at that point in my life, but being the I/O guy from NERSC (and being able to present well!) was enough to get me there.  And it was during that trip to Paris that I got to meet--and literally have conversation over dinner with--<a href=\"https://www.linkedin.com/in/larry-kaplan-b101936\">more</a> <a href=\"https://people.llnl.gov/tgamblin\">industry</a> <a href=\"https://en.wikipedia.org/wiki/David_E._Keyes\">bigshots</a> that I can remember.  And that trip to Paris was not an outlier; pandemic aside, NERSC let me go to Europe at least once or twice every year I've worked there.</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p><b>&lt;div style=\"text-align: center;\"&gt;<b><span style=\"font-size: x-small;\">The first photo I ever took of Notre Dame on the first day I’d ever set foot in Europe.  NERSC sent me there less than a year after I started.</span></b>&lt;/div&gt;</b>&lt;p&gt;Of course, this is not to say that every employee at a DOE HPC facility is wining and dining in Paris every summer.  Many of these opportunities are earned by showing the value of the work you’re doing, just like at any job.  But owing to healthy budgets, travel expenses are rarely the limiting factor in chasing after these opportunities.  In addition, going out into the world and talking about what you do is part of the job at a DOE facility; being a leader in the field of HPC is part of the mission of NERSC, ALCF, and OLCF, so doing high-risk, first-of-a-kind work <i>and telling the world about it</i> is uniquely valued within DOE in a way that it is not in industry.&lt;/p&gt;</p><h3 style=\"text-align: left;\">Smart people</h3><p>A product of these two factors (stable budget and instant credibility) results in coworkers and colleagues who are generally very experienced and capable.  There's an interesting mix of laissez-faire management and rigorous process-driven management as a result.</p><p>Staff are generally given the freedom to choose their own destiny and focus on work that they enjoy much like in any academic environment; it's not hard to pick up passion projects or even move between groups if things get stale on a day-to-day basis.  Since everyone is working on their own slices of HPC, there's also easy access to world experts in different areas of technology if you need one.  For example, I recall once reviewing a storage system that appeared to rely on multiplexing two 12G SAS links over a single 24G SAS.  After one email and a few hours, a coworker confirmed, complete with a citation to the SCSI standards, that this was totally possible.  Even if someone in-house didn't know the answer, I had direct access to an engineering manager at a leading storage vendor who owed me a favor and definitely would've known the answer.  It's really, really hard to find as many smart people in arm's reach in most other HPC centers. </p><p>At the same time, there is rigorous federal oversight on major projects and procurements to ensure that taxpayer dollars are responsibly spent.  This is a double-edged sword because all of the reporting and reviews that go into <a href=\"https://www.energy.gov/articles/doe-build-next-generation-supercomputer-lawrence-berkeley-national-laboratory\">massive</a> <a href=\"https://www.ornl.gov/news/us-department-energy-and-cray-deliver-record-setting-frontier-supercomputer-ornl\">capital</a> <a href=\"https://www.energy.gov/articles/us-department-energy-and-intel-build-first-exascale-supercomputer\">projects</a> make forward progress very slow at times.  All DOE HPC facilities review and re-review everything about these giant supercomputers before making a decision, so by the time the public sees a press release about a new supercomputer, lab staff have spent literal years going over every detail and risk.  It sometimes may not seem that way (how many problems has Aurora had?), but rest assured that every schedule slip or technology change the public hears was preceded by countless hours of meetings about risk and cost minimization.  On the flip-side though, you have the opportunity to learn every gory detail about the system directly from the people who designed it.</p><h3 style=\"text-align: left;\">Pay</h3><p>In <a href=\"https://www.bankrate.com/banking/federal-reserve/younger-workers-sharing-salaries/\">true millennial fashion</a>, I think it's important to have an open discussion about the pay.  DOE labs pay more than any other HPC facility in the world as far as I am aware, and even in the San Francisco Bay Area, salary at NERSC is comparable to the base salaries offered by all the big tech companies.  You can get an idea of what entry-level salaries (think: first job after postdoc or a few years out of undergrad) by searching <a href=\"https://h1bdata.info/\">H1B Visa postings</a>, and anecdotally, I'd wager that a typical HPC job at NERSC pays about 2x that of the same job at a typical US university and 3x-4x that of the same job at a British or European university.  All the labs pay about the same to boot, so an HPC job at somewhere like Oak Ridge can afford you a relatively luxurious lifestyle.</p><p>Don't get me wrong though; affording to buy a Bay Area house on a single NERSC salary alone would be tough in the same way that buying a Bay Area house on any single salary would be.  And while NERSC's compensation is comparable to the <i>base</i> salary of the big tech companies, that base is about all you can get since DOE labs cannot offer equity or substantial bonuses.  This is less of a gap if you're just starting out, but anyone who's <a href=\"https://www.levels.fyi/\">looked at compensation structures in tech</a> knows that stock-based compensation, not base salary, dominates total compensation as you move up.</p><p>So, if money wasn't an issue for me and NERSC is such a great place to work, why would I ever leave?</p><h2 style=\"text-align: left;\">The road ahead for HPC</h2><p>On one hand, HPC's future has never been brighter thanks to how much life (and money!) the AI industry is bringing to the development of HPC technologies.  We have new <a href=\"https://vastdata.com/\">all-flash</a> <a href=\"https://www.weka.io/\">file systems</a>, <a href=\"https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/\">gigantic GPUs</a>, awesome <a href=\"https://www.tomshardware.com/news/intels-sapphire-rapids-to-have-64-gigabytes-of-hbm2e-memory\">CPU memory technologies</a>, and <a href=\"https://arxiv.org/abs/2205.12182\">mixed-precision techniques</a> in the HPC space that were all directly driven by developments primarily intended for AI workloads.  On the other hand, leadership HPC appears to be engaging in unsustainable brinkmanship while midrange HPC is having its value completely undercut by cloud vendors.  I've <a href=\"https://glennklockwood.blogspot.com/2020/05/exascales-long-shadow-and-hpc-being.html\">not been shy about my overall anxiety about where HPC is going</a> because of this, but I'll elaborate now that the exascale race has been won.</p><h3 style=\"text-align: left;\">The future of leadership HPC</h3><p>Without some monumental breakthrough in transistor technology, there is only one path forward in continuing to build faster and faster supercomputers in the next decade: pour more and more energy (and dissipate more and more heat) into larger and larger (and more and more) GPUs.</p><p>The goal post for exascale power keeps moving because that's been the easiest way to hit the mythical exaflop milestone; while the original goal was 20 MW, <a href=\"https://www.nextplatform.com/2021/10/04/first-look-at-oak-ridges-frontier-exascaler-contrasted-to-argonnes-aurora/\">Frontier is coming in at 29 MW</a> and <a href=\"https://www.tomshardware.com/news/nvidia-amd-polaris-supercomputer-department-of-energy\">Aurora at \"under 60 MW.\"</a>  Not only is this just a lot of power to feed into a single room, but the <a href=\"https://www.olcf.ornl.gov/2020/09/23/powering-frontier/\">cost and effort</a> of actually <a href=\"https://www.llnl.gov/news/powering-llnl-prepares-exascale-massive-energy-and-water-upgrade\">building this infrastructure</a> is <a href=\"https://www.lanl.gov/asc/fous/sixty-megawatts-power-available-2025.php\">newsworthy</a> in and of itself these days.  At the current trajectory, the cost of building a new data center and extensive power and cooling infrastructure for every new leadership supercomputer is going to become prohibitive very soon.</p><p>HPC data centers situated in places where the cost of electricity and real estate (stacked atop the risk of earthquake or wildfire) further skew the economics of just adding more power are going to run up against this first.  It used to be easy to dismiss these practicality concerns by arguing that colocating scientists with supercomputers created immeasurable synergy and exchange of ideas, but the fact that science never stopped during the work-from-home days of the pandemic have taken a lot of air out of that argument.</p><p>My guess is that all the 50-60 MW data centers being built for the exascale supercomputers will be the last of their kind, and that there will be no public appetite to keep doubling down.</p><p>Given this, DOE's leadership computing facilities are facing an existential threat: how do you define leadership computing after exascale if you can't just add another 50% more power into your facility?  How do you justify spending another $600 million for a supercomputer that uses the same power but only delivers 15% more performance?  You can pour similarly huge amounts of money into application modernization to accelerate science, but at the end of the day, you'd still be buying a lot of hardware that's not a lot faster.</p><h3 style=\"text-align: left;\">The future of places like NERSC</h3><p>NERSC is probably a little better off since its lack of an exascale machine today gives it at least one more turn of the crank before it hits a hard power limit in its data center.  That gives it the ability to deploy at least one more system after Perlmutter that is significantly (at least 2x) more capable but draws significantly more power.  However, compared to Frontier and Aurora, such a system may still look rather silly when it lands in the same way that Perlmutter looks a bit silly compared Summit, which was funded by the same agency but deployed years earlier.</p><p>And therein lies the dilemma of centers like NERSC--how do you position yourself now so that by the time you deploy an HPC system that is close to maxing out on power, it is sufficiently different from a pure-FLOPS leadership system that it can solve problems that the leadership systems cannot?</p><p>The easy go-to solution is to craft a story around \"data-centric\" supercomputing.  We did this when I was at the San Diego Supercomputer Center when we were budget-limited and had to differentiate our $12 million Comet supercomputer from TACC's $30 million Stampede.  You invest more in the file system than you would for a pure-FLOPS play, you provide low-cost but high-value onramps like Jupyter and science gateways to enable new science communities that have modest computing needs, and you fiddle with policies like allocations and queue priority to better suit interactive and urgent computing workloads.  From a productivity standpoint, this is can be a great story since users will always respond well to lower queue wait times and less frustrations with the file system.  From a system architect's standpoint, though, this is really boring.  The innovation happens in policies and software, not clever hardware or design, so there's very little that's new for a system designer to think about in this case.</p><p>A more innovative approach is to start thinking about how to build a system that does more than just run batch jobs.  Perhaps it gives you a private, fast file system where you can store all your data in a way indistinguishable from your personal laptop.  Perhaps it gives you a convenient place to run a Jupyter notebook that has immediate access to a powerful GPU.  Or perhaps it gives you all the tools to set up an automated process where all you have to do is upload a file to trigger an automatic data analysis and reduction pipeline that returns its output to a shiny HTTP interface.  Such a system may not be able to crank out an exaflop using HPL, but does that matter if it's the only system in the country that supports such automation?</p><p>There <i>are</i> interesting system architecture questions in the latter case, so as a system designer, I much prefer it over the \"data-centric\" angle to non-exaflop supercomputing strategies.  But there remains a problem.</p><h3 style=\"text-align: left;\">The problem: cloud</h3><p>Such a \"more than just batch jobs\" supercomputer actually already exists.  It's called the cloud, and it's far, far ahead of where state-of-the-art large-scale HPC is today--it pioneered the idea of providing an integrated platform where you can twist the infrastructure and its services to exactly fit what you want to get done.  Triggering data analysis based on the arrival of new data has been around for the better part of a decade in the form of serverless computing frameworks like <a href=\"https://docs.microsoft.com/en-us/learn/modules/execute-azure-function-with-triggers/2-determine-best-trigger\">Azure Functions</a>.  If you need to run a Jupyter notebook on a server that has a beefy GPU on it, just pop a few quarters into your favorite cloud provider.  And if you don't even want to worry about what infrastructure you need to make your Jupyter-based machine learning workload go fast, the cloud providers all have <a href=\"https://docs.microsoft.com/en-us/azure/machine-learning/overview-what-is-machine-learning-studio\">integrated machine learning development environments</a> that hide all of the underlying infrastructure.</p><p>And therein lies the problem: the definition of \"innovation\" as non-exaflop HPC runs up against this power wall might actually mean \"catching up to the cloud.\"</p><p>This is not to say that NERSC-like HPC centers are entirely behind the cloud; all the DOE HPC facilities have bigger, faster, and more convenient parallel file systems that are generally always on and where data is always somewhere \"fast.\"  They also provide familiar, managed software environments and more egalitarian support to small- to mid-scale science projects.  DOE HPC also takes the most risk in deploying unproven technologies to shake them out before they become available to the wide market.</p><p>However, those gaps are beginning to close.  You can stick <a href=\"https://azure.microsoft.com/en-us/solutions/high-performance-computing/cray/\">a full Cray EX system, identical to what you might find at NERSC or OLCF, inside Azure</a> nowadays and avoid that whole burdensome mess of building out a 50 MW data center.  You can also integrate such a system with all the rich infrastructure features the cloud has to offer like triggered functions.  And when it comes to being first to market for risky HPC hardware, the cloud has already caught up in many ways--<a href=\"https://azure.microsoft.com/en-us/blog/azure-hbv3-virtual-machines-for-hpc-now-up-to-80-percent-faster-with-amd-milanx-cpus/\">Microsoft deployed AMD Milan-X CPUs in their data centers</a> before any HPC shop did, and more recently, <a href=\"https://www.theregister.com/2022/05/26/amd_azure_microsoft/\">Microsoft invested in AMD MI-200 GPUs</a> before Frontier had a chance to shake them out.</p><p>Given this steep trajectory, I see only two scenarios for large-scale, non-exaflop HPC facilities in the 10+ year horizon:</p><p></p><ol style=\"text-align: left;\"><li>They develop, adopt, steal, or squish cloud technologies into their supercomputers to make them functionally equivalent to cloud HPC deployments.  They may be a little friendlier to scientific users since cloud functionality wasn't designed for scientific computing alone, but they also may not be as stable, mature, or feature-rich as their cloud cousins.</li><li>They find better overall economics in eventually moving to <a href=\"https://www.hpcwire.com/2021/05/13/behind-the-met-offices-procurement-of-a-billion-dollar-microsoft-system/\">massive, long-term, billion-dollar deals</a> where flagship HPC systems and their \"more than just batch jobs\" features are colocated inside cloud datacenters sited at economically advantageous (that is, cheap power, cooling, and labor) locations in the country.</li></ol><p>There's also grey area in between where national HPC facilities consolidate their physical infrastructure in cheap areas to manage costs but still self-manage their infrastructure rather than fully outsource to a commercial cloud.  <a href=\"https://ethz.ch/en/news-and-events/eth-news/news/2021/03/we-dont-just-procure-a-new-computer.html\">CSCS has hinted at this model as their future plan</a> since they cannot build 100 MW datacenters in Switzerland, and this is proof that leading HPC facilities around the world see the writing on the wall and need to maneuver now to ensure they remain relevant beyond the next decade.  Unfortunately, the politics of consolidating the physical infrastructure across the DOE HPC sites would likely be mired in Congressional politics and take at least a decade to work out.  Since serious work towards this hasn't started yet, I don't envision such a grey-area solution emerging before all the DOE facilities hit their power limit.</p><p>Hopefully I've painted a picture of how I perceive the road ahead for large-scale HPC facilities and you can guess which one I think will win out.</p><h2 style=\"text-align: left;\">Final thoughts</h2><p>I have every confidence that there will still be DOE HPC facilities in ten years and that they will still be staffed by some of the brightest minds in HPC.  And even if a cloud-based HPC facility ultimately consumes centers like NERSC, I don't think many people would be out of work.  The vast majority of what DOE's HPC people do is think carefully about technology trends, maintain a deep understanding of user requirements, provide excellent support to its thousands of users, and keep complex supercomputers running well.  Those jobs don't go away if the supercomputer is in the cloud; it's just the physical location, the hands doing physical hardware swaps, and the breadth of vendor interactions that may change.</p><p>For me as a system architect though, it's become too hard for me to catch up to all the new technologies and techniques HPC needs for the future while also building up other staff to be masters of today's I/O challenges.  I found myself at a fork in the road.  One path would mean catching up on a technical level and then getting in front of where the future of HPC lies before it gets there.  The other path would mean trying to steer the entire DOE HPC ship in the right direction, as long as that may take, and have faith that the people I bring along can race far enough ahead to tell me if we're still going where we need to go.  Perhaps a bit selfishly, I chose the former.  I'm just not ready to give up on racing ahead myself yet, and the only way I could hope to catch up was to make it a full-time job.</p><p>I don't claim to know the future, and a lot of what I've laid out is all speculative at best.  NERSC, ALCF, or OLCF very well may build another round of data centers to keep the DOE HPC party going for another decade.  However, there's no denying that the stakes keep getting higher with every passing year.</p><p>That all said, DOE has pulled off stranger things in the past, and it still has a bunch of talented people to make the best of whatever the future holds.</p><p></p>",
            "url": "https://hpc.social/personal-blog/2022/life-and-leaving-nersc/",
            
            
            
            
            
            "date_published": "2022-05-27T06:42:00-06:00",
            "date_modified": "2022-05-27T06:42:00-06:00",
            
                "author": "Glenn K. Lockwood's Blog"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/experimenting-with-igor-s-bluestore-wal/",
            "title": "Experimenting with Igor’s Bluestore WAL",
            "summary": null,
            "content_text": "Igor Fedetov is one of the most knowledgable developers working on Ceph.  He’s started working on replacing our use of RocksDB’s write ahead log with a bluestore native implementation.  After tuning we can achieve up to 122K random write IOPS on a single OSD!  That’s nearly a 50% improvment over the current main branch and over twice as fast as Pacific!",
            "content_html": "<p>Igor Fedetov is one of the most knowledgable developers working on Ceph.  He’s started working on replacing our use of RocksDB’s write ahead log with a bluestore native implementation.  After tuning we can <a href=\"https://docs.google.com/spreadsheets/d/1zETd1Nq_CbLNSh3R2II-z8efQizUjDYfHDBIcMwGNdg/edit?usp=sharing\">achieve</a> up to 122K random write IOPS on a single OSD!  That’s nearly a 50% improvment over the current main branch and over twice as fast as Pacific!</p>",
            "url": "https://hpc.social/personal-blog/2022/experimenting-with-igor-s-bluestore-wal/",
            
            
            
            
            
            "date_published": "2022-05-26T01:00:00-06:00",
            "date_modified": "2022-05-26T01:00:00-06:00",
            
                "author": "Mark Nelson's Blog"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/interesting-links-i-clicked-this-week/",
            "title": "Interesting links I clicked this week",
            "summary": null,
            "content_text": "I watched several really interesting talks from SRECon22 Americas this week, and in particular I&#8217;d like to highlight:Principled Performance Analytics, Narayan Desai and Brent Bryan from Google. Some interesting thoughts on quantitative analysis of live performance data for monitoring and observability purposes, moving past simple percentile analysis.The &#8216;Success&#8217; in SRE is Silent, Casey Rosenthal from Verica.io. Interesting thoughts here on the visibility of reliability, qualitative analysis of systems, and why regulation and certification might not be the right thing for web systems.Building and Running a Diversity-focused Pre-internship program for SRE, from Andrew Ryan at Facebook Meta. Some good lessons-learned here from an early-career internship-like program, in its first year.Taking the 737 to the Max, Nickolas Means from Sym. A really interesting analysis of the Boeing 737 Max failures from both a technical and cultural perspective, complete with some graph tracing to understand failure modes.I also ran across some other articles that I&#8217;ve been actively recommending and sharing with friends and colleagues, including:Plato&#8217;s Dashboards, Fred Hebert at Honeycomb. This article has some great analysis of how easily-measurable metrics are often poor proxies for the information we&#8217;re actually interested in, and discussing qualitative research methods as a way to gain more insight.The End of Roe Will Bring About A Sea Change In The Encryption Debate, Rianna Pfefferkorn from the Stanford Internet Observatory. You should absolutely go read this article, but to sum up: Law enforcement in states than ban abortion is now absolutely part of the threat model that encrypted messaging defends against. No one claiming to be a progressive should be arguing in favor of &#8220;exceptional access&#8221; or other law enforcement access to encryption.",
            "content_html": "<p>I watched several really interesting talks from <a href=\"https://www.usenix.org/conference/srecon22americas/program\">SRECon22 Americas</a> this week, and in particular I&#8217;d like to highlight:</p><ul><li><a href=\"https://www.usenix.org/conference/srecon22americas/presentation/desai\">Principled Performance Analytics</a>, Narayan Desai and Brent Bryan from Google. Some interesting thoughts on quantitative analysis of live performance data for monitoring and observability purposes, moving past simple percentile analysis.</li><li><a href=\"https://www.usenix.org/conference/srecon22americas/presentation/rosenthal\">The &#8216;Success&#8217; in SRE is Silent</a>, Casey Rosenthal from Verica.io. Interesting thoughts here on the visibility of reliability, qualitative analysis of systems, and why regulation and certification might not be the right thing for web systems.</li><li><a href=\"https://www.usenix.org/conference/srecon22americas/presentation/ryan\">Building and Running a Diversity-focused Pre-internship program for SRE</a>, from Andrew Ryan at <s>Facebook</s> Meta. Some good lessons-learned here from an early-career internship-like program, in its first year.</li><li><a href=\"https://www.usenix.org/conference/srecon22americas/presentation/means\">Taking the 737 to the Max</a>, Nickolas Means from Sym. A really interesting analysis of the Boeing 737 Max failures from both a technical and cultural perspective, complete with some graph tracing to understand failure modes.</li></ul><p>I also ran across some other articles that I&#8217;ve been actively recommending and sharing with friends and colleagues, including:</p><ul><li><a href=\"https://ferd.ca/plato-s-dashboards.html\">Plato&#8217;s Dashboards</a>, Fred Hebert at Honeycomb. This article has some great analysis of how easily-measurable metrics are often poor proxies for the information we&#8217;re actually interested in, and discussing qualitative research methods as a way to gain more insight.</li><li><a href=\"https://cyberlaw.stanford.edu/blog/2022/05/end-roe-will-bring-about-sea-change-encryption-debate\">The End of Roe Will Bring About A Sea Change In The Encryption Debate</a>, Rianna Pfefferkorn from the Stanford Internet Observatory. You should absolutely go read this article, but to sum up: Law enforcement in states than ban abortion is now <em>absolutely</em> part of the threat model that encrypted messaging defends against. No one claiming to be a progressive should be arguing in favor of &#8220;exceptional access&#8221; or other law enforcement access to encryption.</li></ul><p></p>",
            "url": "https://hpc.social/personal-blog/2022/interesting-links-i-clicked-this-week/",
            
            
            
            
            
            "date_published": "2022-05-14T19:35:32-06:00",
            "date_modified": "2022-05-14T19:35:32-06:00",
            
                "author": "Thinking Out Loud"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/customizing-command-output-in-ibm-spectrum-lsf/",
            "title": "Customizing command output in IBM Spectrum LSF",
            "summary": null,
            "content_text": "IBM Spectrum LSF provides many ways to query the LSF cluster for information about workloads. As a user, once you’ve submitted a job to LSF, it’s logical to want to understand what has happened to your job.  Has the job started yet?  Is the job pending? If so, why is it pending? And the all important, “Is my job done yet?”.  Of course, LSF provides a very rich CLI which has been developed and refined - over the past three decades. It’s also possible to get JSON-formatted output from various LSF query commands. This is useful for users and administrators alike as JSON-formatted output is easy to parse, and scripting can be used to extract values from the JSON output.This is not meant to be a definitive guide on how to query information in LSF, but rather provides some examples of the various ways that users can query job related information using the LSF CLI.  This will include a look at the -json and -o  options which have been introduced during the lifecycle of LSF v10.1.0 family. The -json option can be used to provide JSON-formatted output from various LSF query commands and the -o  can be used to customize the fields in the output to only those desired.We’ll start with a simple job submission.  Here we submit a test workload as a non-root user in the LSF cluster.$ bsub -o $HOME/output.%J -e $HOME/error.%J ./testjob.shJob &lt;24520&gt; is submitted to default queue &lt;normal&gt;.With the unique jobID number 24520, we can now query LSF for information about the job:$ bjobs 24520JOBID   USER    STAT  QUEUE      FROM_HOST   EXEC_HOST   JOB_NAME   SUBMIT_TIME24520   gsamu   RUN   normal     kilenc      kilenc      *estjob.sh May 10 21:09Adding the -l option to bjobs provides the long output (more details).$ bjobs -l 24520Job &lt;24520&gt;, User &lt;gsamu&gt;, Project &lt;default&gt;, Status &lt;RUN&gt;, Queue &lt;normal&gt;, Com                     mand &lt;./testjob.sh&gt;, Share group charged &lt;/gsamu&gt;Tue May 10 21:09:22: Submitted from host &lt;kilenc&gt;, CWD &lt;$HOME&gt;, Output File &lt;/h                     ome/gsamu/output.24520&gt;, Error File &lt;/home/gsamu/error.245                     20&gt;;Tue May 10 21:09:23: Started 1 Task(s) on Host(s) &lt;kilenc&gt;, Allocated 1 Slot(s)                      on Host(s) &lt;kilenc&gt;, Execution Home &lt;/home/gsamu&gt;, Execut                     ion CWD &lt;/home/gsamu&gt;;Tue May 10 21:10:01: Resource usage collected.                     MEM: 12 Mbytes;  SWAP: 0 Mbytes;  NTHREAD: 5                     PGID: 313588;  PIDs: 313588 313589 313591 313592  MEMORY USAGE: MAX MEM: 12 Mbytes;  AVG MEM: 10 Mbytes SCHEDULING PARAMETERS:           r15s   r1m  r15m   ut      pg    io   ls    it    tmp    swp    mem loadSched   -     -     -     -       -     -    -     -     -      -      -   loadStop    -     -     -     -       -     -    -     -     -      -      -   RESOURCE REQUIREMENT DETAILS: Combined: select[type == local] order[r15s:pg] Effective: select[type == local] order[r15s:pg] It is possible to customize the output format of the bjobs command using the -o  option. In this case, we want to show only some specific details about the job in the output of bjobs. We’ve selected to view: jobID, job status, project name, memory consumed, output and error files. A full list of the available fields for the custom format can be found here.$ bjobs -o \"jobid stat: queue:- project:10  mem:12:G output_file error_file\" 24520JOBID STAT       QUEUE PROJ_NAME  MEM          OUTPUT_FILE ERROR_FILE24520 RUN       normal default    0.01 G       /home/gsamu/output.24520 /home/gsamu/error.24520Adding the -json option, it’s possible to get this customized job output in JSON format.$ bjobs -o \"jobid stat: queue:- project:10  mem:12:G output_file error_file\" -json 24520{  \"COMMAND\":\"bjobs\",  \"JOBS\":1,  \"RECORDS\":[    {      \"JOBID\":\"24520\",      \"STAT\":\"RUN\",      \"QUEUE\":\"normal\",      \"PROJ_NAME\":\"default\",      \"MEM\":\"0.01 G\",      \"OUTPUT_FILE\":\"\\/home\\/gsamu\\/output.24520\",      \"ERROR_FILE\":\"\\/home\\/gsamu\\/error.24520\"    }  ]}Next, let’s look at the bhist command. This can be used to view historical data about jobs.$ bhist 24520Summary of time in seconds spent in various states:JOBID   USER    JOB_NAME  PEND    PSUSP   RUN     USUSP   SSUSP   UNKWN   TOTAL24520   gsamu   *tjob.sh  1       0       457     0       0       0       458       We see that the job command has been truncated. Let’s now run bhist again with the -w option to produce a wide output.$ bhist -w 24520Summary of time in seconds spent in various states:JOBID   USER    JOB_NAME  PEND    PSUSP   RUN     USUSP   SSUSP   UNKWN   TOTAL24520   gsamu   ./testjob.sh 1       0       462     0       0       0       463       And finally, with the -l option to produce a long, detailed output.$ bhist -l 24520Job &lt;24520&gt;, User &lt;gsamu&gt;, Project &lt;default&gt;, Command &lt;./testjob.sh&gt;Tue May 10 21:09:22: Submitted from host &lt;kilenc&gt;, to Queue &lt;normal&gt;, CWD &lt;$HOM                     E&gt;, Output File &lt;/home/gsamu/output.%J&gt;, Error File &lt;/home                     /gsamu/error.%J&gt;;Tue May 10 21:09:23: Dispatched 1 Task(s) on Host(s) &lt;kilenc&gt;, Allocated 1 Slot                     (s) on Host(s) &lt;kilenc&gt;, Effective RES_REQ &lt;select[type ==                      local] order[r15s:pg] &gt;;Tue May 10 21:09:25: Starting (Pid 313588);Tue May 10 21:09:25: Running with execution home &lt;/home/gsamu&gt;, Execution CWD &lt;                     /home/gsamu&gt;, Execution Pid &lt;313588&gt;;Summary of time in seconds spent in various states by  Tue May 10 21:17:26  PEND     PSUSP    RUN      USUSP    SSUSP    UNKWN    TOTAL  1        0        483      0        0        0        484         When the job is done, the bacct command can be used to get detailed accounting information for jobs.$ bacct 24520Accounting information about jobs that are:   - submitted by all users.  - accounted on all projects.  - completed normally or exited  - executed on all hosts.  - submitted to all queues.  - accounted on all service classes.  - accounted to all RC accounts.------------------------------------------------------------------------------SUMMARY:      ( time unit: second )  Total number of done jobs:       1      Total number of exited jobs:     0 Total CPU time consumed:       3.4      Average CPU time consumed:     3.4 Maximum CPU time of a job:     3.4      Minimum CPU time of a job:     3.4 Total wait time in queues:     1.0 Average wait time in queue:    1.0 Maximum wait time in queue:    1.0      Minimum wait time in queue:    1.0 Average turnaround time:       669 (seconds/job) Maximum turnaround time:       669      Minimum turnaround time:       669 Average hog factor of a job:  0.01 ( cpu time / turnaround time ) Maximum hog factor of a job:  0.01      Minimum hog factor of a job:  0.01 Average expansion factor of a job:  1.00 ( turnaround time / run time ) Maximum expansion factor of a job:  1.00 Minimum expansion factor of a job:  1.00 Total Run time consumed:       668      Average Run time consumed:     668 Maximum Run time of a job:     668      Minimum Run time of a job:     668 Scheduler Efficiency for 1 jobs Slot Utilization:          100.00%  Memory Utilization:            100.00% And now the long, detailed output from bacct using the -l parameter.$ bacct -l 24520Accounting information about jobs that are:   - submitted by all users.  - accounted on all projects.  - completed normally or exited  - executed on all hosts.  - submitted to all queues.  - accounted on all service classes.  - accounted to all RC accounts.------------------------------------------------------------------------------Job &lt;24520&gt;, User &lt;gsamu&gt;, Project &lt;default&gt;, Status &lt;DONE&gt;, Queue &lt;normal&gt;, Co                     mmand &lt;./testjob.sh&gt;, Share group charged &lt;/gsamu&gt;Tue May 10 21:09:22: Submitted from host &lt;kilenc&gt;, CWD &lt;$HOME&gt;, Output File &lt;/h                     ome/gsamu/output.%J&gt;, Error File &lt;/home/gsamu/error.%J&gt;;Tue May 10 21:09:23: Dispatched 1 Task(s) on Host(s) &lt;kilenc&gt;, Allocated 1 Slot                     (s) on Host(s) &lt;kilenc&gt;, Effective RES_REQ &lt;select[type ==                      local] order[r15s:pg] &gt;;Tue May 10 21:20:31: Completed &lt;done&gt;.Accounting information about this job:     Share group charged &lt;/gsamu&gt;     CPU_T     WAIT     TURNAROUND   STATUS     HOG_FACTOR    MEM    SWAP      3.37        1            669     done         0.0050    12M      0M------------------------------------------------------------------------------SUMMARY:      ( time unit: second )  Total number of done jobs:       1      Total number of exited jobs:     0 Total CPU time consumed:       3.4      Average CPU time consumed:     3.4 Maximum CPU time of a job:     3.4      Minimum CPU time of a job:     3.4 Total wait time in queues:     1.0 Average wait time in queue:    1.0 Maximum wait time in queue:    1.0      Minimum wait time in queue:    1.0 Average turnaround time:       669 (seconds/job) Maximum turnaround time:       669      Minimum turnaround time:       669 Average hog factor of a job:  0.01 ( cpu time / turnaround time ) Maximum hog factor of a job:  0.01      Minimum hog factor of a job:  0.01 Average expansion factor of a job:  1.00 ( turnaround time / run time ) Maximum expansion factor of a job:  1.00 Minimum expansion factor of a job:  1.00 Total Run time consumed:       668      Average Run time consumed:     668 Maximum Run time of a job:     668      Minimum Run time of a job:     668 Scheduler Efficiency for 1 jobs Slot Utilization:          100.00%  Memory Utilization:            100.00% From jobs to queuesWe’ve looked briefly at querying LSF for job related information.  Let’s now take a closer look at querying LSF for information regarding the queue configuration.  Batch queues are where users submit jobs to. Queues can have a very wide array of attributes and settings.  Below we see a listing of the default queues configured in LSF Suite for HPC. The bqueues command is used to query LSF for the queue configuration.$ bqueuesQUEUE_NAME      PRIO STATUS          MAX JL/U JL/P JL/H NJOBS  PEND   RUN  SUSP admin            50  Open:Active       -    -    -    -     0     0     0     0owners           43  Open:Active       -    -    -    -     0     0     0     0priority         43  Open:Active       -    -    -    -     0     0     0     0night            40  Open:Active       -    -    -    -     0     0     0     0short            35  Open:Active       -    -    -    -     0     0     0     0dataq            33  Open:Active       -    -    -    -     0     0     0     0normal           30  Open:Active       -    -    -    -     0     0     0     0interactive      30  Open:Active       -    -    -    -     0     0     0     0idle             20  Open:Active       -    -    -    -     0     0     0     0The -l option of bqueues can be used to get a more details view about the queues. Here, we look at the long output for the queue normal.$ bqueues -l normalQUEUE: normal  -- For normal low priority jobs, running only if hosts are lightly loaded.  This is the default queue.PARAMETERS/STATISTICSPRIO NICE STATUS          MAX JL/U JL/P JL/H NJOBS  PEND   RUN SSUSP USUSP  RSV PJOBS  30    0  Open:Active       -    -    -    -     0     0     0     0     0    0     0Interval for a host to accept two jobs is 0 secondsSCHEDULING PARAMETERS           r15s   r1m  r15m   ut      pg    io   ls    it    tmp    swp    mem loadSched   -     -     -     -       -     -    -     -     -      -      -   loadStop    -     -     -     -       -     -    -     -     -      -      -  SCHEDULING POLICIES:  FAIRSHARE  NO_INTERACTIVEUSER_SHARES:  [default, 1] SHARE_INFO_FOR: normal/ USER/GROUP   SHARES  PRIORITY  STARTED  RESERVED  CPU_TIME  RUN_TIME   ADJUST  GPU_RUN_TIMEgsamu           1       0.333      0        0         0.0        0       0.000             0elasticsearch     1       0.333      0        0         0.0        0       0.000             0USERS: all  HOSTS:  all Custom output formatting can also be used for the bqueues command. Below is an example of the use of custom output formatting using the -o  parameter. For this example, we display queue name, status and the number of jobs (all states). More details about the bqueues -o  parameter can be found here.$ bqueues -o \"queue_name:12 status:12 njobs\"QUEUE_NAME   STATUS       NJOBSadmin        Open:Active  0owners       Open:Active  0priority     Open:Active  0night        Open:Active  0short        Open:Active  0dataq        Open:Active  0normal       Open:Active  0interactive  Open:Active  0idle         Open:Active  0And for JSON-formatted output, we add the -json parameter.$ bqueues -json -o \"queue_name:12 status:12 njobs\"{  \"COMMAND\":\"bqueues\",  \"QUEUES\":9,  \"RECORDS\":[    {      \"QUEUE_NAME\":\"admin\",      \"STATUS\":\"Open:Active\",      \"NJOBS\":\"0\"    },    {      \"QUEUE_NAME\":\"owners\",      \"STATUS\":\"Open:Active\",      \"NJOBS\":\"0\"    },    {      \"QUEUE_NAME\":\"priority\",      \"STATUS\":\"Open:Active\",      \"NJOBS\":\"0\"    },    {      \"QUEUE_NAME\":\"night\",      \"STATUS\":\"Open:Active\",      \"NJOBS\":\"0\"    },    {      \"QUEUE_NAME\":\"short\",      \"STATUS\":\"Open:Active\",      \"NJOBS\":\"0\"    },    {      \"QUEUE_NAME\":\"dataq\",      \"STATUS\":\"Open:Active\",      \"NJOBS\":\"0\"    },    {      \"QUEUE_NAME\":\"normal\",      \"STATUS\":\"Open:Active\",      \"NJOBS\":\"0\"    },    {      \"QUEUE_NAME\":\"interactive\",      \"STATUS\":\"Open:Active\",      \"NJOBS\":\"0\"    },    {      \"QUEUE_NAME\":\"idle\",      \"STATUS\":\"Open:Active\",      \"NJOBS\":\"0\"    }  ]}From queues to serversFinally, we’ll look at the LSF bhosts command, which is used to display information about the batch hosts in the LSF cluster.$ bhostsHOST_NAME          STATUS       JL/U    MAX  NJOBS    RUN  SSUSP  USUSP    RSV archie             ok              -      2      0      0      0      0      0kilenc             ok              -     32      0      0      0      0      0To view detailed information about a batch host, the -l parameter can be specified for bhosts.  Here we query for information on host archie.$ bhosts -l archieHOST  archieSTATUS           CPUF  JL/U    MAX  NJOBS    RUN  SSUSP  USUSP    RSV DISPATCH_WINDOWok               6.00     -      2      0      0      0      0      0      - CURRENT LOAD USED FOR SCHEDULING:                r15s   r1m  r15m    ut    pg    io   ls    it   tmp   swp   mem  slots  ngpus Total           0.0   0.0   0.0    0%   0.0     1    1   437 3456M    0M  1.7G      2    0.0 Reserved        0.0   0.0   0.0    0%   0.0     0    0     0    0M    0M    0M      -     -                ngpus_physical gpu_shared_avg_ut gpu_shared_avg_mut gpu_mode0 Total                    0.0               0.0                0.0       0.0 Reserved                  -                 -                  -         -                gpu_mode1 gpu_mode2 gpu_mode3 gpu_mode4 gpu_mode5 gpu_mode6 Total               0.0       0.0       0.0       0.0       0.0       0.0 Reserved             -         -         -         -         -         -                gpu_mode7 gpu_temp0 gpu_temp1 gpu_temp2 gpu_temp3 gpu_temp4 Total               0.0       0.0       0.0       0.0       0.0       0.0 Reserved             -         -         -         -         -         -                gpu_temp5 gpu_temp6 gpu_temp7 gpu_ecc0 gpu_ecc1 gpu_ecc2 gpu_ecc3 Total               0.0       0.0       0.0      0.0      0.0      0.0      0.0 Reserved             -         -         -        -        -        -        -                gpu_ecc4 gpu_ecc5 gpu_ecc6 gpu_ecc7 gpu_ut0 gpu_ut1 gpu_ut2 gpu_ut3 Total              0.0      0.0      0.0      0.0     0.0     0.0     0.0     0.0 Reserved            -        -        -        -       -       -       -       -                gpu_ut4 gpu_ut5 gpu_ut6 gpu_ut7 gpu_mut0 gpu_mut1 gpu_mut2 gpu_mut3 Total             0.0     0.0     0.0     0.0      0.0      0.0      0.0      0.0 Reserved           -       -       -       -        -        -        -        -                gpu_mut4 gpu_mut5 gpu_mut6 gpu_mut7 gpu_mtotal0 gpu_mtotal1 Total              0.0      0.0      0.0      0.0         0.0         0.0 Reserved            -        -        -        -           -           -                gpu_mtotal2 gpu_mtotal3 gpu_mtotal4 gpu_mtotal5 gpu_mtotal6 Total                 0.0         0.0         0.0         0.0         0.0 Reserved               -           -           -           -           -                gpu_mtotal7 gpu_mused0 gpu_mused1 gpu_mused2 gpu_mused3 gpu_mused4 Total                 0.0        0.0        0.0        0.0        0.0        0.0 Reserved               -          -          -          -          -          -                gpu_mused5 gpu_mused6 gpu_mused7 gpu_maxfactor Total                0.0        0.0        0.0           0.0 Reserved              -          -          -             -  LOAD THRESHOLD USED FOR SCHEDULING:           r15s   r1m  r15m   ut      pg    io   ls    it    tmp    swp    mem loadSched   -     -     -     -       -     -    -     -     -      -      -   loadStop    -     -     -     -       -     -    -     -     -      -      -    CONFIGURED AFFINITY CPU LIST: allSimilar to bjobs and bqueues, the -o  parameter can be used for custom formatting of output of bhosts. Below is an example of the use of custom output formatting using the -o  parameter. For this example, we display host name, status and the number of jobs (all states). More details about the bqueues -o  parameter can be found here.$ bhosts -o \"host_name:12 status:12 njobs\" HOST_NAME    STATUS       NJOBSarchie       ok           0kilenc       ok           0And adding the -json parameter for JSON-formatted output.$ bhosts -json -o \"host_name:12 status:12 njobs\" {  \"COMMAND\":\"bhosts\",  \"HOSTS\":2,  \"RECORDS\":[    {      \"HOST_NAME\":\"archie\",      \"STATUS\":\"ok\",      \"NJOBS\":\"0\"    },    {      \"HOST_NAME\":\"kilenc\",      \"STATUS\":\"ok\",      \"NJOBS\":\"0\"    }  ]}That concludes our brief look at LSF query commands. We’ve only scratched the surface here in terms of capabilities and query commands for LSF. The LSF command line interface is powerful and flexible including ways to customize the command outputs and to output in JSON-format.  For more details, the complete set of IBM Spectrum LSF documentation can be found online at IBM Documentation here.",
            "content_html": "<p><a href=\"https://www.ibm.com/products/hpc-workload-management\">IBM Spectrum LSF</a> provides many ways to query the LSF cluster for information about workloads. As a user, once you’ve submitted a job to LSF, it’s logical to want to understand what has happened to your job.  Has the job started yet?  Is the job pending? If so, why is it pending? And the all important, “Is my job done yet?”.  Of course, LSF provides a very rich CLI which has been developed and refined - over the past three decades. It’s also possible to get JSON-formatted output from various LSF query commands. This is useful for users and administrators alike as JSON-formatted output is easy to parse, and scripting can be used to extract values from the JSON output.</p><p>This is not meant to be a definitive guide on how to query information in LSF, but rather provides some examples of the various ways that users can query job related information using the LSF CLI.  This will include a look at the <em>-json</em> and <em>-o <!-- raw HTML omitted --></em> options which have been introduced during the lifecycle of LSF v10.1.0 family. The <em>-json</em> option can be used to provide JSON-formatted output from various LSF query commands and the <em>-o <!-- raw HTML omitted --></em> can be used to customize the fields in the output to only those desired.</p><p>We’ll start with a simple job submission.  Here we submit a test workload as a non-root user in the LSF cluster.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ bsub -o $HOME/output.%J -e $HOME/error.%J ./testjob.shJob &lt;24520&gt; is submitted to default queue &lt;normal&gt;.</code></pre></div><p>With the unique jobID number 24520, we can now query LSF for information about the job:</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ bjobs 24520JOBID   USER    STAT  QUEUE      FROM_HOST   EXEC_HOST   JOB_NAME   SUBMIT_TIME24520   gsamu   RUN   normal     kilenc      kilenc      *estjob.sh May 10 21:09</code></pre></div><p>Adding the <em>-l</em> option to <em>bjobs</em> provides the long output (more details).</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ bjobs -l 24520Job &lt;24520&gt;, User &lt;gsamu&gt;, Project &lt;default&gt;, Status &lt;RUN&gt;, Queue &lt;normal&gt;, Com                     mand &lt;./testjob.sh&gt;, Share group charged &lt;/gsamu&gt;Tue May 10 21:09:22: Submitted from host &lt;kilenc&gt;, CWD &lt;$HOME&gt;, Output File &lt;/h                     ome/gsamu/output.24520&gt;, Error File &lt;/home/gsamu/error.245                     20&gt;;Tue May 10 21:09:23: Started 1 Task(s) on Host(s) &lt;kilenc&gt;, Allocated 1 Slot(s)                      on Host(s) &lt;kilenc&gt;, Execution Home &lt;/home/gsamu&gt;, Execut                     ion CWD &lt;/home/gsamu&gt;;Tue May 10 21:10:01: Resource usage collected.                     MEM: 12 Mbytes;  SWAP: 0 Mbytes;  NTHREAD: 5                     PGID: 313588;  PIDs: 313588 313589 313591 313592  MEMORY USAGE: MAX MEM: 12 Mbytes;  AVG MEM: 10 Mbytes SCHEDULING PARAMETERS:           r15s   r1m  r15m   ut      pg    io   ls    it    tmp    swp    mem loadSched   -     -     -     -       -     -    -     -     -      -      -   loadStop    -     -     -     -       -     -    -     -     -      -      -   RESOURCE REQUIREMENT DETAILS: Combined: select[type == local] order[r15s:pg] Effective: select[type == local] order[r15s:pg] </code></pre></div><p>It is possible to customize the output format of the <em>bjobs</em> command using the <em>-o <!-- raw HTML omitted --></em> option. In this case, we want to show only some specific details about the job in the output of bjobs. We’ve selected to view: jobID, job status, project name, memory consumed, output and error files. A full list of the available fields for the custom format can be found <a href=\"https://www.ibm.com/docs/en/spectrum-lsf/10.1.0?topic=options-o\">here</a>.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ bjobs -o \"jobid stat: queue:- project:10  mem:12:G output_file error_file\" 24520JOBID STAT       QUEUE PROJ_NAME  MEM          OUTPUT_FILE ERROR_FILE24520 RUN       normal default    0.01 G       /home/gsamu/output.24520 /home/gsamu/error.24520</code></pre></div><p>Adding the <em>-json</em> option, it’s possible to get this customized job output in JSON format.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ bjobs -o \"jobid stat: queue:- project:10  mem:12:G output_file error_file\" -json 24520{  \"COMMAND\":\"bjobs\",  \"JOBS\":1,  \"RECORDS\":[    {      \"JOBID\":\"24520\",      \"STAT\":\"RUN\",      \"QUEUE\":\"normal\",      \"PROJ_NAME\":\"default\",      \"MEM\":\"0.01 G\",      \"OUTPUT_FILE\":\"\\/home\\/gsamu\\/output.24520\",      \"ERROR_FILE\":\"\\/home\\/gsamu\\/error.24520\"    }  ]}</code></pre></div><p>Next, let’s look at the <em>bhist</em> command. This can be used to view historical data about jobs.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ bhist 24520Summary of time in seconds spent in various states:JOBID   USER    JOB_NAME  PEND    PSUSP   RUN     USUSP   SSUSP   UNKWN   TOTAL24520   gsamu   *tjob.sh  1       0       457     0       0       0       458       </code></pre></div><p>We see that the job command has been truncated. Let’s now run <em>bhist</em> again with the <em>-w</em> option to produce a wide output.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ bhist -w 24520Summary of time in seconds spent in various states:JOBID   USER    JOB_NAME  PEND    PSUSP   RUN     USUSP   SSUSP   UNKWN   TOTAL24520   gsamu   ./testjob.sh 1       0       462     0       0       0       463       </code></pre></div><p>And finally, with the <em>-l</em> option to produce a long, detailed output.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ bhist -l 24520Job &lt;24520&gt;, User &lt;gsamu&gt;, Project &lt;default&gt;, Command &lt;./testjob.sh&gt;Tue May 10 21:09:22: Submitted from host &lt;kilenc&gt;, to Queue &lt;normal&gt;, CWD &lt;$HOM                     E&gt;, Output File &lt;/home/gsamu/output.%J&gt;, Error File &lt;/home                     /gsamu/error.%J&gt;;Tue May 10 21:09:23: Dispatched 1 Task(s) on Host(s) &lt;kilenc&gt;, Allocated 1 Slot                     (s) on Host(s) &lt;kilenc&gt;, Effective RES_REQ &lt;select[type ==                      local] order[r15s:pg] &gt;;Tue May 10 21:09:25: Starting (Pid 313588);Tue May 10 21:09:25: Running with execution home &lt;/home/gsamu&gt;, Execution CWD &lt;                     /home/gsamu&gt;, Execution Pid &lt;313588&gt;;Summary of time in seconds spent in various states by  Tue May 10 21:17:26  PEND     PSUSP    RUN      USUSP    SSUSP    UNKWN    TOTAL  1        0        483      0        0        0        484         </code></pre></div><p>When the job is done, the <em>bacct</em> command can be used to get detailed accounting information for jobs.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ bacct 24520Accounting information about jobs that are:   - submitted by all users.  - accounted on all projects.  - completed normally or exited  - executed on all hosts.  - submitted to all queues.  - accounted on all service classes.  - accounted to all RC accounts.------------------------------------------------------------------------------SUMMARY:      ( time unit: second )  Total number of done jobs:       1      Total number of exited jobs:     0 Total CPU time consumed:       3.4      Average CPU time consumed:     3.4 Maximum CPU time of a job:     3.4      Minimum CPU time of a job:     3.4 Total wait time in queues:     1.0 Average wait time in queue:    1.0 Maximum wait time in queue:    1.0      Minimum wait time in queue:    1.0 Average turnaround time:       669 (seconds/job) Maximum turnaround time:       669      Minimum turnaround time:       669 Average hog factor of a job:  0.01 ( cpu time / turnaround time ) Maximum hog factor of a job:  0.01      Minimum hog factor of a job:  0.01 Average expansion factor of a job:  1.00 ( turnaround time / run time ) Maximum expansion factor of a job:  1.00 Minimum expansion factor of a job:  1.00 Total Run time consumed:       668      Average Run time consumed:     668 Maximum Run time of a job:     668      Minimum Run time of a job:     668 Scheduler Efficiency for 1 jobs Slot Utilization:          100.00%  Memory Utilization:            100.00% </code></pre></div><p>And now the long, detailed output from bacct using the <em>-l</em> parameter.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ bacct -l 24520Accounting information about jobs that are:   - submitted by all users.  - accounted on all projects.  - completed normally or exited  - executed on all hosts.  - submitted to all queues.  - accounted on all service classes.  - accounted to all RC accounts.------------------------------------------------------------------------------Job &lt;24520&gt;, User &lt;gsamu&gt;, Project &lt;default&gt;, Status &lt;DONE&gt;, Queue &lt;normal&gt;, Co                     mmand &lt;./testjob.sh&gt;, Share group charged &lt;/gsamu&gt;Tue May 10 21:09:22: Submitted from host &lt;kilenc&gt;, CWD &lt;$HOME&gt;, Output File &lt;/h                     ome/gsamu/output.%J&gt;, Error File &lt;/home/gsamu/error.%J&gt;;Tue May 10 21:09:23: Dispatched 1 Task(s) on Host(s) &lt;kilenc&gt;, Allocated 1 Slot                     (s) on Host(s) &lt;kilenc&gt;, Effective RES_REQ &lt;select[type ==                      local] order[r15s:pg] &gt;;Tue May 10 21:20:31: Completed &lt;done&gt;.Accounting information about this job:     Share group charged &lt;/gsamu&gt;     CPU_T     WAIT     TURNAROUND   STATUS     HOG_FACTOR    MEM    SWAP      3.37        1            669     done         0.0050    12M      0M------------------------------------------------------------------------------SUMMARY:      ( time unit: second )  Total number of done jobs:       1      Total number of exited jobs:     0 Total CPU time consumed:       3.4      Average CPU time consumed:     3.4 Maximum CPU time of a job:     3.4      Minimum CPU time of a job:     3.4 Total wait time in queues:     1.0 Average wait time in queue:    1.0 Maximum wait time in queue:    1.0      Minimum wait time in queue:    1.0 Average turnaround time:       669 (seconds/job) Maximum turnaround time:       669      Minimum turnaround time:       669 Average hog factor of a job:  0.01 ( cpu time / turnaround time ) Maximum hog factor of a job:  0.01      Minimum hog factor of a job:  0.01 Average expansion factor of a job:  1.00 ( turnaround time / run time ) Maximum expansion factor of a job:  1.00 Minimum expansion factor of a job:  1.00 Total Run time consumed:       668      Average Run time consumed:     668 Maximum Run time of a job:     668      Minimum Run time of a job:     668 Scheduler Efficiency for 1 jobs Slot Utilization:          100.00%  Memory Utilization:            100.00% </code></pre></div><p><strong>From jobs to queues</strong></p><p>We’ve looked briefly at querying LSF for job related information.  Let’s now take a closer look at querying LSF for information regarding the queue configuration.  Batch queues are where users submit jobs to. Queues can have a very wide array of attributes and settings.  Below we see a listing of the default queues configured in LSF Suite for HPC. The <em>bqueues</em> command is used to query LSF for the queue configuration.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ bqueuesQUEUE_NAME      PRIO STATUS          MAX JL/U JL/P JL/H NJOBS  PEND   RUN  SUSP admin            50  Open:Active       -    -    -    -     0     0     0     0owners           43  Open:Active       -    -    -    -     0     0     0     0priority         43  Open:Active       -    -    -    -     0     0     0     0night            40  Open:Active       -    -    -    -     0     0     0     0short            35  Open:Active       -    -    -    -     0     0     0     0dataq            33  Open:Active       -    -    -    -     0     0     0     0normal           30  Open:Active       -    -    -    -     0     0     0     0interactive      30  Open:Active       -    -    -    -     0     0     0     0idle             20  Open:Active       -    -    -    -     0     0     0     0</code></pre></div><p>The <em>-l</em> option of <em>bqueues</em> can be used to get a more details view about the queues. Here, we look at the long output for the queue <em>normal</em>.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ bqueues -l normalQUEUE: normal  -- For normal low priority jobs, running only if hosts are lightly loaded.  This is the default queue.PARAMETERS/STATISTICSPRIO NICE STATUS          MAX JL/U JL/P JL/H NJOBS  PEND   RUN SSUSP USUSP  RSV PJOBS  30    0  Open:Active       -    -    -    -     0     0     0     0     0    0     0Interval for a host to accept two jobs is 0 secondsSCHEDULING PARAMETERS           r15s   r1m  r15m   ut      pg    io   ls    it    tmp    swp    mem loadSched   -     -     -     -       -     -    -     -     -      -      -   loadStop    -     -     -     -       -     -    -     -     -      -      -  SCHEDULING POLICIES:  FAIRSHARE  NO_INTERACTIVEUSER_SHARES:  [default, 1] SHARE_INFO_FOR: normal/ USER/GROUP   SHARES  PRIORITY  STARTED  RESERVED  CPU_TIME  RUN_TIME   ADJUST  GPU_RUN_TIMEgsamu           1       0.333      0        0         0.0        0       0.000             0elasticsearch     1       0.333      0        0         0.0        0       0.000             0USERS: all  HOSTS:  all </code></pre></div><p>Custom output formatting can also be used for the <em>bqueues</em> command. Below is an example of the use of custom output formatting using the <em>-o <!-- raw HTML omitted --></em> parameter. For this example, we display queue name, status and the number of jobs (all states). More details about the <em>bqueues -o <!-- raw HTML omitted --></em> parameter can be found <a href=\"https://www.ibm.com/docs/en/spectrum-lsf/10.1.0?topic=reference-bqueues\">here</a>.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ bqueues -o \"queue_name:12 status:12 njobs\"QUEUE_NAME   STATUS       NJOBSadmin        Open:Active  0owners       Open:Active  0priority     Open:Active  0night        Open:Active  0short        Open:Active  0dataq        Open:Active  0normal       Open:Active  0interactive  Open:Active  0idle         Open:Active  0</code></pre></div><p>And for JSON-formatted output, we add the <em>-json</em> parameter.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ bqueues -json -o \"queue_name:12 status:12 njobs\"{  \"COMMAND\":\"bqueues\",  \"QUEUES\":9,  \"RECORDS\":[    {      \"QUEUE_NAME\":\"admin\",      \"STATUS\":\"Open:Active\",      \"NJOBS\":\"0\"    },    {      \"QUEUE_NAME\":\"owners\",      \"STATUS\":\"Open:Active\",      \"NJOBS\":\"0\"    },    {      \"QUEUE_NAME\":\"priority\",      \"STATUS\":\"Open:Active\",      \"NJOBS\":\"0\"    },    {      \"QUEUE_NAME\":\"night\",      \"STATUS\":\"Open:Active\",      \"NJOBS\":\"0\"    },    {      \"QUEUE_NAME\":\"short\",      \"STATUS\":\"Open:Active\",      \"NJOBS\":\"0\"    },    {      \"QUEUE_NAME\":\"dataq\",      \"STATUS\":\"Open:Active\",      \"NJOBS\":\"0\"    },    {      \"QUEUE_NAME\":\"normal\",      \"STATUS\":\"Open:Active\",      \"NJOBS\":\"0\"    },    {      \"QUEUE_NAME\":\"interactive\",      \"STATUS\":\"Open:Active\",      \"NJOBS\":\"0\"    },    {      \"QUEUE_NAME\":\"idle\",      \"STATUS\":\"Open:Active\",      \"NJOBS\":\"0\"    }  ]}</code></pre></div><p><strong>From queues to servers</strong></p><p>Finally, we’ll look at the LSF <em>bhosts</em> command, which is used to display information about the batch hosts in the LSF cluster.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ bhostsHOST_NAME          STATUS       JL/U    MAX  NJOBS    RUN  SSUSP  USUSP    RSV archie             ok              -      2      0      0      0      0      0kilenc             ok              -     32      0      0      0      0      0</code></pre></div><p>To view detailed information about a batch host, the <em>-l</em> parameter can be specified for <em>bhosts</em>.  Here we query for information on host <em>archie</em>.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ bhosts -l archieHOST  archieSTATUS           CPUF  JL/U    MAX  NJOBS    RUN  SSUSP  USUSP    RSV DISPATCH_WINDOWok               6.00     -      2      0      0      0      0      0      - CURRENT LOAD USED FOR SCHEDULING:                r15s   r1m  r15m    ut    pg    io   ls    it   tmp   swp   mem  slots  ngpus Total           0.0   0.0   0.0    0%   0.0     1    1   437 3456M    0M  1.7G      2    0.0 Reserved        0.0   0.0   0.0    0%   0.0     0    0     0    0M    0M    0M      -     -                ngpus_physical gpu_shared_avg_ut gpu_shared_avg_mut gpu_mode0 Total                    0.0               0.0                0.0       0.0 Reserved                  -                 -                  -         -                gpu_mode1 gpu_mode2 gpu_mode3 gpu_mode4 gpu_mode5 gpu_mode6 Total               0.0       0.0       0.0       0.0       0.0       0.0 Reserved             -         -         -         -         -         -                gpu_mode7 gpu_temp0 gpu_temp1 gpu_temp2 gpu_temp3 gpu_temp4 Total               0.0       0.0       0.0       0.0       0.0       0.0 Reserved             -         -         -         -         -         -                gpu_temp5 gpu_temp6 gpu_temp7 gpu_ecc0 gpu_ecc1 gpu_ecc2 gpu_ecc3 Total               0.0       0.0       0.0      0.0      0.0      0.0      0.0 Reserved             -         -         -        -        -        -        -                gpu_ecc4 gpu_ecc5 gpu_ecc6 gpu_ecc7 gpu_ut0 gpu_ut1 gpu_ut2 gpu_ut3 Total              0.0      0.0      0.0      0.0     0.0     0.0     0.0     0.0 Reserved            -        -        -        -       -       -       -       -                gpu_ut4 gpu_ut5 gpu_ut6 gpu_ut7 gpu_mut0 gpu_mut1 gpu_mut2 gpu_mut3 Total             0.0     0.0     0.0     0.0      0.0      0.0      0.0      0.0 Reserved           -       -       -       -        -        -        -        -                gpu_mut4 gpu_mut5 gpu_mut6 gpu_mut7 gpu_mtotal0 gpu_mtotal1 Total              0.0      0.0      0.0      0.0         0.0         0.0 Reserved            -        -        -        -           -           -                gpu_mtotal2 gpu_mtotal3 gpu_mtotal4 gpu_mtotal5 gpu_mtotal6 Total                 0.0         0.0         0.0         0.0         0.0 Reserved               -           -           -           -           -                gpu_mtotal7 gpu_mused0 gpu_mused1 gpu_mused2 gpu_mused3 gpu_mused4 Total                 0.0        0.0        0.0        0.0        0.0        0.0 Reserved               -          -          -          -          -          -                gpu_mused5 gpu_mused6 gpu_mused7 gpu_maxfactor Total                0.0        0.0        0.0           0.0 Reserved              -          -          -             -  LOAD THRESHOLD USED FOR SCHEDULING:           r15s   r1m  r15m   ut      pg    io   ls    it    tmp    swp    mem loadSched   -     -     -     -       -     -    -     -     -      -      -   loadStop    -     -     -     -       -     -    -     -     -      -      -    CONFIGURED AFFINITY CPU LIST: all</code></pre></div><p>Similar to <em>bjobs</em> and <em>bqueues</em>, the <em>-o <!-- raw HTML omitted --></em> parameter can be used for custom formatting of output of bhosts. Below is an example of the use of custom output formatting using the <em>-o <!-- raw HTML omitted --></em> parameter. For this example, we display host name, status and the number of jobs (all states). More details about the bqueues <em>-o <!-- raw HTML omitted --></em> parameter can be found <a href=\"https://www.ibm.com/docs/en/spectrum-lsf/10.1.0?topic=reference-bhosts\">here</a>.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ bhosts -o \"host_name:12 status:12 njobs\" HOST_NAME    STATUS       NJOBSarchie       ok           0kilenc       ok           0</code></pre></div><p>And adding the <em>-json</em> parameter for JSON-formatted output.</p><div class=\"highlight\"><pre><code class=\"language-text\">$ bhosts -json -o \"host_name:12 status:12 njobs\" {  \"COMMAND\":\"bhosts\",  \"HOSTS\":2,  \"RECORDS\":[    {      \"HOST_NAME\":\"archie\",      \"STATUS\":\"ok\",      \"NJOBS\":\"0\"    },    {      \"HOST_NAME\":\"kilenc\",      \"STATUS\":\"ok\",      \"NJOBS\":\"0\"    }  ]}</code></pre></div><p>That concludes our brief look at LSF query commands. We’ve only scratched the surface here in terms of capabilities and query commands for LSF. The LSF command line interface is powerful and flexible including ways to customize the command outputs and to output in JSON-format.  For more details, the complete set of IBM Spectrum LSF documentation can be found online at IBM Documentation <a href=\"https://www.ibm.com/docs/en/spectrum-lsf/10.1.0\">here</a>.</p>",
            "url": "https://hpc.social/personal-blog/2022/customizing-command-output-in-ibm-spectrum-lsf/",
            
            
            
            
            
            "date_published": "2022-05-12T13:16:02-06:00",
            "date_modified": "2022-05-12T13:16:02-06:00",
            
                "author": "Ramblings of a supercomputing enthusiast."
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/pipelib-simple-library-to-parse-filter-and-sort-things/",
            "title": "Pipelib- Simple Library to Parse, Filter, and Sort Things",
            "summary": null,
            "content_text": "In early April I added an “update” command to Singularity Registry HPC (see the pull request here and needed to start with a list of docker tags andparse them into version strings to sort, and still return the original tag for later use.I wound up creating a custom class and set of functions that use distutils.LooseVersion to support that, but in creating this“hard coded thing” I stepped back and had a question.  Can we more intelligentally compose custom parsing pipelines?Specifically I wanted to:Start with a list of container tags for an image from a registryFilter out anything that looks like a commit, but isn't a string (e.g., latest)Derive a major, minor, and patch version for each, and filter to newestSort!For step 3, as an example if there was a 1.2.3-commitA and 1.2.3-commitB I’d only want to keep one, and the newer one of the two,so I could ask for “unique by patch” and filter the older one out.Ultimately of course I dove right in,and this led to the creation of Pipelib, which was an itch I terribly wanted to scratch! In this quick post, I want to share the overall design, because it was really fun to make.DesignBefore we talk about the design, let me show it to you.import pipelib.steps as stepimport pipelib.pipeline as pipeline# A pipeline to process a list of stringssteps = (   # convert everything to lowercase   step.transform.ToLowercase(),   # don't include anything with \"two\"   ~step.filters.HasPatterns(filters=[\"two\"]))# Strings to processitems = ['item-ONE', 'item-TWO', 'item-two-THREE']p = pipeline.Pipeline(steps)# The updated and transformed itemsupdated = p.run(items)# ['item-one']In the above, we take a pipeline object and add steps to it. That design is fairly simple,as the Pipeline class takes an optional iterable of things to process. I say “things” becausewe can give it steps, composed steps, or even entire other pipelines. Here is an exampleof adding an entire other Pipeline!import pipelib.steps as stepimport pipelib.pipeline as pipelinefruits = [\"Orange\", \"Melon\", \"Watermelon\", \"Fruit23\"]preprocess = pipeline.Pipeline(    steps = (        # Example of chaining steps together        step.filters.HasMaxLength(length=8) &amp; step.filters.HasAllLetters(),    ))# Add this preprocess step alongside other steps (make lowercase)steps = (   step.transform.ToLowercase(),   preprocess,)# Create a new pipeline and runp = pipeline.Pipeline(steps)# We should expect orange and melon!updated = p.run(fruits)['orange', 'melon']Implementation-wise, this is also fairly simple. We can check the underlying class of the provided objectand either add a single step, or insert a set of steps given another pipeline. In fact, pipelib comes with asmall set of “pipelines” that are ready for you to use. For example, here is one tofilter out “things that look like complete or partial git commits”import pipelib.steps as stepimport pipelib.pipeline as pipeline# Pre-generated sets of steps we can useimport pipelib.pipelines as pipelinespipeline.Pipeline(    pipelines.git.RemoveCommits).run([\"832b1c\", \"832b1c645e562d5cc6e376e5a3e058c02a40d92a\", \"123-abcd\"])[\"123-abcd\"]This is something I found useful because people sometimes use commits as Docker tags, and I don’t find this incredibly meaningful as a version to compare to (and want to remove them). Under the hood, it looks like this:RemoveCommits = pipeline.Pipeline(    steps=(        step.filters.HasMinLength(length=8) &amp; ~step.filters.HasAllLowerLettersNumbers(),    ))Do you also notice something interesting in the above? We are actually combining steps akin to logical operations.The above “pipeline” is actually just one step that combined other steps!pipelines.git.RemoveCommits.steps[HasMinLength_AND_NotHasAllLowerLettersNumbers]Let’s step back and talk about some concepts that allow this.ConceptsPipelineAs we’ve seen above, a pipeline is a collection of steps that take, as input, a listing of items and return a parser and filtered list.StepA step is some action in a pipeline. The way this works is that we have different kinds of steps, and this makes them easyto implement and even test. A boolean step is akin to a filter, and is expected to return True or False to indicate if the item passes, e.g., False means it’s filtered out. Boolean steps are neat because they afford different kinds of logic and combination.Logical OperationsLet’s say that we have a step that checks that an input is all letters:step.filters.HasAllLetters()For the above, anything that had a number (e.g., orange123) would be filtered out. But what if we wanted to inverse that, and allow passing of inputs that don’t have all letters (meaning we want numbers or special characters?) We can simply do that:~step.filters.HasAllLetters()Implementation wise, this was really fun to do! For Python to respect the logical operator ~ I simply define the “invert” function for the BooleanStep class.def __invert__(self):    \"\"\"    We can say \"~step\" and reverse the logic.    \"\"\"    self.reverse = True    return selfIt sets an attribute “reverse” to True, and returns itself, that way we use the same step, but with this variable set to be true.What does that do? In the “run” function of the BooleanStep we basically retrieve an outcome from the underlying step (True or False) and simply reverse it given that boolean is True! Again, it’s very simple, and allows for doing things like this:from pipelib.pipeline import Pipelineimport pipelib.steps as stepsPipeline(~steps.filters.HasAllLetters()).run([\"I-have-special-characters\", \"Idonot\"])['I-have-special-characters']Pipeline(steps.filters.HasAllLetters()).run([\"I-have-special-characters\", \"Idonot\"])['Idonot']What if we wanted to combine steps? E.g., what if I want to say “has all letters” OR “has minimum length 10?” If we put the stepsside by side we would only be able to support an AND - allowing passing through of entries that have all letters and the minimum length of 10.Pipelib supports both those operators - AND and OR as follows:&gt; step = steps.filters.HasAllLetters() &amp; steps.filters.HasMinLength(length=10)&gt; stepHasAllLetters_AND_HasMinLengthPipeline(step).run([\"thisonewillpass\", \"thisoneno\", \"notthisone2\"])['thisonewillpass']For both cases above, we are using the “and” and “or functions, respectively, and:Checking for class compatibility (both must be BooleanStep)Creating a list of composed steps to added to a class attribute \"composed\"Add the previous run functions too, naming based on the step class nameDefine a new run function that loops through the composed set, runs, updates and returns a shared resultName the class based on the combined names of the composed classesFor step 4 above, the operation (AND or OR) will vary depending on if the initial call was to “and” or “or”.The main difference between the two is that “OR” starts with a default of False (otherwise it would always return True)and AND starts with a default of True (otherwise it would always return False).And since we are always taking the first class “composed” attribute, this means that you can composesteps with other steps as many times as you like - a new check is simply added to the front or back ofthe list. The result (returned) is the new class that is ready to run. Here is what an OR looks like:&gt; step = steps.filters.HasAllLetters() | steps.filters.HasMinLength(length=10)&gt; stepHasAllLetters_OR_HasMinLengthPipeline(step).run([\"thisonewillpass\", \"veryshort\", \"12345\"])['thisonewillpass', 'veryshort']If you are interested in this function, you can see the entire thing here.Transformation OperationsA base step can be thought of as a transformation. Instead of expecting a boolean to be returned, we areinstead expecting a new value or None. In this respect the transform step can also act as a boolean as a returnof “None” will be removed from the list, however in most cases a transform is intended to perform an operation on the item passed. Here is an example of a transformation operation:Pipeline(steps.transform.ToLowercase()).run([\"AHHHH\"])['ahhhh']Sort OperationsA sort operation is a step that is one level up. Instead of operating on individual items, the stepre-defines a the higher level “run” function and does operations across the iterable.A good example from Pipelib is the use case that originally inspired me - to start with a messylist of Docker tags, do some parsing to derive versions, and return back a sorted list.pipeline.Pipeline(steps.container.ContainerTagSort(ascending=False)).run([\"1.2.3\", \"0.1.0\", \"8.3.2\"])['8.3.2', '1.2.3', '0.1.0']pipeline.Pipeline(steps.container.ContainerTagSort(ascending=True)).run([\"1.2.3\", \"0.1.0\", \"8.3.2\"])['0.1.0', '1.2.3', '8.3.2']In the above we also demonstrate that steps can take parameters, such as the order of a sort!This particular sorting step also allows you to say you want to return unique major, minor, or patchversions.pipeline.Pipeline(steps.container.ContainerTagSort(unique_major=True)).run([\"1.2.3\", \"1.1.0\", \"8.3.2\"])['8.3.2', '1.2.3']And if you wanted to do a more comprehensive clean up and sort, you could do something like this.WrapperPipelib needed a way to be able to pass around some parsed version of an item, but still maintainthe original. For example, let’s say I’m parsing Docker tags into something that resembles a loosesemantic version, I might have filtered 1.2.3-boop to be just 1.2.3, but at the end of theday I need the original tag to pull. Pipelib accomplishes this via wrappers.A wrapper is conceptually that - an internal wrapper class to an item that allows for storingan original value, and still doing operations to change a current state. Wrappers are used inside steps and allow for things like sorting and comparison. You probably don’t need to worry about wrappersunless you want to develop for pipelib. By default, wrappers and “extracted away” to return the basictypes. However, you can ask Pipelib to not do this unwrapping, and then you can get backthe derived and original values:tags  = [\"1.2.3\", \"1.1.0\", \"8.3.2\"]updated = pipeline.Pipeline(steps.container.ContainerTagSort()).run(tags, unwrap=False)# Notice that this just looks like a set of strings...updated['8.3.2', '1.2.3']# But actually we have wrappers, that each have an _original attributetype(updated[0])pipelib.wrappers.version.VersionWrapperConclusionI’ve had so much fun making this library! Like many of my projects it’s probably not super useful,but if you see a cool use case please let me know! I’m also happy to develop custom pipelines or stepsfor a use case that you might be interested in. Please don’t hesitate to ask me for help, I’m always runningout of fun things to do :)  Why should I care?Arguably you could just hard code this kind of filtering and sorting, but I think theidea of being able to customize and assemble steps is a cool one. If the steps are providedin a library it might might it slightly easier, or your work more reproducible because someone else can use the steps. And if you don’t care? That’s okay too. I recognize this wasmostly a fun project, and yet-another-itch I really wanted to scratch because I’ve nevermade a design like this before, either in terms of the idea or underlying testing and automation.",
            "content_html": "<p>In early April I added an “update” command to Singularity Registry HPC (<a href=\"https://github.com/singularityhub/singularity-hpc/pull/538\" target=\"_blank\">see the pull request here</a> and needed to start with a list of docker tags andparse them into version strings to sort, and still return the original tag for later use.I wound up creating a <a href=\"https://github.com/singularityhub/singularity-hpc/blob/main/shpc/main/container/update/versions.py\" target=\"_blank\">custom class and set of functions</a> that use <a href=\"https://github.com/python/cpython/blob/bd030b633f98ea5d9f93ef0105a51d2faf67070d/Lib/distutils/version.py#L269\" target=\"_blank\">distutils.LooseVersion</a> to support that, but in creating this“hard coded thing” I stepped back and had a question.</p><blockquote>  <p>Can we more intelligentally compose custom parsing pipelines?</p></blockquote><p>Specifically I wanted to:</p><ol class=\"custom-counter\"><li>Start with a list of container tags for an image from a registry</li><li>Filter out anything that looks like a commit, but isn't a string (e.g., latest)</li><li>Derive a major, minor, and patch version for each, and filter to newest</li><li>Sort!</li></ol><p>For step 3, as an example if there was a <code class=\"language-plaintext highlighter-rouge\">1.2.3-commitA</code> and <code class=\"language-plaintext highlighter-rouge\">1.2.3-commitB</code> I’d only want to keep one, and the newer one of the two,so I could ask for “unique by patch” and filter the older one out.Ultimately of course I <a href=\"https://twitter.com/vsoch/status/1516197732708282369\" target=\"_blank\">dove right in</a>,and this led to the creation of <a href=\"https://vsoch.github.io/pipelib\" target=\"_blank\">Pipelib</a>, which was an itch I terribly wanted to scratch! In this quick post, I want to share the overall design, because it was really fun to make.</p><div style=\"padding: 20px;\"><img src=\"https://raw.githubusercontent.com/vsoch/pipelib/main/docs/assets/pipelib-small.png\" /></div><h2 id=\"design\">Design</h2><p>Before we talk about the design, let me show it to you.</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">import</span> <span class=\"nn\">pipelib.steps</span> <span class=\"k\">as</span> <span class=\"n\">step</span><span class=\"kn\">import</span> <span class=\"nn\">pipelib.pipeline</span> <span class=\"k\">as</span> <span class=\"n\">pipeline</span><span class=\"c1\"># A pipeline to process a list of strings</span><span class=\"n\">steps</span> <span class=\"o\">=</span> <span class=\"p\">(</span>   <span class=\"c1\"># convert everything to lowercase</span>   <span class=\"n\">step</span><span class=\"p\">.</span><span class=\"n\">transform</span><span class=\"p\">.</span><span class=\"n\">ToLowercase</span><span class=\"p\">(),</span>   <span class=\"c1\"># don't include anything with \"two\"</span>   <span class=\"o\">~</span><span class=\"n\">step</span><span class=\"p\">.</span><span class=\"n\">filters</span><span class=\"p\">.</span><span class=\"n\">HasPatterns</span><span class=\"p\">(</span><span class=\"n\">filters</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">\"two\"</span><span class=\"p\">])</span><span class=\"p\">)</span><span class=\"c1\"># Strings to process</span><span class=\"n\">items</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s\">'item-ONE'</span><span class=\"p\">,</span> <span class=\"s\">'item-TWO'</span><span class=\"p\">,</span> <span class=\"s\">'item-two-THREE'</span><span class=\"p\">]</span><span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">pipeline</span><span class=\"p\">.</span><span class=\"n\">Pipeline</span><span class=\"p\">(</span><span class=\"n\">steps</span><span class=\"p\">)</span><span class=\"c1\"># The updated and transformed items</span><span class=\"n\">updated</span> <span class=\"o\">=</span> <span class=\"n\">p</span><span class=\"p\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">items</span><span class=\"p\">)</span><span class=\"c1\"># ['item-one']</span></code></pre></div></div><p>In the above, we take a pipeline object and add steps to it. That design is fairly simple,as the Pipeline class takes an optional iterable of things to process. I say “things” becausewe can give it steps, composed steps, or even entire other pipelines. Here is an exampleof adding an entire other Pipeline!</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">import</span> <span class=\"nn\">pipelib.steps</span> <span class=\"k\">as</span> <span class=\"n\">step</span><span class=\"kn\">import</span> <span class=\"nn\">pipelib.pipeline</span> <span class=\"k\">as</span> <span class=\"n\">pipeline</span><span class=\"n\">fruits</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s\">\"Orange\"</span><span class=\"p\">,</span> <span class=\"s\">\"Melon\"</span><span class=\"p\">,</span> <span class=\"s\">\"Watermelon\"</span><span class=\"p\">,</span> <span class=\"s\">\"Fruit23\"</span><span class=\"p\">]</span><span class=\"n\">preprocess</span> <span class=\"o\">=</span> <span class=\"n\">pipeline</span><span class=\"p\">.</span><span class=\"n\">Pipeline</span><span class=\"p\">(</span>    <span class=\"n\">steps</span> <span class=\"o\">=</span> <span class=\"p\">(</span>        <span class=\"c1\"># Example of chaining steps together</span>        <span class=\"n\">step</span><span class=\"p\">.</span><span class=\"n\">filters</span><span class=\"p\">.</span><span class=\"n\">HasMaxLength</span><span class=\"p\">(</span><span class=\"n\">length</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">)</span> <span class=\"o\">&amp;</span> <span class=\"n\">step</span><span class=\"p\">.</span><span class=\"n\">filters</span><span class=\"p\">.</span><span class=\"n\">HasAllLetters</span><span class=\"p\">(),</span>    <span class=\"p\">)</span><span class=\"p\">)</span><span class=\"c1\"># Add this preprocess step alongside other steps (make lowercase)</span><span class=\"n\">steps</span> <span class=\"o\">=</span> <span class=\"p\">(</span>   <span class=\"n\">step</span><span class=\"p\">.</span><span class=\"n\">transform</span><span class=\"p\">.</span><span class=\"n\">ToLowercase</span><span class=\"p\">(),</span>   <span class=\"n\">preprocess</span><span class=\"p\">,</span><span class=\"p\">)</span><span class=\"c1\"># Create a new pipeline and run</span><span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">pipeline</span><span class=\"p\">.</span><span class=\"n\">Pipeline</span><span class=\"p\">(</span><span class=\"n\">steps</span><span class=\"p\">)</span><span class=\"c1\"># We should expect orange and melon!</span><span class=\"n\">updated</span> <span class=\"o\">=</span> <span class=\"n\">p</span><span class=\"p\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">fruits</span><span class=\"p\">)</span><span class=\"p\">[</span><span class=\"s\">'orange'</span><span class=\"p\">,</span> <span class=\"s\">'melon'</span><span class=\"p\">]</span></code></pre></div></div><p>Implementation-wise, this is also fairly simple. We can check the underlying class of the provided objectand either add a single step, or insert a set of steps given another pipeline. In fact, pipelib comes with asmall set of “pipelines” that are ready for you to use. For example, here is one tofilter out “things that look like complete or partial git commits”</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">import</span> <span class=\"nn\">pipelib.steps</span> <span class=\"k\">as</span> <span class=\"n\">step</span><span class=\"kn\">import</span> <span class=\"nn\">pipelib.pipeline</span> <span class=\"k\">as</span> <span class=\"n\">pipeline</span><span class=\"c1\"># Pre-generated sets of steps we can use</span><span class=\"kn\">import</span> <span class=\"nn\">pipelib.pipelines</span> <span class=\"k\">as</span> <span class=\"n\">pipelines</span><span class=\"n\">pipeline</span><span class=\"p\">.</span><span class=\"n\">Pipeline</span><span class=\"p\">(</span>    <span class=\"n\">pipelines</span><span class=\"p\">.</span><span class=\"n\">git</span><span class=\"p\">.</span><span class=\"n\">RemoveCommits</span><span class=\"p\">).</span><span class=\"n\">run</span><span class=\"p\">([</span><span class=\"s\">\"832b1c\"</span><span class=\"p\">,</span> <span class=\"s\">\"832b1c645e562d5cc6e376e5a3e058c02a40d92a\"</span><span class=\"p\">,</span> <span class=\"s\">\"123-abcd\"</span><span class=\"p\">])</span><span class=\"p\">[</span><span class=\"s\">\"123-abcd\"</span><span class=\"p\">]</span></code></pre></div></div><p>This is something I found useful because people sometimes use commits as Docker tags, and I don’t find this incredibly meaningful as a version to compare to (and want to remove them). Under the hood, it looks like this:</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">RemoveCommits</span> <span class=\"o\">=</span> <span class=\"n\">pipeline</span><span class=\"p\">.</span><span class=\"n\">Pipeline</span><span class=\"p\">(</span>    <span class=\"n\">steps</span><span class=\"o\">=</span><span class=\"p\">(</span>        <span class=\"n\">step</span><span class=\"p\">.</span><span class=\"n\">filters</span><span class=\"p\">.</span><span class=\"n\">HasMinLength</span><span class=\"p\">(</span><span class=\"n\">length</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">)</span> <span class=\"o\">&amp;</span> <span class=\"o\">~</span><span class=\"n\">step</span><span class=\"p\">.</span><span class=\"n\">filters</span><span class=\"p\">.</span><span class=\"n\">HasAllLowerLettersNumbers</span><span class=\"p\">(),</span>    <span class=\"p\">)</span><span class=\"p\">)</span></code></pre></div></div><p>Do you also notice something interesting in the above? We are actually combining steps akin to logical operations.The above “pipeline” is actually just one step that combined other steps!</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">pipelines</span><span class=\"p\">.</span><span class=\"n\">git</span><span class=\"p\">.</span><span class=\"n\">RemoveCommits</span><span class=\"p\">.</span><span class=\"n\">steps</span><span class=\"p\">[</span><span class=\"n\">HasMinLength_AND_NotHasAllLowerLettersNumbers</span><span class=\"p\">]</span></code></pre></div></div><p>Let’s step back and talk about some concepts that allow this.</p><h2 id=\"concepts\">Concepts</h2><h3 id=\"pipeline\">Pipeline</h3><p>As we’ve seen above, a pipeline is a collection of steps that take, as input, a listing of items and return a parser and filtered list.</p><h3 id=\"step\">Step</h3><p>A step is some action in a pipeline. The way this works is that we have different kinds of steps, and this makes them easyto implement and even test. A <em>boolean</em> step is akin to a filter, and is expected to return True or False to indicate if the item passes, e.g., False means it’s filtered out. Boolean steps are neat because they afford different kinds of logic and combination.</p><h4 id=\"logical-operations\">Logical Operations</h4><p>Let’s say that we have a step that checks that an input is all letters:</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">step</span><span class=\"p\">.</span><span class=\"n\">filters</span><span class=\"p\">.</span><span class=\"n\">HasAllLetters</span><span class=\"p\">()</span></code></pre></div></div><p>For the above, anything that had a number (e.g., orange123) would be filtered out. But what if we wanted to inverse that, and allow passing of inputs that don’t have all letters (meaning we want numbers or special characters?) We can simply do that:</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">~</span><span class=\"n\">step</span><span class=\"p\">.</span><span class=\"n\">filters</span><span class=\"p\">.</span><span class=\"n\">HasAllLetters</span><span class=\"p\">()</span></code></pre></div></div><p>Implementation wise, this was really fun to do! For Python to respect the logical operator <code class=\"language-plaintext highlighter-rouge\">~</code> I simply define the “<strong>invert</strong>” function for the BooleanStep class.</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">def</span> <span class=\"nf\">__invert__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>    <span class=\"s\">\"\"\"    We can say \"~step\" and reverse the logic.    \"\"\"</span>    <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">reverse</span> <span class=\"o\">=</span> <span class=\"bp\">True</span>    <span class=\"k\">return</span> <span class=\"bp\">self</span></code></pre></div></div><p>It sets an attribute “reverse” to True, and returns itself, that way we use the same step, but with this variable set to be true.What does that do? In the “run” <a href=\"https://github.com/vsoch/pipelib/blob/69d7d4ac677a24a31ffa9322f03090cf074442c8/pipelib/steps/step.py#L217-L238\" target=\"_blank\">function</a> of the BooleanStep we basically retrieve an outcome from the underlying step (True or False) and simply reverse it given that boolean is True! Again, it’s very simple, and allows for doing things like this:</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">pipelib.pipeline</span> <span class=\"kn\">import</span> <span class=\"n\">Pipeline</span><span class=\"kn\">import</span> <span class=\"nn\">pipelib.steps</span> <span class=\"k\">as</span> <span class=\"n\">steps</span><span class=\"n\">Pipeline</span><span class=\"p\">(</span><span class=\"o\">~</span><span class=\"n\">steps</span><span class=\"p\">.</span><span class=\"n\">filters</span><span class=\"p\">.</span><span class=\"n\">HasAllLetters</span><span class=\"p\">()).</span><span class=\"n\">run</span><span class=\"p\">([</span><span class=\"s\">\"I-have-special-characters\"</span><span class=\"p\">,</span> <span class=\"s\">\"Idonot\"</span><span class=\"p\">])</span><span class=\"p\">[</span><span class=\"s\">'I-have-special-characters'</span><span class=\"p\">]</span><span class=\"n\">Pipeline</span><span class=\"p\">(</span><span class=\"n\">steps</span><span class=\"p\">.</span><span class=\"n\">filters</span><span class=\"p\">.</span><span class=\"n\">HasAllLetters</span><span class=\"p\">()).</span><span class=\"n\">run</span><span class=\"p\">([</span><span class=\"s\">\"I-have-special-characters\"</span><span class=\"p\">,</span> <span class=\"s\">\"Idonot\"</span><span class=\"p\">])</span><span class=\"p\">[</span><span class=\"s\">'Idonot'</span><span class=\"p\">]</span></code></pre></div></div><p>What if we wanted to combine steps? E.g., what if I want to say “has all letters” OR “has minimum length 10?” If we put the stepsside by side we would only be able to support an AND - allowing passing through of entries that have all letters and the minimum length of 10.Pipelib supports both those operators - AND and OR as follows:</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;</span> <span class=\"n\">step</span> <span class=\"o\">=</span> <span class=\"n\">steps</span><span class=\"p\">.</span><span class=\"n\">filters</span><span class=\"p\">.</span><span class=\"n\">HasAllLetters</span><span class=\"p\">()</span> <span class=\"o\">&amp;</span> <span class=\"n\">steps</span><span class=\"p\">.</span><span class=\"n\">filters</span><span class=\"p\">.</span><span class=\"n\">HasMinLength</span><span class=\"p\">(</span><span class=\"n\">length</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">)</span><span class=\"o\">&gt;</span> <span class=\"n\">step</span><span class=\"n\">HasAllLetters_AND_HasMinLength</span><span class=\"n\">Pipeline</span><span class=\"p\">(</span><span class=\"n\">step</span><span class=\"p\">).</span><span class=\"n\">run</span><span class=\"p\">([</span><span class=\"s\">\"thisonewillpass\"</span><span class=\"p\">,</span> <span class=\"s\">\"thisoneno\"</span><span class=\"p\">,</span> <span class=\"s\">\"notthisone2\"</span><span class=\"p\">])</span><span class=\"p\">[</span><span class=\"s\">'thisonewillpass'</span><span class=\"p\">]</span></code></pre></div></div><p>For both cases above, we are using the “<strong>and</strong>” and “<strong>or</strong> functions, respectively, and:</p><ol class=\"custom-counter\"><li>Checking for class compatibility (both must be BooleanStep)</li><li>Creating a list of composed steps to added to a class attribute \"composed\"</li><li>Add the previous run functions too, naming based on the step class name</li><li>Define a new run function that loops through the composed set, runs, updates and returns a shared result</li><li>Name the class based on the combined names of the composed classes</li></ol><p>For step 4 above, the operation (AND or OR) will vary depending on if the initial call was to “<strong>and</strong>” or “<strong>or</strong>”.The main difference between the two is that “OR” starts with a default of False (otherwise it would always return True)and AND starts with a default of True (otherwise it would always return False).And since we are always taking the first class “composed” attribute, this means that you can composesteps with other steps as many times as you like - a new check is simply added to the front or back ofthe list. The result (returned) is the new class that is ready to run. Here is what an OR looks like:</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;</span> <span class=\"n\">step</span> <span class=\"o\">=</span> <span class=\"n\">steps</span><span class=\"p\">.</span><span class=\"n\">filters</span><span class=\"p\">.</span><span class=\"n\">HasAllLetters</span><span class=\"p\">()</span> <span class=\"o\">|</span> <span class=\"n\">steps</span><span class=\"p\">.</span><span class=\"n\">filters</span><span class=\"p\">.</span><span class=\"n\">HasMinLength</span><span class=\"p\">(</span><span class=\"n\">length</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">)</span><span class=\"o\">&gt;</span> <span class=\"n\">step</span><span class=\"n\">HasAllLetters_OR_HasMinLength</span><span class=\"n\">Pipeline</span><span class=\"p\">(</span><span class=\"n\">step</span><span class=\"p\">).</span><span class=\"n\">run</span><span class=\"p\">([</span><span class=\"s\">\"thisonewillpass\"</span><span class=\"p\">,</span> <span class=\"s\">\"veryshort\"</span><span class=\"p\">,</span> <span class=\"s\">\"12345\"</span><span class=\"p\">])</span><span class=\"p\">[</span><span class=\"s\">'thisonewillpass'</span><span class=\"p\">,</span> <span class=\"s\">'veryshort'</span><span class=\"p\">]</span></code></pre></div></div><p>If you are interested in this function, you can see the entire thing <a href=\"https://github.com/vsoch/pipelib/blob/832b1c645e562d5cc6e376e5a3e058c02a40d92a/pipelib/steps/step.py#L177-L241\" target=\"_blank\">here</a>.</p><h4 id=\"transformation-operations\">Transformation Operations</h4><p>A base step can be thought of as a transformation. Instead of expecting a boolean to be returned, we areinstead expecting a new value or None. In this respect the transform step can also act as a boolean as a returnof “None” will be removed from the list, however in most cases a transform is intended to perform an operation on the item passed. Here is an example of a transformation operation:</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">Pipeline</span><span class=\"p\">(</span><span class=\"n\">steps</span><span class=\"p\">.</span><span class=\"n\">transform</span><span class=\"p\">.</span><span class=\"n\">ToLowercase</span><span class=\"p\">()).</span><span class=\"n\">run</span><span class=\"p\">([</span><span class=\"s\">\"AHHHH\"</span><span class=\"p\">])</span><span class=\"p\">[</span><span class=\"s\">'ahhhh'</span><span class=\"p\">]</span></code></pre></div></div><h4 id=\"sort-operations\">Sort Operations</h4><p>A sort operation is a step that is one level up. Instead of operating on individual items, the stepre-defines a the higher level “run” function and does operations across the iterable.A good example from Pipelib is the use case that originally inspired me - to start with a messylist of Docker tags, do some parsing to derive versions, and return back a sorted list.</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">pipeline</span><span class=\"p\">.</span><span class=\"n\">Pipeline</span><span class=\"p\">(</span><span class=\"n\">steps</span><span class=\"p\">.</span><span class=\"n\">container</span><span class=\"p\">.</span><span class=\"n\">ContainerTagSort</span><span class=\"p\">(</span><span class=\"n\">ascending</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">)).</span><span class=\"n\">run</span><span class=\"p\">([</span><span class=\"s\">\"1.2.3\"</span><span class=\"p\">,</span> <span class=\"s\">\"0.1.0\"</span><span class=\"p\">,</span> <span class=\"s\">\"8.3.2\"</span><span class=\"p\">])</span><span class=\"p\">[</span><span class=\"s\">'8.3.2'</span><span class=\"p\">,</span> <span class=\"s\">'1.2.3'</span><span class=\"p\">,</span> <span class=\"s\">'0.1.0'</span><span class=\"p\">]</span><span class=\"n\">pipeline</span><span class=\"p\">.</span><span class=\"n\">Pipeline</span><span class=\"p\">(</span><span class=\"n\">steps</span><span class=\"p\">.</span><span class=\"n\">container</span><span class=\"p\">.</span><span class=\"n\">ContainerTagSort</span><span class=\"p\">(</span><span class=\"n\">ascending</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)).</span><span class=\"n\">run</span><span class=\"p\">([</span><span class=\"s\">\"1.2.3\"</span><span class=\"p\">,</span> <span class=\"s\">\"0.1.0\"</span><span class=\"p\">,</span> <span class=\"s\">\"8.3.2\"</span><span class=\"p\">])</span><span class=\"p\">[</span><span class=\"s\">'0.1.0'</span><span class=\"p\">,</span> <span class=\"s\">'1.2.3'</span><span class=\"p\">,</span> <span class=\"s\">'8.3.2'</span><span class=\"p\">]</span></code></pre></div></div><p>In the above we also demonstrate that steps can take parameters, such as the order of a sort!This particular sorting step also allows you to say you want to return unique major, minor, or patchversions.</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">pipeline</span><span class=\"p\">.</span><span class=\"n\">Pipeline</span><span class=\"p\">(</span><span class=\"n\">steps</span><span class=\"p\">.</span><span class=\"n\">container</span><span class=\"p\">.</span><span class=\"n\">ContainerTagSort</span><span class=\"p\">(</span><span class=\"n\">unique_major</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)).</span><span class=\"n\">run</span><span class=\"p\">([</span><span class=\"s\">\"1.2.3\"</span><span class=\"p\">,</span> <span class=\"s\">\"1.1.0\"</span><span class=\"p\">,</span> <span class=\"s\">\"8.3.2\"</span><span class=\"p\">])</span><span class=\"p\">[</span><span class=\"s\">'8.3.2'</span><span class=\"p\">,</span> <span class=\"s\">'1.2.3'</span><span class=\"p\">]</span></code></pre></div></div><p>And if you wanted to do a more comprehensive clean up and sort, you could do <a href=\"https://vsoch.github.io/pipelib/getting_started/user-guide.html#a-real-world-example-docker-tags\" target=\"_blank\">something like this</a>.</p><h3 id=\"wrapper\">Wrapper</h3><p>Pipelib needed a way to be able to pass around some parsed version of an item, but still maintainthe original. For example, let’s say I’m parsing Docker tags into something that resembles a loosesemantic version, I might have filtered <code class=\"language-plaintext highlighter-rouge\">1.2.3-boop</code> to be just <code class=\"language-plaintext highlighter-rouge\">1.2.3</code>, but at the end of theday I need the original tag to pull. Pipelib accomplishes this via wrappers.</p><p>A wrapper is conceptually that - an internal wrapper class to an item that allows for storingan original value, and still doing operations to change a current state. Wrappers are used inside steps and allow for things like sorting and comparison. You probably don’t need to worry about wrappersunless you want to develop for pipelib. By default, wrappers and “extracted away” to return the basictypes. However, you can ask Pipelib to not do this unwrapping, and then you can get backthe derived and original values:</p><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">tags</span>  <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s\">\"1.2.3\"</span><span class=\"p\">,</span> <span class=\"s\">\"1.1.0\"</span><span class=\"p\">,</span> <span class=\"s\">\"8.3.2\"</span><span class=\"p\">]</span><span class=\"n\">updated</span> <span class=\"o\">=</span> <span class=\"n\">pipeline</span><span class=\"p\">.</span><span class=\"n\">Pipeline</span><span class=\"p\">(</span><span class=\"n\">steps</span><span class=\"p\">.</span><span class=\"n\">container</span><span class=\"p\">.</span><span class=\"n\">ContainerTagSort</span><span class=\"p\">()).</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">tags</span><span class=\"p\">,</span> <span class=\"n\">unwrap</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">)</span><span class=\"c1\"># Notice that this just looks like a set of strings...</span><span class=\"n\">updated</span><span class=\"p\">[</span><span class=\"s\">'8.3.2'</span><span class=\"p\">,</span> <span class=\"s\">'1.2.3'</span><span class=\"p\">]</span><span class=\"c1\"># But actually we have wrappers, that each have an _original attribute</span><span class=\"nb\">type</span><span class=\"p\">(</span><span class=\"n\">updated</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span><span class=\"n\">pipelib</span><span class=\"p\">.</span><span class=\"n\">wrappers</span><span class=\"p\">.</span><span class=\"n\">version</span><span class=\"p\">.</span><span class=\"n\">VersionWrapper</span></code></pre></div></div><h2 id=\"conclusion\">Conclusion</h2><p>I’ve had so much fun making this library! Like many of my projects it’s probably not super useful,but if you see a cool use case please let me know! I’m also happy to develop custom pipelines or stepsfor a use case that you might be interested in. Please don’t hesitate to ask me for help, I’m always runningout of fun things to do :)</p><blockquote>  <p>Why should I care?</p></blockquote><p>Arguably you could just hard code this kind of filtering and sorting, but I think theidea of being able to customize and assemble steps is a cool one. If the steps are providedin a library it might might it slightly easier, or your work more reproducible because someone else can use the steps. And if you don’t care? That’s okay too. I recognize this wasmostly a fun project, and yet-another-itch I really wanted to scratch because I’ve nevermade a design like this before, either in terms of the idea or <a href=\"https://twitter.com/vsoch/status/1521670410852442112\" target=\"_blank\">underlying testing and automation</a>.</p>",
            "url": "https://hpc.social/personal-blog/2022/pipelib-simple-library-to-parse-filter-and-sort-things/",
            
            
            
            
            
            "date_published": "2022-05-07T13:30:00-06:00",
            "date_modified": "2022-05-07T13:30:00-06:00",
            
                "author": "Vanessasaurus"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/the-research-software-ecosystem/",
            "title": "The Research Software Ecosystem",
            "summary": null,
            "content_text": "We recently published the Research Software Encyclopedia and also have added several new parsers for obtaining new data, meaning the total collectionof curated research software is greater than 1500entries. In honor of this collection, and of a library I’m working on called CiteLang, I wanted to do a small study to better understand:What are the most valuable dependencies in our community, across languages?What are the most valuable dependencies in our community, by language?What is the credit allocation for each repository?CiteLangTo step back for a second, let’s talk again about CiteLang. It has many functions - one of thembeing an ability to assess opensource contributions via git, but it’s main purpose is to be a markdown syntax for citing software,meaning that we can:Generate basic software credit trees, graphs, and markdown summaries.Derive a new, customizable model of credit based on published packages and dependencies.Provide a way to cite software in a paper and give credit without needing DOIs.As a simple example, I can run CiteLang over this markdown file with CiteLang references:# SummaryPortability and reproducibility of complex software stacks is essential for researchers to perform their work. High Performance Computing (HPC) environments add another level of complexity, where possibly conflicting dependencies must co-exist. Although container technologies like Singularity @conda{name=singularity} make it possible to \"bring your own environment,\" without any form of central strategy to manage containers, researchers who seek reproducibility via using containers are tasked with managing their own container collection, often not taking care to ensure that a particular digest or version is used. The reproducibility of the work is at risk, as they cannot easily install and use containers, nor can they share their software with others.Singularity Registry HPC (shpc) @pypi{name=singularity-hpc} is the first of its kind to provide an easy means for a researcher to add their research software for sharing and collaboration with other researchers to an existing collection of over 200 popular scientific libraries @github{name=autamus/registry} @github{name=spack/spack, release=0.17}. The software installs containers as environment modules that are easyto use and read documentation for, and exposes aliases for commands in the container that the researcher can add to their pipeline without thinking about complex interactions with a container. The simple addition of an entry to the registry maintained by shpc comes down to adding a yaml file, and after doing this, another researcher can easily install the same software, down to the digest, to reproduce the original work.# References&lt;!--citelang start--&gt;&lt;!--citelang end--&gt;And then run citelang render paper.md to get a nice rendered table alongside your paper! What CiteLang does is find the references in the paper, they look like this:@conda{name=singularity}@pypi{name=singularity-hpc}@github{name=autamus/registry} @github{name=spack/spack, release=0.17}Each of the references above is a package manager with a package name and (optionally) a version, and we can load in the metadatafor each and then generate a table that you see here that summarizes credit across dependencies. In this model, we give some allocation of credit (default is 50%) to the main work (paper or software) citing the software, and then recursively parse dependencies up to some minimum level of credit to calculate scores. Dependencies shared across libraries are averaged together. The final table represents the credit that you give not only to the top level software, but to all nested dependencies, for the work that you did. And that’s only the basics! CiteLang takes this simple ability to parse references and extends it to automation, graphs, badges, and more! You can read more about CiteLang here.  Publish or perish? How about neither? I just need to keep writing software!But do you see what is happening above? We aren’t requiring some artificial publicationin order to cite software. We are citing it based on its actual usage, as a known dependency to some other software.In a nutshell, we don’t believe that “the traditional academic way” of citing papers makes sense for software, and insteadof using DOIs we can use package managers and metadata as a source of truth, and derive the real value of a piece of softwarebased on this ecosystem. This means that as a research software engineer, you can just keep doing what you are already doing, and ifsomeone uses CiteLang to summarize their work, given that your software is published to a package managed you’ll get credit. Thereare so many cool ideas around this! But let’s start at the beginning. We first want to show how to summarize an ecosystem.That is exactly what we are going to do in this post.The Research Software EcosytemStarting with these curated repositories from a  set of scrapers including the Journal of Open Source Software, the HAL Research Software Database, the Research Software NL Dictionary, ROpenSci, and The Molecular Sciences Software Institute, we can do a basic analysis to identify the most used (and thus valued) pieces of software in our ecosystem. My analysis plan was to:Start with the current database.For each repository, look for requirements files to parse.Derive dependency data based on this requirements file.Combine and rank to discover the top dependencies!This of course is limited to the subset of software in our database, and the ability of CiteLang to parse a requirements file.Currently we parse setup.py and requirements.txt (Python), DESCRIPTION (R), go.mod (Go), package.json (npm), and Gemfile (ruby). Based on thebreakdown of the languages found in the RSEPedia, this is a reasonable start!  But it’s also kind of sad to see that my favorite languages (Go and Rust) are barely represented in our community. Also, the aboveshould tell you that the R and Python results likely have some meaningful interpretation, but the others not so much, only because we don’t have a big enough sample. So for all of the abovesteps, for these 1500+ repositories and many languages, I wanted th entire process to be automated, always have potential for easy improvement,and run at some regular interval as new software comes into the Research Software Encyclopedia (also automated) so we can derive changes over time.If you dont’ care to read further:View the Research Software EcosystemCheck out Languages hereResults for Dependencies hereIndividual Repositories hereFor this first publication of the interface we have the following metrics:  And I’m so excited because a tiny vision I had a few years ago to provide (and use) a community research software database is comingto live! So without further adeiu, I’m just going to jump into the cool results! It will be fun to see how these change over time.PythonLadies and gents, dinosaurs and rabbits! Your Python results:  So here is the first awesome insight. Is anyone really surprised to see numpy as the number one library?The credit value here says that the average Python repository is attributing about 3% of credit to numpy, meaning it is a direct or indirect dependency. Let that sink in! Here is the irony - when is the last time you cited numpy? You probably haven’t, because you’ve cited somethingthat uses it. We don’t remember numpy despite the fact that it’s so core to everything that we do.  The fact that the most widely used library is rarely cited is huge evidence for why a manual “write papers and cite DOIs” approach just won’t work for software.What else do we see in this list? Let me name a few things. First, we can’t be so terrible at remembering to look at or visualizethings because matplotlib is second. At least for research software, this is telling us that making plots or charts is important.The next (possibly surprising) result is that documentation and testing is at least represented, and this might be a biased samplebecause we include repositories that are peer reviewed (e.g., JoSS) and documentation and testing is necessary for that. Given this need for Python, sphinx and pytest come up as leaders to provide that. So here is another nugget of insight:  Some of us are so  busy focusing on domain-specific software that we forget the importance of the “less sexy” research software that helps us test, document, view things, or even create simple data structures.This kind of “base” software has always been what I’ve been most interested in, and ironically what people tell me time and time again“That’s not research software.” Oh really? So something that is entirely powering the research community is not research software?Of course I have my own strong opinions about a taxonomy for research software, but I would encourage those of you who are very dismissive to take a step back andconsider what you are really saying.The next insight is that we see a lot of libraries for data formats (e.g., pyaml, h5py, lxml, and more lower in the list) and this is an attestment to how important being able to read, serialize, and save data is.The final insight is the fact that requests is high in the list. For those of you not familiar, requests is a library for doing that, makinghttp requests to get content from some webby place. This is an attestment to the fact that our work is increasingly relying on external APIs,automation, or other resources provided on the web.You can see the full Python results here.RI’m less of an R programmer these days, but I think that these results also make sense.  We don’t see any huge leaders in the same way as we see numpy in Python, but not surprisingly the leader packagefor the R language is, well, R! I at first thought this was a bug, but actually R DESCRIPTION files that we parse do commonly include a pinned version of R:Depends: R (&gt;= 3.4.1), TailRank, ...And so we actually can give credit to the language proper! If you don’t feel this is appropriate, feel free to skip this line and considerthe top package jsonlite. This is also why I think json would be represented in Python if it wasn’t part of the standard library. Us research folks - we need our json! Overall I think we see a similar pattern here as we saw with Python. The libraries that float to the top are those that involve data structures (jsonlite, yaml), webby requests or similar (httr, curl), documentation and testing (knitr, rmarkdown) and graphics or visualization.  What does this tell us about what is undervalued in research software? Again, it’s not the domain specific libraries, but rather the core stuff that enables those libraries.You can see the full R results here.ProjectsIf you are interested in a specific project in the RSEPedia, we also provide a project-specific table and badge! You can browse projects from here, and here is an example of a badge generated for a project called  github.com/ORNL/tx2 (and on GitHub). Without even looking I can tell you we have some machine learning and/or visualization going on here (scikit-learn! umap! pandas! matplotlib)!  Notice how numpy (as an example) shows up at multiple points in the tree - when we calculate an overall credit, say, for the ecosystem, we take that into account! And we can then peek at the project-specific table and sort of verify that yes, this is a Python ML/visualization project:  And we see some surprises! Like, the slack-sdk? What? Believe it or not, that is pulled in by tqdm. The project-specific tables (and the description at the top) also give you a better sense of how CiteLang allocatescredit. The top level package is given 50%, and then the other 50% is given to all dependencies in the same fashion.We cut off at a value of 0.001, and we do that in case we might be parsing dependencies forever down to some infintesimally small amount.Finally, every project serves its own raw data  and the site is searchable, because sites should be. 😄️DiscussionI’m so happy (and a bit relieved, to be honest) to finally be able to show what I’ve been saying for years - that the most valuable software for research, and the software that is driving domain-specific research software, are the unsexy libraries that have to do with data structures, (maybe standards), documentation or testing, and data formats or retrieval. These are the packages that you aren’t going to remember to cite. Also, this set is totally leaving out the software we use on a day to day basis in our CI, which arguably isn’t research software but has done more for the research community than anything I can think of - containers, version control (git), and continuous integration. We’d be a mess without it. We need to be more thankful and aware of this, and for some of y’all that turn down your nose to anything that isn’t a domain-science library, perhaps take a pause. Next, let’s talk about limitations and hopes for the future.A Living DatabaseI wouldn’t have been happy with myself to simply publish software at one point in time and call it a day.The Research Software Encyclopedia is updated weekly, and so I’ve designed this analysis to do the same!This means that while we do cache a result for a newly added piece of software, we do continue to grow the analysis as new software is added. And since the tool will always use the newly updated CiteLang, any improvements to the parsers there will be reflected here! And if anyone wants to run the entire thing again (outside of the limit of GitHub Actions) they can clone the repository, nuke the _repos folder, and run the scripts again.Language GapsThe biggest gap in the RSEPedia is with respect to what we don’t see. First, despite being a prominent language, we don’t see anything for C++, because there isn’t a package manager with an API to use it. If you have a nifty (or even hacky) idea for how to parse a requirements file, I want to hear it. The RSEPedia has support for spack, but most research-oriented C++ projects are not going to go out of their way to publish their package there, and we get no signal of the package being in spack when we clone the repository. Sppaaaaaack (sorry, it’s a bit of a tic at this point!) 😁️We also don’t see standard modules or libraries provided within a language. E.g., I can almost guarantee you a ton of Python libraries are importing json, but since it’s not a package manager library we wouldn’t see it. I suspect citelang could come up with a way to derive credit for these libraries by way of abstract syntax trees or just parsing the source code, although I haven’t done this yet because I’m not convinced it’s something people are as interested in. If you want to say thank you for the Python standard library, there is a donate button on their contribution page (or you could contribute code). There is an even deeper level of parsing (at least for Python) that looks at function signatures, and I wrote a library called caliper in early 2021 to do that, and it’s able to generate function databases for Python software of interest. This would be cool to do for some kind of (unrelated) compatibility analysis here, but yes that’s very different.Parsing LimitationFor all requirements files except for Python, we are forced to do static parsing. While not perfect because bugs can happen for niche cases of someone defining requirements in a weird way, it’s a reasonable start. There is always room for improvement, or adding more static parsers for requirements files I have not considered yet.However, this is not the case for the Python parsing (either requirements.txt or setup.py)! For Python these results are likely very good because we wrap the pypi package manager install command to derive a list of packages and versions from either a setup.py or requirements.txt. Don’t worry - nothing is installed, we either just parse the requirements file and return the results, or we use the solveragainst a setup.py to come to an equivalent list. We originally had a static parser (and still use this as a fallback) however I talked to @alecbcs and he had this fantastic idea! Will it likely need updates as time goes on, giventhe functions are private? Sure. But I’m happy to do that to get the much more accurate listing.In practice, the only setup.py files that I was not able to parse either had a bug (e.g., trying to read a file that doesn’t exist in the repository) or they were trying to use modules outside of the standard library. For all of the cases of broken-ness, I opened issues on the respective repositories so we might have a better chance at parsing in the future! One detail is that we parse the first requirements file found. For a primary requirements file in the root of the repository, this is the best outcome. However, some repos don’t have a file in the root, and perhaps we find one in a documentation folder instead. Either way, the result represents our best effort at finding and parsing requirements given a cloned repository we don’t know the structure of in advance.Final ThoughtsHere are my final takeaways:Publication is not for Research SoftwareA system of credit that relies on software engineers to do extra manual work (to write papers) is never going to fully capture the ecosystem and give proper credit. It will only capture those that have the time and possibly privilege to take the extra time to write a paper.Publication only makes sense given that a piece of software is paired alongside a robust result, in which case fine, write the paper and also champion the software.Publication Does not Actually Capture CreditA system that also only skims the superficial top (the name of one package) and does not dig deep into a dependency tree is also going to miss insights and deserved attributions of credit. As the numpy example shows, nobody is actually citing numpy, but a ton of projects are using it somewhere in their dependency tree, so it deserves a lot of credit.We Can Do BetterI have a pet peeve. I’m frankly just tired of people writing about credit and attribution but not doing anything about it. We could extend that to other things, but it’s especially an issue for this topic. Ironically they are writing papers and improving their publication record as they write about how publication and research software is a strained process. I may not have solved this problem, but damn at least I’m trying to actually do something about it instead of spurting gas.I find this idea exciting because there are so many directions you can go with it. When I first designed the idea I imagined a database and online interface where you could essentially connect your GitHub repository, and akin to a builder service, parse your repository on some event and derive a new credit or citation graph. Or you could have some set akin to the RSEPedia that are also updated regularly. And then, by way of having that database, we could do these same queries (that currently I’m doing statically) to say “What are the most important libraries for this language? Across the ecosystem?” or “How has this changed over time?” It would be a true way to derive the value of a library without needing people to publish papers, and totally automated and integrated with package managers, which is where people already should be putting their software.Heck, if someone gave me a cloud and a little bit of funding I’d love to work on this. Are there good reasons or use cases? I don’t know, but maybe.So what do you think?",
            "content_html": "<p>We recently published <a href=\"https://openresearchsoftware.metajnl.com/articles/10.5334/jors.359/\" target=\"_blank\">the Research Software Encyclopedia</a> and also have added several new parsers for obtaining new data, meaning the total collectionof <a href=\"https://rseng.github.io/software/\" target=\"_blank\">curated research software</a> is greater than 1500entries. In honor of this collection, and of a library I’m working on called <a href=\"https://vsoch.github.io/citelang/getting_started/user-guide.html\" target=\"_blank\">CiteLang</a>, I wanted to do a small study to better understand:</p><ol class=\"custom-counter\"><li>What are the most valuable dependencies in our community, across languages?</li><li>What are the most valuable dependencies in our community, by language?</li><li>What is the credit allocation for each repository?</li></ol><h2 id=\"citelang\">CiteLang</h2><p>To step back for a second, let’s talk again about CiteLang. It has many functions - one of thembeing an ability to <a href=\"https://vsoch.github.io/2022/citelang-contrib/\" target=\"_blank\">assess opensource contributions</a> via git, but it’s main purpose is to be a markdown syntax for citing software,meaning that we can:</p><ol class=\"custom-counter\"><li>Generate basic software credit trees, graphs, and markdown summaries.</li><li>Derive a new, customizable model of credit based on published packages and dependencies.</li><li>Provide a way to cite software in a paper and give credit without needing DOIs.</li></ol><p>As a simple example, I can run CiteLang over this markdown file with CiteLang references:</p><div class=\"language-md highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"gh\"># Summary</span>Portability and reproducibility of complex software stacks is essential for researchers to perform their work. High Performance Computing (HPC) environments add another level of complexity, where possibly conflicting dependencies must co-exist. Although container technologies like Singularity @conda{name=singularity} make it possible to \"bring your own environment,\" without any form of central strategy to manage containers, researchers who seek reproducibility via using containers are tasked with managing their own container collection, often not taking care to ensure that a particular digest or version is used. The reproducibility of the work is at risk, as they cannot easily install and use containers, nor can they share their software with others.Singularity Registry HPC (shpc) @pypi{name=singularity-hpc} is the first of its kind to provide an easy means for a researcher to add their research software for sharing and collaboration with other researchers to an existing collection of over 200 popular scientific libraries @github{name=autamus/registry} @github{name=spack/spack, release=0.17}. The software installs containers as environment modules that are easyto use and read documentation for, and exposes aliases for commands in the container that the researcher can add to their pipeline without thinking about complex interactions with a container. The simple addition of an entry to the registry maintained by shpc comes down to adding a yaml file, and after doing this, another researcher can easily install the same software, down to the digest, to reproduce the original work.<span class=\"gh\"># References</span><span class=\"c\">&lt;!--citelang start--&gt;</span><span class=\"c\">&lt;!--citelang end--&gt;</span></code></pre></div></div><p>And then run <code class=\"language-plaintext highlighter-rouge\">citelang render paper.md</code> to get a <a href=\"https://gist.github.com/vsoch/41b4559d8f87eb9d6e62945e02689428\" target=\"_blank\">nice rendered table alongside your paper</a>! What CiteLang does is find the references in the paper, they look like this:</p><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>@conda{name=singularity}@pypi{name=singularity-hpc}@github{name=autamus/registry} @github{name=spack/spack, release=0.17}</code></pre></div></div><p>Each of the references above is a package manager with a package name and (optionally) a version, and we can load in the metadatafor each and then generate a table <a href=\"https://gist.github.com/vsoch/41b4559d8f87eb9d6e62945e02689428\" target=\"_blank\">that you see here</a> that summarizes credit across dependencies. In this model, we give some allocation of credit (default is 50%) to the main work (paper or software) citing the software, and then recursively parse dependencies up to some minimum level of credit to calculate scores. Dependencies shared across libraries are averaged together. The final table represents the credit that you give not only to the top level software, but to all nested dependencies, for the work that you did. And that’s only the basics! CiteLang takes this simple ability to parse references and extends it to automation, graphs, badges, and more! You can read more about CiteLang <a href=\"https://vsoch.github.io/citelang/getting_started/index.html\" target=\"_blank\">here</a>.</p><blockquote>  <p>Publish or perish? How about neither? I just need to keep writing software!</p></blockquote><p>But do you see what is happening above? We aren’t requiring some artificial publicationin order to cite software. We are citing it based on its actual usage, as a known dependency to some other software.In a nutshell, we don’t believe that “the traditional academic way” of citing papers makes sense for software, and insteadof using DOIs we can use package managers and metadata as a source of truth, and derive the real value of a piece of softwarebased on this ecosystem. This means that as a research software engineer, you can just keep doing what you are already doing, and ifsomeone uses CiteLang to summarize their work, given that your software is published to a package managed you’ll get credit. Thereare so many cool ideas around this! But let’s start at the beginning. We first want to show how to summarize an ecosystem.That is exactly what we are going to do in this post.</p><h2 id=\"the-research-software-ecosytem\">The Research Software Ecosytem</h2><p>Starting with these curated repositories from a <a href=\"https://rseng.github.io/rse/getting-started/scrapers/index.html\" target=\"_blank\"> set of scrapers</a> including the Journal of Open Source Software, the HAL Research Software Database, the Research Software NL Dictionary, ROpenSci, and The Molecular Sciences Software Institute, we can do a basic analysis to identify the most used (and thus valued) pieces of software in our ecosystem. My analysis plan was to:</p><ol class=\"custom-counter\"><li>Start with the current database.</li><li>For each repository, look for requirements files to parse.</li><li>Derive dependency data based on this requirements file.</li><li>Combine and rank to discover the top dependencies!</li></ol><p>This of course is limited to the subset of software in our database, and the ability of CiteLang to parse a requirements file.Currently we parse setup.py and requirements.txt (Python), DESCRIPTION (R), go.mod (Go), package.json (npm), and Gemfile (ruby). Based on the<a href=\"https://rseng.github.io/rsepedia-analysis/analysis/languages/\" target=\"_blank\">breakdown of the languages</a> found in the RSEPedia, this is a reasonable start!</p><div style=\"padding: 20px;\">  <img src=\"https://vsoch.github.io/assets/images/posts/citelang/languages.png\" /></div><p>But it’s also kind of sad to see that my favorite languages (Go and Rust) are barely represented in our community. Also, the aboveshould tell you that the R and Python results likely have some meaningful interpretation, but the others not so much, only because we don’t have a big enough sample. So for all of the abovesteps, for these 1500+ repositories and many languages, I wanted th entire process to be automated, always have potential for easy improvement,and run at some regular interval as new software comes into the Research Software Encyclopedia (also automated) so we can derive changes over time.If you dont’ care to read further:</p><ol class=\"custom-counter\"><li><a href=\"https://rseng.github.io/rsepedia-analysis/\" target=\"_blank\">View the Research Software Ecosystem</a></li><li><a href=\"https://rseng.github.io/rsepedia-analysis/analysis/languages/\" target=\"_blank\">Check out Languages here</a></li><li><a href=\"https://rseng.github.io/rsepedia-analysis/analysis/dependencies/\" target=\"_blank\">Results for Dependencies here</a></li><li><a href=\"https://rseng.github.io/rsepedia-analysis/analysis/repos/\" target=\"_blank\">Individual Repositories here</a></li></ol><p>For this first publication of the interface we have the following metrics:</p><div style=\"padding: 20px;\">  <img src=\"https://vsoch.github.io/assets/images/posts/citelang/ecosystem.png\" /></div><p>And I’m so excited because a tiny vision I had a few years ago to provide (and use) a community research software database is comingto live! So without further adeiu, I’m just going to jump into the cool results! It will be fun to see how these change over time.</p><h3 id=\"python\">Python</h3><p>Ladies and gents, dinosaurs and rabbits! Your Python results:</p><div style=\"padding: 20px;\">  <img src=\"https://vsoch.github.io/assets/images/posts/citelang/python-deps.png\" /></div><p>So here is the first awesome insight. Is anyone really surprised to see numpy as the number one library?The credit value here says that the average Python repository is attributing about 3% of credit to numpy, meaning it is a direct or indirect dependency. Let that sink in! Here is the irony - when is the last time you cited numpy? You probably haven’t, because you’ve cited somethingthat uses it. We don’t remember numpy despite the fact that it’s so core to everything that we do.</p><blockquote>  <p>The fact that the most widely used library is rarely cited is huge evidence for why a manual “write papers and cite DOIs” approach just won’t work for software.</p></blockquote><p>What else do we see in this list? Let me name a few things. First, we can’t be so terrible at remembering to look at or visualizethings because matplotlib is second. At least for research software, this is telling us that making plots or charts is important.The next (possibly surprising) result is that documentation and testing is at least represented, and this might be a biased samplebecause we include repositories that are peer reviewed (e.g., JoSS) and documentation and testing is necessary for that. Given this need for Python, sphinx and pytest come up as leaders to provide that. So here is another nugget of insight:</p><blockquote>  <p>Some of us are so  busy focusing on domain-specific software that we forget the importance of the “less sexy” research software that helps us test, document, view things, or even create simple data structures.</p></blockquote><p>This kind of “base” software has always been what I’ve been most interested in, and ironically what people tell me time and time again“That’s not research software.” Oh really? So something that is entirely powering the research community is not research software?Of course I have my own <a href=\"https://rseng.github.io/software/repository/github/0x0f0f0f/Metatheory.jl/annotate-taxonomy/\" target=\"_blank\">strong opinions</a> about a taxonomy for research software, but I would encourage those of you who are very dismissive to take a step back andconsider what you are really saying.</p><p>The next insight is that we see a lot of libraries for data formats (e.g., pyaml, h5py, lxml, and more lower in the list) and this is an attestment to how important being able to read, serialize, and save data is.</p><p>The final insight is the fact that requests is high in the list. For those of you not familiar, requests is a library for doing that, makinghttp requests to get content from some webby place. This is an attestment to the fact that our work is increasingly relying on external APIs,automation, or other resources provided on the web.</p><p>You can see <a href=\"https://rseng.github.io/rsepedia-analysis/analysis/python/\" target=\"_blank\">the full Python results here</a>.</p><h3 id=\"r\">R</h3><p>I’m less of an R programmer these days, but I think that these results also make sense.</p><div style=\"padding: 20px;\">  <img src=\"https://vsoch.github.io/assets/images/posts/citelang/r-deps.png\" /></div><p>We don’t see any huge leaders in the same way as we see numpy in Python, but not surprisingly the leader packagefor the R language is, well, R! I at first thought this was a bug, but actually R <code class=\"language-plaintext highlighter-rouge\">DESCRIPTION</code> files that we parse do commonly include a pinned version of R:</p><pre><code class=\"language-DESCRIPTION\">Depends: R (&gt;= 3.4.1), TailRank, ...</code></pre><p>And so we actually can give credit to the language proper! If you don’t feel this is appropriate, feel free to skip this line and considerthe top package jsonlite. This is also why I think json would be represented in Python if it wasn’t part of the standard library. Us research folks - we need our json! Overall I think we see a similar pattern here as we saw with Python. The libraries that float to the top are those that involve data structures (jsonlite, yaml), webby requests or similar (httr, curl), documentation and testing (knitr, rmarkdown) and graphics or visualization.  What does this tell us about what is undervalued in research software? Again, it’s not the domain specific libraries, but rather the core stuff that enables those libraries.</p><p>You can see <a href=\"https://rseng.github.io/rsepedia-analysis/analysis/R/\" target=\"_blank\">the full R results here</a>.</p><h3 id=\"projects\">Projects</h3><p>If you are interested in a specific project in the RSEPedia, we also provide a project-specific table and badge! You can <a href=\"https://rseng.github.io/rsepedia-analysis/analysis/repos/\" target=\"_blank\">browse projects from here</a>, and here is an example of a badge generated for a project called  <a href=\"https://rseng.github.io/rsepedia-analysis/repos/github/ORNL/tx2/README\" target=\"_blank\">github.com/ORNL/tx2</a> <a href=\"https://github.com/ORNL/tx2\" target=\"_blank\">(and on GitHub)</a>. Without even looking I can tell you we have some machine learning and/or visualization going on here (scikit-learn! umap! pandas! matplotlib)!</p><div style=\"padding: 20px;\">  <img src=\"https://vsoch.github.io/assets/images/posts/citelang/project.png\" /></div><p>Notice how numpy (as an example) shows up at multiple points in the tree - when we calculate an overall credit, say, for the ecosystem, we take that into account! And we can then peek at the project-specific table and sort of verify that yes, this is a Python ML/visualization project:</p><div style=\"padding: 20px;\">  <img src=\"https://vsoch.github.io/assets/images/posts/citelang/project-table.png\" /></div><p>And we see some surprises! Like, the slack-sdk? What? Believe it or not, that is pulled in by <a href=\"https://github.com/tqdm/tqdm/blob/4f208e72552c4d916aa4fe6a955349ee8b2ed353/setup.cfg#L87\" target=\"_blank\">tqdm</a>. The project-specific tables (and the description at the top) also give you a better sense of how CiteLang allocatescredit. The top level package is given 50%, and then the other 50% is given to all dependencies in the same fashion.We cut off at a value of 0.001, and we do that in case we might be parsing dependencies forever down to some infintesimally small amount.</p><p>Finally, every project serves its own <a href=\"https://rseng.github.io/rsepedia-analysis/repos/github/ORNL/tx2/data.json\" target=\"_blank\">raw data</a></p><div style=\"padding: 20px;\">  <img src=\"https://vsoch.github.io/assets/images/posts/citelang/json-data.png\" /></div><p>and the site is searchable, because sites should be. 😄️</p><h2 id=\"discussion\">Discussion</h2><p>I’m so happy (and a bit relieved, to be honest) to finally be able to show what I’ve been saying for years - that the most valuable software for research, and the software that is driving domain-specific research software, are the unsexy libraries that have to do with data structures, (maybe standards), documentation or testing, and data formats or retrieval. These are the packages that you aren’t going to remember to cite. Also, this set is totally leaving out the software we use on a day to day basis in our CI, which arguably isn’t research software but has done more for the research community than anything I can think of - containers, version control (git), and continuous integration. We’d be a mess without it. We need to be more thankful and aware of this, and for some of y’all that turn down your nose to anything that isn’t a domain-science library, perhaps take a pause. Next, let’s talk about limitations and hopes for the future.</p><h2 id=\"a-living-database\">A Living Database</h2><p>I wouldn’t have been happy with myself to simply publish software at one point in time and call it a day.The Research Software Encyclopedia is updated weekly, and so I’ve designed this analysis to do the same!This means that while we do cache a result for a newly added piece of software, we do continue to grow the analysis as new software is added. And since the tool will always use the newly updated <a href=\"https://github.com/vsoch/citelang\" target=\"_blank\">CiteLang</a>, any improvements to the parsers there will be reflected here! And if anyone wants to run the entire thing again (outside of the limit of GitHub Actions) they can clone the repository, nuke the _repos folder, and run the scripts again.</p><h3 id=\"language-gaps\">Language Gaps</h3><p>The biggest gap in the RSEPedia is with respect to what we don’t see. First, despite being a prominent language, we don’t see anything for C++, because there isn’t a package manager with an API to use it. If you have a nifty (or even hacky) idea for how to parse a requirements file, <a href=\"https://github.com/vsoch/citelang/issues\" target=\"_blank\">I want to hear it</a>. The RSEPedia has support for spack, but most research-oriented C++ projects are not going to go out of their way to publish their package there, and we get no signal of the package being in spack when we clone the repository. Sppaaaaaack (sorry, it’s a bit of a tic at this point!) 😁️</p><p>We also don’t see standard modules or libraries provided within a language. E.g., I can almost guarantee you a ton of Python libraries are importing json, but since it’s not a package manager library we wouldn’t see it. I suspect citelang could come up with a way to derive credit for these libraries by way of abstract syntax trees or just parsing the source code, although I haven’t done this yet because I’m not convinced it’s something people are as interested in. If you want to say thank you for the Python standard library, there is a <a href=\"https://www.python.org/psf/contrib/\" target=\"_blank\">donate button</a> on their contribution page (or you could contribute code). There is an even deeper level of parsing (at least for Python) that looks at function signatures, and I wrote a library called <a href=\"https://github.com/vsoch/caliper\" target=\"_blank\">caliper</a> in early 2021 to do that, and it’s able to generate <a href=\"https://raw.githubusercontent.com/vsoch/caliper-metrics/main/pypi/tensorflow/functiondb/functiondb-0.12.0rc1.json\" target=\"_blank\">function databases</a> for Python software of interest. This would be cool to do for some kind of (unrelated) compatibility analysis here, but yes that’s very different.</p><h3 id=\"parsing-limitation\">Parsing Limitation</h3><p>For all requirements files except for Python, we are forced to do static parsing. While not perfect because bugs can happen for niche cases of someone defining requirements in a weird way, it’s a reasonable start. There is always room for improvement, or adding more static parsers for requirements files I have not considered yet.</p><p>However, this is not the case for the Python parsing (either requirements.txt or setup.py)! For Python these results are likely very good because we wrap the pypi package manager install command to derive a list of packages and versions from either a setup.py or requirements.txt. Don’t worry - nothing is installed, we either just parse the requirements file and return the results, or we use the solveragainst a setup.py to come to an equivalent list. We originally had a static parser (and still use this as a fallback) however I talked to <a href=\"https://github.com/alecbcs\" target=\"_blank\">@alecbcs</a> and he had this fantastic idea! Will it likely need updates as time goes on, giventhe functions are private? Sure. But I’m happy to do that to get the much more accurate listing.</p><p>In practice, the only setup.py files that I was not able to parse either had a bug (e.g., trying to read a file that doesn’t exist in the repository) or they were trying to use modules outside of the standard library. For all of the cases of broken-ness, I opened issues on the respective repositories so we might have a better chance at parsing in the future! One detail is that we parse the first requirements file found. For a primary requirements file in the root of the repository, this is the best outcome. However, some repos don’t have a file in the root, and perhaps we find one in a documentation folder instead. Either way, the result represents our best effort at finding and parsing requirements given a cloned repository we don’t know the structure of in advance.</p><h3 id=\"final-thoughts\">Final Thoughts</h3><p>Here are my final takeaways:</p><h4 id=\"publication-is-not-for-research-software\">Publication is not for Research Software</h4><p>A system of credit that relies on software engineers to do extra manual work (to write papers) is never going to fully capture the ecosystem and give proper credit. It will only capture those that have the time and possibly privilege to take the extra time to write a paper.Publication only makes sense given that a piece of software is paired alongside a robust result, in which case fine, write the paper and also champion the software.</p><h4 id=\"publication-does-not-actually-capture-credit\">Publication Does not Actually Capture Credit</h4><p>A system that also only skims the superficial top (the name of one package) and does not dig deep into a dependency tree is also going to miss insights and deserved attributions of credit. As the numpy example shows, nobody is actually citing numpy, but a ton of projects are using it somewhere in their dependency tree, so it deserves a lot of credit.</p><h4 id=\"we-can-do-better\">We Can Do Better</h4><p>I have a pet peeve. I’m frankly just tired of people writing about credit and attribution but not doing anything about it. We could extend that to other things, but it’s especially an issue for this topic. Ironically they are writing <em>papers</em> and improving their publication record as they write about how publication and research software is a strained process. I may not have solved this problem, but damn at least I’m trying to actually do something about it instead of spurting gas.</p><p>I find this idea exciting because there are so many directions you can go with it. When I first designed the idea I imagined a database and online interface where you could essentially connect your GitHub repository, and akin to a builder service, parse your repository on some event and derive a new credit or citation graph. Or you could have some set akin to the RSEPedia that are also updated regularly. And then, by way of having that database, we could do these same queries (that currently I’m doing statically) to say “What are the most important libraries for this language? Across the ecosystem?” or “How has this changed over time?” It would be a true way to derive the value of a library without needing people to publish papers, and totally automated and integrated with package managers, which is where people already should be putting their software.Heck, if someone gave me a cloud and a little bit of funding I’d love to work on this. Are there good reasons or use cases? I don’t know, but maybe.</p><p>So what do you think?</p>",
            "url": "https://hpc.social/personal-blog/2022/the-research-software-ecosystem/",
            
            
            
            
            
            "date_published": "2022-04-24T13:30:00-06:00",
            "date_modified": "2022-04-24T13:30:00-06:00",
            
                "author": "Vanessasaurus"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/spooky-allocator-issues-and-fixes/",
            "title": "Spooky Allocator Issues and Fixes",
            "summary": null,
            "content_text": "Recently we started noticing performance issues in the main branch of Ceph that ultimately were traced back to a commit last summer that changed parts of our AVL and hybrid disk allocator implementations in bluestore.  Strangly, the issue only affected some of the NVMe drives in our test lab but not others.  The quick fix was to always update and save the allocator’s cursor position so that we don’t search (and fail) over and over in fast-fit mode for every allocation request.  Another interesting offshoot of this though is that it may be much nicer to limit fast-fit searches based on time rather than byte distance or the number of iterations.",
            "content_html": "<p>Recently we started noticing performance issues in the main branch of Ceph that ultimately were traced back to a commit last summer that changed parts of our AVL and hybrid disk allocator implementations in bluestore.  Strangly, the issue only affected some of the NVMe drives in our test lab but not others.  The quick <a href=\"https://github.com/ceph/ceph/pull/45884\">fix</a> was to always update and save the allocator’s cursor position so that we don’t search (and fail) over and over in fast-fit mode for every allocation request.  Another interesting offshoot of this though is that it may be much <a href=\"https://github.com/ceph/ceph/pull/45771\">nicer</a> to limit fast-fit searches based on time rather than byte distance or the number of iterations.</p>",
            "url": "https://hpc.social/personal-blog/2022/spooky-allocator-issues-and-fixes/",
            
            
            
            
            
            "date_published": "2022-04-13T01:00:00-06:00",
            "date_modified": "2022-04-13T01:00:00-06:00",
            
                "author": "Mark Nelson's Blog"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/lsf-hookin-up-with-the-criu/",
            "title": "LSF hookin' up with the CRIU",
            "summary": null,
            "content_text": "With the unpredicable spring weather here in Southern Ontario, weekend projectsare the order of the day. Whether it&rsquo;s fixing my bike for spring, repairing things in the home which I&rsquo;ve neglected for far long or topics relating to IT which have been percolating in my head, I am a textbook busybody.A few decades back, when I was a support engineer at Platform Computing, I hadmy first experience working with clients using both kernel-level and user-levelcheckpoint and restart through the HPC workload scheduler Platform LSF (nowIBM Spectrum LSF). I distinctly recall that user-level library was a bit trickyas you had to link your home grown code against it - and it had numerouslimitations which I can&rsquo;t recall off the top of my head. Back then, like today,IBM Spectrum LSF provides a number of ways that administrators can extend capabilities using plug-ins.Checkpoint and restart is an example where plug-ins can be used. More about thislater.I&rsquo;ve been keeping an eye on the project known as CRIU for some time. CRIU, which stands for Checkpoint/Restore In Userspaceprovides checkpoint and restart functionality on Linux. And I thought it may bean interesting weekend project to integrate CRIU with LSF. As it turns out,I was not blazing any trails here as I found that there are others alreadyusing CRIU with LSF today. Nevertheless, I decided to give it a try.My system of choice for this tinkering was a dual-socket POWER9 based systemrunning CentOS Stream 8 and IBM Spectrum LSF Suite for HPC v10.2.0.12. TheLSF online documentation contains information on the specificationsof the LSF plugins for checkpoint and restart. The plugins are known as echkpnt and erestart, where the &ldquo;e&rdquo; denotes external.Here is a quick rundown on the steps to integrate CRIU with LSF.It turns out that my system already had criu installed. It&rsquo;s a dependencyon runc which was installed as part of podman. This step really dependson your distro. In my case, dnf provides criu was my friend.# uname -aLinux kilenc 4.18.0-373.el8.ppc64le #1 SMP Tue Mar 22 15:28:39 UTC 2022 ppc64le ppc64le ppc64le GNU/Linux# criuUsage:  criu dump|pre-dump -t PID [&lt;options&gt;]  criu restore [&lt;options&gt;]  criu check [--feature FEAT]  criu page-server  criu service [&lt;options&gt;]  criu dedup  criu lazy-pages -D DIR [&lt;options&gt;]Commands:  dump           checkpoint a process/tree identified by pid  pre-dump       pre-dump task(s) minimizing their frozen time  restore        restore a process/tree  check          checks whether the kernel support is up-to-date  page-server    launch page server  service        launch service  dedup          remove duplicates in memory dump  cpuinfo dump   writes cpu information into image file  cpuinfo check  validates cpu information read from image fileTry -h|--help for more infoThe criu command needs to be run as root to be able to checkpointprocesses. As we are going to leverage criu directly in the LSF echkpnt anderestart scripts, I chose to enable sudo access for criu. To do this I simplyadded the following to /etc/sudoers.gsamu   ALL=NOPASSWD:/usr/sbin/criuNext, I tested that the basic criu functionality was working. I foundthis to be a useful blog on how to perform a simple test.With criu installed and working (see step 3), the next steps was to createthe echkpnt and erestart scripts which would ultimately call the appropriatecriu dump and criu restore commands. These scripts will be named echkpnt.criu and erestart.criu. The .criu extension denotes the checkpoint andrestart method name in LSF. The checkpoint method is specified at the time ofjob submission in LSF.The key for the echkpnt.criu script is to build out the list of PIDs forthe job in question. For this I used an inelegant approach - simplyscraping the output of the LSF bjobs -l command. This listof PIDs is then used as arguments to the criu dump command.The example echkpnt.criu script is included below.I used a simple approach as well for erestart.criu. As per the specificationfor erestart, the key is to create a new LSF jobfile which containsthe appropriate criu restore invocation, pointing to the checkpointdata. The example erestart.criu script is included below.With the echkpnt.criu and erestart.criu scripts in the $LSF_SERVERDIRdirectory, the process to perform a checkpoint and restart of LSF jobs isstraight forward using bchkpnt and brestart commands respectively.Here is a simple example.Submit a job as checkpointable. The checkpoint method criu is specified as well as the location where the checkpoint data will be written to.$ bsub -k \"/home/gsamu/checkpoint_data method=criu\" ./criu_testJob &lt;12995&gt; is submitted to default queue &lt;normal&gt;.The executable criu_test simply writes a message to standard out every 3 seconds.$ bpeek 12995&lt;&lt; output from stdout &gt;&gt;0: Sleeping for three seconds ...1: Sleeping for three seconds ...2: Sleeping for three seconds ...3: Sleeping for three seconds ...4: Sleeping for three seconds ...Next, we see that LSF has detected the job PIDS. Now we&rsquo;re ready to perform the checkpoint.$ bjobs -l 12995 Job &lt;12995&gt;, User &lt;gsamu&gt;, Project &lt;default&gt;, Status &lt;RUN&gt;, Queue &lt;normal&gt;, Com                     mand &lt;./criu_test&gt;, Share group charged &lt;/gsamu&gt;Tue Apr 12 08:48:28: Submitted from host &lt;kilenc&gt;, CWD &lt;$HOME&gt;, C                     heckpoint directory &lt;/home/gsamu/checkpoint_data/12995&gt;;Tue Apr 12 08:48:29: Started 1 Task(s) on Host(s) &lt;kilenc&gt;, Alloc                     ated 1 Slot(s) on Host(s) &lt;kilenc&gt;, Executio                     n Home &lt;/home/gsamu&gt;, Execution CWD &lt;/home/gsamu&gt;;Tue Apr 12 08:48:38: Resource usage collected.                     MEM: 12 Mbytes;  SWAP: 0 Mbytes;  NTHREAD: 4                     PGID: 418130;  PIDs: 418130 418131 418133    MEMORY USAGE: MAX MEM: 12 Mbytes;  AVG MEM: 6 Mbytes  SCHEDULING PARAMETERS:           r15s   r1m  r15m   ut      pg    io   ls    it    tmp    swp    mem loadSched   -     -     -     -       -     -    -     -     -      -      -   loadStop    -     -     -     -       -     -    -     -     -      -      -    RESOURCE REQUIREMENT DETAILS: Combined: select[type == local] order[r15s:pg] Effective: select[type == local] order[r15s:pg] Initiate the checkpoint using the LSF bchkpnt command. The -k option is specified which will result in the job being checkpointed and killed.$ bchkpnt -k 12995Job &lt;12995&gt; is being checkpointedWe see in the history of the job using the bhist command that the checkpoint was initiated and succeeded. The job was subsequently killed (TERM_CHKPNT).$ bhist -l 12995 Job &lt;12995&gt;, User &lt;gsamu&gt;, Project &lt;default&gt;, Command &lt;./criu_test&gt;Tue Apr 12 08:48:28: Submitted from host &lt;kilenc&gt;, to Queue &lt;norm                     al&gt;, CWD &lt;$HOME&gt;, Checkpoint directory &lt;/home/gsamu/checkp                     oint_data/12995&gt;;Tue Apr 12 08:48:29: Dispatched 1 Task(s) on Host(s) &lt;kilenc&gt;, Al                     located 1 Slot(s) on Host(s) &lt;kilenc&gt;, Effec                     tive RES_REQ &lt;select[type == local] order[r15s:pg] &gt;;Tue Apr 12 08:48:31: Starting (Pid 418130);Tue Apr 12 08:48:31: Running with execution home &lt;/home/gsamu&gt;, Execution CWD &lt;                     /home/gsamu&gt;, Execution Pid &lt;418130&gt;;Tue Apr 12 08:54:14: Checkpoint initiated (actpid 419029);Tue Apr 12 08:54:15: Checkpoint succeeded (actpid 419029);Tue Apr 12 08:54:15: Exited with exit code 137. The CPU time used is 2.1 second                     s;Tue Apr 12 08:54:15: Completed &lt;exit&gt;; TERM_CHKPNT: job killed after checkpoint                     ing;\t\t       MEMORY USAGE:MAX MEM: 12 Mbytes;  AVG MEM: 11 Mbytes Summary of time in seconds spent in various states by  Tue Apr 12 08:54:15  PEND     PSUSP    RUN      USUSP    SSUSP    UNKWN    TOTAL  1        0        346      0        0        0        347         Restart the job from the checkpoint data with the LSF brestart command. A new jobID is assigned.$ brestart /home/gsamu/checkpoint_data/ 12995 Job &lt;12996&gt; is submitted to queue &lt;normal&gt;.$ bjobs -l 12996 Job &lt;12996&gt;, User &lt;gsamu&gt;, Project &lt;default&gt;, Status &lt;RUN&gt;, Queue &lt;normal&gt;, Com                     mand &lt;./criu_test&gt;, Share group charged &lt;/gsamu&gt;Tue Apr 12 08:55:57: Submitted from host &lt;kilenc&gt;, CWD &lt;$HOME&gt;, R                     estart, Checkpoint directory &lt;/home/gsamu/checkpoint_data/                     /12996&gt;;Tue Apr 12 08:55:58: Started 1 Task(s) on Host(s) &lt;kilenc&gt;, Alloc                     ated 1 Slot(s) on Host(s) &lt;kilenc&gt;, Executio                     n Home &lt;/home/gsamu&gt;, Execution CWD &lt;/home/gsamu&gt;;Tue Apr 12 08:56:07: Resource usage collected.                     MEM: 14 Mbytes;  SWAP: 0 Mbytes;  NTHREAD: 5                     PGID: 420069;  PIDs: 420069 420070 420073 420074 420076    MEMORY USAGE: MAX MEM: 14 Mbytes;  AVG MEM: 14 Mbytes  SCHEDULING PARAMETERS:           r15s   r1m  r15m   ut      pg    io   ls    it    tmp    swp    mem loadSched   -     -     -     -       -     -    -     -     -      -      -   loadStop    -     -     -     -       -     -    -     -     -      -      -    RESOURCE REQUIREMENT DETAILS: Combined: select[type == local] order[r15s:pg] Effective: select[type == local] order[r15s:pg] Viewing the standard output of the job, we see the point where it was killed and that it has picked up from where it left off.$ bpeek 12996&lt;&lt; output from stdout &gt;&gt;0: Sleeping for three seconds ...1: Sleeping for three seconds ...2: Sleeping for three seconds ...3: Sleeping for three seconds ...4: Sleeping for three seconds ...….….110: Sleeping for three seconds ...111: Sleeping for three seconds ...112: Sleeping for three seconds ...113: Sleeping for three seconds .../home/gsamu/.lsbatch/1649767708.12995: line 8: 418133 Killed                  ./criu_test114: Sleeping for three seconds ...115: Sleeping for three seconds ...116: Sleeping for three seconds ...117: Sleeping for three seconds ...118: Sleeping for three seconds ...119: Sleeping for three seconds ...120: Sleeping for three seconds ...........We&rsquo;ve demonstrated how one can integrate CRIU checkpoint and restartwith IBM Spectrum LSF using the echkpnt and erestart interfaces.As highlighted earlier, LSF provides a number of plugin interfaceswhich provides flexibility to organizations looking to do site specificcustomizations.",
            "content_html": "<p>With the unpredicable spring weather here in Southern Ontario, weekend projectsare the order of the day. Whether it&rsquo;s fixing my bike for spring, repairing things in the home which I&rsquo;ve neglected for far long or topics relating to IT which have been percolating in my head, I am a textbook busybody.</p><p>A few decades back, when I was a support engineer at Platform Computing, I hadmy first experience working with clients using both kernel-level and user-levelcheckpoint and restart through the HPC workload scheduler Platform LSF (nowIBM Spectrum LSF). I distinctly recall that user-level library was a bit trickyas you had to link your home grown code against it - and it had numerouslimitations which I can&rsquo;t recall off the top of my head. Back then, like today,<a href=\"https://www.ibm.com/products/hpc-workload-management\">IBM Spectrum LSF</a> provides a number of ways that administrators can extend capabilities using plug-ins.Checkpoint and restart is an example where plug-ins can be used. More about thislater.</p><p>I&rsquo;ve been keeping an eye on the project known as <a href=\"https://criu.org/Main_Page\">CRIU</a> for some time. CRIU, which stands for <em>Checkpoint/Restore In Userspace</em>provides checkpoint and restart functionality on Linux. And I thought it may bean interesting weekend project to integrate CRIU with LSF. As it turns out,I was not blazing any trails here as I found that there are others <a href=\"https://labs.icahn.mssm.edu/minervalab/documentation/job-checkpoint/\">alreadyusing CRIU with LSF</a> today. Nevertheless, I decided to give it a try.</p><p>My system of choice for this tinkering was a dual-socket POWER9 based systemrunning CentOS Stream 8 and IBM Spectrum LSF Suite for HPC v10.2.0.12. TheLSF online documentation contains information on the specificationsof the LSF plugins for <a href=\"https://www.ibm.com/docs/en/spectrum-lsf/10.1.0?topic=restart-configuration-enable-job-checkpoint\">checkpoint and restart</a>. The plugins are known as <em>echkpnt</em> and <em>erestart</em>, where the &ldquo;e&rdquo; denotes external.</p><p>Here is a quick rundown on the steps to integrate CRIU with LSF.</p><ul><li>It turns out that my system already had <em>criu</em> installed. It&rsquo;s a dependencyon <em>runc</em> which was installed as part of <em>podman</em>. This step really dependson your distro. In my case, <em>dnf provides criu</em> was my friend.</li></ul><div class=\"highlight\"><pre><code class=\"language-plaintext\"># uname -aLinux kilenc 4.18.0-373.el8.ppc64le #1 SMP Tue Mar 22 15:28:39 UTC 2022 ppc64le ppc64le ppc64le GNU/Linux# criuUsage:  criu dump|pre-dump -t PID [&lt;options&gt;]  criu restore [&lt;options&gt;]  criu check [--feature FEAT]  criu page-server  criu service [&lt;options&gt;]  criu dedup  criu lazy-pages -D DIR [&lt;options&gt;]Commands:  dump           checkpoint a process/tree identified by pid  pre-dump       pre-dump task(s) minimizing their frozen time  restore        restore a process/tree  check          checks whether the kernel support is up-to-date  page-server    launch page server  service        launch service  dedup          remove duplicates in memory dump  cpuinfo dump   writes cpu information into image file  cpuinfo check  validates cpu information read from image fileTry -h|--help for more info</code></pre></div><ul><li>The <em>criu</em> command needs to be run as root to be able to checkpointprocesses. As we are going to leverage criu directly in the LSF echkpnt anderestart scripts, I chose to enable sudo access for criu. To do this I simplyadded the following to <em>/etc/sudoers</em>.</li></ul><div class=\"highlight\"><pre><code class=\"language-plaintext\">gsamu   ALL=NOPASSWD:/usr/sbin/criu</code></pre></div><ul><li><p>Next, I tested that the basic <em>criu</em> functionality was working. I foundthis to be a useful <a href=\"https://www.redhat.com/en/blog/how-can-process-snapshotrestore-help-save-your-day\">blog</a> on how to perform a simple test.</p></li><li><p>With criu installed and working (see step 3), the next steps was to createthe <em>echkpnt</em> and <em>erestart</em> scripts which would ultimately call the appropriate<em>criu dump</em> and <em>criu restore</em> commands. These scripts will be named <em>echkpnt.criu</em> and <em>erestart.criu</em>. The <em>.criu</em> extension denotes the checkpoint andrestart method name in LSF. The checkpoint method is specified at the time ofjob submission in LSF.</p></li></ul><p>The key for the <em>echkpnt.criu</em> script is to build out the list of PIDs forthe job in question. For this I used an inelegant approach - simplyscraping the output of the LSF <em>bjobs -l</em> command. This listof PIDs is then used as arguments to the <em>criu dump</em> command.The example <em>echkpnt.criu</em> script is included below.</p><!-- raw HTML omitted --><p>I used a simple approach as well for <em>erestart.criu</em>. As per the specificationfor <em>erestart</em>, the key is to create a new LSF jobfile which containsthe appropriate <em>criu restore</em> invocation, pointing to the checkpointdata. The example <em>erestart.criu</em> script is included below.</p><!-- raw HTML omitted --><ul><li><p>With the <em>echkpnt.criu</em> and <em>erestart.criu</em> scripts in the $LSF_SERVERDIRdirectory, the process to perform a checkpoint and restart of LSF jobs isstraight forward using <em>bchkpnt</em> and <em>brestart</em> commands respectively.Here is a simple example.</p></li><li><p>Submit a job as checkpointable. The checkpoint method <em>criu</em> is specified as well as the location where the checkpoint data will be written to.</p></li></ul><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ bsub -k \"/home/gsamu/checkpoint_data method=criu\" ./criu_testJob &lt;12995&gt; is submitted to default queue &lt;normal&gt;.</code></pre></div><ul><li>The executable <em>criu_test</em> simply writes a message to standard out every 3 seconds.</li></ul><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ bpeek 12995&lt;&lt; output from stdout &gt;&gt;0: Sleeping for three seconds ...1: Sleeping for three seconds ...2: Sleeping for three seconds ...3: Sleeping for three seconds ...4: Sleeping for three seconds ...</code></pre></div><ul><li><p>Next, we see that LSF has detected the job PIDS. Now we&rsquo;re ready to perform the checkpoint.<div class=\"highlight\"><pre><code class=\"language-plaintext\">$ bjobs -l 12995 Job &lt;12995&gt;, User &lt;gsamu&gt;, Project &lt;default&gt;, Status &lt;RUN&gt;, Queue &lt;normal&gt;, Com                     mand &lt;./criu_test&gt;, Share group charged &lt;/gsamu&gt;Tue Apr 12 08:48:28: Submitted from host &lt;kilenc&gt;, CWD &lt;$HOME&gt;, C                     heckpoint directory &lt;/home/gsamu/checkpoint_data/12995&gt;;Tue Apr 12 08:48:29: Started 1 Task(s) on Host(s) &lt;kilenc&gt;, Alloc                     ated 1 Slot(s) on Host(s) &lt;kilenc&gt;, Executio                     n Home &lt;/home/gsamu&gt;, Execution CWD &lt;/home/gsamu&gt;;Tue Apr 12 08:48:38: Resource usage collected.                     MEM: 12 Mbytes;  SWAP: 0 Mbytes;  NTHREAD: 4                     PGID: 418130;  PIDs: 418130 418131 418133    MEMORY USAGE: MAX MEM: 12 Mbytes;  AVG MEM: 6 Mbytes  SCHEDULING PARAMETERS:           r15s   r1m  r15m   ut      pg    io   ls    it    tmp    swp    mem loadSched   -     -     -     -       -     -    -     -     -      -      -   loadStop    -     -     -     -       -     -    -     -     -      -      -    RESOURCE REQUIREMENT DETAILS: Combined: select[type == local] order[r15s:pg] Effective: select[type == local] order[r15s:pg] </code></pre></div></p></li><li><p>Initiate the checkpoint using the LSF <em>bchkpnt</em> command. The <em>-k</em> option is specified which will result in the job being checkpointed and killed.<div class=\"highlight\"><pre><code class=\"language-plaintext\">$ bchkpnt -k 12995Job &lt;12995&gt; is being checkpointed</code></pre></div></p></li><li><p>We see in the history of the job using the bhist command that the checkpoint was initiated and succeeded. The job was subsequently killed (TERM_CHKPNT).<div class=\"highlight\"><pre><code class=\"language-plaintext\">$ bhist -l 12995 Job &lt;12995&gt;, User &lt;gsamu&gt;, Project &lt;default&gt;, Command &lt;./criu_test&gt;Tue Apr 12 08:48:28: Submitted from host &lt;kilenc&gt;, to Queue &lt;norm                     al&gt;, CWD &lt;$HOME&gt;, Checkpoint directory &lt;/home/gsamu/checkp                     oint_data/12995&gt;;Tue Apr 12 08:48:29: Dispatched 1 Task(s) on Host(s) &lt;kilenc&gt;, Al                     located 1 Slot(s) on Host(s) &lt;kilenc&gt;, Effec                     tive RES_REQ &lt;select[type == local] order[r15s:pg] &gt;;Tue Apr 12 08:48:31: Starting (Pid 418130);Tue Apr 12 08:48:31: Running with execution home &lt;/home/gsamu&gt;, Execution CWD &lt;                     /home/gsamu&gt;, Execution Pid &lt;418130&gt;;Tue Apr 12 08:54:14: Checkpoint initiated (actpid 419029);Tue Apr 12 08:54:15: Checkpoint succeeded (actpid 419029);Tue Apr 12 08:54:15: Exited with exit code 137. The CPU time used is 2.1 second                     s;Tue Apr 12 08:54:15: Completed &lt;exit&gt;; TERM_CHKPNT: job killed after checkpoint                     ing;\t\t       MEMORY USAGE:MAX MEM: 12 Mbytes;  AVG MEM: 11 Mbytes Summary of time in seconds spent in various states by  Tue Apr 12 08:54:15  PEND     PSUSP    RUN      USUSP    SSUSP    UNKWN    TOTAL  1        0        346      0        0        0        347         </code></pre></div></p></li><li><p>Restart the job from the checkpoint data with the LSF <em>brestart</em> command. A new jobID is assigned.<div class=\"highlight\"><pre><code class=\"language-plaintext\">$ brestart /home/gsamu/checkpoint_data/ 12995 Job &lt;12996&gt; is submitted to queue &lt;normal&gt;.$ bjobs -l 12996 Job &lt;12996&gt;, User &lt;gsamu&gt;, Project &lt;default&gt;, Status &lt;RUN&gt;, Queue &lt;normal&gt;, Com                     mand &lt;./criu_test&gt;, Share group charged &lt;/gsamu&gt;Tue Apr 12 08:55:57: Submitted from host &lt;kilenc&gt;, CWD &lt;$HOME&gt;, R                     estart, Checkpoint directory &lt;/home/gsamu/checkpoint_data/                     /12996&gt;;Tue Apr 12 08:55:58: Started 1 Task(s) on Host(s) &lt;kilenc&gt;, Alloc                     ated 1 Slot(s) on Host(s) &lt;kilenc&gt;, Executio                     n Home &lt;/home/gsamu&gt;, Execution CWD &lt;/home/gsamu&gt;;Tue Apr 12 08:56:07: Resource usage collected.                     MEM: 14 Mbytes;  SWAP: 0 Mbytes;  NTHREAD: 5                     PGID: 420069;  PIDs: 420069 420070 420073 420074 420076    MEMORY USAGE: MAX MEM: 14 Mbytes;  AVG MEM: 14 Mbytes  SCHEDULING PARAMETERS:           r15s   r1m  r15m   ut      pg    io   ls    it    tmp    swp    mem loadSched   -     -     -     -       -     -    -     -     -      -      -   loadStop    -     -     -     -       -     -    -     -     -      -      -    RESOURCE REQUIREMENT DETAILS: Combined: select[type == local] order[r15s:pg] Effective: select[type == local] order[r15s:pg] </code></pre></div></p></li><li><p>Viewing the standard output of the job, we see the point where it was killed and that it has picked up from where it left off.<br /><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ bpeek 12996&lt;&lt; output from stdout &gt;&gt;0: Sleeping for three seconds ...1: Sleeping for three seconds ...2: Sleeping for three seconds ...3: Sleeping for three seconds ...4: Sleeping for three seconds ...….….110: Sleeping for three seconds ...111: Sleeping for three seconds ...112: Sleeping for three seconds ...113: Sleeping for three seconds .../home/gsamu/.lsbatch/1649767708.12995: line 8: 418133 Killed                  ./criu_test114: Sleeping for three seconds ...115: Sleeping for three seconds ...116: Sleeping for three seconds ...117: Sleeping for three seconds ...118: Sleeping for three seconds ...119: Sleeping for three seconds ...120: Sleeping for three seconds ...........</code></pre></div></p></li></ul><p>We&rsquo;ve demonstrated how one can integrate CRIU checkpoint and restartwith IBM Spectrum LSF using the <em>echkpnt</em> and <em>erestart</em> interfaces.As highlighted earlier, LSF provides a number of plugin interfaceswhich provides flexibility to organizations looking to do site specificcustomizations.</p>",
            "url": "https://hpc.social/personal-blog/2022/lsf-hookin-up-with-the-criu/",
            
            
            
            
            
            "date_published": "2022-04-12T19:32:04-06:00",
            "date_modified": "2022-04-12T19:32:04-06:00",
            
                "author": "Ramblings of a supercomputing enthusiast."
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/relivin-the-90-s-amiga-style/",
            "title": "Relivin' the 90's - Amiga style",
            "summary": null,
            "content_text": "Although I very much started my experience with home computers with IBMcompatibles running MSDOS in the late 1980&rsquo;s, I&rsquo;m a lifelong, self-professedCommodore-Amiga addict. I distinctly recall the launch of the Amiga A1000 andbeing dazzled by it&rsquo;s multimedia capabilities around the same time thatI had a PC XT with CGA graphics. I was instantly hooked. Having great videogames for the time was just icing on the cake.I started my Amiga experience with an A500, which I quickly traded in for anA2000 model, which I still have today. I came across an A3000 in the late 1990&rsquo;sfor a small sum which I added to my collection. The A3000 is my favouriteAmiga system with onboard SCSI and it&rsquo;s design is reminiscent of pizza-boxUNIX servers which were common back in the day.The majority of my friends at the time were all-in on PCs. But for me therewas just something a bit clinical and boring about them. The Amiga filledthis gap for me and continues to do so. It&rsquo;s probably one of bigreasons why I still to this day tinker so much with non-X86 systems.Retro computing is a hobby that requires much time. So it&rsquo;s sometimeschallenging to juggle this hobby with other things, especially as the weatherturns warmer here in Southern Ontario. My A3000 system was one that I waslooking to prioritize for resurrection this spring. This is in particularbecause the last thing I tinkered with on the A3000 roughly 20 years back was Amiga UNIX. Yes, my A3000 sat in storage for around 20 years!  In the mid to late 90&rsquo;s, I ventured out to an Amiga speciality shop in London, Ontario (Canada) for a clearance they were having.  It&rsquo;s there that I happened across Amiga UNIX software (tape and manuals), as well the Commodore A2410 High Resolution Graphics board, Commodore A2065 Ethernet board and a Wangtek 5150ES tape drive(which is mounted in a SUN Microsystems external case). Here is a view of theAmiga ShowConfig output.I had the foresight to remove the motherboard RTC batteries beforestoring the systems. But my A3000 refused to boot when I took it out of storagelate in 2021. After much fiddling, I decided to reach out to a local Amigarepair specialist. The gentleman worked at Comspec(?), which did repair workfor Commodore back in the day.I recently got my A3000 back after the fault was corrected, and a newreplaceable coin battery for the RTC was installed. The fault turnedout to be an issue with some of the ZIP memory sockets. Because of thedifficulty and cost in purchasing ZIP memory back in the day, I purchaseda ProvTech AmiFast 3000 ZIP to SIMM converter which allows me to use 72-pin SIMM memory.With a working A3000 system, it was time to look at software once again.I found my old dusty Amiga OS 3.5 and OS 3.9 original media sets. Withsome effort I was able to get Amiga OS 3.9 installed on the system. It&rsquo;snot that the installation is difficult, it was more a matter of getting myCDROM working and clearing out some of the cobwebs on my Amiga knowledge.Additionally, I was able to successfully boot Amiga UNIX off an externalSCSI disk which I installed back in roughly &lsquo;98 or &lsquo;99. I plan to writemore about Amiga UNIX in a subsequent post. For those who are curious aboutAmiga UNIX there is a fantastic wiki here.Back to Amiga OS 3.9. After getting it installed successfully I had a few goals:Get my Amiga online via the A2065 Ethernet boardGet a high resolution Workbench (desktop) via the A2410Relive the memories!Amiga on the &lsquo;NetI recall back in the day various Amiga TCP/IP implementations such asAS225 and AmiTCP. Consulting with the gentleman who repaired my Amiga,he suggested Roadshow. I&rsquo;d neverheard of Roadshow before, but downloaded and got the trial version workingeasily. I required to copy the a2065.device driver for theA2065 board to the system and created the necessary configuration filein SYS:Devs/NetInterfaces. The configuration file A2065 is shown in theimage below.A quick aside here. I had to create a CD with a bunch of softwareincluding Roadshow and a number of utilities from Aminet such as the A2065 device driver. Aminet is one of the goto places for Amigasoftware on the net.I found Roadshow so easy to get setup and working that I purchased a licensefor it. I also purchased licenses at the same time for GoADF, which is a great utility for managing ADF (Amiga Disk Format) files.With a working TCP/IP stack, I installed the trial version of iBrowse, in addition to the FTP utility RNOXfer (from Aminet).  With a working FTP client, I could now more easily move files to and from the A3000.  This definitely helped for the next stage.Just a note that browsing on the Amiga is definitely a retro experience.This is in no way a slight at the fine folks who develop and maintainbrowsers such as iBrowse.  I&rsquo;m considering updating my iBrowse demolicense to a full license in the future as well.I also took the opportunity at this time to install an NTP client. Even thoughmy Amiga now has a working RTC, I still like to use NTP to keep the clockaccurately set.  For this I used the AmiTimeKeeper utility from Aminet.I pointed it as I do normally to the NTP servers at the National ResearchCouncil (NRC) of Canada.  TimeKeeper has a CLI interface as well as a UIstatus window to provide information on the synchronization status.Workbench à la A2410It was time to move on to having a high resolution Workbench (desktop) experience. I also own a Picasso II video card which is presently in my Amiga 2000 system. Using P96 or CyberGraphX on the Picasso II was quite straightforward inthe past.  My goal this time was to use the A2410, which from what I couldread, was supported in CyberGraphX V4.Thing is, when I went to install CyberGraphX V4 from my original media,I did not see the A2410 listed. It was only when I applied the update(s) that I could see the A2410 listed as a supported video card. Note the final patch version of CyberGraphX I&rsquo;m using is from cgxv42_rc6.lha which I downloadedfrom Aminet here.The A2410 CyberGraphX (CGX) driver installed without a hitch, gettingit to work was a challenge. Although I could get a Workbench to appear in thedesired resolution and colours, when I double clicked on any icon on the desktop, the system would hang. It was only through trial and error that I discoveredthat some specific CyberGraphX variables had to be set.  The screenshot below of the CyberGraphX settings tool shows the current, woring settings. Ultimately,the hang seemed to be addressed by enabling the CGX SUPERGELS variable.Here is a look at the CGX showcgxconfig tool output.A screenshot of the Workbench driven by the A2410 is shown below. Theperformance is not great, but it does work, and I&rsquo;m super pleased aboutthat. On the subject of graphics cards, I&rsquo;ve had my eye on the MNT ZZ9000 which I&rsquo;m considering purchasing to breathe more life into my A3000.The next stage in this journey is to get the same configuration workingwith Amiga OS 3.2, which I purchased from the folks at Retro Rewind in Toronto. According to what I&rsquo;ve read, I need to downgrade theintuition.library version to get CyberGraphX working with OS 3.2. I&rsquo;llwrite more about this when I have the opportunity.And now, I&rsquo;m ready to begin to relive those memories!Update!  Here are some photos of the A2065, A2410 and A3000 daughterboardfrom my system.",
            "content_html": "<p>Although I very much started my experience with home computers with IBMcompatibles running MSDOS in the late 1980&rsquo;s, I&rsquo;m a lifelong, self-professedCommodore-Amiga addict. I distinctly recall the launch of the Amiga A1000 andbeing dazzled by it&rsquo;s multimedia capabilities around the same time thatI had a PC XT with CGA graphics. I was instantly hooked. Having great videogames for the time was just icing on the cake.</p><p>I started my Amiga experience with an A500, which I quickly traded in for anA2000 model, which I still have today. I came across an A3000 in the late 1990&rsquo;sfor a small sum which I added to my collection. The A3000 is my favouriteAmiga system with onboard SCSI and it&rsquo;s design is reminiscent of pizza-boxUNIX servers which were common back in the day.</p><p>The majority of my friends at the time were all-in on PCs. But for me therewas just something a bit clinical and boring about them. The Amiga filledthis gap for me and continues to do so. It&rsquo;s probably one of bigreasons why I still to this day tinker so much with non-X86 systems.</p><p>Retro computing is a hobby that requires much time. So it&rsquo;s sometimeschallenging to juggle this hobby with other things, especially as the weatherturns warmer here in Southern Ontario. My A3000 system was one that I waslooking to prioritize for resurrection this spring. This is in particularbecause the last thing I tinkered with on the A3000 roughly 20 years back was Amiga UNIX. Yes, my A3000 sat in storage for around 20 years!  In the mid to late 90&rsquo;s, I ventured out to an Amiga speciality shop in London, Ontario (Canada) for a clearance they were having.  It&rsquo;s there that I happened across Amiga UNIX software (tape and manuals), as well the <a href=\"http://amiga.resource.cx/exp/a2410\">Commodore A2410 High Resolution Graphics board</a>, <a href=\"http://amiga.resource.cx/exp/a2065\">Commodore A2065 Ethernet board</a> and a Wangtek 5150ES tape drive(which is mounted in a SUN Microsystems external case). Here is a view of theAmiga <em>ShowConfig</em> output.</p><figure><img src=\"https://www.gaborsamu.com/images/system_config.png\" /></figure><p>I had the foresight to remove the motherboard RTC batteries beforestoring the systems. But my A3000 refused to boot when I took it out of storagelate in 2021. After much fiddling, I decided to reach out to a local Amigarepair specialist. The gentleman worked at Comspec(?), which did repair workfor Commodore back in the day.</p><p>I recently got my A3000 back after the fault was corrected, and a newreplaceable coin battery for the RTC was installed. The fault turnedout to be an issue with some of the ZIP memory sockets. Because of thedifficulty and cost in purchasing ZIP memory back in the day, I purchaseda <a href=\"http://amiga.resource.cx/exp/amifast\">ProvTech AmiFast 3000</a> ZIP to SIMM converter which allows me to use 72-pin SIMM memory.</p><p>With a working A3000 system, it was time to look at software once again.I found my old dusty Amiga OS 3.5 and OS 3.9 original media sets. Withsome effort I was able to get Amiga OS 3.9 installed on the system. It&rsquo;snot that the installation is difficult, it was more a matter of getting myCDROM working and clearing out some of the cobwebs on my Amiga knowledge.</p><p>Additionally, I was able to successfully boot Amiga UNIX off an externalSCSI disk which I installed back in roughly &lsquo;98 or &lsquo;99. I plan to writemore about Amiga UNIX in a subsequent post. For those who are curious aboutAmiga UNIX there is a fantastic wiki <a href=\"https://www.amigaunix.com/doku.php\">here</a>.</p><p>Back to Amiga OS 3.9. After getting it installed successfully I had a few goals:</p><ul><li>Get my Amiga online via the A2065 Ethernet board</li><li>Get a high resolution Workbench (desktop) via the A2410</li><li>Relive the memories!</li></ul><p><strong>Amiga on the &lsquo;Net</strong></p><p>I recall back in the day various Amiga TCP/IP implementations such asAS225 and AmiTCP. Consulting with the gentleman who repaired my Amiga,he suggested <a href=\"http://roadshow.apc-tcp.de/index-en.php\">Roadshow</a>. I&rsquo;d neverheard of Roadshow before, but downloaded and got the trial version workingeasily. I required to copy the a2065.device driver for theA2065 board to the system and created the necessary configuration filein <em>SYS:Devs/NetInterfaces</em>. The configuration file A2065 is shown in theimage below.</p><figure><img src=\"https://www.gaborsamu.com/images/roadshow1.png\" /></figure><p>A quick aside here. I had to create a CD with a bunch of softwareincluding Roadshow and a number of utilities from <a href=\"http://aminet.net/\">Aminet</a> such as the A2065 device driver. Aminet is one of the goto places for Amigasoftware on the net.</p><p>I found Roadshow so easy to get setup and working that I purchased a licensefor it. I also purchased licenses at the same time for <a href=\"http://www.bitplan.pl/goadf/\">GoADF</a>, which is a great utility for managing ADF (Amiga Disk Format) files.</p><p>With a working TCP/IP stack, I installed the trial version of <a href=\"https://www.ibrowse-dev.net/\">iBrowse</a>, in addition to the FTP utility RNOXfer (from Aminet).  With a working FTP client, I could now more easily move files to and from the A3000.  This definitely helped for the next stage.</p><figure><img src=\"https://www.gaborsamu.com/images/rnoxfer.png\" /></figure><p>Just a note that browsing on the Amiga is definitely a retro experience.This is in no way a slight at the fine folks who develop and maintainbrowsers such as iBrowse.  I&rsquo;m considering updating my iBrowse demolicense to a full license in the future as well.</p><p>I also took the opportunity at this time to install an NTP client. Even thoughmy Amiga now has a working RTC, I still like to use NTP to keep the clockaccurately set.  For this I used the AmiTimeKeeper utility from Aminet.I pointed it as I do normally to the NTP servers at the National ResearchCouncil (NRC) of Canada.  TimeKeeper has a CLI interface as well as a UIstatus window to provide information on the synchronization status.</p><figure><img src=\"https://www.gaborsamu.com/images/timekeeper.png\" /></figure><p><strong>Workbench à la A2410</strong></p><p>It was time to move on to having a high resolution Workbench (desktop) experience. I also own a Picasso II video card which is presently in my Amiga 2000 system. Using P96 or CyberGraphX on the Picasso II was quite straightforward inthe past.  My goal this time was to use the A2410, which from what I couldread, was supported in CyberGraphX V4.</p><p>Thing is, when I went to install CyberGraphX V4 from my original media,I did not see the A2410 listed. It was only when I applied the update(s) that I could see the A2410 listed as a supported video card. Note the final patch version of CyberGraphX I&rsquo;m using is from <em>cgxv42_rc6.lha</em> which I downloadedfrom Aminet <a href=\"https://aminet.net/package/driver/video/CyberGraphX_4.3rc6\">here</a>.</p><p>The A2410 CyberGraphX (CGX) driver installed without a hitch, gettingit to work was a challenge. Although I could get a Workbench to appear in thedesired resolution and colours, when I double clicked on any icon on the desktop, the system would hang. It was only through trial and error that I discoveredthat some specific CyberGraphX variables had to be set.  The screenshot below of the CyberGraphX settings tool shows the current, woring settings. Ultimately,the hang seemed to be addressed by enabling the CGX <em>SUPERGELS</em> variable.</p><figure><img src=\"https://www.gaborsamu.com/images/cyber_prefs.png\" /></figure><p>Here is a look at the CGX <em>showcgxconfig</em> tool output.</p><figure><img src=\"https://www.gaborsamu.com/images/cyber_config.png\" /></figure><p>A screenshot of the Workbench driven by the A2410 is shown below. Theperformance is not great, but it does work, and I&rsquo;m super pleased aboutthat. On the subject of graphics cards, I&rsquo;ve had my eye on the MNT <a href=\"https://shop.mntmn.com/products/zz9000-for-amiga-preorder\">ZZ9000</a> which I&rsquo;m considering purchasing to breathe more life into my A3000.</p><figure><img src=\"https://www.gaborsamu.com/images/desktop_clean.png\" /></figure><p>The next stage in this journey is to get the same configuration workingwith Amiga OS 3.2, which I purchased from the folks at <a href=\"https://retrorewind.ca/\">Retro Rewind</a> in Toronto. According to what I&rsquo;ve read, I need to downgrade theintuition.library version to get CyberGraphX working with OS 3.2. I&rsquo;llwrite more about this when I have the opportunity.</p><p>And now, I&rsquo;m ready to begin to relive those memories!</p><p>Update!  Here are some photos of the A2065, A2410 and A3000 daughterboardfrom my system.</p><figure><img src=\"https://www.gaborsamu.com/images/a2065.jpg\" /></figure><figure><img src=\"https://www.gaborsamu.com/images/a2410.jpg\" /></figure><figure><img src=\"https://www.gaborsamu.com/images/daughterboard.jpg\" /></figure><figure><img src=\"https://www.gaborsamu.com/images/1992.jpg\" /></figure>",
            "url": "https://hpc.social/personal-blog/2022/relivin-the-90-s-amiga-style/",
            
            
            
            
            
            "date_published": "2022-03-29T13:53:09-06:00",
            "date_modified": "2022-03-29T13:53:09-06:00",
            
                "author": "Ramblings of a supercomputing enthusiast."
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/an-unstructured-rant-on-running-long-lived-software-services/",
            "title": "An unstructured rant on running long-lived software services",
            "summary": null,
            "content_text": "&#8211; Be kind to your colleagues. Be kind to your users. Be kind to yourself. This is a long haul and you’ll all fuck up.⁃ The natural environment for your code is production. It will run there longer than it does anywhere else. Design for prod first, and if possible, make your dev environment act like prod.⁃ Legacy code is the only code worth caring about.⁃ Users do weird stuff, but they usually have a very good reason, at least in their context. Learn from them.⁃ It’s 2022, please do structured logging.⁃ Contexts and tracing make everyone&#8217;s lives easier when it comes time to debug. At minimum, include a unique request id with every request and plumb it through the system.⁃ Do your logging in a separate thread. It sucks to find a daemon blocked and hanging because of a full disk or a down syslog server.⁃ Don’t page for individual machines going down. Do provide an easy or automated way for bad nodes to get thrown out of the system.&#8211; Be prepared for your automation to be the problem, and include circuit breakers or kill switches to stop it. I&#8217;ve seen health checks that started flagging every machine in the fleet as bad, whether it was healthy or not. We didn&#8217;t bring down prod because the code assumed if it flagged more than 15% of the fleet as bad, the problem was probably with the test, not the service.⁃ Make sure you have a way to know who your users are. If you allow anonymous access, you&#8217;ll discover in five years that a business-critical team you&#8217;ve never heard of is relying on you.⁃ Make sure you have a way to turn off access for an individual machine, user, etc. If your system does anything more expensive than sending network requests, it will be possible for a single bad client to overwhelm a distributed system with thousands of servers. Turning off their access is easier than begging them to stop.⁃ If you don’t implement QOS early on, it will be hellish to add it later, and you will certainly need it if your system lasts long enough.⁃ If you provide a client library, and your system is internal only, have it send logs to the same system as your servers. This will help trace issues back to misbehaving clients so much.⁃ Track the build time for every deployed server binary and monitor how old they are. If your CI process deploys daily, week-old binaries are a problem. Month-old binaries are a major incident.⁃ If you can get away with it (internal services): track the age of client library builds and either refuse to support builds older than X, or just cut them off entirely. It sucks to support requests from year-old clients, force them to upgrade!⁃ Despite all this, you will at some point start getting requests from an ancient software version, or otherwise malformed. Make sure these requests don’t break anything.⁃ Backups are a pain, and the tooling is often bad, but I swear they will save you one day. Take the time to invest in them.⁃ Your CI process should exercise your turnup process, your decommission process, and your backups workflow. Life will suck later if you discover one of these is broken.⁃ Third party services go down. Your service goes down too, but they probably won’t happen at the same time. Be prepared to either operate without them, or mirror them yourself⁃ Your users will never, ever care if you’re down because of a dependency. Every datacenter owned by AWS could be hit by a meteor at the same time, but your user will only ever ask “why doesn’t my service work?”⁃ Have good human relationships with your software dependencies. Know the people who develop them, keep in touch with them, make sure you understand each other. This is especially true internally but also important with external deps. In the end, software is made of people.⁃ If users don’t have personal buy-in to the security policy, they will find ways to work around them and complain about you for making their lives harder. Take the time to educate them, or you&#8217;ll be fighting them continuously.",
            "content_html": "<p>&#8211; Be kind to your colleagues. Be kind to your users. Be kind to yourself. This is a long haul and you’ll all fuck up.</p><p>⁃ The natural environment for your code is production. It will run there longer than it does anywhere else. Design for prod first, and if possible, make your dev environment act like prod.</p><p>⁃ Legacy code is the only code worth caring about.</p><p>⁃ Users do weird stuff, but they usually have a very good reason, at least in their context. Learn from them.</p><p>⁃ It’s 2022, <em>please</em> do structured logging.</p><p>⁃ Contexts and tracing make everyone&#8217;s lives easier when it comes time to debug. At minimum, include a unique request id with every request and plumb it through the system.</p><p>⁃ Do your logging in a separate thread. It sucks to find a daemon blocked and hanging because of a full disk or a down syslog server.</p><p>⁃ Don’t page for individual machines going down. Do provide an easy or automated way for bad nodes to get thrown out of the system.</p><p>&#8211; Be prepared for your automation to be the problem, and include circuit breakers or kill switches to stop it. I&#8217;ve seen health checks that started flagging every machine in the fleet as bad, whether it was healthy or not. We didn&#8217;t bring down prod because the code assumed if it flagged more than 15% of the fleet as bad, the problem was probably with the test, not the service.</p><p>⁃ Make sure you have a way to know who your users are. If you allow anonymous access, you&#8217;ll discover in five years that a business-critical team you&#8217;ve never heard of is relying on you.</p><p>⁃ Make sure you have a way to turn off access for an individual machine, user, etc. If your system does anything more expensive than sending network requests, it will be possible for a single bad client to overwhelm a distributed system with thousands of servers. Turning off their access is easier than begging them to stop.</p><p>⁃ If you don’t implement QOS early on, it will be hellish to add it later, and you will certainly need it if your system lasts long enough.</p><p>⁃ If you provide a client library, and your system is internal only, have it send logs to the same system as your servers. This will help trace issues back to misbehaving clients so much.</p><p>⁃ Track the build time for every deployed server binary and monitor how old they are. If your CI process deploys daily, week-old binaries are a problem. Month-old binaries are a major incident.</p><p>⁃ If you can get away with it (internal services): track the age of client library builds and either refuse to support builds older than X, or just cut them off entirely. It sucks to support requests from year-old clients, force them to upgrade!</p><p>⁃ Despite all this, you will at some point start getting requests from an ancient software version, or otherwise malformed. Make sure these requests don’t break anything.</p><p>⁃ Backups are a pain, and the tooling is often bad, but I swear they will save you one day. Take the time to invest in them.</p><p>⁃ Your CI process should exercise your turnup process, your decommission process, and your backups workflow. Life will suck later if you discover one of these is broken.</p><p>⁃ Third party services go down. Your service goes down too, but they probably won’t happen at the same time. Be prepared to either operate without them, or mirror them yourself</p><p>⁃ Your users will never, ever care if you’re down because of a dependency. Every datacenter owned by AWS could be hit by a meteor at the same time, but <em>your</em> user will only ever ask “why doesn’t my service work?”</p><p>⁃ Have good human relationships with your software dependencies. Know the people who develop them, keep in touch with them, make sure you understand each other. This is especially true internally but also important with external deps. In the end, software is made of people.</p><p>⁃ If users don’t have personal buy-in to the security policy, they <em>will</em> find ways to work around them and complain about you for making their lives harder. Take the time to educate them, or you&#8217;ll be fighting them continuously.</p>",
            "url": "https://hpc.social/personal-blog/2022/an-unstructured-rant-on-running-long-lived-software-services/",
            
            
            
            
            
            "date_published": "2022-03-12T16:00:00-07:00",
            "date_modified": "2022-03-12T16:00:00-07:00",
            
                "author": "Thinking Out Loud"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/what-i-ve-learned-from-looking-at-1-500-jobs-leading-research-computing-teams/",
            "title": "What I've Learned from Looking at 1,500 Jobs Leading Research Computing Teams",
            "summary": null,
            "content_text": "Job numbers continue to grow; lots of data and product management jobs; IR groups at Universities becoming bigger employers(Note: This post is adapted from #111 of the Research Computing Teams Newsletter)A year and a half ago I posted my observations on the first 500 jobs posted to the job board - we’re getting close to 1,500 now, and it’s worth taking a look to see what if anything has changed in research computing team leadership and management jobs1.There are some trends that have continued since the posting.  The jobs in industry are growing vastly beyond what I would have imagined possible when I started in research computing in the 1990s.  (The number of jobs working with biomedical data of one sort or another in particular is just astonishing.)  Rather than technical computing being a niche, it’s utterly mainstream now.  There are a lot of jobs out there, and I don’t even bother posting generic “data science manager” jobs unless they’re connected to some real complex research questions - which happens a lot, whether it’s fraud detection or improving financial modelling or supporting biomedical research.  Some really fun-looking jobs that would probably feel a lot like working at a research computing centre keep coming up at consultancies –– go visit a client and help them with their data science/data engineering/etc needs.  There’s also a growing number of data science/engineering jobs at Universities that fall under the Provost/VP Operations rather than the VPR’s side of the house — Institutional Research, looking at (say) student success in support of the teaching mission.Because of the growth in number of jobs, it is very much a candidate’s market out there.  I’m seeing postings –– especially for the traditional academic “director of research computing” jobs –— stay open for cringe-inducing periods of time.  A few in particular I’ve watched with vicarious embarrassment continue coming up in the listings for 8+ months.  That’s a bad sign for us as hiring managers - the market for individual contributors is at least as tight - but it’s amazing news for us as individuals.When I wrote that post in late 2020 it was just regulated industries like health/biotech or financial services that were developing data governance or other data management jobs, but now data management is popping up everywhere, whether it’s retail or logistics or anywhere else.   These are being joined, again first in the regulated industries, by data privacy or data risk management jobs.  Privacy-preserving data analysis jobs (and teams supporting same with software development) are also starting to be more common (and there’s a lot of cool research and technology work to be done there!)I’m also (finally!) starting to see a explicitly product management jobs in research computing, both academic and private-sector.  You see it around data management — bundling and curating of data into real data products — but also in software development, especially around analysis pipelines for some reason.Probably related to the growth of product vs project thinking, I’m starting to see a lot of “delivery manager” jobs that would have been called “project managers” just a year ago.   Projects are defined by having clearly defined start- and end-points up-front.  “Delivery” jobs seem to focus on sustained, ongoing work, more appropriate for long-lived products.These products that keep coming up often combine data, software, and systems one way or another.  That really points to weaknesses around organizing by type of skills - the research software engineering movement, for instance - as the lines between software and systems in this DevOps, infrastructure-as-code era is very fuzzy; and as data grows more and more important, data skills are needed everywhere.Especially for us as managers or leads, but especially for individual contributors as they grow their skills, it’s important to have a pretty holistic view of research computing and data and not try to break it up into silos.  The growing number of data engineering jobs is a great example.  That work often involves all three of software, systems, and data expertise.   Data engineering is getting so broad and important that not only are there different sub-fields, in large organizations there are likely to be completely distinct data engineering teams doing different work.  Trying to decide which of those jobs are “research software engineering” jobs and which aren’t is not a productive way forward, for those candidates or for us as a community.Needless to say, the growth of remote jobs has been off the charts - especially in the private sector, although the academic institutions are gamely doing what they can to keep up (often hampered by institutional policies).Late June 2022 update: At the time that I write this, there’s a slow down in hiring in tech, especially among early stage-startups.  That slowdown due to economic conditions as I write this is not, as far as I can tell, affecting these more research-oriented kinds of jobs.  The job board doesn’t have a lot of jobs from startups anyway.  For larger organizations, the biotech firms or the banking firms doing fraud detection research or the computing providers or academic groups or…  clearly do not view these roles as “nice to haves” that can wait until there’s a bit more economic certainty.            What counts as such a job?  Any job that involves leading, or mentoring people, or managing projects, programs, or products, in software, systems, or data curation/management/engineering/analysis to support the solution of research problems is a good fit.  If you are hiring for such a job, feel free to submit it to the job board. &#8617;      ",
            "content_html": "<h2 id=\"job-numbers-continue-to-grow-lots-of-data-and-product-management-jobs-ir-groups-at-universities-becoming-bigger-employers\">Job numbers continue to grow; lots of data and product management jobs; IR groups at Universities becoming bigger employers</h2><p>(Note: This post is adapted from <a href=\"https://www.researchcomputingteams.org/newsletter_issues/0111\">#111</a> of the <a href=\"https://www.researchcomputingteams.org\">Research Computing Teams Newsletter</a>)</p><p>A year and a half ago I <a href=\"https://www.dursi.ca/post/jobs_managing_research_computing_teams\">posted</a> my observations on the first 500 jobs posted to <a href=\"https://www.researchcomputingteams.org/jobs\">the job board</a> - we’re getting close to 1,500 now, and it’s worth taking a look to see what if anything has changed in research computing team leadership and management jobs<sup id=\"fnref:1\"><a class=\"footnote\" href=\"https://www.dursi.ca/feed.xml#fn:1\" rel=\"footnote\">1</a></sup>.</p><p>There are some trends that have continued since the posting.  The jobs in industry are growing vastly beyond what I would have imagined possible when I started in research computing in the 1990s.  (The number of jobs working with biomedical data of one sort or another in particular is just astonishing.)  Rather than technical computing being a niche, it’s utterly mainstream now.  There are a <em>lot</em> of jobs out there, and I don’t even bother posting generic “data science manager” jobs unless they’re connected to some real complex research questions - which happens a lot, whether it’s fraud detection or improving financial modelling or supporting biomedical research.  Some really fun-looking jobs that would probably feel a lot like working at a research computing centre keep coming up at consultancies –– go visit a client and help them with their data science/data engineering/<em>etc</em> needs.  There’s also a growing number of data science/engineering jobs at Universities that fall under the Provost/VP Operations rather than the VPR’s side of the house — Institutional Research, looking at (say) student success in support of the teaching mission.</p><p>Because of the growth in number of jobs, it is very much a candidate’s market out there.  I’m seeing postings –– <em>especially</em> for the traditional academic “director of research computing” jobs –— stay open for cringe-inducing periods of time.  A few in particular I’ve watched with vicarious embarrassment continue coming up in the listings for 8+ months.  That’s a bad sign for us as hiring managers - the market for individual contributors is at least as tight - but it’s amazing news for us as individuals.</p><p>When I wrote that post in late 2020 it was just regulated industries like health/biotech or financial services that were developing data governance or other data management jobs, but now data management is popping up everywhere, whether it’s retail or logistics or anywhere else.   These are being joined, again first in the regulated industries, by data privacy or data risk management jobs.  Privacy-preserving data analysis jobs (and teams supporting same with software development) are also starting to be more common (and there’s a <em>lot</em> of cool research and technology work to be done there!)</p><p>I’m also (finally!) starting to see a explicitly <em>product</em> management jobs in research computing, both academic and private-sector.  You see it around data management — bundling and curating of data into real data products — but also in software development, especially around analysis pipelines for some reason.</p><p>Probably related to the growth of product <em>vs</em> project thinking, I’m starting to see a lot of “delivery manager” jobs that would have been called “project managers” just a year ago.   Projects are defined by having clearly defined start- and end-points up-front.  “Delivery” jobs seem to focus on sustained, ongoing work, more appropriate for long-lived products.</p><p>These products that keep coming up often combine data, software, and systems one way or another.  That really points to weaknesses around organizing by type of skills - the research software engineering movement, for instance - as the lines between software and systems in this DevOps, infrastructure-as-code era is very fuzzy; and as data grows more and more important, data skills are needed everywhere.</p><p>Especially for us as managers or leads, but especially for individual contributors as they grow their skills, it’s important to have a pretty holistic view of research computing and data and not try to break it up into silos.  The growing number of data engineering jobs is a great example.  That work often involves all three of software, systems, and data expertise.   Data engineering is getting so broad and important that not only are there different sub-fields, in large organizations there are likely to be <a href=\"https://medium.com/data-arena/team-topologies-for-data-engineering-teams-a15c5eb3849c\">completely distinct data engineering teams</a> doing different work.  Trying to decide which of those jobs are “research software engineering” jobs and which aren’t is not a productive way forward, for those candidates or for us as a community.</p><p>Needless to say, the growth of remote jobs has been off the charts - especially in the private sector, although the academic institutions are gamely doing what they can to keep up (often hampered by institutional policies).</p><p><strong>Late June 2022 update</strong>: At the time that I write this, there’s a slow down in hiring in tech, especially among early stage-startups.  That slowdown due to economic conditions as I write this is <em>not</em>, as far as I can tell, affecting these more research-oriented kinds of jobs.  The job board doesn’t have a lot of jobs from startups anyway.  For larger organizations, the biotech firms or the banking firms doing fraud detection research or the computing providers or academic groups or…  clearly do not view these roles as “nice to haves” that can wait until there’s a bit more economic certainty.</p><hr /><div class=\"footnotes\">  <ol>    <li id=\"fn:1\">      <p>What counts as such a job?  Any job that involves leading, or mentoring people, or managing projects, programs, or products, in software, systems, or data curation/management/engineering/analysis to support the solution of research problems is a good fit.  If you are hiring for such a job, feel free to <a href=\"https://airtable.com/shrL6QGic3Mv9JFrs\">submit it to the job board</a>. <a class=\"reversefootnote\" href=\"https://www.dursi.ca/feed.xml#fnref:1\">&#8617;</a></p>    </li>  </ol></div>",
            "url": "https://hpc.social/personal-blog/2022/what-i-ve-learned-from-looking-at-1-500-jobs-leading-research-computing-teams/",
            
            
            
            
            
            "date_published": "2022-02-26T00:00:00-07:00",
            "date_modified": "2022-02-26T00:00:00-07:00",
            
                "author": "Jonathan Dursi's Blog"
            
        }
    
    ]
}
