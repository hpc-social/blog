{
    "version": "https://jsonfeed.org/version/1",
    "title": "hpc.social - Aggregated Personal Blog",
    "home_page_url": "https://hpc.social/personal-blog/",
    "feed_url": "https://hpc.social/personal-blog/feed.json",
    "description": "Shared personal experiences and stories",
    "icon": "https://hpc.social/personal-blog/assets/images/apple-touch-icon.png",
    "favicon": "https://hpc.social/personal-blog/assets/images/favicon.png",
    "expired": false,
    
    "author":  {
        "name": "hpc.social",
        "url": null,
        "avatar": null
    },
    
"items": [
    
        {
            "id": "https://hpc.social/personal-blog/2025/why-generic-software-design-advice-is-often-useless/",
            "title": "Why generic software design advice is often useless",
            "summary": null,
            "content_text": "In You can&#8217;t design software you don&#8217;t work on, Sean Goedecke discusses why generic advice on the design of software systems is often unhelpful.When you’re doing real work, concrete factors dominate generic factors. Having a clear understanding of what the code looks like right now is far, far more important than having a good grasp on general design patterns or principles.This tracks with my experience not just of software systems, but also systems with a hardware component (eg ML training clusters) or a facility component (eg datacenters). The specifics of your system absolutely dominate any general design guidance.As the manager of a team that publishes reference architectures, I do think that it’s helpful to clearly understand where your specific design differs from generic advice. If you’re going off the beaten path, you should know you’re doing that! And be able to plan for any additional validation involved in doing that.But relatedly, this is part of why I think that any generic advice should be based on some actually existing system. If you are telling someone they should follow a given principle, you should be able to point to an implementation that does follow that principle. Or else you’re just speculating into the void. Which admittedly can be fun but is not nearly as valuable as speaking from experience.",
            "content_html": "<p>In <em><a href=\"https://www.seangoedecke.com/you-cant-design-software-you-dont-work-on/\">You can&#8217;t design software you don&#8217;t work on</a>, </em>Sean Goedecke discusses why generic advice on the design of software systems is often unhelpful.</p><blockquote class=\"wp-block-quote is-layout-flow wp-block-quote-is-layout-flow\"><p><strong>When you’re doing real work, concrete factors dominate generic factors</strong>. Having a clear understanding of what the code looks like right now is far, far more important than having a good grasp on general design patterns or principles.</p></blockquote><p>This tracks with my experience not just of software systems, but also systems with a hardware component (eg ML training clusters) or a facility component (eg datacenters). The specifics of your system absolutely dominate any general design guidance.</p><p>As the manager of a team that publishes reference architectures, I do think that it’s helpful to clearly understand where your specific design differs from generic advice. If you’re going off the beaten path, you should k<em>now </em>you’re doing that! And be able to plan for any additional validation involved in doing that.</p><p>But relatedly, this is part of why I think that any generic advice should be based on some actually existing system. If you are telling someone they should follow a given principle, you should be able to point to an implementation that <em>does</em> follow that principle. </p><p>Or else you’re just speculating into the void. Which admittedly can be <em>fun</em> but is not nearly as valuable as speaking from experience.</p>",
            "url": "https://hpc.social/personal-blog/2025/why-generic-software-design-advice-is-often-useless/",
            
            
            
            
            
            "date_published": "2025-12-30T03:33:15-07:00",
            "date_modified": "2025-12-30T03:33:15-07:00",
            
                "author": "Thinking Out Loud"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2025/large-software-systems/",
            "title": "Large software systems",
            "summary": null,
            "content_text": "In Nobody understands how large software products work, Sean Goedecke makes a number of good points about how difficult it is to really grasp large software systems.In particular, some features impact every part of the system in unforeseen ways:Why are these features complicated? Because&nbsp;they affect every single other feature you build. If you add organizations and policy controls, you must build a policy control for every new feature you add. If you localize your product, you must include translations for every new feature. And so on. Eventually you’re in a position where you’re trying to figure out whether a self-hosted enterprise customer in the EU is entitled to access a particular feature, and&nbsp;nobody knows&nbsp;&#8211; you have to go and read through the code or do some experimenting to figure it out.Sean also points out that eventually the code itself has to be the source of truth, and debugging requires deep investigation of the continually-changing system.I’ve seen this happen in a bunch of different orgs, and it does seem to be true, especially for products with a large number of collaborating teams. I would add that in addition to the code itself, you often need to have conversations with the relevant teams to discern intent and history. Documentation only goes so far, eventually you need talk to people.",
            "content_html": "<p>In <em><a href=\"https://seangoedecke.com/nobody-knows-how-software-products-work/\">Nobody understands how large software products work</a></em>, Sean Goedecke makes a number of good points about how difficult it is to really grasp large software systems.</p><p>In particular, some features impact every part of the system in unforeseen ways:</p><blockquote class=\"wp-block-quote is-layout-flow wp-block-quote-is-layout-flow\"><p>Why are these features complicated? Because&nbsp;<strong>they affect every single other feature you build</strong>. If you add organizations and policy controls, you must build a policy control for every new feature you add. If you localize your product, you must include translations for every new feature. And so on. Eventually you’re in a position where you’re trying to figure out whether a self-hosted enterprise customer in the EU is entitled to access a particular feature, and&nbsp;<em>nobody knows</em>&nbsp;&#8211; you have to go and read through the code or do some experimenting to figure it out.</p></blockquote><p>Sean also points out that eventually the code itself has to be the source of truth, and debugging requires deep investigation of the continually-changing system.</p><p>I’ve seen this happen in a bunch of different orgs, and it does seem to be true, especially for products with a large number of collaborating teams. I would add that in addition to the code itself, you often need to have conversations with the relevant teams to discern intent and history. Documentation only goes so far, eventually you need talk to people.</p>",
            "url": "https://hpc.social/personal-blog/2025/large-software-systems/",
            
            
            
            
            
            "date_published": "2025-12-28T19:50:27-07:00",
            "date_modified": "2025-12-28T19:50:27-07:00",
            
                "author": "Thinking Out Loud"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2025/sc-25-recap/",
            "title": "SC'25 recap",
            "summary": null,
            "content_text": "  The annual SC conference was held last week, drawing over  16,000 registrants and 560 exhibitors  to in St. Louis, Missouri to talk about high-performance computing, artificial  intelligence, infrastructure, and science. It was my tenth time attending  in-person (12th overall), and as is always the case, it was a great week to  reconnect with colleagues, hear what people are worrying about, and get a  finger on the pulse of the now-rapidly changing HPC industry.Outside the SC'25 convention center on the only clear day of the week.Although every SC I've attended always felt a little different from the  previous year, this one felt quite different. Part of that results from my own  personal circumstances: this is the first year I attended as an employee of  VAST Data, and so the people with whom I met and the technical problems to  which I paid attention were certainly biased towards those most relevant to my  work. But the backdrop of the whole conference has also shifted. It's been  three SC conferences since ChatGPT came out, and it's now undeniable that AI  isn't simply on the horizon; it's shaping the field of HPC and scientific  computing. What used to be an argument of \"us vs. them\" is now more like \"them (and us?)\"  As has become tradition, I'm sharing some of my thoughts from the week with  the world in the hopes that someone finds this interesting and insightful.  I've roughly organized them into two areas big themes and the exhibition hall.Big themesTheme 1: The big number is losing its shineTop500The Gordon Bell PrizeFixing problems caused by the big numberTheme 2: HPC policy is becoming AI policyTheme 3: AI discourse is growing upAgentic workflowsData and agent-centric service infrastructureThe exhibit hallBy the numbersInteresting new technologyDell IR700HPE Cray GX5000Big themes  HPC has always been at the center of a tension between keeping things the same  (supercomputers are the most stable the day they are turned off) and pushing  the technological envelope (which is the fastest way to unlock new discovery).  The desire to push the envelope has always been a \"pull\" towards the future;  researchers first led with kooky ideas (like DAOS and Kokkos), and as those  ideas turn from research into production, they make new technologies (like  all-flash and AMD GPUs) accessible to scientists.  What hasn't historically happened, though, is a strong \"push\" towards the  future. Scientific HPC centers push themselves to justify building the next  big supercomputer, but it's been a given that there will always be another big  machine, so this push has been internal and gentle. Combined with the  not-so-urgent pull of HPC researchers, every center has gotten a new machine  every five years or so.  This is the year where it became clear to me that AI is now exerting a strong  push on the HPC industry--a shove even, forcing HPC centers around the world  to align themselves on an AI mission if they want to survive. All the  big-money HPC systems being announced this year are clearly being positioned  as AI-first and AI-motivated, and these announcements are going well beyond  simply peppering \"AI\" throughout the press release and otherwise acting as if  it was business-as-usual. This is the first SC where I saw scientists,  architects, and decision-makers being being forced to confront real tradeoffs  favor either HPC or AI, and they are beginning to choose AI.  This push-and-pull on HPC towards the future manifested in three big themes.  Theme 1: The big number is losing its shine  HPC has long organized itself around treating the big machine and the big  number as its top priority, and this is why the two largest HPC conferences of  the year honor the semiannual release of the Top500 list on their main stage.  However, this year felt like the first time that one number (that somehow  reflects \"performance\") dominated the conversation. Instead, the discourse was  more diffuse and discussed \"performance and x\" or \"the supercomputer and x.\"Top500  The place where this was most evident to me was at the  Top500 BOF, where the latest list was unveiled.  The biggest announcement was that Europe now has its first benchmark-confirmed  exascale system in JUPITER, which ran a full-system HPL at  1,000,184 TFLOPS  for two hours and seven minutes. However, JUPITER didn't get any stage time at  the BOF since, like Aurora, it actually debuted on a previous list with a  sub-exascale run. This run pushed it over the finish exascale finish line, but  if the Top500 list metadata is to be believed, the run used 100% of JUPITER's  5,884 nodes to break the barrier--a feat that is unlikely to be reproduced on  any production applications, since it is rare to have zero failed nodes in any  large-scale production environment.  So, while there was little fanfare for Europe in breaking the exaflops barrier  with its new big machine and big number, there were some big  announcements--one overt, and others more muted.  The biggest news was that the Top500 list is changing hands.  Whereas it has historically been controlled by three people--Jack Dongarra,  Horst Simon, and Erich Strohmaier--it will be transitioning to be  community-controlled under the stewardship of ACM SIGHPC. Dongarra, Simon, and  Strohmaier will still be on the steering committee under the ACM stewardship,  but this new governance structure opens the doors for new ideas to breathe new  life into the way systems are ranked and, more broadly, how \"performance\" is  meant to be interpreted from Rmax.  At present, the list (and related lists) are bound by rules that, in the  present day of reduced-precision accelerators, make little sense. For example,  using the Ozaki scheme within the LU decomposition is not allowed by Top500  despite the fact that it can produce the same answer with the same numerical  accuracy much faster than hardware FP64. And while the HPL-MxP benchmark does  allow solving the same problem using more creative methods, Strohmaier  highlighted a problem there too: it never dictated how to deal with multiple  levels of mixed precision until AIST broke the rankings. AIST ran HPL-MxP at  both 16-bit and 8-bit precisions, resulting in their ABCI 3.0 system  simultaneously ranking at #6 and #10.  These sorts of issues make it easy to question the value of leaderboards like  Top500 or HPL-MxP, as their definition of \"performance\" becomes increasingly  further divorced from how large supercomputers are really used. The past few  years have shown that there hasn't been the time or energy to get ahead of  these ambiguities amongst the three men maintaining the list, so transitioning  it to ACM will hopefully be a positive move that will give the list a chance  to be revitalized.  To their credit,  the incipient stagnation of the Top500 list was called out by  Strohmaier during his analysis of the list, acknowledging that \"growth has  tremendously slowed down compared to what it used to be\" and \"we don't have  proof of what is actually the reason for that:\"All the key highlights of this SC's Top500 list.China has stopped submitting, the AI and hyperscale providers really never  started submitting, and retired systems are being thrown off the list long  before they fall off the bottom. To me, this was a tacit acknowledgment that  the list does not have a bright future out to 2030 unless it is modernized to  be relevant to the way in which today's largest systems are actually being  used--which is not DGEMM.  The final surprising acknowledgment during Strohmaier's talk was that  the list is trailing the state of the art in hardware by  quite a bit. He pointed out that Blackwell systems are only now starting to  appear even though they've been shipping in volume for the better part of a  year. While he hypothesized that there is \"uneasiness\" about Blackwell in an  HPC context, the reality is that there are no Blackwells for HPC until the  Blackwell orders for hyperscale AI have been fulfilled. HPC is second in line,  and even then, the only Blackwells I could find on this year's Top500 list  were NVL8 configurations--not the NVL72 configurations that have been filling  up hyperscale datacenters like  Fairwater.  Strohmaier pointed out that Blackwell, by virtue of its HBM3e (vs. Hopper's  HBM3), is showing up higher on the HPCG list (which is a memory bandwidth  test) than on Top500 (which is an FP64 FLOPS test). He phrased this as  evidence that \"not everything is bad for the HPC community,\" but I would have  phrased my conclusion a little differently:    Blackwell is actually great for HPC, because most real workloads are    memory-bandwidth bound, not FLOPS bound. The fact that B200 offers similar    FP64 FLOPS at higher memory bandwidth means that real applications will get    higher effective use of those FP64 FLOPS.      Despite the above, Blackwell doesn't perform well on Top500 because HPL    doesn't reflect the reality that memory bandwidth is important. It follows    that HPL doesn't reflect the reality of real HPC applications. A Blackwell    system can be significantly better for real HPC applications than a    comparably sized Hopper system even though it may rank lower than Hopper on    Top500.      Blackwell isn't showing up in volume now because the HPC community is second    in line. The HPC community isn't uneasy as much as it is completely locked    out. The first NVIDIA-based exascale system debuted in November 2025 despite    its GPU being three years old, suggesting that if big Blackwell systems ever    appear on Top500, it'll happen in 2026-2027.    All of this is a roundabout way of showing that the big number--in this case,  the HPL score--no longer leads meaningful conversation around how useful a  system is for science.The Gordon Bell Prize  Another major indicator of the changing tide away from the big number was the  work that won this year's  Gordon Bell Prize. The winning  paper, titled \"Real-time Bayesian inference at extreme scale: A digital twin for tsunami    early warning applied to the Cascadia subduction zone,\" wasn't the typical case of running a huge simulation for a few hours and  reporting some result. Rather, it described a four-step workflow that  culminates in the desired insight popping out of a computation that runs  across only 128 nodes and completes in less than 0.2 seconds. Furthermore, the  hero run part could be decomposed into trivially parallel components, allowing  the bulk of the computation to be geographically distributed across HPC  centers or GPUs spread across on-prem and cloud providers.  My understanding of the work is that there was a massive \"offline\" computation  to precompute a few key matrices (Phase 1) followed by two shorter offline  steps that turn those matrices into the core of the digital twin. The last  step, which was \"online\" and designed to be computed in real-time, could then  take this core and solve the input problem with extremely low latency. This  workflow front-loads a hero run in such a way that, if an earthquake were to  occur, the risk of tsunami could be calculated in less than a second using  only modest compute resources and the precomputed core.  The authors eschewed methods that generated tons of FLOPS in favor of methods  that were less FLOPS-efficient but got to the answer faster. In the authors'  own words:    As shown in Fig. 7, higher FLOP/s does not necessarily lead to faster time-to-solution. On    MI300A nodes of El Capitan, the best-performing    implementation, Fused PA, achieves a lower percentage (5.2%) of theoretical    peak FLOP/s than Fused MF (5.5%) but is faster.    Interestingly, the hero computation here was embarrassingly parallel(ish) as  well; in their demonstration run, the hero run (Phase 1) was broken into 621  independent calculations each requiring 128 nodes (512 A100 GPUs) for about an  hour. Because they are independent, these tasks could be parallelized across  multiple HPC centers as well, and my understanding of the data volumes  involved are modest; Phase 1 would require a single shared copy of the input  mesh (a hundred GiB?) per HPC center, and each of the 621 tasks would output  around 8 GiB which would have to be copied back.  While I don't understand the mathematics behind this work, the paper took what  would've been a huge exascale-class mathematical problem (\"10 years on a  sustained 1 EFLOP/s machine\") and reformulated it into a workflow that solves  the problem faster and more usefully. Instead of brute-forcing the problem  with a big supercomputer, they split it into separate offline and online  parts, and this naturally allowed the most computationally expensive part to  be geographically distributable.  This work surrendered the need for a single big machine, and it didn't produce  a big-number result. But it did win the Gordon Bell Prize, again signaling  that the HPC community is beginning to look beyond performance-only and think  about awarding innovation according to outcomes, not just FLOPS.  The talk for this paper can be viewed  here in the SC25 Digital Experience.  Fixing problems caused by the big number  Most of my perception around the HPC community beginning to de-emphasize the  singular big machine or big number arose from organic interactions I had with  colleagues and customers though. It's hard to summarize how these  conversations went, but the  Lustre Community BoF  is a good example of what I saw elsewhere.  Lustre has long been the gold standard in high-performance parallel I/O in the  HPC community because it was designed from day one to deliver high bandwidth  above all else. As a result, Lustre already has the big number solved in many  ways, and events like the Lustre BOF are a great case study in what it looks  like for a performance-first technology to be pushed into adapting to deliver  more than just a big number.  First, the ever-innovative Stéphane Thiell from Stanford discussed the process  and tooling he developed to enable online capacity expansion of a Lustre file  system. The basis for it was a distributed, fault-tolerant tool he developed  that uses redis, lfs find, and lfs migrate to manage the state of file  migrations across Lustre servers as the file system is rebalanced. While a  part of me thought this was a great tool that would be super helpful for many  others, another part of me was kind of horrified.  Maybe I've been spoiled by working in hyperscale and AI these past three  years, but online capacity expansion and rebalancing is a built-in capability  of all distributed storage systems these days. All the major cloud object  stores do this, as do all modern parallel file systems including Quobyte,  VAST, and WEKA. Of course, none of these modern systems are as efficient (on a  per-CPU core or per-SSD basis) as Lustre at delivering peak performance. But  Stéphane's talk made me realize the price that's paid for this great  performance.  Andreas Dilger and others went on to talk about Lustre futures, and as they  were speaking, I noticed that nobody was talking about performance  improvements to Lustre. Rather, feature development was focused on catching up  in every other dimension--data governance, reliability, manageability, and  others. For example, Andreas talked a bit about the upcoming \"multi-tenancy\"  features coming to Lustre:It's a lot of work to retrofit multitenancy into a performance-first file system.I put “multi-tenancy” in quotes because these changes really representtrying to back into a security posture that is fundamentally different from theone that Lustre was designed around. In the pursuit of performance, Lustre (aswith most other HPC technologies) was designed assuming that security wassomeone else’s problem. By the time someone could log into a system that couldmount a Lustre file system, they had already been authenticated, and it was upto the OS on each compute node to authorize any interactions with Lustre itself.This is the “implicit trust” model.  The problem, of course, is that the rest of the world has adopted a \"zero  trust\" model which makes many things (except performance!) generally easier.  Compliance is easier when the system assumes that everything is encrypted as a  default and key management can be delegated to a third party. Because Lustre  didn't do this from the outset, it is going through this process of  retrofitting encryption in various places and using a mixture of nodemaps,  UID/GID maps, and shared secrets to patch over all the places where trust was  fundamentally implicit.  Later on in the BOF, panelists acknowledged (some half-heartedly) that  manageability of Lustre was a barrier. One panelist admitted that it took five  years of work to almost get to the point where a Lustre update can be done  without crashing applications. Another panelist said that multitenancy in  Lustre is easy if you follow a million steps, and that his company  was developing script-based ways to simplify this. While the idea of using  scripts to simplify operations is not bad, from a secure supply chain  standpoint, relying on third-party bash scripts to enable features required  for legal compliance is horrifying.  I don't mean to pick on Lustre alone here; other HPC technologies such as  InfiniBand, Slurm, and DAOS are facing the same reality: retrofitting modern  requirements like security and manageability into architectures that  prioritized performance and scalability over everything else are now going  through similar contortions to meet modern requirements around data  governance. For those HPC centers who do not have to worry about compliance  (which is most of open-science computing), these technologies will continue to  be just fine.  However, the  successes of these modern file systemsacross leading HPC centers  and the proliferation of alternative technologies such as  Kubernetes-based HPC and  MRC over Ethernet  tells me that HPC coming around to the idea that marginal increases in  performance are no longer worth missing out on factors that weigh heavily on  day-to-day operations like manageability, reliability, and flexibility.  Theme 2: HPC policy is becoming AI policy  Some of the biggest news at SC was not actually showcased at the conference  despite being what many people wanted to talk about in side conversations: HPC  policy is rapidly becoming AI policy, resulting in a slew of huge (but poorly  defined) \"public-private partnerships.\"  As a bit of background, the Oak Ridge Leadership Computing facility announced  its next system, Discovery, in late October--this was the result of a  \"typical\" supercomputer procurement process that  first came into the public eye in 2023. However, the Discovery announcement also included mention of a smaller  system, Lux, which will \"leverage the Oracle Cloud Infrastructure (OCI)\" (whatever that means) to provide earlier access to AMD MI355X GPUs ahead of  Discovery's full-scale deployment.  Then, two days later, Argonne National Laboratory announced a  similar arrangement with Oracle Cloud and NVIDIA  to deliver a small (Lux-sized) GPU supercomputer named Equinox, followed by a  much-larger 100,000-GPU supercomputer named Solstice. Neither Equinox nor  Solstice are attached to a \"typical\" supercomputer procurement; the follow-on  to Aurora, to be named  Helios, is  still in planning  and will be deployed in 2028. This strongly suggests that, whatever  \"public-private partnership\" means to the DOE, it is not the same as the  typical leadership computing systems; it is its own AI-centric program.  At SC itself, Evangelos Floros (EuroHPC's head of infrastructure) also  mentioned the \"need for public-private partnerships\" to realize EuroHPC's goal  of building \"AI Gigafactories\" with \"100,000 advanced AI processors\" across  Europe.\"Need for public-private partnerships\" to fund AI factories is recognized by EuroHPC too.Again, what exactly this \"public-private partnership\" model entails in Europe  was never really defined.  What was clear is that both American and European efforts are declaring the  need to build massive (100K+ GPU) supercomputers for AI, the traditional HPC  centers will be the public stewards of them, and \"public-private partnerships\"  are the only way to realize them since governments alone cannot foot the bill.  The Top500 BOF also included a short, awkward talk by Rick Stevens titled \"The  DOE AI Initiatives\" that amounted to Stevens saying he had nothing to say.  What really happened, I suspect, is that DOE's new \"Genesis Mission,\" which was announced the week after the SC conference, was a week  late and therefore couldn't be discussed as originally planned. If Stevens had  been able to describe the Genesis Mission, though, I'm sure he would've also  described \"public-private partnership\" as a key aspect, since the same  language is used in the  Executive Order that established Genesis. And I'm sure his description would've been no clearer about what this  really means than what EuroHPC or the OCI/DOE descriptions have stated.  Most revealing was my observation that, even outside of the proper conference  program, nobody really knew what any of this meant. I talked to plenty of my  colleagues from both government HPC and hyperscale cloud organizations, and  the only consistent message was that there aren't many concrete facts backing  up the the press releases right now. It appears that these partnerships were  brokered far outside the usual channels that large supercomputer procurements  are normally done, and the people in charge of actually delivering on the  promises of the press releases are still figuring out what is possible.  Connecting the dots between Lux/Equinox/Solstice, Genesis, and a recent  RFI  and  RFP  from DOE to allow  hyperscalers to build AI factories on federal land, it appears that what is happening is...    The DOE has a bunch of land that is adjacent to the National Labs that is    undeveloped but has the infrastructure to support massive AI factories.    Specifically named is a 110-acre parcel at Argonne that can accommodate up    to 1 GW \"AI data park,\" and a 100-acre parcel at Oak Ridge with up to 800    MW. These details were disclosed in    an RFI they issued earlier in the spring.      The    Solstice press release    specifically said that DOE envisions \"shared investments and shared    computing power between government and industry.\" Given the RFI/RFP were    about land leases, these public-private partnerships may involve splitting    the costs of space/power/cooling (the land and infrastructure being leased)    and the capital/operations (the supercomputer cloud services being built)    between the Labs and Oracle.    A potential model for operations is that cloud providers are allowed to build  and operate commercial AI cloud services adjacent to the DOE HPC facilities in  exchange for the DOE Genesis Mission being entitled to some of those AI cloud  capabilities. Exactly how much supercomputing resources hyperscalers like OCI  would give to DOE, and exactly how much it would cost the DOE Labs to serve as  landlords, is probably still undefined. But seeing as how power is the single  biggest limiter in AI these days, I expect this model will only spread costs  around, not actually lower them.  If this is indeed how Genesis plays out, this would establish a bizarre new  way for the government to acquire HPC (or AI) capabilities that completely  sidesteps the standard procurement model. Instead of plunking down a hundred  million dollars a year to finance a new leadership supercomputer, we might be  moving into a world where the Labs plunk down a hundred million dollars a year  to cover the costs of power, space, and cooling for a cloud provider. And  instead of owning a leadership supercomputer, these national HPC facilities  wind up consuming HPC (well, AI) resources from cloud providers--hopefully at  a cost that reflects the fact that the cloud providers are also profiting from  cycles being sold off of these machines to commercial AI customers.  But again, this is all speculation based on the consistencies I heard  throughout the conference and the experience I had trying to build these sorts  of partnership with the HPC community while I worked at Microsoft. I may be  right, or I may be wildly wrong. There are probably only a handful of people  in the world with a clear idea of what these partnerships are meant to look  like right now, and they are all way above the heads of the people at the HPC  centers who will be tasked with executing on the vision.  Selfishly, I am also left with a bit of heartburn over all of this news. I put  a lot of personal time and energy into giving the HPC community the  information it needed to feel comfortable about partnering with hyperscale AI  infrastructure providers while I was at Microsoft, and it often felt like a  Sisyphean task. Within months of me giving up and moving on from my career at  a cloud provider, seeing a complete reversal of policy from the leadership HPC  folks--and to see the \"other guy\" in pole position--is a bit of a slap in the  face.  I also couldn't help but notice that the cloud provider in all the headlines  in the US didn't seem to demonstrate a very strong and unified presence at SC  this year. Comically, they didn't even use their own brand's colors for their  booth on the exhibit floor. And the color scheme they did use left no room for  Oak Ridge's Lux system, which will be AMD-based, to be showcased.Oracle's booth at SC25. Their brand color is red, not green. Or so I thought.Though I may have read too much into this, it feels like these public-private  partnerships are not necessarily composed of equal partners with equal levels  of commitment.  More broadly, I left the conference concerned that the discourse happening  around these cloud-HPC/AI integrations--at least in the US--appears to have  regressed compared to where it was when I worked at Microsoft. Many of the  things we had to figure out years ago (cybersecurity models, impacts on jobs  at the HPC centers) seem to have reset to zero. And sidestepping the  procurement processes for leadership computing to enable these public-private  partnerships will either require significant new funding (of which Genesis  provides none; the executive order as-written appears to recolor existing  money) or robbing Peter (the budget funding the next generation of leadership  HPCs) to pay Paul (the cloud providers serving up compute resources for AI).  As a result, I can envision a future where all of the money that used to fund  leadership computing for science becomes money to fund commercial AI  factories, resulting in a slow evaporation of the LCFs as their HPC  capabilities shrink in size and relevance.  Though there's lots more to be said on this topic, it's all based on  conjecture. So, maybe the best thing to do is quietly wait and see.  Theme 3: AI discourse is growing up  This was the first SC where it felt like the discourse around AI's role in the  future of scientific computing actually carried some substance. Whereas  previous years saw talk that mostly revolved around basic ideas like \"do LLMs  hallucinate too much?\" or \"can ChatGPT write MPI code?,\" I sat in on a number  of interesting talks and conversations that skipped the question of \"is AI  useful?\" and went straight to \"this is how AI is proving useful to us.\"  Maybe it's related to the previous theme: HPC money is becoming AI money, so  AI research is becoming required to stay afloat. Or maybe it's because 2025  has been the year of agentic AI, and agents allow LLMs to be integrated much  more surgically into complex workflows. Or maybe confirmation bias led me to  sit in sessions and talk with people who are at the frontier of applying AI to  scientific discovery. Whatever the case may be, I was glad to hear so much  discussion from researchers around the importance of all the connective tissue  required to operationalize AI in scientific computing.Agentic workflows  A great example of this was the  1st International Symposium on Artificial Intelligence and Extreme-Scale    Workflows, which happened on Friday. One of the invited speakers, Dr. Katrin Heitmann,  connected a lot of dots in my head with a talk she gave on how massive-scale,  physics-based simulation workflows can benefit from agentic AI.Heitmann's vision on how agentic approaches can augment (but not replace) humans in complex scientific workflows.The crux of the challenge faced by most massive-scale simulation (like  HACC, the cosmology code  for which she is famous) is that they generate massive amounts of data. The  most recent HACC run  generated hundreds of terabytes of compressed data per checkpoint and over a  hundred petabytes of data in the end; this cosmological simulation serves as a  reference dataset from which downstream cosmological research can draw when  exploring targeted questions. The challenge, of course, is finding relevant  pieces of the simulated universe from amidst a hundred petabytes of raw data.  Dr. Heitmann's premise is that agents and tools have very specific scopes and  capabilities, and researchers have control over which of these tools they wish  to use. However, they can hand off these tools to an agentic workflow to let  it autonomously sift through all of the data, looking for specific features  within the simulated universe that are relevant. A specific example she gave  was the process of examining 500 million galaxy clusters; with an agentic,  AI-driven approach, a postdoc was able to interactively sift through these  objects without examining each one individually. For truly interesting  objects, a separate agent could go search the literature and provide an  explanation as to why it may be interesting, absolving the postdoc from having  to make round trips between the dataset and external literature.  That all said, it was clear from this talk (and others) that integrating  agentic AI into scientific inquiry is still in its early days. But what I  appreciated about this talk (and the entire workshop) is that it sidestepped  pedestrian questions about trustworthiness by acknowledging that the goal  isn't full autonomy, but rather, enabling researchers to do things faster.  There is still a human at the start and the end of the workflow just as there  always has been, but agents can reduce the number of times a human must be in  the loop.  Data and agent-centric service infrastructure  Even when AI wasn't the main topic of discussion, it was clear to me at this  SC that AI is influencing the way researchers are thinking about the  infrastructure surrounding supercomputers. A great example of this was the  keynote at the PDSW workshop,  given by the ever-insightful  Dr. Rob Ross, where he offered a retrospective on the work his team has    done over the last two decades, what he felt they got right, what they missed, and what's ahead.  Towards the end of his presentation, he made the case that \"science is  increasingly multi-modal.\" But rather than talk about multimodality in the AI  sense, he was emphasizing that there's more to scientific computing than  performance:Domain science, provenance, search, and resilience are equal partners to performance in scientific computing.Taken at face value, this slide positions performance on equal footingwith domain science, provenance, findability, and his argument was that we’vemoved beyond the world where the only storage problem that HPC faces ischeckpointing. Just as Dr. Heitmann would say on Friday, Dr. Ross’ argument wasthat the increasing volume of scientific data coming out of both exascalesimulation and scientific instruments is driving the field towards moreautomation. And with automation comes a greater need to understand dataprovenance–after all, if automation produces a surprising result, a humanultimately has to go back and understand exactly how the automation generatedthat result.  He also point out that in this coming world of automation-by-necessity,  infrastructure itself might have to be rethought. After all, traditional  technologies like parallel file systems were designed to make the lives of  human researchers easier; when the primary consumer of data becomes AI agents,  not humans, there may be better ways to organize and expose data than through  files and directories. A human might repeatedly cd and ls to find a specific  dataset on a file system, whereas an agent use a query a flat index to find  the same data in a single step.  At the end of the same PDSW workshop, I was fortunate enough to contribute to  a panel  where many of these same themes--how will data systems change as AI plays a  greater role in scientific discovery--were discussed. Although we touched on a  lot of topics, what stuck with me was a general acknowledgment that, while HPC  has always talked about data management and provenance as being important,  they were always treated as a \"nice to have\" rather than a \"must have.\"  However, as was echoed across many presentations (including the two I  described above), governance and provenance are now becoming non-negotiable as  larger datasets drive us towards AI-driven automation.  Regardless of what you think about AI's ability to accelerate scientific  discovery, I left SC with the feeling that AI is forcing the HPC community to  grow up with regards to how seriously it takes data management:    The size and velocity of datasets generated by simulation or experiment is    growing beyond any single person's ability to analyze it by hand. The    complexity of these data are also making it harder to develop    herustics-based or analytical approaches to combing through all of it.      The best path forward to understanding these data is through AI (via    purpose-built models for analysis) or AI-driven data exploration (via    autonomous, agentic workflows).      Automation or autonomous workflows will always act under authority delegated    to them by human researchers, meaning there is a growing need to be able to    inspect how these workflows arrived at the conclusions they generate.      Understanding how an answer was achieved requires significantly better data    management features such as governance, provenance, and auditability. A    result is ultimately only useful if a human can trust it, and that trust    comes from understanding which data informed that conclusion, how that data    was created, and how it was modified over time.    Put differently, checkpointing was the main concern of I/O research because  I/O performance was the first scalability issue that scientific computing ran  into as supercomputers and scientific instruments got bigger. However, we're  now at a point where issues ancillary to performance have reached the limits  of scalability. Dr. Ross's multi-modal slide indicate that provenance,  indices/search, and resilience are some examples of these new barriers, but  there are plenty more as well.  In a sense, this theme is the opposite side of the same coin as the first  theme I discussed--that the big number is losing its shine. The hardest  questions going forward aren't the obvious ones about scaling performance;  they are about scaling everything else to keep up. AI seems to be the  technology that has cleared a path to these data management hurdles, but the  benefits of adopting strong data management practices and systems will extend  far beyond the reach of just enabling AI-based automation.The exhibit hall  The exhibit hall has long been one of my favorite parts of attending SC  because it's a great way to get a feeling for what technologies and vendors  are hot, where the innovation is trending, and what sorts of commercial  problems are worth solving. Every year I feel like I have less and less time  to walk the exhibit hall though, and the layout and composition of this year's  exhibition meant I only saw a small fraction of what I wanted to see in the  few days it was up.  The most common comment I heard about the exhibit this year is captured in  Doug Eadline's article,  SC25 Observations: More Pumps than Processors  (which is well worth the read!). The same commentary was repeated throughout  the OCP conference in October as well, suggesting that there is a lot of money  to be made (or at least the prospect of money) in helping datacenters get  outfitted for the liquid cooling demanded by the next generation of  large-scale GPU infrastructure. However, I found the overwhelming amount of  space devoted to liquid cooling companies acutely problematic at SC25 this  year for two reasons:Most SC attendees have nothing to do with liquid cooling. A    colleague of mine who operates supercomputers for the energy sector asked    one of these big liquid cooling vendors what he could do to actually engage    with them. After all, he doesn't buy liquid cooling infrastructure; he buys    whole supercomputers that come with heat exchangers and CDUs that are    integrated into the solution. The vendor had no good answer, because the    reality is that the typical supercomputer user or buyer has no say over what    piping, coolant, or exchangers are used inside the machine itself. The whole    point of buying an integrated supercomputer is to not have to deal with that    level of details.  These liquid cooling vendors soaked up a ton of floor space. A few of these physical infrastructure providers had massive (50x50)    booths sprinkled across the exhibit hall. Combined with the fact that the    average SC attendee has nothing to do with liquid cooling meant that the    booths that were more likely to be relevant to a typical attendee were much    further apart than they had to be.    The end result was that the exhibit hall was absolutely gargantuan and yet  information-sparse. In fact, this year saw a secondary exhibit hall in the old  football stadium serve as overflow space, because the entire primary exhibit  hall was full. What's worse is that this overflow space was (as best as I  could tell) completely disconnected from the main hall, and the only time I  ever saw it was from the dining area used to serve lunch for the tutorials.The exhibit hall's overflow space being set up in the former football stadium.I would’ve been furious if if I had been stuck with a booth in thisoverflow space, because I can’t imagine the foot traffic in there was very high.I personally couldn’t even find the entrance to this second exhibition area inthe few hours I had to look for it.  I can't help but think the SC organizers leaned far too much into booking up  as much space (and therefore exhibitor dollars) as possible without thinking  about the dilutive effects of having such a massive vendor count. Some vendors  definitely benefitted from having a good location near one of the hall  entrances, but I also heard a nontrivial amount of grumbling around how little  traffic there was at some of the big booths. It wouldn't surprise me if there  was a contraction of the HPC mainstays at SC26.By the numbers  Rather than rely solely on anecdotes though, it's also fun to take a  quantitative look at the changes in exhibitors relative to last year. Since I  spent the time figuring out how to generate tree maps for my SC24 recap last  year, I figured I should re-run the same analysis to compare SC25 to SC24.  Of the biggest booths who were exhibiting for the first time this year, it  should be no surprise that the two biggest new entrants were Danfoss (liquid  cooling infrastructure) and Mitsubishi Heavy Industries (gas turbines and  other large-scale infrastructure): New exhibitors with the largest booths.Of the other top new exhibitors, some (Solidigm, Sandisk, C-DAC, MinIO, and  University of Missouri Quantum Innovation Center) were quite relevant to the  typical SC attendee. Arm was also back after having skipped SC24. But there  were scores of new exhibitors whose services and products seem much more  relevant to very niche aspects of physical datacenter infrastructure.  Of the exhibitors who didn't show up to SC25 but had big booths at SC24, there  was a diverse mix of markets:Vendors who didn't show up to SC'25 but had big booths at SC'24.Sadly, higher ed and government popped up on this list (see  Doug Eadline's take on this for more). A bunch of datacenter infrastructure providers also vanished, including  Valvoline and Boundary Electric; this suggests that some of the top new  vendors of this year (Danfoss, Mitsubishi) may similarly vanish entirely next  year after realizing that SC isn't really their crowd. But I was also  surprised to see some big names in AI vanish; Iris Energy (IREN) is a GPU  cloud provider that just inked a multi-billion dollar deal with Microsoft;  Ingrasys manufactures much of the world's GB200 NVL72 infrastructure; Groq,  Sambanova, and SambaNova also inexplicably vanished.  Perhaps more interesting are the top growers; these vendors exhibited both  last year and this year, but went significantly larger on their booth sizes:Biggest increases in booth size at SC'25 vs. SC'24.Legrand, which provides datacenter infrastructure bits, likely grew as a  result of it acquiring USystems and merging USystems' booth with Legrand's  booth this year. The other big booth expansions are mostly household names  though; Gates, EBARA, and GRC are cooling vendors that the typical SC attendee  can't do much with, but the others are organizations with whom a researcher or  HPC datacenter operator might actually talk to.  Finally, the top contractions in booth space are a mix of service providers,  HPC facilities or research centers, and component suppliers:Biggest decreases in booth size at SC'25 vs. SC'24.Of the biggest vendors who downsized, Carahsoft is a component reseller and  service provider, Stulz is a liquid cooling company, HLRS is a German  supercomputer center, and Viridien is an HPC services company that primarily  serves the energy sector. It is surprising to see AWS shrink while Microsoft  grew, and it is doubly surprising to see Oracle shrink when it's at the center  of the biggest HPC deployment news of the season. Given that these booth sizes  are chosen a year in advance, this may speak to how unexpected the turn of  events were that resulted in Oracle carrying the cloud services end of DOE's  big public-private partnerships.Interesting new technology  For reasons I'll discuss later, I didn't have much time to walk the exhibit  hall floor. Combined with the fact that everything was so spread out and  diffuse, I just didn't get a great sense of what interesting new technology  was being introduced this year beyond what tended to stick out. And amidst all  the giant CDUs and liquid cooling infrastructure, it was hard for anything to  stick out except really big compute cabinets.Dell IR700  Dell's booth had a fully loaded IR7000 rack on display (as they did at  GTC earlier in the year) with 36 GB200 NVL4 sleds. At 50OU high (almost eight feet tall), this thing  is physically huge:Dell's 50OU IR7000 rack, fully loaded. This is what TACC Horizon will be built from.Unlike the version they had on display at GTC though, this one had both the  front door and a full rear-door heat exchanger installed:HUGE rear-door heat exchanger on the back of the Dell IR7000 rack.What's notable about this platform is that we now know that it is the basis  for both  TACC's upcoming Horizon system  (which will have  28 of these fully loaded racks) and  NERSC's upcoming Doudna system  (which will have Vera Rubin rather than Blackwell). This rack was nominally  designed for hyperscale AI and is the basis for Dell's GB200 NVL72 (XE9712)  deployments at places like CoreWeave and xAI, which means that it'll be  thoroughly tested at scale long before TACC or NERSC have it up and running.  This is the opposite of what has historically happened: before AI, it was  usually government HPC that had to debug new rack-scale architectures before  industry would touch it.HPE Cray GX5000  However, government HPC will still have a chance to debug a new supercomputing  platform in the recently announced  Cray GX (formally  called \"the HPE Cray Supercomputing GX platform\"), which is the successor to  the current Cray EX platform. This is the platform that the  Discovery supercomputer  at OLCF will use, and HPE had a CPU-only blade (Cray GX250) and a rack mockup on display at SC:HPE's new GX blade form factor. This one appears to be the GX250, the 8-socket CPU-only blade.It's hard to tell the size of this blade from the photo, but if you look at  the relative size of the CPU socket and the DIMM slots, you can get a sense of  how physically massive it is--it's like a coffee table. It also isn't  perfectly rectangular; Cray decided to put this unusual protrusion on the  front of the blades which is where the four NICs and eight E1.S SSDs are  housed:A look at the side of the Cray GX blade's \"nose\" showing the side-mounted NIC ports.This nose(?) adds more surface area to the front of the rack, and it makes  more sense when you see a rack full of these nodes. HPE had a full GX5000 rack  with mocked-up cardboard nodes in their booth as well:Fully loaded GX5000 rack. The nodes were cardboard, but pretty nice cardboard.By having the NIC ports (which are Slingshot 400) face the sides of the rack  rather than stick out the front, the bend radius of all that copper doesn't  have to be quite as dramatic to route it along the sides of these node noses.  And unlike previous Cray designs, there's also no midplane or backplane that  connect the nodes in a rack to the rack-local switches; everything connects  through discrete copper or optical cables.  At the center of the rack is a liquid-cooled switch chassis, and each rack can  support either 8-, 16-, or 32-switch configurations. Each switch is a 64-port  Slingshot 400 switch, and I think the premise is that a single GX5000 rack is  always exactly one dragonfly group. If you want a smaller group, you use a  switch chassis with fewer switches.  Interestingly, this GX will also support non-Slingshot Ethernet and XDR  InfiniBand switches. Given that both XDR InfiniBand and 800G Ethernet are  shipping today and have twice the bandwidth that Slingshot 400 will have when  it starts shipping in a year, perhaps the Slingshot 400 option is just a  stopgap until HPE's investments in Ultra Ethernet result in a product. The  lack of a network backplane in the rack also makes it easier for the rack to  accommodate the non-dragonfly topologies that would be required for InfiniBand  or Ethernet.  The rear of the rack is remarkably unremarkable in that it simply contains a  rear bus bar and the liquid cooling manifolds and mates. In this sense, the  rack looks very  OCP-like; the boring stuff is in the back, everything exciting is serviced from the  front, and the rack itself is passive plumbing. Like any OCP ORv3 rack, power  shelves slot in just as server blades do, and they use the same liquid cooling  infrastructure as the rest of the rack. They power the bus bar, and the blades  and switches draw from the same bus bar.  Compared to an ORv3 rack though, these GX racks are wider and shorter. The  width probably offers more flexibility for future NVIDIA or AMD GPU boards,  but I was surprised that Cray didn't go ultra tall like Dell's 50OU IR7000. I  was also surprised to hear that Cray is launching GX with a 400 kW cabinet  design; power appears to already be a limiting factor in the nodes launching  with GX. A single 400 kW GX rack can support40 CPU-only blades (81,920 cores of Venice)28 AMD Venice+MI430X blades (112 GPUs)24 NVIDIA Vera+Rubin blades (192 GPUs)  For reference, the demo GX5000 rack pictured above had only 29 blades and 16  switches. I assume that fitting 40 blades into the rack requires using the  smallest dragonfly group possible.  On the cooling front, the GX5000 rack will launch with support for the same  1.6 MW CDUs as the current Cray EX platform. I heard talk of a neat sidecar  CDU option as well, but the person with whom I spoke at the HPE booth said  that would come a little later.  Overall, I was surprised by how un-exotic the new Cray GX platform is compared  to what the AI world has been doing with ORv3 racks. The fact that Cray and  Dell's designs are more similar than different suggests that the HPC/AI world  is converging on a place where the future is uncertain, and flexibility is  more important that highly engineered racks that optimize for very specific  nodes and networks. It also suggests that the real value of buying Cray is  higher up the stack; liquid cooling, power delivery, and rack integration is  becoming commoditized thanks to AI.  I was also surprised that Cray's next-generation design is not obviously  superior to what the hyperscale community is designing. Whereas the GX rack  caps out at 400 kW, Dell's will allegedly scale up to 480 kW. That said,  today's IR7000 racks shipping for Horizon are only 215 kW (for GPU racks) and  100 kW (for CPU-only racks) according to a talk given by Dan Stanzione:The physical configuration of TACC's upcoming Horizon supercomputer.So until the final specifications for the Rubin GPU are released, I suspect we  won't know whether Cray still leads the pack in terms of compute density, or  if Dell made the better bet by aligning its supercomputing platform on a  standard OCP rack design.",
            "content_html": "<p>  The annual SC conference was held last week, drawing over  <a href=\"https://www.hpcwire.com/2025/11/19/sc25-observations-more-pumps-than-processors/\">16,000 registrants and 560 exhibitors</a>  to in St. Louis, Missouri to talk about high-performance computing, artificial  intelligence, infrastructure, and science. It was my tenth time attending  in-person (12th overall), and as is always the case, it was a great week to  reconnect with colleagues, hear what people are worrying about, and get a  finger on the pulse of the now-rapidly changing HPC industry.</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure><figcaption class=\"image-caption\">Outside the SC'25 convention center on the only clear day of the week.</figcaption></figure></div><p>Although every SC I've attended always felt a little different from the  previous year, this one felt quite different. Part of that results from my own  personal circumstances: this is the first year I attended as an employee of  VAST Data, and so the people with whom I met and the technical problems to  which I paid attention were certainly biased towards those most relevant to my  work. But the backdrop of the whole conference has also shifted. It's been  three SC conferences since ChatGPT came out, and it's now undeniable that AI  isn't simply on the horizon; it's shaping the field of HPC and scientific  computing. What used to be an argument of \"<a href=\"https://blog.glennklockwood.com/2024/05/isc24-recap.html#section11\">us vs. them</a>\" is now more like \"them (and us?)\"<span></span></p><p></p><p>  As has become tradition, I'm sharing some of my thoughts from the week with  the world in the hopes that someone finds this interesting and insightful.  I've roughly organized them into two areas big themes and the exhibition hall.</p><ul style=\"text-align: left;\"><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#big-themes\">Big themes</a><ul><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#theme-1-the-big-number-is-losing-its-shine\">Theme 1: The big number is losing its shine</a><ul><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#top500\">Top500</a></li><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#the-gordon-bell-prize\">The Gordon Bell Prize</a></li><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#fixing-problems-caused-by-the-big-number\">Fixing problems caused by the big number</a></li></ul></li><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#theme-2-hpc-policy-is-becoming-ai-policy\">Theme 2: HPC policy is becoming AI policy</a></li><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#theme-3-ai-discourse-is-growing-up\">Theme 3: AI discourse is growing up</a><ul><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#agentic-workflows\">Agentic workflows</a></li><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#data-and-agentcentric-service-infrastructure\">Data and agent-centric service infrastructure</a></li></ul></li></ul></li><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#the-exhibit-hall\">The exhibit hall</a><ul><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#by-the-numbers\">By the numbers</a></li><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#interesting-new-technology\">Interesting new technology</a><ul><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#dell-ir700\">Dell IR700</a></li><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#hpe-cray-gx5000\">HPE Cray GX5000</a></li></ul></li></ul></li></ul><h2 id=\"big-themes\">Big themes</h2><p>  HPC has always been at the center of a tension between keeping things the same  (supercomputers are the most stable the day they are turned off) and pushing  the technological envelope (which is the fastest way to unlock new discovery).  The desire to push the envelope has always been a \"pull\" towards the future;  researchers first led with kooky ideas (like DAOS and Kokkos), and as those  ideas turn from research into production, they make new technologies (like  all-flash and AMD GPUs) accessible to scientists.</p><p>  What hasn't historically happened, though, is a strong \"push\" towards the  future. Scientific HPC centers push themselves to justify building the next  big supercomputer, but it's been a given that there will always be another big  machine, so this push has been internal and gentle. Combined with the  not-so-urgent pull of HPC researchers, every center has gotten a new machine  every five years or so.</p><p>  This is the year where it became clear to me that AI is now exerting a strong  push on the HPC industry--a shove even, forcing HPC centers around the world  to align themselves on an AI mission if they want to survive. All the  big-money HPC systems being announced this year are clearly being positioned  as AI-first and AI-motivated, and these announcements are going well beyond  simply peppering \"AI\" throughout the press release and otherwise acting as if  it was business-as-usual. This is the first SC where I saw scientists,  architects, and decision-makers being being forced to confront real tradeoffs  favor either HPC or AI, and they are beginning to choose AI.</p><p>  This push-and-pull on HPC towards the future manifested in three big themes.</p><h3 id=\"theme-1-the-big-number-is-losing-its-shine\">  Theme 1: The big number is losing its shine</h3><p>  HPC has long organized itself around treating the big machine and the big  number as its top priority, and this is why the two largest HPC conferences of  the year honor the semiannual release of the Top500 list on their main stage.  However, this year felt like the first time that one number (that somehow  reflects \"performance\") dominated the conversation. Instead, the discourse was  more diffuse and discussed \"performance and x\" or \"the supercomputer and x.\"</p><h4 id=\"top500\">Top500</h4><p>  The place where this was most evident to me was at the  <a href=\"https://sc25.conference-program.com/presentation/?id=bof117&amp;sess=sess409\">Top500 BOF</a>, where the latest list was unveiled.</p><p>  The biggest announcement was that Europe now has its first benchmark-confirmed  exascale system in JUPITER, which ran a full-system HPL at  <a href=\"https://mastodon.social/@andih/115566907716591104\">1,000,184 TFLOPS</a>  for two hours and seven minutes. However, JUPITER didn't get any stage time at  the BOF since, like Aurora, it actually debuted on a previous list with a  sub-exascale run. This run pushed it over the finish exascale finish line, but  if the Top500 list metadata is to be believed, the run used 100% of JUPITER's  5,884 nodes to break the barrier--a feat that is unlikely to be reproduced on  any production applications, since it is rare to have zero failed nodes in any  large-scale production environment.</p><p>  So, while there was little fanfare for Europe in breaking the exaflops barrier  with its new big machine and big number, there were some big  announcements--one overt, and others more muted.</p><p>  The biggest news was that <strong>the Top500 list is changing hands</strong>.  Whereas it has historically been controlled by three people--Jack Dongarra,  Horst Simon, and Erich Strohmaier--it will be transitioning to be  community-controlled under the stewardship of ACM SIGHPC. Dongarra, Simon, and  Strohmaier will still be on the steering committee under the ACM stewardship,  but this new governance structure opens the doors for new ideas to breathe new  life into the way systems are ranked and, more broadly, how \"performance\" is  meant to be interpreted from Rmax.</p><p>  At present, the list (and related lists) are bound by rules that, in the  present day of reduced-precision accelerators, make little sense. For example,  using the Ozaki scheme within the LU decomposition is not allowed by Top500  despite the fact that it can produce the same answer with the same numerical  accuracy much faster than hardware FP64. And while the HPL-MxP benchmark does  allow solving the same problem using more creative methods, Strohmaier  highlighted a problem there too: it never dictated how to deal with multiple  levels of mixed precision until AIST broke the rankings. AIST ran HPL-MxP at  both 16-bit and 8-bit precisions, resulting in their ABCI 3.0 system  simultaneously ranking at #6 and #10.</p><p>  These sorts of issues make it easy to question the value of leaderboards like  Top500 or HPL-MxP, as their definition of \"performance\" becomes increasingly  further divorced from how large supercomputers are really used. The past few  years have shown that there hasn't been the time or energy to get ahead of  these ambiguities amongst the three men maintaining the list, so transitioning  it to ACM will hopefully be a positive move that will give the list a chance  to be revitalized.</p><p>  To their credit,  <strong>the incipient stagnation of the Top500 list</strong> was called out by  Strohmaier during his analysis of the list, acknowledging that \"growth has  tremendously slowed down compared to what it used to be\" and \"we don't have  proof of what is actually the reason for that:\"</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure><figcaption class=\"image-caption\">All the key highlights of this SC's Top500 list.</figcaption></figure></div><p>China has stopped submitting, the AI and hyperscale providers really never  started submitting, and retired systems are being thrown off the list long  before they fall off the bottom. To me, this was a tacit acknowledgment that  the list does not have a bright future out to 2030 unless it is modernized to  be relevant to the way in which today's largest systems are actually being  used--which is not DGEMM.</p><p>  The final surprising acknowledgment during Strohmaier's talk was that  <strong>the list is trailing the state of the art in hardware</strong> by  quite a bit. He pointed out that Blackwell systems are only now starting to  appear even though they've been shipping in volume for the better part of a  year. While he hypothesized that there is \"uneasiness\" about Blackwell in an  HPC context, the reality is that there are no Blackwells for HPC until the  Blackwell orders for hyperscale AI have been fulfilled. HPC is second in line,  and even then, the only Blackwells I could find on this year's Top500 list  were NVL8 configurations--not the NVL72 configurations that have been filling  up hyperscale datacenters like  <a href=\"https://glennklockwood.com/garden/systems/Fairwater\">Fairwater</a>.</p><p>  Strohmaier pointed out that Blackwell, by virtue of its HBM3e (vs. Hopper's  HBM3), is showing up higher on the HPCG list (which is a memory bandwidth  test) than on Top500 (which is an FP64 FLOPS test). He phrased this as  evidence that \"not everything is bad for the HPC community,\" but I would have  phrased my conclusion a little differently:</p><ol type=\"1\"><li>    Blackwell is actually great for HPC, because most real workloads are    memory-bandwidth bound, not FLOPS bound. The fact that B200 offers similar    FP64 FLOPS at higher memory bandwidth means that real applications will get    higher effective use of those FP64 FLOPS.  </li><li>    Despite the above, Blackwell doesn't perform well on Top500 because HPL    doesn't reflect the reality that memory bandwidth is important. It follows    that HPL doesn't reflect the reality of real HPC applications. A Blackwell    system can be significantly better for real HPC applications than a    comparably sized Hopper system even though it may rank lower than Hopper on    Top500.  </li><li>    Blackwell isn't showing up in volume now because the HPC community is second    in line. The HPC community isn't uneasy as much as it is completely locked    out. The first NVIDIA-based exascale system debuted in November 2025 despite    its GPU being three years old, suggesting that if big Blackwell systems ever    appear on Top500, it'll happen in 2026-2027.  </li></ol><p>  All of this is a roundabout way of showing that the big number--in this case,  the HPL score--no longer leads meaningful conversation around how useful a  system is for science.</p><h4 id=\"the-gordon-bell-prize\">The Gordon Bell Prize</h4><p>  Another major indicator of the changing tide away from the big number was the  work that won this year's  <a href=\"https://awards.acm.org/bell\">Gordon Bell Prize</a>. The winning  paper, titled \"<a href=\"https://arxiv.org/html/2504.16344v2\">Real-time Bayesian inference at extreme scale: A digital twin for tsunami    early warning applied to the Cascadia subduction zone</a>,\" wasn't the typical case of running a huge simulation for a few hours and  reporting some result. Rather, it described a four-step workflow that  culminates in the desired insight popping out of a computation that runs  across only 128 nodes and completes in less than 0.2 seconds. Furthermore, the  hero run part could be decomposed into trivially parallel components, allowing  the bulk of the computation to be geographically distributed across HPC  centers or GPUs spread across on-prem and cloud providers.</p><p>  My understanding of the work is that there was a massive \"offline\" computation  to precompute a few key matrices (Phase 1) followed by two shorter offline  steps that turn those matrices into the core of the digital twin. The last  step, which was \"online\" and designed to be computed in real-time, could then  take this core and solve the input problem with extremely low latency. This  workflow front-loads a hero run in such a way that, if an earthquake were to  occur, the risk of tsunami could be calculated in less than a second using  only modest compute resources and the precomputed core.</p><p>  The authors eschewed methods that generated tons of FLOPS in favor of methods  that were less FLOPS-efficient but got to the answer faster. In the authors'  own words:</p><blockquote><p>    As shown in Fig. <a href=\"https://arxiv.org/html/2504.16344v2#S7.F7\">7</a>, higher FLOP/s does not necessarily lead to faster time-to-solution. On    MI300A nodes of <em>El Capitan</em>, the best-performing    implementation, Fused PA, achieves a lower percentage (5.2%) of theoretical    peak FLOP/s than Fused MF (5.5%) but is faster.  </p></blockquote><p>  Interestingly, the hero computation here was embarrassingly parallel(ish) as  well; in their demonstration run, the hero run (Phase 1) was broken into 621  independent calculations each requiring 128 nodes (512 A100 GPUs) for about an  hour. Because they are independent, these tasks could be parallelized across  multiple HPC centers as well, and my understanding of the data volumes  involved are modest; Phase 1 would require a single shared copy of the input  mesh (a hundred GiB?) per HPC center, and each of the 621 tasks would output  around 8 GiB which would have to be copied back.</p><p>  While I don't understand the mathematics behind this work, the paper took what  would've been a huge exascale-class mathematical problem (\"10 years on a  sustained 1 EFLOP/s machine\") and reformulated it into a workflow that solves  the problem faster and more usefully. Instead of brute-forcing the problem  with a big supercomputer, they split it into separate offline and online  parts, and this naturally allowed the most computationally expensive part to  be geographically distributable.</p><p>  This work surrendered the need for a single big machine, and it didn't produce  a big-number result. But it did win the Gordon Bell Prize, again signaling  that the HPC community is beginning to look beyond performance-only and think  about awarding innovation according to outcomes, not just FLOPS.</p><p>  The talk for this paper can be viewed  <a href=\"https://sc25.conference-program.com/presentation/?id=gb106&amp;sess=sess577\">here in the SC25 Digital Experience</a>.</p><h4 id=\"fixing-problems-caused-by-the-big-number\">  Fixing problems caused by the big number</h4><p>  Most of my perception around the HPC community beginning to de-emphasize the  singular big machine or big number arose from organic interactions I had with  colleagues and customers though. It's hard to summarize how these  conversations went, but the  <a href=\"https://sc25.conference-program.com/presentation/?id=bof197&amp;sess=sess439\">Lustre Community BoF</a>  is a good example of what I saw elsewhere.</p><p>  Lustre has long been the gold standard in high-performance parallel I/O in the  HPC community because it was designed from day one to deliver high bandwidth  above all else. As a result, Lustre already has the big number solved in many  ways, and events like the Lustre BOF are a great case study in what it looks  like for a performance-first technology to be pushed into adapting to deliver  more than just a big number.</p><p>  First, the ever-innovative Stéphane Thiell from Stanford discussed the process  and tooling he developed to enable online capacity expansion of a Lustre file  system. The basis for it was a distributed, fault-tolerant tool he developed  that uses redis, lfs find, and lfs migrate to manage the state of file  migrations across Lustre servers as the file system is rebalanced. While a  part of me thought this was a great tool that would be super helpful for many  others, another part of me was kind of horrified.</p><p>  Maybe I've been spoiled by working in hyperscale and AI these past three  years, but online capacity expansion and rebalancing is a built-in capability  of all distributed storage systems these days. All the major cloud object  stores do this, as do all modern parallel file systems including Quobyte,  VAST, and WEKA. Of course, none of these modern systems are as efficient (on a  per-CPU core or per-SSD basis) as Lustre at delivering peak performance. But  Stéphane's talk made me realize the price that's paid for this great  performance.</p><p>  Andreas Dilger and others went on to talk about Lustre futures, and as they  were speaking, I noticed that nobody was talking about performance  improvements to Lustre. Rather, feature development was focused on catching up  in every other dimension--data governance, reliability, manageability, and  others. For example, Andreas talked a bit about the upcoming \"multi-tenancy\"  features coming to Lustre:</p><p></p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure><figcaption class=\"image-caption\">It's a lot of work to retrofit multitenancy into a performance-first file system.</figcaption></figure></div><p>I put “multi-tenancy” in quotes because these changes really representtrying to back into a security posture that is fundamentally different from theone that Lustre was designed around. In the pursuit of performance, Lustre (aswith most other HPC technologies) was designed assuming that security wassomeone else’s problem. By the time someone could log into a system that couldmount a Lustre file system, they had already been authenticated, and it was upto the OS on each compute node to authorize any interactions with Lustre itself.This is the “implicit trust” model.</p><p></p><p>  The problem, of course, is that the rest of the world has adopted a \"zero  trust\" model which makes many things (except performance!) generally easier.  Compliance is easier when the system assumes that everything is encrypted as a  default and key management can be delegated to a third party. Because Lustre  didn't do this from the outset, it is going through this process of  retrofitting encryption in various places and using a mixture of nodemaps,  UID/GID maps, and shared secrets to patch over all the places where trust was  fundamentally implicit.</p><p>  Later on in the BOF, panelists acknowledged (some half-heartedly) that  manageability of Lustre was a barrier. One panelist admitted that it took five  years of work to almost get to the point where a Lustre update can be done  without crashing applications. Another panelist said that multitenancy in  Lustre is easy <em>if you follow a million steps</em>, and that his company  was developing script-based ways to simplify this. While the idea of using  scripts to simplify operations is not bad, from a secure supply chain  standpoint, relying on third-party bash scripts to enable features required  for legal compliance is horrifying.</p><p>  I don't mean to pick on Lustre alone here; other HPC technologies such as  InfiniBand, Slurm, and DAOS are facing the same reality: retrofitting modern  requirements like security and manageability into architectures that  prioritized performance and scalability over everything else are now going  through similar contortions to meet modern requirements around data  governance. For those HPC centers who do not have to worry about compliance  (which is most of open-science computing), these technologies will continue to  be just fine.</p><p>  However, the  <a href=\"https://blocksandfiles.com/2025/11/18/vast-data-dell-versity-and-spectra-logic-are-shining-storage-stars-on-taccs-horizon/\">successes of these modern file systems</a><a href=\"https://blocksandfiles.com/2025/07/04/vast-doudna-supercomputer-storage/\">across leading HPC centers</a>  and the proliferation of alternative technologies such as  <a href=\"https://nrp.ai\">Kubernetes-based HPC</a> and  <a href=\"https://blogs.microsoft.com/blog/2025/11/12/infinite-scale-the-architecture-behind-the-azure-ai-superfactory/?utm_source=chatgpt.com\">MRC over Ethernet</a>  tells me that HPC coming around to the idea that marginal increases in  performance are no longer worth missing out on factors that weigh heavily on  day-to-day operations like manageability, reliability, and flexibility.</p><h3 id=\"theme-2-hpc-policy-is-becoming-ai-policy\">  Theme 2: HPC policy is becoming AI policy</h3><p>  Some of the biggest news at SC was not actually showcased at the conference  despite being what many people wanted to talk about in side conversations: HPC  policy is rapidly becoming AI policy, resulting in a slew of huge (but poorly  defined) \"public-private partnerships.\"</p><p>  As a bit of background, the Oak Ridge Leadership Computing facility announced  its next system, Discovery, in late October--this was the result of a  \"typical\" supercomputer procurement process that  <a href=\"https://www.nextplatform.com/2023/10/02/the-first-peeks-at-the-doe-post-exascale-supercomputers/\">first came into the public eye in 2023</a>. However, the Discovery announcement also included mention of a smaller  system, Lux, which will \"<a href=\"https://www.olcf.ornl.gov/2025/10/27/ornl-amd-and-hpe-to-deliver-does-newest-ai-supercomputers-discovery-and-lux/\">leverage the Oracle Cloud Infrastructure (OCI)</a>\" (whatever that means) to provide earlier access to AMD MI355X GPUs ahead of  Discovery's full-scale deployment.</p><p>  Then, two days later, Argonne National Laboratory announced a  <a href=\"https://www.energy.gov/articles/energy-department-announces-new-partnership-nvidia-and-oracle-build-largest-doe-ai\">similar arrangement with Oracle Cloud and NVIDIA</a>  to deliver a small (Lux-sized) GPU supercomputer named Equinox, followed by a  much-larger 100,000-GPU supercomputer named Solstice. Neither Equinox nor  Solstice are attached to a \"typical\" supercomputer procurement; the follow-on  to Aurora, to be named  <a href=\"https://intro-hpc-bootcamp.alcf.anl.gov/sites/hpc/files/2025-09/WelcomeToHPC_Papka.pdf\">Helios</a>, is  <a href=\"https://www.alcf.anl.gov/draft-technical-requirements-alcf-4-system\">still in planning</a>  and will be deployed in 2028. This strongly suggests that, whatever  \"public-private partnership\" means to the DOE, it is not the same as the  typical leadership computing systems; it is its own AI-centric program.</p><p>  At SC itself, Evangelos Floros (EuroHPC's head of infrastructure) also  mentioned the \"need for public-private partnerships\" to realize EuroHPC's goal  of building \"AI Gigafactories\" with \"100,000 advanced AI processors\" across  Europe.</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure><figcaption class=\"image-caption\">\"Need for public-private partnerships\" to fund AI factories is recognized by EuroHPC too.</figcaption></figure></div><p>Again, what exactly this \"public-private partnership\" model entails in Europe  was never really defined.</p><p>  What was clear is that both American and European efforts are declaring the  need to build massive (100K+ GPU) supercomputers for AI, the traditional HPC  centers will be the public stewards of them, and \"public-private partnerships\"  are the only way to realize them since governments alone cannot foot the bill.</p><p>  The Top500 BOF also included a short, awkward talk by Rick Stevens titled \"The  DOE AI Initiatives\" that amounted to Stevens saying he had nothing to say.  What really happened, I suspect, is that DOE's new \"<a href=\"https://genesis.energy.gov\">Genesis Mission</a>,\" which was announced the week <em>after</em> the SC conference, was a week  late and therefore couldn't be discussed as originally planned. If Stevens had  been able to describe the Genesis Mission, though, I'm sure he would've also  described \"public-private partnership\" as a key aspect, since the same  language is used in the  <a href=\"https://www.whitehouse.gov/presidential-actions/2025/11/launching-the-genesis-mission/\">Executive Order that established Genesis</a>. And I'm sure his description would've been no clearer about what this  really means than what EuroHPC or the OCI/DOE descriptions have stated.</p><p>  Most revealing was my observation that, even outside of the proper conference  program, nobody really knew what any of this meant. I talked to plenty of my  colleagues from both government HPC and hyperscale cloud organizations, and  the only consistent message was that there aren't many concrete facts backing  up the the press releases right now. It appears that these partnerships were  brokered far outside the usual channels that large supercomputer procurements  are normally done, and the people in charge of actually delivering on the  promises of the press releases are still figuring out what is possible.</p><p>  Connecting the dots between Lux/Equinox/Solstice, Genesis, and a recent  <a href=\"https://www.energy.gov/sites/default/files/2025-04/RFI%20to%20Inform%20Public%20Bids%20to%20Construct%20AI%20Infrastructure%20%28website%20copy%29.pdf\">RFI</a>  and  <a href=\"https://sam.gov/workspace/contract/opp/7864e8f4d61f42dc811ba095a41c8368/view\">RFP</a>  from DOE to allow  <a href=\"https://www.energy.gov/articles/doe-announces-site-selection-ai-data-center-and-energy-infrastructure-development-federal\">hyperscalers to build AI factories on federal land</a>, it appears that what is happening is...</p><ul><li>    The DOE has a bunch of land that is adjacent to the National Labs that is    undeveloped but has the infrastructure to support massive AI factories.    Specifically named is a 110-acre parcel at Argonne that can accommodate up    to 1 GW \"AI data park,\" and a 100-acre parcel at Oak Ridge with up to 800    MW. These details were disclosed in    <a href=\"https://www.energy.gov/sites/default/files/2025-04/RFI%20to%20Inform%20Public%20Bids%20to%20Construct%20AI%20Infrastructure%20%28website%20copy%29.pdf\">an RFI they issued earlier in the spring</a>.  </li><li>    The    <a href=\"https://www.energy.gov/articles/energy-department-announces-new-partnership-nvidia-and-oracle-build-largest-doe-ai\">Solstice press release</a>    specifically said that DOE envisions \"shared investments and shared    computing power between government and industry.\" Given the RFI/RFP were    about land leases, these public-private partnerships may involve splitting    the costs of space/power/cooling (the land and infrastructure being leased)    and the capital/operations (the supercomputer cloud services being built)    between the Labs and Oracle.  </li></ul><p>  A potential model for operations is that cloud providers are allowed to build  and operate commercial AI cloud services adjacent to the DOE HPC facilities in  exchange for the DOE Genesis Mission being entitled to some of those AI cloud  capabilities. Exactly how much supercomputing resources hyperscalers like OCI  would give to DOE, and exactly how much it would cost the DOE Labs to serve as  landlords, is probably still undefined. But seeing as how power is the single  biggest limiter in AI these days, I expect this model will only spread costs  around, not actually lower them.</p><p>  If this is indeed how Genesis plays out, this would establish a bizarre new  way for the government to acquire HPC (or AI) capabilities that completely  sidesteps the standard procurement model. Instead of plunking down a hundred  million dollars a year to finance a new leadership supercomputer, we might be  moving into a world where the Labs plunk down a hundred million dollars a year  to cover the costs of power, space, and cooling for a cloud provider. And  instead of owning a leadership supercomputer, these national HPC facilities  wind up consuming HPC (well, AI) resources from cloud providers--hopefully at  a cost that reflects the fact that the cloud providers are also profiting from  cycles being sold off of these machines to commercial AI customers.</p><p>  But again, this is all speculation based on the consistencies I heard  throughout the conference and the experience I had trying to build these sorts  of partnership with the HPC community while I worked at Microsoft. I may be  right, or I may be wildly wrong. There are probably only a handful of people  in the world with a clear idea of what these partnerships are meant to look  like right now, and they are all way above the heads of the people at the HPC  centers who will be tasked with executing on the vision.</p><p>  Selfishly, I am also left with a bit of heartburn over all of this news. I put  a lot of personal time and energy into giving the HPC community the  information it needed to feel comfortable about partnering with hyperscale AI  infrastructure providers while I was at Microsoft, and it often felt like a  Sisyphean task. Within months of me giving up and moving on from my career at  a cloud provider, seeing a complete reversal of policy from the leadership HPC  folks--and to see the \"other guy\" in pole position--is a bit of a slap in the  face.</p><p>  I also couldn't help but notice that the cloud provider in all the headlines  in the US didn't seem to demonstrate a very strong and unified presence at SC  this year. Comically, they didn't even use their own brand's colors for their  booth on the exhibit floor. And the color scheme they did use left no room for  Oak Ridge's Lux system, which will be AMD-based, to be showcased.</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure><figcaption class=\"image-caption\">Oracle's booth at SC25. Their brand color is red, not green. Or so I thought.</figcaption></figure></div><p>Though I may have read too much into this, it feels like these public-private  partnerships are not necessarily composed of equal partners with equal levels  of commitment.</p><p>  More broadly, I left the conference concerned that the discourse happening  around these cloud-HPC/AI integrations--at least in the US--appears to have  regressed compared to where it was when I worked at Microsoft. Many of the  things we had to figure out years ago (cybersecurity models, impacts on jobs  at the HPC centers) seem to have reset to zero. And sidestepping the  procurement processes for leadership computing to enable these public-private  partnerships will either require significant new funding (of which Genesis  provides none; the executive order as-written appears to recolor existing  money) or robbing Peter (the budget funding the next generation of leadership  HPCs) to pay Paul (the cloud providers serving up compute resources for AI).  As a result, I can envision a future where all of the money that used to fund  leadership computing for science becomes money to fund commercial AI  factories, resulting in a slow evaporation of the LCFs as their HPC  capabilities shrink in size and relevance.</p><p>  Though there's lots more to be said on this topic, it's all based on  conjecture. So, maybe the best thing to do is quietly wait and see.</p><h3 id=\"theme-3-ai-discourse-is-growing-up\">  Theme 3: AI discourse is growing up</h3><p>  This was the first SC where it felt like the discourse around AI's role in the  future of scientific computing actually carried some substance. Whereas  previous years saw talk that mostly revolved around basic ideas like \"do LLMs  hallucinate too much?\" or \"can ChatGPT write MPI code?,\" I sat in on a number  of interesting talks and conversations that skipped the question of \"is AI  useful?\" and went straight to \"this is how AI is proving useful to us.\"</p><p>  Maybe it's related to the previous theme: HPC money is becoming AI money, so  AI research is becoming required to stay afloat. Or maybe it's because 2025  has been the year of agentic AI, and agents allow LLMs to be integrated much  more surgically into complex workflows. Or maybe confirmation bias led me to  sit in sessions and talk with people who are at the frontier of applying AI to  scientific discovery. Whatever the case may be, I was glad to hear so much  discussion from researchers around the importance of all the connective tissue  required to operationalize AI in scientific computing.</p><h4 id=\"agentic-workflows\">Agentic workflows</h4><p>  A great example of this was the  <a href=\"https://aiexscale.github.io\">1st International Symposium on Artificial Intelligence and Extreme-Scale    Workflows</a>, which happened on Friday. One of the invited speakers, Dr. Katrin Heitmann,  connected a lot of dots in my head with a talk she gave on how massive-scale,  physics-based simulation workflows can benefit from agentic AI.</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure><figcaption class=\"image-caption\">Heitmann's vision on how agentic approaches can augment (but not replace) humans in complex scientific workflows.</figcaption></figure></div><p>The crux of the challenge faced by most massive-scale simulation (like  <a href=\"https://cpac.hep.anl.gov/projects/hacc/\">HACC</a>, the cosmology code  for which she is famous) is that they generate massive amounts of data. The  <a href=\"https://www.anl.gov/cels/article/simulating-the-cosmos-frontiere-sets-new-record-with-trillionparticle-universe-model\">most recent HACC run</a>  generated hundreds of terabytes of compressed data per checkpoint and over a  hundred petabytes of data in the end; this cosmological simulation serves as a  reference dataset from which downstream cosmological research can draw when  exploring targeted questions. The challenge, of course, is finding relevant  pieces of the simulated universe from amidst a hundred petabytes of raw data.</p><p>  Dr. Heitmann's premise is that agents and tools have very specific scopes and  capabilities, and researchers have control over which of these tools they wish  to use. However, they can hand off these tools to an agentic workflow to let  it autonomously sift through all of the data, looking for specific features  within the simulated universe that are relevant. A specific example she gave  was the process of examining 500 million galaxy clusters; with an agentic,  AI-driven approach, a postdoc was able to interactively sift through these  objects without examining each one individually. For truly interesting  objects, a separate agent could go search the literature and provide an  explanation as to why it may be interesting, absolving the postdoc from having  to make round trips between the dataset and external literature.</p><p>  That all said, it was clear from this talk (and others) that integrating  agentic AI into scientific inquiry is still in its early days. But what I  appreciated about this talk (and the entire workshop) is that it sidestepped  pedestrian questions about trustworthiness by acknowledging that the goal  isn't full autonomy, but rather, enabling researchers to do things faster.  There is still a human at the start and the end of the workflow just as there  always has been, but agents can reduce the number of times a human must be in  the loop.</p><h4 id=\"data-and-agentcentric-service-infrastructure\">  Data and agent-centric service infrastructure</h4><p>  Even when AI wasn't the main topic of discussion, it was clear to me at this  SC that AI is influencing the way researchers are thinking about the  infrastructure surrounding supercomputers. A great example of this was the  keynote at the <a href=\"https://www.pdsw.org/index.shtml\">PDSW workshop</a>,  given by the ever-insightful  <a href=\"https://sc25.conference-program.com/presentation/?id=misc185&amp;sess=sess202\">Dr. Rob Ross, where he offered a retrospective on the work his team has    done over the last two decades</a>, what he felt they got right, what they missed, and what's ahead.</p><p>  Towards the end of his presentation, he made the case that \"science is  increasingly multi-modal.\" But rather than talk about multimodality in the AI  sense, he was emphasizing that there's more to scientific computing than  performance:</p><p></p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure><figcaption class=\"image-caption\">Domain science, provenance, search, and resilience are equal partners to performance in scientific computing.</figcaption></figure></div><p>Taken at face value, this slide positions performance on equal footingwith domain science, provenance, findability, and his argument was that we’vemoved beyond the world where the only storage problem that HPC faces ischeckpointing. Just as Dr. Heitmann would say on Friday, Dr. Ross’ argument wasthat the increasing volume of scientific data coming out of both exascalesimulation and scientific instruments is driving the field towards moreautomation. And with automation comes a greater need to understand dataprovenance–after all, if automation produces a surprising result, a humanultimately has to go back and understand exactly how the automation generatedthat result.</p><p></p><p>  He also point out that in this coming world of automation-by-necessity,  infrastructure itself might have to be rethought. After all, traditional  technologies like parallel file systems were designed to make the lives of  human researchers easier; when the primary consumer of data becomes AI agents,  not humans, there may be better ways to organize and expose data than through  files and directories. A human might repeatedly cd and ls to find a specific  dataset on a file system, whereas an agent use a query a flat index to find  the same data in a single step.</p><p>  At the end of the same PDSW workshop, I was fortunate enough to contribute to  <a href=\"https://sc25.conference-program.com/presentation/?id=miscp112&amp;sess=sess202\">a panel</a>  where many of these same themes--how will data systems change as AI plays a  greater role in scientific discovery--were discussed. Although we touched on a  lot of topics, what stuck with me was a general acknowledgment that, while HPC  has always talked about data management and provenance as being important,  they were always treated as a \"nice to have\" rather than a \"must have.\"  However, as was echoed across many presentations (including the two I  described above), governance and provenance are now becoming non-negotiable as  larger datasets drive us towards AI-driven automation.</p><p>  Regardless of what you think about AI's ability to accelerate scientific  discovery, I left SC with the feeling that AI is forcing the HPC community to  grow up with regards to how seriously it takes data management:</p><ul><li>    The size and velocity of datasets generated by simulation or experiment is    growing beyond any single person's ability to analyze it by hand. The    complexity of these data are also making it harder to develop    herustics-based or analytical approaches to combing through all of it.  </li><li>    The best path forward to understanding these data is through AI (via    purpose-built models for analysis) or AI-driven data exploration (via    autonomous, agentic workflows).  </li><li>    Automation or autonomous workflows will always act under authority delegated    to them by human researchers, meaning there is a growing need to be able to    inspect how these workflows arrived at the conclusions they generate.  </li><li>    Understanding how an answer was achieved requires significantly better data    management features such as governance, provenance, and auditability. A    result is ultimately only useful if a human can trust it, and that trust    comes from understanding which data informed that conclusion, how that data    was created, and how it was modified over time.  </li></ul><p>  Put differently, checkpointing was the main concern of I/O research because  I/O performance was the first scalability issue that scientific computing ran  into as supercomputers and scientific instruments got bigger. However, we're  now at a point where issues ancillary to performance have reached the limits  of scalability. Dr. Ross's multi-modal slide indicate that provenance,  indices/search, and resilience are some examples of these new barriers, but  there are plenty more as well.</p><p>  In a sense, this theme is the opposite side of the same coin as the first  theme I discussed--that the big number is losing its shine. The hardest  questions going forward aren't the obvious ones about scaling performance;  they are about scaling everything else to keep up. AI seems to be the  technology that has cleared a path to these data management hurdles, but the  benefits of adopting strong data management practices and systems will extend  far beyond the reach of just enabling AI-based automation.</p><h2 id=\"the-exhibit-hall\">The exhibit hall</h2><p>  The exhibit hall has long been one of my favorite parts of attending SC  because it's a great way to get a feeling for what technologies and vendors  are hot, where the innovation is trending, and what sorts of commercial  problems are worth solving. Every year I feel like I have less and less time  to walk the exhibit hall though, and the layout and composition of this year's  exhibition meant I only saw a small fraction of what I wanted to see in the  few days it was up.</p><p>  The most common comment I heard about the exhibit this year is captured in  Doug Eadline's article,  <a href=\"https://www.hpcwire.com/2025/11/26/sc25-observations-more-pumps-than-processors/\">SC25 Observations: More Pumps than Processors</a>  (which is well worth the read!). The same commentary was repeated throughout  the OCP conference in October as well, suggesting that there is a lot of money  to be made (or at least the prospect of money) in helping datacenters get  outfitted for the liquid cooling demanded by the next generation of  large-scale GPU infrastructure. However, I found the overwhelming amount of  space devoted to liquid cooling companies acutely problematic at SC25 this  year for two reasons:</p><ol type=\"1\"><li><strong>Most SC attendees have nothing to do with liquid cooling</strong>. A    colleague of mine who operates supercomputers for the energy sector asked    one of these big liquid cooling vendors what he could do to actually engage    with them. After all, he doesn't buy liquid cooling infrastructure; he buys    whole supercomputers that come with heat exchangers and CDUs that are    integrated into the solution. The vendor had no good answer, because the    reality is that the typical supercomputer user or buyer has no say over what    piping, coolant, or exchangers are used inside the machine itself. The whole    point of buying an integrated supercomputer is to not have to deal with that    level of details.  </li><li><strong>These liquid cooling vendors soaked up a ton of floor space</strong>. A few of these physical infrastructure providers had massive (50x50)    booths sprinkled across the exhibit hall. Combined with the fact that the    average SC attendee has nothing to do with liquid cooling meant that the    booths that were more likely to be relevant to a typical attendee were much    further apart than they had to be.  </li></ol><p>  The end result was that the exhibit hall was absolutely gargantuan and yet  information-sparse. In fact, this year saw a secondary exhibit hall in the old  football stadium serve as overflow space, because the entire primary exhibit  hall was full. What's worse is that this overflow space was (as best as I  could tell) completely disconnected from the main hall, and the only time I  ever saw it was from the dining area used to serve lunch for the tutorials.</p><p></p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure><figcaption class=\"image-caption\">The exhibit hall's overflow space being set up in the former football stadium.</figcaption></figure></div><p>I would’ve been furious if if I had been stuck with a booth in thisoverflow space, because I can’t imagine the foot traffic in there was very high.I personally couldn’t even find the entrance to this second exhibition area inthe few hours I had to look for it.</p><p></p><p>  I can't help but think the SC organizers leaned far too much into booking up  as much space (and therefore exhibitor dollars) as possible without thinking  about the dilutive effects of having such a massive vendor count. Some vendors  definitely benefitted from having a good location near one of the hall  entrances, but I also heard a nontrivial amount of grumbling around how little  traffic there was at some of the big booths. It wouldn't surprise me if there  was a contraction of the HPC mainstays at SC26.</p><h3 id=\"by-the-numbers\">By the numbers</h3><p>  Rather than rely solely on anecdotes though, it's also fun to take a  quantitative look at the changes in exhibitors relative to last year. Since I  spent the time figuring out how to generate tree maps for my SC24 recap last  year, I figured I should re-run the same analysis to compare SC25 to SC24.</p><p>  Of the biggest booths who were exhibiting for the first time this year, it  should be no surprise that the two biggest new entrants were Danfoss (liquid  cooling infrastructure) and Mitsubishi Heavy Industries (gas turbines and  other large-scale infrastructure):</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure> <figcaption class=\"image-caption\">New exhibitors with the largest booths.</figcaption></figure></div><p>Of the other top new exhibitors, some (Solidigm, Sandisk, C-DAC, MinIO, and  University of Missouri Quantum Innovation Center) were quite relevant to the  typical SC attendee. Arm was also back after having skipped SC24. But there  were scores of new exhibitors whose services and products seem much more  relevant to very niche aspects of physical datacenter infrastructure.</p><p>  Of the exhibitors who didn't show up to SC25 but had big booths at SC24, there  was a diverse mix of markets:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure><figcaption class=\"image-caption\">Vendors who didn't show up to SC'25 but had big booths at SC'24.</figcaption></figure></div><p>Sadly, higher ed and government popped up on this list (see  <a href=\"https://www.hpcwire.com/2025/11/26/sc25-observations-more-pumps-than-processors/\">Doug Eadline's take on this for more</a>). A bunch of datacenter infrastructure providers also vanished, including  Valvoline and Boundary Electric; this suggests that some of the top new  vendors of this year (Danfoss, Mitsubishi) may similarly vanish entirely next  year after realizing that SC isn't really their crowd. But I was also  surprised to see some big names in AI vanish; Iris Energy (IREN) is a GPU  cloud provider that just inked a multi-billion dollar deal with Microsoft;  Ingrasys manufactures much of the world's GB200 NVL72 infrastructure; Groq,  Sambanova, and SambaNova also inexplicably vanished.</p><p>  Perhaps more interesting are the top growers; these vendors exhibited both  last year and this year, but went significantly larger on their booth sizes:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure><figcaption class=\"image-caption\">Biggest increases in booth size at SC'25 vs. SC'24.</figcaption></figure></div><p>Legrand, which provides datacenter infrastructure bits, likely grew as a  result of it acquiring USystems and merging USystems' booth with Legrand's  booth this year. The other big booth expansions are mostly household names  though; Gates, EBARA, and GRC are cooling vendors that the typical SC attendee  can't do much with, but the others are organizations with whom a researcher or  HPC datacenter operator might actually talk to.</p><p>  Finally, the top contractions in booth space are a mix of service providers,  HPC facilities or research centers, and component suppliers:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure><figcaption class=\"image-caption\">Biggest decreases in booth size at SC'25 vs. SC'24.</figcaption></figure></div><p>Of the biggest vendors who downsized, Carahsoft is a component reseller and  service provider, Stulz is a liquid cooling company, HLRS is a German  supercomputer center, and Viridien is an HPC services company that primarily  serves the energy sector. It is surprising to see AWS shrink while Microsoft  grew, and it is doubly surprising to see Oracle shrink when it's at the center  of the biggest HPC deployment news of the season. Given that these booth sizes  are chosen a year in advance, this may speak to how unexpected the turn of  events were that resulted in Oracle carrying the cloud services end of DOE's  big public-private partnerships.</p><h3 id=\"interesting-new-technology\">Interesting new technology</h3><p>  For reasons I'll discuss later, I didn't have much time to walk the exhibit  hall floor. Combined with the fact that everything was so spread out and  diffuse, I just didn't get a great sense of what interesting new technology  was being introduced this year beyond what tended to stick out. And amidst all  the giant CDUs and liquid cooling infrastructure, it was hard for anything to  stick out except really big compute cabinets.</p><h4 id=\"dell-ir700\">Dell IR700</h4><p>  Dell's booth had a fully loaded IR7000 rack on display (as they did at  <a href=\"https://blog.glennklockwood.com/2025/03/gtc-2025-recap.html#dells-480-kw-ir7000\">GTC earlier in the year</a>) with 36 GB200 NVL4 sleds. At 50OU high (almost eight feet tall), this thing  is physically huge:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure><figcaption class=\"image-caption\">Dell's 50OU IR7000 rack, fully loaded. This is what TACC Horizon will be built from.</figcaption></figure></div><p>Unlike the version they had on display at GTC though, this one had both the  front door and a full rear-door heat exchanger installed:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure><figcaption class=\"image-caption\">HUGE rear-door heat exchanger on the back of the Dell IR7000 rack.</figcaption></figure></div><p>What's notable about this platform is that we now know that it is the basis  for both  <a href=\"https://tacc.utexas.edu/systems/horizon/\">TACC's upcoming Horizon system</a>  (which will have  <a href=\"https://glennklockwood.com/garden/systems/Horizon\">28 of these fully loaded racks</a>) and  <a href=\"https://www.nersc.gov/what-we-do/computing-for-science/doudna-system\">NERSC's upcoming Doudna system</a>  (which will have Vera Rubin rather than Blackwell). This rack was nominally  designed for hyperscale AI and is the basis for Dell's GB200 NVL72 (XE9712)  deployments at places like CoreWeave and xAI, which means that it'll be  thoroughly tested at scale long before TACC or NERSC have it up and running.  This is the opposite of what has historically happened: before AI, it was  usually government HPC that had to debug new rack-scale architectures before  industry would touch it.</p><h4 id=\"hpe-cray-gx5000\">HPE Cray GX5000</h4><p>  However, government HPC will still have a chance to debug a new supercomputing  platform in the recently announced  <a href=\"https://glennklockwood.com/garden/Cray-GX\">Cray GX</a> (formally  called \"the HPE Cray Supercomputing GX platform\"), which is the successor to  the current Cray EX platform. This is the platform that the  <a href=\"https://glennklockwood.com/garden/systems/Discovery\">Discovery supercomputer</a>  at OLCF will use, and HPE had a CPU-only blade (<a href=\"https://glennklockwood.com/garden/nodes/Cray-GX250\">Cray GX250</a>) and a rack mockup on display at SC:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure><figcaption class=\"image-caption\">HPE's new GX blade form factor. This one appears to be the GX250, the 8-socket CPU-only blade.</figcaption></figure></div><p>It's hard to tell the size of this blade from the photo, but if you look at  the relative size of the CPU socket and the DIMM slots, you can get a sense of  how physically massive it is--it's like a coffee table. It also isn't  perfectly rectangular; Cray decided to put this unusual protrusion on the  front of the blades which is where the four NICs and eight E1.S SSDs are  housed:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure><figcaption class=\"image-caption\">A look at the side of the Cray GX blade's \"nose\" showing the side-mounted NIC ports.</figcaption></figure></div><p>This nose(?) adds more surface area to the front of the rack, and it makes  more sense when you see a rack full of these nodes. HPE had a full GX5000 rack  with mocked-up cardboard nodes in their booth as well:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure><figcaption class=\"image-caption\">Fully loaded GX5000 rack. The nodes were cardboard, but pretty nice cardboard.</figcaption></figure></div><p>By having the NIC ports (which are Slingshot 400) face the sides of the rack  rather than stick out the front, the bend radius of all that copper doesn't  have to be quite as dramatic to route it along the sides of these node noses.  And unlike previous Cray designs, there's also no midplane or backplane that  connect the nodes in a rack to the rack-local switches; everything connects  through discrete copper or optical cables.</p><p>  At the center of the rack is a liquid-cooled switch chassis, and each rack can  support either 8-, 16-, or 32-switch configurations. Each switch is a 64-port  Slingshot 400 switch, and I think the premise is that a single GX5000 rack is  always exactly one dragonfly group. If you want a smaller group, you use a  switch chassis with fewer switches.</p><p>  Interestingly, this GX will also support non-Slingshot Ethernet and XDR  InfiniBand switches. Given that both XDR InfiniBand and 800G Ethernet are  shipping today and have twice the bandwidth that Slingshot 400 will have when  it starts shipping in a year, perhaps the Slingshot 400 option is just a  stopgap until HPE's investments in Ultra Ethernet result in a product. The  lack of a network backplane in the rack also makes it easier for the rack to  accommodate the non-dragonfly topologies that would be required for InfiniBand  or Ethernet.</p><p>  The rear of the rack is remarkably unremarkable in that it simply contains a  rear bus bar and the liquid cooling manifolds and mates. In this sense, the  rack looks very  <a href=\"https://www.opencompute.org/documents/open-rack-base-specification-version-3-pdf\">OCP-like</a>; the boring stuff is in the back, everything exciting is serviced from the  front, and the rack itself is passive plumbing. Like any OCP ORv3 rack, power  shelves slot in just as server blades do, and they use the same liquid cooling  infrastructure as the rest of the rack. They power the bus bar, and the blades  and switches draw from the same bus bar.</p><p>  Compared to an ORv3 rack though, these GX racks are wider and shorter. The  width probably offers more flexibility for future NVIDIA or AMD GPU boards,  but I was surprised that Cray didn't go ultra tall like Dell's 50OU IR7000. I  was also surprised to hear that Cray is launching GX with a 400 kW cabinet  design; power appears to already be a limiting factor in the nodes launching  with GX. A single 400 kW GX rack can support</p><ul><li>40 CPU-only blades (81,920 cores of Venice)</li><li>28 AMD Venice+MI430X blades (112 GPUs)</li><li>24 NVIDIA Vera+Rubin blades (192 GPUs)</li></ul><p>  For reference, the demo GX5000 rack pictured above had only 29 blades and 16  switches. I assume that fitting 40 blades into the rack requires using the  smallest dragonfly group possible.</p><p>  On the cooling front, the GX5000 rack will launch with support for the same  1.6 MW CDUs as the current Cray EX platform. I heard talk of a neat sidecar  CDU option as well, but the person with whom I spoke at the HPE booth said  that would come a little later.</p><p>  Overall, I was surprised by how un-exotic the new Cray GX platform is compared  to what the AI world has been doing with ORv3 racks. The fact that Cray and  Dell's designs are more similar than different suggests that the HPC/AI world  is converging on a place where the future is uncertain, and flexibility is  more important that highly engineered racks that optimize for very specific  nodes and networks. It also suggests that the real value of buying Cray is  higher up the stack; liquid cooling, power delivery, and rack integration is  becoming commoditized thanks to AI.</p><p>  I was also surprised that Cray's next-generation design is not obviously  superior to what the hyperscale community is designing. Whereas the GX rack  caps out at 400 kW, Dell's will allegedly scale up to 480 kW. That said,  today's IR7000 racks shipping for Horizon are only 215 kW (for GPU racks) and  100 kW (for CPU-only racks) according to a talk given by Dan Stanzione:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure><figcaption class=\"image-caption\">The physical configuration of TACC's upcoming Horizon supercomputer.</figcaption></figure></div><p>So until the final specifications for the Rubin GPU are released, I suspect we  won't know whether Cray still leads the pack in terms of compute density, or  if Dell made the better bet by aligning its supercomputing platform on a  standard OCP rack design.</p>",
            "url": "https://hpc.social/personal-blog/2025/sc-25-recap/",
            
            
            
            
            
            "date_published": "2025-12-01T14:34:00-07:00",
            "date_modified": "2025-12-01T14:34:00-07:00",
            
                "author": "Glenn K. Lockwood's Blog"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2025/the-trap-of-prioritizing-impact/",
            "title": "The trap of prioritizing impact",
            "summary": null,
            "content_text": "(I wrote this originally as a comment in RLS in response to a staff-level engineer who was frustrated at how little they got to code anymore, and it resonated with enough folks that maybe it’s worth sharing here!)There’s a trap I’ve seen a lot of staff+ folks fall into where they over-prioritize the idea that they should always be doing “the right, most effective thing for the company”. When I see engineers complain that they don’t get to code enough, I often suspect they’ve fallen prey to this.I say that’s a trap! because I see people do this at the expense of their own job satisfaction and growth, which is bad for both them and (eventually) for the company which is likely to lose them.I don’t blame people for falling into this trap, it’s what we’re rewarded for. I’ve fallen into it! I have stopped doing technical work I cared about, prioritized #impact, and fought fires wherever they arose. I have spent all my time mentoring and teaching and none coding. The result was often grateful colleagues, but also burnout and leaving jobs I otherwise liked.Whereas when I’ve allowed myself to be like 30% selfish — picking some of my work because it was fun and technical, even when doing so was not the “most impactful” thing I could do — I was happier, learned more, and stayed in roles longer.An example: I worked on a team that was doing capacity planning poorly and was buying too much hardware. (On-prem, physical hardware.) I could have solved the problem with a spreadsheet, but that was boring and made my soul hurt.What I did instead was dig into how our container scheduling platform worked, and wrote a nifty little CLI tool that would look at the team’s configured workloads and spit out a capacity requirement calculation. It took about three times as long as the spreadsheet would have, but it was fun and accomplished the same goal and gave me some experience in the container platform. And it wasn’t that much of a time sink.Was that better for the company? No idea. I hope it was — I hear the tool is still maintained and no one has replaced it with a spreadsheet yet! But that’s a happy accident.Was it better for me? Absolutely! It was a bit selfish, but it made an otherwise tedious task more fun and I learned some useful tricks.So — if you wish you had more time to code… go code a bit more. Don’t let the idea of being more effective guilt you into giving it up. Your career is your career and you should enjoy it.",
            "content_html": "<p>(I wrote this originally as a comment in <a href=\"https://randsinrepose.com/welcome-to-rands-leadership-slack/\">RLS </a>in response to a staff-level engineer who was frustrated at how little they got to code anymore, and it resonated with enough folks that maybe it’s worth sharing here!)</p><p>There’s a trap I’ve seen a lot of staff+ folks fall into where they over-prioritize the idea that they should always be doing “the right, most effective thing for the company”. When I see engineers complain that they don’t get to code enough, I often suspect they’ve fallen prey to this.</p><p>I say <strong><em>that’s a trap</em></strong>! because I see people do this at the expense of their own job satisfaction and growth, which is bad for both them and (eventually) for the company which is likely to lose them.</p><p>I don’t blame people for falling into this trap, it’s what we’re rewarded for. I’ve fallen into it! I have stopped doing technical work I cared about, prioritized #impact, and fought fires wherever they arose. I have spent all my time mentoring and teaching and none coding. The result was often grateful colleagues, but also burnout and leaving jobs I otherwise liked.</p><p>Whereas when I’ve allowed myself to be like 30% selfish — picking some of my work because it was fun and technical, even when doing so was not the “most impactful” thing I could do — I was happier, learned more, and stayed in roles longer.</p><p>An example: I worked on a team that was doing capacity planning poorly and was buying too much hardware. (On-prem, physical hardware.) I could have solved the problem with a spreadsheet, but that was boring and made my soul hurt.</p><p>What I did instead was dig into how our container scheduling platform worked, and wrote a nifty little CLI tool that would look at the team’s configured workloads and spit out a capacity requirement calculation. It took about three times as long as the spreadsheet would have, but it was fun and accomplished the same goal and gave me some experience in the container platform. And it wasn’t that much of a time sink.</p><p>Was that better for the company? No idea. I hope it was — I hear the tool is still maintained and no one has replaced it with a spreadsheet yet! But that’s a happy accident.</p><p>Was it better for me? Absolutely! It was a bit selfish, but it made an otherwise tedious task more fun and I learned some useful tricks.</p><p>So — if you wish you had more time to code… go code a bit more. Don’t let the idea of being more effective guilt you into giving it up. Your career is your career and you should enjoy it.</p>",
            "url": "https://hpc.social/personal-blog/2025/the-trap-of-prioritizing-impact/",
            
            
            
            
            
            "date_published": "2025-09-20T14:46:41-06:00",
            "date_modified": "2025-09-20T14:46:41-06:00",
            
                "author": "Thinking Out Loud"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2025/lessons-learned-from-three-years-in-cloud-supercomputing/",
            "title": "Lessons learned from three years in cloud supercomputing",
            "summary": null,
            "content_text": "I recently decided to leave Microsoft after having spent just over three years there, first as a storage product manager, then as a compute engineer. Although I touched many parts of Azure's infrastructure during that time, everything I did was at the intersection of large-scale supercomputing and hyperscale cloud. There was no shortage of interesting systems to figure out and problems to solve, but as I began to wrap my arms around the totality of hyperscale AI training in the cloud, I also began to see the grand challenges that lay ahead.Outside Microsoft's Silicon Valley Campus minutes after I was escorted off the premises.Although many of those challenges would probably be fun and exciting to tackle, the more I learned, the more I found myself asking the same two questions: what did I want to do with the rest of my career, and was the path I was following going in the right direction? I spent a lot of time thinking about this, and my decision to leave Microsoft ultimately reflects the answer at which I arrived. But rather than indulge myself by recounting my introspection, I thought I would share some of the things that I learned while at Microsoft in the hopes that others find value in my experience.To that end, I've split this post into two sections:Things I've observed about HPC and technology trends from the perspective of a cloud/hyperscale/AI practitioner and provider, andThings I've realized about jobs and careers from the perspective of someone who's now worked in academia, a successful startup, government, and now Big Tech and is about halfway through his careerI consider this to be the concluding chapter of a three-part series that began with Life and leaving NERSC and continued with How has life after leaving the Labs been going.Also, please note that I authored this the day after my employment at Microsoft ended, and I was not beholden to any company or organization at the time of writing. The views expressed below are mine alone.HPCEverything I did at Microsoft touched supercomputers in one way or another, and my day job was exclusively supporting Microsoft's largest AI training supercomputers. Despite that, I did a lot of moonlighting in support of Azure's Federal business, and this is how I justified giving talks at events like like NERSC@50, SC, and Salishan in my last year. It's also what let me straddle both worlds: I had a rare, first-hand knowledge of how the de facto largest supercomputers in the world were built and used, and I had a front-row seat for how leaders in the traditional supercomputing world perceived (and sometimes misunderstood) what we were doing in the cloud.Before I get into specific observations though, I should clarify some nomenclature that I will use throughout:Supercomputers are the piles of compute nodes with a high-speed interconnect that are designed to solve one big problem in parallel. This is a generic term to describe the instrument, not its workload.HPC, traditional HPC, modsim, and scientific computing all refer to the ecosystem built around using something like MPI to solve a problem rooted in some type of science. Every big supercomputer run by DOE, procured through EuroHPC, and sited at the world-famous, government-funded supercomputer centers falls into this category.Cloud, hyperscale, and AI training all refer to the ecosystem built to train large language models. The supercomputers are run by hyperscale companies like Microsoft, Amazon, or Meta whose backgrounds have not historically been in the world of supercomputing.I realize that these are not very precise, but they're the easiest way to contrast what I learned inside Microsoft (a hyperscale cloud) with the world I came from prior (traditional HPC).HPC wants to be like the cloud, not in itWhen I left NERSC in May 2022, I speculated that the future of large-scale supercomputer centers would be follow one of two paths:They develop and squish cloud technologies into their supercomputers to make them more cloud-like, orThey abandon the idea of buying individual systems and instead enter into long-term relationships where flagship HPC systems are colocated inside cloud datacenters sited in places with low-cost, low-carbon power.I was hoping that the desire to continue building systems after passing the exascale milestone would make the next click-stop follow path #2, but early indications (across the global HPC landscape) are that the community has chosen path #1.HPC centers around the world are embracing the idea of cloudifying on-prem supercomputers by adding virtualization, containerization, and integration with other services to enable complex workflows. And as a part of that, they're reinventing many of the technology integrations that have always been first-class citizens in cloud: CSCS added capabilities to create \"versatile software-defined clusters\" on their latest Cray system, Alps. NERSC's next system, Doudna, is envisioned to allow its users to \"move from programming the supercomputer to programming the datacenter.\" But none of these systems are actually using commercial cloud services in non-trivial ways.In the year or two that followed ChatGPT, the notion of large-scale supercomputers in the cloud was a green field, and cloud providers were open to chasing all sorts of silly ideas. This made it the ideal time for the leadership HPC computing community to get a seat at the hyperscale table. Although their budgets couldn't compete with AI, HPC centers could've drafted on the investments of AI buildout and offered the societal impacts of using GPUs for science as a nice complement to the societal impacts of using GPUs for AI training.Much to my dismay, though, that window of opportunity was spent decrying the investment in hyperscale and AI rather than trying to exploit it; that window was the year of \"us versus them.\" And unfortunately, that window has essentially closed as accountants and CFOs have now sharpened their pencils and are searching for returns on the investments made in GPU infrastructure. The intrinsic value of supercomputing infrastructure in the cloud has been reduced to the point where Microsoft's CEO outright said they were turning away customers who just wanted to pay for GPU clusters, because higher-quality revenue could be made from inferencing services that use those same GPUs.So even if the HPC community woke up tomorrow and realized the long-term benefits of partnering with commercial clouds (instead of trying to copy them), I don't think cloud providers would respond with the same enthusiasm to meet in the middle now as they would have a year or two ago. I don't think this was a deliberate decision on behalf of the cloud providers, and they may not even fully realize this change. But the future of hyperscale supercomputing is rapidly crystallizing, and because HPC wasn't present in the solution, there's no room for it in the final structure.Cloud is expensive, but not for the reasons most thinkIt's been easy to write off the cloud as too expensive for HPC, and most people do silly math based on public list prices for VMs to justify their position. The narrative usually goes something like, \"if a single GPU VM costs $40/hr, then running 10,000 of them for five years will cost 17X more than our on-prem supercomputer!\" That's not how it works, and nobody pays that price. That $40/hr is the maximum possible price, and it includes the cost to the cloud provider of keeping nodes idle in the event that someone shows up and suddenly wants to use one on-demand.But even if you cut out all the profit for the cloud provider and just look at the cost of the physical infrastructure, building a supercomputer in the cloud is just more expensive than putting a bunch of whitebox nodes into a traditional HPC datacenter. There's a couple reasons for this, and here are a couple in no particular order:High availability: Every cloud datacenter has redundant power, and most of them have very redundant power. This is provisioned independently of whatever goes inside of that datacenter, so when you deploy a 10 MW supercomputer inside a 10 MW cloud datacenter, that comes with at least 10 MW of backup diesel generators, UPSes, and the electrical infrastructure. HPC workloads don't really need this, but it's hard to deploy HPC in the cloud without a ton of generators and UPSes coming along for the ride. This is changing with AI-specific cloud datacenters now being built, but these AI datacenters still have way more redundant power than a typical on-prem HPC datacenter. Building a cloud datacenter with the minimal redundancy that a traditional HPC datacenter has would mean that facility couldn't ever be used for anything but HPC, and that would undercut the overall flexibility upon which cloud economics are built.Cloud-side infrastructure: Every compute node has to be attached to the frontend cloud network in addition to a backend high-speed network like InfiniBand, unlike a traditional supercomputer where nodes are only attached to one high-speed network. While the cost of the smart NIC in each node is just a couple hundred dollars, every cloud supercomputer has to have a complete frontend network built out to support every single compute node--that's a ton of switches, routers, and fiber that must be properly provisioned all the way up to the cloud region in which those nodes are deployed. This frontend network is what enables all the cool cloud features on every node (full SDN, integration with other cloud services, etc), but these features aren't generally worth their cost when running meat-and-potatoes HPC workloads like MPI jobs by themselves. Their value only really shines through when executing complex workflows that, for example, couple an MPI job with stateful services and globally accessible data sharing with fine-grained access controls, all fully automated through programmable APIs and full RBAC.AI-optimized system architecture: AI-optimized GPU supercomputers contain a bunch of components that your typical Cray or Eviden simply wouldn't have. I wrote about the differences between AI and HPC supercomputers elsewhere, but in brief, AI workloads specifically benefit from having tens of terabytes of local SSDs and all-optical (no copper) RDMA fabrics. These add to the COGS (cost of goods sold) of an AI-optimized supercomputer, meaning that that a supercomputer with a thousand GPUs designed for AI is going to be more expensive than one designed for scientific computing no matter where it's deployed. And cloud providers are all optimizing their supercomputers for AI.There's a bunch of other cloud \"stuff\" that is required as well; every cloud region has a first footprint which is a LOT of general-purpose servers and storage that is required to support the basic cloud control plane. Before any user-facing cloud resources (including supercomputers) can be deployed, there has to be tens or hundreds of racks of this cloud \"stuff\" that is up and running. And although the cost of that first footprint is amortized over many customers in larger or older cloud regions, larger single-use infrastructures (like supercomputers) carry a proportionally larger fraction of the cost to deploy the first footprint.So when you look at the cost of running a single compute node in a cloud supercomputer, there are a bunch of extra ingredients baked in that you wouldn't get by just signing a check over to an OEM:a high availability SLA, afforded in part by all those generators and UPSesslick cloud service integrations, privacy features, virtual networking, afforded by that frontend cloud networkbetter performance for AI training or inferencing workloads, afforded by extra SSDs and all-optical interconnectsa bunch of other typical TCO stuff--the power consumed by the node, the opportunity cost of free floor tiles in your datacenter, and the engineers and technicians that keep it all runningUltimately, someone needs to pay for all of these extra ingredients. Cloud providers could just eat the costs themselves and sell the supercomputing service at a price comparable to what a customer would pay for an on-prem supercomputer--and sometimes they do. But this dilutes the profitability of the deal, and it increases the risks of the cloud provider losing money if unexpected issues arise during execution. Losing money is objectively bad business, so it's usually cloud customers who are paying for all these extra capabilities regardless of if they use them or not.So if all you want to do is run big MPI jobs, and you have no use for the extra availability, cloud integrations, privacy and security, and programmable infrastructure, sure--the price per-node is going to be higher in the cloud than on-prem. You're paying for a bunch of features that you don't need....Although sometimes it isSometimes buying a supercomputer in the cloud is straight up more expensive because of the value it provides though. For example, I remember a case where a large AI company needed to train a big LLM on many thousands of GPUs, so they signed an agreement which gave them exclusive access to a cloud supercomputer that strongly resembled a specific GPU system in the DOE complex. Because I used to work in the DOE, I knew how much DOE paid to buy their GPU cluster, and I also knew that three years of maintenance was included in that cost.What amazed me is what this AI company was willing to pay (roughly) the same price that DOE paid for their on-prem supercomputer, but in exchange, get exclusive access to a comparably capable cloud supercomputer (same GPUs model, similar GPU count, similar interconnect) for one year only. Put differently, being able to use a big, cutting-edge GPU cluster was worth up to 3x more to this AI company than it was to the DOE.While it may sound like I'm spilling secrets here, the reality is that anyone working for a cloud provider wouldn't be able to tell which AI deal I was describing here--they all look like this, and they're all willing to spend significantly more than the HPC community for the same compute capability. This gives you a sense of the real value that AI companies place on all the benefits that cloud-based supercomputers can provide.This isn't all bad for HPC, though. Every fat deal with an AI company means that there can be another deal with an HPC center that has slim margins. For example, let's say an AI company is willing to pay a billion dollars for a supercomputer whose TCO is only $330M--that means the cloud provider gets 67% margin. If the cloud provider's overall margin target is 50%, that means it can sell an identical supercomputer to an HPC customer at zero profit (for $330M) and still walk away happy. Thus, it is possible for the price of a supercomputer for HPC to be subsidized by all the money that the AI industry is throwing into supercomputing. Whether or not a cloud provider ever cuts deals like this is a business decision though--and as I said earlier, I don't think they're as open to silly ideas now as they used to be.The real hurdle that I was never able to overcome out, though, is a result of the fact that there is finite expertise in HPC and AI in the world. HPC-AI is ultimately a zero-sum game, and every hour spent working with an HPC customer is usually an hour that isn't being spent working with a much more profitable AI customer. I constantly ran into this problem working in hyperscale AI; my full-time job was to deal with AI customers, but I enjoyed interacting with HPC customers too. As a result, I had to do a lot of my the HPC-specific work (preparing conference presentations, for example) on nights, weekends, and vacations. It was just hard to tell people that I couldn't help improve job uptime on a massive training run because I was preparing a talk for a workshop that, frankly, might be openly hostile to my message.Influencing the cloud is hardBecause the difference in investment is so big between HPC and AI, many of the carrots that the HPC community has traditionally dangled in front of HPC vendors aren't very enticing to the hyperscale AI community. For example, both US and European HPC programs have relied heavily on non-recurring engineering (NRE) contracts with industry partners to incentivize the creation of products that are well-suited for scientific computing; PathFoward and Horizon 2020 both come to mind as well-funded, successful efforts on this front.However, HPC is the only customer community that really tries to do this, and it echoes a time when the HPC community was at the forefront of scale and innovation. Nowadays, the prospect of accepting $1M/year NRE contract to implement XYZ is completely unappetizing to a hyperscaler; it would probably cost more than $1M/year just to figure out how a company with $250 billion in annual revenue can handle such an unusual type of contract and payment. Add to to this the weird intellectual property rules (like disentangling a 40% cost sharing advance waiver for a tiny project within a multi-billion-dollar business), and it can become a corporate quagmire to go anywhere near NRE projects. Companies with well-insulated HPC silos can probably manage this better, but part of hyperscale economics is that everything overlaps with everything else as much as possible across supercomputing, general-purpose computing, hardware, and software.As a result of this, I really struggled to understand how a $20M/year service contract and a $1M/year NRE contract is materially different from a $21M/year service contract in the cloud world. For most (non-HPC) cloud customers, the RFP comes in saying \"we need XYZ\" and some product manager notes customer demand for XYZ. If the demand is large enough, the feature winds up on roadmap, and the cloud provider develops it as a part of regular business. If there is no other demand, then an NRE contract isn't really going to change that; maintaining feature XYZ long-term will cost far more than a couple million dollars, so implementing it would be a bad decision. This isn't unique to cloud, for what it's worth; while there are some successful HPC NRE stories, there are far more NRE-originated products that had no product-market fit and were simply abandoned after the associated supercomputer was retired.As best as I can tell, NRE has become a way for big HPC customers to maintain the illusion that they are influencing hyperscalers. A hyperscaler could propose some NRE, and an HPC buyer could fund it, and there could be weekly meetings where the two get together and pretend like they're collaborating and codesigning. The hyperscaler could write milestone reports, and they could attend quarterly business reviews with the customer. But this feels like an act. You simply can't move a $250B/year company that isn't solely organized around supercomputing with the lure of a couple million a year.This is not to say that NRE and codesign have no purpose in HPC! I'm sure component vendors (GPUs, networking, and the like) can make minor tweaks that offer big upside for the HPC community. But I learned that, as in several other dimensions, the HPC community is being pushed towards buying whatever is already on the truck, and NRE isn't going to have the impact that it once did.CareerIn addition to learning about how the hyperscale supercomputer world works in practice, my time at Microsoft exposed me to a segment of the supercomputing community that I didn't know existed: junior software engineers who were unwittingly thrown into the deep end of HPC straight out of college and were desperate to find their footing in both the technology and their careers overall. Maybe the most impactful work I did in the past three years was not technical at all, but instead came through some internal talks I gave on my professional journey in HPC and the one-on-one conversations that followed.Since I've gotten such positive feedback when I talk and write about this aspect of HPC, I'll also share some things I've learned about choosing the right employer and job during my time at Microsoft.People matterI learned that the right team matters more than the right job. It is profoundly important to me that I get to work with people with the same level of passion and curiosity, even if we are working on different problems.In retrospect, I realize that I have been very lucky that my career has progressed through organizations that were packed to the gills with people with whom I shared values. They wanted to go to conferences to share their work, they wanted to hear about how others are solving similar challenges, and they weren't afraid to present (and challenge) new ideas. As I learned over the last three years though, I think these traits are acutely concentrated in the HPC world since HPC itself originated from academia and a culture of independence and self-direction. They certainly aren't universal to all workplaces.To be clear, I am not saying that my coworkers at Microsoft weren't passionate or curious. But I did learn that, at big tech companies, you can have a perfectly successful career by keeping your head down and cranking away at the tasks given to you. If the work changes one day, it's actually a virtue to be able to walk away from the old project and turn your complete attention to a new one. Did the company just cancel the product you've been working on? No problem. If you were good at writing code for Windows update, you'll probably be just fine at coordinating planned maintenances for supercomputers. A colleague of mine called these people \"survivors,\" because they will do the best they can with whatever they're given.While this agility is great if you love programming, it can also engender numbness and dispassion for any specific application area. If a \"survivor\" can just as easily program for HoloLens as they can for GPU telemetry, they also likely don't really care about either HoloLens or GPUs. This isn't a bad thing, and I am certainly not passing judgment on people who don't care about GPUs. But it does mean that it's harder for someone who really cares about GPUs to connect with a teammate who really doesn't. And this has many knock-on effects in day-to-day work; it's only natural for people who share common values to help each other out, while relative strangers are less likely to go that extra mile. Finding that common ground to promote \"some person on team X\" to \"my trusted colleague on team X\" is that much harder.This difficulty in finding my community amidst all the survivors is what led me to look outside of my company to find my people. I went to events like the Smoky Mountains Conference and NERSC@50 and took the stage to literally beg the HPC community to give me a reason to work with them. By the letter of my job description, I was never supposed to be on stage; I was supposed to spending all my time behind my desk, thinking about the reliability of our biggest supercomputers. But I liked working with the people in the HPC community, and I liked working with our HPC sales organization, because we all shared common values; we were passionate about HPC and the mission of advancing scientific computing. So, I wound up spending a lot of time working on simple things with HPC folks and not enough time doing my day job.Company culture matters, tooIn an organization where individuals don't often share a lot of common ground, I learned that it's incumbent upon everyone to make a deliberate effort to maintain a culture of working together and helping each other out. A positive workplace culture won't happen by itself across a massive organization. To this end, Satya has a bunch of corporate culture mantras that are often repeated to keep reminding people of the way employees should treat each other.For example, he has a mantra of \"be a learn-it-all, not a know-it-all.\" But I found that many people struggled to really understand how to do this in practice; when confronted with a tough problem (\"your database keeps timing out when we point a thousand nodes at it at once\"), it's often too easy to just be a know-it-all (\"nobody else does that, so you are doing it wrong\") rather than a learn-it-all (\"why are you doing it that way?\"). And the older a company is, the harder it is for decades-long veterans to maintain openness to new challenges in the silo they've built around themselves.I've worked with HPC users for long enough to know that this attitude is pervasive anywhere you put a bunch of smart people with different perspectives into a room. However, it wasn't until I came to Microsoft that I learned that there's something to be gained by explicitly and repeatedly reminding people that they should strive to understand at least as much as they try to explain. Should I ever find myself in a leadership position, this is definitely a mantra I will carry with me and repeat to others, and I will credit my time at Microsoft with appreciating how to really live this mentality, not just parrot it.Being good at things isn't always a jobPeople tell me that I'm pretty good at a bunch of stuff: figuring out how technologies work, explaining complex concepts in understandable ways, and taking a critical look at data and figuring out what's missing. And I enjoy doing these things; this is why I post to my blog, maintain my digital garden, and love getting on stage and giving presentations. But people also say that, because I'm good at these things, there'd be no shortage of opportunities for me in the HPC industry should I ever go looking.However, I've learned that a job has to be an amalgamation of responsibilities that create value, and connecting \"things I'm good at\" with \"things that need to be done\" is not always straightforward. For example, if I am good at learning things and share what I learned with others, what kind of jobs actually turn that into a responsibility?Developers don't really do this at all. Their job is really to keep those git commits coming. Sometimes this requires learning new things, but writing blog posts or giving talks is not in the job description, so they don't count for much on performance reviews.Product managers do a little of this. I had to learn a few things and then repeat them a lot when I was a PM. Over and over. To customers, to executives, to partner teams. It was 5% learning and 95% sharing.Salespeople also do a little of this. They have to stay current on customer needs and product features, then repeat them a lot.System architects do a fair amount of this. I had to learn about what technologies are on the horizon, figure out how to piece them into an idea that could be implemented, then explain why it'd all be a good idea to others.Educators do a lot of this. The technology industry is always moving, so learning is required to stay up to date. They also get to be selective about the ideas worth sharing and downplay the rest.Each one of these roles has its own downsides too; for example, product managers and salespeople often have to nag people a lot, which I don't think anyone likes. And many of these roles require sharing knowledge with people who really don't want to hear it. After all, what customer is eager to talk to every salesperson who comes in the door?Trying to find the ideal job is not just a matter of being good at many things; it's a matter of finding specific jobs that contain a maximal number of things you're good at and a minimal number of things you don't want to do. It's an NP-hard problem, and I've come to realize that the only way to solve it is through trial-and-error. I'm sure some people get lucky and figure out the optimal path on their first try, but for the rest of us, the only way to approach the optimal path is to continuously reflect and not longer on a known-suboptimal path for any longer than is necessary.I've given up on trying to find the perfect job, because I've learned that it probably doesn't exist. I'm good at some things, I'm bad at some things; I enjoy some responsibilities, and I dislike some responsibilities. As with every other job I've had, I learned a lot about all four of these categories during my time at Microsoft, and my choice of next step has been informed by that. I don't expect it to be perfect, but I have high hopes that it will be a step in the right direction.You don't have to be your employerWhen I left the government for a corporate job, one of my biggest worries was losing credibility with peers whose opinions I respected. It's easy to dismiss the viewpoint of someone at a large vendor with a rationalization like, \"of course they'd say that; it's their job,\" but I learned that the HPC community isn't so reductive. People are smart, and most were willing to engage with the quality of my ideas before checking the affiliation on my conference badge.The trick, of course, was finding ways to share ideas in a way that didn't upset my corporate overlords but had substantive value to my audience. I think I figured this out, and in short, I found that leading with honesty and precision works best. The HPC community was built on sharing experiences and learnings about what does and doesn't work, so embracing that--rather than name-dropping products and making hyperbolic claims--seemed to keep me getting invited back to the HPC conferences and workshops that I wanted to attend.I wasn't completely intentional in building whatever credibility I've gained over the last three years, but I was intentional in avoiding work that would clearly compromise it. I never want to be accused of misrepresenting the limits of my understanding, so I will never present a slide containing statements or plots that I can't substantiate. I also never want to be accused of misrepresenting the truth, so I am as forthright as possible in disclosing when I do (or don't) have an incentive to say something.Because I stayed true to myself, I think I was the same person at Microsoft as I was at NERSC or SDSC. That continuity helped my peers quickly recalibrate after I became a vendor, and I think this helped me do more than if I had gone all-in on the role of a cloud spokesperson. Of course, there were times when I had to take on an employer-specific persona, but that's just business, and I've found that peers recognize that this is just a part of the game that we all must play.The result of all this wasn't clear to me until after I started telling people I was leaving Microsoft. There are a bunch of HPC-specific projects I undertook on the side (e.g., reviewing and advising on research, serving on panels), and I started notifying people that I would have to find other Microsoft engineers to take over these obligations since I was leaving. Much to my surprise though, everyone responded the same way: the request to have me help was specifically to me, not my employer. Short of any conflicts of interest, they didn't care who employed me and valued my contributions regardless of who was signing my paychecks.So, after three years working for an HPC vendor, I have learned that most people won't define you by your employer as long as you don't define yourself by your employer. It is possible to work for a company that sells HPC and still maintain your own identity as a person, but it requires thoughtful effort and a supportive (or indifferent!) employer. If you act like a company shill, you will be regarded as one, but not many jobs in industry actually require that to fulfill your responsibilities.Happiness sometimes costs moneyI think most people would agree that, while money can't buy happiness, it certainly helps. What I didn't realize until recently, though, is a reciprocal truth: sometimes happiness costs money.A year ago, I wrote about how the pay in industry compares to working at the national labs, and I described how my golden handcuffs were structured. An optimist might say that these vesting schedules are a way to keep a happy employee from being lured away, but I think it's equally common that these are truly handcuffs. They are a constant reminder that, even in the darkest of days, there is a six-figure reason to grit one's teeth and persevere.I've come to realize that there is an adverse correlation between a few factors:Smaller organizations offer more flexibility to mold a job around your preferences, because there is more work scope spread across fewer people.Larger organizations can afford to offer larger total compensation, but flexibility is limited to the scope of any single team.I kind of thought about it like this:When I realized that I should explore other paths, I had to determine where in this continuum I wanted to wind up: do I care more about a fat paycheck, or do I care more about enjoying my day-to-day responsibilities? And once offers started coming in, exactly how much of a pay cut was I willing to take in exchange for the flexibility that I would receive?By the time I handed in my resignation at Microsoft, I knew exactly how much this happiness was worth to me. Alternatively, I found out how much opportunity cost I was willing to pay for the ability (hopefully!) to reconnect with my day-to-day work. The calculus was an interesting exercise involving a bunch of Monte Carlo simulation which I won't detail here, but as it turns out, I was willing to pay a lot of money for the chance to do something that aligned more completely with what I wanted to do with the rest of my career. In the end, I gave up hundreds of thousands in unvested stock, and I am taking a six-figure pay cut in annual base pay when I start my next job. For me, though, this was a fair price to pay.Final thoughtsAfter three years in the world of hyperscale supercomputing, I have come away with two major learnings that now shape how I think about the future.On the technical front, I think the HPC community has chosen to keep going its own way and reinvent the cloud rather than work meaningfully with hyperscale cloud providers. There was a brief window of opportunity where the mountain may have actually come to Muhammed, and the trajectory of scientific computing could have fundamentally changed to align with the growth trajectory of hyperscale AI. However, I don't think the HPC community was ready to take a big swing during those early days post-ChatGPT or do an earnest assessment of what that future could've looked like. I also worry that the window has closed, and the HPC community never even realized what was on the table.On the career front, I've realized that success is multidimensional. Money is one axis, but so are impact, people, and purpose. The relative importance of each is not always obvious either; they only became clearer to me as I tried different jobs across the space. I've found that the ability to work with like-minded people and the opportunity to learn and share are the most important dimensions to me, but also I recognize that I am privileged in others. Finding stacks of money can be easy for those who work in AI, but there are no shortcuts to building (and retaining!) teams of great people. Anyone who can do the latter well should not be undervalued.There's a lot more that I didn't have time to organize and write, but I have every intention of continuing to be myself, regardless of where I work, in the future. I will keep writing, posting, and talking about what I'm learning in supercomputing whenever I can. And along those lines, I hope that writing all this out helps others figure out what's important to them and where they want to go.",
            "content_html": "<p>I recently decided to leave Microsoft after having spent just over three years there, first as a storage product manager, then as a compute engineer. Although I touched many parts of Azure's infrastructure during that time, everything I did was at the intersection of large-scale supercomputing and hyperscale cloud. There was no shortage of interesting systems to figure out and problems to solve, but as I began to wrap my arms around the totality of hyperscale AI training in the cloud, I also began to see the grand challenges that lay ahead.</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure><figcaption class=\"image-caption\">Outside Microsoft's Silicon Valley Campus minutes after I was escorted off the premises.</figcaption></figure></div><p>Although many of those challenges would probably be fun and exciting to tackle, the more I learned, the more I found myself asking the same two questions: what did I want to do with the rest of my career, and was the path I was following going in the right direction? I spent a lot of time thinking about this, and my decision to leave Microsoft ultimately reflects the answer at which I arrived. But rather than indulge myself by recounting my introspection, I thought I would share some of the things that I learned while at Microsoft in the hopes that others find value in my experience.</p><p>To that end, I've split this post into two sections:</p><ol type=\"1\"><li>Things I've observed about <a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#hpc\"><strong>HPC and technology trends</strong></a> from the perspective of a cloud/hyperscale/AI practitioner and provider, and</li><li>Things I've realized about <a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#career\"><strong>jobs and careers</strong></a> from the perspective of someone who's now worked in <a href=\"https://www.sdsc.edu/\">academia</a>, a <a href=\"https://www.cnbc.com/2019/09/12/10x-genomics-txg-biotech-start-up-surges-in-ipo-debut.html\">successful startup</a>, <a href=\"https://www.nersc.gov/\">government</a>, and now <a href=\"https://www.microsoft.com/\">Big Tech</a> and is about halfway through his career</li></ol><p>I consider this to be the concluding chapter of a three-part series that began with <a href=\"https://blog.glennklockwood.com/2022/05/life-and-leaving-nersc.html\">Life and leaving NERSC</a> and continued with <a href=\"https://blog.glennklockwood.com/2024/08/how-has-life-after-leaving-labs-been.html\">How has life after leaving the Labs been going</a>.</p><p>Also, please note that I authored this the day after my employment at Microsoft ended, and I was not beholden to any company or organization at the time of writing. <i>The views expressed below are mine alone</i>.</p><!--<ul><ul><li><a href=\"#hpc\">HPC</a><ul><li><a href=\"#hpc-wants-to-be-like-the-cloud-not-in-it\">HPC wants to be like the cloud, not in it</a></li><li><a href=\"#cloud-is-expensive-but-not-for-the-reasons-most-think\">Cloud is expensive, but not for the reasons most think</a></li><li><a href=\"#although-sometimes-it-is\">...Although sometimes it is</a></li><li><a href=\"#influencing-the-cloud-is-hard\">Influencing the cloud is hard</a></li></ul></li><li><a href=\"#career\">Career</a><ul><li><a href=\"#people-matter\">People matter</a></li><li><a href=\"#company-culture-matters-too\">Company culture matters, too</a></li><li><a href=\"#being-good-at-things-isnt-always-a-job\">Being good at things isn't always a job</a></li><li><a href=\"#you-dont-have-to-be-your-employer\">You don't have to be your employer</a></li><li><a href=\"#happiness-sometimes-costs-money\">Happiness sometimes costs money</a></li></ul></li><li><a href=\"#final-thoughts\">Final thoughts</a></li></ul></ul>--><h2 id=\"hpc\">HPC</h2><p>Everything I did at Microsoft touched supercomputers in one way or another, and my day job was exclusively supporting Microsoft's largest AI training supercomputers. Despite that, I did a lot of moonlighting in support of Azure's Federal business, and this is how I justified giving talks at events like like <a href=\"https://sites.google.com/lbl.gov/nersc50-nug/home\">NERSC@50</a>, <a href=\"https://sc24.supercomputing.org\">SC</a>, and <a href=\"https://www.glennklockwood.com/garden/Salishan\">Salishan</a> in my last year. It's also what let me straddle both worlds: I had a rare, first-hand knowledge of how the <a href=\"https://www.glennklockwood.com/garden/systems/Eagle\">de facto largest supercomputers in the world</a> were built and used, and I had a front-row seat for how leaders in the traditional supercomputing world perceived (and sometimes misunderstood) what we were doing in the cloud.</p><p>Before I get into specific observations though, I should clarify some nomenclature that I will use throughout:</p><ul><li><strong>Supercomputers</strong> are the piles of compute nodes with a high-speed interconnect that are designed to solve one big problem in parallel. This is a generic term to describe the instrument, not its workload.</li><li><strong>HPC</strong>, <strong>traditional HPC</strong>, <strong>modsim</strong>, and <strong>scientific computing</strong> all refer to the ecosystem built around using something like MPI to solve a problem rooted in some type of science. Every big supercomputer run by DOE, procured through EuroHPC, and sited at the world-famous, government-funded supercomputer centers falls into this category.</li><li><strong>Cloud</strong>, <strong>hyperscale</strong>, and <strong>AI training</strong> all refer to the ecosystem built to train large language models. The supercomputers are run by hyperscale companies like Microsoft, Amazon, or Meta whose backgrounds have not historically been in the world of supercomputing.</li></ul><p>I realize that these are not very precise, but they're the easiest way to contrast what I learned inside Microsoft (a hyperscale cloud) with the world I came from prior (traditional HPC).</p><h3 id=\"hpc-wants-to-be-like-the-cloud-not-in-it\">HPC wants to be like the cloud, not in it</h3><p>When I left NERSC in May 2022, <a href=\"https://blog.glennklockwood.com/2022/05/life-and-leaving-nersc.html\">I speculated that the future of large-scale supercomputer centers</a> would be follow one of two paths:</p><ol type=\"1\"><li>They develop and squish cloud technologies into their supercomputers to make them more cloud-like, or</li><li>They abandon the idea of buying individual systems and instead enter into long-term relationships where flagship HPC systems are colocated inside cloud datacenters sited in places with low-cost, low-carbon power.</li></ol><p>I was hoping that the desire to continue building systems after passing the exascale milestone would make the next click-stop follow path #2, but early indications (across the global HPC landscape) are that the community has chosen path #1.</p><p>HPC centers around the world are embracing the idea of cloudifying on-prem supercomputers by adding virtualization, containerization, and integration with other services to enable complex workflows. And as a part of that, they're reinventing many of the technology integrations that have always been first-class citizens in cloud: CSCS added capabilities to create <a href=\"https://www.cscs.ch/publications/news/2024/new-research-infrastructure-alps-supercomputer-inaugurated\">\"versatile software-defined clusters\" on their latest Cray system, Alps</a>. NERSC's next system, Doudna, is envisioned to allow its users to \"<a href=\"https://www.vastdata.com/sharedeverything/how-nersc-is-rewriting-the-role-of-the-supercomputer\">move from programming the supercomputer to programming the datacenter</a>.\" But none of these systems are actually using commercial cloud services in non-trivial ways.</p><p>In the year or two that followed ChatGPT, the notion of large-scale supercomputers in the cloud was a green field, and cloud providers were open to chasing all sorts of silly ideas. This made it the ideal time for the leadership HPC computing community to get a seat at the hyperscale table. Although their budgets couldn't compete with AI, HPC centers could've drafted on the investments of AI buildout and offered the societal impacts of using GPUs for science as a nice complement to the societal impacts of using GPUs for AI training.</p><p>Much to my dismay, though, that window of opportunity was spent decrying the investment in hyperscale and AI rather than trying to exploit it; that window was the year of \"<a href=\"https://blog.glennklockwood.com/2024/05/isc24-recap.html#section11\">us versus them</a>.\" And unfortunately, that window has essentially closed as accountants and CFOs have now sharpened their pencils and are searching for returns on the investments made in GPU infrastructure. The intrinsic value of supercomputing infrastructure in the cloud has been reduced to the point where <a href=\"https://www.theregister.com/2024/10/31/microsoft_q1_fy_2025/\">Microsoft's CEO outright said they were turning away customers who just wanted to pay for GPU clusters</a>, because higher-quality revenue could be made from inferencing services that use those same GPUs.</p><p>So even if the HPC community woke up tomorrow and realized the long-term benefits of partnering with commercial clouds (instead of trying to copy them), I don't think cloud providers would respond with the same enthusiasm to meet in the middle now as they would have a year or two ago. I don't think this was a deliberate decision on behalf of the cloud providers, and they may not even fully realize this change. But the future of hyperscale supercomputing is rapidly crystallizing, and because HPC wasn't present in the solution, there's no room for it in the final structure.</p><h3 id=\"cloud-is-expensive-but-not-for-the-reasons-most-think\">Cloud is expensive, but not for the reasons most think</h3><p>It's been easy to write off the cloud as too expensive for HPC, and most people do silly math based on public list prices for VMs to justify their position. The narrative usually goes something like, \"<a href=\"https://info.ornl.gov/sites/publications/Files/Pub202373.pdf\">if a single GPU VM costs $40/hr, then running 10,000 of them for five years will cost 17X more than our on-prem supercomputer!</a>\" That's not how it works, and nobody pays that price. That $40/hr is the maximum possible price, and it includes the cost to the cloud provider of keeping nodes idle in the event that someone shows up and suddenly wants to use one on-demand.</p><p>But even if you cut out all the profit for the cloud provider and just look at the cost of the physical infrastructure, building a supercomputer in the cloud is just more expensive than putting a bunch of whitebox nodes into a traditional HPC datacenter. There's a couple reasons for this, and here are a couple in no particular order:</p><p><strong>High availability</strong>: Every cloud datacenter has redundant power, and most of them have <em>very</em> redundant power. This is provisioned independently of whatever goes inside of that datacenter, so when you deploy a 10 MW supercomputer inside a 10 MW cloud datacenter, that comes with at least 10 MW of backup diesel generators, UPSes, and the electrical infrastructure. HPC workloads don't really need this, but it's hard to deploy HPC in the cloud without a ton of generators and UPSes coming along for the ride. This is changing with AI-specific cloud datacenters now being built, but these AI datacenters still have way more redundant power than a typical on-prem HPC datacenter. Building a cloud datacenter with the minimal redundancy that a traditional HPC datacenter has would mean that facility couldn't ever be used for anything but HPC, and that would undercut the overall flexibility upon which cloud economics are built.</p><p><strong>Cloud-side infrastructure</strong>: Every compute node has to be attached to the frontend cloud network in addition to a backend high-speed network like InfiniBand, unlike a traditional supercomputer where nodes are only attached to one high-speed network. While the cost of the smart NIC in each node is just a couple hundred dollars, every cloud supercomputer has to have a complete frontend network built out to support every single compute node--that's a ton of switches, routers, and fiber that must be properly provisioned all the way up to the cloud region in which those nodes are deployed. This frontend network is what enables all the cool cloud features on every node (full SDN, integration with other cloud services, etc), but these features aren't generally worth their cost when running meat-and-potatoes HPC workloads like MPI jobs by themselves. Their value only really shines through when executing complex workflows that, for example, couple an MPI job with stateful services and globally accessible data sharing with fine-grained access controls, all fully automated through programmable APIs and full RBAC.</p><p><strong>AI-optimized system architecture</strong>: AI-optimized GPU supercomputers contain a bunch of components that your typical Cray or Eviden simply wouldn't have. I wrote about the <a href=\"https://www.glennklockwood.com/garden/differences-between-AI-and-HPC\">differences between AI and HPC supercomputers elsewhere</a>, but in brief, AI workloads specifically benefit from having tens of terabytes of local SSDs and all-optical (no copper) RDMA fabrics. These add to the COGS (cost of goods sold) of an AI-optimized supercomputer, meaning that that a supercomputer with a thousand GPUs designed for AI is going to be more expensive than one designed for scientific computing no matter where it's deployed. And cloud providers are all optimizing their supercomputers for AI.</p><p>There's a bunch of other cloud \"stuff\" that is required as well; every cloud region has a first footprint which is a LOT of general-purpose servers and storage that is required to support the basic cloud control plane. Before any user-facing cloud resources (including supercomputers) can be deployed, there has to be tens or hundreds of racks of this cloud \"stuff\" that is up and running. And although the cost of that first footprint is amortized over many customers in larger or older cloud regions, larger single-use infrastructures (like supercomputers) carry a proportionally larger fraction of the cost to deploy the first footprint.</p><p>So when you look at the cost of running a single compute node in a cloud supercomputer, there are a bunch of extra ingredients baked in that you wouldn't get by just signing a check over to an OEM:</p><ul><li>a high availability SLA, afforded in part by all those generators and UPSes</li><li>slick cloud service integrations, privacy features, virtual networking, afforded by that frontend cloud network</li><li>better performance for AI training or inferencing workloads, afforded by extra SSDs and all-optical interconnects</li><li>a bunch of other typical TCO stuff--the power consumed by the node, the opportunity cost of free floor tiles in your datacenter, and the engineers and technicians that keep it all running</li></ul><p>Ultimately, someone needs to pay for all of these extra ingredients. Cloud providers <em>could</em> just eat the costs themselves and sell the supercomputing service at a price comparable to what a customer would pay for an on-prem supercomputer--and sometimes they do. But this dilutes the profitability of the deal, and it increases the risks of the cloud provider losing money if unexpected issues arise during execution. Losing money is objectively bad business, so it's usually cloud customers who are paying for all these extra capabilities regardless of if they use them or not.</p><p>So if all you want to do is run big MPI jobs, and you have no use for the extra availability, cloud integrations, privacy and security, and programmable infrastructure, sure--the price per-node is going to be higher in the cloud than on-prem. You're paying for a bunch of features that you don't need.</p><h3 id=\"although-sometimes-it-is\">...Although sometimes it is</h3><p>Sometimes buying a supercomputer in the cloud is straight up more expensive because of the value it provides though. For example, I remember a case where a large AI company needed to train a big LLM on many thousands of GPUs, so they signed an agreement which gave them exclusive access to a cloud supercomputer that strongly resembled a specific GPU system in the DOE complex. Because I used to work in the DOE, I knew how much DOE paid to buy their GPU cluster, and I also knew that three years of maintenance was included in that cost.</p><p>What amazed me is what this AI company was willing to pay (roughly) the same price that DOE paid for their on-prem supercomputer, but in exchange, get exclusive access to a comparably capable cloud supercomputer (same GPUs model, similar GPU count, similar interconnect) for <em>one year only</em>. Put differently, being able to use a big, cutting-edge GPU cluster was worth up to 3x more to this AI company than it was to the DOE.</p><p>While it may sound like I'm spilling secrets here, the reality is that anyone working for a cloud provider wouldn't be able to tell which AI deal I was describing here--they all look like this, and they're all willing to spend significantly more than the HPC community for the same compute capability. This gives you a sense of the real value that AI companies place on all the benefits that cloud-based supercomputers can provide.</p><p>This isn't all bad for HPC, though. Every fat deal with an AI company means that there can be another deal with an HPC center that has slim margins. For example, let's say an AI company is willing to pay a billion dollars for a supercomputer whose TCO is only $330M--that means the cloud provider gets 67% margin. If the cloud provider's overall margin target is 50%, that means it can sell an identical supercomputer to an HPC customer at zero profit (for $330M) and still walk away happy. Thus, it is possible for the price of a supercomputer for HPC to be subsidized by all the money that the AI industry is throwing into supercomputing. Whether or not a cloud provider ever cuts deals like this is a business decision though--and as I said earlier, I don't think they're as open to silly ideas now as they used to be.</p><p>The real hurdle that I was never able to overcome out, though, is a result of the fact that there is finite expertise in HPC and AI in the world. HPC-AI is ultimately a zero-sum game, and every hour spent working with an HPC customer is usually an hour that isn't being spent working with a much more profitable AI customer. I constantly ran into this problem working in hyperscale AI; my full-time job was to deal with AI customers, but I enjoyed interacting with HPC customers too. As a result, I had to do a lot of my the HPC-specific work (preparing conference presentations, for example) on nights, weekends, and vacations. It was just hard to tell people that I couldn't help improve job uptime on a massive training run because I was preparing a talk for a workshop that, frankly, might be openly hostile to my message.</p><h3 id=\"influencing-the-cloud-is-hard\">Influencing the cloud is hard</h3><p>Because the difference in investment is so big between HPC and AI, many of the carrots that the HPC community has traditionally dangled in front of HPC vendors aren't very enticing to the hyperscale AI community. For example, both US and European HPC programs have relied heavily on non-recurring engineering (NRE) contracts with industry partners to incentivize the creation of products that are well-suited for scientific computing; <a href=\"https://www.energy.gov/articles/department-energy-awards-six-research-contracts-totaling-258-million-accelerate-us\">PathFoward</a> and <a href=\"https://research-and-innovation.ec.europa.eu/funding/funding-opportunities/funding-programmes-and-open-calls/horizon-2020_en\">Horizon 2020</a> both come to mind as well-funded, successful efforts on this front.</p><p>However, HPC is the only customer community that really tries to do this, and it echoes a time when the HPC community was at the forefront of scale and innovation. Nowadays, the prospect of accepting $1M/year NRE contract to implement XYZ is completely unappetizing to a hyperscaler; it would probably cost more than $1M/year just to figure out how a company with <a href=\"https://www.microsoft.com/investor/reports/ar24/\">$250 billion in annual revenue</a> can handle such an unusual type of contract and payment. Add to to this the weird intellectual property rules (like disentangling a <a href=\"https://www.energy.gov/gc/articles/advance-patent-waiver-wa2017-007?utm_source=chatgpt.com\">40% cost sharing advance waiver</a> for a tiny project within a multi-billion-dollar business), and it can become a corporate quagmire to go anywhere near NRE projects. Companies with well-insulated HPC silos can probably manage this better, but part of hyperscale economics is that everything overlaps with everything else as much as possible across supercomputing, general-purpose computing, hardware, and software.</p><p>As a result of this, I really struggled to understand how a $20M/year service contract and a $1M/year NRE contract is materially different from a $21M/year service contract in the cloud world. For most (non-HPC) cloud customers, the RFP comes in saying \"we need XYZ\" and some product manager notes customer demand for XYZ. If the demand is large enough, the feature winds up on roadmap, and the cloud provider develops it as a part of regular business. If there is no other demand, then an NRE contract isn't really going to change that; maintaining feature XYZ long-term will cost far more than a couple million dollars, so implementing it would be a bad decision. This isn't unique to cloud, for what it's worth; while there are some successful HPC NRE stories, there are far more NRE-originated products that had no product-market fit and were <a href=\"https://cug.org/proceedings/cug2016_proceedings/includes/files/pap105s2-file1.pdf\">simply abandoned</a> after the associated supercomputer was retired.</p><p>As best as I can tell, NRE has become a way for big HPC customers to maintain the illusion that they are influencing hyperscalers. A hyperscaler could propose some NRE, and an HPC buyer could fund it, and there could be weekly meetings where the two get together and pretend like they're collaborating and codesigning. The hyperscaler could write milestone reports, and they could attend quarterly business reviews with the customer. But this feels like an act. You simply can't move a $250B/year company that isn't solely organized around supercomputing with the lure of a couple million a year.</p><p>This is not to say that NRE and codesign have no purpose in HPC! I'm sure component vendors (GPUs, networking, and the like) can make minor tweaks that offer big upside for the HPC community. But I learned that, as in several other dimensions, the HPC community is being pushed towards buying whatever is already on the truck, and NRE isn't going to have the impact that it once did.</p><h2 id=\"career\">Career</h2><p>In addition to learning about how the hyperscale supercomputer world works in practice, my time at Microsoft exposed me to a segment of the supercomputing community that I didn't know existed: junior software engineers who were unwittingly thrown into the deep end of HPC straight out of college and were desperate to find their footing in both the technology and their careers overall. Maybe the most impactful work I did in the past three years was not technical at all, but instead came through some internal talks I gave on my professional journey in HPC and the one-on-one conversations that followed.</p><p>Since I've gotten such positive feedback when I talk and write about this aspect of HPC, I'll also share some things I've learned about choosing the right employer and job during my time at Microsoft.</p><h3 id=\"people-matter\">People matter</h3><p>I learned that the right team matters more than the right job. It is profoundly important to me that I get to work with people with the same level of passion and curiosity, even if we are working on different problems.</p><p>In retrospect, I realize that I have been very lucky that my career has progressed through organizations that were packed to the gills with people with whom I shared values. They wanted to go to conferences to share their work, they wanted to hear about how others are solving similar challenges, and they weren't afraid to present (and challenge) new ideas. As I learned over the last three years though, I think these traits are acutely concentrated in the HPC world since HPC itself originated from academia and a culture of independence and self-direction. They certainly aren't universal to all workplaces.</p><p>To be clear, I am not saying that my coworkers at Microsoft weren't passionate or curious. But I did learn that, at big tech companies, you can have a perfectly successful career by keeping your head down and cranking away at the tasks given to you. If the work changes one day, it's actually a virtue to be able to walk away from the old project and turn your complete attention to a new one. Did the company just <a href=\"https://www.theverge.com/2024/10/1/24259369/microsoft-hololens-2-discontinuation-support\">cancel the product you've been working on</a>? No problem. If you were good at writing code for Windows update, you'll probably be just fine at coordinating planned maintenances for supercomputers. A colleague of mine called these people \"survivors,\" because they will do the best they can with whatever they're given.</p><p>While this agility is great if you love programming, it can also engender numbness and dispassion for any specific application area. If a \"survivor\" can just as easily program for HoloLens as they can for GPU telemetry, they also likely don't really <em>care</em> about either HoloLens or GPUs. This isn't a bad thing, and I am certainly not passing judgment on people who don't care about GPUs. But it does mean that it's harder for someone who really cares about GPUs to connect with a teammate who really doesn't. And this has many knock-on effects in day-to-day work; it's only natural for people who share common values to help each other out, while relative strangers are less likely to go that extra mile. Finding that common ground to promote \"some person on team X\" to \"my trusted colleague on team X\" is that much harder.</p><p>This difficulty in finding my community amidst all the survivors is what led me to look outside of my company to find my people. I went to events like the <a href=\"https://www.olcf.ornl.gov/tag/smoky-mountain-conference/\">Smoky Mountains Conference</a> and <a href=\"https://sites.google.com/lbl.gov/nersc50-nug/home\">NERSC@50</a> and took the stage to literally beg the HPC community to give me a reason to work with them. By the letter of my job description, I was never supposed to be on stage; I was supposed to spending all my time behind my desk, thinking about the reliability of our biggest supercomputers. But I liked working with the people in the HPC community, and I liked working with our HPC sales organization, because we all shared common values; we were passionate about HPC and the mission of advancing scientific computing. So, I wound up spending a lot of time working on simple things with HPC folks and not enough time doing my day job.</p><h3 id=\"company-culture-matters-too\">Company culture matters, too</h3><p>In an organization where individuals don't often share a lot of common ground, I learned that it's incumbent upon everyone to make a deliberate effort to maintain a culture of working together and helping each other out. A positive workplace culture won't happen by itself across a massive organization. To this end, Satya has a bunch of corporate culture mantras that are often repeated to keep reminding people of the way employees should treat each other.</p><p>For example, he has a mantra of \"<a href=\"https://www.msn.com/en-us/money/other/how-satya-nadella-created-a-learn-it-all-culture-at-microsoft-to-help-it-become-a-3-trillion-powerhouse/ar-BB1qWoRY\">be a learn-it-all, not a know-it-all</a>.\" But I found that many people struggled to really understand how to do this in practice; when confronted with a tough problem (\"your database keeps timing out when we point a thousand nodes at it at once\"), it's often too easy to just be a know-it-all (\"nobody else does that, so you are doing it wrong\") rather than a learn-it-all (\"why are you doing it that way?\"). And the older a company is, the harder it is for decades-long veterans to maintain openness to new challenges in the silo they've built around themselves.</p><p>I've worked with HPC users for long enough to know that this attitude is pervasive anywhere you put a bunch of smart people with different perspectives into a room. However, it wasn't until I came to Microsoft that I learned that there's something to be gained by explicitly and repeatedly reminding people that they should strive to understand at least as much as they try to explain. Should I ever find myself in a leadership position, this is definitely a mantra I will carry with me and repeat to others, and I will credit my time at Microsoft with appreciating how to really live this mentality, not just parrot it.</p><h3 id=\"being-good-at-things-isnt-always-a-job\">Being good at things isn't always a job</h3><p>People tell me that I'm pretty good at a bunch of stuff: figuring out how technologies work, explaining complex concepts in understandable ways, and taking a critical look at data and figuring out what's missing. And I enjoy doing these things; this is why I post to <a href=\"https://blog.glennklockwood.com/\">my blog</a>, maintain <a href=\"https://www.glennklockwood.com/garden/\">my digital garden</a>, and love <a href=\"https://www.youtube.com/playlist?list=PLtPey-3r1oZS0S5pPcWq-L4yrT9-R0gIm\">getting on stage and giving presentations</a>. But people also say that, because I'm good at these things, there'd be no shortage of opportunities for me in the HPC industry should I ever go looking.</p><p>However, I've learned that a <em>job</em> has to be an amalgamation of <em>responsibilities</em> that create value, and connecting \"things I'm good at\" with \"things that need to be done\" is not always straightforward. For example, if I am <em>good at</em> learning things and share what I learned with others, what kind of jobs actually turn that into a <em>responsibility</em>?</p><ul><li><strong>Developers</strong> don't really do this at all. Their job is really to keep those git commits coming. Sometimes this requires learning new things, but writing blog posts or giving talks is not in the job description, so they don't count for much on performance reviews.</li><li><strong>Product managers</strong> do a little of this. I had to learn a few things and then repeat them a lot when I was a PM. Over and over. To customers, to executives, to partner teams. It was 5% learning and 95% sharing.</li><li><strong>Salespeople</strong> also do a little of this. They have to stay current on customer needs and product features, then repeat them a lot.</li><li><strong>System architects</strong> do a fair amount of this. I had to learn about what technologies are on the horizon, figure out how to piece them into an idea that could be implemented, then explain why it'd all be a good idea to others.</li><li><strong>Educators</strong> do a lot of this. The technology industry is always moving, so learning is required to stay up to date. They also get to be selective about the ideas worth sharing and downplay the rest.</li></ul><p>Each one of these roles has its own downsides too; for example, product managers and salespeople often have to nag people a lot, which I don't think anyone likes. And many of these roles require sharing knowledge with people who really don't want to hear it. After all, what customer is eager to talk to every salesperson who comes in the door?</p><p>Trying to find the ideal job is not just a matter of being good at many things; it's a matter of finding specific jobs that contain a maximal number of things you're good at and a minimal number of things you don't want to do. It's an NP-hard problem, and I've come to realize that the only way to solve it is through trial-and-error. I'm sure some people get lucky and figure out the optimal path on their first try, but for the rest of us, the only way to approach the optimal path is to continuously reflect and not longer on a known-suboptimal path for any longer than is necessary.</p><p>I've given up on trying to find the perfect job, because I've learned that it probably doesn't exist. I'm good at some things, I'm bad at some things; I enjoy some responsibilities, and I dislike some responsibilities. As with every other job I've had, I learned a lot about all four of these categories during my time at Microsoft, and my choice of next step has been informed by that. I don't expect it to be perfect, but I have high hopes that it will be a step in the right direction.</p><h3 id=\"you-dont-have-to-be-your-employer\">You don't have to be your employer</h3><p>When I left the government for a corporate job, one of my biggest worries was losing credibility with peers whose opinions I respected. It's easy to dismiss the viewpoint of someone at a large vendor with a rationalization like, \"of course they'd say that; it's their job,\" but I learned that the HPC community isn't so reductive. People are smart, and most were willing to engage with the quality of my ideas before checking the affiliation on my conference badge.</p><p>The trick, of course, was finding ways to share ideas in a way that didn't upset my corporate overlords but had substantive value to my audience. I think I figured this out, and in short, I found that leading with honesty and precision works best. The HPC community was built on sharing experiences and learnings about what does and doesn't work, so embracing that--rather than name-dropping products and making hyperbolic claims--seemed to keep me getting invited back to the HPC conferences and workshops that I wanted to attend.</p><p>I wasn't completely intentional in building whatever credibility I've gained over the last three years, but I was intentional in avoiding work that would clearly compromise it. I never want to be accused of misrepresenting the limits of my understanding, so I will never present a slide containing statements or plots that I can't substantiate. I also never want to be accused of misrepresenting the truth, so I am as forthright as possible in disclosing when I do (or don't) have an incentive to say something.</p><p>Because I stayed true to myself, I think I was the same person at Microsoft as I was at NERSC or SDSC. That continuity helped my peers quickly recalibrate after I became a vendor, and I think this helped me do more than if I had gone all-in on the role of a cloud spokesperson. Of course, there were times when I had to take on an employer-specific persona, but that's just business, and I've found that peers recognize that this is just a part of the game that we all must play.</p><p>The result of all this wasn't clear to me until after I started telling people I was leaving Microsoft. There are a bunch of HPC-specific projects I undertook on the side (e.g., reviewing and advising on research, serving on panels), and I started notifying people that I would have to find other Microsoft engineers to take over these obligations since I was leaving. Much to my surprise though, everyone responded the same way: the request to have me help was specifically to me, not my employer. Short of any conflicts of interest, they didn't care who employed me and valued my contributions regardless of who was signing my paychecks.</p><p>So, after three years working for an HPC vendor, I have learned that most people won't define you by your employer as long as you don't define yourself by your employer. It is possible to work for a company that sells HPC and still maintain your own identity as a person, but it requires thoughtful effort and a supportive (or indifferent!) employer. If you act like a company shill, you will be regarded as one, but not many jobs in industry actually <em>require</em> that to fulfill your responsibilities.</p><h3 id=\"happiness-sometimes-costs-money\">Happiness sometimes costs money</h3><p>I think most people would agree that, while money can't buy happiness, it certainly helps. What I didn't realize until recently, though, is a reciprocal truth: sometimes happiness costs money.</p><p>A year ago, I wrote about <a href=\"https://blog.glennklockwood.com/2024/08/how-has-life-after-leaving-labs-been.html#pay-good\">how the pay in industry compares to working at the national labs</a>, and I described how my golden handcuffs were structured. An optimist might say that these vesting schedules are a way to keep a happy employee from being lured away, but I think it's equally common that these are truly handcuffs. They are a constant reminder that, even in the darkest of days, there is a six-figure reason to grit one's teeth and persevere.</p><p>I've come to realize that there is an adverse correlation between a few factors:</p><ul><li>Smaller organizations offer more flexibility to mold a job around your preferences, because there is more work scope spread across fewer people.</li><li>Larger organizations can afford to offer larger total compensation, but flexibility is limited to the scope of any single team.</li></ul><p>I kind of thought about it like this:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>When I realized that I should explore other paths, I had to determine where in this continuum I wanted to wind up: do I care more about a fat paycheck, or do I care more about enjoying my day-to-day responsibilities? And once offers started coming in, exactly how much of a pay cut was I willing to take in exchange for the flexibility that I would receive?</p><p>By the time I handed in my resignation at Microsoft, I knew exactly how much this happiness was worth to me. Alternatively, I found out how much opportunity cost I was willing to pay for the ability (hopefully!) to reconnect with my day-to-day work. The calculus was an interesting exercise involving a bunch of Monte Carlo simulation which I won't detail here, but as it turns out, I was willing to pay a lot of money for the chance to do something that aligned more completely with what I wanted to do with the rest of my career. In the end, I gave up hundreds of thousands in unvested stock, and I am taking a six-figure pay cut in annual base pay when I start my next job. For me, though, this was a fair price to pay.</p><h2 id=\"final-thoughts\">Final thoughts</h2><p>After three years in the world of hyperscale supercomputing, I have come away with two major learnings that now shape how I think about the future.</p><p>On the technical front, I think the HPC community has chosen to keep going its own way and reinvent the cloud rather than work meaningfully with hyperscale cloud providers. There was a brief window of opportunity where <a href=\"https://idiomorigins.org/origin/if-the-mountain-wont-come-to-muhammad-then-muhammed-must-go-to-the-mountain\">the mountain may have actually come to Muhammed</a>, and the trajectory of scientific computing could have fundamentally changed to align with the growth trajectory of hyperscale AI. However, I don't think the HPC community was ready to take a big swing during those early days post-ChatGPT or do an earnest assessment of what that future could've looked like. I also worry that the window has closed, and the HPC community never even realized what was on the table.</p><p>On the career front, I've realized that success is multidimensional. Money is one axis, but so are impact, people, and purpose. The relative importance of each is not always obvious either; they only became clearer to me as I tried different jobs across the space. I've found that the ability to work with like-minded people and the opportunity to learn and share are the most important dimensions to me, but also I recognize that I am privileged in others. Finding stacks of money can be easy for those who work in AI, but there are no shortcuts to building (and retaining!) teams of great people. Anyone who can do the latter well should not be undervalued.</p><p>There's a lot more that I didn't have time to organize and write, but I have every intention of continuing to be myself, regardless of where I work, in the future. I will keep writing, posting, and talking about what I'm learning in supercomputing whenever I can. And along those lines, I hope that writing all this out helps others figure out what's important to them and where they want to go.</p>",
            "url": "https://hpc.social/personal-blog/2025/lessons-learned-from-three-years-in-cloud-supercomputing/",
            
            
            
            
            
            "date_published": "2025-07-11T05:26:00-06:00",
            "date_modified": "2025-07-11T05:26:00-06:00",
            
                "author": "Glenn K. Lockwood's Blog"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2025/isc-25-recap/",
            "title": "ISC'25 recap",
            "summary": null,
            "content_text": "I had the pleasure of attending the 40th annual ISC High Performance conference this month in Hamburg, Germany. It was a delightful way to take the pulse of the high-performance computing community and hear what the top minds in the field are thinking about.The main foyer of Congress Center Hamburg, and the view that greeted me on the first morning of ISC'25. The conference felt a little quieter than usual this year, and there didn't seem to be as many big ideas and bold claims as in years past. There was a new Top 10 system announced, but it was built using previous-generation Hopper GPUs. There were a record number of exhibitors, but many of the big ones (Intel, AMD; the big three cloud providers) were all absent. And while there were some exciting new technologies (like AMD MI350-series GPUs and Ultra Ethernet v1.0) debuting during the week, they actually debuted elsewhere and were simply referenced throughout the week's talks.This year's ISC really felt like the place where the big news of the industry was being repeated in the context of scientific computing instead of being stated for the first time. And maybe this is the future of HPC conferences: rather than being where new technology is announced, perhaps ISC will become where the scientific community tries to figure out how they can use others' technology to solve problems. That idea--figuring out how to make use of whatever the AI industry is releasing--was certainly pervasive throughout the ISC program this year. The conference's theme of \"connecting the dots\" felt very appropriate as a result; rather than defining new dots, the conference was all about trying to make sense of the dots that have already been drawn.I took plenty of notes to try to keep track of everything that was being discussed, and as has become tradition, I've tried to summarize some of the key themes in this post.Table of contentsZettascaleOzaki, Ozaki, OzakiTop500JUPITERHPC-AI system intersectionOther new entrantsHPC around the worldHPC in ChinaElsewhere in AsiaThe Middle EastExhibitorsCloud, or lack thereofParting thoughtsZettascaleNow that exascale is squarely in the rear-view mirror of HPC, an increasing number of high-profile speakers began pushing on zettascale as the next major milestone. Like the early days of exascale, most of the discourse was less about what can be achieved with zettascale and more about the technology challenges that need to be surmounted for HPC to continue moving forward. And to that end, using zettascale to justify tackling big hardware and software challenges wasn't a bad thing, but it felt like every talk about zettascale this year was still more fanciful than anything else.The opening keynote, \"HPC and Al - A Path Towards Sustainable Innovation\" was delivered by a duo of CTOs: Mark Papermaster (of AMD) and Scott Atchley (of Oak Ridge Leadership Computing Facility). It was a textbook keynote: it had inspiring plots going up and to the right that showed huge potential! It had scary linear extrapolations showing that staying the course won't do! It had amazing science results enabled by big iron! It even had a surprise product debut in MI355X! ChatGPT couldn't have come up with a better structure for a keynote presentation. But as is my wont, I listened to the talk with a little skepticism and found myself raising an eyebrow a few times.A part of Papermaster's presentation involved an extrapolation to zettascale by 2035 and claimed that HPC is approaching an \"energy wall:\"Extrapolating ten years on a semilog plot is a great way to cause alarm in people who don't pay close attention to axes.He specifically said that we'd need 1 GW per supercomputer to reach zettascale by 2035 on the current trajectory. He then used this to motivate \"holistic co-design\" as the only way to reach zettascale, and he went on to talk about all the same things we heard about leading up to exascale: increase locality and integration to reduce power and increase performance.While I agree that we should aspire to do better than a gigawatt datacenter, this notion that there is an \"energy wall\" that stands between us and zettascale is a bit farcical; there's nothing special about a 1 GW zettascale supercomputer, just like there was nothing special about 20 MW for exascale. You might argue that building a supercomputer that consumes all the power of a nuclear reactor might be fundamentally more difficult than one that consumes only 20 MW, and you'd be right--which is why the first gigawatt supercomputers probably aren't going to look like the supercomputers of today.Papermaster's \"energy wall\" slide reminded me of the great horse manure crisis of 1984, where people extrapolated from today using an evolutionary, not revolutionary, trajectory. If building a single gigawatt supercomputer is inconceivable, then build four 250 MW supercomputers and put a really fast network between them to support a single, synchronous job. The AI industry is already headed down this road; Google, Microsoft, and OpenAI have already talked about how they synchronously train across multiple supercomputers, and Microsoft announced their 400 Tb/s \"AI WAN\" for this last month as a means to enabling wide-area training.Granted, it's unlikely that the HPC community will be building massive, distributed supercomputers the way hyperscale is. But I was disappointed that the keynote only went as far as saying \"a gigawatt supercomputer is crazy, so we need codesign at the node/rack scale.\" Codesign to reach zettascale will probably require a whole new approach that, for example, accounts for algorithms that synchronize communication across multiple datacenters and power plants. The infrastructure for that is already forming, with the US developing its Integrated Research Infrastructure (IRI) and Europe shaping up to have over a dozen AI factories. Zettascale by 2035 may very well exist for the scientific computing community, but it'll probably look a lot more like hyperscale zettascale rather than a single massive building. A single machine plugged into a gigawatt nuclear reactor only happens if business-as-usual is extrapolated out another ten years as Papermaster did, and the codesign required to achieve that isn't very meaningful.Prof. Satoshi Matsuoka also gave a talk on the big stage about Fugaku-NEXT, which Japan has branded as a zettascale system. His vision, which will be realized before 2030, aims to deploy a single, 40 MW supercomputer (much like Fugaku was) where:10x-20x speedup comes from hardware improvements2x-8x speedup comes from mixed precision or emulation (more on this below)10x-25x speedup comes from surrogate models or physics-informed neural networksThe net result is a 200x-4000x speedup over Fugaku. His rationale is that this will result in a system that is effectively equivalent to somewhere between 88 EF and 1.7 ZF FP64. It's not literally doing that many calculations per second, but the science outcomes are equivalent to a brute-force approach using a much larger system.I thought this approach to reaching zettascale was much more realistic than the Papermaster approach, but it does require the scientific computing community to redefine its metrics of success. If HPL was a bad benchmark for exascale, it is irrelevant to zettascale since it's unlikely that anyone will ever run HPL on a zettascale system. At best, we'll probably see something like HPL-MxP that captures the 10x-20x hardware speedup and the 2x-8x mixed-precision or emulated FP64 reach hundreds of exaflops, but the 10x-25x from surrogate models will be domain-specific and defy simplistic ranking. If I had to guess, the first zettascale systems will be benchmarked through Gordon Bell prize papers that say things like \"simulating this result using conventional FP64 would have required over 1 ZF for 24 hours.\"Ozaki, Ozaki, OzakiAlthough Prof. Matsuoka evoked the 2x-8x speedup from mixed precision or emulation when claiming Fugaku-NEXT would be zettascale, he was far from the only speaker to talk about mixed precision and emulation. In fact, it seemed like everyone wanted to talk about emulating FP64, specifically using NVIDIA's low-precision tensor cores and the Ozaki scheme (or its derivatives). By the end of the week, I was absolutely sick of hearing about Ozaki.For the unindoctrinated, this Ozaki scheme (and similar methods with less-catchy names) is a way to emulate matrix-matrix multiplications at high precision using low-precision matrix operations. It's become so hot because, despite requiring more arithmetic operations than a DGEMM implemented using WMMA/MFMA instructions, it can crank out a ton of FP64-equivalent operations per unit time. This is a result of the ridiculously nonlinear increases in throughput of low-precision tensor/matrix cores on modern GPUs; for example, Blackwell GPUs can perform over 100x more 8-bit ops than 64-bit ops despite being being only 8x smaller. As a result, you can burn a ton of 8-bit ops to emulate a single 64-bit matrix operation and still realize a significant net speedup over hardware-native FP64. Matsuoka presented the following slide to illustrate that:Dr. Uchino's estimates of how many FP64 FLOPS one can emulate using INT8 as presented by Satoshi Matsuoka.Emulation offers a way for scientific apps that need high-precision arithmetic to directly use AI-optimized accelerators that lack FP64 in hardware, so it's worth talking about at conferences like ISC. But it seems like everyone wanted to name-drop Ozaki, and the actual discussion around emulation was generally a rehash of content presented earlier in the year at conferences like GTC25.While hearing about FP64 emulation and Ozaki schemes got tiring throughout the week, I had to remind myself that I hadn't even heard about Ozaki before September 2024 at the Smoky Mountains Conference. The fact that the Ozaki scheme went from relative algorithmic obscurity to being the star of the show in nine months is either a reflection of its incredible importance in scientific computing or a testament to the reach of NVIDIA's marketing.Cynically, I'll bet that NVIDIA is probably doing everything it can to make sure the world knows about the Ozaki scheme, and ISC was a part of that. When the datasheets for Rubin GPUs are released, I'll bet the performance table has a row claiming a bazillion FP64 FLOPS, and there will be a tiny footnote that clarifies they're citing emulated FP64 precision. They did it with structured sparsity, and I'm sure they'll do it for emulated DGEMM.Although the Ozaki scheme is perhaps over-hyped considering how narrow its applicability is to the broad range of compute primitives used in scientific computing, I do anticipate that it is the tip of the iceberg. If 2025 was the year of the Ozaki scheme, 2026 may be the year of the emulated FP64 version of FFTs, sparse solvers, stencils, or other key algorithms. We're seeing signs of that already; David Keyes and Hatem Ltaief both presented material at ISC on using mixed-precision matrix operations for other scientific problems, and I mentioned their work in my earlier GTC25 blog. I'm not sure \"the Keyes scheme\" or \"the Ltaief scheme\" is as catchy as \"the Ozaki scheme,\" but I expect to hear more about these other emulation techniques before ISC26.Top500On the topic of matrix-matrix multiplication, I can't get too much farther without talking about the Top500 list released at ISC. Although there was no new #1 system, Europe's first exascale system, JUPITER, made its sub-exascale debut. There were also a number of new entries in Top50, and surprisingly, many of them came from companies who offer GPUs-as-a-Service for AI training rather than the usual public-sector sites delivering cycles for scientific research. However, all the new entries were still using previous-generation Hopper GPUs despite huge Blackwell coming online, exposing a perceptible lag between the state of the art in supercomputers for AI and traditional HPC.As with last year, I felt a growing tension between what the Top500 list brings to the discussion and where the large-scale supercomputing industry is headed. As I wrote earlier, mixed-precision and emulated FP64 was a hot topic in the technical program, but the emphasis of the Top500 session was still squarely on bulk-synchronous FP64 performance. HPL-MxP awards were handed out, but they all wound up in the hands of systems who were also at the top of the regular HPL list. Nobody is submitting HPL-MxP-only scores, and there was no meaningful discussion about the role that the Ozaki scheme will play going forward in Top500's future.Opining about the long-term future of the Top500 list is a whole separate blog post though, so I'll focus more on what was covered at this year's session.JUPITERJUPITER was the only new entrant into the Top 10, and it posted at #4 with an average 793 PF over a hundred-minute run. Though it hasn't broken the 1 EF barrier yet, JUPITER is noteworthy for a few reasons:It is expected to be Europe's first exascale system. Given this HPL run used only 79% of the Booster Module's 5,884 GH200 nodes, some basic extrapolation puts the full-system run just a hair above 1 EF. Jülich will either have to run with 100% node availability or get a few extra nodes to exceed 1 EF though.JUPITER is also now the biggest NVIDIA-based supercomputer on Top500, pushing Microsoft's H100 SXM5 system (Eagle) down to #5. JUPITER is also Eviden's biggest system and a strong affirmation that Europe isn't dependent on HPE/Cray to deliver on-prem systems of this scale.JUPITER was also installed into a modular datacenter, an approach that is emerging as a preferred method for rapidly deploying large GPU systems in Europe. This setup allowed Jülich to place shipping container-like modules on a concrete foundation in just a few months. However, because the datacenter is form-fit to the JUPITER system without much extra space, it's impossible to take a glamor shot of the entire machine from far away. As a result, most photos of JUPITER show only the datacenter modules that wrap the supercomputer racks. For example, Prof. Thomas Lippert shared this photo of JUPITER during his presentation:JUPITER's modular datacenter as seen from a drone flying overhead.As Lippert was describing JUPITER, I couldn't help but compare it to the AI supercomputers I support at my day job. Like JUPITER, our supercomputers (like Eagle) aren't very photogenic because they're crammed into form-fitted buildings, and they are best photographed from the sky rather than the ground. For example, here's a photo of one of Microsoft's big GB200 supercomputers that I presented later in the week:A slide showing one of Microsoft's big GB200 supercomputers that I presented at the SuperCompCloud workshop later in the week. The big two-story building in the center houses GPUs, and the long white building on the right houses storage and CPU-only nodes.JUPITER may be the first exascale system listed on Top500 that doesn't have fancy rack graphics, but I don't think it will be the last.I also found myself wondering if these modular datacenters are trading short-term upsides with long-term downsides. While they accelerate deployment time for one-off supercomputers, it wasn't clear to me if these modular structures is reusable. Does the entire datacenter retire along with JUPITER after 5-7 years?Hyperscalers use modular datacenters too, but the modularity is more coarse-grained to support a wider variety of systems over multiple decades. They're also physically more capacious, allowing them to deploy more CDUs and transformers per rack or row to retrofit them for whatever power and cooling demands evolve into over the full depreciation life of the datacenter building.HPC-AI system intersectionAs with last year, Erich Strohmeier did a walkthrough of Top500 highlights, and he argued that \"hyperscale\" is defined as anything bigger than 50 MW, and therefore the Top500 list is hyperscale. It wasn't clear what value there was in trying to tie the Top500 list to hyperscale in this way, but there were a few ways in which Top500 is beginning to intersect with hyperscale AI.Foremost is the way in which some exascale systems have been appearing on the list: they first appear after HPL is run on a big but partially deployed machine, then six months later, the full-system run is listed. Aurora and JUPITER both follow this pattern. What's not obvious is that many massive AI supercomputers also do something like this; for example, the Eagle system's 561 PF run was analogous to Aurora's initial 585 PF run or JUPITER's 793 PF run. The difference is that systems like Eagle typically enter production training after that first big tranche of GPUs is online, so there is never an opportunity to run HPL as more of the system powers up. Instead, the production training job simply expands to consume all the new GPUs as new tranches come online.This iteration of the Top500 list also saw a number of bona fide commercial AI training clusters from smaller GPU-as-a-Service and \"AI factory\" providers post results, giving the public a view of what these systems actually look like:Nebius listed ISEG2 at #13 with a 624-node, 202 PF H200 SXM cluster, following their 2023 Top500 debut with a 190-node, 46 PF H100 SXM cluster. Nebius was spun out of Yandex, the Russian tech conglomerate.Northern Data Group debuted Njoerd at #26 with a 244-node H100 SXM cluster. Northern Data Group started out as a German bitcoin mining company.FPT debuted at #36 with a 127-node H200 SXM cluster and #38 with a 127-node H100 SXM cluster. FPT is a Vietnamese technology conglomerate.It's notable that none of these systems resemble the sovereign AI systems or EuroHPC AI Factories cropping up in Europe, which are attached to traditional HPC centers and built on familiar HPC platforms like Cray EX or BullSequana. Rather, they're essentially NVIDIA reference architectures that resemble DGX SuperPods but are stamped out by companies like Supermicro, Gigabyte, and ASUS.While it's nice of these GPU-as-a-Service companies to participate in the Top500 list, I did not see anyone from these companies in the technical program in any other way. And I did not see anyone from the bigger GPU-as-a-Service providers (CoreWeave, Crusoe, Lambda, etc) contributing either. Thus, while these companies are participating in Top500, it doesn't seem like they're genuinely interested in being a part of the HPC community.Other new entrantsIf you take a step back and look at the ten largest systems that made their debut at ISC'25, they broadly divide into two categories. Here's the list:RankSystemPlatformSite4JUPITER BoosterGH200Jülich11Isambard-AI phase 2GH200Bristol13ISEG2H200 SXM5Nebius15ABCI 3.0H200 SXM5AIST17Discovery 6GH200ExxonMobil18SSC-24H100 SXM5Samsung26NjoerdH100 SXM5Northern Data Group27ABCI-QH100 SXM5AIST33AI-03MI210Core4236FPT AI Factory JapanH200 SXM5FPTAside from Core42's weird MI210 cluster, every new big system was either GH200 (for traditional HPC) or H100/H200 SXM5 (for AI). This suggests a few interesting things:None of the AI cloud/GPUaaS providers are talking about GH200. It seems that GH200 is squarely for scientific computing, and Hopper HGX systems is preferred for AI at scale.Despite debuting on Top500 two years ago, H100 is still making its way into the hands of HPC and AI sites. This could mean one of several things:H100 is more affordable now (Jensen says he can't give them away),there was a huge backlog of H100 orders, orit's just taking some places a really long time to get H100 up and runningBlackwell is not relevant to HPC right now. There are no big Blackwell systems on this list, nor was Blackwell discussed in any sessions I attended during the week. This is despite large GB200 systems being public, up, and benchmarked. For example, CoreWeave, IBM, and NVIDIA ran MLPerf Training across 39 racks (624 nodes) of a GB200 NVL72 system named Carina just last month. They did not appear to bother with HPL, though.From all this, it seems like there is a definite lag forming between what qualifies as \"leadership computing\" to HPC people and AI people. Today's leadership HPC (Hopper GPUs) is yesterday's leadership AI, and today's leadership AI (Blackwell GPUs) isn't on the radar of leadership HPC yet. Maybe GB200 will begin appearing one or two years later as the AI people move on to Vera-Rubin.So, if I had to guess, I think the top-end of Top500 in 2027 could look like one of three things:It will contain HPC systems with state-of-the-art, HPC-specific variants of accelerators that are completely irrelevant to AI. Large AI training systems will simply disappear from the list, because HPL has ceased to be a meaningful measure of their capability. GB200/GB300 simply never appear on Top500.It will contain HPC systems with previous-generation Blackwell accelerators after Jensen (the chief revenue destroyer) gets on stage and tells the world that Blackwell is junk because Rubin is awesome. The AI industry gobbles up all the Rubin GPUs, and HPC picks up the scraps they leave behind.Top500 starts allowing FP64 emulation, and all bets are off on how ridiculous the top systems' numbers look. In this case, top systems just skip the 1-10 exaflops range and start debuting at tens of exaflops.I have no idea where things will go, but we're starting to see big HPC deals targeting Vera Rubin that line up with the same time Rubin will land for the AI industry in 2H2026. So maybe Blackwell is just a hiccup, and option #1 is the most likely outcome.HPC around the worldThough Blackwell's absence from Top500 was easy to overlook, China's continued absence was much more obvious. Even though no new Chinese systems have been listed in a few years now though, representatives from several Chinese supercomputing centers still contributed invited talks throughout the week.In that context, I appreciated how fully ISC embraces its international scope. I found myself attending a lot of \"HPC Around the World\" track sessions this year, partly because I work for a multinational corporation and have to stay aware of potential needs outside of the usual US landscape. But there's also been a sharp rise in the amount of serious HPC that is now occurring outside of the USA under the banner of \"sovereign AI,\" and I've been keen to understand how \"sovereign AI\" compares to the US-based AI infrastructure in which I work.Before getting too deep into that though, China is worth discussing on its own since they had a such prominent presence in the ISC program this year.HPC in ChinaFollowing the single-track opening keynote on the first day of ISC is the single-track Jack Dongarra Early Career Award Lecture, and this year's talk was given by awardee Prof. Lin Gan from Tsinghua University. In addition, Dr. Yutong Lu gave two separate talks--including the closing keynote--which shed light on the similarities and differences between how China and the US/Europe are tackling the challenges of exascale and beyond.China is in a position where it does not have access to US-made GPUs, forcing them to develop their own home-grown processors and accelerators to meet their needs for leadership computing. As a result, both speakers gave talks that (refreshingly) revolved around non-GPU technologies as the basis for exascale supercomputers. Although neither Gan nor Lu revealed anything that wasn't already written about in the Gordon Bell prize papers, I took away a few noteworthy observations:The most public Chinese exascale system is always called the \"New Sunway\" or \"Next Generation Sunway,\" never \"OceanLight\" as has been reported in western media. There still aren't any photos of the machine either, and Dr. Gan used stock diagrams of the predecessor Sunway TaihuLight to represent New Sunway. There was no mention of the Tianhe Xingyi/TH-3 supercomputer at all.Chinese leadership computing details remain deliberately obfuscated despite the openness to present at ISC. For example, Lu presented the following English-language table from the 2024 China Top100 HPC list:No.VendorSystemSiteYearApplicationCPU CoresLinpack (Tflops)Peak (Tflops)Efficiency (%)1Server ProviderSupercomputing system mainframe system, heterogeneous many-core processorSupercomputing Center2023computing service15,974,400487,540620,00078.72Server ProviderInternet Company Mainframe System, CPU+GPU heterogeneous many-core processorInternet company2022computing service460,000208,260390,00053.43Server ProviderInternet Company Mainframe System, CPU+GPU heterogeneous many-core processorInternet company2021computing service285,000125,040240,00052.14NRCPCSunway TaihuLight, 40960*Sunway SW26010 260C 1.45GHz, customized interconnectionNSCC-WX2016supercomputing center10,649,60093,015125,43674.25Server ProviderInternet Company Mainframe System, CPU+GPU heterogeneous many-core processorInternet company2021computing service190,00087,040160,00051.26NUDTTianhe-2A, TH-IVB-MTX Cluster + 35584*Intel Xeon E5-2692v2 12C 2.2GHz + 35584 Matrix-2000, TH Express-2NSCC-GZ2017supercomputing center427,00861,445100,67961.07Server ProviderInternet Company Mainframe System, CPU+GPU heterogeneous many-core processorInternet company2021computing service120,00055,880110,00050.88Server ProviderShenweiJing Supercomputer System, 1024*SW26010Pro heterogeneous many-core processor 390C MPE 2.1 GHzComputing Company2022scientific computing399,36012,91214,36289.99Server ProviderSupercomputing Center System, 992*SW26010Pro heterogeneous many-core processor 390C MPE 2.1 GHzSupercomputing Center2021scientific computing386,88012,56913,913.090.310BSCCC/IntelBSCCC T6 Section 5360*Intel Xeon Platinum 9242 homogeneous many-core processor 48C 2.3 GHz, EDRBSCCC2021computing service257,28010,83718,935.057.2The #1 system is almost definitely built on SW26010P processors just like the big New Sunway system that Gan discussed (15,974,400 cores / 390 cores per SW26010P = 40,960 nodes), but it's significantly smaller than the 39M cores on which the work Gan highlighted was run. Clearly, China's biggest systems aren't on their own Top100 list, and their #1 listed system only says its processors are \"heterogeneous many-core\" despite smaller entries explicitly listing SW26010P (Pro) processors.Chinese leadership computing struggles aren't being hidden. Lu specifically called out a \"lack of a new system\" in 2024, echoing earlier sentiments from other leaders in Chinese HPC who have referred to \"some difficulties in recent years\" and a \"cold winter\" of HPC. She also said that their leadership systems are \"relatively\" stable rather than trying to overstate the greatness of Chinese HPC technology. But as with above, she didn't get into specifics; by comparison, Scott Atchley (of Oak Ridge Leadership Computing Facility) specifically quoted a 10-12 hour mean time between job interrupt on Frontier after his keynote. Whether 10-12 hours is \"relatively stable\" remained unspoken.Performance portability wasn't a top-line concern despite how hard it seems to port applications to Chinese accelerators. SW26010P is weird in that it has a host core and offload cores with scratchpads, and its native programming model (Athread) is very CUDA-like as a result. Gan made it seem that China is investing a lot of effort into \"fine-grained optimizations\" using OpenACC and Athread, and he showed all the ways in which they're rewriting a lot of the kernels and decompositions in complex applications (like CAM) to make this work. This sounds like an performance portability nightmare, yet there wasn't much talk about Chinese equivalents to performance portability frameworks like Kokkos, RAJA, or alpaka.Lu did name-drop a few frameworks that unify HPC and AI performance portability from around the world:Yutong Lu's only reference to software that enhances portability and productivity. Not quite the same as what Kokkos, Raja, and alpaka aim to solve, though.However, these were more about aligning efforts across scientific computing and AI rather than enabling scientific apps to run seamlessly across China's different exascale accelerators.Application focus areas in China seem similar to everywhere else. Classical and quantum materials modeling, climate and ocean modeling, electronic structure calculations, and genomics were all mentioned by Gan and Lu in their talks. There was no mention of stockpile stewardship or any defense-related applications of HPC, though I'm sure China is using big supercomputers in these efforts just as US and European nations do. The only unusual application that I noticed was Gan's mention of implementing reverse time migration (RTM) on FPGAs; I've only ever heard of RTM in the context of oil exploration. Though I'm no expert, I didn't think many HPC centers spent a lot of time focusing on that technique. I do know KAUST has done some work optimizing RTM applications with Aramco in the space, but most other national supercomputing centers keep oil and gas at arm's length. Gan's RTM work may be related to earthquake modeling rather than petroleum, but it stood out nonetheless.Nobody talked about GPUs. Gan spent a healthy amount of time talking about applying FPGAs and NPUs to scientific problems, but these are areas of research that are on the fringes of mainstream HPC. I'm not sure if this reflected his own interests or priority research directions in China, but given that Chinese researchers cannot procure NVIDIA or AMD GPUs, perhaps FPGAs and NPUs are being pursued as a potential next-best-thing. Necessity truly is the mother of invention, and China might be the driver of a disproportionate amount of innovation around dataflow processing and reduced precision for modeling and simulation workloads.Nobody talked about storage either. I'm not sure if this suggests China has a lopsided interest in compute over holistic system design, or if they just talked about their biggest challenges (which are using home-grown accelerators productively). Granted, keynote speakers rarely talk about storage, but I didn't see much participation from China in any of the subsystem-specific sessions I attended either. This is particularly notable since, for a time, Chinese research labs were dominating the IO500 list with their home-made file systems. Networking was mentioned in passing in Lu's closing keynote, but not much beyond another example of technology fragmentation, and there were no specific Chinese interconnects being discussed during the week.China is in the thick of AI just like the rest of the world. Lu said that 30% of the cycles on their big HPC systems go to AI, which is right in line with anecdotes from other HPC sites that put their figures at somewhere up to 50%. She also presented the Chinese taxonomy of the three ways in which AI and scientific computing can mesh together: HPC for AI (training LLMs on supercomputers), HPC by AI (AI for system design and operations), and HPC and AI (AI in the loop with simulation). China is also neck-deep in figuring out how to exploit reduced precision (or \"intelligent computing,\" as Lu branded it) and has pivoted from being \"performance driven\" (which I took to mean HPL-driven) to \"target driven\" (which I took to mean scientific outcome-driven). This is consistent with their recent Gordon Bell prize win and non-participation in either Top500 or China Top100.China is embracing geo-distributed supercomputing and complex workflows, much like the US. Lu specifically called out \"Computility Net,\" a catchy name that sounded a lot like the US DOE's Integrated Research Infrastructure (IRI). She described it as a national effort to combine supercomputing with \"commodity IT\" resources (perhaps Chinese cloud?) to enable \"resource sharing\" through a \"service grid.\" In her closing keynote, she even name-dropped IRI:The Chinese vision for Computility Net, which seems analogous to the US Integrated Research Infrastructure, as presented by Yutong Lu.She did liken Computility to both IRI in the US and PRACE in the EU though, and in my mind, PRACE is nothing like IRI. Rather, PRACE is more like TeraGrid/XSEDE/ACCESS in that it federates access to HPC systems across different institutions, whereas IRI's ambition is to tightly integrate computational and experimental facilities around the country. But from the above slide, it sounds like Computility Net is closer to IRI since it is coupled to \"Supercomputing internet\" (akin to ESnet?) and bridging compute and data across eastern and western China.Elsewhere in AsiaAlthough Chinese researchers headlined a few sessions at ISC, a number of other Asian nations presented their national supercomputing strategies as well. Japan and Korea have mature, world-class HPC programs, but I was surprised to see how ambitious India has become to catch up. Smaller nations were also represented, but it was clear to me that their focus is spread across midrange HPC, partnering with large centers in Korea/Japan, and innovating around the edges of supercomputing. And perhaps unsurprisingly, every nation represented had a story around both quantum computing and artificial intelligence regardless of how modest their production modsim infrastructure was.India appears to rapidly catching up to the US, Europe, and Japan much in the same way China was fifteen years ago. Representatives from C-DAC, the R&amp;D organization that owns the national supercomputing mission in India, gave a far-reaching presentation about India's ambition to achieve exascale by 2030. Their current strategy appears to be broad and capacity-oriented, with forty petascale clusters spread across India for academic, industrial, and domain-specific research. They have a comprehensive, if generic, strategy that involves international collaboration in some regards, reliance on open-source software to fill out their HPC environment story, and home-grown hardware and infrastructure:India's ambitious strategy towards exascale in 2030. This slide has it all, from home-grown CPUs and networks to five systems deployed in six years.I was surprised to hear about their ambitions to deploy their own CPUs and interconnect though. India is pursuing both ARM and RISC-V for their own CPUs for a future 200 PF system, and they're already deploying their \"InfiniBand-like\" interconnect, TRINETRA, which uses funny NICs with 6x100G ports or 10x200G ports rather than fewer, faster serdes. I didn't hear mention of their AI acceleration plans, but rolling their own commercialized CPU and interconnect in itself is a lot to bite off. Given that India is the world's fastest growing economy though, these plans to go from 20 PF in 2025 to 1 EF in 2030 may not be that far-fetched. Perhaps the Indian national strategy will become clearer during the inaugural Supercomputing India 2025 conferece this December.The Korea Institute of Science and Technology Information also took the stage to describe their next national supercomputer, KISTI-6, which was first announced in May 2025. It will be a 588 PF Cray EX254n system with 2,084 nodes of GH200, similar to Alps and Isambard-AI. This is quite a step up from its predecessor, which was an air-cooled KNL system, but it's unlikely it will unseat Fugaku; the 588 PF number cited appears to be the sum of 2,084 GH200 nodes, 800 Turin CPU nodes, and 20 H200 SXM5 nodes. The HPL score of its GH200 nodes will place it below Alps and somewhere around 350 PF, likely joining a flood of multi-hundred-petaflops GH200 systems that will appear between now and ISC26.Singapore (NSCC) and Taiwan (NCHC) both presented their national programs as well, but they appear to be much more nascent, and the size of their HPC infrastructure was presented as aggregate capacity, not capability. Their strategies involve partnership with Japan or Korea, but both had specific carveouts for both sovereign AI and quantum computing. Interestingly, their use cases for AI both had a strong story about training models that understood the diversity of languages and dialects represented in their nations. For example, it is not unusual for people to switch languages or dialects mid-sentence in Singapore, and the big Western models aren't designed for that reality. Similarly, Taiwan has 16 indigenous tribes with 42 dialects. It seemed like enabling LLMs that reflect the breadth languages used in Singapore and Taiwan have become the responsibility of these nations' respective national supercomputing efforts.That said, that noble mission didn't seem to be matched with substantial training infrastructure; these localized models will be relying on a couple hundred GPUs here and there, wedged into existing HPC centers. Thus, these sovereign models are probably going to be fine-tuned variants of open models, aligning with my earlier observation that these smaller nations will be innovating around the edges of HPC and AI.What was missing? Although Vietnam, Thailand, Malaysia, and other Asian nations have strong HPC programs centered around industrial uses, they were not represented in ISC's HPC Around the World track. Also absent was any meaningful discussion around cloud; while everyone had a throwaway line about cloud in their presentations, the fact that the only big clouds in Asia are Chinese and American probably makes it unappealing to integrate them into the core of these nations' national HPC strategies. Speaking from experience, this is quite different from the attitudes of commercial HPC users across Asia who are all too happy to let someone else run HPC datacenters for them.The Middle EastAlthough KAUST has been a world-class HPC center in the Middle East for the past fifteen years, AI seems to be where the majority of new investment into HPC is going.In describing new efforts in Saudi Arabia, Prof. David Keyes casually mentioned the Saudi HUMAIN effort, which will build 500 MW of datacenter capacity and 18,000 GB300 GPUs, after describing the Shaheen-3 GH200 upgrade that \"might (barely)\" put it back in the Top20 by SC'25. Similarly, Dr. Horst Simon walked through a few of Abu Dhabi's university clusters (each having dozens of GPU nodes) after skating through an announcement that a 5 GW AI campus was also being built in Abu Dhabi. The gap between investment in AI and investment in HPC was striking.I also had a brief conversation with someone from one of the major Abu Dhabi universities, and I was very surprised to find that I was talking to a real AI practitioner--not an HPC person moonlighting in AI--who spoke at the same depth as the customers with whom I work in my day job. The nature of his work made it clear to me that, despite his university not having a Top500 system, he was familiar with running training and inference at scales and with sophistication that is far beyond the experience of most ISC attendees.These interactions led me to the conclusion that the Middle East's approach to \"sovereign AI\" is quite different from Europe's. Rather than building HPC systems with GPUs, letting HPC centers operate them, and calling them sovereign AI platforms, nations like Saudi Arabia and UAE are keeping HPC and AI separate. Like in the US, they are going straight to hyperscale with AI, and they have no preconceived notion that anything resembling a supercomputer must be hosted at a supercomputer center.Of course, only nations like Saudi Arabia and UAE can afford to do this, because they have trillion-dollar sovereign wealth funds to invest in massive infrastructure buildout that doesn't isn't contingent on public consensus or the latest election cycle. Just as UAE's Core42 can build a 5 GW datacenter campus with little oversight, these nations can easily mis-step and invest a ton of money in an AI technology that turns out to be a flop. In the end, it seems like these Middle Eastern nations are willing to take bigger risks in how they build out their sovereign AI infrastructure, because they are largely starting from a blank sheet of paper. They aren't limiting themselves to 20 MW supercomputers like the HPC world had.All things being equal, this might turn out to be an advantage over other nations who are more hesitant to deviate from the tried-and-true course of buying a Cray or a Bull, sticking some GPUs in it, and calling it AI. If these Middle Eastern nations do everything right, they stand to get a lot further and move a lot faster in sovereign AI than Europe, and it'll be fascinating to see how quickly they catch up with the sort of frontier AI research being done private industry. But, as with the US AI industry, it doesn't seem like these AI practitioners are going to be attending ISC in the same way European sovereign AI folks do; the roads of HPC and AI seem to run parallel without intersecting in the Middle East.ExhibitorsISC had a record number of exhibitors this year, and as usual, I tried to set aside at least an hour or two to walk the floor and see what technologies are on the horizon. This year, though, the exhibit hall was not a great representation of the rest of the conference. Everyone I talked to about the exhibit said one of two things:There are a LOT of quantum companies.A lot of big companies were noticeably absent.It also didn't feel like the biggest exhibit ever, partially because of #2, and partially because many of the exhibitors--one in five--was exhibiting for the first time this year. This meant a lot of the booths were small and barebones, and many of them belonged to either companies at the periphery of HPC (such as companies that make dripless couplers for liquid cooling) or small startups who just had a desk, a few pens, and some brochures.On the first point, it was true--quantum computing was well represented, with 22% of exhibitors identifying as being involved in the field in some form. In fact, quantum felt over-represented, since the ISC technical program certainly didn't have such a large fraction of talks on quantum computing topics. I didn't have time to actually talk with any of these quantum companies though, so wasn't able to get a sense of why the startup ecosystem around quantum computing was so rich in Europe as compared to the US.While there was an abundance of quantum this year, a number of the big HPC and HPC-adjacent companies were noticeably absent:Amazon, Azure, and Google did not have booths despite having booths last year. Amazon and Google still sponsored the conference at the lowest tier (bronze) though, while Microsoft did not sponsor at all.Intel had neither booth nor sponsorship despite having the #3 system on Top500. I don't think they held a party this year, either. AMD didn't have a booth, but they sponsored (and gave the opening keynote!)WEKA neither had a booth nor sponsored the conference this year, although they were the leading sponsor of the Student Cluster Competition. Competitors DDN, VAST, Quobyte, and BeeGFS all had booths, but only VAST sponsored. Curiously, Pure and Scality, which do not big footholds in leadership HPC, did both booths and sponsorship.These companies who chose not to have a booth still sent people to the conference and were conducting meetings as usual, though. This suggests that there's something amiss with how large companies perceive the return on investment of having a booth at ISC. I don't have any insider knowledge here, but I was surprised by the pullback since ISC has historically been very good at incentivizing attendees to walk through the expo hall by putting it between the technical sessions and the food breaks.As I walked the exhibit floor, I found that prominent booths spanned the whole HPC stack: software, system integrators, component makers (CPUs, GPUs, HBM and DDR, and SSD and HDD), and datacenter infrastructure were all exhibiting. The most eye-catching booths were those with big iron on display: HPE/Cray had a full EX4000 cabinet and CDU on display, and there were a few Eviden BullSequana nodes floating around.The Cray EX4000 cabinet (right) and its CDU (left) on display at the ISC'25 exhibition hall. One of the most eye-catching displays, even they've been on display at ISC and SC for a few years now.Sadly, though, there were no full BullSequana X3000 racks on display. I've still never seen one in real life.Infrastructure companies like Motivair (who manufactures the CDUs for Cray EX) and Rittal (which I know as a company that manufactures racks) also had big liquid-liquid head exchangers on display with shiny steel piping. Here's a smaller version of the Cray EX CDU that Motivair was displaying:A close-up view of a smaller liquid-liquid heat exchanger CDU on display at the Motivair booth right next to HPE's. Strangely, the mechanics of these systems dovetails with what I've learned as a part of my other hobby outside of HPC, which is operating a multi-family residential high-rise.I got to chatting with some good folks at Motivair, and I learned that the 1.2 MW variant that is used with Cray EX has a 4\" connection--the same size as the water main in my coop. Since I recently helped with the replacement of my building's water main, this led me down a rabbithole where I realized that the flow rates for this CDU is roughly the same as my apartment building too, which is to say, a single Cray CDU moves as much fluid as a 55-unit apartment building. Incidentally, a single Cray EX cabinet supports roughly the same electrical capacity as my 55-unit building too--I am in the process of replacing our 1,200 A service panel, which comes out to about the same 400 kVA as fully loaded EX.Aside from the Cray cabinets and CDUs, which are no longer new to ISC, I couldn't put my finger on any particularly outstanding booths this year though. The exhibit felt like a sea of smaller companies, none of which really grabbed me. This isn't to say that big vendors were wholly absent though. Despite not having booths, all three big cloud providers threw parties during the week: AWS and NVIDIA teamed up on a big party with over a thousand registrants, while Google and Microsoft held smaller parties towards the end of the week. HPE also threw a lovely event that was off the beaten path along the Elbe, resulting in a less-crowded affair that made it easy to catch up with old friends.I may be reading too much into this year's exhibit, but it felt like ISC might be transforming into an event for smaller companies to gain visibility in the HPC market, while larger companies apply their pennies only in the parts of the conference with the highest return. Whether a company chose to have a booth, sponsor the conference, and/or throw a party seemed to defy a consistent pattern though, so perhaps other factors were at play this year.Cloud, or lack thereofBecause I work for a large cloud service provider, I attended as many cloud HPC sessions as I could, and frankly, I was disappointed. The clear message I got by the end of the week was that Europe--or perhaps just ISC--doesn't really care about the cloud. This is quite different from the view in the US, where the emergence of massive AI supercomputers has begun to shift opinions to the point where the successor to the Frontier supercomputer at OLCF might wind up in the cloud. I suppose cloud is a lot less attractive outside of the US, since all the major cloud providers are US corporations, but the way in which cloud topics were incorporated into the ISC program this year sometimes felt like a box-checking exercise.For example, I attended the BOF on \"Towards a Strategy for Future Research Infrastructures\" which I expected to be a place where we discussed the best ways to integrate traditional HPC with stateful services and other workflow components. While cloud was mentioned by just about every panelist, it was almost always in a throwaway statement, lumped in with \"the edge\" or cited as a vague benefit to \"new workflows and interactive analysis\" with no further detail. One speaker even cited egress fees as a big challenge which, to me, means they haven't actually talked to a cloud provider in the last five to ten years. If egress fees are what stop you from using the cloud, you're talking to the wrong account team.I get it though; there are times where cloud often doesn't offer enough obvious benefit for HPC to justify the effort required to figure it out. In those cases, it's incumbent on cloud providers to provide a better story. But I was also disappointed by the invited session called \"Bridging the Gap: HPC in the Cloud and Cloud Technologies in HPC,\" which I hoped would be the place where cloud providers could make this case. Instead, only two of the three CSPs were even invited to speak, and it was clear that the speakers did not all get the same assignment with their invitations. Granted, the CSP for whom I work was the one not invited (so I came in a little biased), but I was surprised by how differently each speaker used their time.Dr. Maxime Martinasso from CSCS gave a talk from the perspective of trying to add cloud-like capabilities to a supercomputer, which is a recurring pattern across a number of sites (including many in the US DOE) and projects. He explained the way they're creating an infrastructure-as-code domain-specific language that sits on top of Alps, their Cray EX system, to give users the ability to bring their own software stacks (all the way down through Slurm) to the supercomputer. It was clearly a ton of work on CSCS's part to develop this capability, and yet the talk's \"future work\" slide contained a bunch of features which those of us in the cloud would consider \"P0\"--priority zero, or essential for a minimum viable product.By the end of Martinasso's talk, I realized that CSCS's perspective is that, unlike commercial cloud, these cloudy features aren't P0; having a supercomputer on the floor is. He made the case that CSCS has a need to explore diverse computing architectures and accelerators (as evidenced by the five different node types in Alps!), and putting them all on a single RDMA fabric isn't something any cloud provider will do. As a result, adding any new cloud-like capability to the heterogeneous supercomputer is just gravy, and the fact that true cloud is more \"cloudy\" than Alps is irrelevant since the cloud will never support the intra-fabric heterogeneity that Alps does.The other two speakers represented big cloud providers, and their talks had a bit more product pitch in them. One speaker talked through the challenges the cloud is facing in trying to fold supercomputing principles into existing cloud infrastructure (a theme I repeated in my talk later in the week) before talking about specific products that have arisen from that. It touched on some interesting technologies that the HPC world hasn't yet adopted (like optical circuit switching--super cool stuff for programmable fabrics), and I learned a few things about how that provider might bring new HPC capabilities to the table for specific workloads.The other speaker, though, presented a textbook pitch deck. I've give almost the same exact presentation, down to showing the same sort of customer stories and product comparison tables, during customer briefings. Execs in the audience would eat it up while engineers' eyes would glaze over, and having to do that song and dance is partly why I didn't make it as a product manager. I was incredulous that such a presentation was an invited talk at one of the most prestigious HPC conferences in the world.This is not to say I was mad at the speaker. He did exactly what one would expect from a leader in the sales side of an organization, hitting all the notes you'd want in a textbook pitch aimed at the C-suite. Rather, I was disappointed by the choice by the session organizers; when you invite someone whose job is driving business at one of the largest cloud providers to speak, you should fully expect a broad and salesy presentation. I don't think it's a stretch to say that most ISC attendees aren't looking for these sorts of high-level talks designed for enterprise decision-makers; they want insight and technical depth.Was I miffed that a competitor got to give a twenty-minute sales pitch during a session at which I wasn't invited to speak? Absolutely. And do I think I could've given a talk that even the most ardent cloud-hater would find something interesting in it? Probably. But since that didn't happen, the best I can do is complain about it on the Internet and hope that next year's program committee puts more care into organizing an invited speaker session on cloud and HPC.Thankfully, I was given the opportunity to talk a little about my work at the SuperCompCloud workshop on Friday. That workshop felt like what the \"Bridging the Gap\" invited session should've been, and there were roughly equal parts of presentations on adding cloud-like features to their HPC infrastructure and adding HPC-like features to cloud infrastructure. From my perspective, the workshop was great; I got to see how traditional HPC centers are adopting cloud practices into their operations, and I could explain how we overcame some of the challenges they're facing in Azure. But to my point at the outset of this section--that Europe doesn't really care about the cloud--the majority of speakers at SuperCompCloud were American.Parting thoughtsAs I said at the outset, there were way more sessions that I missed than I attended. In addition, a lot of the big headlines of the week were coincident with, not made at, the conference. A few noteworthy announcements during the week that I won't go into detail about include:£750M was awarded to EPCC to deploy what sounds like the UK's first exascale system. This announcement's overlap with ISC was a total coincidence, so EPCC didn't have many details to share.The Ultra Ethernet Consortium announced the long-awaited version 1 of its spec. I'm not sure how relevant this is to HPC yet, but given how many networking talks compared themselves against InfiniBand, I think there's a lot of appetite for a high-performance, non-proprietary alternative.Sadly, HPC_Guru announced his retirement mid-week as well. It's not clear this was deliberately timed with ISC, but it was acknowledged on the big stage during the ISC closing statements and resulted in a lot of recognition online. I credit HPC_Guru, whoever he is, with a lot of the success I've enjoyed in my career, as he amplified my voice as far back as 2009 when I first started on Twitter. Maybe with his retirement, I should try to do for others what he did for me.And along the lines of reflecting back over the years, this was ISC's 40th anniversary, and the organizers had a few wonderful features to commemorate the milestone. Addison Snell organized a panel where a variety of attendees got to discuss the impact that the conference has had on them over the past 40 years, and I was delighted to find that I was not the only person to reflect back on how ISC has shaped my career. As critical as I can be of specific speakers and sessions when I write up these notes, I do hope it goes without saying that I wouldn't bother doing all this for a conference that wasn't deeply engaging and rewarding to be a part of.Going back to this year's theme of connecting the dots, I think it's apt. Some ways in which HPC connected dots at ISC this year were obvious; the conference brought together people with a common interest in high-performance computing from across 54 countries and seven continents this year. But this year's conference also made it clear that the role of HPC going forward may be connecting the dots between different technologies being developed for AI, cloud, enterprise, and other markets and the problems in scientific computing that need to be solved.The latest and greatest Blackwell GPUs barely registered at ISC this year, and the HPC community seems OK with that now. Instead of the focus being on the absolute top-end in high-performance accelerators, HPC's focus was on connecting the dots between last generation's GPUs and today's grand challenges in science. Instead of showcasing the newest innovations in secure computing in the cloud, HPC's focus was in connecting the dots between a few relevant pieces of zero trust and big-iron on-prem supercomputers.HPC has always been about figuring out ways to use stuff invented for someone else to solve scientific challenges--connecting the dots. Beowulf clusters started that way, GPGPU computing started that way, and emulating DGEMMs (and other primitives) on AI accelerators will probably follow the same pattern. But different nations are drawing different lines between the dots; while the US might draw a shorter line between commercial cloud and HPC at scale, Europe is drawing shorter lines between HPC for scientific computing and HPC for sovereign AI.If we accept that connecting the dots may be where the HPC community can make the most impact, then it's fitting that ISC chose to carry forward the theme of \"connecting the dots\" into ISC'26. This break from the tradition of introducing a new tagline each year suggests that, at times, optimizing what we already have can take us further than than pursuing something completely new. After 40 years, ISC remains not only a showcase of innovation, but a reflection of how the HPC community (and its role in the technology landscape) is evolving. If we continue to embrace this theme of stitching together breakthroughs instead of spotlighting them individually, the HPC community is likely to be more relevant than ever alongside--not in spite of--the overwhelming momentum of hyperscale and AI.",
            "content_html": "<p>I had the pleasure of attending the 40th annual ISC High Performance conference this month in Hamburg, Germany. It was a delightful way to take the pulse of the high-performance computing community and hear what the top minds in the field are thinking about.</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure><figcaption class=\"image-caption\">The main foyer of Congress Center Hamburg, and the view that greeted me on the first morning of ISC'25. </figcaption></figure></div><p>The conference felt a little quieter than usual this year, and there didn't seem to be as many big ideas and bold claims as in years past. There was <a href=\"https://www.theregister.com/2025/06/10/jupiter_europes_top_super/\">a new Top 10 system announced</a>, but it was built using previous-generation Hopper GPUs. There were a <a href=\"https://isc-hpc.com/the-isc-2025-exhibition-sets-new-records/\">record number of exhibitors</a>, but many of the big ones (Intel, AMD; the big three cloud providers) were all absent. And while there were some exciting new technologies (like <a href=\"https://www.tomshardware.com/pc-components/gpus/amd-announces-mi350x-and-mi355x-ai-gpus-claims-up-to-4x-generational-gain-up-to-35x-faster-inference-performance\">AMD MI350-series GPUs</a> and <a href=\"https://ultraethernet.org/ultra-ethernet-consortium-uec-launches-specification-1-0-transforming-ethernet-for-ai-and-hpc-at-scale/\">Ultra Ethernet v1.0</a>) debuting during the week, they actually debuted elsewhere and were simply referenced throughout the week's talks.</p><p>This year's ISC really felt like the place where the big news of the industry was being repeated in the context of scientific computing instead of being stated for the first time. And maybe this is the future of HPC conferences: rather than being where new technology is announced, perhaps ISC will become where the scientific community tries to figure out how they can use others' technology to solve problems. That idea--figuring out how to make use of whatever the AI industry is releasing--was certainly pervasive throughout the ISC program this year. The conference's theme of \"connecting the dots\" felt very appropriate as a result; rather than defining new dots, the conference was all about trying to make sense of the dots that have already been drawn.</p><p>I took plenty of notes to try to keep track of everything that was being discussed, and as has become tradition, I've tried to summarize some of the key themes in this post.</p><h2 style=\"text-align: left;\">Table of contents</h2><ul><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#zettascale\">Zettascale</a></li><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#ozaki-ozaki-ozaki\">Ozaki, Ozaki, Ozaki</a></li><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#top500\">Top500</a><ul><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#jupiter\">JUPITER</a></li><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#hpcai-system-intersection\">HPC-AI system intersection</a></li><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#other-new-entrants\">Other new entrants</a></li></ul></li><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#hpc-around-the-world\">HPC around the world</a><ul><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#hpc-in-china\">HPC in China</a></li><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#elsewhere-in-asia\">Elsewhere in Asia</a></li><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#the-middle-east\">The Middle East</a></li></ul></li><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#exhibitors\">Exhibitors</a></li><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#cloud-or-lack-thereof\">Cloud, or lack thereof</a></li><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#parting-thoughts\">Parting thoughts</a></li></ul><h2 id=\"zettascale\">Zettascale</h2><p>Now that exascale is squarely in the rear-view mirror of HPC, an increasing number of high-profile speakers began pushing on zettascale as the next major milestone. Like the early days of exascale, most of the discourse was less about what can be achieved with zettascale and more about the technology challenges that need to be surmounted for HPC to continue moving forward. And to that end, using zettascale to justify tackling big hardware and software challenges wasn't a bad thing, but it felt like every talk about zettascale this year was still more fanciful than anything else.</p><p>The opening keynote, \"HPC and Al - A Path Towards Sustainable Innovation\" was delivered by a duo of CTOs: Mark Papermaster (of AMD) and Scott Atchley (of Oak Ridge Leadership Computing Facility). It was a textbook keynote: it had inspiring plots going up and to the right that showed huge potential! It had scary linear extrapolations showing that staying the course won't do! It had amazing science results enabled by big iron! It even had a surprise product debut in MI355X! ChatGPT couldn't have come up with a better structure for a keynote presentation. But as is my wont, I listened to the talk with a little skepticism and found myself raising an eyebrow a few times.</p><p>A part of Papermaster's presentation involved an extrapolation to zettascale by 2035 and claimed that HPC is approaching an \"energy wall:\"</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure><figcaption class=\"image-caption\">Extrapolating ten years on a semilog plot is a great way to cause alarm in people who don't pay close attention to axes.</figcaption></figure></div><p>He specifically said that we'd need 1 GW per supercomputer to reach zettascale by 2035 on the current trajectory. He then used this to motivate \"holistic co-design\" as the only way to reach zettascale, and he went on to talk about all the same things we heard about leading up to exascale: increase locality and integration to reduce power and increase performance.</p><p>While I agree that we should aspire to do better than a gigawatt datacenter, this notion that there is an \"energy wall\" that stands between us and zettascale is a bit farcical; there's nothing special about a 1 GW zettascale supercomputer, just like there was nothing special about 20 MW for exascale. You might argue that building a supercomputer that consumes all the power of a nuclear reactor might be fundamentally more difficult than one that consumes only 20 MW, and you'd be right--which is why the first gigawatt supercomputers probably aren't going to look like the supercomputers of today.</p><p>Papermaster's \"energy wall\" slide reminded me of <a href=\"https://fee.org/articles/the-great-horse-manure-crisis-of-1894/\">the great horse manure crisis of 1984</a>, where people extrapolated from today using an evolutionary, not revolutionary, trajectory. If building a single gigawatt supercomputer is inconceivable, then build four 250 MW supercomputers and put a really fast network between them to support a single, synchronous job. The AI industry is already headed down this road; <a href=\"https://glennklockwood.com/garden/multicluster-training\">Google, Microsoft, and OpenAI have already talked about how they synchronously train across multiple supercomputers</a>, and Microsoft announced their <a href=\"https://view.officeapps.live.com/op/view.aspx?src=https%3A%2F%2Fmediusdownload.event.microsoft.com%2Ftranscripts%2FD6K5%2FKEY020%2FKEY020.docx%3Fsv%3D2018-03-28%26sr%3Db%26sig%3D0gs30Jf82r%252BresqqpGIGKRSOtKrvicgbpqh5Tdkigpg%253D%26se%3D2025-06-24T18%253A46%253A13Z%26sp%3Dr&amp;wdOrigin=BROWSELINK\">400 Tb/s \"AI WAN\" for this last month</a> as a means to enabling wide-area training.</p><p>Granted, it's unlikely that the HPC community will be building massive, distributed supercomputers the way hyperscale is. But I was disappointed that the keynote only went as far as saying \"a gigawatt supercomputer is crazy, so we need codesign at the node/rack scale.\" Codesign to reach zettascale will probably require a whole new approach that, for example, accounts for algorithms that <a href=\"https://github.com/NVIDIA/nccl/pull/1659\">synchronize communication across multiple datacenters</a> and power plants. The infrastructure for that is already forming, with the US developing its Integrated Research Infrastructure (IRI) and Europe shaping up to have over a dozen AI factories. Zettascale by 2035 may very well exist for the scientific computing community, but it'll probably look a lot more like hyperscale zettascale rather than a single massive building. A single machine plugged into a gigawatt nuclear reactor only happens if business-as-usual is extrapolated out another ten years as Papermaster did, and the codesign required to achieve that isn't very meaningful.</p><p>Prof. Satoshi Matsuoka also gave a talk on the big stage about <a href=\"https://glennklockwood.com/garden/systems/FugakuNEXT\">Fugaku-NEXT</a>, which Japan has branded as a zettascale system. His vision, which will be realized before 2030, aims to deploy a single, 40 MW supercomputer (much like <a href=\"https://www.glennklockwood.com/garden/systems/Fugaku\">Fugaku</a> was) where:</p><ul><li>10x-20x speedup comes from hardware improvements</li><li>2x-8x speedup comes from mixed precision or emulation (more on this below)</li><li>10x-25x speedup comes from surrogate models or physics-informed neural networks</li></ul><p>The net result is a 200x-4000x speedup over Fugaku. His rationale is that this will result in a system that is effectively equivalent to somewhere between 88 EF and 1.7 ZF FP64. It's not literally doing that many calculations per second, but the science outcomes are equivalent to a brute-force approach using a much larger system.</p><p>I thought this approach to reaching zettascale was much more realistic than the Papermaster approach, but it does require the scientific computing community to redefine its metrics of success. If HPL was a bad benchmark for exascale, it is irrelevant to zettascale since it's unlikely that anyone will ever run HPL on a zettascale system. At best, we'll probably see something like <a href=\"https://hpl-mxp.org\">HPL-MxP</a> that captures the 10x-20x hardware speedup and the 2x-8x mixed-precision or emulated FP64 reach hundreds of exaflops, but the 10x-25x from surrogate models will be domain-specific and defy simplistic ranking. If I had to guess, the first zettascale systems will be benchmarked through Gordon Bell prize papers that say things like \"simulating this result using conventional FP64 would have required over 1 ZF for 24 hours.\"</p><h2 id=\"ozaki-ozaki-ozaki\">Ozaki, Ozaki, Ozaki</h2><p>Although Prof. Matsuoka evoked the 2x-8x speedup from mixed precision or emulation when claiming <a href=\"https://www.glennklockwood.com/garden/systems/FugakuNEXT\">Fugaku-NEXT</a> would be zettascale, he was far from the only speaker to talk about mixed precision and emulation. In fact, it seemed like everyone wanted to talk about emulating FP64, specifically using NVIDIA's low-precision tensor cores and the <a href=\"https://doi.org/10.1007/s11075-011-9478-1\">Ozaki scheme</a> (or its derivatives). By the end of the week, I was absolutely sick of hearing about Ozaki.</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure></figure></div><p>For the unindoctrinated, this Ozaki scheme (and similar methods with less-catchy names) is a way to emulate matrix-matrix multiplications at high precision using low-precision matrix operations. It's become so hot because, despite requiring more arithmetic operations than a DGEMM implemented using WMMA/MFMA instructions, it can crank out a ton of FP64-equivalent operations per unit time. This is a result of the ridiculously nonlinear increases in throughput of low-precision tensor/matrix cores on modern GPUs; for example, Blackwell GPUs can perform over 100x more 8-bit ops than 64-bit ops despite being being only 8x smaller. As a result, you can burn a ton of 8-bit ops to emulate a single 64-bit matrix operation and still realize a significant net speedup over hardware-native FP64. Matsuoka presented the following slide to illustrate that:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure><figcaption class=\"image-caption\">Dr. Uchino's estimates of how many FP64 FLOPS one can emulate using INT8 as presented by Satoshi Matsuoka.</figcaption></figure></div><p>Emulation offers a way for scientific apps that need high-precision arithmetic to directly use AI-optimized accelerators that lack FP64 in hardware, so it's worth talking about at conferences like ISC. But it seems like <em>everyone</em> wanted to name-drop Ozaki, and the actual discussion around emulation was generally a rehash of content presented earlier in the year at conferences like <a href=\"https://blog.glennklockwood.com/2025/03/gtc-2025-recap.html\">GTC25</a>.</p><p>While hearing about FP64 emulation and Ozaki schemes got tiring throughout the week, I had to remind myself that I hadn't even heard about Ozaki before September 2024 at the Smoky Mountains Conference. The fact that the Ozaki scheme went from relative algorithmic obscurity to being the star of the show in nine months is either a reflection of its incredible importance in scientific computing or a testament to the reach of NVIDIA's marketing.</p><p>Cynically, I'll bet that NVIDIA is probably doing everything it can to make sure the world knows about the Ozaki scheme, and ISC was a part of that. When the datasheets for Rubin GPUs are released, I'll bet the performance table has a row claiming a bazillion FP64 FLOPS, and there will be a tiny footnote that clarifies they're citing emulated FP64 precision. They did it with structured sparsity, and I'm sure they'll do it for emulated DGEMM.</p><p>Although the Ozaki scheme is perhaps over-hyped considering how narrow its applicability is to the broad range of compute primitives used in scientific computing, I do anticipate that it is the tip of the iceberg. If 2025 was the year of the Ozaki scheme, 2026 may be the year of the emulated FP64 version of FFTs, sparse solvers, stencils, or other key algorithms. We're seeing signs of that already; David Keyes and Hatem Ltaief both presented material at ISC on using mixed-precision matrix operations for other scientific problems, and I mentioned <a href=\"https://blog.glennklockwood.com/2025/03/gtc-2025-recap.html#for-science\">their work in my earlier GTC25 blog</a>. I'm not sure \"the Keyes scheme\" or \"the Ltaief scheme\" is as catchy as \"the Ozaki scheme,\" but I expect to hear more about these other emulation techniques before ISC26.</p><h2 id=\"top500\">Top500</h2><p>On the topic of matrix-matrix multiplication, I can't get too much farther without talking about the Top500 list released at ISC. Although there was no new #1 system, Europe's first exascale system, JUPITER, made its sub-exascale debut. There were also a number of new entries in Top50, and surprisingly, many of them came from companies who offer GPUs-as-a-Service for AI training rather than the usual public-sector sites delivering cycles for scientific research. However, all the new entries were still using previous-generation Hopper GPUs despite huge Blackwell coming online, exposing a perceptible lag between the state of the art in supercomputers for AI and traditional HPC.</p><p>As with last year, I felt a growing tension between what the Top500 list brings to the discussion and where the large-scale supercomputing industry is headed. As I wrote earlier, mixed-precision and emulated FP64 was a hot topic in the technical program, but the emphasis of the Top500 session was still squarely on bulk-synchronous FP64 performance. HPL-MxP awards were handed out, but they all wound up in the hands of systems who were also at the top of the regular HPL list. Nobody is submitting HPL-MxP-only scores, and there was no meaningful discussion about the role that the Ozaki scheme will play going forward in Top500's future.</p><p>Opining about the long-term future of the Top500 list is a whole separate blog post though, so I'll focus more on what was covered at this year's session.</p><h3 id=\"jupiter\">JUPITER</h3><p>JUPITER was the only new entrant into the Top 10, and it posted at #4 with an average 793 PF over a hundred-minute run. Though it hasn't broken the 1 EF barrier yet, JUPITER is noteworthy for a few reasons:</p><ul><li>It is expected to be Europe's first exascale system. Given this HPL run <a href=\"https://bsky.app/profile/andih.bsky.social/post/3lrvrguvtzc2b\">used only 79% of the Booster Module's 5,884 GH200 nodes</a>, some basic extrapolation puts the full-system run just a hair above 1 EF. Jülich will either have to run with 100% node availability or get a few extra nodes to exceed 1 EF though.</li><li>JUPITER is also now the biggest NVIDIA-based supercomputer on Top500, pushing Microsoft's H100 SXM5 system (Eagle) down to #5. JUPITER is also Eviden's biggest system and a strong affirmation that Europe isn't dependent on HPE/Cray to deliver on-prem systems of this scale.</li></ul><p>JUPITER was also installed into a modular datacenter, an approach that is emerging as a preferred method for rapidly deploying large GPU systems in Europe. This setup allowed Jülich to place shipping container-like modules on a concrete foundation in just a few months. However, because the datacenter is form-fit to the JUPITER system without much extra space, it's impossible to take a glamor shot of the entire machine from far away. As a result, most photos of JUPITER show only the datacenter modules that wrap the supercomputer racks. For example, Prof. Thomas Lippert shared this photo of JUPITER during his presentation:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure><figcaption class=\"image-caption\">JUPITER's modular datacenter as seen from a drone flying overhead.</figcaption></figure></div><p>As Lippert was describing JUPITER, I couldn't help but compare it to the AI supercomputers I support at my day job. Like JUPITER, our supercomputers (like Eagle) aren't very photogenic because they're crammed into form-fitted buildings, and they are best photographed from the sky rather than the ground. For example, here's a photo of one of Microsoft's big GB200 supercomputers that I presented later in the week:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure><figcaption class=\"image-caption\">A slide showing one of Microsoft's big GB200 supercomputers that I presented at the SuperCompCloud workshop later in the week. The big two-story building in the center houses GPUs, and the long white building on the right houses storage and CPU-only nodes.</figcaption></figure></div><p>JUPITER may be the first exascale system listed on Top500 that doesn't have fancy rack graphics, but I don't think it will be the last.</p><p>I also found myself wondering if these modular datacenters are trading short-term upsides with long-term downsides. While they accelerate deployment time for one-off supercomputers, it wasn't clear to me if these modular structures is reusable. Does the entire datacenter retire along with JUPITER after 5-7 years?</p><p>Hyperscalers use modular datacenters too, but the modularity is more coarse-grained to support a wider variety of systems over multiple decades. They're also physically more capacious, allowing them to deploy more CDUs and transformers per rack or row to retrofit them for whatever power and cooling demands evolve into over the full depreciation life of the datacenter building.</p><h3 id=\"hpcai-system-intersection\">HPC-AI system intersection</h3><p>As with last year, Erich Strohmeier did a walkthrough of Top500 highlights, and he argued that \"hyperscale\" is defined as anything bigger than 50 MW, and therefore the Top500 list is hyperscale. It wasn't clear what value there was in trying to tie the Top500 list to hyperscale in this way, but there were a few ways in which Top500 is beginning to intersect with hyperscale AI.</p><p>Foremost is the way in which some exascale systems have been appearing on the list: they first appear after HPL is run on a big but partially deployed machine, then six months later, the full-system run is listed. Aurora and JUPITER both follow this pattern. What's not obvious is that many massive AI supercomputers also do something like this; for example, the Eagle system's 561 PF run was analogous to <a href=\"https://top500.org/lists/top500/2023/11/\">Aurora's initial 585 PF run</a> or JUPITER's 793 PF run. The difference is that systems like Eagle typically enter production training after that first big tranche of GPUs is online, so there is never an opportunity to run HPL as more of the system powers up. Instead, the production training job simply expands to consume all the new GPUs as new tranches come online.</p><p>This iteration of the Top500 list also saw a number of bona fide commercial AI training clusters from smaller GPU-as-a-Service and \"AI factory\" providers post results, giving the public a view of what these systems actually look like:</p><ul><li>Nebius listed <a href=\"https://top500.org/system/180366/\">ISEG2</a> at #13 with a 624-node, 202 PF H200 SXM cluster, following their 2023 Top500 debut with a 190-node, 46 PF H100 SXM cluster. Nebius was spun out of Yandex, the Russian tech conglomerate.</li><li>Northern Data Group debuted <a href=\"https://top500.org/system/180378/\">Njoerd</a> at #26 with a 244-node H100 SXM cluster. Northern Data Group started out as a German bitcoin mining company.</li><li>FPT debuted at #36 with a <a href=\"https://top500.org/system/180399/\">127-node H200 SXM cluster</a> and #38 with a <a href=\"https://top500.org/system/180387/\">127-node H100 SXM cluster</a>. FPT is a Vietnamese technology conglomerate.</li></ul><p>It's notable that none of these systems resemble the sovereign AI systems or EuroHPC AI Factories cropping up in Europe, which are attached to traditional HPC centers and built on familiar HPC platforms like Cray EX or BullSequana. Rather, they're essentially NVIDIA reference architectures that resemble DGX SuperPods but are stamped out by companies like Supermicro, Gigabyte, and ASUS.</p><p>While it's nice of these GPU-as-a-Service companies to participate in the Top500 list, I did not see anyone from these companies in the technical program in any other way. And I did not see anyone from the bigger GPU-as-a-Service providers (CoreWeave, Crusoe, Lambda, etc) contributing either. Thus, while these companies are participating in Top500, it doesn't seem like they're genuinely interested in being a part of the HPC community.</p><h3 id=\"other-new-entrants\">Other new entrants</h3><p>If you take a step back and look at the ten largest systems that made their debut at ISC'25, they broadly divide into two categories. Here's the list:</p><div><table style=\"border-collapse: collapse; font-family: sans-serif; font-size: 0.9em; width: 100%;\"><thead style=\"background-color: #f2f2f2;\"><tr><th style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: left;\">Rank</th><th style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: left;\">System</th><th style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: left;\">Platform</th><th style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: left;\">Site</th></tr></thead><tbody><tr style=\"background-color: white;\"><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">4</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">JUPITER Booster</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">GH200</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">Jülich</td></tr><tr style=\"background-color: #f9f9f9;\"><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">11</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">Isambard-AI phase 2</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">GH200</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">Bristol</td></tr><tr style=\"background-color: white;\"><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">13</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">ISEG2</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">H200 SXM5</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">Nebius</td></tr><tr style=\"background-color: #f9f9f9;\"><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">15</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">ABCI 3.0</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">H200 SXM5</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">AIST</td></tr><tr style=\"background-color: white;\"><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">17</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">Discovery 6</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">GH200</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">ExxonMobil</td></tr><tr style=\"background-color: #f9f9f9;\"><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">18</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">SSC-24</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">H100 SXM5</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">Samsung</td></tr><tr style=\"background-color: white;\"><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">26</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">Njoerd</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">H100 SXM5</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">Northern Data Group</td></tr><tr style=\"background-color: #f9f9f9;\"><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">27</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">ABCI-Q</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">H100 SXM5</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">AIST</td></tr><tr style=\"background-color: white;\"><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">33</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">AI-03</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">MI210</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">Core42</td></tr><tr style=\"background-color: #f9f9f9;\"><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">36</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">FPT AI Factory Japan</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">H200 SXM5</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">FPT</td></tr></tbody></table></div><p>Aside from Core42's weird MI210 cluster, every new big system was either GH200 (for traditional HPC) or H100/H200 SXM5 (for AI). This suggests a few interesting things:</p><ul><li>None of the AI cloud/GPUaaS providers are talking about GH200. It seems that GH200 is squarely for scientific computing, and Hopper HGX systems is preferred for AI at scale.</li><li>Despite debuting on Top500 two years ago, H100 is still making its way into the hands of HPC and AI sites. This could mean one of several things:<ul><li>H100 is more affordable now (<a href=\"https://www.nextplatform.com/2025/05/08/supermicro-hiccups-on-hopper-pulls-40-billion-guidance-for-fiscal-2026/#:~:text=“But%20I%20said,say%20that.”%20%5Blaughter%5D\">Jensen says he can't give them away</a>),</li><li>there was a huge backlog of H100 orders, or</li><li>it's just taking some places a really long time to get H100 up and running</li></ul></li><li>Blackwell is not relevant to HPC right now. There are no big Blackwell systems on this list, nor was Blackwell discussed in any sessions I attended during the week. This is despite large GB200 systems being public, up, and benchmarked. For example, <a href=\"https://github.com/mlcommons/training_results_v5.0/blob/main/IBM%2BCoreWeave%2BNVIDIA/systems/carina_ngpu2496_ngc25.04_nemo.json\">CoreWeave, IBM, and NVIDIA ran MLPerf Training across 39 racks (624 nodes) of a GB200 NVL72 system named Carina just last month</a>. They did not appear to bother with HPL, though.</li></ul><p>From all this, it seems like there is a definite lag forming between what qualifies as \"leadership computing\" to HPC people and AI people. Today's leadership HPC (Hopper GPUs) is yesterday's leadership AI, and today's leadership AI (Blackwell GPUs) isn't on the radar of leadership HPC yet. Maybe GB200 will begin appearing one or two years later as the AI people move on to Vera-Rubin.</p><p>So, if I had to guess, I think the top-end of Top500 in 2027 could look like one of three things:</p><ol type=\"1\"><li>It will contain HPC systems with state-of-the-art, HPC-specific variants of accelerators that are completely irrelevant to AI. Large AI training systems will simply disappear from the list, because HPL has ceased to be a meaningful measure of their capability. GB200/GB300 simply never appear on Top500.</li><li>It will contain HPC systems with previous-generation Blackwell accelerators after Jensen (the chief revenue destroyer) gets on stage and tells the world that Blackwell is junk because Rubin is awesome. The AI industry gobbles up all the Rubin GPUs, and HPC picks up the scraps they leave behind.</li><li>Top500 starts allowing FP64 emulation, and all bets are off on how ridiculous the top systems' numbers look. In this case, top systems just skip the 1-10 exaflops range and start debuting at tens of exaflops.</li></ol><p>I have no idea where things will go, but we're starting to see <a href=\"https://www.nersc.gov/what-we-do/computing-for-science/doudna-system\">big HPC deals</a> <a href=\"https://blogs.nvidia.com/blog/blue-lion-vera-rubin/\">targeting Vera Rubin</a> that line up with the same time Rubin will land for the AI industry in 2H2026. So maybe Blackwell is just a hiccup, and option #1 is the most likely outcome.</p><h2 id=\"hpc-around-the-world\">HPC around the world</h2><p>Though Blackwell's absence from Top500 was easy to overlook, China's continued absence was much more obvious. Even though no new Chinese systems have been listed in a few years now though, representatives from several Chinese supercomputing centers still contributed invited talks throughout the week.</p><p>In that context, I appreciated how fully ISC embraces its international scope. I found myself attending a lot of \"HPC Around the World\" track sessions this year, partly because I work for a multinational corporation and have to stay aware of potential needs outside of the usual US landscape. But there's also been a sharp rise in the amount of serious HPC that is now occurring outside of the USA under the banner of \"sovereign AI,\" and I've been keen to understand how \"sovereign AI\" compares to the US-based AI infrastructure in which I work.</p><p>Before getting too deep into that though, China is worth discussing on its own since they had a such prominent presence in the ISC program this year.</p><h3 id=\"hpc-in-china\">HPC in China</h3><p>Following the single-track opening keynote on the first day of ISC is the single-track Jack Dongarra Early Career Award Lecture, and this year's talk was given by awardee Prof. Lin Gan from Tsinghua University. In addition, Dr. Yutong Lu gave two separate talks--including the closing keynote--which shed light on the similarities and differences between how China and the US/Europe are tackling the challenges of exascale and beyond.</p><p>China is in a position where it does not have access to US-made GPUs, forcing them to develop their own home-grown processors and accelerators to meet their needs for leadership computing. As a result, both speakers gave talks that (refreshingly) revolved around non-GPU technologies as the basis for exascale supercomputers. Although neither Gan nor Lu revealed anything that wasn't already written about in the Gordon Bell prize papers, I took away a few noteworthy observations:</p><p><strong>The most public Chinese exascale system is always called the \"New Sunway\" or \"Next Generation Sunway,\" never \"OceanLight\"</strong> as has been reported in western media. There still aren't any photos of the machine either, and Dr. Gan used stock diagrams of the predecessor Sunway TaihuLight to represent New Sunway. There was no mention of the Tianhe Xingyi/TH-3 supercomputer at all.</p><p><strong>Chinese leadership computing details remain deliberately obfuscated despite the openness to present at ISC.</strong> For example, Lu presented the following English-language table from the <a href=\"https://www.csiam.org.cn/1003/202411/2246.html\">2024 China Top100 HPC list</a>:</p><div><table style=\"border-collapse: collapse; font-family: sans-serif; font-size: 0.75em; white-space: nowrap;\"><thead style=\"background-color: #f2f2f2;\"><tr><th style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: left;\">No.</th><th style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: left;\">Vendor</th><th style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: left;\">System</th><th style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: left;\">Site</th><th style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: left;\">Year</th><th style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: left;\">Application</th><th style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">CPU Cores</th><th style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">Linpack (Tflops)</th><th style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">Peak (Tflops)</th><th style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">Efficiency (%)</th></tr></thead><tbody><tr style=\"background-color: white;\"><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">1</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">Server Provider</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">Supercomputing system mainframe system, heterogeneous many-core processor</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">Supercomputing Center</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">2023</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">computing service</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">15,974,400</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">487,540</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">620,000</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">78.7</td></tr><tr style=\"background-color: #f9f9f9;\"><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">2</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">Server Provider</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">Internet Company Mainframe System, CPU+GPU heterogeneous many-core processor</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">Internet company</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">2022</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">computing service</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">460,000</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">208,260</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">390,000</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">53.4</td></tr><tr style=\"background-color: white;\"><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">3</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">Server Provider</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">Internet Company Mainframe System, CPU+GPU heterogeneous many-core processor</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">Internet company</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">2021</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">computing service</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">285,000</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">125,040</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">240,000</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">52.1</td></tr><tr style=\"background-color: #f9f9f9;\"><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">4</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">NRCPC</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">Sunway TaihuLight, 40960*Sunway SW26010 260C 1.45GHz, customized interconnection</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">NSCC-WX</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">2016</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">supercomputing center</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">10,649,600</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">93,015</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">125,436</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">74.2</td></tr><tr style=\"background-color: white;\"><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">5</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">Server Provider</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">Internet Company Mainframe System, CPU+GPU heterogeneous many-core processor</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">Internet company</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">2021</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">computing service</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">190,000</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">87,040</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">160,000</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">51.2</td></tr><tr style=\"background-color: #f9f9f9;\"><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">6</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">NUDT</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">Tianhe-2A, TH-IVB-MTX Cluster + 35584*Intel Xeon E5-2692v2 12C 2.2GHz + 35584 Matrix-2000, TH Express-2</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">NSCC-GZ</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">2017</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">supercomputing center</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">427,008</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">61,445</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">100,679</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">61.0</td></tr><tr style=\"background-color: white;\"><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">7</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">Server Provider</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">Internet Company Mainframe System, CPU+GPU heterogeneous many-core processor</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">Internet company</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">2021</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">computing service</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">120,000</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">55,880</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">110,000</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">50.8</td></tr><tr style=\"background-color: #f9f9f9;\"><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">8</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">Server Provider</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">ShenweiJing Supercomputer System, 1024*SW26010Pro heterogeneous many-core processor 390C MPE 2.1 GHz</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">Computing Company</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">2022</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">scientific computing</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">399,360</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">12,912</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">14,362</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">89.9</td></tr><tr style=\"background-color: white;\"><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">9</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">Server Provider</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">Supercomputing Center System, 992*SW26010Pro heterogeneous many-core processor 390C MPE 2.1 GHz</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">Supercomputing Center</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">2021</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">scientific computing</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">386,880</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">12,569</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">13,913.0</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">90.3</td></tr><tr style=\"background-color: #f9f9f9;\"><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">10</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">BSCCC/Intel</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">BSCCC T6 Section 5360*Intel Xeon Platinum 9242 homogeneous many-core processor 48C 2.3 GHz, EDR</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">BSCCC</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">2021</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px;\">computing service</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">257,280</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">10,837</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">18,935.0</td><td style=\"border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;\">57.2</td></tr></tbody></table></div><p>The #1 system is almost definitely built on SW26010P processors just like the big New Sunway system that Gan discussed (15,974,400 cores / 390 cores per SW26010P = 40,960 nodes), but it's significantly smaller than the 39M cores on which the work Gan highlighted was run. Clearly, China's biggest systems aren't on their own Top100 list, and their #1 listed system only says its processors are \"heterogeneous many-core\" despite smaller entries explicitly listing SW26010P (Pro) processors.</p><p><strong>Chinese leadership computing struggles aren't being hidden</strong>. Lu specifically called out a \"lack of a new system\" in 2024, echoing earlier sentiments from other leaders in Chinese HPC who have referred to <a href=\"https://news.sciencenet.cn/htmlnews/2024/11/534141.shtm\">\"some difficulties in recent years\" and a \"cold winter\" of HPC</a>. She also said that their leadership systems are \"relatively\" stable rather than trying to overstate the greatness of Chinese HPC technology. But as with above, she didn't get into specifics; by comparison, Scott Atchley (of Oak Ridge Leadership Computing Facility) specifically quoted a 10-12 hour mean time between job interrupt on Frontier after his keynote. Whether 10-12 hours is \"relatively stable\" remained unspoken.</p><p><strong>Performance portability wasn't a top-line concern despite how hard it seems to port applications to Chinese accelerators.</strong> SW26010P is weird in that it has a host core and offload cores with scratchpads, and its native programming model (Athread) is very CUDA-like as a result. Gan made it seem that China is investing a lot of effort into \"fine-grained optimizations\" using OpenACC and Athread, and he showed all the ways in which they're rewriting a lot of the kernels and decompositions in complex applications (like <a href=\"https://www.cesm.ucar.edu/models/cam\">CAM</a>) to make this work. This sounds like an performance portability nightmare, yet there wasn't much talk about Chinese equivalents to performance portability frameworks like Kokkos, RAJA, or alpaka.</p><p>Lu did name-drop a few frameworks that unify HPC and AI performance portability from around the world:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure><figcaption class=\"image-caption\">Yutong Lu's only reference to software that enhances portability and productivity. Not quite the same as what Kokkos, Raja, and alpaka aim to solve, though.</figcaption></figure></div><p>However, these were more about aligning efforts across scientific computing and AI rather than enabling scientific apps to run seamlessly across China's different exascale accelerators.</p><p><strong>Application focus areas in China seem similar to everywhere else.</strong> Classical and quantum materials modeling, climate and ocean modeling, electronic structure calculations, and genomics were all mentioned by Gan and Lu in their talks. There was no mention of stockpile stewardship or any defense-related applications of HPC, though I'm sure China is using big supercomputers in these efforts just as US and European nations do. The only unusual application that I noticed was Gan's mention of implementing reverse time migration (RTM) on FPGAs; I've only ever heard of RTM in the context of oil exploration. Though I'm no expert, I didn't think many HPC centers spent a lot of time focusing on that technique. I do know KAUST has done some work optimizing RTM applications with Aramco in the space, but most other national supercomputing centers keep oil and gas at arm's length. Gan's RTM work may be related to earthquake modeling rather than petroleum, but it stood out nonetheless.</p><p><strong>Nobody talked about GPUs.</strong> Gan spent a healthy amount of time talking about applying FPGAs and NPUs to scientific problems, but these are areas of research that are on the fringes of mainstream HPC. I'm not sure if this reflected his own interests or priority research directions in China, but given that Chinese researchers cannot procure NVIDIA or AMD GPUs, perhaps FPGAs and NPUs are being pursued as a potential next-best-thing. Necessity truly is the mother of invention, and China might be the driver of a disproportionate amount of innovation around dataflow processing and reduced precision for modeling and simulation workloads.</p><p><strong>Nobody talked about storage either.</strong> I'm not sure if this suggests China has a lopsided interest in compute over holistic system design, or if they just talked about their biggest challenges (which are using home-grown accelerators productively). Granted, keynote speakers rarely talk about storage, but I didn't see much participation from China in any of the subsystem-specific sessions I attended either. This is particularly notable since, for a time, Chinese research labs were dominating the IO500 list with their home-made file systems. Networking was mentioned in passing in Lu's closing keynote, but not much beyond another example of technology fragmentation, and there were no specific Chinese interconnects being discussed during the week.</p><p><strong>China is in the thick of AI just like the rest of the world.</strong> Lu said that 30% of the cycles on their big HPC systems go to AI, which is right in line with anecdotes from other HPC sites that put their figures at <a href=\"https://csc.fi/en/media-release/lumis-capacity-in-high-demand-to-be-succeeded-by-an-ai-optimized-supercomputer/?utm_source=chatgpt.com\">somewhere up to 50%</a>. She also presented the Chinese taxonomy of the three ways in which AI and scientific computing can mesh together: HPC for AI (training LLMs on supercomputers), HPC by AI (AI for system design and operations), and HPC and AI (AI in the loop with simulation). China is also neck-deep in figuring out how to exploit reduced precision (or \"intelligent computing,\" as Lu branded it) and has pivoted from being \"performance driven\" (which I took to mean HPL-driven) to \"target driven\" (which I took to mean scientific outcome-driven). This is consistent with their recent Gordon Bell prize win and non-participation in either Top500 or China Top100.</p><p><strong>China is embracing geo-distributed supercomputing and complex workflows</strong>, much like the US. Lu specifically called out \"Computility Net,\" a catchy name that sounded a lot like the US DOE's Integrated Research Infrastructure (IRI). She described it as a national effort to combine supercomputing with \"commodity IT\" resources (perhaps Chinese cloud?) to enable \"resource sharing\" through a \"service grid.\" In her closing keynote, she even name-dropped IRI:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure><figcaption class=\"image-caption\">The Chinese vision for Computility Net, which seems analogous to the US Integrated Research Infrastructure, as presented by Yutong Lu.</figcaption></figure></div><p>She did liken Computility to both IRI in the US and PRACE in the EU though, and in my mind, PRACE is nothing like IRI. Rather, PRACE is more like TeraGrid/XSEDE/ACCESS in that it federates access to HPC systems across different institutions, whereas IRI's ambition is to tightly integrate computational and experimental facilities around the country. But from the above slide, it sounds like Computility Net is closer to IRI since it is coupled to \"Supercomputing internet\" (akin to ESnet?) and bridging compute and data across eastern and western China.</p><h3 id=\"elsewhere-in-asia\">Elsewhere in Asia</h3><p>Although Chinese researchers headlined a few sessions at ISC, a number of other Asian nations presented their national supercomputing strategies as well. Japan and Korea have mature, world-class HPC programs, but I was surprised to see how ambitious India has become to catch up. Smaller nations were also represented, but it was clear to me that their focus is spread across midrange HPC, partnering with large centers in Korea/Japan, and innovating around the edges of supercomputing. And perhaps unsurprisingly, every nation represented had a story around both quantum computing and artificial intelligence regardless of how modest their production modsim infrastructure was.</p><p><strong>India</strong> appears to rapidly catching up to the US, Europe, and Japan much in the same way China was fifteen years ago. Representatives from C-DAC, the R&amp;D organization that owns the national supercomputing mission in India, gave a far-reaching presentation about India's ambition to achieve exascale by 2030. Their current strategy appears to be broad and capacity-oriented, with forty petascale clusters spread across India for academic, industrial, and domain-specific research. They have a comprehensive, if generic, strategy that involves international collaboration in some regards, reliance on open-source software to fill out their HPC environment story, and home-grown hardware and infrastructure:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure><figcaption class=\"image-caption\">India's ambitious strategy towards exascale in 2030. This slide has it all, from home-grown CPUs and networks to five systems deployed in six years.</figcaption></figure></div><p>I was surprised to hear about their ambitions to deploy their own CPUs and interconnect though. India is pursuing both ARM and RISC-V for their own CPUs for a future 200 PF system, and they're already deploying their \"InfiniBand-like\" interconnect, TRINETRA, which uses funny NICs with <a href=\"https://cdac.in/index.aspx?id=product_details&amp;productId=TrinetraHPCInterconnect\">6x100G ports or 10x200G ports</a> rather than fewer, faster serdes. I didn't hear mention of their AI acceleration plans, but rolling their own commercialized CPU and interconnect in itself is a lot to bite off. Given that India is the world's fastest growing economy though, these plans to go from 20 PF in 2025 to 1 EF in 2030 may not be that far-fetched. Perhaps the Indian national strategy will become clearer during the inaugural <a href=\"https://sc-india.in\">Supercomputing India 2025 conferece</a> this December.</p><p>The <strong>Korea Institute of Science and Technology Information</strong> also took the stage to describe their next national supercomputer, <a href=\"https://www.glennklockwood.com/garden/systems/KISTI-6\">KISTI-6</a>, which was first announced in May 2025. It will be a 588 PF Cray EX254n system with 2,084 nodes of GH200, similar to <a href=\"https://www.glennklockwood.com/garden/systems/Alps\">Alps</a> and <a href=\"https://www.glennklockwood.com/garden/systems/Isambard-AI\">Isambard-AI</a>. This is quite a step up from its predecessor, which was an air-cooled KNL system, but it's unlikely it will unseat Fugaku; the 588 PF number cited appears to be the sum of 2,084 GH200 nodes, 800 Turin CPU nodes, and 20 H200 SXM5 nodes. The HPL score of its GH200 nodes will place it below <a href=\"https://www.glennklockwood.com/garden/systems/Alps\">Alps</a> and somewhere around 350 PF, likely joining a flood of multi-hundred-petaflops GH200 systems that will appear between now and ISC26.</p><p><strong>Singapore (NSCC) and Taiwan (NCHC)</strong> both presented their national programs as well, but they appear to be much more nascent, and the size of their HPC infrastructure was presented as aggregate capacity, not capability. Their strategies involve partnership with Japan or Korea, but both had specific carveouts for both sovereign AI and quantum computing. Interestingly, their use cases for AI both had a strong story about training models that understood the diversity of languages and dialects represented in their nations. For example, it is not unusual for people to switch languages or dialects mid-sentence in Singapore, and the big Western models aren't designed for that reality. Similarly, Taiwan has 16 indigenous tribes with 42 dialects. It seemed like enabling LLMs that reflect the breadth languages used in Singapore and Taiwan have become the responsibility of these nations' respective national supercomputing efforts.</p><p>That said, that noble mission didn't seem to be matched with substantial training infrastructure; these localized models will be relying on a couple hundred GPUs here and there, wedged into existing HPC centers. Thus, these sovereign models are probably going to be fine-tuned variants of open models, aligning with my earlier observation that these smaller nations will be innovating around the edges of HPC and AI.</p><p><strong>What was missing?</strong> Although Vietnam, Thailand, Malaysia, and other Asian nations have strong HPC programs centered around industrial uses, they were not represented in ISC's HPC Around the World track. Also absent was any meaningful discussion around cloud; while everyone had a throwaway line about cloud in their presentations, the fact that the only big clouds in Asia are Chinese and American probably makes it unappealing to integrate them into the core of these nations' national HPC strategies. Speaking from experience, this is quite different from the attitudes of commercial HPC users across Asia who are all too happy to let someone else run HPC datacenters for them.</p><h3 id=\"the-middle-east\">The Middle East</h3><p>Although KAUST has been a world-class HPC center in the Middle East for the past fifteen years, AI seems to be where the majority of new investment into HPC is going.</p><p>In describing new efforts in Saudi Arabia, Prof. David Keyes casually mentioned the Saudi HUMAIN effort, which will build 500 MW of datacenter capacity and 18,000 GB300 GPUs, after describing the Shaheen-3 GH200 upgrade that \"might (barely)\" put it back in the Top20 by SC'25. Similarly, Dr. Horst Simon walked through a few of Abu Dhabi's university clusters (each having dozens of GPU nodes) after skating through an announcement that a 5 GW AI campus was also being built in Abu Dhabi. The gap between investment in AI and investment in HPC was striking.</p><p>I also had a brief conversation with someone from one of the major Abu Dhabi universities, and I was very surprised to find that I was talking to a real AI practitioner--not an HPC person moonlighting in AI--who spoke at the same depth as the customers with whom I work in my day job. The nature of his work made it clear to me that, despite his university not having a Top500 system, he was familiar with running training and inference at scales and with sophistication that is far beyond the experience of most ISC attendees.</p><p>These interactions led me to the conclusion that the Middle East's approach to \"sovereign AI\" is quite different from Europe's. Rather than building HPC systems with GPUs, letting HPC centers operate them, and calling them sovereign AI platforms, nations like Saudi Arabia and UAE are keeping HPC and AI separate. Like in the US, they are going straight to hyperscale with AI, and they have no preconceived notion that anything resembling a supercomputer must be hosted at a supercomputer center.</p><p>Of course, only nations like Saudi Arabia and UAE can afford to do this, because they have trillion-dollar sovereign wealth funds to invest in massive infrastructure buildout that doesn't isn't contingent on public consensus or the latest election cycle. Just as UAE's Core42 can build a 5 GW datacenter campus with little oversight, these nations can easily mis-step and invest a ton of money in an AI technology that turns out to be a flop. In the end, it seems like these Middle Eastern nations are willing to take bigger risks in how they build out their sovereign AI infrastructure, because they are largely starting from a blank sheet of paper. They aren't limiting themselves to 20 MW supercomputers like the HPC world had.</p><p>All things being equal, this might turn out to be an advantage over other nations who are more hesitant to deviate from the tried-and-true course of buying a Cray or a Bull, sticking some GPUs in it, and calling it AI. If these Middle Eastern nations do everything right, they stand to get a lot further and move a lot faster in sovereign AI than Europe, and it'll be fascinating to see how quickly they catch up with the sort of frontier AI research being done private industry. But, as with the US AI industry, it doesn't seem like these AI practitioners are going to be attending ISC in the same way European sovereign AI folks do; the roads of HPC and AI seem to run parallel without intersecting in the Middle East.</p><h2 id=\"exhibitors\">Exhibitors</h2><p>ISC had a <a href=\"https://isc-hpc.com/the-isc-2025-exhibition-sets-new-records/\">record number of exhibitors this year</a>, and as usual, I tried to set aside at least an hour or two to walk the floor and see what technologies are on the horizon. This year, though, the exhibit hall was not a great representation of the rest of the conference. Everyone I talked to about the exhibit said one of two things:</p><ol type=\"1\"><li>There are a LOT of quantum companies.</li><li>A lot of big companies were noticeably absent.</li></ol><p>It also didn't feel like the biggest exhibit ever, partially because of #2, and partially because many of the exhibitors--one in five--was exhibiting for the first time this year. This meant a lot of the booths were small and barebones, and many of them belonged to either companies at the periphery of HPC (such as companies that make dripless couplers for liquid cooling) or small startups who just had a desk, a few pens, and some brochures.</p><p>On the first point, it was true--quantum computing was well represented, with 22% of exhibitors identifying as being involved in the field in some form. In fact, quantum felt over-represented, since the ISC technical program certainly didn't have such a large fraction of talks on quantum computing topics. I didn't have time to actually talk with any of these quantum companies though, so wasn't able to get a sense of why the startup ecosystem around quantum computing was so rich in Europe as compared to the US.</p><p>While there was an abundance of quantum this year, a number of the big HPC and HPC-adjacent companies were noticeably absent:</p><ul><li>Amazon, Azure, and Google did not have booths despite having booths last year. Amazon and Google still sponsored the conference at the lowest tier (bronze) though, while Microsoft did not sponsor at all.</li><li>Intel had neither booth nor sponsorship despite having the #3 system on Top500. I don't think they held a party this year, either. AMD didn't have a booth, but they sponsored (and gave the opening keynote!)</li><li>WEKA neither had a booth nor sponsored the conference this year, although they were the leading sponsor of the Student Cluster Competition. Competitors DDN, VAST, Quobyte, and BeeGFS all had booths, but only VAST sponsored. Curiously, Pure and Scality, which do not big footholds in leadership HPC, did both booths and sponsorship.</li></ul><p>These companies who chose not to have a booth still sent people to the conference and were conducting meetings as usual, though. This suggests that there's something amiss with how large companies perceive the return on investment of having a booth at ISC. I don't have any insider knowledge here, but I was surprised by the pullback since ISC has historically been very good at incentivizing attendees to walk through the expo hall by putting it between the technical sessions and the food breaks.</p><p>As I walked the exhibit floor, I found that prominent booths spanned the whole HPC stack: software, system integrators, component makers (CPUs, GPUs, HBM and DDR, and SSD and HDD), and datacenter infrastructure were all exhibiting. The most eye-catching booths were those with big iron on display: HPE/Cray had a full EX4000 cabinet and CDU on display, and there were a few Eviden BullSequana nodes floating around.</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure><figcaption class=\"image-caption\">The Cray EX4000 cabinet (right) and its CDU (left) on display at the ISC'25 exhibition hall. One of the most eye-catching displays, even they've been on display at ISC and SC for a few years now.</figcaption></figure></div><p>Sadly, though, there were no full BullSequana X3000 racks on display. I've still never seen one in real life.</p><p>Infrastructure companies like Motivair (who manufactures the CDUs for Cray EX) and Rittal (which I know as a company that manufactures racks) also had big liquid-liquid head exchangers on display with shiny steel piping. Here's a smaller version of the Cray EX CDU that Motivair was displaying:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure><figcaption class=\"image-caption\">A close-up view of a smaller liquid-liquid heat exchanger CDU on display at the Motivair booth right next to HPE's. Strangely, the mechanics of these systems dovetails with what I've learned as a part of my other hobby outside of HPC, which is operating a multi-family residential high-rise.</figcaption></figure></div><p>I got to chatting with some good folks at Motivair, and I learned that the 1.2 MW variant that is used with Cray EX has a 4\" connection--the same size as the water main in <a href=\"https://glennklockwood.com/garden/LRA\">my coop</a>. Since I recently helped with the replacement of my building's water main, this led me down a rabbithole where I realized that the flow rates for this CDU is roughly the same as my apartment building too, which is to say, a single Cray CDU moves as much fluid as a 55-unit apartment building. Incidentally, a single Cray EX cabinet supports roughly the same electrical capacity as my 55-unit building too--I am in the process of replacing our 1,200 A service panel, which comes out to about the same 400 kVA as fully loaded EX.</p><p>Aside from the Cray cabinets and CDUs, which are no longer new to ISC, I couldn't put my finger on any particularly outstanding booths this year though. The exhibit felt like a sea of smaller companies, none of which really grabbed me. This isn't to say that big vendors were wholly absent though. Despite not having booths, all three big cloud providers threw parties during the week: AWS and NVIDIA teamed up on a big party with over a thousand registrants, while Google and Microsoft held smaller parties towards the end of the week. HPE also threw a lovely event that was off the beaten path along the Elbe, resulting in a less-crowded affair that made it easy to catch up with old friends.</p><p>I may be reading too much into this year's exhibit, but it felt like ISC might be transforming into an event for smaller companies to gain visibility in the HPC market, while larger companies apply their pennies only in the parts of the conference with the highest return. Whether a company chose to have a booth, sponsor the conference, and/or throw a party seemed to defy a consistent pattern though, so perhaps other factors were at play this year.</p><h2 id=\"cloud-or-lack-thereof\">Cloud, or lack thereof</h2><p>Because I work for a large cloud service provider, I attended as many cloud HPC sessions as I could, and frankly, I was disappointed. The clear message I got by the end of the week was that Europe--or perhaps just ISC--doesn't really care about the cloud. This is quite different from the view in the US, where the emergence of <a href=\"https://www.glennklockwood.com/garden/systems/Eagle\">massive AI supercomputers</a> has begun to shift opinions to the point where <a href=\"https://www.theregister.com/2024/07/24/oak_ridge_discovery/\">the successor to the Frontier supercomputer at OLCF might wind up in the cloud</a>. I suppose cloud is a lot less attractive outside of the US, since all the major cloud providers are US corporations, but the way in which cloud topics were incorporated into the ISC program this year sometimes felt like a box-checking exercise.</p><p>For example, I attended the BOF on \"Towards a Strategy for Future Research Infrastructures\" which I expected to be a place where we discussed the best ways to integrate traditional HPC with stateful services and other workflow components. While cloud was mentioned by just about every panelist, it was almost always in a throwaway statement, lumped in with \"the edge\" or cited as a vague benefit to \"new workflows and interactive analysis\" with no further detail. One speaker even cited egress fees as a big challenge which, to me, means they haven't actually talked to a cloud provider in the last five to ten years. If egress fees are what stop you from using the cloud, you're talking to the wrong account team.</p><p>I get it though; there are times where cloud often doesn't offer enough obvious benefit for HPC to justify the effort required to figure it out. In those cases, it's incumbent on cloud providers to provide a better story. But I was also disappointed by the invited session called \"Bridging the Gap: HPC in the Cloud and Cloud Technologies in HPC,\" which I hoped would be the place where cloud providers could make this case. Instead, only two of the three CSPs were even invited to speak, and it was clear that the speakers did not all get the same assignment with their invitations. Granted, the CSP for whom I work was the one not invited (so I came in a little biased), but I was surprised by how differently each speaker used their time.</p><p>Dr. Maxime Martinasso from CSCS gave a talk from the perspective of trying to add cloud-like capabilities to a supercomputer, which is a recurring pattern across a number of sites (including many in the US DOE) and projects. He explained the way they're creating an infrastructure-as-code domain-specific language that sits on top of <a href=\"https://www.glennklockwood.com/garden/systems/Alps\">Alps</a>, their Cray EX system, to give users the ability to bring their own software stacks (all the way down through Slurm) to the supercomputer. It was clearly a ton of work on CSCS's part to develop this capability, and yet the talk's \"future work\" slide contained a bunch of features which those of us in the cloud would consider \"P0\"--priority zero, or essential for a minimum viable product.</p><p>By the end of Martinasso's talk, I realized that CSCS's perspective is that, unlike commercial cloud, these cloudy features aren't P0; having a supercomputer on the floor is. He made the case that CSCS has a need to explore diverse computing architectures and accelerators (as evidenced by the five different node types in Alps!), and putting them all on a single RDMA fabric isn't something any cloud provider will do. As a result, adding any new cloud-like capability to the heterogeneous supercomputer is just gravy, and the fact that true cloud is more \"cloudy\" than Alps is irrelevant since the cloud will never support the intra-fabric heterogeneity that Alps does.</p><p>The other two speakers represented big cloud providers, and their talks had a bit more product pitch in them. One speaker talked through the challenges the cloud is facing in trying to fold supercomputing principles into existing cloud infrastructure (a theme I repeated in my talk later in the week) before talking about specific products that have arisen from that. It touched on some interesting technologies that the HPC world hasn't yet adopted (like optical circuit switching--super cool stuff for programmable fabrics), and I learned a few things about how that provider might bring new HPC capabilities to the table for specific workloads.</p><p>The other speaker, though, presented a textbook pitch deck. I've give almost the same exact presentation, down to showing the same sort of customer stories and product comparison tables, during customer briefings. Execs in the audience would eat it up while engineers' eyes would glaze over, and having to do that song and dance is partly why I didn't make it as a product manager. <a href=\"https://bsky.app/profile/glennklockwood.com/post/3lrd3usnt222d\">I was incredulous</a> that such a presentation was an invited talk at one of the most prestigious HPC conferences in the world.</p><p>This is not to say I was mad at the speaker. He did exactly what one would expect from a leader in the sales side of an organization, hitting all the notes you'd want in a textbook pitch aimed at the C-suite. Rather, I was disappointed by the choice by the session organizers; when you invite someone whose job is driving business at one of the largest cloud providers to speak, you should fully expect a broad and salesy presentation. I don't think it's a stretch to say that most ISC attendees aren't looking for these sorts of high-level talks designed for enterprise decision-makers; they want insight and technical depth.</p><p>Was I miffed that a competitor got to give a twenty-minute sales pitch during a session at which I wasn't invited to speak? Absolutely. And do I think I could've given a talk that even the most ardent cloud-hater would find something interesting in it? Probably. But since that didn't happen, the best I can do is complain about it on the Internet and hope that next year's program committee puts more care into organizing an invited speaker session on cloud and HPC.</p><p>Thankfully, I was given the opportunity to talk a little about my work at the <a href=\"https://sites.google.com/view/supercompcloud/isc25-9th-supercompcloud-workshop#h.fur7sdv6h19a\">SuperCompCloud workshop</a> on Friday. That workshop felt like what the \"Bridging the Gap\" invited session should've been, and there were roughly equal parts of presentations on adding cloud-like features to their HPC infrastructure and adding HPC-like features to cloud infrastructure. From my perspective, the workshop was great; I got to see how traditional HPC centers are adopting cloud practices into their operations, and I could explain how we overcame some of the challenges they're facing in Azure. But to my point at the outset of this section--that Europe doesn't really care about the cloud--the majority of speakers at SuperCompCloud were American.</p><h2 id=\"parting-thoughts\">Parting thoughts</h2><p>As I said at the outset, there were way more sessions that I missed than I attended. In addition, a lot of the big headlines of the week were coincident with, not made at, the conference. A few noteworthy announcements during the week that I won't go into detail about include:</p><ol type=\"1\"><li><a href=\"https://www.ed.ac.uk/news/university-set-to-host-ps750m-national-supercomputer\">£750M was awarded to EPCC</a> to deploy what sounds like the UK's first exascale system. This announcement's overlap with ISC was a total coincidence, so EPCC didn't have many details to share.</li><li><a href=\"https://ultraethernet.org/ultra-ethernet-consortium-uec-launches-specification-1-0-transforming-ethernet-for-ai-and-hpc-at-scale/\">The Ultra Ethernet Consortium announced the long-awaited version 1 of its spec</a>. I'm not sure how relevant this is to HPC yet, but given how many networking talks compared themselves against InfiniBand, I think there's a lot of appetite for a high-performance, non-proprietary alternative.</li><li>Sadly, <a href=\"https://www.hpcwire.com/2025/06/11/farwell-hpc-guru/\">HPC_Guru announced his retirement</a> mid-week as well. It's not clear this was deliberately timed with ISC, but it was acknowledged on the big stage during the ISC closing statements and resulted in a lot of <a href=\"https://bsky.app/profile/hpcguru.bsky.social/post/3lrcsdbwa522c\">recognition</a> <a href=\"https://x.com/hpc_guru/status/1932688759310725425?s=61\">online</a>. I credit HPC_Guru, whoever he is, with a lot of the success I've enjoyed in my career, as he amplified my voice as far back as 2009 when I first started on Twitter. Maybe with his retirement, I should try to do for others what he did for me.</li></ol><p>And along the lines of reflecting back over the years, this was ISC's 40th anniversary, and the organizers had a few wonderful features to commemorate the milestone. Addison Snell organized a panel where a variety of attendees got to discuss the impact that the conference has had on them over the past 40 years, and I was delighted to find that I was not the only person to <a href=\"https://glennklockwood.com/garden/ISC-conference#isc-40th-anniversary-panel\">reflect back on how ISC has shaped my career</a>. As critical as I can be of specific speakers and sessions when I write up these notes, I do hope it goes without saying that I wouldn't bother doing all this for a conference that wasn't deeply engaging and rewarding to be a part of.</p><p>Going back to this year's theme of connecting the dots, I think it's apt. Some ways in which HPC connected dots at ISC this year were obvious; the conference brought together people with a common interest in high-performance computing from across 54 countries and seven continents this year. But this year's conference also made it clear that the role of HPC going forward may be connecting the dots between different technologies being developed for AI, cloud, enterprise, and other markets and the problems in scientific computing that need to be solved.</p><p>The latest and greatest Blackwell GPUs barely registered at ISC this year, and the HPC community seems OK with that now. Instead of the focus being on the absolute top-end in high-performance accelerators, HPC's focus was on connecting the dots between last generation's GPUs and today's grand challenges in science. Instead of showcasing the newest innovations in secure computing in the cloud, HPC's focus was in connecting the dots between a few relevant pieces of zero trust and big-iron on-prem supercomputers.</p><p>HPC has always been about figuring out ways to use stuff invented for someone else to solve scientific challenges--connecting the dots. Beowulf clusters started that way, GPGPU computing started that way, and emulating DGEMMs (and other primitives) on AI accelerators will probably follow the same pattern. But different nations are drawing different lines between the dots; while the US might draw a shorter line between commercial cloud and HPC at scale, Europe is drawing shorter lines between HPC for scientific computing and HPC for sovereign AI.</p><p>If we accept that connecting the dots may be where the HPC community can make the most impact, then it's fitting that ISC chose to carry forward the theme of \"connecting the dots\" into ISC'26. This break from the tradition of introducing a new tagline each year suggests that, at times, optimizing what we already have can take us further than than pursuing something completely new. After 40 years, ISC remains not only a showcase of innovation, but a reflection of how the HPC community (and its role in the technology landscape) is evolving. If we continue to embrace this theme of stitching together breakthroughs instead of spotlighting them individually, the HPC community is likely to be more relevant than ever alongside--not in spite of--the overwhelming momentum of hyperscale and AI.</p>",
            "url": "https://hpc.social/personal-blog/2025/isc-25-recap/",
            
            
            
            
            
            "date_published": "2025-06-24T05:58:00-06:00",
            "date_modified": "2025-06-24T05:58:00-06:00",
            
                "author": "Glenn K. Lockwood's Blog"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2025/surfing-the-singularity-adventures-in-quantum-chemistry/",
            "title": "Surfing the Singularity- Adventures in Quantum Chemistry",
            "summary": null,
            "content_text": "&lt;div class=\"separator\" style=\"clear: both; text-align: center;\"&gt;&lt;/div&gt;&lt;div class=\"separator\" style=\"clear: both; text-align: center;\"&gt;&lt;/div&gt;In this installment of the Surfing the Singularity blog we go vlog, giving an overview of quantum computing today with application to chemistry. Quantum computing is rapidly advancing, with improvements in machine size, error correction, and scalability. And yet, there's always a desire to drive towards advancements and scientific applications which are just out of reach of today's technologies. New algorithms lead the way.&nbsp;In this video, will give a brief overview of quantum computing, what it means, where we are on the product roadmaps, and explore an emergent algorithm for pushing the boundaries of chemical modeling beyond what is possible with today's classical machines. Enjoy.&nbsp;- andy&nbsp;P.S. Begging your forgiveness for being a YouTube newb...&nbsp;&lt;p&gt;&lt;/p&gt;",
            "content_html": "<div class=\"separator\" style=\"clear: both; text-align: center;\"><br /></div><p><br />&lt;div class=\"separator\" style=\"clear: both; text-align: center;\"&gt;&lt;/div&gt;<br />&lt;div class=\"separator\" style=\"clear: both; text-align: center;\"&gt;<br />&lt;/div&gt;</p><p>In this installment of the Surfing the Singularity blog we go vlog, giving an overview of quantum computing today with application to chemistry. Quantum computing is rapidly advancing, with improvements in machine size, error correction, and scalability. And yet, there's always a desire to drive towards advancements and scientific applications which are just out of reach of today's technologies. New algorithms lead the way.&nbsp;</p><p>In this video, will give a brief overview of quantum computing, what it means, where we are on the product roadmaps, and explore an emergent algorithm for pushing the boundaries of chemical modeling beyond what is possible with today's classical machines. Enjoy.&nbsp;</p><p>- andy&nbsp;</p><p><br /></p><p>P.S. Begging your forgiveness for being a YouTube newb...&nbsp;</p><p></p><div class=\"separator\" style=\"clear: both; text-align: center;\"><br /></div><p><br />&lt;p&gt;&lt;/p&gt;</p>",
            "url": "https://hpc.social/personal-blog/2025/surfing-the-singularity-adventures-in-quantum-chemistry/",
            
            
            
            
            
            "date_published": "2025-03-11T13:11:00-06:00",
            "date_modified": "2025-03-11T13:11:00-06:00",
            
                "author": "Surfing the Singularity"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2025/llm-training-without-a-parallel-file-system/",
            "title": "LLM training without a parallel file system",
            "summary": null,
            "content_text": "The illustrious Jeff Denworth recently posted a hot take across social media, claiming that training large language models (LLMs) doesn't require massive, expensive parallel file systems:As someone who's been working on one of the largest supercomputers on the planet--one that has no parallel file system at all--I was surprised by how many incredulous or curious responses followed. I guess supercomputers and parallel file systems are like peas and carrots in so many people's minds that the idea of being able to run a massive parallel compute job without a massive parallel file system is so unintuitive that it is unbelievable.I've given talks about how LLM training uses storage in the past, but I realized I've never written it down. So, for the benefit of humankind, let's talk about how these supercomputers without parallel file systems work.The workloadThough the actual model training on giant GPU supercomputers gets all the attention, the full process of training an LLM is a little more involved. A colleague of mine at Microsoft gave a great overview of this storage-centric, end-to-end picture at SNIA SDC24; broadly, training an LLM involves the following steps:Data ingestion: This is where crawlers scrape the Internet and pull down raw html, images, videos, and other media. These raw data are indexed and shoved into a data warehouse. At scale, this can be hundreds or thousands of petabytes of data for frontier models.Data preparation: This is where the raw data is converted into tokenized data. It amounts to a huge data analytics problem that uses well-documented text and image processing pipelines that filter, deduplicate, and otherwise clean the raw garbage on the Internet using frameworks like Apache Spark. The hundreds of petabytes of input get reduced down by 10x-1000x.Model training: This is where the tokenized data is shoveled through the LLM on giant GPU clusters in little batches. As the data is processed, the model weights are updated, and those weights are checkpointed to storage. If a compute node crashes and the job fails, that checkpoint is used to restart, just like a traditional scientific HPC application. There might be fine-tuning and the like happening as part of this too, but I won't talk about that.Model deployment and inferencing: This is where the final model is copied across giant fields of inferencing servers, and a web service sits in front of it all to transform REST API requests into actual inferencing queries that run on the GPUs. This isn't training, but we'll talk about it anyway.To understand why a parallel file system offers no particular benefit to any of these steps, let's take a closer look at what's going on in each one.Data ingestionData ingestion is a widely distributed process that involves minimal computation; you just need a lot of Internet-facing network connectivity and CPU cores to drive independent processes connecting to other people's public HTTP servers. I don't know a lot about what this process looks like, because it never relies on anything resembling a supercomputer.To the best of my knowledge, data ingestion just pulls HTML, images, or video streams from the Internet and packs them into data containers. As it is packing webpages into these files, it is building a separate index that stores metadata about the webpage (URL, encoding, date of access) and its location (the file in which the webpage's contents are stored and the byte offset within that file). Thousands of VMs might be performing these tasks completely independently, and because they do not need to synchronize with each other at any step, it can be better to distribute these scrapers around the world rather than centralize all of them in a single datacenter.While one could store each scraped HTML page in a file that's organized in a parallel file system, accessing those files would be very slow--a full crawl of all the data would require scanning hundreds of billions of little files. So instead of implementing data containers using files and the index using a file system directory tree, it's better to implement data containers on top of object stores and use a distributed key-value store for the index. The fact that scraped data is write-once (and therefore doesn't need features like file locking or read-modify-write), is a natural fit for object stores' design around object immutability.Data preparationOnce raw data is indexed and saved in object stores, the first phase of computation comes into play. I've documented this data processing pipeline on my LLM training datasets page, but a lot of it amounts to running Apache Spark-like pipelines that chew through all the raw data in a trivially parallel way.These data processing pipelines are very well defined from the days when Hadoop was all the rage, and their data access patterns map well to the strengths of object stores. Each processing task might read a couple hundred megabytes of data from an object all at once, process it in-memory, then dump it back out to objects all at once. File systems offer no benefit here, because each task reads once and writes once rather than skipping around inside individual objects.There is a significant compute workload here, and there are points in the data processing pipeline where global synchronization of all tasks is required. Specifically, the process of deduplicating input data--which is a critical step to getting a high-quality model these days--requires comparing every piece of data to every other piece of data. As a result, this data preparation phase is often done in a centralized location that is adjacent the object store containing all the raw data scraped from the previous step. The clusters used for data processing can resemble traditional CPU-based supercomputers (think a system like TACC's Frontera), and in some cases, they might even have full RDMA fabrics to accelerate the all-to-all deduplication step.Critically, this data processing step is not done on the GPU nodes that actually train the model. Data processing is usually limited by I/O bandwidth to storage, and you never want your GPUs stalling out because they're waiting for data. Parallel file system vendors might tell you that the only way to avoid this GPU starvation issue is to plug every GPU node into a super-fast parallel file system, but the reality is that people just do this I/O-heavy step on completely separate supercomputers before training on GPUs ever begins.CPU nodes are significantly cheaper than GPUs, so buying cheap object storage and a cheap CPU cluster is more cost-effective than buying an expensive file system and wasting your GPU nodes on trivially parallel text processing tasks. To illustrate this, consider some normalized list prices from Azure:$1.00 gets you a 96-core general-purpose VM with 384 GB of RAM$1.65 gets you a 176-core HPC-optimized VM with NDR InfiniBand and 768 GB of RAM$22.55 gets you a 96-core, 8x H100 GPU VM with NDR InfiniBandGiven that GPUs don't give you a 13x-22x speedup for data processing despite the 13x-22x the price, it makes no sense to perform this data processing on GPU nodes inline with training.One could argue that the GPUs are sitting idle while the data processing cluster is working anyway, but rest assured that AI model shops have no shortage of work to keep their GPUs busy. Data processing for the next model on a CPU cluster often happens at the same time the current model is being trained on the GPU cluster. In cases where there isn't enough work to keep both CPU and GPU clusters busy around the clock, also remember that most of this stuff happens in the cloud, and cloud providers can sell those idle CPU or GPU cycles to another customer in between training campaigns.Model trainingHuge, distributed training jobs are where most people would think a fast parallel file system is required for both reading input data and writing out checkpoints. After all, the need for fast checkpointing and restart were the primary driver behind the creation of parallel file systems.While parallel file systems certainly can be used for training, they are not the most cost-effective or scalable way to train across tens of thousands of GPUs. To better illustrate the reasons why this is, let's consider the processes of reading inputs and writing checkpoints separately.Reading training dataTraining a model on GPUs, whether it be on one or a thousand nodes, follows a simple cycle (this is a \"step\" in LLM training parlance) that's repeated over and over:A batch of tokenized data is loaded into GPU memoryThat data is then processed through the neural network and the model weights are adjustedAll GPUs synchronize their updated weightsIt's tempting to imagine the I/O load generated by step #1 as being the same as it would be for a traditional HPC job: data is read from a parallel file system into compute memory at the start of every single step:In years past, storage vendors would've insisted that this repeated, random re-reading of input data at every step requires a super-fast parallel file system to keep up. However, two factors make that untrue:The input data isn't millions of little text or image files. As described in the data ingest and data processing steps, these small files are packaged into large objects before the GPUs ever see them.Tokenized data is very dense compared to raw input, so the amount of bytes being read over the course of hundreds or thousands of steps is actually quite small.To quantify #2, consider the Llama-3 405b model, which was trained on a significant fraction of the public Internet--15.6 trillion tokens. That sounds like a lot of information until you realize that the size of a typical token is between 3 and 5 bytes depending on the tokenizer and encoding. This means that the entire 405-billion parameter Llama-3 model, which was trained using 16,000 GPUs, only had to load 60 TB of tokens from storage. That divides out to 3.75 GB of tokens processed by each GPU over the entire course of a 54-day run.When you consider how few bytes are required to train an LLM, it should become clear that the biggest I/O challenge in the performance-critical training loop isn't raw bandwidth; it's performance variability. As such, the best way to ensure that GPUs do not stall out due to read requests is to eliminate as much I/O performance variability as possible. To do this, you have to minimize the sources of contention that might arise between the storage devices and the network that connects them to the GPUs. While you can do this using sophisticated quality-of-service in both the storage servers and interconnect, there is an easier way.Just stick some local SSDs in every GPU node.This ensures that no contention will occur when loading data from storage into the GPU, because the only network between them is the PCIe on the node. In addition, using node-local NVMe allows storage capacity and storage performance to scale linearly with GPU performance. By comparison, a remote storage system (whether it be parallel file or object) won't get any bigger or faster as you add more GPUs to the training job, resulting in each GPU losing efficiency due to I/O as more GPUs are added to the training job.In practice, model training uses local SSDs like this:At the start of a training job, data is read from remote storage into the local SSDs in a distributed fashion once. Because the tokenized data is so small, many replicas of the entire dataset can be stored across the job's GPU nodes as well; for example, if you were to train Llama-3 405b on NVIDIA DGX H100 nodes, you could fit the entire training dataset (all 60 TB of it) on just three nodes since each node comes with 30 TB of local SSD. Given that the model was trained on 16,000 GPUs (2,000 nodes), that translates to storing hundreds of replicas of the entire training set. This has a few major benefits:GPUs never have to wait for shared storage to return data before they can compute. Everything they need is on the local SSDs.When a GPU node fails, its input data can be recovered from a surviving GPU node over the backend InfiniBand. After training starts, input data never has to be read from shared storage again.It's common to scale up training over time by adding more GPUs (more data-parallel domains) to the job as it stabilizes. When this happens, I/O performance scales linearly because these new GPUs never have to fight over shared storage.A reasonable critique of this approach is that data management becomes more complicated; either the training framework has to keep track of which SSDs and nodes have copies of which input data, or a distributed, client-side shared namespace like WEKA Converged Mode or CoreWeave LOTA has to sit between your application and your data. In practice though, frontier models are trained for exactly one epoch; that is, every input token is processed exactly one time to achieve optimal model quality. Because no two GPUs will ever need to read the same input token, there's never a need to copy input tokens between nodes inside the training loop. I also acknowledge that the above description is greatly simplified; the entire node-local SSD capacity cannot be filled with input data, as space is also needed for checkpoints and other temporary data. However, the fact remains that super high-bandwidth or super high-capacity parallel file systems are not necessary for loading input tokens during training. AI training clusters are built with a ton of local SSDs to do the heavy lifting, and the input data for LLMs is small enough to fit in just a handful of GPU nodes.Writing model checkpointsThough the read workload of LLM training is modest at best, the write workload can be quite intense at scale because the probability of failure increases superlinearly with the size of the training job. However, unlike with scientific HPC jobs, the checkpoint size does not scale as a function of the job size; the checkpoint for a 405 billion-parameter model trained on 16,000 nodes is the same size as the checkpoint for that model trained on three nodes. This is a result of the fact that every training step is followed by a global synchronization which makes each data-parallel copy of the model identical. Only one copy of those model weights, which amounts to under a hundred terabytes for state-of-the-art LLMs, needs to be saved: Kartik and Colleen Tartow at VAST wrote a quantitative breakdown of the true I/O requirements of checkpointing, and they illustrate how even a trillion-parameter model can achieve 99.7% forward progress (only 0.3% time spent checkpointing) when training across 3,072 GPUs with a modest 273 GB/s file system. A parallel file system is not required to get that level of performance; for example, HDD-based Azure Blob achieved over 1 TB/s when benchmarked with IOR for writes at scale.As with reading input tokens though, the real goal for checkpointing at scale is to remove any dependence on shared storage from the training loop entirely. And again, the best way to do this is to simply checkpoint to node-local storage. However, special care must be taken to ensure that the checkpoints don't get lost when a node crashes.In practice, LLM training is now done with asynchronous, multilevel checkpointing. This technique provides the scalability of checkpointing to node-local storage and the durability of shared storage:The key to this checkpointing process is hierarchical data synchronization:Model weights are first copied from GPU memory into the node's CPU memory after every training step. This checkpoint is governed by the CPU-GPU bandwidth (either PCIe or NVLink/Infinity Fabric), and a 500 GB checkpoint can complete in a second. The benefit of checkpointing to DRAM is that the GPU can unblock and begin computing the next step very quickly. However, this checkpoint in DRAM is not protected and will be lost if the node crashes.To protect against node crashes, the checkpoint is then asynchronously copied from CPU DRAM to a neighbor node's local SSD using RDMA. Now if a node crashes, it can restore from a checkpoint that is stored on its neighboring node's SSD via InfiniBand. Reading and writing a 500 GB checkpoint to neighboring SSDs might take ten seconds, so this asynchronous replication might be done for every tenth DRAM checkpoint.To store many checkpoints long-term, checkpoints are also asynchronously copied from node-local SSD to shared storage. This might take a minute or two per 500 GB checkpoint, so this last-level checkpoint copy might be done once every ten minutes.This hierarchical checkpointing scheme allows the GPUs to spend only a second checkpointing while being able to recover from job, node, and even cluster-level failures by tailoring the checkpoint tiering frequencies to the performance of each storage tier being used. The cost of recovering from a catastrophic failure might be re-computing up to ten minutes worth of training, but given the rarity of such events, this scheme balances the performance (and risks) of checkpointing to DRAM against hard drive prices (and suffering their performance) for a durable object store.To this latter point, the requirements of the shared storage system at the bottom of this checkpointing hierarchy are very modest:The checkpoint only needs to complete in the time between successive last-level checkpoint copies. If the 500 GB checkpoint is drained to shared storage only once every ten minutes, our shared storage only needs to deliver 1 GB/s of total bandwidth.The write pattern from node-local NVMe to shared storage is arbitrary, because it is a simple copy operation of a fully formed checkpoint file. Unlike direct-to-storage checkpoints, there are no weirdly shaped tensors being serialized into a file on the fly; rather, opaque bits are streaming from a local checkpoint file into a remote object using whatever transfer size and parallelism gives the highest write bandwidth.This combination of modest write bandwidth and simple, sequential, large-block writes is ideally suited for object stores. This isn't to say a parallel file system cannot work here, but this checkpointing scheme does not benefit from directory structure, fine-grained consistency semantics, or any of the other complexities that drive up the cost of parallel file systems.The catch, of course, is that checkpointing using these schemes can be complicated to implement. Fortunately, a growing number of training frameworks support both writing and restoring checkpoints using asynchronous and hierarchical approaches. Model developers never have to worry about interacting with specific files or objects; instead, the framework manages data locality during checkpoint and restart underneath a high-level API.Model deployment and inferencingOnce a model is trained, putting it into production as an inferencing service is the final step of its lifecycle. From a storage and I/O standpoint, this is a lot more complicated than training because it marries an enterprise service delivery model (failover, load balancing, authentication, and scaling) with copies of a trained model running across HPC infrastructure. When you hear vendors talking about key-value stores, vector databases, and RAG, that is all happening at this stage.Setting aside everything but the storage attached to the GPU cluster though, the I/O requirements of inferencing are relatively straightforward:When provisioning a GPU node for inferencing, model weights must be loaded from shared storage as fast as possible.When using an LLM to search documents, a vector database is required to perform the similarity search that augments the LLM query with the relevant documents. This is the basis for RAG.Key-value caches are often used to reduce the latency for different parts of the inferencing pipeline by storing context including the conversation or frequently accessed contextual documents.As the inferencing demand evolves, different models and weights may be swapped in and out of individual GPU servers.A parallel file system is not particularly useful for any of these; the only place in which their high bandwidth would be a benefit is in loading and re-loading model weights (#1 and #4). But as with hierarchical checkpointing, those I/O operations are whole-object, read-only copies that are a natural fit for object APIs. Complex directory structures and strong consistency simply aren't necessary here.Objects are good enough, maybe betterNone of the steps in this model training lifecycle uniquely benefit from the capabilities that parallel file systems offer:Data ingestion involves hundreds of petabytes of small documents, but they are immediately packaged and indexed into large data containers. Their metadata is stored in a separate key-value store, so the directory hierarchy of a file system isn't used, and once data has been packaged and indexed, it's never modified in-place. The bandwidth requirements are modest as well since web crawling is the rate-limiting step.Data processing is an I/O-intensive data analytics workload. Read bandwidth is critical here, but data is accessed in large transactions and most of the computation is embarrassingly parallel. This workload runs on standalone analytics clusters, so even though the read bandwidth here is rate-limiting, slower storage is not going to impact GPU utilization on training clusters in any way. This step also reduces data by 100x or more, so the write requirements are also modest.Training requires both loading input tokens and checkpointing model weights. However, both of these workloads lean on node-local NVMe in every node to eliminate slowdowns due to noisy neighbors. Input data is staged to node-local storage only once at the beginning of a training campaign, and checkpoints are asynchronously bled out to shared storage without impacting GPU utilization.Inferencing involves infrequent, read-only, bulk loading of model weights into GPU nodes. While key-value caches and vector databases are also used in inferencing, parallel file systems offer no particular benefit for them.The I/O patterns of each of these steps map nicely to object storage since they are predominantly write-once and whole-file transactions. Parallel file systems certainly can be used, and workloads will benefit from the high bandwidth they offer. However, they come with the cost of features that aren't necessary--either literal costs (in the case of appliances or proprietary software) or figurative costs (allocating people to manage the complexities of debugging a parallel file system).The importance of this latter point is hard to appreciate if you've never used a supercomputer without a parallel file systems. However, I recently sat in on the validation of a brand-new H200 training cluster where various InfiniBand congestion and routing issues were being worked out. It wasn't until someone said \"eviction\" in some nontechnical context that I realized that the sporadic file system evictions during fabric instability were simply a non-issue. There was no cleanup of mount points after major fabric events because there was no persistent, fragile client-server state being maintained. I/Os between GPU nodes or nodes and storage might have failed during a rough patch, but they recovered and resumed on their own as soon as the fabric came back. Similarly, identity didn't matter, and all tests could be run as root because there was no implicit trust between the client kernel and remote storage. Removing the dependence between compute nodes, LDAP, and healthy file system mounts completely eliminates many of the challenges of standing up new clusters quickly.An ideal AI training cluster architectureThe workloads I described above form a rough outline for an AI training infrastructure which has:A bunch of GPU nodes with a strong RDMA backend like InfiniBand. Each node should have at least enough node-local SSD to store a substantial amount of the input tokens to be used for training, enough space for hierarchical checkpointing, and enough I/O bandwidth to these SSDs to support draining checkpoints from partner nodes' DRAM in just a few seconds. A separate frontend network that connects to storage is also a good idea; it ensures that asynchronous checkpoint draining won't interfere with weight synchronization in the training loop.A separate CPU cluster for data processing pipelines. A strong backend network will benefit the deduplication step (which is critical to producing high-quality training datasets), but more emphasis should be placed on optimizing large-transaction reads from storage. Given that CPU nodes are so much cheaper than GPU nodes, separating the data processing nodes from training nodes allows you cut more corners when optimizing this CPU cluster. Keeping data processing out-of-band of actual model training means your most data-intensive step (data processing) is decoupled from your most expensive step (training).A scalable object store that supports basic write-once semantics with modest I/O bandwidth at scale. This matches the needs of the workloads with the price-performance of the storage system and simplifies the recovery process if the interconnect between compute and storage gets congested. It can also serve the data needs of all stages of the training pipeline: hundreds of petabytes of raw training data, hundreds of terabytes of input tokens, and tens of terabytes of model weights all have similar performance needs and can be stored on the same infrastructure with the appropriate QOS settings.A pool of general-purpose compute infrastructure for hosting the raw training data indices. This can also be used to support vector databases, raw context documents for RAG, and any other ancillary services required for production inferencing.By eschewing a high-performance parallel file system and localizing I/O performance to inside the GPU cluster with node-local NVMe, a vanilla network between the GPU cluster and the other subsystems is sufficient. Although less high-performance, these non-critical bits (ideally) have lower complexity, maintenance, and supportability as well, allowing (again, ideally) more resources to be sloshed towards supporting the high-value GPU infrastructure.Incidentally, this architecture happens to be how most of the largest AI training clusters on which I work are designed.But parallel files aren't all badOf course, having no parallel file system presents some usability challenges if users are expecting to be able to SSH into a login node and have a complete user environment ready. The user experience for the above infrastructure works best for those who are comfortable developing software in containers and launching pods rather than developing software in vim and submitting Slurm jobs. I do not advocate for throwing out parallel file systems if they're already ingrained in users' workflows!In addition, the latest crop of modern, distributed file systems all now support multi-protocol data access. For example, WEKA, VAST, and Qumulo, all support S3 (object) interfaces as first-class citizens. Users who want the traditional HPC experience can play with their data using a file mount as they always have, while those who are coming in from the cloud-native side have equal access to those same data as objects. Supporting multiprotocol access to data in AI environments doesn't reduce the need to overbuild infrastructure or support stateful file mounts across all compute nodes, but it does provide an onramp for users to get comfortable moving away from the traditional HPC user experience.Finally, a few of the leading-edge parallel-file-system-turned-AI-storage platforms are also shipping features that make them valuable for the deployment and inferencing part of the lifecycle. For example, WEKA has their WARRP reference architecture for RAG, and VAST has its InsightEngine--both use the unique architectures underneath their file interfaces to accelerate vector queries far beyond what you would get from running a vector database on, say, Lustre. These so-called \"AI data platforms,\" despite starting as parallel file systems, are spreading their relevance out to the entire LLM lifecycle, filling needs for file, object, and structured data with a single storage system.This is all to say that parallel file systems aren't bad, and they aren't going anywhere. But they aren't required to train frontier models either, and as I've tried to describe above, some of the largest supercomputers on the planet are designed not to require them.",
            "content_html": "<p>The illustrious Jeff Denworth recently posted a hot take across social media, claiming that training large language models (LLMs) doesn't require massive, expensive parallel file systems:</p><p></p><p><br /></p><p>As someone who's been working on <a href=\"https://glennklockwood.com/garden/systems/Eagle\">one of the largest supercomputers on the planet</a>--one that has no parallel file system at all--I was surprised by how many incredulous or curious responses followed. I guess supercomputers and parallel file systems are like peas and carrots in so many people's minds that the idea of being able to run a massive parallel compute job without a massive parallel file system is so unintuitive that it is unbelievable.</p><p>I've given talks about how LLM training uses storage in the past, but I realized I've never written it down. So, for the benefit of humankind, let's talk about how these supercomputers without parallel file systems work.<span></span></p><p></p><div class=\"separator\" style=\"clear: both; display: none; text-align: center;\"></div><h2 style=\"text-align: left;\">The workload</h2><p>Though the actual model training on giant GPU supercomputers gets all the attention, the full process of training an LLM is a little more involved. A colleague of mine at Microsoft gave <a href=\"https://www.sniadeveloper.org/events/agenda/session/670\">a great overview of this storage-centric, end-to-end picture at SNIA SDC24</a>; broadly, training an LLM involves the following steps:</p><p></p><ol style=\"text-align: left;\"><li><b>Data ingestion</b>: This is where crawlers scrape the Internet and pull down raw html, images, videos, and other media. These raw data are indexed and shoved into a data warehouse. At scale, this can be hundreds or thousands of petabytes of data for <a href=\"https://glennklockwood.com/garden/frontier-model\">frontier models</a>.</li><li><b>Data preparation</b>: This is where the raw data is converted into tokenized data. It amounts to a huge data analytics problem that uses well-documented text and image processing pipelines that filter, deduplicate, and otherwise clean the raw garbage on the Internet using frameworks like Apache Spark. The hundreds of petabytes of input get reduced down by 10x-1000x.</li><li><b>Model training</b>: This is where the tokenized data is shoveled through the LLM on giant GPU clusters in little batches. As the data is processed, the model weights are updated, and those weights are checkpointed to storage. If a compute node crashes and the job fails, that checkpoint is used to restart, just like a traditional scientific HPC application. There might be fine-tuning and the like happening as part of this too, but I won't talk about that.</li><li><b>Model deployment and inferencing</b>: This is where the final model is copied across giant fields of inferencing servers, and a web service sits in front of it all to transform REST API requests into actual inferencing queries that run on the GPUs. This isn't training, but we'll talk about it anyway.</li></ol><p style=\"text-align: left;\">To understand why a parallel file system offers no particular benefit to any of these steps, let's take a closer look at what's going on in each one.</p><h3 style=\"text-align: left;\">Data ingestion</h3><p style=\"text-align: left;\">Data ingestion is a widely distributed process that involves minimal computation; you just need a lot of Internet-facing network connectivity and CPU cores to drive independent processes connecting to other people's public HTTP servers. I don't know a lot about what this process looks like, because it never relies on anything resembling a supercomputer.</p><p style=\"text-align: left;\">To the best of my knowledge, data ingestion just pulls HTML, images, or video streams from the Internet and packs them into <i>data containers</i>. As it is packing webpages into these files, it is building a separate <i>index</i> that stores metadata about the webpage (URL, encoding, date of access) and its location (the file in which the webpage's contents are stored and the byte offset within that file). Thousands of VMs might be performing these tasks completely independently, and because they do not need to synchronize with each other at any step, it can be better to distribute these scrapers around the world rather than centralize all of them in a single datacenter.</p><p style=\"text-align: left;\">While one <i>could</i> store each scraped HTML page in a file that's organized in a parallel file system, accessing those files would be very slow--a full crawl of all the data would require scanning hundreds of billions of little files. So instead of implementing <i>data containers</i> using files and the <i>index</i> using a file system directory tree, it's better to implement data containers on top of object stores and use a distributed key-value store for the index. The fact that scraped data is write-once (and therefore doesn't need features like file locking or read-modify-write), is a natural fit for object stores' design around object immutability.</p><h3 style=\"text-align: left;\">Data preparation</h3><p style=\"text-align: left;\">Once raw data is indexed and saved in object stores, the first phase of computation comes into play. I've documented this data processing pipeline on my <a href=\"https://glennklockwood.com/garden/LLM-training-datasets#computational-requirements\">LLM training datasets page</a>, but a lot of it amounts to running Apache Spark-like pipelines that chew through all the raw data in a trivially parallel way.</p><p style=\"text-align: left;\">These data processing pipelines are very well defined from the days when Hadoop was all the rage, and their data access patterns map well to the strengths of object stores. Each processing task might read a couple hundred megabytes of data from an object all at once, process it in-memory, then dump it back out to objects all at once. File systems offer no benefit here, because each task reads once and writes once rather than skipping around inside individual objects.</p><p style=\"text-align: left;\">There is a significant compute workload here, and there are points in the data processing pipeline where global synchronization of all tasks is required. Specifically, the process of deduplicating input data--which is <a href=\"https://arxiv.org/abs/2107.06499\">a critical step to getting a high-quality model these days</a>--requires comparing every piece of data to every other piece of data. As a result, this data preparation phase is often done in a centralized location that is adjacent the object store containing all the raw data scraped from the previous step. The clusters used for data processing can resemble traditional CPU-based supercomputers (think a system like <a href=\"https://tacc.utexas.edu/systems/frontera/\">TACC's Frontera</a>), and in some cases, they might even have full RDMA fabrics to accelerate the all-to-all deduplication step.</p><p style=\"text-align: left;\">Critically, this data processing step is not done on the GPU nodes that actually train the model. Data processing is usually limited by I/O bandwidth to storage, and you never want your GPUs stalling out because they're waiting for data. Parallel file system vendors might tell you that the only way to avoid this GPU starvation issue is to plug every GPU node into a super-fast parallel file system, but the reality is that people just do this I/O-heavy step on completely separate supercomputers before training on GPUs ever begins.</p><p style=\"text-align: left;\">CPU nodes are significantly cheaper than GPUs, so buying cheap object storage and a cheap CPU cluster is more cost-effective than buying an expensive file system and wasting your GPU nodes on trivially parallel text processing tasks. To illustrate this, consider some normalized list prices from Azure:</p><p style=\"text-align: left;\"></p><ul style=\"text-align: left;\"><li>$1.00 gets you a 96-core general-purpose VM with 384 GB of RAM</li><li>$1.65 gets you a 176-core HPC-optimized VM with NDR InfiniBand and 768 GB of RAM</li><li>$22.55 gets you a 96-core, 8x H100 GPU VM with NDR InfiniBand</li></ul><div>Given that GPUs don't give you a 13x-22x speedup for data processing despite the 13x-22x the price, it makes no sense to perform this data processing on GPU nodes inline with training.</div><p></p><p style=\"text-align: left;\">One could argue that the GPUs are sitting idle while the data processing cluster is working anyway, but rest assured that AI model shops have no shortage of work to keep their GPUs busy. Data processing for the next model on a CPU cluster often happens at the same time the current model is being trained on the GPU cluster. In cases where there isn't enough work to keep both CPU and GPU clusters busy around the clock, also remember that most of this stuff happens in the cloud, and cloud providers can sell those idle CPU or GPU cycles to another customer in between training campaigns.</p><h3 style=\"text-align: left;\">Model training</h3><p style=\"text-align: left;\">Huge, distributed training jobs are where most people would think a fast parallel file system is required for both reading input data and writing out checkpoints. After all, the need for fast checkpointing and restart were the primary driver behind the creation of parallel file systems.</p><p style=\"text-align: left;\">While parallel file systems certainly <i>can</i> be used for training, they are not the most cost-effective or scalable way to train across tens of thousands of GPUs. To better illustrate the reasons why this is, let's consider the processes of reading inputs and writing checkpoints separately.</p><h4 style=\"text-align: left;\">Reading training data</h4><p style=\"text-align: left;\">Training a model on GPUs, whether it be on one or a thousand nodes, follows a simple cycle (this is a \"step\" in LLM training parlance) that's repeated over and over:</p><p style=\"text-align: left;\"></p><ol style=\"text-align: left;\"><li>A batch of tokenized data is loaded into GPU memory</li><li>That data is then processed through the neural network and the model weights are adjusted</li><li>All GPUs synchronize their updated weights</li></ol><p style=\"text-align: left;\">It's tempting to imagine the I/O load generated by step #1 as being the same as it would be for a traditional HPC job: data is read from a parallel file system into compute memory at the start of every single step:</p><p></p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p style=\"text-align: left;\">In years past, storage vendors would've insisted that this repeated, random re-reading of input data at every step requires a super-fast parallel file system to keep up. However, two factors make that untrue:</p><p style=\"text-align: left;\"></p><ol style=\"text-align: left;\"><li>The input data isn't millions of little text or image files. As described in the data ingest and data processing steps, these small files are packaged into large objects before the GPUs ever see them.</li><li>Tokenized data is very dense compared to raw input, so the amount of bytes being read over the course of hundreds or thousands of steps is actually quite small.</li></ol><p></p><p style=\"text-align: left;\">To quantify #2, consider the <a href=\"https://arxiv.org/abs/2407.21783\">Llama-3 405b model</a>, which was trained on a significant fraction of the public Internet--15.6 <i>trillion</i> tokens. That sounds like a lot of information until you realize that <a href=\"https://glennklockwood.com/garden/LLM-training-datasets#tokenized-data\">the size of a typical token is between 3 and 5 bytes</a> depending on the tokenizer and encoding. This means that the entire 405-billion parameter Llama-3 model, which was trained using 16,000 GPUs, only had to load 60 TB of tokens from storage. That divides out to 3.75 GB of tokens processed by each GPU over the entire course of a 54-day run.</p><p style=\"text-align: left;\">When you consider how few bytes are required to train an LLM, it should become clear that the biggest I/O challenge in the performance-critical training loop isn't raw bandwidth; it's performance variability. As such, the best way to ensure that GPUs do not stall out due to read requests is to eliminate as much I/O performance variability as possible. To do this, you have to minimize the sources of contention that might arise between the storage devices and the network that connects them to the GPUs. While you <i>can</i> do this using sophisticated quality-of-service in both the storage servers and interconnect, there is an easier way.</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p style=\"text-align: left;\">Just stick some local SSDs in every GPU node.</p><p style=\"text-align: left;\">This ensures that no contention will occur when loading data from storage into the GPU, because the only network between them is the PCIe on the node. In addition, using node-local NVMe allows storage capacity and storage performance to scale linearly with GPU performance. By comparison, a remote storage system (whether it be parallel file or object) won't get any bigger or faster as you add more GPUs to the training job, resulting in each GPU losing efficiency due to I/O as more GPUs are added to the training job.</p><p style=\"text-align: left;\">In practice, model training uses local SSDs like this:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p style=\"text-align: left;\">At the start of a training job, data is read from remote storage into the local SSDs in a distributed fashion <i>once</i>. Because the tokenized data is so small, many replicas of the entire dataset can be stored across the job's GPU nodes as well; for example, if you were to train Llama-3 405b on NVIDIA DGX H100 nodes, <b>you could fit the entire training dataset (all 60 TB of it) on just three nodes</b> since each node comes with 30 TB of local SSD. Given that the model was trained on 16,000 GPUs (2,000 nodes), that translates to storing hundreds of replicas of the entire training set. This has a few major benefits:</p><p style=\"text-align: left;\"></p><ol style=\"text-align: left;\"><li>GPUs never have to wait for shared storage to return data before they can compute. Everything they need is on the local SSDs.</li><li>When a GPU node fails, its input data can be recovered from a surviving GPU node over the backend InfiniBand. After training starts, input data never has to be read from shared storage again.</li><li>It's common to scale up training over time by adding more GPUs (more data-parallel domains) to the job as it stabilizes. When this happens, I/O performance scales linearly because these new GPUs never have to fight over shared storage.</li></ol><p></p><p style=\"text-align: left;\">A reasonable critique of this approach is that data management becomes more complicated; either the training framework has to keep track of which SSDs and nodes have copies of which input data, or a distributed, client-side shared namespace like <a href=\"https://www.weka.io/resources/solution-brief/weka-data-platform-converged-mode/\">WEKA Converged Mode</a> or <a href=\"https://docs.coreweave.com/docs/products/storage/object-storage/concepts/lota\">CoreWeave LOTA</a> has to sit between your application and your data. In practice though, frontier models are trained for exactly one epoch; that is, <a href=\"https://glennklockwood.com/garden/scaling-laws#applying-scaling-laws\">every input token is processed exactly one time to achieve optimal model quality</a>. Because no two GPUs will ever need to read the same input token, there's never a need to copy input tokens between nodes inside the training loop. </p><p style=\"text-align: left;\">I also acknowledge that the above description is greatly simplified; the entire node-local SSD capacity cannot be filled with input data, as space is also needed for checkpoints and other temporary data. However, the fact remains that super high-bandwidth or super high-capacity parallel file systems are not necessary for loading input tokens during training. AI training clusters are built with a ton of local SSDs to do the heavy lifting, and the input data for LLMs is small enough to fit in just a handful of GPU nodes.</p><h4 style=\"text-align: left;\">Writing model checkpoints</h4><p style=\"text-align: left;\">Though the read workload of LLM training is modest at best, the write workload can be quite intense at scale because the probability of failure increases superlinearly with the size of the training job. However, unlike with scientific HPC jobs, <b>the checkpoint size does not scale as a function of the job size</b>; the checkpoint for a 405 billion-parameter model trained on 16,000 nodes is the same size as the checkpoint for that model trained on three nodes. This is a result of the fact that every training step is followed by a global synchronization which makes each data-parallel copy of the model identical. Only one copy of those model weights, which amounts to under a hundred terabytes for state-of-the-art LLMs, needs to be saved:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><span style=\"text-align: left;\"> </span></div><p style=\"text-align: left;\">Kartik and Colleen Tartow at VAST wrote <a href=\"https://www.vastdata.com/blog/a-checkpoint-on-checkpoints-in-llms\">a quantitative breakdown of the true I/O requirements of checkpointing</a>, and they illustrate how even a trillion-parameter model can achieve 99.7% forward progress (only 0.3% time spent checkpointing) when training across 3,072 GPUs with a modest 273 GB/s file system. A parallel file system is not required to get that level of performance; for example, HDD-based <a href=\"https://x.com/glennklockwood/status/1795548752628867132\">Azure Blob achieved over 1 TB/s when benchmarked with IOR</a> for writes at scale.</p><p style=\"text-align: left;\">As with reading input tokens though, the real goal for checkpointing at scale is to remove any dependence on shared storage from the training loop entirely. And again, the best way to do this is to simply checkpoint to node-local storage. However, special care must be taken to ensure that the checkpoints don't get lost when a node crashes.</p><p style=\"text-align: left;\">In practice, LLM training is now done with asynchronous, multilevel checkpointing. This technique provides the scalability of checkpointing to node-local storage and the durability of shared storage:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p style=\"text-align: left;\">The key to this checkpointing process is hierarchical data synchronization:</p><p style=\"text-align: left;\"></p><ol style=\"text-align: left;\"><li><b>Model weights are first copied from GPU memory into the node's CPU memory</b> after every training step. This checkpoint is governed by the CPU-GPU bandwidth (either PCIe or NVLink/Infinity Fabric), and a 500 GB checkpoint can complete in a second. The benefit of checkpointing to DRAM is that the GPU can unblock and begin computing the next step very quickly. However, this checkpoint in DRAM is not protected and will be lost if the node crashes.</li><li>To protect against node crashes, the <b>checkpoint is then asynchronously copied from CPU DRAM to a neighbor node's local SSD</b> using RDMA. Now if a node crashes, it can restore from a checkpoint that is stored on its neighboring node's SSD via InfiniBand. Reading and writing a 500 GB checkpoint to neighboring SSDs might take ten seconds, so this asynchronous replication might be done for every tenth DRAM checkpoint.</li><li>To store many checkpoints long-term, <b>checkpoints are also asynchronously copied from node-local SSD to shared storage</b>. This might take a minute or two per 500 GB checkpoint, so this last-level checkpoint copy might be done once every ten minutes.</li></ol><p style=\"text-align: left;\">This hierarchical checkpointing scheme allows the GPUs to spend only a second checkpointing while being able to recover from job, node, and even cluster-level failures by tailoring the checkpoint tiering frequencies to the performance of each storage tier being used. The cost of recovering from a catastrophic failure might be re-computing up to ten minutes worth of training, but given the rarity of such events, this scheme balances the performance (and risks) of checkpointing to DRAM against hard drive prices (and suffering their performance) for a durable object store.</p><p style=\"text-align: left;\">To this latter point, the requirements of the shared storage system at the bottom of this checkpointing hierarchy are very modest:</p><p style=\"text-align: left;\"></p><ul style=\"text-align: left;\"><li>The checkpoint only needs to complete in the time between successive last-level checkpoint copies. If the 500 GB checkpoint is drained to shared storage only once every ten minutes, our shared storage only needs to deliver 1 GB/s of total bandwidth.</li><li>The write pattern from node-local NVMe to shared storage is arbitrary, because it is a simple copy operation of a fully formed checkpoint file. Unlike direct-to-storage checkpoints, there are no weirdly shaped tensors being serialized into a file on the fly; rather, opaque bits are streaming from a local checkpoint file into a remote object using whatever transfer size and parallelism gives the highest write bandwidth.</li></ul><p>This combination of modest write bandwidth and simple, sequential, large-block writes is ideally suited for object stores. This isn't to say a parallel file system cannot work here, but this checkpointing scheme does not benefit from directory structure, fine-grained consistency semantics, or any of the other complexities that drive up the cost of parallel file systems.</p><p>The catch, of course, is that checkpointing using these schemes can be complicated to implement. Fortunately, a <a href=\"https://www.linkedin.com/posts/jeffreydenworth_reducing-model-checkpointing-times-by-over-activity-7289273345269800960-zBj7\">growing number of training frameworks</a> support both writing and restoring checkpoints using asynchronous and hierarchical approaches. Model developers never have to worry about interacting with specific files or objects; instead, the framework manages data locality during checkpoint and restart underneath a high-level API.</p><h3 style=\"text-align: left;\">Model deployment and inferencing</h3><p style=\"text-align: left;\">Once a model is trained, putting it into production as an inferencing service is the final step of its lifecycle. From a storage and I/O standpoint, this is a lot more complicated than training because it marries an enterprise service delivery model (failover, load balancing, authentication, and scaling) with copies of a trained model running across HPC infrastructure. When you hear vendors talking about key-value stores, vector databases, and RAG, that is all happening at this stage.</p><p style=\"text-align: left;\">Setting aside everything but the storage attached to the GPU cluster though, the I/O requirements of inferencing are relatively straightforward:</p><p style=\"text-align: left;\"></p><ol style=\"text-align: left;\"><li>When provisioning a GPU node for inferencing, model weights must be loaded from shared storage as fast as possible.</li><li>When using an LLM to search documents, a vector database is required to perform the similarity search that augments the LLM query with the relevant documents. This is the basis for RAG.</li><li>Key-value caches are often used to reduce the latency for different parts of the inferencing pipeline by storing context including the conversation or frequently accessed contextual documents.</li><li>As the inferencing demand evolves, different models and weights may be swapped in and out of individual GPU servers.</li></ol><p style=\"text-align: left;\">A parallel file system is not particularly useful for any of these; the only place in which their high bandwidth would be a benefit is in loading and re-loading model weights (#1 and #4). But as with hierarchical checkpointing, those I/O operations are whole-object, read-only copies that are a natural fit for object APIs. Complex directory structures and strong consistency simply aren't necessary here.</p><h2 style=\"text-align: left;\">Objects are good enough, maybe better</h2><p style=\"text-align: left;\">None of the steps in this model training lifecycle uniquely benefit from the capabilities that parallel file systems offer:</p><p style=\"text-align: left;\"></p><ul style=\"text-align: left;\"><li>Data ingestion involves hundreds of petabytes of small documents, but they are immediately packaged and indexed into large data containers. Their metadata is stored in a separate key-value store, so the directory hierarchy of a file system isn't used, and once data has been packaged and indexed, it's never modified in-place. The bandwidth requirements are modest as well since web crawling is the rate-limiting step.</li><li>Data processing is an I/O-intensive data analytics workload. Read bandwidth is critical here, but data is accessed in large transactions and most of the computation is embarrassingly parallel. This workload runs on standalone analytics clusters, so even though the read bandwidth here is rate-limiting, slower storage is not going to impact GPU utilization on training clusters in any way. This step also reduces data by 100x or more, so the write requirements are also modest.</li><li>Training requires both loading input tokens and checkpointing model weights. However, both of these workloads lean on node-local NVMe in every node to eliminate slowdowns due to noisy neighbors. Input data is staged to node-local storage only once at the beginning of a training campaign, and checkpoints are asynchronously bled out to shared storage without impacting GPU utilization.</li><li>Inferencing involves infrequent, read-only, bulk loading of model weights into GPU nodes. While key-value caches and vector databases are also used in inferencing, parallel file systems offer no particular benefit for them.</li></ul><p style=\"text-align: left;\">The I/O patterns of each of these steps map nicely to object storage since they are predominantly write-once and whole-file transactions. Parallel file systems certainly can be used, and workloads will benefit from the high bandwidth they offer. However, they come with the cost of features that aren't necessary--either literal costs (in the case of appliances or proprietary software) or figurative costs (allocating people to manage the complexities of debugging a parallel file system).</p><p style=\"text-align: left;\">The importance of this latter point is hard to appreciate if you've never used a supercomputer without a parallel file systems. However, I recently sat in on the validation of <a href=\"https://www.top500.org/system/180349/\">a brand-new H200 training cluster</a> where various InfiniBand congestion and routing issues were being worked out. It wasn't until someone said \"eviction\" in some nontechnical context that I realized that the sporadic file system evictions during fabric instability were simply a non-issue. There was no cleanup of mount points after major fabric events because there was no persistent, fragile client-server state being maintained. I/Os between GPU nodes or nodes and storage might have failed during a rough patch, but they recovered and resumed on their own as soon as the fabric came back. Similarly, identity didn't matter, and all tests could be run as root because there was no implicit trust between the client kernel and remote storage. Removing the dependence between compute nodes, LDAP, and healthy file system mounts completely eliminates many of the challenges of standing up new clusters quickly.</p><h3 style=\"text-align: left;\">An ideal AI training cluster architecture</h3><p style=\"text-align: left;\">The workloads I described above form a rough outline for an AI training infrastructure which has:</p><p style=\"text-align: left;\"></p><ol style=\"text-align: left;\"><li><b>A bunch of GPU nodes with a strong RDMA backend like InfiniBand</b>. Each node should have at least enough node-local SSD to store a substantial amount of the input tokens to be used for training, enough space for hierarchical checkpointing, and enough I/O bandwidth to these SSDs to support draining checkpoints from partner nodes' DRAM in just a few seconds. A separate frontend network that connects to storage is also a good idea; it ensures that asynchronous checkpoint draining won't interfere with weight synchronization in the training loop.</li><li><b>A separate CPU cluster for data processing pipelines</b>. A strong backend network will benefit the deduplication step (which is critical to producing high-quality training datasets), but more emphasis should be placed on optimizing large-transaction reads from storage. Given that CPU nodes are so much cheaper than GPU nodes, separating the data processing nodes from training nodes allows you cut more corners when optimizing this CPU cluster. Keeping data processing out-of-band of actual model training means your most data-intensive step (data processing) is decoupled from your most expensive step (training).</li><li><b>A scalable object store that supports basic write-once semantics with modest I/O bandwidth at scale</b>. This matches the needs of the workloads with the price-performance of the storage system and simplifies the recovery process if the interconnect between compute and storage gets congested. It can also serve the data needs of all stages of the training pipeline: hundreds of petabytes of raw training data, hundreds of terabytes of input tokens, and tens of terabytes of model weights all have similar performance needs and can be stored on the same infrastructure with the appropriate QOS settings.</li><li><b>A pool of general-purpose compute infrastructure for hosting the raw training data indices</b>. This can also be used to support vector databases, raw context documents for RAG, and any other ancillary services required for production inferencing.</li></ol><p style=\"text-align: left;\">By eschewing a high-performance parallel file system and localizing I/O performance to inside the GPU cluster with node-local NVMe, a vanilla network between the GPU cluster and the other subsystems is sufficient. Although less high-performance, these non-critical bits (ideally) have lower complexity, maintenance, and supportability as well, allowing (again, ideally) more resources to be sloshed towards supporting the high-value GPU infrastructure.</p><p style=\"text-align: left;\">Incidentally, this architecture happens to be how most of the largest AI training clusters on which I work are designed.</p><h3 style=\"text-align: left;\">But parallel files aren't all bad</h3><p style=\"text-align: left;\">Of course, having no parallel file system presents some usability challenges if users are expecting to be able to SSH into a login node and have a complete user environment ready. The user experience for the above infrastructure works best for those who are comfortable developing software in containers and launching pods rather than developing software in vim and submitting Slurm jobs. <i>I do not advocate for throwing out parallel file systems if they're already ingrained in users' workflows!</i></p><p style=\"text-align: left;\">In addition, the latest crop of modern, distributed file systems all now support multi-protocol data access. For example, <a href=\"https://docs.weka.io/4.0/additional-protocols/s3\">WEKA</a>, <a href=\"https://support.vastdata.com/s/article/UUID-67c215f7-63a8-5d58-196e-5066199a6f60\">VAST</a>, and <a href=\"https://docs.qumulo.com/administrator-guide/s3-api/configuring-using-s3-api.html\">Qumulo</a>, all support S3 (object) interfaces as first-class citizens. Users who want the traditional HPC experience can play with their data using a file mount as they always have, while those who are coming in from the cloud-native side have equal access to those same data as objects. Supporting multiprotocol access to data in AI environments doesn't reduce the need to overbuild infrastructure or support stateful file mounts across all compute nodes, but it does provide an onramp for users to get comfortable moving away from the traditional HPC user experience.</p><p style=\"text-align: left;\">Finally, a few of the leading-edge parallel-file-system-turned-AI-storage platforms are also shipping features that make them valuable for the deployment and inferencing part of the lifecycle. For example, WEKA has their <a href=\"https://www.weka.io/resources/reference-architecture/warrp-weka-ai-rag-reference-platform/\">WARRP reference architecture for RAG</a>, and <a href=\"https://www.vastdata.com/press-releases/vast-data-unveils-vast-insightengine-with-nvidia\">VAST has its InsightEngine</a>--both use the unique architectures underneath their file interfaces to accelerate vector queries far beyond what you would get from running a vector database on, say, Lustre. These so-called \"AI data platforms,\" despite starting as parallel file systems, are spreading their relevance out to the entire LLM lifecycle, filling needs for file, object, and structured data with a single storage system.</p><p style=\"text-align: left;\">This is all to say that parallel file systems aren't bad, and they aren't going anywhere. But they aren't required to train frontier models either, and as I've tried to describe above, some of the largest supercomputers on the planet are designed not to require them.</p><p></p><p></p><p></p><p></p>",
            "url": "https://hpc.social/personal-blog/2025/llm-training-without-a-parallel-file-system/",
            
            
            
            
            
            "date_published": "2025-02-02T03:59:00-07:00",
            "date_modified": "2025-02-02T03:59:00-07:00",
            
                "author": "Glenn K. Lockwood's Blog"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2025/surfing-the-singularity-the-world-is-not-flat/",
            "title": "Surfing the Singularity - The World is Not Flat",
            "summary": null,
            "content_text": "As Bill Gates recalls in his recent book-bumping interview with the Wall Street Journal, in the early innocent days of Microsoft he and his co-founder Paul Allen didn't believe in having an office in Washington, D.C.[1] They were soon to learn that was a mistake.[2] Compare and contrast with the scene in the Capital Rotunda last week for the inauguration of the new populist administration - Microsoft, Amazon, Facebook, Apple, Google, TikTok, and of course Tesla, all represented by their CEOs.[3] Microsoft's market capitalization as of this writing is now greater than the GDP of France.[4] Elon Musk's personal wealth is on par with the GDP of Denmark. Meta's platforms reach an estimated 40% of the world's population. Apple ad from yesteryear. We're now long past 1984.Consider these other inconvenient truths about global technology: that NVIDIA does not make the GPUs it designs, that most are manufactured by TSMC in Taiwan, which is about as far away from China as Cuba is from Florida. Software talent is globally distributed, and prices vary widely. There are some very good schools in some of these relatively inexpensive places - in the 2024 edition of the ACM student programming contest, MIT placed the highest among US teams, in 11th place.[5] Software salaries 2023, by country, exchange rate normalized.[6]AI programs and quantum computing initiatives are increasingly becoming nationalized as a strategic imperative. It is assumed that the country which is first to Artificial General Intelligence (AGI) will be the first to be able to use it to take control of the world. This fear is fueling an arms race in AI architectures, and the chips and the energy stations which power them. But is this a rational fear? Unlearn What You Have LearnedDeepSeek, with its deja vu inducing name and similarly eerily similar whale icon, is a new AI chatbot model wholly owned out of Hangzhou, China. And its #1 on the Apple app store, with a bullet. And yes, its quite up front that it tracks your data. There are several notable features and claims about this model: that is was trained in a fraction of the typical cost and time on a fraction of the typical hardware. That it performs about as well the OpenAI model released last month, maybe not as well on some things like straight math, but perhaps better at general writing, maybe with a bit more \"personality\". It shows its work - how it arrived at the answer its providing to the chat prompt. And, for the kicker from the totalitarian state, its open source up on Hugging Face.[7] Shots fired. NVIDIA shares tumbled for a loss of $600B in one day in response, the largest single day loss in history. But what I'd like to know is, if this is the open source model, what's the real one like? Recent Federal governments have tried to enforce policies limiting technology exports to China, but in truth they've long had their own development programs - China has not participated in standardized HPC metric sharing and supercomputer ranking since 2017.Red Flag on the TrackAlong with NVIDIA, power generation stocks were also down hard - GE Vernova down almost 20%. This does not mean the recent trend of tech companies buying nuclear power plants won't continue, or that we won't continue to hear of existing power stations giving data centers a direct hard wire bypassing the municipal grid. But clearly this open ended hunger for power - watts and GPU cycles - is not sustainable. And DeepSeek exposes that bare.But make no mistake - this race is not over, its just warming up. OpenAI with their new Operators product currently defines AGI as a gaggle of collaborating AI agents, each with its own unique set of capabilities and goals. NVIDIA CEO Jansen Huang does his part in driving the GPU-dependent AI hype cycle by saying IT departments will become the new HR departments - for AI employees.[9] Goldman Sachs is telling clients to expect AI employees this year.[10] Cost avoidance will be a major driver.[11] And why not, when the same major technology companies report seemly amazing results using AI for software development? Google saves 50% on code migration time with AI! It gives *me* FOMO! [12]The CEO of Anthropic predicts that by 2027 AI will be generally better than humans at almost everything.[13] Well, at some things maybe better than others. Turns out, what's the number one occupation we expect to be replaced by AI? Why, AI engineers and data scientists.Job skills impacted by generative AI, ranked. Maybe I should have been a plumber.[14]This is vast uncharted territory for companies, especially for those of size, wedded to their legacy political structures and being either un-nimble or worse, fragile. People are not machines. The mistakes AI agents make are not of the same kind made by humans - current generative AI models are intentionally designed to make stuff up, to not just say \"I don't know\".[15] As a manager, you'll get no benefit of human insights into the truth such as from body language, though you may get to understand the \"personality\" and general performance characteristics of your AI employees over time. That is, until the managers are replaced by AI. But until then, what does your management interface look like? Is it perhaps similar to the IDE for a senior software engineer who manages a team of AI coders? In the AI-laced future, there will still be a place for HCI/UX designers.Chef of the FutureFinally, the November 2024 report from the National Academies on the \"future of work\" says \"its impossible to predict exactly the nature of the coming changes in AI and all their effects on the economy and society\".[16] This includes how it changes the nature of various jobs, or outright eliminates them. Continuing education will be key to a resilient workforce, and it turns out, AI might even play a role in that.As shown in Washington last week, there are new business trends being driven by a rejuvenated alliance between big technology and big government, and it is therefore time for astute technology and business leaders to pay attention to both.[17] For example, tomorrow (January 30, 2025) OpenAI is holding a previously scheduled closed door meeting in Washington regarding its own current agentic technology innovations, and what they potentially imply for the US people and its government. I imagine the topic of DeepSeek will now disrupt the meeting agenda, somewhat. At minimum, its a topic for a future blog. Regards. - andyReferences[0] Photo by AJ Colores on Unsplash, https://unsplash.com/@ajcolores      [1] Bill Gates interview by the Wall Street Journal, January 2025, https://www.youtube.com/watch?v=4LL-ynK_exM[2] US vs. Microsoft, https://en.wikipedia.org/wiki/United_States_v._Microsoft_Corp[3] https://apnews.com/article/trump-inauguration-tech-billionaires-zuckerberg-musk-wealth-0896bfc3f50d941d62cebc3074267ecd[4] https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)[5] https://icpc.global/worldfinals/results [6] https://www.reddit.com/r/dataisbeautiful/comments/17a63yo/oc_2023_developer_compensation_by_country taken from the 2023 Stack Overflow developer survey.[7] DeepSeek at Hugging Face: https://huggingface.co/organizations/deepseek-ai/activity/all[8] OpenAI Operators: https://www.nytimes.com/2025/01/23/technology/openai-operator-launch.html [9] NVIDIA CEO Jensen Huang on IT as the new HR: https://www.aol.com/finance/nvidia-jensen-huang-says-become-133641793.html[10] Goldman Sachs on the rise of AI employees: https://it.slashdot.org/story/25/01/21/2213230/managing-ai-agents-as-employees-is-the-challenge-of-2025-says-goldman-sachs-cio[11] https://www.msn.com/en-us/money/markets/why-cost-avoidance-became-an-ai-buzzword-for-holding-down-headcount/ar-BB1rmJSx[12] https://developers.slashdot.org/story/25/01/17/2156235/google-reports-halving-code-migration-time-with-ai-help[13] https://arstechnica.com/ai/2025/01/anthropic-chief-says-ai-could-surpass-almost-all-humans-at-almost-everything-shortly-after-2027/[14] https://reports.weforum.org/docs/WEF_Future_of_Jobs_Report_2025.pdf[15] https://slashdot.org/story/25/01/23/1645242/ai-mistakes-are-very-different-from-human-mistakes[16] https://nap.nationalacademies.org/resource/27644/interactive/[17] https://hbr.org/2024/11/navigating-the-new-geopolitics-of-tech",
            "content_html": "<div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>As Bill Gates recalls in his recent book-bumping interview with the Wall Street Journal, in the early innocent days of Microsoft he and his co-founder Paul Allen didn't believe in having an office in Washington, D.C.[1] They were soon to learn that was a mistake.[2] Compare and contrast with the scene in the Capital Rotunda last week for the inauguration of the new populist administration - Microsoft, Amazon, Facebook, Apple, Google, TikTok, and of course Tesla, all represented by their CEOs.[3] Microsoft's market capitalization as of this writing is now greater than the GDP of France.[4] Elon Musk's personal wealth is on par with the GDP of Denmark. Meta's platforms reach an estimated 40% of the world's population. </p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Apple ad from yesteryear. We're now long past 1984.</td></tr></tbody></table><p>Consider these other inconvenient truths about global technology: that NVIDIA does not make the GPUs it designs, that most are manufactured by TSMC in Taiwan, which is about as far away from China as Cuba is from Florida. Software talent is globally distributed, and prices vary widely. There are some very good schools in some of these relatively inexpensive places - in the 2024 edition of the ACM student programming contest, MIT placed the highest among US teams, in 11th place.[5] </p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Software salaries 2023, by country, exchange rate normalized.[6]</td></tr></tbody></table><p>AI programs and quantum computing initiatives are increasingly becoming nationalized as a strategic imperative. It is assumed that the country which is first to Artificial General Intelligence (AGI) will be the first to be able to use it to take control of the world. This fear is fueling an arms race in AI architectures, and the chips and the energy stations which power them. But is this a rational fear? </p><h3 style=\"text-align: left;\">Unlearn What You Have Learned</h3><p>DeepSeek, with its deja vu inducing name and similarly eerily similar whale icon, is a new AI chatbot model wholly owned out of Hangzhou, China. And its #1 on the Apple app store, with a bullet. And yes, its quite up front that it tracks your data. There are several notable features and claims about this model: that is was trained in a fraction of the typical cost and time on a fraction of the typical hardware. That it performs about as well the OpenAI model released last month, maybe not as well on some things like straight math, but perhaps better at general writing, maybe with a bit more \"personality\". It shows its work - how it arrived at the answer its providing to the chat prompt. And, for the kicker from the totalitarian state, its open source up on Hugging Face.[7] </p><p>Shots fired. NVIDIA shares tumbled for a loss of $600B in one day in response, the largest single day loss in history. But what I'd like to know is, if this is the open source model, what's the real one like? Recent Federal governments have tried to enforce policies limiting technology exports to China, but in truth they've long had their own development programs - China has not participated in standardized HPC metric sharing and supercomputer ranking since 2017.</p><h3 style=\"text-align: left;\">Red Flag on the Track</h3><p>Along with NVIDIA, power generation stocks were also down hard - GE Vernova down almost 20%. This does not mean the recent trend of tech companies buying nuclear power plants won't continue, or that we won't continue to hear of existing power stations giving data centers a direct hard wire bypassing the municipal grid. But clearly this open ended hunger for power - watts and GPU cycles - is not sustainable. And DeepSeek exposes that bare.</p><p>But make no mistake - this race is not over, its just warming up. OpenAI with their new Operators product currently defines AGI as a gaggle of collaborating AI agents, each with its own unique set of capabilities and goals. NVIDIA CEO Jansen Huang does his part in driving the GPU-dependent AI hype cycle by saying IT departments will become the new HR departments - for AI employees.[9] Goldman Sachs is telling clients to expect AI employees this year.[10] Cost avoidance will be a major driver.[11] And why not, when the same major technology companies report seemly amazing results using AI for software development? Google saves 50% on code migration time with AI! It gives *me* FOMO! [12]</p><p>The CEO of Anthropic predicts that by 2027 AI will be generally better than humans at almost everything.[13] Well, at some things maybe better than others. Turns out, what's the number one occupation we expect to be replaced by AI? Why, AI engineers and data scientists.</p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Job skills impacted by generative AI, ranked. <br />Maybe I should have been a plumber.[14]</td></tr></tbody></table><p>This is vast uncharted territory for companies, especially for those of size, wedded to their legacy political structures and being either un-nimble or worse, fragile. People are not machines. The mistakes AI agents make are not of the same kind made by humans - current generative AI models are intentionally designed to make stuff up, to not just say \"I don't know\".[15] As a manager, you'll get no benefit of human insights into the truth such as from body language, though you may get to understand the \"personality\" and general performance characteristics of your AI employees over time. That is, until the managers are replaced by AI. But until then, what does your management interface look like? Is it perhaps similar to the IDE for a senior software engineer who manages a team of AI coders? In the AI-laced future, there will still be a place for HCI/UX designers.</p><h3 style=\"text-align: left;\">Chef of the Future</h3><p>Finally, the November 2024 report from the National Academies on the \"future of work\" says \"its impossible to predict exactly the nature of the coming changes in AI and all their effects on the economy and society\".[16] This includes how it changes the nature of various jobs, or outright eliminates them. Continuing education will be key to a resilient workforce, and it turns out, AI might even play a role in that.</p><p>As shown in Washington last week, there are new business trends being driven by a rejuvenated alliance between big technology and big government, and it is therefore time for astute technology and business leaders to pay attention to both.[17] For example, tomorrow (January 30, 2025) OpenAI is holding a previously scheduled closed door meeting in Washington regarding its own current agentic technology innovations, and what they potentially imply for the US people and its government. I imagine the topic of DeepSeek will now disrupt the meeting agenda, somewhat. </p><p>At minimum, its a topic for a future blog. Regards. - andy</p><p><br /></p><h3 style=\"text-align: left;\">References</h3><div><div><span style=\"font-size: x-small;\">[0] Photo by AJ Colores on Unsplash, https://unsplash.com/@ajcolores</span></div><div><span style=\"font-size: x-small;\">      </span></div><div><span style=\"font-size: x-small;\">[1] Bill Gates interview by the Wall Street Journal, January 2025, https://www.youtube.com/watch?v=4LL-ynK_exM</span></div><div><span style=\"font-size: x-small;\"><br /></span></div><div><span style=\"font-size: x-small;\">[2] US vs. Microsoft, https://en.wikipedia.org/wiki/United_States_v._Microsoft_Corp</span></div><div><span style=\"font-size: x-small;\"><br /></span></div><div><span style=\"font-size: x-small;\">[3] https://apnews.com/article/trump-inauguration-tech-billionaires-zuckerberg-musk-wealth-0896bfc3f50d941d62cebc3074267ecd</span></div><div><span style=\"font-size: x-small;\"><br /></span></div><div><span style=\"font-size: x-small;\">[4] https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)</span></div><div><span style=\"font-size: x-small;\"><br /></span></div><div><span style=\"font-size: x-small;\">[5] https://icpc.global/worldfinals/results </span></div><div><span style=\"font-size: x-small;\"><br /></span></div><div><span style=\"font-size: x-small;\">[6] https://www.reddit.com/r/dataisbeautiful/comments/17a63yo/oc_2023_developer_compensation_by_country taken from the 2023 Stack Overflow developer survey.</span></div><div><span style=\"font-size: x-small;\"><br /></span></div><div><span style=\"font-size: x-small;\">[7] DeepSeek at Hugging Face: https://huggingface.co/organizations/deepseek-ai/activity/all</span></div><div><span style=\"font-size: x-small;\"><br /></span></div><div><span style=\"font-size: x-small;\">[8] OpenAI Operators: https://www.nytimes.com/2025/01/23/technology/openai-operator-launch.html</span></div><div><span style=\"font-size: x-small;\"> </span></div><div><span style=\"font-size: x-small;\">[9] NVIDIA CEO Jensen Huang on IT as the new HR: https://www.aol.com/finance/nvidia-jensen-huang-says-become-133641793.html</span></div><div><span style=\"font-size: x-small;\"><br /></span></div><div><span style=\"font-size: x-small;\">[10] Goldman Sachs on the rise of AI employees: https://it.slashdot.org/story/25/01/21/2213230/managing-ai-agents-as-employees-is-the-challenge-of-2025-says-goldman-sachs-cio</span></div><div><span style=\"font-size: x-small;\"><br /></span></div><div><span style=\"font-size: x-small;\">[11] https://www.msn.com/en-us/money/markets/why-cost-avoidance-became-an-ai-buzzword-for-holding-down-headcount/ar-BB1rmJSx</span></div><div><span style=\"font-size: x-small;\"><br /></span></div><div><span style=\"font-size: x-small;\">[12] https://developers.slashdot.org/story/25/01/17/2156235/google-reports-halving-code-migration-time-with-ai-help</span></div><div><span style=\"font-size: x-small;\"><br /></span></div><div><span style=\"font-size: x-small;\">[13] https://arstechnica.com/ai/2025/01/anthropic-chief-says-ai-could-surpass-almost-all-humans-at-almost-everything-shortly-after-2027/</span></div><div><span style=\"font-size: x-small;\"><br /></span></div><div><span style=\"font-size: x-small;\">[14] https://reports.weforum.org/docs/WEF_Future_of_Jobs_Report_2025.pdf</span></div><div><span style=\"font-size: x-small;\"><br /></span></div><div><span style=\"font-size: x-small;\">[15] https://slashdot.org/story/25/01/23/1645242/ai-mistakes-are-very-different-from-human-mistakes</span></div><div><span style=\"font-size: x-small;\"><br /></span></div><div><span style=\"font-size: x-small;\">[16] https://nap.nationalacademies.org/resource/27644/interactive/</span></div><div><span style=\"font-size: x-small;\"><br /></span></div><div><span style=\"font-size: x-small;\">[17] https://hbr.org/2024/11/navigating-the-new-geopolitics-of-tech</span></div><div><span style=\"font-size: x-small;\"><br /></span></div><div><br /></div></div>",
            "url": "https://hpc.social/personal-blog/2025/surfing-the-singularity-the-world-is-not-flat/",
            
            
            
            
            
            "date_published": "2025-01-29T18:11:00-07:00",
            "date_modified": "2025-01-29T18:11:00-07:00",
            
                "author": "Surfing the Singularity"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2025/fine-tuning-ai-models-with-instructlab-under-ibm-lsf/",
            "title": "Fine tuning AI models with InstructLab under IBM LSF",
            "summary": null,
            "content_text": "OverviewAll the best for 2025! This blog looks back on a demo which I created for SC24last November to demonstrate InstructLab workflows running on an IBM LSFcluster. Let’s begin with a bit of background. I’d like to thank MichaelSpriggs, STSM, IBM LSF for his contributions to this blog.When I think of tuning, what immediately comes to my mind are visions of anexpert mechanic trying to extract the most from an engine. This blog isfocused on an entirely different type of tuning, AI model tuning. Like tuningan engine, AI model tuning can be used to ensure a better fit for a given AImodel for your business.Released by IBM and Red Hat in May 2024, InstructLab is an open-source projectwhich provides the ability to fine-tune LLMs by adding skills and knowledge,without having to retrain the model from scratch. InstructLab can run onresource-constrained systems such as laptops, but also supports GPUs. Much hasbeen written about InstructLab and this blog is not intended to provide anin-depth look at InstructLab. Rather, the objective here is to demonstrate howInstructLab workloads can be distributed and managed in a high-performancecomputing cluster with GPUs using the IBM LSF workload scheduler. Recently, IBMpublished a paper describing the infrastructure used to train the Granite familyof AI foundation models. The paper describes the Vela and Blue Vela environmentsin detail. In particular, the Blue Vela environment is built on a software stackusing Red Hat Enterprise Linux, IBM LSF and Storage Scale. Learn more in thedetailed paper here.The demo workflow consists of two LSF jobs. The first job generates syntheticdata, which is used to teach the LLM new skills or knowledge. The second job,which depends upon the successful completion of the first, is the training job,where the new skills or knowledge are incorporated into an existing base model.A simple LSF job dependency is used to ensure the training job only runs afterthe successful completion of the synthetic data generation step.The environment used is equipped with Nvidia GPUs.  InstructLab jobs will berun with the options for GPU support, and the jobs will be submitted to LSFwith the appropriate GPU scheduling directives. Furthermore, it is assumed thatthe users' $HOME directory is available on all hosts in the cluster. Note that Irequire neither root access, nor a user account that is an LSF administrator, toinstall and use InstructLab on the LSF cluster.ConfigurationThe HPC cluster is configured as follows:Red Hat Enterprise Linux v8.8IBM LSF v10.0.1.15InstructLab v0.19.4Miniforge v3 (24.9.0-0)NVIDIA CUDA v12.6Compute nodes are equipped with 8 x Nvidia H100 GPUsInstall InstructLabLog in to a compute node in the LSF cluster equipped with GPUs. If ssh accessis disabled to compute nodes, then submit an interactive LSF batch job. This jobrequests 8 GPUs on a single system and will set them to exclusive executionmode.$ bsub -Is -R \"span[hosts=1]\" -gpu \"num=8:j_exclusive=yes\" bashInstall and set up a Conda environment. This will enable you to install aself-contained Conda environment for your user account with the necessaryPython version needed for InstructLab. Miniforge is installed in the defaultlocation and the option to update the users shell profile to start the Condaenvironment are selected. We assume here a shared $HOME directory.$ cd $HOME$ curl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"$ bash Miniforge3-$(uname)-$(uname -m).shBefore proceeding, you must logout and log back in to activate theenvironment. Next, a Conda environment is created with name my_env. Here we’llspecify Python v3.11, which is a requirement for InstructLab.conda create --name my_env -c anaconda python=3.11conda activate my_envNext, install InstructLab. Here, version 0.19.4 of InstructLab is specified.This was the version of InstructLab available in the timeframe preceding theSC24 event. Follow the installation steps in the official InstructLabdocumentation here.$ pip install instructlab==0.19.4Next, perform the installation of InstructLab with Nvidia CUDA support. Thisis required for InstructLab to utilize the GPUs. Without this step, InstructLabwill run on the CPUs. Note that CUDA v12.6 is installed on the system and thevariables set below reflect this.$ export CMAKE_ARGS=\"-DLLAMA_CUBLAS=on -DCUDA_PATH=/usr/local/cuda-12.6 -DCUDAToolkit_ROOT=/usr/local/cuda-12.6 -DCUDAToolkit_INCLUDE_DIR=/usr/local/cuda-12/include -DCUDAToolkit_LIBRARY_DIR=/usr/local/cuda-12.6/lib64\"$ export PATH=/usr/local/cuda-12.6/bin:$PATH$ pip cache remove llama_cpp_python$ CMAKE_ARGS=\"-DLLAMA_CUDA=on -DLLAMA_NATIVE=off\" pip install 'instructlab[cuda]'$ pip install vllm@git+https://github.com/opendatahub-io/vllm@v0.6.2Configure InstructLabWith the installation of InstructLab complete, the next step is to run theinitialization. This will setup paths to models, taxonomy repo as well as theGPU configuration.$ ilab config initBy default InstructLab stores models, training checkpoints and other fileswithin ~/.cache and ~/.local/share/instructlab. If you have limited storagecapacity available in $HOME, then you may opt to disable training checkpointfiles. This can be done by setting the following option in ~/.config/instructlab/config.yaml as follows.train:  checkpoint_at_epoch: falseNext, we download the required models. The ilab model list command can beused to list the models which are available. Note that a HuggingFace token isrequired to download certain models. Please set HF_TOKEN in the environmentwith the appropriate token.$ export HF_TOKEN=&lt;HuggingFace token&gt;$ ilab model download$ ilab model download --repository=instructlab/granite-7b-lab$ ilab model list+--------------------------------------+---------------------+---------+| Model Name                           | Last Modified       | Size    |+--------------------------------------+---------------------+---------+| instructlab/granite-7b-lab           | 2024-12-27 20:37:29 | 12.6 GB || mistral-7b-instruct-v0.2.Q4_K_M.gguf | 2024-12-27 16:55:46 | 4.1 GB  || merlinite-7b-lab-Q4_K_M.gguf         | 2024-12-27 16:48:39 | 4.1 GB  |+--------------------------------------+---------------------+---------+Generate synthetic data &amp; AI model trainingNext, is the synthetic data generation step, which will be executed on GPUs.This step is a prerequisite to teaching the LLM new skills/knowledge viatraining.Here we use example knowledge from the InstructLab github about Taylor Swiftfans, who are known as “Swifties”. This is timely because Taylor Swift recentlywrapped up 6 concerts in Toronto, Canada, where I happen to be based. Copyattribution.txt and qna.yaml from the following location.By default, the InstructLab taxonomy is found in ~/.local/share/instructlab/taxonomy. Here we create the directories fandom/swifties under ~/.local/share/instructlab/taxonomy/knowledge/arts/fandom and copy the files from step 1 intothis location.$ mkdir -p ~/.local/share/instructlab/taxonomy/knowledge/arts/fandom/swifties$ cp &lt;path_to&gt;/attribution.txt ~/.local/share/instructlab/taxonomy/knowledge/arts/fandom/swifties$ cp &lt;path_to&gt;/qna.yaml ~/.local/share/instructlab/taxonomy/knowledge/arts/fandom/swiftiesWith the Swifties taxonomy in place, check for any syntax errors with thecommand ilab taxonomy diff. It should report that the taxonomy is valid ifthere are no syntax errors.$ ilab taxonomy diffknowledge/arts/fandom/swifties/qna.yamlTaxonomy in /u/gsamu/.local/share/instructlab/taxonomy is valid :)With the taxonomy in place and having confirmed that the syntax is valid,it’s now time to run the synthetic data generation job through LSF. Here we willrequest 8 GPUs on a single server in exclusive execution mode. For theInstructLab ilab command, specify the &ndash;gpus 8 and &ndash;pipeline full options.Standard output is written to the $HOME/job-output with filename specification&lt;LSF_JOBID&gt;.out. The $HOME/job-output directory must already exist.$ mkdir -p $HOME/job-output$ bsub -o $HOME/job-output/%J.out -R \"span[hosts=1]\" -gpu \"num=8:j_exclusive=yes\" ilab data generate --pipeline full --gpus 8Job &lt;1131&gt; is submitted to default queue &lt;normal&gt;.During job execution, the LSF bpeek command can be used to monitor the jobstandard output.$ bpeek -f 1131 &lt;&lt; output from stdout &gt;&gt;INFO 2025-01-02 09:51:29,503 numexpr.utils:146: Note: detected 96 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.INFO 2025-01-02 09:51:29,504 numexpr.utils:149: Note: NumExpr detected 96 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.INFO 2025-01-02 09:51:29,504 numexpr.utils:162: NumExpr defaulting to 16 threads.INFO 2025-01-02 09:51:30,038 datasets:59: PyTorch version 2.3.1 available.INFO 2025-01-02 09:51:31,226 instructlab.model.backends.llama_cpp💯 Trying to connect to model server at http://127.0.0.1:8000/v1WARNING 2025-01-02 09:51:56,356 instructlab.data.generate:270: Disabling SDG batching - unsupported with llama.cpp servingGenerating synthetic data using 'full' pipeline, '/u/gsamu/.cache/instructlab/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf' model, '/u/gsamu/.local/share/instructlab/taxonomy' taxonomy, against http://127.0.0.1:55779/v1 serverINFO 2025-01-02 09:51:56,861 instructlab.sdg.generate_data:356: Synthesizing new instructions. If you aren't satisfied with the generated instructions, interrupt training (Ctrl-C) and try adjusting your YAML files. Adding more examples may help.INFO 2025-01-02 09:51:56,872 instructlab.sdg.pipeline:153: Running pipeline single-threadedINFO 2025-01-02 09:51:56,872 instructlab.sdg.pipeline:197: Running block: duplicate_document_colINFO 2025-01-02 09:51:56,872 instructlab.sdg.pipeline:198: Dataset({    features: ['icl_document', 'document', 'document_outline', 'domain', 'icl_query_1', 'icl_query_2', 'icl_query_3', 'icl_response_1', 'icl_response_2', 'icl_response_3'],    num_rows: 35})INFO 2025-01-02 09:51:58,286 instructlab.sdg.llmblock:51: LLM server supports batched inputs: FalseINFO 2025-01-02 09:51:58,286 instructlab.sdg.pipeline:197: Running block: gen_spellcheckINFO 2025-01-02 09:51:58,286 instructlab.sdg.pipeline:198: Dataset({    features: ['icl_document', 'document', 'document_outline', 'domain', 'icl_query_1', 'icl_query_2', 'icl_query_3', 'icl_response_1', 'icl_response_2', 'icl_response_3', 'base_document'],    num_rows: 35})/u/gsamu/miniforge3/envs/my_env/lib/python3.11/site-packages/llama_cpp/llama.py:1054: RuntimeWarning: Detected duplicate leading \"&lt;s&gt;\" in prompt, this will likely reduce response quality, consider removing it...  warnings.warn(INFO 2025-01-02 09:57:42,264 instructlab.sdg.pipeline:197: Running block: flatten_auxiliary_columnsINFO 2025-01-02 09:57:42,264 instructlab.sdg.pipeline:198: Dataset({    features: ['icl_document', 'document', 'document_outline', 'domain', 'icl_query_1', 'icl_query_2', 'icl_query_3', 'icl_response_1', 'icl_response_2', 'icl_response_3', 'base_document', 'spellcheck'],    num_rows: 35})INFO 2025-01-02 09:57:42,279 instructlab.sdg.pipeline:197: Running block: rename_to_document_columnINFO 2025-01-02 09:57:42,279 instructlab.sdg.pipeline:198: Dataset({    features: ['icl_document', 'document', 'document_outline', 'domain', 'icl_query_1', 'icl_query_2', 'icl_query_3', 'icl_response_1', 'icl_response_2', 'icl_response_3', 'dataset_type', 'corrected_document'],    num_rows: 70})INFO 2025-01-02 09:57:42,282 instructlab.sdg.pipeline:197: Running block: gen_knowledgeINFO 2025-01-02 09:57:42,282 instructlab.sdg.pipeline:198: Dataset({    features: ['icl_document', 'raw_document', 'document_outline', 'domain', 'icl_query_1', 'icl_query_2', 'icl_query_3', 'icl_response_1', 'icl_response_2', 'icl_response_3', 'dataset_type', 'document'],    num_rows: 70})……During the runtime of the job, it’s possible to view GPU related metricsusing the LSF lsload and bhosts commands. First, we need to identify the hostwhere the job has been dispatched to using the LSF bjobs command. In this casethe job was dispatched to host p1-r01-n4. Note that details GPU accountingmetrics are available once the job runs to completion.$ bjobs -wJOBID   USER    STAT  QUEUE      FROM_HOST   EXEC_HOST   JOB_NAME   SUBMIT_TIME1131    gsamu   RUN   normal     rmf-login-1 p1-r01-n4   ilab data generate --pipeline full --gpus 8 Jan  2 14:51$ lsload -w -gpu p1-r01-n4HOST_NAME                 status ngpus gpu_shared_avg_mut gpu_shared_avg_ut ngpus_physicalp1-r01-n4                     ok     8                 2%                7%              8$ bhosts -w -gpu p1-r01-n4HOST_NAME            GPU_ID                MODEL     MUSED      MRSV  NJOBS    RUN   SUSP    RSV p1-r01-n4                 0   NVIDIAH10080GBHBM3        2G        0G      1      1      0      0                          1   NVIDIAH10080GBHBM3        2G        0G      1      1      0      0                          2   NVIDIAH10080GBHBM3        2G        0G      1      1      0      0                          3   NVIDIAH10080GBHBM3        2G        0G      1      1      0      0                          4   NVIDIAH10080GBHBM3        2G        0G      1      1      0      0                          5   NVIDIAH10080GBHBM3        2G        0G      1      1      0      0                          6   NVIDIAH10080GBHBM3        2G        0G      1      1      0      0                          7   NVIDIAH10080GBHBM3        2G        0G      1      1      0      0After job completion, it’s possible to view details about the job includingGPU utilization which LSF collects by leveraging NVIDIA DCGM. These metrics areavailable upon job completion using both the LSF bhist and bjobs commands.$ bhist -l -gpu 1131Job &lt;1131&gt;, User &lt;gsamu&gt;, Project &lt;default&gt;, Command &lt;ilab data generate --pipe                          line full --gpus 8&gt;Thu Jan  2 14:51:23 2025: Submitted from host &lt;rmf-login-1&gt;, to Queue &lt;normal&gt;,                           CWD &lt;$HOME&gt;, Output File &lt;/u/gsamu/job-output/%J.out                          &gt;, Requested Resources &lt;span[hosts=1]&gt;, Requested GPU                           &lt;num=8:j_exclusive=yes&gt;;Thu Jan  2 14:51:24 2025: Dispatched 1 Task(s) on Host(s) &lt;p1-r01-n4&gt;, Allocate                          d 1 Slot(s) on Host(s) &lt;p1-r01-n4&gt;, Effective RES_REQ                           &lt;select[((ngpus&gt;0)) &amp;&amp; (type == local)] order[r15s:p                          g] rusage[ngpus_physical=8.00] span[hosts=1] &gt;;Thu Jan  2 14:51:25 2025: Starting (Pid 3095851);Thu Jan  2 14:51:25 2025: External Message \"p1-r01-n4:gpus=0,1,2,3,4,5,6,7;EFFE                          CTIVE GPU REQ: num=8:mode=shared:mps=no:j_exclusive=y                          es:gvendor=nvidia;\" was posted from \"gsamu\" to messag                          e box 0;Thu Jan  2 14:51:26 2025: Running with execution home &lt;/u/gsamu&gt;, Execution CWD                           &lt;/u/gsamu&gt;, Execution Pid &lt;3095851&gt;;Thu Jan  2 16:08:05 2025: Done successfully. The CPU time used is 4624.0 second                          s;                          HOST: p1-r01-n4; CPU_TIME: 4624 seconds                                                        GPU ID: 0                                  Total Execution Time: 4597 seconds                                  Energy Consumed: 579704 Joules                                  SM Utilization (%): Avg 9, Max 15, Min 0                                  Memory Utilization (%): Avg 2, Max 100, Min 0                                  Max GPU Memory Used: 1956642816 bytes                              GPU ID: 1                                  Total Execution Time: 4597 seconds                                  Energy Consumed: 503956 Joules                                  SM Utilization (%): Avg 7, Max 11, Min 0                                  Memory Utilization (%): Avg 2, Max 5, Min 0                                  Max GPU Memory Used: 1767899136 bytes                              GPU ID: 2                                  Total Execution Time: 4597 seconds                                  Energy Consumed: 501754 Joules                                  SM Utilization (%): Avg 7, Max 11, Min 0                                  Memory Utilization (%): Avg 2, Max 5, Min 0                                  Max GPU Memory Used: 1784676352 bytes                              GPU ID: 3                                  Total Execution Time: 4597 seconds                                  Energy Consumed: 525195 Joules                                  SM Utilization (%): Avg 7, Max 11, Min 0                                  Memory Utilization (%): Avg 2, Max 54, Min 0                                  Max GPU Memory Used: 1767899136 bytes                              GPU ID: 4                                  Total Execution Time: 4597 seconds                                  Energy Consumed: 525331 Joules                                  SM Utilization (%): Avg 7, Max 12, Min 0                                  Memory Utilization (%): Avg 2, Max 5, Min 0                                  Max GPU Memory Used: 1767899136 bytes                              GPU ID: 5                                  Total Execution Time: 4597 seconds                                  Energy Consumed: 502416 Joules                                  SM Utilization (%): Avg 7, Max 11, Min 0                                  Memory Utilization (%): Avg 2, Max 5, Min 0                                  Max GPU Memory Used: 1784676352 bytes                              GPU ID: 6                                  Total Execution Time: 4597 seconds                                  Energy Consumed: 508720 Joules                                  SM Utilization (%): Avg 7, Max 12, Min 0                                  Memory Utilization (%): Avg 2, Max 5, Min 0                                  Max GPU Memory Used: 1784676352 bytes                              GPU ID: 7                                  Total Execution Time: 4597 seconds                                  Energy Consumed: 491041 Joules                                  SM Utilization (%): Avg 6, Max 12, Min 0                                  Memory Utilization (%): Avg 2, Max 4, Min 0                                  Max GPU Memory Used: 1933574144 bytesGPU Energy Consumed: 4138117.000000 JoulesThu Jan  2 16:08:05 2025: Post job process done successfully;GPU_ALLOCATION: HOST             TASK GPU_ID  GI_PLACEMENT/SIZE    CI_PLACEMENT/SIZE    MODEL        MTOTAL  FACTOR MRSV    SOCKET NVLINK/XGMI                       p1-r01-n4        0    0       -                    -                    NVIDIAH10080 80G     9.0    0G      0      -                                                 0    1       -                    -                    NVIDIAH10080 80G     9.0    0G      0      -                                                 0    2       -                    -                    NVIDIAH10080 80G     9.0    0G      0      -                                                 0    3       -                    -                    NVIDIAH10080 80G     9.0    0G      0      -                                                 0    4       -                    -                    NVIDIAH10080 80G     9.0    0G      1      -                                                 0    5       -                    -                    NVIDIAH10080 80G     9.0    0G      1      -                                                 0    6       -                    -                    NVIDIAH10080 80G     9.0    0G      1      -                                                 0    7       -                    -                    NVIDIAH10080 80G     9.0    0G      1      -                               MEMORY USAGE:MAX MEM: 2 Gbytes;  AVG MEM: 1 Gbytes; MEM Efficiency: 0.00%CPU USAGE:CPU PEAK: 1.69 ;  CPU PEAK DURATION: 52 second(s)CPU AVERAGE EFFICIENCY: 100.69% ;  CPU PEAK EFFICIENCY: 169.23%Summary of time in seconds spent in various states by  Thu Jan  2 16:08:05 2025  PEND     PSUSP    RUN      USUSP    SSUSP    UNKWN    TOTAL  1        0        4601     0        0        0        4602 When the synthetic data generation job completes, it’s output can be viewedat ~/job-output/.out. The synthetic data sets will comprise files inthe directory ~/.local/share/instructlab/datasets. These files will be named*skills_train_msgs_*.jsonl* and *knowledge_train_msgs_*.jsonl*.With the synthetic data generation step complete, it’s now time to run thetraining. We first set 2 environment variables to point to the followingfiles:  ~/.local/share/instructlab/datasets/knowledge_train_msgs_2025-01-02T09_51_56.jsonl  and ~./.local/share/instructlab/datasets/skills_train_msgs_2025-01-02T09_51_56.jsonl.Afterward, we submit the training job to LSF requesting 8 GPUs and with ilaboptions &ndash;pipeline accelerated, &ndash;gpus 8, &ndash;device cuda and&ndash;data-path pointing to the two above data files that were produced in thesynthetic data generation step.$ export SKILLS_PATH=/u/gsamu/.local/share/instructlab/datasets/skills_train_msgs_2025-01-02T09_51_56.jsonl$ export KNOWLEDGE_PATH=/u/gsamu/.local/share/instructlab/datasets/knowledge_train_msgs_2025-01-02T09_51_56.jsonl$ bsub -o $HOME/job-output/%J.out -R \"span[hosts=1]\" -gpu \"num=8:j_exclusive=yes\" ilab model train --pipeline accelerated --data-path $SKILLS_PATH --data-path $KNOWLEDGE_PATH --device cuda --gpus 8Job &lt;1135&gt; is submitted to default queue &lt;normal&gt;.During job execution, the LSF bpeek command can be used to monitor thejob standard output.$ bpeek -f 1135&lt;&lt; output from stdout &gt;&gt;LoRA is disabled (rank=0), ignoring all additional LoRA args[2025-01-02 12:52:04,359] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)INFO 2025-01-02 12:52:09,061 numexpr.utils:146: Note: detected 96 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.INFO 2025-01-02 12:52:09,061 numexpr.utils:149: Note: NumExpr detected 96 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.INFO 2025-01-02 12:52:09,061 numexpr.utils:162: NumExpr defaulting to 16 threads.INFO 2025-01-02 12:52:09,304 datasets:59: PyTorch version 2.3.1 available.You are using the default legacy behaviour of the &lt;class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'&gt;. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.INFO 2025-01-02 12:52:09,653 root:617: Special tokens: eos: [32000], pad: [32001], bos: [32005], system: [32004], user: [32002], assistant: [32003]INFO 2025-01-02 12:52:09,923 root:617: number of dropped samples: 0 -- out of 641 data arguments are:{\"data_path\":\"/u/gsamu/.local/share/instructlab/datasets/knowledge_train_msgs_2025-01-02T09_51_56.jsonl\",\"data_output_path\":\"/u/gsamu/.local/share/instructlab/internal\",\"max_seq_len\":4096,\"model_path\":\"/u/gsamu/.cache/instructlab/models/instructlab/granite-7b-lab\",\"chat_tmpl_path\":\"/u/gsamu/miniforge3/envs/my_env/lib/python3.11/site-packages/instructlab/training/chat_templates/ibm_generic_tmpl.py\",\"num_cpu_procs\":16}tokenizing the dataset with /u/gsamu/.cache/instructlab/models/instructlab/granite-7b-lab tokenizer...ten largest length percentiles:quantile 90th: 1459.0quantile 91th: 1466.0quantile 92th: 1469.6000000000001quantile 93th: 1478.2quantile 94th: 1483.0quantile 95th: 1488.0quantile 96th: 1497.1999999999998quantile 97th: 1516.5999999999997quantile 98th: 1540.6000000000001quantile 99th: 1656.0000000000016quantile 100th: 2578.0at 4096 max sequence length, the number of samples to be dropped is 0(0.00% of total)quantile 0th: 368.0quantile 1th: 393.0quantile 2th: 411.2quantile 3th: 421.2quantile 4th: 427.2quantile 5th: 442.0quantile 6th: 604.4quantile 7th: 631.8quantile 8th: 653.8000000000001quantile 9th: 679.8quantile 10th: 742.0at 20 min sequence length, the number of samples to be dropped is 0checking the validity of the samples...Categorizing training data type...unmasking the appropriate message content... Samples Previews...……During the runtime of the training job, we can observe some GPU utilizationinformation using the LSF lsload and bhosts commands.  First we need to identifythe server on which the training job is running. This is done using the bjobscommand and checking for the execution host of the job.$ bjobs -wJOBID   USER    STAT  QUEUE      FROM_HOST   EXEC_HOST   JOB_NAME   SUBMIT_TIME1135    gsamu   RUN   normal     rmf-login-1 p1-r01-n1   ilab model train --pipeline accelerated --data-path /u/gsamu/.local/share/instructlab/datasets/skills_train_msgs_2025-01-02T09_51_56.jsonl --data-path /u/gsamu/.local/share/instructlab/datasets/knowledge_train_msgs_2025-01-02T09_51_56.jsonl --device cuda --gpus 8 Jan  2 17:51$ lsload -w -gpu p1-r01-n1HOST_NAME                 status ngpus gpu_shared_avg_mut gpu_shared_avg_ut ngpus_physicalp1-r01-n1                     ok     8                 0%               22%              8$ bhosts -w -gpu p1-r01-n1HOST_NAME            GPU_ID                MODEL     MUSED      MRSV  NJOBS    RUN   SUSP    RSV p1-r01-n1                 0   NVIDIAH10080GBHBM3       10G        0G      1      1      0      0                          1   NVIDIAH10080GBHBM3       10G        0G      1      1      0      0                          2   NVIDIAH10080GBHBM3       10G        0G      1      1      0      0                          3   NVIDIAH10080GBHBM3       10G        0G      1      1      0      0                          4   NVIDIAH10080GBHBM3       10G        0G      1      1      0      0                          5   NVIDIAH10080GBHBM3       10G        0G      1      1      0      0                          6   NVIDIAH10080GBHBM3       10G        0G      1      1      0      0                          7   NVIDIAH10080GBHBM3       10G        0G      1      1      0      0Once the job is complete, detailed GPU accounting can again be viewed usingthe LSF bhist command as follows below.$ bhist -l -gpu 1135Job &lt;1135&gt;, User &lt;gsamu&gt;, Project &lt;default&gt;, Command &lt;ilab model train --pipeli                          ne accelerated --data-path /u/gsamu/.local/share/inst                          ructlab/datasets/skills_train_msgs_2025-01-02T09_51_5                          6.jsonl --data-path /u/gsamu/.local/share/instructlab                          /datasets/knowledge_train_msgs_2025-01-02T09_51_56.js                          onl --device cuda --gpus 8&gt;Thu Jan  2 17:51:48 2025: Submitted from host &lt;rmf-login-1&gt;, to Queue &lt;normal&gt;,                           CWD &lt;$HOME/.local/share/instructlab/checkpoints&gt;, Ou                          tput File &lt;/u/gsamu/job-output/%J.out&gt;, Requested Res                          ources &lt;span[hosts=1]&gt;, Requested GPU &lt;num=8:j_exclus                          ive=yes&gt;;Thu Jan  2 17:51:48 2025: Dispatched 1 Task(s) on Host(s) &lt;p1-r01-n1&gt;, Allocate                          d 1 Slot(s) on Host(s) &lt;p1-r01-n1&gt;, Effective RES_REQ                           &lt;select[((ngpus&gt;0)) &amp;&amp; (type == local)] order[r15s:p                          g] rusage[ngpus_physical=8.00] span[hosts=1] &gt;;Thu Jan  2 17:51:49 2025: Starting (Pid 3462241);Thu Jan  2 17:51:49 2025: Running with execution home &lt;/u/gsamu&gt;, Execution CWD                           &lt;/u/gsamu/.local/share/instructlab/checkpoints&gt;, Exe                          cution Pid &lt;3462241&gt;;Thu Jan  2 17:51:49 2025: External Message \"p1-r01-n1:gpus=0,1,2,3,4,5,6,7;EFFE                          CTIVE GPU REQ: num=8:mode=shared:mps=no:j_exclusive=y                          es:gvendor=nvidia;\" was posted from \"gsamu\" to messag                          e box 0;Thu Jan  2 17:57:56 2025: Done successfully. The CPU time used is 3024.0 second                          s;                          HOST: p1-r01-n1; CPU_TIME: 3024 seconds                                                        GPU ID: 0                                  Total Execution Time: 365 seconds                                  Energy Consumed: 98890 Joules                                  SM Utilization (%): Avg 20, Max 100, Min 0                                  Memory Utilization (%): Avg 9, Max 62, Min 0                                  Max GPU Memory Used: 53022294016 bytes                              GPU ID: 1                                  Total Execution Time: 365 seconds                                  Energy Consumed: 97697 Joules                                  SM Utilization (%): Avg 53, Max 100, Min 0                                  Memory Utilization (%): Avg 9, Max 58, Min 0                                  Max GPU Memory Used: 53087305728 bytes                              GPU ID: 2                                  Total Execution Time: 365 seconds                                  Energy Consumed: 94820 Joules                                  SM Utilization (%): Avg 53, Max 100, Min 0                                  Memory Utilization (%): Avg 9, Max 62, Min 0                                  Max GPU Memory Used: 53221523456 bytes                              GPU ID: 3                                  Total Execution Time: 365 seconds                                  Energy Consumed: 98014 Joules                                  SM Utilization (%): Avg 53, Max 100, Min 0                                  Memory Utilization (%): Avg 9, Max 59, Min 0                                  Max GPU Memory Used: 53041168384 bytes                              GPU ID: 4                                  Total Execution Time: 365 seconds                                  Energy Consumed: 99246 Joules                                  SM Utilization (%): Avg 53, Max 100, Min 0                                  Memory Utilization (%): Avg 9, Max 60, Min 0                                  Max GPU Memory Used: 53045362688 bytes                              GPU ID: 5                                  Total Execution Time: 365 seconds                                  Energy Consumed: 94952 Joules                                  SM Utilization (%): Avg 53, Max 100, Min 0                                  Memory Utilization (%): Avg 9, Max 65, Min 0                                  Max GPU Memory Used: 53047459840 bytes                              GPU ID: 6                                  Total Execution Time: 365 seconds                                  Energy Consumed: 98227 Joules                                  SM Utilization (%): Avg 53, Max 100, Min 0                                  Memory Utilization (%): Avg 9, Max 63, Min 0                                  Max GPU Memory Used: 53127151616 bytes                              GPU ID: 7                                  Total Execution Time: 365 seconds                                  Energy Consumed: 94582 Joules                                  SM Utilization (%): Avg 52, Max 100, Min 0                                  Memory Utilization (%): Avg 9, Max 65, Min 0                                  Max GPU Memory Used: 53481570304 bytesGPU Energy Consumed: 776428.000000 JoulesThu Jan  2 17:57:56 2025: Post job process done successfully;GPU_ALLOCATION: HOST             TASK GPU_ID  GI_PLACEMENT/SIZE    CI_PLACEMENT/SIZE    MODEL        MTOTAL  FACTOR MRSV    SOCKET NVLINK/XGMI                       p1-r01-n1        0    0       -                    -                    NVIDIAH10080 80G     9.0    0G      0      -                                                 0    1       -                    -                    NVIDIAH10080 80G     9.0    0G      0      -                                                 0    2       -                    -                    NVIDIAH10080 80G     9.0    0G      0      -                                                 0    3       -                    -                    NVIDIAH10080 80G     9.0    0G      0      -                                                 0    4       -                    -                    NVIDIAH10080 80G     9.0    0G      1      -                                                 0    5       -                    -                    NVIDIAH10080 80G     9.0    0G      1      -                                                 0    6       -                    -                    NVIDIAH10080 80G     9.0    0G      1      -                                                 0    7       -                    -                    NVIDIAH10080 80G     9.0    0G      1      -                               MEMORY USAGE:MAX MEM: 104 Gbytes;  AVG MEM: 16 Gbytes; MEM Efficiency: 0.00%CPU USAGE:CPU PEAK: 17.86 ;  CPU PEAK DURATION: 49 second(s)CPU AVERAGE EFFICIENCY: 856.60% ;  CPU PEAK EFFICIENCY: 1785.71%Summary of time in seconds spent in various states by  Thu Jan  2 17:57:56 2025  PEND     PSUSP    RUN      USUSP    SSUSP    UNKWN    TOTAL  0        0        368      0        0        0        368         Finally, with the model successfully trained, let’s chat with the new modelto check the result. Here’s we’ll pose it Swiftie specific questions. Note thatthe output from the training is written to ~/.local/share/instructlab/checkpoints/hf_format. We’ll take the model from the latest checkpoint directory that wascreated. Here again, we launch the model chat job via LSF as an interactivebatch job (i.e. bsub -Is).$ grep hf_format 1135.outModel saved in /u/gsamu/.local/share/instructlab/checkpoints/hf_format/samples_886Model saved in /u/gsamu/.local/share/instructlab/checkpoints/hf_format/samples_1776Model saved in /u/gsamu/.local/share/instructlab/checkpoints/hf_format/samples_2658Model saved in /u/gsamu/.local/share/instructlab/checkpoints/hf_format/samples_3546Model saved in /u/gsamu/.local/share/instructlab/checkpoints/hf_format/samples_4435$ bsub -Is -R \"span[hosts=1]\" -gpu \"num=8:j_exclusive=yes\" ilab model chat --model /u/gsamu/.local/share/instructlab/checkpoints/hf_format/samples_4435Job &lt;1146&gt; is submitted to default queue &lt;interactive&gt;.&lt;&lt;Waiting for dispatch ...&gt;&gt;&lt;&lt;Starting on p1-r01-n2&gt;&gt;INFO 2025-01-02 15:06:07,600 instructlab.model.backends.vllm:105: Trying to connect to model server at http://127.0.0.1:8000/v1INFO 2025-01-02 15:06:08,876 instructlab.model.backends.vllm:308: vLLM starting up on pid 3744375 at http://127.0.0.1:41531/v1INFO 2025-01-02 15:06:08,876 instructlab.model.backends.vllm:114: Starting a temporary vLLM server at http://127.0.0.1:41531/v1INFO 2025-01-02 15:06:08,876 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 1/120INFO 2025-01-02 15:06:12,244 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 2/120INFO 2025-01-02 15:06:15,614 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 3/120INFO 2025-01-02 15:06:18,801 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 4/120INFO 2025-01-02 15:06:21,952 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 5/120INFO 2025-01-02 15:06:25,391 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 6/120INFO 2025-01-02 15:06:28,638 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 7/120INFO 2025-01-02 15:06:32,103 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 8/120INFO 2025-01-02 15:06:35,296 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 9/120INFO 2025-01-02 15:06:38,616 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 10/120INFO 2025-01-02 15:06:42,015 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 11/120INFO 2025-01-02 15:06:45,435 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 12/120INFO 2025-01-02 15:06:48,679 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 13/120INFO 2025-01-02 15:06:52,025 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 14/120INFO 2025-01-02 15:06:55,317 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 15/120INFO 2025-01-02 15:06:58,604 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 16/120INFO 2025-01-02 15:07:01,927 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 17/120INFO 2025-01-02 15:07:05,287 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 18/120INFO 2025-01-02 15:07:08,763 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 19/120INFO 2025-01-02 15:07:12,131 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 20/120INFO 2025-01-02 15:07:15,476 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 21/120INFO 2025-01-02 15:07:18,881 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 22/120INFO 2025-01-02 15:07:22,203 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 23/120INFO 2025-01-02 15:07:25,599 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 24/120INFO 2025-01-02 15:07:28,991 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 25/120INFO 2025-01-02 15:07:32,234 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 26/120INFO 2025-01-02 15:07:35,714 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 27/120INFO 2025-01-02 15:07:38,974 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 28/120INFO 2025-01-02 15:07:42,265 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 29/120INFO 2025-01-02 15:07:45,582 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 30/120INFO 2025-01-02 15:07:45,586 instructlab.model.backends.vllm:136: vLLM engine successfully started at http://127.0.0.1:41531/v1╭────────────────────────────────────────────────────────────── system ──────────────────────────────────────────────────────────────╮│ Welcome to InstructLab Chat w/ SAMPLES_4435 (type /h for help)                                                                     │╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯&gt;&gt;&gt; Tell me everything you know about Swifties.                                                                           [S][default]╭─────────────────────────────────────────────────────────── samples_4435 ───────────────────────────────────────────────────────────╮│ Swifties are the fandom of the American singer-songwriter Taylor Swift.                                                            ││ Regarded by journalists as one of the largest, most devoted, and influential fan bases, Swifties are known for their high levels   ││ of participation, creativity, community, fanaticism, and cultural impact on the music industry and popular culture. They are a     ││ subject of widespread coverage in the mainstream media.                                                                            ││                                                                                                                                    ││ Critics have opined that Swift has redefined artist-fan relationships by establishing an intimate connection with Swifties. She    ││ has frequently engaged with, helped, credited, and prioritized her fans, who have offered unprecedented support and interest in    ││ her works irrespective of her wavering reception in the media. They continued to support Swift through her genre transitions,      ││ unexpected artistic pivots, and her highly publicized controversies such as the 2019 masters dispute, while instigating the        ││ political scrutiny of Ticketmaster that led to implementation of various laws and stimulated economic growth with the Eras Tour.   ││ Swift's releases, promotional efforts, and fashion have garnered attention for incorporating Easter eggs and clues that are        │......ConclusionsWe’ve demonstrated a simple InstructLab workflow that is scheduled by IBM LSFin a compute cluster equipped with GPUs.  As part of this example, LSF GPUscheduling and accounting for GPU workloads was highlighted. For organizationslooking to productionize InstructLab and where there is a pool of GPU equippedcompute resources, LSF provides an ideal way to manage demand from a usercommunity looking to run these intensive workloads.At the recent SC24 event, the demonstration went beyond what is shown in thisblog. It incorporated single click job submission via LSF Application Centerusing a custom template that was created for InstructLab to submits both thesynthetic data generation job, as well the training job with a single click.The demo environment was on IBM Cloud using instances equipped with Nvidia GPUs.The compute instances were automatically scaled up and down by the LSF resourceconnector. This will be the topic for a future blog.",
            "content_html": "<p><strong>Overview</strong></p><p>All the best for 2025! This blog looks back on a demo which I created for <a href=\"https://sc24.supercomputing.org\">SC24</a>last November to demonstrate InstructLab workflows running on an <a href=\"https://www.ibm.com/products/hpc-workload-management\">IBM LSF</a>cluster. Let’s begin with a bit of background. I’d like to thank MichaelSpriggs, STSM, IBM LSF for his contributions to this blog.</p><p>When I think of tuning, what immediately comes to my mind are visions of anexpert mechanic trying to extract the most from an engine. This blog isfocused on an entirely different type of tuning, AI model tuning. Like tuningan engine, AI model tuning can be used to ensure a better fit for a given AImodel for your business.</p><p>Released by IBM and Red Hat in May 2024, <a href=\"https://research.ibm.com/blog/instruct-lab\">InstructLab</a> is an open-source projectwhich provides the ability to fine-tune LLMs by adding skills and knowledge,without having to retrain the model from scratch. InstructLab can run onresource-constrained systems such as laptops, but also supports GPUs. Much hasbeen written about InstructLab and this blog is not intended to provide anin-depth look at InstructLab. Rather, the objective here is to demonstrate howInstructLab workloads can be distributed and managed in a high-performancecomputing cluster with GPUs using the IBM LSF workload scheduler. Recently, IBMpublished a paper describing the infrastructure used to train the Granite familyof AI foundation models. The paper describes the Vela and Blue Vela environmentsin detail. In particular, the Blue Vela environment is built on a software stackusing Red Hat Enterprise Linux, IBM LSF and Storage Scale. Learn more in thedetailed paper <a href=\"https://arxiv.org/abs/2407.05467\">here</a>.</p><p>The demo workflow consists of two LSF jobs. The first job generates syntheticdata, which is used to teach the LLM new skills or knowledge. The second job,which depends upon the successful completion of the first, is the training job,where the new skills or knowledge are incorporated into an existing base model.A simple LSF job dependency is used to ensure the training job only runs afterthe successful completion of the synthetic data generation step.</p><p>The environment used is equipped with Nvidia GPUs.  InstructLab jobs will berun with the options for GPU support, and the jobs will be submitted to LSFwith the appropriate GPU scheduling directives. Furthermore, it is assumed thatthe users' $HOME directory is available on all hosts in the cluster. Note that Irequire neither root access, nor a user account that is an LSF administrator, toinstall and use InstructLab on the LSF cluster.</p><p><strong>Configuration</strong></p><p>The HPC cluster is configured as follows:</p><ul><li>Red Hat Enterprise Linux v8.8</li><li>IBM LSF v10.0.1.15</li><li>InstructLab v0.19.4</li><li>Miniforge v3 (24.9.0-0)</li><li>NVIDIA CUDA v12.6</li><li>Compute nodes are equipped with 8 x Nvidia H100 GPUs</li></ul><p><strong>Install InstructLab</strong></p><ol><li>Log in to a compute node in the LSF cluster equipped with GPUs. If ssh accessis disabled to compute nodes, then submit an interactive LSF batch job. This jobrequests 8 GPUs on a single system and will set them to exclusive executionmode.</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ bsub -Is -R \"span[hosts=1]\" -gpu \"num=8:j_exclusive=yes\" bash</code></pre></div><ol start=\"2\"><li>Install and set up a Conda environment. This will enable you to install aself-contained Conda environment for your user account with the necessaryPython version needed for InstructLab. Miniforge is installed in the defaultlocation and the option to update the users shell profile to start the Condaenvironment are selected. We assume here a shared $HOME directory.</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ cd $HOME$ curl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"$ bash Miniforge3-$(uname)-$(uname -m).sh</code></pre></div><ol start=\"3\"><li>Before proceeding, you must logout and log back in to activate theenvironment. Next, a Conda environment is created with name <em>my_env</em>. Here we’llspecify Python v3.11, which is a requirement for InstructLab.</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\">conda create --name my_env -c anaconda python=3.11conda activate my_env</code></pre></div><ol start=\"4\"><li>Next, install InstructLab. Here, version 0.19.4 of InstructLab is specified.This was the version of InstructLab available in the timeframe preceding theSC24 event. Follow the installation steps in the official InstructLabdocumentation <a href=\"https://github.com/instructlab/instructlab?tab=readme-ov-file#-installing-ilab\">here</a>.</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ pip install instructlab==0.19.4</code></pre></div><ol start=\"5\"><li>Next, perform the installation of InstructLab with Nvidia CUDA support. Thisis required for InstructLab to utilize the GPUs. Without this step, InstructLabwill run on the CPUs. Note that CUDA v12.6 is installed on the system and thevariables set below reflect this.</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ export CMAKE_ARGS=\"-DLLAMA_CUBLAS=on -DCUDA_PATH=/usr/local/cuda-12.6 -DCUDAToolkit_ROOT=/usr/local/cuda-12.6 -DCUDAToolkit_INCLUDE_DIR=/usr/local/cuda-12/include -DCUDAToolkit_LIBRARY_DIR=/usr/local/cuda-12.6/lib64\"$ export PATH=/usr/local/cuda-12.6/bin:$PATH$ pip cache remove llama_cpp_python$ CMAKE_ARGS=\"-DLLAMA_CUDA=on -DLLAMA_NATIVE=off\" pip install 'instructlab[cuda]'$ pip install vllm@git+https://github.com/opendatahub-io/vllm@v0.6.2</code></pre></div><p><strong>Configure InstructLab</strong></p><ol><li>With the installation of InstructLab complete, the next step is to run theinitialization. This will setup paths to models, taxonomy repo as well as theGPU configuration.</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ ilab config init</code></pre></div><ol start=\"2\"><li>By default InstructLab stores models, training checkpoints and other fileswithin <em>~/.cache</em> and <em>~/.local/share/instructlab</em>. If you have limited storagecapacity available in $HOME, then you may opt to disable training checkpointfiles. This can be done by setting the following option in <em>~/.config/instructlab/config.yaml</em> as follows.</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\">train:  checkpoint_at_epoch: false</code></pre></div><ol start=\"3\"><li>Next, we download the required models. The ilab model list command can beused to list the models which are available. Note that a <a href=\"https://huggingface.co\">HuggingFace</a> token isrequired to download certain models. Please set HF_TOKEN in the environmentwith the appropriate token.</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ export HF_TOKEN=&lt;HuggingFace token&gt;$ ilab model download$ ilab model download --repository=instructlab/granite-7b-lab$ ilab model list+--------------------------------------+---------------------+---------+| Model Name                           | Last Modified       | Size    |+--------------------------------------+---------------------+---------+| instructlab/granite-7b-lab           | 2024-12-27 20:37:29 | 12.6 GB || mistral-7b-instruct-v0.2.Q4_K_M.gguf | 2024-12-27 16:55:46 | 4.1 GB  || merlinite-7b-lab-Q4_K_M.gguf         | 2024-12-27 16:48:39 | 4.1 GB  |+--------------------------------------+---------------------+---------+</code></pre></div><p><strong>Generate synthetic data &amp; AI model training</strong></p><p>Next, is the synthetic data generation step, which will be executed on GPUs.This step is a prerequisite to teaching the LLM new skills/knowledge viatraining.</p><ol><li><p>Here we use example knowledge from the InstructLab github about Taylor Swiftfans, who are known as “Swifties”. This is timely because Taylor Swift recentlywrapped up 6 concerts in Toronto, Canada, where I happen to be based. Copyattribution.txt and qna.yaml from the following <a href=\"https://github.com/mairin/taxonomy/tree/swifties/knowledge/arts/music/fandom/swifties\">location</a>.</p></li><li><p>By default, the InstructLab taxonomy is found in <em>~/.local/share/instructlab/taxonomy</em>. Here we create the directories fandom/swifties under <em>~/.local/share/instructlab/taxonomy/knowledge/arts/fandom</em> and copy the files from step 1 intothis location.</p></li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ mkdir -p ~/.local/share/instructlab/taxonomy/knowledge/arts/fandom/swifties$ cp &lt;path_to&gt;/attribution.txt ~/.local/share/instructlab/taxonomy/knowledge/arts/fandom/swifties$ cp &lt;path_to&gt;/qna.yaml ~/.local/share/instructlab/taxonomy/knowledge/arts/fandom/swifties</code></pre></div><ol start=\"3\"><li>With the Swifties taxonomy in place, check for any syntax errors with thecommand <em>ilab taxonomy diff</em>. It should report that the taxonomy is valid ifthere are no syntax errors.</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ ilab taxonomy diffknowledge/arts/fandom/swifties/qna.yamlTaxonomy in /u/gsamu/.local/share/instructlab/taxonomy is valid :)</code></pre></div><ol start=\"4\"><li>With the taxonomy in place and having confirmed that the syntax is valid,it’s now time to run the synthetic data generation job through LSF. Here we willrequest 8 GPUs on a single server in exclusive execution mode. For theInstructLab ilab command, specify the <em>&ndash;gpus 8 and &ndash;pipeline full</em> options.Standard output is written to the $HOME/job-output with filename specification&lt;LSF_JOBID&gt;.out. The $HOME/job-output directory must already exist.</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ mkdir -p $HOME/job-output$ bsub -o $HOME/job-output/%J.out -R \"span[hosts=1]\" -gpu \"num=8:j_exclusive=yes\" ilab data generate --pipeline full --gpus 8Job &lt;1131&gt; is submitted to default queue &lt;normal&gt;.</code></pre></div><ol start=\"5\"><li>During job execution, the LSF <em>bpeek</em> command can be used to monitor the jobstandard output.</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ bpeek -f 1131 &lt;&lt; output from stdout &gt;&gt;INFO 2025-01-02 09:51:29,503 numexpr.utils:146: Note: detected 96 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.INFO 2025-01-02 09:51:29,504 numexpr.utils:149: Note: NumExpr detected 96 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.INFO 2025-01-02 09:51:29,504 numexpr.utils:162: NumExpr defaulting to 16 threads.INFO 2025-01-02 09:51:30,038 datasets:59: PyTorch version 2.3.1 available.INFO 2025-01-02 09:51:31,226 instructlab.model.backends.llama_cpp💯 Trying to connect to model server at http://127.0.0.1:8000/v1WARNING 2025-01-02 09:51:56,356 instructlab.data.generate:270: Disabling SDG batching - unsupported with llama.cpp servingGenerating synthetic data using 'full' pipeline, '/u/gsamu/.cache/instructlab/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf' model, '/u/gsamu/.local/share/instructlab/taxonomy' taxonomy, against http://127.0.0.1:55779/v1 serverINFO 2025-01-02 09:51:56,861 instructlab.sdg.generate_data:356: Synthesizing new instructions. If you aren't satisfied with the generated instructions, interrupt training (Ctrl-C) and try adjusting your YAML files. Adding more examples may help.INFO 2025-01-02 09:51:56,872 instructlab.sdg.pipeline:153: Running pipeline single-threadedINFO 2025-01-02 09:51:56,872 instructlab.sdg.pipeline:197: Running block: duplicate_document_colINFO 2025-01-02 09:51:56,872 instructlab.sdg.pipeline:198: Dataset({    features: ['icl_document', 'document', 'document_outline', 'domain', 'icl_query_1', 'icl_query_2', 'icl_query_3', 'icl_response_1', 'icl_response_2', 'icl_response_3'],    num_rows: 35})INFO 2025-01-02 09:51:58,286 instructlab.sdg.llmblock:51: LLM server supports batched inputs: FalseINFO 2025-01-02 09:51:58,286 instructlab.sdg.pipeline:197: Running block: gen_spellcheckINFO 2025-01-02 09:51:58,286 instructlab.sdg.pipeline:198: Dataset({    features: ['icl_document', 'document', 'document_outline', 'domain', 'icl_query_1', 'icl_query_2', 'icl_query_3', 'icl_response_1', 'icl_response_2', 'icl_response_3', 'base_document'],    num_rows: 35})/u/gsamu/miniforge3/envs/my_env/lib/python3.11/site-packages/llama_cpp/llama.py:1054: RuntimeWarning: Detected duplicate leading \"&lt;s&gt;\" in prompt, this will likely reduce response quality, consider removing it...  warnings.warn(INFO 2025-01-02 09:57:42,264 instructlab.sdg.pipeline:197: Running block: flatten_auxiliary_columnsINFO 2025-01-02 09:57:42,264 instructlab.sdg.pipeline:198: Dataset({    features: ['icl_document', 'document', 'document_outline', 'domain', 'icl_query_1', 'icl_query_2', 'icl_query_3', 'icl_response_1', 'icl_response_2', 'icl_response_3', 'base_document', 'spellcheck'],    num_rows: 35})INFO 2025-01-02 09:57:42,279 instructlab.sdg.pipeline:197: Running block: rename_to_document_columnINFO 2025-01-02 09:57:42,279 instructlab.sdg.pipeline:198: Dataset({    features: ['icl_document', 'document', 'document_outline', 'domain', 'icl_query_1', 'icl_query_2', 'icl_query_3', 'icl_response_1', 'icl_response_2', 'icl_response_3', 'dataset_type', 'corrected_document'],    num_rows: 70})INFO 2025-01-02 09:57:42,282 instructlab.sdg.pipeline:197: Running block: gen_knowledgeINFO 2025-01-02 09:57:42,282 instructlab.sdg.pipeline:198: Dataset({    features: ['icl_document', 'raw_document', 'document_outline', 'domain', 'icl_query_1', 'icl_query_2', 'icl_query_3', 'icl_response_1', 'icl_response_2', 'icl_response_3', 'dataset_type', 'document'],    num_rows: 70})……</code></pre></div><ol start=\"6\"><li>During the runtime of the job, it’s possible to view GPU related metricsusing the LSF <em>lsload</em> and <em>bhosts</em> commands. First, we need to identify the hostwhere the job has been dispatched to using the LSF bjobs command. In this casethe job was dispatched to host <em>p1-r01-n4</em>. Note that details GPU accountingmetrics are available once the job runs to completion.</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ bjobs -wJOBID   USER    STAT  QUEUE      FROM_HOST   EXEC_HOST   JOB_NAME   SUBMIT_TIME1131    gsamu   RUN   normal     rmf-login-1 p1-r01-n4   ilab data generate --pipeline full --gpus 8 Jan  2 14:51$ lsload -w -gpu p1-r01-n4HOST_NAME                 status ngpus gpu_shared_avg_mut gpu_shared_avg_ut ngpus_physicalp1-r01-n4                     ok     8                 2%                7%              8$ bhosts -w -gpu p1-r01-n4HOST_NAME            GPU_ID                MODEL     MUSED      MRSV  NJOBS    RUN   SUSP    RSV p1-r01-n4                 0   NVIDIAH10080GBHBM3        2G        0G      1      1      0      0                          1   NVIDIAH10080GBHBM3        2G        0G      1      1      0      0                          2   NVIDIAH10080GBHBM3        2G        0G      1      1      0      0                          3   NVIDIAH10080GBHBM3        2G        0G      1      1      0      0                          4   NVIDIAH10080GBHBM3        2G        0G      1      1      0      0                          5   NVIDIAH10080GBHBM3        2G        0G      1      1      0      0                          6   NVIDIAH10080GBHBM3        2G        0G      1      1      0      0                          7   NVIDIAH10080GBHBM3        2G        0G      1      1      0      0</code></pre></div><ol start=\"7\"><li>After job completion, it’s possible to view details about the job includingGPU utilization which LSF collects by leveraging NVIDIA DCGM. These metrics areavailable upon job completion using both the LSF <em>bhist</em> and <em>bjobs</em> commands.</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ bhist -l -gpu 1131Job &lt;1131&gt;, User &lt;gsamu&gt;, Project &lt;default&gt;, Command &lt;ilab data generate --pipe                          line full --gpus 8&gt;Thu Jan  2 14:51:23 2025: Submitted from host &lt;rmf-login-1&gt;, to Queue &lt;normal&gt;,                           CWD &lt;$HOME&gt;, Output File &lt;/u/gsamu/job-output/%J.out                          &gt;, Requested Resources &lt;span[hosts=1]&gt;, Requested GPU                           &lt;num=8:j_exclusive=yes&gt;;Thu Jan  2 14:51:24 2025: Dispatched 1 Task(s) on Host(s) &lt;p1-r01-n4&gt;, Allocate                          d 1 Slot(s) on Host(s) &lt;p1-r01-n4&gt;, Effective RES_REQ                           &lt;select[((ngpus&gt;0)) &amp;&amp; (type == local)] order[r15s:p                          g] rusage[ngpus_physical=8.00] span[hosts=1] &gt;;Thu Jan  2 14:51:25 2025: Starting (Pid 3095851);Thu Jan  2 14:51:25 2025: External Message \"p1-r01-n4:gpus=0,1,2,3,4,5,6,7;EFFE                          CTIVE GPU REQ: num=8:mode=shared:mps=no:j_exclusive=y                          es:gvendor=nvidia;\" was posted from \"gsamu\" to messag                          e box 0;Thu Jan  2 14:51:26 2025: Running with execution home &lt;/u/gsamu&gt;, Execution CWD                           &lt;/u/gsamu&gt;, Execution Pid &lt;3095851&gt;;Thu Jan  2 16:08:05 2025: Done successfully. The CPU time used is 4624.0 second                          s;                          HOST: p1-r01-n4; CPU_TIME: 4624 seconds                                                        GPU ID: 0                                  Total Execution Time: 4597 seconds                                  Energy Consumed: 579704 Joules                                  SM Utilization (%): Avg 9, Max 15, Min 0                                  Memory Utilization (%): Avg 2, Max 100, Min 0                                  Max GPU Memory Used: 1956642816 bytes                              GPU ID: 1                                  Total Execution Time: 4597 seconds                                  Energy Consumed: 503956 Joules                                  SM Utilization (%): Avg 7, Max 11, Min 0                                  Memory Utilization (%): Avg 2, Max 5, Min 0                                  Max GPU Memory Used: 1767899136 bytes                              GPU ID: 2                                  Total Execution Time: 4597 seconds                                  Energy Consumed: 501754 Joules                                  SM Utilization (%): Avg 7, Max 11, Min 0                                  Memory Utilization (%): Avg 2, Max 5, Min 0                                  Max GPU Memory Used: 1784676352 bytes                              GPU ID: 3                                  Total Execution Time: 4597 seconds                                  Energy Consumed: 525195 Joules                                  SM Utilization (%): Avg 7, Max 11, Min 0                                  Memory Utilization (%): Avg 2, Max 54, Min 0                                  Max GPU Memory Used: 1767899136 bytes                              GPU ID: 4                                  Total Execution Time: 4597 seconds                                  Energy Consumed: 525331 Joules                                  SM Utilization (%): Avg 7, Max 12, Min 0                                  Memory Utilization (%): Avg 2, Max 5, Min 0                                  Max GPU Memory Used: 1767899136 bytes                              GPU ID: 5                                  Total Execution Time: 4597 seconds                                  Energy Consumed: 502416 Joules                                  SM Utilization (%): Avg 7, Max 11, Min 0                                  Memory Utilization (%): Avg 2, Max 5, Min 0                                  Max GPU Memory Used: 1784676352 bytes                              GPU ID: 6                                  Total Execution Time: 4597 seconds                                  Energy Consumed: 508720 Joules                                  SM Utilization (%): Avg 7, Max 12, Min 0                                  Memory Utilization (%): Avg 2, Max 5, Min 0                                  Max GPU Memory Used: 1784676352 bytes                              GPU ID: 7                                  Total Execution Time: 4597 seconds                                  Energy Consumed: 491041 Joules                                  SM Utilization (%): Avg 6, Max 12, Min 0                                  Memory Utilization (%): Avg 2, Max 4, Min 0                                  Max GPU Memory Used: 1933574144 bytesGPU Energy Consumed: 4138117.000000 JoulesThu Jan  2 16:08:05 2025: Post job process done successfully;GPU_ALLOCATION: HOST             TASK GPU_ID  GI_PLACEMENT/SIZE    CI_PLACEMENT/SIZE    MODEL        MTOTAL  FACTOR MRSV    SOCKET NVLINK/XGMI                       p1-r01-n4        0    0       -                    -                    NVIDIAH10080 80G     9.0    0G      0      -                                                 0    1       -                    -                    NVIDIAH10080 80G     9.0    0G      0      -                                                 0    2       -                    -                    NVIDIAH10080 80G     9.0    0G      0      -                                                 0    3       -                    -                    NVIDIAH10080 80G     9.0    0G      0      -                                                 0    4       -                    -                    NVIDIAH10080 80G     9.0    0G      1      -                                                 0    5       -                    -                    NVIDIAH10080 80G     9.0    0G      1      -                                                 0    6       -                    -                    NVIDIAH10080 80G     9.0    0G      1      -                                                 0    7       -                    -                    NVIDIAH10080 80G     9.0    0G      1      -                               MEMORY USAGE:MAX MEM: 2 Gbytes;  AVG MEM: 1 Gbytes; MEM Efficiency: 0.00%CPU USAGE:CPU PEAK: 1.69 ;  CPU PEAK DURATION: 52 second(s)CPU AVERAGE EFFICIENCY: 100.69% ;  CPU PEAK EFFICIENCY: 169.23%Summary of time in seconds spent in various states by  Thu Jan  2 16:08:05 2025  PEND     PSUSP    RUN      USUSP    SSUSP    UNKWN    TOTAL  1        0        4601     0        0        0        4602 </code></pre></div><ol start=\"8\"><li><p>When the synthetic data generation job completes, it’s output can be viewedat <em>~/job-output/<!-- raw HTML omitted -->.out</em>. The synthetic data sets will comprise files inthe directory <em>~/.local/share/instructlab/datasets</em>. These files will be named*skills_train_msgs_*.jsonl* and *knowledge_train_msgs_*.jsonl*.</p></li><li><p>With the synthetic data generation step complete, it’s now time to run thetraining. We first set 2 environment variables to point to the followingfiles:  <em>~/.local/share/instructlab/datasets/knowledge_train_msgs_2025-01-02T09_51_56.jsonl</em>  and <em>~./.local/share/instructlab/datasets/skills_train_msgs_2025-01-02T09_51_56.jsonl</em>.</p></li></ol><p>Afterward, we submit the training job to LSF requesting 8 GPUs and with ilaboptions <em>&ndash;pipeline accelerated</em>, <em>&ndash;gpus 8</em>, <em>&ndash;device cuda</em> and<em>&ndash;data-path</em> pointing to the two above data files that were produced in thesynthetic data generation step.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ export SKILLS_PATH=/u/gsamu/.local/share/instructlab/datasets/skills_train_msgs_2025-01-02T09_51_56.jsonl$ export KNOWLEDGE_PATH=/u/gsamu/.local/share/instructlab/datasets/knowledge_train_msgs_2025-01-02T09_51_56.jsonl$ bsub -o $HOME/job-output/%J.out -R \"span[hosts=1]\" -gpu \"num=8:j_exclusive=yes\" ilab model train --pipeline accelerated --data-path $SKILLS_PATH --data-path $KNOWLEDGE_PATH --device cuda --gpus 8Job &lt;1135&gt; is submitted to default queue &lt;normal&gt;.</code></pre></div><ol start=\"10\"><li>During job execution, the LSF <em>bpeek</em> command can be used to monitor thejob standard output.</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ bpeek -f 1135&lt;&lt; output from stdout &gt;&gt;LoRA is disabled (rank=0), ignoring all additional LoRA args[2025-01-02 12:52:04,359] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)INFO 2025-01-02 12:52:09,061 numexpr.utils:146: Note: detected 96 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.INFO 2025-01-02 12:52:09,061 numexpr.utils:149: Note: NumExpr detected 96 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.INFO 2025-01-02 12:52:09,061 numexpr.utils:162: NumExpr defaulting to 16 threads.INFO 2025-01-02 12:52:09,304 datasets:59: PyTorch version 2.3.1 available.You are using the default legacy behaviour of the &lt;class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'&gt;. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.INFO 2025-01-02 12:52:09,653 root:617: Special tokens: eos: [32000], pad: [32001], bos: [32005], system: [32004], user: [32002], assistant: [32003]INFO 2025-01-02 12:52:09,923 root:617: number of dropped samples: 0 -- out of 641 data arguments are:{\"data_path\":\"/u/gsamu/.local/share/instructlab/datasets/knowledge_train_msgs_2025-01-02T09_51_56.jsonl\",\"data_output_path\":\"/u/gsamu/.local/share/instructlab/internal\",\"max_seq_len\":4096,\"model_path\":\"/u/gsamu/.cache/instructlab/models/instructlab/granite-7b-lab\",\"chat_tmpl_path\":\"/u/gsamu/miniforge3/envs/my_env/lib/python3.11/site-packages/instructlab/training/chat_templates/ibm_generic_tmpl.py\",\"num_cpu_procs\":16}tokenizing the dataset with /u/gsamu/.cache/instructlab/models/instructlab/granite-7b-lab tokenizer...ten largest length percentiles:quantile 90th: 1459.0quantile 91th: 1466.0quantile 92th: 1469.6000000000001quantile 93th: 1478.2quantile 94th: 1483.0quantile 95th: 1488.0quantile 96th: 1497.1999999999998quantile 97th: 1516.5999999999997quantile 98th: 1540.6000000000001quantile 99th: 1656.0000000000016quantile 100th: 2578.0at 4096 max sequence length, the number of samples to be dropped is 0(0.00% of total)quantile 0th: 368.0quantile 1th: 393.0quantile 2th: 411.2quantile 3th: 421.2quantile 4th: 427.2quantile 5th: 442.0quantile 6th: 604.4quantile 7th: 631.8quantile 8th: 653.8000000000001quantile 9th: 679.8quantile 10th: 742.0at 20 min sequence length, the number of samples to be dropped is 0checking the validity of the samples...Categorizing training data type...unmasking the appropriate message content... Samples Previews...……</code></pre></div><ol start=\"11\"><li>During the runtime of the training job, we can observe some GPU utilizationinformation using the LSF lsload and bhosts commands.  First we need to identifythe server on which the training job is running. This is done using the bjobscommand and checking for the execution host of the job.</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ bjobs -wJOBID   USER    STAT  QUEUE      FROM_HOST   EXEC_HOST   JOB_NAME   SUBMIT_TIME1135    gsamu   RUN   normal     rmf-login-1 p1-r01-n1   ilab model train --pipeline accelerated --data-path /u/gsamu/.local/share/instructlab/datasets/skills_train_msgs_2025-01-02T09_51_56.jsonl --data-path /u/gsamu/.local/share/instructlab/datasets/knowledge_train_msgs_2025-01-02T09_51_56.jsonl --device cuda --gpus 8 Jan  2 17:51$ lsload -w -gpu p1-r01-n1HOST_NAME                 status ngpus gpu_shared_avg_mut gpu_shared_avg_ut ngpus_physicalp1-r01-n1                     ok     8                 0%               22%              8$ bhosts -w -gpu p1-r01-n1HOST_NAME            GPU_ID                MODEL     MUSED      MRSV  NJOBS    RUN   SUSP    RSV p1-r01-n1                 0   NVIDIAH10080GBHBM3       10G        0G      1      1      0      0                          1   NVIDIAH10080GBHBM3       10G        0G      1      1      0      0                          2   NVIDIAH10080GBHBM3       10G        0G      1      1      0      0                          3   NVIDIAH10080GBHBM3       10G        0G      1      1      0      0                          4   NVIDIAH10080GBHBM3       10G        0G      1      1      0      0                          5   NVIDIAH10080GBHBM3       10G        0G      1      1      0      0                          6   NVIDIAH10080GBHBM3       10G        0G      1      1      0      0                          7   NVIDIAH10080GBHBM3       10G        0G      1      1      0      0</code></pre></div><ol start=\"12\"><li>Once the job is complete, detailed GPU accounting can again be viewed usingthe LSF <em>bhist</em> command as follows below.</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ bhist -l -gpu 1135Job &lt;1135&gt;, User &lt;gsamu&gt;, Project &lt;default&gt;, Command &lt;ilab model train --pipeli                          ne accelerated --data-path /u/gsamu/.local/share/inst                          ructlab/datasets/skills_train_msgs_2025-01-02T09_51_5                          6.jsonl --data-path /u/gsamu/.local/share/instructlab                          /datasets/knowledge_train_msgs_2025-01-02T09_51_56.js                          onl --device cuda --gpus 8&gt;Thu Jan  2 17:51:48 2025: Submitted from host &lt;rmf-login-1&gt;, to Queue &lt;normal&gt;,                           CWD &lt;$HOME/.local/share/instructlab/checkpoints&gt;, Ou                          tput File &lt;/u/gsamu/job-output/%J.out&gt;, Requested Res                          ources &lt;span[hosts=1]&gt;, Requested GPU &lt;num=8:j_exclus                          ive=yes&gt;;Thu Jan  2 17:51:48 2025: Dispatched 1 Task(s) on Host(s) &lt;p1-r01-n1&gt;, Allocate                          d 1 Slot(s) on Host(s) &lt;p1-r01-n1&gt;, Effective RES_REQ                           &lt;select[((ngpus&gt;0)) &amp;&amp; (type == local)] order[r15s:p                          g] rusage[ngpus_physical=8.00] span[hosts=1] &gt;;Thu Jan  2 17:51:49 2025: Starting (Pid 3462241);Thu Jan  2 17:51:49 2025: Running with execution home &lt;/u/gsamu&gt;, Execution CWD                           &lt;/u/gsamu/.local/share/instructlab/checkpoints&gt;, Exe                          cution Pid &lt;3462241&gt;;Thu Jan  2 17:51:49 2025: External Message \"p1-r01-n1:gpus=0,1,2,3,4,5,6,7;EFFE                          CTIVE GPU REQ: num=8:mode=shared:mps=no:j_exclusive=y                          es:gvendor=nvidia;\" was posted from \"gsamu\" to messag                          e box 0;Thu Jan  2 17:57:56 2025: Done successfully. The CPU time used is 3024.0 second                          s;                          HOST: p1-r01-n1; CPU_TIME: 3024 seconds                                                        GPU ID: 0                                  Total Execution Time: 365 seconds                                  Energy Consumed: 98890 Joules                                  SM Utilization (%): Avg 20, Max 100, Min 0                                  Memory Utilization (%): Avg 9, Max 62, Min 0                                  Max GPU Memory Used: 53022294016 bytes                              GPU ID: 1                                  Total Execution Time: 365 seconds                                  Energy Consumed: 97697 Joules                                  SM Utilization (%): Avg 53, Max 100, Min 0                                  Memory Utilization (%): Avg 9, Max 58, Min 0                                  Max GPU Memory Used: 53087305728 bytes                              GPU ID: 2                                  Total Execution Time: 365 seconds                                  Energy Consumed: 94820 Joules                                  SM Utilization (%): Avg 53, Max 100, Min 0                                  Memory Utilization (%): Avg 9, Max 62, Min 0                                  Max GPU Memory Used: 53221523456 bytes                              GPU ID: 3                                  Total Execution Time: 365 seconds                                  Energy Consumed: 98014 Joules                                  SM Utilization (%): Avg 53, Max 100, Min 0                                  Memory Utilization (%): Avg 9, Max 59, Min 0                                  Max GPU Memory Used: 53041168384 bytes                              GPU ID: 4                                  Total Execution Time: 365 seconds                                  Energy Consumed: 99246 Joules                                  SM Utilization (%): Avg 53, Max 100, Min 0                                  Memory Utilization (%): Avg 9, Max 60, Min 0                                  Max GPU Memory Used: 53045362688 bytes                              GPU ID: 5                                  Total Execution Time: 365 seconds                                  Energy Consumed: 94952 Joules                                  SM Utilization (%): Avg 53, Max 100, Min 0                                  Memory Utilization (%): Avg 9, Max 65, Min 0                                  Max GPU Memory Used: 53047459840 bytes                              GPU ID: 6                                  Total Execution Time: 365 seconds                                  Energy Consumed: 98227 Joules                                  SM Utilization (%): Avg 53, Max 100, Min 0                                  Memory Utilization (%): Avg 9, Max 63, Min 0                                  Max GPU Memory Used: 53127151616 bytes                              GPU ID: 7                                  Total Execution Time: 365 seconds                                  Energy Consumed: 94582 Joules                                  SM Utilization (%): Avg 52, Max 100, Min 0                                  Memory Utilization (%): Avg 9, Max 65, Min 0                                  Max GPU Memory Used: 53481570304 bytesGPU Energy Consumed: 776428.000000 JoulesThu Jan  2 17:57:56 2025: Post job process done successfully;GPU_ALLOCATION: HOST             TASK GPU_ID  GI_PLACEMENT/SIZE    CI_PLACEMENT/SIZE    MODEL        MTOTAL  FACTOR MRSV    SOCKET NVLINK/XGMI                       p1-r01-n1        0    0       -                    -                    NVIDIAH10080 80G     9.0    0G      0      -                                                 0    1       -                    -                    NVIDIAH10080 80G     9.0    0G      0      -                                                 0    2       -                    -                    NVIDIAH10080 80G     9.0    0G      0      -                                                 0    3       -                    -                    NVIDIAH10080 80G     9.0    0G      0      -                                                 0    4       -                    -                    NVIDIAH10080 80G     9.0    0G      1      -                                                 0    5       -                    -                    NVIDIAH10080 80G     9.0    0G      1      -                                                 0    6       -                    -                    NVIDIAH10080 80G     9.0    0G      1      -                                                 0    7       -                    -                    NVIDIAH10080 80G     9.0    0G      1      -                               MEMORY USAGE:MAX MEM: 104 Gbytes;  AVG MEM: 16 Gbytes; MEM Efficiency: 0.00%CPU USAGE:CPU PEAK: 17.86 ;  CPU PEAK DURATION: 49 second(s)CPU AVERAGE EFFICIENCY: 856.60% ;  CPU PEAK EFFICIENCY: 1785.71%Summary of time in seconds spent in various states by  Thu Jan  2 17:57:56 2025  PEND     PSUSP    RUN      USUSP    SSUSP    UNKWN    TOTAL  0        0        368      0        0        0        368         </code></pre></div><ol start=\"13\"><li>Finally, with the model successfully trained, let’s chat with the new modelto check the result. Here’s we’ll pose it Swiftie specific questions. Note thatthe output from the training is written to <em>~/.local/share/instructlab/checkpoints/hf_format</em>. We’ll take the model from the latest checkpoint directory that wascreated. Here again, we launch the model chat job via LSF as an interactivebatch job (i.e. <em>bsub -Is</em>).</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ grep hf_format 1135.outModel saved in /u/gsamu/.local/share/instructlab/checkpoints/hf_format/samples_886Model saved in /u/gsamu/.local/share/instructlab/checkpoints/hf_format/samples_1776Model saved in /u/gsamu/.local/share/instructlab/checkpoints/hf_format/samples_2658Model saved in /u/gsamu/.local/share/instructlab/checkpoints/hf_format/samples_3546Model saved in /u/gsamu/.local/share/instructlab/checkpoints/hf_format/samples_4435</code></pre></div><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ bsub -Is -R \"span[hosts=1]\" -gpu \"num=8:j_exclusive=yes\" ilab model chat --model /u/gsamu/.local/share/instructlab/checkpoints/hf_format/samples_4435Job &lt;1146&gt; is submitted to default queue &lt;interactive&gt;.&lt;&lt;Waiting for dispatch ...&gt;&gt;&lt;&lt;Starting on p1-r01-n2&gt;&gt;INFO 2025-01-02 15:06:07,600 instructlab.model.backends.vllm:105: Trying to connect to model server at http://127.0.0.1:8000/v1INFO 2025-01-02 15:06:08,876 instructlab.model.backends.vllm:308: vLLM starting up on pid 3744375 at http://127.0.0.1:41531/v1INFO 2025-01-02 15:06:08,876 instructlab.model.backends.vllm:114: Starting a temporary vLLM server at http://127.0.0.1:41531/v1INFO 2025-01-02 15:06:08,876 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 1/120INFO 2025-01-02 15:06:12,244 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 2/120INFO 2025-01-02 15:06:15,614 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 3/120INFO 2025-01-02 15:06:18,801 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 4/120INFO 2025-01-02 15:06:21,952 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 5/120INFO 2025-01-02 15:06:25,391 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 6/120INFO 2025-01-02 15:06:28,638 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 7/120INFO 2025-01-02 15:06:32,103 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 8/120INFO 2025-01-02 15:06:35,296 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 9/120INFO 2025-01-02 15:06:38,616 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 10/120INFO 2025-01-02 15:06:42,015 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 11/120INFO 2025-01-02 15:06:45,435 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 12/120INFO 2025-01-02 15:06:48,679 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 13/120INFO 2025-01-02 15:06:52,025 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 14/120INFO 2025-01-02 15:06:55,317 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 15/120INFO 2025-01-02 15:06:58,604 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 16/120INFO 2025-01-02 15:07:01,927 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 17/120INFO 2025-01-02 15:07:05,287 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 18/120INFO 2025-01-02 15:07:08,763 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 19/120INFO 2025-01-02 15:07:12,131 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 20/120INFO 2025-01-02 15:07:15,476 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 21/120INFO 2025-01-02 15:07:18,881 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 22/120INFO 2025-01-02 15:07:22,203 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 23/120INFO 2025-01-02 15:07:25,599 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 24/120INFO 2025-01-02 15:07:28,991 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 25/120INFO 2025-01-02 15:07:32,234 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 26/120INFO 2025-01-02 15:07:35,714 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 27/120INFO 2025-01-02 15:07:38,974 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 28/120INFO 2025-01-02 15:07:42,265 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 29/120INFO 2025-01-02 15:07:45,582 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 30/120INFO 2025-01-02 15:07:45,586 instructlab.model.backends.vllm:136: vLLM engine successfully started at http://127.0.0.1:41531/v1╭────────────────────────────────────────────────────────────── system ──────────────────────────────────────────────────────────────╮│ Welcome to InstructLab Chat w/ SAMPLES_4435 (type /h for help)                                                                     │╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯&gt;&gt;&gt; Tell me everything you know about Swifties.                                                                           [S][default]╭─────────────────────────────────────────────────────────── samples_4435 ───────────────────────────────────────────────────────────╮│ Swifties are the fandom of the American singer-songwriter Taylor Swift.                                                            ││ Regarded by journalists as one of the largest, most devoted, and influential fan bases, Swifties are known for their high levels   ││ of participation, creativity, community, fanaticism, and cultural impact on the music industry and popular culture. They are a     ││ subject of widespread coverage in the mainstream media.                                                                            ││                                                                                                                                    ││ Critics have opined that Swift has redefined artist-fan relationships by establishing an intimate connection with Swifties. She    ││ has frequently engaged with, helped, credited, and prioritized her fans, who have offered unprecedented support and interest in    ││ her works irrespective of her wavering reception in the media. They continued to support Swift through her genre transitions,      ││ unexpected artistic pivots, and her highly publicized controversies such as the 2019 masters dispute, while instigating the        ││ political scrutiny of Ticketmaster that led to implementation of various laws and stimulated economic growth with the Eras Tour.   ││ Swift's releases, promotional efforts, and fashion have garnered attention for incorporating Easter eggs and clues that are        │......</code></pre></div><p><strong>Conclusions</strong></p><p>We’ve demonstrated a simple InstructLab workflow that is scheduled by IBM LSFin a compute cluster equipped with GPUs.  As part of this example, LSF GPUscheduling and accounting for GPU workloads was highlighted. For organizationslooking to productionize InstructLab and where there is a pool of GPU equippedcompute resources, LSF provides an ideal way to manage demand from a usercommunity looking to run these intensive workloads.</p><p>At the recent SC24 event, the demonstration went beyond what is shown in thisblog. It incorporated single click job submission via LSF Application Centerusing a custom template that was created for InstructLab to submits both thesynthetic data generation job, as well the training job with a single click.The demo environment was on IBM Cloud using instances equipped with Nvidia GPUs.The compute instances were automatically scaled up and down by the LSF resourceconnector. This will be the topic for a future blog.</p>",
            "url": "https://hpc.social/personal-blog/2025/fine-tuning-ai-models-with-instructlab-under-ibm-lsf/",
            
            
            
            
            
            "date_published": "2025-01-06T19:36:24-07:00",
            "date_modified": "2025-01-06T19:36:24-07:00",
            
                "author": "Ramblings of a supercomputing enthusiast."
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2025/surfing-the-singularity-please-hold-for-the-next-available-agent/",
            "title": "Surfing the Singularity - \"Please hold for the next available agent\"",
            "summary": null,
            "content_text": "&lt;div style=\"text-align: left;\"&gt;&lt;/div&gt;Our imaginations, having been so stimulated by the \"innovation trigger\" of early interactions with ChatGPT and its LLM kin, having experienced the illusion of the algorithm reading your mind, we have now firmly entered into the period of inflated expectations. Any day now we expect a knock on the door to be informed by some HAL Junior that not only are we now out of a job, we've also got 20 minutes to evacuate the premise before its bulldozed to make way for another solar farm and data center. AGI is only just one product announcement away, or maybe two, but certainly three at most... Nose Deep There is a strong desire on the part of companies trafficking in AI to generate not just chatbot hallucinations but also customers for real business use cases, meaning revenue, and now. To do that we're going to need hardware, fast, lots of it, and gigajoules to power it. So AWS buys a new data center in PA adjacent to a 2.5GW nuclear power plant.[1] Not to be outdone Microsoft re-revs up Three Mile Island (albeit with a catchy rebranding laughable by 1970's standards), with 100% of the power going to their regional AI data centers.[2] Three Mile Island nuclear power plant, aka the \"Crane Clean Energy Center\".  After vigorous expectations the trough of disillusionment will soon follow. Already Microsoft hints that demand for AI-oriented chips is waning.[3] Practical, as you'll have a hard time getting them anyway - the data-center grade GPU chips on which AI computation rely are in short supply - NVIDIA via their TSMC outsource manufacturing partner is fully booked for Blackwell GPU orders for the next 12 months.[4] AWS has recently announced to customers (like me) new limitations on availability of certain NVIDIA GPU instances. (Consider also that AI competes with crypto for these scarce GPUs.) Intel suggests it will ship mass quantities of chips for AI-ready PCs and other mobile devices in 2025, but the stock traders are not yet buying it, with the stock currently fallen over 50% year-over-year. In the end, and as evidenced by the long term investments, we of course expect the march of techno-progress to continue, but in the short run, aligning expectations with reality may remain a challenge.The August 2024 Gartner Hype Cycle for Emerging Technologies. Generative AI - weee! [5]What does OpenAI say about all this? First, the desire to be non-profit has bumped up against the realities of scaling up the models. Will they continue to scale up, yielding better and deeper performance on the road to artificial general intelligence simply by scaling up, or will they hit a theoretical wall?[6] Sam Altman says succinctly: \"there is no wall\".[7] The nuclear-powered race is on, be it sustainable or not.\"Your wait time is now less than...\"But as we argued in the last blog [8], we don't need dystopia-inducing super-human AGI in order to make productive and disruptive use of artificial intelligence technologies - a domain-tuned artificial capable intelligence (ACI) is enough.[9] Or a collaborating set of them. OpenAI's strategic product roadmap is more than a little vague [10], but in theory after chatbots capable of basic reasoning comes the age of agents - think: allowing Alexa to auto-restock your pantry via a hotline to Bezos when it overhears you say you're low on sugar. Such \"AI\" does such a good job doing basic thinks like, oh I dunno, controlling the lights in your home now, what could go wrong?! Truth is, today's LLMs perform only so-so on standardized benchmarks, and while they improve all the time [11], the current state of the art is not yet ready to be trusted and at times seems like snake oil.[12]Today's agents tend to be domain-specific and tailored to narrow purpose - Salesforce.com agents for common customer interactions, ServiceNow agents helping the human agent perform repetitive or summary tasks in handling case loads, but not replacing the human.[13,14] Google Gemini can add events to your calendar, help you plan travel, but is not yet trusted to actually borrow your credit card and book it. Keeping the human-in-the-loop will remain for now, as a stepping stone to full automation.If you visit agent marketplaces like Agent.ai or SwarmZero.ai, you'll see on the order of hundreds of agents available to handle what are largely small, mundane, and repetitive tasks. There are similar domain agent marketplaces on OpenAI's site, Anthropic's, GitHub, Hugging Face, and more. Let's go along with the current norm and define \"assistants\" as gaggles of agents loosely collaborating to accomplish more complex tasks, perhaps as part of a hybrid AI-human team or for some cases ultimately on behalf of the entire organization, and yet, still not requiring full-on AGI. (Consider what just one techno-savvy entrepreneur with a diverse collection of AI auto-orgs might do.)The missing elements are reliable agent accuracy, which yields trust, and the hardware and power to run it all. Trust, unfortunately in the near term, may play second fiddle to profit, as the AI snake oil is sold to companies and governments and ultimately end users, most of whom barely understand it.In fact, the scientists themselves barely understand it. The deep learning networks that power today's LLMs are generally black boxes, layers upon layers of neural networks, numeric weights and matrix computations, where its pretty difficult to tell where any given word, image fragment, or concept is held in the vast space of the model, and how with various feed-forward and back-propagation processes in the network it is used in computing responses.A GPT model formed by combining successive attention and neural net layers. Input comes in at the left, and its black boxes all the way down.[15]   Black box or not, as Sam Altman says, deep learning just works.[16] Sort of - AGI is unlikely without strong ontological and reasoning abilities and a tactile understanding of the physical world.[17] And deep learning itself is not without its problems. If the training data is biased, so will be the results. Trainers have to be alert to overfitting the model to the training data in a way that makes the model ineffective on new data. And implementors need better tools which help introspect and observe the model to provide verification, to illuminate the black box. Until then, any technology which cannot be understood is indistinguishable from magic.Hell-o OperatorAI is a broad term, encompassing many technologies, machine learning being just one of them, and deep learning based on neural networks being an even further niche. In many ways, given the black box nature of the solution, AI has become a substitute word for \"automation\", and/or \"program\", or \"algorithm\". And the ill-defined AI landscape is moving fast. Twelve months ago the buzz was about the emergence of the \"prompt engineer\" role in lieu of computer programmers, and today, not so much. Instead we now have thin but actionable (i.e. product-oriented) definitions like \"agent\" and \"assistant\" and a new suite of tools and cute icons to put on enterprise architecture diagrams. This is not to even mention the human and organizational impact of new agent-based workflows characterized by iterative, non-waterfall business processes - not something well understood or appreciated outside of software engineering circles.In this turbulent time, with vendors leapfrogging each other's capabilities and performance, there is no and cannot be any real standardization, no agreed abstractions on which to base a unifying orchestration layer. Move fast and break things, fix them later if they live long enough. Let the prototype knowingly become the short-lived product, and iterate, maybe. Think: sqrt of web time. Think: ChatGPT + IFTTT.[18] That is not an enterprise IT solution, nor one manageable for most individuals. That is a fine mess.Thankfully, we'll soon have AI assistants to fix it for us. - andy (linkedin: andygallojr)References[1] https://www.datacenterdynamics.com/en/news/aws-acquires-talens-nuclear-data-center-campus-in-pennsylvania/[2] https://www.datacenterdynamics.com/en/news/three-mile-island-nuclear-power-plant-to-return-as-microsoft-signs-20-year-835mw-ai-data-center-ppa/Readers unfamiliar with the nuclear accident at Three Mile Island in 1979 can read the summary here: https://en.wikipedia.org/wiki/Three_Mile_Island_accident[3] https://finance.yahoo.com/news/nvidia-stocks-correction-accelerated-since-020804144.html[4] https://www.smbom.com/news/14253[5] Gartner Hype Cycle for Emerging Technologies, August 2024, https://emt.gartnerweb.com/ngw/globalassets/en/newsroom/images/graphs/august_2024_ethc.png[6] \"The Computational Limits of Deep Learning\", https://arxiv.org/pdf/2007.05558[7] Sam Altman on X: \"there is no wall\", https://x.com/sama/status/1856941766915641580[8] Surfing the Singularity blog, https://surfthesing.blogspot.com/2024/12/surfing-singularity-coming-wave-book.html[9] \"The Coming Wave\", M. Suleyman, Crown Pub., 2023[10] https://www.theneurondaily.com/p/openais-leaked-agi-roadmap[11] 12 Days of OpenAI, Day 12: https://www.youtube.com/watch?v=SKBG1sqdyIU [12] \"AI Snake Oil\", Narayanan &amp; Kapoor, Princeton U. Press, 2024[13] https://www.salesforce.com/news/stories/einstein-sales-agents-announcement[14] https://www.servicenow.com/standard/resource-center/data-sheet/ds-virtual-agent.html[15] https://miro.medium.com/v2/0*-8c-MXmNvcvTLdHH.png We recommend the following video for those not familiar with this architecture:  https://youtu.be/KJtZARuO3JY?si=Muq2xRdSTaa9LMXb[16] https://ia.samaltman.com/[17] Yann LeCun on Lex Fridman podcast, https://www.youtube.com/watch?v=5t1vTLU7s40[18] https://ifttt.com/chatgpt",
            "content_html": "<div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p><br />&lt;div style=\"text-align: left;\"&gt;<br />&lt;/div&gt;</p><div><span style=\"font-family: verdana;\">Our imaginations, having been so stimulated by the \"innovation trigger\" of early interactions with ChatGPT and its LLM kin, having experienced the illusion of the algorithm reading your mind, we have now firmly entered into the period of inflated expectations. Any day now we expect a knock on the door to be informed by some HAL Junior that not only are we now out of a job, we've also got 20 minutes to evacuate the premise before its bulldozed to make way for another solar farm and data center. AGI is only just one product announcement away, or maybe two, but certainly three at most... </span></div><div><h3 style=\"text-align: left;\"><span style=\"font-family: verdana;\">Nose Deep </span></h3></div><div><span style=\"font-family: verdana;\">There is a strong desire on the part of companies trafficking in AI to generate not just chatbot hallucinations but also customers for real business use cases, meaning revenue, and now. To do that we're going to need hardware, fast, lots of it, and gigajoules to power it. So AWS buys a new data center in PA adjacent to a 2.5GW nuclear power plant.[1] Not to be outdone Microsoft re-revs up Three Mile Island (albeit with a catchy rebranding laughable by 1970's standards), with 100% of the power going to their regional AI data centers.[2] </span></div><div><span style=\"font-family: verdana;\"><br /></span><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://media.datacenterdynamics.com/media/images/Constellation_Three_Mile_Island.width-358.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" height=\"280\" src=\"https://media.datacenterdynamics.com/media/images/Constellation_Three_Mile_Island.width-358.png\" width=\"400\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><span style=\"font-family: verdana;\">Three Mile Island nuclear power plant, aka the \"Crane Clean Energy Center\".<br /></span>  </td></tr></tbody></table></div><div><span style=\"font-family: verdana;\">After vigorous expectations the trough of disillusionment will soon follow. Already Microsoft hints that demand for AI-oriented chips is waning.[3] Practical, as you'll have a hard time getting them anyway - the data-center grade GPU chips on which AI computation rely are in short supply - NVIDIA via their TSMC outsource manufacturing partner is fully booked for Blackwell GPU orders for the next 12 months.[4] AWS has recently announced to customers (like me) new limitations on availability of certain NVIDIA GPU instances. (Consider also that AI competes with crypto for these scarce GPUs.) Intel suggests it will ship mass quantities of chips for AI-ready PCs and other mobile devices in 2025, but the stock traders are not yet buying it, with the stock currently fallen over 50% year-over-year. In the end, and as evidenced by the long term investments, we of course expect the march of techno-progress to continue, but in the short run, aligning expectations with reality may remain a challenge.</span></div><div><span style=\"font-family: verdana;\"><br /></span></div><div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><div style=\"font-family: verdana;\">The August 2024 Gartner Hype Cycle for Emerging Technologies. </div><div style=\"font-family: verdana;\">Generative AI - weee! [5]</div></td></tr></tbody></table></div><div><span style=\"font-family: verdana;\"><div style=\"text-align: center;\"><br /></div>What does OpenAI say about all this? First, the desire to be non-profit has bumped up against the realities of scaling up the models. Will they continue to scale up, yielding better and deeper performance on the road to artificial general intelligence simply by scaling up, or will they hit a theoretical wall?[6] Sam Altman says succinctly: \"there is no wall\".[7] The nuclear-powered race is on, be it sustainable or not.</span></div><div><span style=\"font-family: verdana;\"><br /></span><h3 style=\"text-align: left;\"><span style=\"font-family: verdana;\">\"Your wait time is now less than...\"</span></h3></div><div><span style=\"font-family: verdana;\">But as we argued in the last blog [8], we don't need dystopia-inducing super-human AGI in order to make productive and disruptive use of artificial intelligence technologies - a domain-tuned artificial capable intelligence (ACI) is enough.[9] Or a collaborating set of them. </span></div><div><span style=\"font-family: verdana;\"><br />OpenAI's strategic product roadmap is more than a little vague [10], but in theory after chatbots capable of basic reasoning comes the age of agents - think: allowing Alexa to auto-restock your pantry via a hotline to Bezos when it overhears you say you're low on sugar. Such \"AI\" does such a good job doing basic thinks like, oh I dunno, controlling the lights in your home now, what could go wrong?! Truth is, today's LLMs perform only so-so on standardized benchmarks, and while they improve all the time [11], the current state of the art is not yet ready to be trusted and at times seems like snake oil.[12]</span></div><div><span style=\"font-family: verdana;\"><br />Today's agents tend to be domain-specific and tailored to narrow purpose - Salesforce.com agents for common customer interactions, ServiceNow agents helping the human agent perform repetitive or summary tasks in handling case loads, but not replacing the human.[13,14] Google Gemini can add events to your calendar, help you plan travel, but is not yet trusted to actually borrow your credit card and book it. Keeping the human-in-the-loop will remain for now, as a stepping stone to full automation.</span></div><div><span style=\"font-family: verdana;\"><br />If you visit agent marketplaces like Agent.ai or SwarmZero.ai, you'll see on the order of hundreds of agents available to handle what are largely small, mundane, and repetitive tasks. There are similar domain agent marketplaces on OpenAI's site, Anthropic's, GitHub, Hugging Face, and more. Let's go along with the current norm and define \"assistants\" as gaggles of agents loosely collaborating to accomplish more complex tasks, perhaps as part of a hybrid AI-human team or for some cases ultimately on behalf of the entire organization, and yet, still not requiring full-on AGI. (Consider what just one techno-savvy entrepreneur with a diverse collection of AI auto-orgs might do.)</span></div><div><span style=\"font-family: verdana;\"><br />The missing elements are reliable agent accuracy, which yields trust, and the hardware and power to run it all. Trust, unfortunately in the near term, may play second fiddle to profit, as the AI snake oil is sold to companies and governments and ultimately end users, most of whom barely understand it.</span></div><div><span style=\"font-family: verdana;\"><br />In fact, the scientists themselves barely understand it. The deep learning networks that power today's LLMs are generally black boxes, layers upon layers of neural networks, numeric weights and matrix computations, where its pretty difficult to tell where any given word, image fragment, or concept is held in the vast space of the model, and how with various feed-forward and back-propagation processes in the network it is used in computing responses.</span></div><div><span style=\"font-family: verdana;\"><br /></span></div><div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://miro.medium.com/v2/0*-8c-MXmNvcvTLdHH.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" height=\"360\" src=\"https://miro.medium.com/v2/0*-8c-MXmNvcvTLdHH.png\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><span style=\"font-family: verdana; text-align: left;\">A GPT model formed by combining successive attention and neural net layers. Input comes in at the left, and its black boxes all the way down.[15]</span></td></tr></tbody></table><div class=\"separator\" style=\"clear: both; text-align: center;\"> <span>  </span></div><span style=\"font-family: verdana;\"><br /></span></div><div><span style=\"font-family: verdana;\">Black box or not, as Sam Altman says, deep learning just works.[16] Sort of - AGI is unlikely without strong ontological and reasoning abilities and a tactile understanding of the physical world.[17] And deep learning itself is not without its problems. If the training data is biased, so will be the results. Trainers have to be alert to overfitting the model to the training data in a way that makes the model ineffective on new data. And implementors need better tools which help introspect and observe the model to provide verification, to illuminate the black box. Until then, any technology which cannot be understood is indistinguishable from magic.</span></div><div><span style=\"font-family: verdana;\"><br /></span><h3 style=\"text-align: left;\"><span style=\"font-family: verdana;\">Hell-o Operator</span></h3></div><div><span style=\"font-family: verdana;\">AI is a broad term, encompassing many technologies, machine learning being just one of them, and deep learning based on neural networks being an even further niche. In many ways, given the black box nature of the solution, AI has become a substitute word for \"automation\", and/or \"program\", or \"algorithm\". And the ill-defined AI landscape is moving fast. Twelve months ago the buzz was about the emergence of the \"prompt engineer\" role in lieu of computer programmers, and today, not so much. Instead we now have thin but actionable (i.e. product-oriented) definitions like \"agent\" and \"assistant\" and a new suite of tools and cute icons to put on enterprise architecture diagrams. This is not to even mention the human and organizational impact of new agent-based workflows characterized by iterative, non-waterfall business processes - not something well understood or appreciated outside of software engineering circles.</span></div><div><span style=\"font-family: verdana;\"><br />In this turbulent time, with vendors leapfrogging each other's capabilities and performance, there is no and cannot be any real standardization, no agreed abstractions on which to base a unifying orchestration layer. Move fast and break things, fix them later if they live long enough. Let the prototype knowingly become the short-lived product, and iterate, maybe. Think: sqrt of web time. Think: ChatGPT + IFTTT.[18] That is not an enterprise IT solution, nor one manageable for most individuals. That is a fine mess.</span></div><div><span style=\"font-family: verdana;\"><br />Thankfully, we'll soon have AI assistants to fix it for us. </span></div><div><span style=\"font-family: verdana;\"><br /></span></div><div><span style=\"font-family: verdana;\">- andy <span style=\"font-size: 16px;\">(linkedin: andygallojr)</span><br /><br /><br /></span><h3 style=\"text-align: left;\"><span style=\"font-family: verdana;\">References</span></h3><span style=\"font-family: verdana;\">[1] <a href=\"https://www.datacenterdynamics.com/en/news/aws-acquires-talens-nuclear-data-center-campus-in-pennsylvania/\">https://www.datacenterdynamics.com/en/news/aws-acquires-talens-nuclear-data-center-campus-in-pennsylvania/</a><br />[2] <a href=\"https://www.datacenterdynamics.com/en/news/three-mile-island-nuclear-power-plant-to-return-as-microsoft-signs-20-year-835mw-ai-data-center-ppa/\">https://www.datacenterdynamics.com/en/news/three-mile-island-nuclear-power-plant-to-return-as-microsoft-signs-20-year-835mw-ai-data-center-ppa/</a></span></div><div><span style=\"font-family: verdana;\">Readers unfamiliar with the nuclear accident at Three Mile Island in 1979 can read the summary here: <a href=\"https://en.wikipedia.org/wiki/Three_Mile_Island_accident\">https://en.wikipedia.org/wiki/Three_Mile_Island_accident</a></span></div><div><span style=\"font-family: verdana;\">[3] <a href=\"https://finance.yahoo.com/news/nvidia-stocks-correction-accelerated-since-020804144.html\">https://finance.yahoo.com/news/nvidia-stocks-correction-accelerated-since-020804144.html</a><br />[4] <a href=\"https://www.smbom.com/news/14253\">https://www.smbom.com/news/14253</a><br />[5] Gartner Hype Cycle for Emerging Technologies, August 2024, <a href=\"https://emt.gartnerweb.com/ngw/globalassets/en/newsroom/images/graphs/august_2024_ethc.png\">https://emt.gartnerweb.com/ngw/globalassets/en/newsroom/images/graphs/august_2024_ethc.png</a><br />[6] \"The Computational Limits of Deep Learning\", <a href=\"https://arxiv.org/pdf/2007.05558\">https://arxiv.org/pdf/2007.05558</a><br />[7] Sam Altman on X: \"there is no wall\", <a href=\"https://x.com/sama/status/1856941766915641580\">https://x.com/sama/status/1856941766915641580</a></span></div><div><span style=\"font-family: verdana;\">[8] Surfing the Singularity blog, <a href=\"https://surfthesing.blogspot.com/2024/12/surfing-singularity-coming-wave-book.html\">https://surfthesing.blogspot.com/2024/12/surfing-singularity-coming-wave-book.html</a></span></div><div><span style=\"font-family: verdana;\">[9] <span style=\"background-color: black; font-size: 15px;\">\"The Coming Wave\", </span><span style=\"background-color: black; font-size: 15px;\">M. Suleyman, Crown Pub., 2023</span><br />[10] <a href=\"https://www.theneurondaily.com/p/openais-leaked-agi-roadmap\">https://www.theneurondaily.com/p/openais-leaked-agi-roadmap</a><br />[11] 12 Days of OpenAI, Day 12: <a href=\"https://www.youtube.com/watch?v=SKBG1sqdyIU\">https://www.youtube.com/watch?v=SKBG1sqdyIU</a> <br />[12] \"AI Snake Oil\", Narayanan &amp; Kapoor, Princeton U. Press, 2024<br />[13] <a href=\"https://www.salesforce.com/news/stories/einstein-sales-agents-announcement\">https://www.salesforce.com/news/stories/einstein-sales-agents-announcement</a><br />[14] <a href=\"https://www.servicenow.com/standard/resource-center/data-sheet/ds-virtual-agent.html\">https://www.servicenow.com/standard/resource-center/data-sheet/ds-virtual-agent.html</a><br />[15] <a href=\"https://miro.medium.com/v2/0*-8c-MXmNvcvTLdHH.png\">https://miro.medium.com/v2/0*-8c-MXmNvcvTLdHH.png</a> We recommend the following video for those not familiar with this architecture:  <a href=\"https://youtu.be/KJtZARuO3JY?si=Muq2xRdSTaa9LMXb\">https://youtu.be/KJtZARuO3JY?si=Muq2xRdSTaa9LMXb</a><br />[16] <a href=\"https://ia.samaltman.com/\">https://ia.samaltman.com/</a><br />[17] Yann LeCun on Lex Fridman podcast, <a href=\"https://www.youtube.com/watch?v=5t1vTLU7s40\">https://www.youtube.com/watch?v=5t1vTLU7s40</a><br />[18] <a href=\"https://ifttt.com/chatgpt\">https://ifttt.com/chatgpt</a><br /><br /></span></div>",
            "url": "https://hpc.social/personal-blog/2025/surfing-the-singularity-please-hold-for-the-next-available-agent/",
            
            
            
            
            
            "date_published": "2025-01-03T17:00:00-07:00",
            "date_modified": "2025-01-03T17:00:00-07:00",
            
                "author": "Surfing the Singularity"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2024/surfing-the-singularity-the-coming-wave-a-book-report/",
            "title": "Surfing the Singularity - \"The Coming Wave\" (a book report)",
            "summary": null,
            "content_text": "Mustapha Suleyman knows a thing or two about AI.  Originally co-founder of DeepMind, a company and IP eventually acquired by Google, Mr. Suleyman is now CEO of AI at Microsoft. In this latest \"Surfing the Singularity\" blog installment, we'll review his recent book \"The Coming Wave\". Hang ten!Go Where You Wanna GoAs a game, Go is notorious for its huge array of potential moves, exponentially more complex than chess for example, where computer models beat the best chess player way back in 1997. In 2016, DeepMind's model AlphaGo beat the best Go player in world after being trained the better part of a year with reinforced machine learning on a data set of human Go games and computer-vs-computer play. The following year, DeepMind's AlphaZero exceeded that performance in just a few days of training computation without ever being shown a single Go game, just having been described the rules of the game.[1]   &lt;div style=\"text-align: center;\"&gt;Alas, born at the wrong time.&lt;/div&gt;In his Bill Gates-recommended [2] book \"The Coming Wave\", Mr. Suleyman's dystopian thesis is this: that the combination of AI, synthetic biology, and a host of other general purpose technologies such as robotics and additive manufacturing will combine into a major technological wave which will wash over the human race and alter it in unprecedented ways. Much as in past waves - the harnessing of fire, the wheel, the printing press, the combustion engine - each set off dramatic and often cataclysmic societal change the likes of which was certainly not obvious or expected by the \"engineers\" which developed the tooling. Call it the \"rule of unintended consequences\". The author supposes there have been about two dozen such waves over human history, and as expected in these times, the rate of arrival of transformational technologies is accelerating.Take the printing press. Originally in 1440 there is but one device, the lab prototype. Fifty years later there are 1,000 printing presses in Europe. From producing just a few thousand hand-copied manuscripts a year, the bookmakers now produced millions. Demand for books soars, cost per unit drops, adoption deepens. What was the impact of this new information proliferation in the society? As Suleyman writes: \"Gutenberg just wanted to make money printing Bibles. Yet his press catalyzed the Scientific Revolution and the Reformation, and so became the greatest threat to the Catholic Church since its establishment.\" And in spite of the efforts of certain Byzantine lords to control the press, proliferation of the technology was and is the default, driven by FOMO, at least.Straight Outta CoruscantThe mass-scale rollout of AI is already underway, hand-in-hand with surveillance devices at the edge, high speed networking, nearly bottomless storage, and high performance computing on demand to make sense of it all. All so \"The Algorithm\" can feed you tailored news (and ads) with your morning coffee. And more, much more. Large Language Models (LLMs), trained on the corpus of human written and other creative output can now generate helpful suggestions in a variety of useful contexts (such as blog writing). And as I wrote about in my last blog [3], LLMs are useful code assistants too, although here current state of the art is about a 50% success rate on senior-level software development tasks. So yes, there is room to grow, but in line with the acceleration of the rate of change, we expect that gap to be closed in short time.What then? A whole host of human-centric but generally rule-oriented tasks - think: back-office work in the finance and insurance industry - will become fair game for AI augmentation, meaning, human replacement. We see the rise of autonomous vehicles - think: bus and cab drivers, mail and package delivery, pilots. Air traffic controllers. Call centers. Medical radiology readers. Not one of these applications requires a super \"artificial general intelligence\" (AGI), simply a good model tailored to a specific task set, aka \"artificial capable intelligence\" (ACI). This is nearly all line-of-sight to market.What then is not here now? The author spends a good amount of time discussing the rise and impact of artificial and synthetic biology, CRISPR gene hacking and the like. Not being personally equipped to analyze such biotechnologies, I'm simply going to leave that one to the reader, suggest its some heady stuff, but otherwise stay in the domain of the electro-mechanical. But even with this scope limitation, what is the wider wave?&lt;p&gt;&lt;/p&gt;Consider the rise of the bots, farm bots. GPS-guided autonomous tractors (already a thing).[4] These robots don’t look like C3PO tending the moisture reapers on Tatooine - these robots look like farm equipment and are painted John Deere green. Amazon and Walmart distribution warehouses are already highly automated - combined with autonomous vehicles and AI-driven back-office work, how many employees do we think Amazon will need in 2 years? In 5 years? They currently employ 1.5 million people and reduced headcount 5% in the last 2 years while growing revenues over 20%.[5]&lt;p&gt;&lt;/p&gt;Mind the GapAnd while your local Joe the Plumber [6] may continue to have a job visiting homes for some time to come, the use of single-task robotic automation in construction, especially new commercial construction, and property maintenance is on the rise. Concrete and paint-spraying robots. High rise window washers. Roofers, and general laborers to move material around a job site - bots - flying, floating, swimming, walking, drilling, boring bots. And more factory bots too, using traditional and additive techniques, to print their own parts. Bots that make bots. And an unemployment office at the Department of Labor run by AI.[7]What about joining up with Uncle Sam, see the world, serve your country? Drone warfare in Ukraine has shown the folly of the massing of expensively equipped troops, and in the Red Sea the risks associated with large and high priced floating projections of power. Hypersonic weapons, beyond the capabilities of a human in-the-loop system to thwart. The result is asymmetric bot-on-bot warfare, beyond the battlefield, beyond borders. What are we to do with legions of technically unemployed, if they are not even useful for cannon fodder? And what are we to do with the State, if it cannot provide a system which benefits the population, which can keep it protected from proliferating technological threats? With information, wealth, and power centralized in the hands of a self-selected few, is it pitchforks and torches to the barricades then?[8]While its clear and no surprise the coming wave will benefit those with technical and financial authority, there is a chance of a boomerang effect which will result in forces in the opposite direction. The tooling, including the availability of sophisticated AI models and the means to run them, is being democratized. While increasing in capability, the cost of military-grade drones has decreased orders of magnitude in the last decade.[9] Rabble-rousing AI deepfakes proliferate. As Mr. Suleyman says, \"anyone motivated to sow instability now has an easier time of it\", not just state actors, agents, or oligarchs, but anyone with a few thousand dollars and an axe to grind. And considering the examples from recent 21st century past, if a rogue actor were to leverage the technology for nefarious purposes (think: 9/11 and the Patriot Act), there would surely be immediate call by the population for protection, likely but perhaps not exclusively by the State, backed by pervasive security surveillance. And this time, the means to fully execute on that wish exists. The China Syndrome&lt;p style=\"text-align: center;\"&gt;”The system works! That’s not the problem!”&lt;/p&gt;&lt;div style=\"text-align: left;\"&gt;It is a coming wave of contradictions and competing forces, and it sounds disruptive and quite unpleasant to say the least, perhaps even a human catastrophe. And besides avoiding the topic of bioengineering, we also haven’t yet discussed what happens when we actually do get to superhuman generalized AI - we’re still talking here about relatively dumb AI with human actors in charge, in theory.&lt;/div&gt;&lt;p&gt;&lt;/p&gt;The author Mr. Suleyman concludes that the containment of this new technology - this artificial intelligence backed by autonomous mobility - a containment which has rarely if ever been possible (nukes being maybe the sole exception), must be done successfully, and urgently. Its a good sentiment, albeit one which may be too optimistic, even blindly. Can the march of this autonomous AI \"progress\" with its obvious and as yet to be seen additional consequences be stopped? I would argue, and the author would likely in the final analysis have to admit, that it cannot.What to do about it? Maybe we should give serious thought to the existential question of what it actually means to be human.[10] Or, alternatively, as Timothy Leary said...[11]Until next time.  - andyReferences &amp; Amusements[1] \"The Coming Wave\", Mustapha Suleyman, Crown Pub., 2023[2] Bill Gates blog, https://www.gatesnotes.com/holiday-books-2024[3] \"Surfing the Singularity: Super Grover!\", https://surfthesing.blogspot.com/2024/12/surfing-singularity-super-grover.html[4] \"John Deere Robot Planter\", https://www.cnet.com/tech/john-deere-robot-planter-the-future-of-farming-looks-like-fewer-chemicals/[5] https://www.statista.com/statistics/234488/number-of-amazon-employees/ and https://www.statista.com/statistics/266282/annual-net-revenue-of-amazoncom/[6] https://www.nytimes.com/2023/08/28/us/politics/samuel-wurzelbacher-joe-the-plumber-dead.html[7] https://www.dol.gov/agencies/oasam/centers-offices/ocio/ai-inventory[8] https://www.stlouisfed.org/community-development-research/the-state-of-us-wealth-inequality[9] https://www.technologyreview.com/2023/01/30/1067348/mass-market-military-drones-have-changed-the-way-wars-are-fought/[10] https://www.organism.earth/library/document/unapologetically-human[11] https://www.youtube.com/watch?v=IPSzTBP5PAU",
            "content_html": "<p><span style=\"font-family: verdana;\">Mustapha Suleyman knows a thing or two about AI.  Originally co-founder of DeepMind, a company and IP eventually acquired by Google, Mr. Suleyman is now CEO of AI at Microsoft. In this latest \"Surfing the Singularity\" blog installment, we'll review his recent book \"The Coming Wave\". Hang ten!</span></p><p><span style=\"font-family: verdana;\"><br /></span></p><p><span style=\"font-family: verdana; font-size: x-large;\">Go Where You Wanna Go</span></p><p><span style=\"font-family: verdana;\">As a game, Go is notorious for its huge array of potential moves, exponentially more complex than chess for example, where computer models beat the best chess player way back in 1997. In 2016, DeepMind's model AlphaGo beat the best Go player in world after being trained the better part of a year with reinforced machine learning on a data set of human Go games and computer-vs-computer play. The following year, DeepMind's AlphaZero exceeded that performance in just a few days of training computation without ever being shown a single Go game, just having been described the rules of the game.[1]   </span></p><p><span style=\"font-family: verdana;\"></span></p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p><br />&lt;div style=\"text-align: center;\"&gt;Alas, born at the wrong time.&lt;/div&gt;</p><p></p><p><span style=\"font-family: verdana;\">In his Bill Gates-recommended [2] book \"The Coming Wave\", Mr. Suleyman's dystopian thesis is this: that the combination of AI, synthetic biology, and a host of other general purpose technologies such as robotics and additive manufacturing will combine into a major technological wave which will wash over the human race and alter it in unprecedented ways. Much as in past waves - the harnessing of fire, the wheel, the printing press, the combustion engine - each set off dramatic and often cataclysmic societal change the likes of which was certainly not obvious or expected by the \"engineers\" which developed the tooling. Call it the \"rule of unintended consequences\". The author supposes there have been about two dozen such waves over human history, and as expected in these times, the rate of arrival of transformational technologies is accelerating.</span></p><p><span style=\"font-family: verdana;\">Take the printing press. Originally in 1440 there is but one device, the lab prototype. Fifty years later there are 1,000 printing presses in Europe. From producing just a few thousand hand-copied manuscripts a year, the bookmakers now produced millions. Demand for books soars, cost per unit drops, adoption deepens. What was the impact of this new information proliferation in the society? As Suleyman writes: \"Gutenberg just wanted to make money printing Bibles. Yet his press catalyzed the Scientific Revolution and the Reformation, and so became the greatest threat to the Catholic Church since its establishment.\" And in spite of the efforts of certain Byzantine lords to control the press, proliferation of the technology was and is the default, driven by FOMO, at least.</span></p><p><span style=\"font-family: verdana;\"><br /></span></p><p><span style=\"font-family: verdana; font-size: x-large;\">Straight Outta Coruscant</span></p><p><span style=\"font-family: verdana;\">The mass-scale rollout of AI is already underway, hand-in-hand with surveillance devices at the edge, high speed networking, nearly bottomless storage, and high performance computing on demand to make sense of it all. All so \"The Algorithm\" can feed you tailored news (and ads) with your morning coffee. And more, much more. Large Language Models (LLMs), trained on the corpus of human written and other creative output can now generate helpful suggestions in a variety of useful contexts (such as blog writing). And as I wrote about in my last blog [3], LLMs are useful code assistants too, although here current state of the art is about a 50% success rate on senior-level software development tasks. So yes, there is room to grow, but in line with the acceleration of the rate of change, we expect that gap to be closed in short time.</span></p><p><span style=\"font-family: verdana;\">What then? A whole host of human-centric but generally rule-oriented tasks - think: back-office work in the finance and insurance industry - will become fair game for AI augmentation, meaning, human replacement. We see the rise of autonomous vehicles - think: bus and cab drivers, mail and package delivery, pilots. Air traffic controllers. Call centers. Medical radiology readers. Not one of these applications requires a super \"artificial general intelligence\" (AGI), simply a good model tailored to a specific task set, aka \"artificial capable intelligence\" (ACI). This is nearly all line-of-sight to market.</span></p><p><span style=\"font-family: verdana;\">What then is not here now? The author spends a good amount of time discussing the rise and impact of artificial and synthetic biology, CRISPR gene hacking and the like. Not being personally equipped to analyze such biotechnologies, I'm simply going to leave that one to the reader, suggest its some heady stuff, but otherwise stay in the domain of the electro-mechanical. But even with this scope limitation, what is the wider wave?</span></p><p></p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p><span style=\"font-family: verdana;\">&lt;p&gt;<span style=\"font-family: verdana;\"><br /></span>&lt;/p&gt;Consider the rise of the bots, farm bots. GPS-guided autonomous tractors (already a thing).</span><span style=\"font-family: verdana;\">[4]</span><span style=\"font-family: verdana;\"> These robots don’t look like C3PO tending the moisture reapers on Tatooine - these robots look like farm equipment and are painted John Deere green. Amazon and Walmart distribution warehouses are already highly automated - combined with autonomous vehicles and AI-driven back-office work, how many employees do we think Amazon will need in 2 years? In 5 years? They currently employ 1.5 million people and reduced headcount 5% in the last 2 years while growing revenues over 20%.[5]</span>&lt;p&gt;&lt;/p&gt;</p><p><span style=\"font-family: verdana;\"><br /></span></p><p><span style=\"font-family: verdana;\"><span style=\"font-size: x-large;\">Mind the Gap</span></span></p><p><span style=\"font-family: verdana;\">And while your local Joe the Plumber [6] may continue to have a job visiting homes for some time to come, the use of single-task robotic automation in construction, especially new commercial construction, and property maintenance is on the rise. Concrete and paint-spraying robots. High rise window washers. Roofers, and general laborers to move material around a job site - bots - flying, floating, swimming, walking, drilling, boring bots. And more factory bots too, using traditional and additive techniques, to print their own parts. Bots that make bots. And an unemployment office at the Department of Labor run by AI.[7]</span></p><p><span style=\"font-family: verdana;\">What about joining up with Uncle Sam, see the world, serve your country? Drone warfare in Ukraine has shown the folly of the massing of expensively equipped troops, and in the Red Sea the risks associated with large and high priced floating projections of power. Hypersonic weapons, beyond the capabilities of a human in-the-loop system to thwart. The result is asymmetric bot-on-bot warfare, beyond the battlefield, beyond borders. What are we to do with legions of technically unemployed, if they are not even useful for cannon fodder? And what are we to do with the State, if it cannot provide a system which benefits the population, which can keep it protected from proliferating technological threats? With information, wealth, and power centralized in the hands of a self-selected few, is it pitchforks and torches to the barricades then?[8]</span></p><p><span style=\"font-family: verdana;\"></span></p><div class=\"separator\" style=\"clear: both; text-align: center;\"><span style=\"font-family: verdana;\"></span></div><p></p><p><span style=\"font-family: verdana;\">While its clear and no surprise the coming wave will benefit those with technical and financial authority, there is a chance of a boomerang effect which will result in forces in the opposite direction. The tooling, including the availability of sophisticated AI models and the means to run them, is being democratized. While increasing in capability, the cost of military-grade drones has decreased orders of magnitude in the last decade.[9] Rabble-rousing AI deepfakes proliferate. As Mr. Suleyman says, \"anyone motivated to sow instability now has an easier time of it\", not just state actors, agents, or oligarchs, but anyone with a few thousand dollars and an axe to grind. And considering the examples from recent 21st century past, if a rogue actor were to leverage the technology for nefarious purposes (think: 9/11 and the Patriot Act), there would surely be immediate call by the population for protection, likely but perhaps not exclusively by the State, backed by pervasive security surveillance. And this time, the means to fully execute on that wish exists. </span></p><p><span style=\"font-family: verdana;\"><br /></span></p><p><span style=\"font-family: verdana;\"><span style=\"font-size: x-large;\">The China Syndrome</span></span></p><p><span style=\"font-family: verdana;\"><span style=\"font-size: x-large;\"></span></span></p><div class=\"separator\" style=\"clear: both; text-align: center;\"><span style=\"font-size: x-large;\"></span></div><p><span style=\"font-family: verdana;\">&lt;p style=\"text-align: center;\"&gt;<span style=\"font-family: verdana;\">”</span><span color=\"rgba(0, 0, 0, 0.87)\" face=\"Roboto, Helvetica, Arial, sans-serif\" style=\"background-color: white; font-size: 16px; letter-spacing: 0.5px;\">The system works! That’s not the problem!”</span>&lt;/p&gt;</span><span style=\"font-family: verdana;\">&lt;div style=\"text-align: left;\"&gt;It is a coming wave of contradictions and competing forces, and it sounds disruptive and quite unpleasant to say the least, perhaps even a human catastrophe. And besides avoiding the topic of bioengineering, we also haven’t yet discussed what happens when we actually do get to superhuman generalized AI - we’re still talking here about relatively dumb AI with human actors in charge, in theory.&lt;/div&gt;</span>&lt;p&gt;&lt;/p&gt;</p><p><span style=\"font-family: verdana;\">The author Mr. Suleyman concludes that the containment of this new technology - this artificial intelligence backed by autonomous mobility - a containment which has rarely if ever been possible (nukes being maybe the sole exception), must be done successfully, and urgently. Its a good sentiment, albeit one which may be too optimistic, even blindly. Can the march of this autonomous AI \"progress\" with its obvious and as yet to be seen additional consequences be stopped? I would argue, and the author would likely in the final analysis have to admit, that it cannot.</span></p><p><span style=\"font-family: verdana;\">What to do about it? Maybe we should give serious thought to the existential question of what it actually means to be human.[10] Or, alternatively, as Timothy Leary said...[11]</span></p><p><span style=\"font-family: verdana;\">Until next time.  </span><span style=\"font-family: verdana;\">- andy</span></p><p><span style=\"font-family: verdana;\"><br /></span></p><p><br /></p><p><span style=\"font-family: verdana; font-size: x-large;\">References &amp; Amusements</span></p><p><span style=\"font-family: verdana;\">[1] \"The Coming Wave\", </span><span style=\"font-family: verdana;\">Mustapha Suleyman, Crown Pub., 2023</span></p><p><span style=\"font-family: verdana;\">[2] Bill Gates blog, https://www.gatesnotes.com/holiday-books-2024</span></p><p><span style=\"font-family: verdana;\">[3] \"Surfing the Singularity: Super Grover!\", </span><span style=\"font-family: verdana;\">https://surfthesing.blogspot.com/2024/12/surfing-singularity-super-grover.html</span></p><p><span style=\"font-family: verdana;\">[4] \"John Deere Robot Planter\"</span><span style=\"color: #020203;\"><span style=\"font-family: verdana;\">, </span></span><span style=\"font-family: verdana;\">https://www.cnet.com/tech/john-deere-robot-planter-the-future-of-farming-looks-like-fewer-chemicals/</span></p><p><span style=\"font-family: verdana;\">[5] https://www.statista.com/statistics/234488/number-of-amazon-employees/ and https://www.statista.com/statistics/266282/annual-net-revenue-of-amazoncom/</span></p><p><span style=\"font-family: verdana;\">[6] https://www.nytimes.com/2023/08/28/us/politics/samuel-wurzelbacher-joe-the-plumber-dead.html</span></p><p><span style=\"font-family: verdana;\">[7] https://www.dol.gov/agencies/oasam/centers-offices/ocio/ai-inventory</span></p><p><span style=\"font-family: verdana;\">[8] https://www.stlouisfed.org/community-development-research/the-state-of-us-wealth-inequality</span></p><p><span style=\"font-family: verdana;\">[9] https://www.technologyreview.com/2023/01/30/1067348/mass-market-military-drones-have-changed-the-way-wars-are-fought/</span></p><p><span style=\"font-family: verdana;\">[10] https://www.organism.earth/library/document/unapologetically-human</span></p><p><span style=\"font-family: verdana;\">[11] https://www.youtube.com/watch?v=IPSzTBP5PAU</span></p><p><span style=\"font-family: verdana;\"><br /></span></p><p><br /></p>",
            "url": "https://hpc.social/personal-blog/2024/surfing-the-singularity-the-coming-wave-a-book-report/",
            
            
            
            
            
            "date_published": "2024-12-18T17:00:00-07:00",
            "date_modified": "2024-12-18T17:00:00-07:00",
            
                "author": "Surfing the Singularity"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2024/surfing-the-singularity-super-grover/",
            "title": "Surfing the Singularity - Super Grover!",
            "summary": null,
            "content_text": "Hello and happy holidays to all. In this blog installment I'll report back from SuperComputing 2024, offer up a programmer-friendly view of the quantum computing space with a code tour of Grover's algorithm, and share some of my own thoughts on using the latest crop of AI programmer assistant tools. (Sadly, not this Grover.)It was a pleasant SC24 high performance computing (HPC) conference in November. Having attended in past either in-person (Atlanta this year) or virtual, this year I chose virtual again. The big loss was being unable to troll the enormous vendor hall, but otherwise, webcasts make it much easier to be in two places at one time or to skim topics of passing interest.[1] There's a new top HPC machine (that we know of) - El Capitan, and its powered by AMD, containing about a million CPU cores and about 10 million GPU cores.[2] Molecular dynamics papers presented using a GPU-accelerated exascale computer reminds that, quantum aside for a moment, the real work is still being done in the classical world.[3] NVIDIA showcased the growing fusion of AI and HPC with their \"superchip\" designs - incorporating a CPU and a GPU on the same chip.[4] And why not, the money keeps flowing, the current outgoing federal administration now locking in the CHIPS Act funding before the end of the term.[5]NVIDIA's Grace Hopper architecture [6]But beyond the incremental improvements in compute, storage, cooling, power consumption and the like, it seemed to me, through my remote goggles, that the real action was happening on the sidelines of SC24, in the quantum computing space.Quantum Chip on Shoulder There's significant skepticism of quantum computing from the HPC community at the moment. Quantum computers today are toys in comparison to HPC, and stakeholders in HPC and classical computing (which would include myself) might wonder aloud \"what's the point?\" For some applications (like the fluid dynamics apps my company uses), quantum utility is perhaps still a decade away.[7] But the groundwork is being laid today, and when we understand that there are useful problems which can be solved on quantum computers, and only on quantum computers, we might at least allow the playing to continue. And we might be surprised at how fast quantum computing is progressing, and also just churning. Even those in the industry are hedging their bets - on which qubit technologies and which companies will be the winners - and as such are changing partners on a regular basis [8,9,10], although this sometimes means needing to unproductively reinvent the wheel (how many Python quantum circuit libraries do we need?)[11]A couple of new announcements from Big Blue caught my eye. First, an early demonstrator of incorporating an IBM quantum computer into an HPC data center, unifying the resource scheduling, was shown at RPI with their AiMOS cluster and their first-in-the-nation academic installation.[12] The second, and more important, was the paper in Nature demonstrating the union of two quantum computers via classical networks, providing another avenue for scaling up hybrid quantum computing.[13] But today's QPUs are still noisy, fragile, small, expensive, and scarce. Maturity is still a ways off. QPUs are not fungible - to be successful in executing an application on a quantum computer, we must understand the error profile of that specific device! Not just that brand of quantum computer product, but this instance of that product! We need hands-on examples to grow the personal and team experience with quantum while we await stabilized hardware and the productivity-enabling software abstractions which can only come after industry maturity, and these early (head-butting) experiences. Making Your Quantum BonesToday's education in and around quantum computing is still focused on the experimental audience. We are still doing experiments about quantum more than experiments with quantum. The educational material which does exist, and there is an increasing amount, is focused on a community which is very comfortable with quantum physics and its associated mathematics. Most people, myself included, do not fit this description. As a computer science graduate, adjunct prof, and software engineer by trade, I want to see higher level programming abstractions, not those centered around qubits and gates which clearly does not scale for large programs. We will be waiting a while. So in the meantime, we need to see some examples using today's technology and syntax but which are more suited for the Comp Sci student audience, to help begin to bridge that gap. If we visit the Algorithm Zoo [14], a collection of quantum algorithms which show a computational advantage over similar classical approaches, we find some things (but not many) which might look familiar to a CS undergrad, but the implementations, when they exist, are often broken. In this current early phase of the quantum era, vendors are playing free and loose with their SDKs, and releases with significant object model refactorings and breaking changes are the norm. pattern_match.pySo I offer here an example of a quantum program which itself likely has a short shelf life. You can find it here.[15, &amp; above] It shows a cooked up example of using Grover's algorithm, which is a quantum algorithm for search in unstructured (e.g. unsorted) data. Classically, \"cuz Murphy's Law\", you might need to walk the entire dataset to find the item of interest (or show its not there) - we call this an O(n) algorithm. Using Grover's algorithm, you can do the search with a strong probability of success in O(sqrt(n)) time - a quadratic speedup, and one worth pursuing for many applications. Note we are only saying we can perform the search with high probability - this is quantum computing, and everything is a probability. And while the example shown isn't necessarily the most common application of Grover, understanding Grover is worthwhile, as it appears as a sub-procedure in many other quantum algorithms.Here are a few key points to take away, even if you don't take the time to look at the documented code example:We're going to store a small set of strings - binary strings of 1's and 0's representing a small dataset - into a quantum system of qubits. Since our strings are chosen to be 16 binary digits long, we will use 16 qubits. This is a large enough number of qubits to show some non-trivial problems, but not so many as to not be runnable on a simulator on your laptop. (The performance does not scale linearly with the number of qubits.)A system of n qubits contains 2**n possible states (combinations of 0's and 1's). That's a lot. We will have far fewer state strings in our dataset - just a handful for this experiment.After initializing the qubits, we will mark each state in the quantum system which corresponds to a binary string in our dataset. To do this we will use the phase of the qubit, which is an extra useful lever you do not find in classical bits (among other unique quantum advantages).Three qubits in superposition.[16]Marking one of the states by flipping its phase.[16]Using Grover's algorithm, we will amplify the probability of finding the marked states relative to other background states. I.e., the signal is separated from the noise. To do this we iteratively apply an oracle quantum circuit to the initial system.The states, with our target amplified, after some number of iterations of Grover's algorithm.[16]Note that we use the same 16 qubits to encode all of the 16-digit strings in the dataset. Try that with a classical computer!We then use Grover's algorithm again using a target string, applying the oracle against the now prepared quantum system, and returning the result that the target either is or is not in the dataset.Hopefully this gives some flavor of what its like to program a quantum computer using an example most classical programmers today can understand. There are places where the code can potentially be improved - I welcome your input in the comments section below.AI AI, Oh. During these quantum coding explorations, during other coding work, and while writing documents (like this one), I've also been experimenting with a sequence of so-called \"AI assistants\". Starting with Copilot as a plugin for the popular VS Code IDE, I quickly switched to the Codeium plugin which performed better for my purposes, mostly because it took more of my code into context while making suggestions.[17] Since, the various vendors have leap-frogged each other in features and quality of results, and so while this tool comparison may be stale, its also the case that any new comparison would soon itself become stale. While the VS Code IDE provides hooks for plugins, some vendors, most notably Cursor [18] took the approach that to provide a true AI assistant (AIA) for a modern software developer you needed plugin hooks in many places, and control over things like the rendering of change sets. So they forked the VS Code IDE entirely. And they weren't the only ones to do this - Codeium now also has its own IDE called Windsurf - this is what I'm currently using. This can't be end state for the discussion and more leap-frogging is expected, so watch this space. I've now seen cases where people are putting \"Cursor\" on their resume - not a good idea to commit to that, but it is a good idea to start using one of these modern tools in your work, and to better understand how to adopt your work practices in light of their potential.In other words, to experiment while these AIA tools improve. The head of Anthropic, a major AI player and the maker of the model driving my current IDE, is proud to say their model reaches 50% success rates on a popular software engineering metric.[19,20] Do you want to pair program with an assistant which is only right 50% of the time? I can tell you that its got its ups and downs.On the positive, for mundane tasks, it can do a pretty good job. Questions like \"how do you do X in Python?\" which you might have previously taken to Stack Overflow can now be answered in the IDE with custom-made code samples. You can turn your question into an instruction: Me: modify this code to take the target string as a command line argument and default to the current target string if none is providedAIA: I've modified the code to...And it will describe in words and code a working solution, albeit in my case one I found to be slightly over-engineered:Me: use sys to determine if a single arg was providedAIA: I'll modify the code to use sys.argv for a simpler command line argument handling...In another interaction, we debated if an AIA-proposed solution was mathematically efficient, at one point:AIA: Would you like me to: 1) Implement a simpler but less efficient version that demonstrates the concept? 2) Or explore how to properly implement the quantum multiplexing needed for efficient database loading?Me: yes, but make it in a new fileAnd it did. The main reason for asking for a new file rather than change in place was that the AIA is still often wrong! And reviewing a large amount of inline change is hard for the human eye and mind.Me: but now your proposed solution doesn't load the database or even look at it!AIA: Ah, you're absolutely right again! And it then goes on to suggest further changes, which you might wonder why those changes weren't proposed in the first place? Keep in mind, its only (proudly) right 50% of the time. It is, as we say, \"generative AI\" after all - it doesn't so much \"hallucinate\" - it is designed to make s***... I mean, \"stuff\" up. (The idea that this GPT-based technology such as it is would be even capable of producing generalized AI (AGI) is an open question, indeed.)But these tools can still be useful, not just for single-line \"tab\" completions, but now as we see here, in higher level conversations with the programmer. It can help articulate requirements, and write test cases, and help drive CI/CD pipelines. And it will improve in scope and accuracy, and custom AIA models tuned for specific programming domains (e.g. quantum) already exist.[21] This is truly the death and rebirth of computer programming, as we have come to experience it. In 1975, IBM's Fred Brooks published the seminal book \"The Mythical Man-Month\" which described, among other things, the software team which one would want to wrap around a senior technical engineer - a team of as many as 10 specialized professionals to handle the documentation, testing, business communications, and more common technical tasks so your senior contributor (\"the surgeon\") can focus on great ideas and great architecture.[22] But however, in today's DevOps culture, where we expect our senior engineers to be \"full stack\", to do it all, to play all roles, the AI tooling brings back some sanity and reminds that there are tasks best left delegated.2025It's that time in the calendar when everyone offers up their forecasts for the coming year. I'll not wade into that. My only prediction is this - that in 2025, the rate of change in key emerging and difficult to humanly understand (nearly black-box) technologies like AI and quantum computing will continue to accelerate, in many cases, beyond our ability to comprehend or predict. This is the so-called singularity, and it's evolving and emerging during a period also marked by political, military, and economic upheaval. Surf's up. Happy New Year. - andyReferences[0] Photo by Ben Wicks on Unsplash[1] SC24 schedule: https://sc24.conference-program.com/[2] El Capitan hardware overview: https://hpc.llnl.gov/documentation/user-guides/using-el-capitan-systems/hardware-overview[3] \"Breaking the Million-Electron and 1 EFLOP/s Barriers: Biomolecular-Scale Ab Initio Molecular Dynamics Using MP2 Potentials\", https://dl.acm.org/doi/pdf/10.1109/SC41406.2024.00015[4] NVIDIA SC24 superchip press release: https://www.datacenterdynamics.com/en/news/nvidia-announces-new-gb200-nvl4-superchip-at-sc24-but-says-theres-still-value-to-be-found-in-grace-hopper/[5] Tracking CHIPS Act funding: https://www.semiconductors.org/chips-incentives-awards/[6] NVIDIA Grace Hopper architecture: https://developer-blogs.nvidia.com/wp-content/uploads/2022/11/grace-hopper-overview.png[7] \"Exploring quantum use cases for the aerospace industry\", IBM white paper, https://www.ibm.com/thought-leadership/institute-business-value/en-us/report/quantum-aerospace[8] IonQ with NVIDIA SC24 press release: https://ionq.com/news/ionq-to-advance-hybrid-quantum-computing-with-new-chemistry-application-and[9] Microsoft and Atom quantum press releease: https://azure.microsoft.com/en-us/blog/quantum/2024/11/19/microsoft-and-atom-computing-offer-a-commercial-quantum-machine-with-the-largest-number-of-entangled-logical-qubits-on-record/[10] Alice &amp; Bob logical qubit lib press release : https://alice-bob.com/newsroom/logical-qubit-emulator-felis-quantum-cloud-alice-bob/[11] Quantinuum stack press release: https://www.quantinuum.com/blog/announcing-the-launch-of-quantinuum-nexus-our-all-in-one-quantum-computing-platform[12] RPI's experiments with HPC and quantum co-scheduling: https://www.ibm.com/quantum/blog/supercomputing-24[13] \"Combining quantum processors with real-time classical communication\", Nature, Nov 2024, https://www.nature.com/articles/s41586-024-08178-2[14] Algorithm Zoo: https://quantumalgorithmzoo.org/[15] Pattern match example code: https://github.com/agallojr/research-notes/blob/02253900f33d784402f0cd0b3ed4d9d360544605/quantum/src/qiskit/pattern_match.py[16] \"QC — Grover’s algorithm\", J. Hui, https://jonathan-hui.medium.com/qc-grovers-algorithm-cd81e61cf248[17] Codeium Windsurf IDE: https://codeium.com/windsurf[18] Cursor IDE: https://www.cursor.com/[19] Dario Amodei, CEO Anthropic, on Lex Fridman podcast: https://www.youtube.com/watch?v=ugvHCXCOmm4&amp;t=20s&amp;pp=ygULbGV4IGZyaWRtYW4%3D[20] SWE-bench: https://www.swebench.com/[21] IBM Qiskit Code Assistant: https://www.ibm.com/quantum/blog/qiskit-code-assistant[22] \"The Mythical Man-Month\", Fred Brooks, 1975: https://web.eecs.umich.edu/~weimerw/2018-481/readings/mythical-man-month.pdf",
            "content_html": "<header class=\"pt4\"><p style=\"line-height: 1.2; text-align: left;\"><span style=\"font-weight: normal;\"><span style=\"font-family: verdana;\"><span color=\"rgba(255, 255, 255, 0.9)\" face=\"-apple-system, system-ui, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, &quot;Helvetica Neue&quot;, &quot;Fira Sans&quot;, Ubuntu, Oxygen, &quot;Oxygen Sans&quot;, Cantarell, &quot;Droid Sans&quot;, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;, &quot;Lucida Grande&quot;, Helvetica, Arial, sans-serif\" style=\"background-color: #1b1f23; font-size: 16px;\">Hello and happy holidays to all. In this blog installment I'll report back from SuperComputing 2024, offer up a programmer-friendly view of the quantum computing space with a code tour of Grover's algorithm, and share some of my own thoughts on using the latest crop of AI programmer assistant tools.</span><span class=\"white-space-pre\" color=\"rgba(255, 255, 255, 0.9)\" face=\"var(--artdeco-reset-typography-font-family-sans)\"> </span></span></span></p></header><div class=\"relative reader__grid\"><div><div><div class=\"reader-article-content reader-article-content--content-blocks\" dir=\"ltr\"><div class=\"reader-content-blocks-container\" tabindex=\"0\"><div class=\"reader-image-block reader-image-block--resize\"><figure class=\"reader-image-block__figure\"><div class=\"ivm-image-view-model\"><div class=\"ivm-view-attr__img-wrapper\"><img alt=\"\" class=\"ivm-view-attr__img--centered reader-image-block__img evi-image lazy-image ember-view\" id=\"ember869\" src=\"https://media.licdn.com/dms/image/v2/D4E12AQEXGYm_FH40rg/article-inline_image-shrink_1500_2232/article-inline_image-shrink_1500_2232/0/1733613595470?e=1740009600&amp;v=beta&amp;t=27x_IfTeyWu00AhmowXKRB402xwRZI5SLAg3MUm9UGc\" /></div></div><figcaption class=\"reader-image-block__figure-image-caption display-block full-width text-body-small-open t-sans text-align-center t-black--light\"><span style=\"font-family: verdana;\">(Sadly, not this Grover.)</span></figcaption></figure></div><p class=\"ember-view reader-text-block__paragraph\" id=\"ember870\"><span style=\"font-family: verdana;\">It was a pleasant SC24 high performance computing (HPC) conference in November. Having attended in past either in-person (Atlanta this year) or virtual, this year I chose virtual again. The big loss was being unable to troll the enormous vendor hall, but otherwise, webcasts make it much easier to be in two places at one time or to skim topics of passing interest.[1]<span class=\"white-space-pre\" face=\"var(--artdeco-reset-typography-font-family-sans)\"> </span></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember871\"><span style=\"font-family: verdana;\">There's a new top HPC machine (that we know of) - El Capitan, and its powered by AMD, containing about a million CPU cores and about 10 million GPU cores.[2] Molecular dynamics papers presented using a GPU-accelerated exascale computer reminds that, quantum aside for a moment, the real work is still being done in the classical world.[3] NVIDIA showcased the growing fusion of AI and HPC with their \"superchip\" designs - incorporating a CPU and a GPU on the same chip.[4] And why not, the money keeps flowing, the current outgoing federal administration now locking in the CHIPS Act funding before the end of the term.[5]</span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember872\"><br /></p><div class=\"reader-image-block reader-image-block--full-width\"><figure class=\"reader-image-block__figure\"><div class=\"ivm-image-view-model\"><div class=\"ivm-view-attr__img-wrapper\"><img alt=\"\" class=\"ivm-view-attr__img--centered reader-image-block__img evi-image lazy-image ember-view\" id=\"ember873\" src=\"https://media.licdn.com/dms/image/v2/D4E12AQE0Sq5LcwFEAQ/article-inline_image-shrink_1500_2232/article-inline_image-shrink_1500_2232/0/1733610500704?e=1740009600&amp;v=beta&amp;t=Dr2oQXKZQk3AXM8R_LhkmyXIRrnOeaejjn8q97YVy_w\" /></div></div><figcaption class=\"reader-image-block__figure-image-caption display-block full-width text-body-small-open t-sans text-align-center t-black--light\"><span style=\"font-family: verdana;\">NVIDIA's Grace Hopper architecture [6]</span></figcaption></figure></div><p class=\"ember-view reader-text-block__paragraph\" id=\"ember874\"><span style=\"font-family: verdana;\">But beyond the incremental improvements in compute, storage, cooling, power consumption and the like, it seemed to me, through my remote goggles, that the real action was happening on the sidelines of SC24, in the quantum computing space.</span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember874\"><span style=\"font-family: verdana;\"><br /></span></p><h2><span style=\"font-family: verdana;\">Quantum Chip on Shoulder<span class=\"white-space-pre\" face=\"var(--artdeco-reset-typography-font-family-sans)\"> </span></span></h2><p class=\"ember-view reader-text-block__paragraph\" id=\"ember876\"><span style=\"font-family: verdana;\">There's significant skepticism of quantum computing from the HPC community at the moment. Quantum computers today are toys in comparison to HPC, and stakeholders in HPC and classical computing (which would include myself) might wonder aloud \"what's the point?\" For some applications (like the fluid dynamics apps my company uses), quantum utility is perhaps still a decade away.[7]<span class=\"white-space-pre\" face=\"var(--artdeco-reset-typography-font-family-sans)\"> </span></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember877\"><span style=\"font-family: verdana;\">But the groundwork is being laid today, and when we understand that there are useful problems which can be solved on quantum computers, and only on quantum computers, we might at least allow the playing to continue. And we might be surprised at how fast quantum computing is progressing, and also just churning. Even those in the industry are hedging their bets - on which qubit technologies and which companies will be the winners - and as such are changing partners on a regular basis [8,9,10], although this sometimes means needing to unproductively reinvent the wheel (how many Python quantum circuit libraries do we need?)[11]</span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember878\"><span style=\"font-family: verdana;\">A couple of new announcements from Big Blue caught my eye. First, an early demonstrator of incorporating an IBM quantum computer into an HPC data center, unifying the resource scheduling, was shown at RPI with their AiMOS cluster and their first-in-the-nation academic installation.[12] The second, and more important, was the paper in Nature demonstrating the union of two quantum computers via classical networks, providing another avenue for scaling up hybrid quantum computing.[13]<span class=\"white-space-pre\" face=\"var(--artdeco-reset-typography-font-family-sans)\"> </span></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember879\"><span style=\"font-family: verdana;\">But today's QPUs are still noisy, fragile, small, expensive, and scarce. Maturity is still a ways off. QPUs are not fungible - to be successful in executing an application on a quantum computer, we must understand the error profile of that specific device! Not just that brand of quantum computer product, but this instance of that product! We need hands-on examples to grow the personal and team experience with quantum while we await stabilized hardware and the productivity-enabling software abstractions which can only come after industry maturity, and these early (head-butting) experiences.<span class=\"white-space-pre\" face=\"var(--artdeco-reset-typography-font-family-sans)\"> </span></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember879\"><span style=\"font-family: verdana;\"><span class=\"white-space-pre\" face=\"var(--artdeco-reset-typography-font-family-sans)\"><br /></span></span></p><h2><span style=\"font-family: verdana;\">Making Your Quantum Bones</span></h2><p class=\"ember-view reader-text-block__paragraph\" id=\"ember881\"><span style=\"font-family: verdana;\">Today's education in and around quantum computing is still focused on the experimental audience. We are still doing experiments about quantum more than experiments with quantum. The educational material which does exist, and there is an increasing amount, is focused on a community which is very comfortable with quantum physics and its associated mathematics. Most people, myself included, do not fit this description. As a computer science graduate, adjunct prof, and software engineer by trade, I want to see higher level programming abstractions, not those centered around qubits and gates which clearly does not scale for large programs. We will be waiting a while. So in the meantime, we need to see some examples using today's technology and syntax but which are more suited for the Comp Sci student audience, to help begin to bridge that gap.<span class=\"white-space-pre\" face=\"var(--artdeco-reset-typography-font-family-sans)\"> </span></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember882\"><span style=\"font-family: verdana;\">If we visit the Algorithm Zoo [14], a collection of quantum algorithms which show a computational advantage over similar classical approaches, we find some things (but not many) which might look familiar to a CS undergrad, but the implementations, when they exist, are often broken. In this current early phase of the quantum era, vendors are playing free and loose with their SDKs, and releases with significant object model refactorings and breaking changes are the norm.<span class=\"white-space-pre\" face=\"var(--artdeco-reset-typography-font-family-sans)\"> </span></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember883\"><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://github.com/agallojr/research-notes/blob/02253900f33d784402f0cd0b3ed4d9d360544605/quantum/src/qiskit/pattern_match.py\" target=\"_self\"><span style=\"font-family: verdana;\">pattern_match.py</span></a></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember884\"><span style=\"font-family: verdana;\">So I offer here an example of a quantum program which itself likely has a short shelf life. You can find it here.[15, &amp; above] It shows a cooked up example of using Grover's algorithm, which is a quantum algorithm for search in unstructured (e.g. unsorted) data. Classically, \"cuz Murphy's Law\", you might need to walk the entire dataset to find the item of interest (or show its not there) - we call this an O(n) algorithm. Using Grover's algorithm, you can do the search with a strong probability of success in O(sqrt(n)) time - a quadratic speedup, and one worth pursuing for many applications. Note we are only saying we can perform the search with high probability - this is quantum computing, and everything is a probability. And while the example shown isn't necessarily the most common application of Grover, understanding Grover is worthwhile, as it appears as a sub-procedure in many other quantum algorithms.</span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember885\"><span style=\"font-family: verdana;\">Here are a few key points to take away, even if you don't take the time to look at the documented code example:</span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember886\"></p><ul><li><span style=\"font-family: verdana;\">We're going to store a small set of strings - binary strings of 1's and 0's representing a small dataset - into a quantum system of qubits.<span class=\"white-space-pre\" face=\"var(--artdeco-reset-typography-font-family-sans)\"> </span></span></li><li><span style=\"font-family: verdana;\">Since our strings are chosen to be 16 binary digits long, we will use 16 qubits. This is a large enough number of qubits to show some non-trivial problems, but not so many as to not be runnable on a simulator on your laptop. (The performance does not scale linearly with the number of qubits.)</span></li><li><span style=\"font-family: verdana;\">A system of n qubits contains 2**n possible states (combinations of 0's and 1's). That's a lot. We will have far fewer state strings in our dataset - just a handful for this experiment.</span></li><li><span style=\"font-family: verdana;\">After initializing the qubits, we will mark each state in the quantum system which corresponds to a binary string in our dataset. To do this we will use the phase of the qubit, which is an extra useful lever you do not find in classical bits (among other unique quantum advantages).</span></li></ul><p style=\"color: black;\"></p><div class=\"reader-image-block reader-image-block--full-width\"><figure class=\"reader-image-block__figure\"><div class=\"ivm-image-view-model\"><div class=\"ivm-view-attr__img-wrapper\"><img alt=\"\" class=\"ivm-view-attr__img--centered reader-image-block__img evi-image lazy-image ember-view\" id=\"ember887\" src=\"https://media.licdn.com/dms/image/v2/D4E12AQH3qvl7gip82A/article-inline_image-shrink_1500_2232/article-inline_image-shrink_1500_2232/0/1733610781858?e=1740009600&amp;v=beta&amp;t=5rAwLjF3EHyEDVvd1GHOAO65F-dKDYWy8gCvJ27vCF4\" /></div></div><figcaption class=\"reader-image-block__figure-image-caption display-block full-width text-body-small-open t-sans text-align-center t-black--light\"><span style=\"font-family: verdana;\">Three qubits in superposition.[16]</span></figcaption></figure></div><div class=\"reader-image-block reader-image-block--full-width\"><figure class=\"reader-image-block__figure\"><div class=\"ivm-image-view-model\"><div class=\"ivm-view-attr__img-wrapper\"><img alt=\"\" class=\"ivm-view-attr__img--centered reader-image-block__img evi-image lazy-image ember-view\" id=\"ember888\" src=\"https://media.licdn.com/dms/image/v2/D4E12AQGrCHC-cx2zUg/article-inline_image-shrink_1500_2232/article-inline_image-shrink_1500_2232/0/1733610877320?e=1740009600&amp;v=beta&amp;t=lHNGjay8jH78KrwCl9WJwEIHqXuIj4LIX6gviS9nVSM\" /></div></div><figcaption class=\"reader-image-block__figure-image-caption display-block full-width text-body-small-open t-sans text-align-center t-black--light\"><span style=\"font-family: verdana;\">Marking one of the states by flipping its phase.[16]</span></figcaption></figure></div><p class=\"ember-view reader-text-block__paragraph\" id=\"ember889\"></p><ul><li><span style=\"font-family: verdana;\">Using Grover's algorithm, we will amplify the probability of finding the marked states relative to other background states. I.e., the signal is separated from the noise. To do this we iteratively apply an oracle quantum circuit to the initial system.</span></li></ul><p style=\"color: black;\"></p><div class=\"reader-image-block reader-image-block--full-width\"><figure class=\"reader-image-block__figure\"><div class=\"ivm-image-view-model\"><div class=\"ivm-view-attr__img-wrapper\"><img alt=\"\" class=\"ivm-view-attr__img--centered reader-image-block__img evi-image lazy-image ember-view\" id=\"ember890\" src=\"https://media.licdn.com/dms/image/v2/D4E12AQHu1UQVg1dbqQ/article-inline_image-shrink_1500_2232/article-inline_image-shrink_1500_2232/0/1733610938287?e=1740009600&amp;v=beta&amp;t=-vmo3-9uS3FY-kCQiudQ-Ui0eEUyxl0Ao7k45IwSOMg\" /></div></div><figcaption class=\"reader-image-block__figure-image-caption display-block full-width text-body-small-open t-sans text-align-center t-black--light\"><span style=\"font-family: verdana;\">The states, with our target amplified, after some number of iterations of Grover's algorithm.[16]</span></figcaption></figure></div><p class=\"ember-view reader-text-block__paragraph\" id=\"ember891\"></p><ul><li><span style=\"font-family: verdana;\">Note that we use the same 16 qubits to encode all of the 16-digit strings in the dataset. Try that with a classical computer!</span></li><li><span style=\"font-family: verdana;\">We then use Grover's algorithm again using a target string, applying the oracle against the now prepared quantum system, and returning the result that the target either is or is not in the dataset.</span></li></ul><p style=\"color: black;\"></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember892\"><span style=\"font-family: verdana;\">Hopefully this gives some flavor of what its like to program a quantum computer using an example most classical programmers today can understand. There are places where the code can potentially be improved - I welcome your input in the comments section below.</span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember892\"><span style=\"font-family: verdana;\"><br /></span></p><h2><span style=\"font-family: verdana;\">AI AI, Oh.<span class=\"white-space-pre\" face=\"var(--artdeco-reset-typography-font-family-sans)\"> </span></span></h2><p class=\"ember-view reader-text-block__paragraph\" id=\"ember894\"><span style=\"font-family: verdana;\">During these quantum coding explorations, during other coding work, and while writing documents (like this one), I've also been experimenting with a sequence of so-called \"AI assistants\". Starting with Copilot as a plugin for the popular VS Code IDE, I quickly switched to the Codeium plugin which performed better for my purposes, mostly because it took more of my code into context while making suggestions.[17] Since, the various vendors have leap-frogged each other in features and quality of results, and so while this tool comparison may be stale, its also the case that any new comparison would soon itself become stale.<span class=\"white-space-pre\" face=\"var(--artdeco-reset-typography-font-family-sans)\"> </span></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember895\"><span style=\"font-family: verdana;\">While the VS Code IDE provides hooks for plugins, some vendors, most notably Cursor [18] took the approach that to provide a true AI assistant (AIA) for a modern software developer you needed plugin hooks in many places, and control over things like the rendering of change sets. So they forked the VS Code IDE entirely. And they weren't the only ones to do this - Codeium now also has its own IDE called Windsurf - this is what I'm currently using. This can't be end state for the discussion and more leap-frogging is expected, so watch this space. I've now seen cases where people are putting \"Cursor\" on their resume - not a good idea to commit to that, but it is a good idea to start using one of these modern tools in your work, and to better understand how to adopt your work practices in light of their potential.</span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember896\"><span style=\"font-family: verdana;\">In other words, to experiment while these AIA tools improve. The head of Anthropic, a major AI player and the maker of the model driving my current IDE, is proud to say their model reaches 50% success rates on a popular software engineering metric.[19,20] Do<span class=\"white-space-pre\" face=\"var(--artdeco-reset-typography-font-family-sans)\"> </span><span face=\"var(--artdeco-reset-typography-font-family-sans)\">you</span><span class=\"white-space-pre\" face=\"var(--artdeco-reset-typography-font-family-sans)\"> </span>want to pair program with an assistant which is only right 50% of the time? I can tell you that its got its ups and downs.</span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember897\"><span style=\"font-family: verdana;\">On the positive, for mundane tasks, it can do a pretty good job. Questions like \"how do you do X in Python?\" which you might have previously taken to Stack Overflow can now be answered in the IDE with custom-made code samples. You can turn your question into an instruction:<span class=\"white-space-pre\" face=\"var(--artdeco-reset-typography-font-family-sans)\"> </span></span></p><blockquote class=\"ember-view reader-text-block__blockquote\" id=\"ember898\"><span style=\"color: #b6d7a8; font-family: courier;\"><span face=\"var(--artdeco-reset-typography-font-family-sans)\">Me</span>: modify this code to take the target string as a command line argument and default to the current target string if none is provided</span></blockquote><blockquote class=\"ember-view reader-text-block__blockquote\" id=\"ember899\"><span style=\"color: #cc0000; font-family: courier;\"><span face=\"var(--artdeco-reset-typography-font-family-sans)\">AIA</span>: I've modified the code to...</span></blockquote><p class=\"ember-view reader-text-block__paragraph\" id=\"ember900\"><span style=\"font-family: verdana;\">And it will describe in words and code a working solution, albeit in my case one I found to be slightly over-engineered:</span></p><blockquote class=\"ember-view reader-text-block__blockquote\" id=\"ember901\"><span style=\"color: #b6d7a8; font-family: courier;\"><span face=\"var(--artdeco-reset-typography-font-family-sans)\">Me</span>: use sys to determine if a single arg was provided</span></blockquote><blockquote class=\"ember-view reader-text-block__blockquote\" id=\"ember902\"><span style=\"color: #cc0000; font-family: courier;\"><span face=\"var(--artdeco-reset-typography-font-family-sans)\">AIA</span>: I'll modify the code to use sys.argv for a simpler command line argument handling...</span></blockquote><p class=\"ember-view reader-text-block__paragraph\" id=\"ember903\"><span style=\"font-family: verdana;\">In another interaction, we debated if an AIA-proposed solution was mathematically efficient, at one point:</span></p><blockquote class=\"ember-view reader-text-block__blockquote\" id=\"ember904\"><span style=\"color: #cc0000; font-family: courier;\"><span face=\"var(--artdeco-reset-typography-font-family-sans)\">AIA</span>: Would you like me to: 1) Implement a simpler but less efficient version that demonstrates the concept? 2) Or explore how to properly implement the quantum multiplexing needed for efficient database loading?</span></blockquote><blockquote class=\"ember-view reader-text-block__blockquote\" id=\"ember905\"><span style=\"color: #b6d7a8; font-family: courier;\"><span face=\"var(--artdeco-reset-typography-font-family-sans)\">Me</span>: yes, but make it in a new file</span></blockquote><p class=\"ember-view reader-text-block__paragraph\" id=\"ember906\"><span style=\"font-family: verdana;\">And it did. The main reason for asking for a new file rather than change in place was that the AIA is still often wrong! And reviewing a large amount of inline change is hard for the human eye and mind.</span></p><blockquote class=\"ember-view reader-text-block__blockquote\" id=\"ember907\"><span style=\"color: #b6d7a8; font-family: courier;\"><span face=\"var(--artdeco-reset-typography-font-family-sans)\">Me</span>: but now your proposed solution doesn't load the database or even look at it!</span></blockquote><blockquote class=\"ember-view reader-text-block__blockquote\" id=\"ember908\"><span style=\"color: #cc0000; font-family: courier;\"><span face=\"var(--artdeco-reset-typography-font-family-sans)\">AIA</span>: Ah, you're absolutely right again!<span class=\"white-space-pre\" face=\"var(--artdeco-reset-typography-font-family-sans)\"> </span></span></blockquote><p class=\"ember-view reader-text-block__paragraph\" id=\"ember909\"><span style=\"font-family: verdana;\">And it then goes on to suggest further changes, which you might wonder why those changes weren't proposed in the first place? Keep in mind, its only (proudly) right 50% of the time. It is, as we say, \"generative AI\" after all - it doesn't so much \"hallucinate\" - it is designed to make s***... I mean, \"stuff\" up. (The idea that this GPT-based technology such as it is would be even capable of producing generalized AI (AGI) is an open question, indeed.)</span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember910\"><span style=\"font-family: verdana;\">But these tools can still be useful, not just for single-line \"tab\" completions, but now as we see here, in higher level conversations with the programmer. It can help articulate requirements, and write test cases, and help drive CI/CD pipelines. And it will improve in scope and accuracy, and custom AIA models tuned for specific programming domains (e.g. quantum) already exist.[21]<span class=\"white-space-pre\" face=\"var(--artdeco-reset-typography-font-family-sans)\"> </span></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember911\"><span style=\"font-family: verdana;\">This is truly the death and rebirth of computer programming, as we have come to experience it. In 1975, IBM's Fred Brooks published the seminal book \"The Mythical Man-Month\" which described, among other things, the software team which one would want to wrap around a senior technical engineer - a team of as many as 10 specialized professionals to handle the documentation, testing, business communications, and more common technical tasks so your senior contributor (\"the surgeon\") can focus on great ideas and great architecture.[22] But however, in today's DevOps culture, where we expect our senior engineers to be \"full stack\", to do it all, to play all roles, the AI tooling brings back some sanity and reminds that there are tasks best left delegated.</span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember911\"><span style=\"font-family: verdana;\"><br /></span></p><h2><span style=\"font-family: verdana;\">2025</span></h2><p class=\"ember-view reader-text-block__paragraph\" id=\"ember913\"><span style=\"font-family: verdana;\">It's that time in the calendar when everyone offers up their forecasts for the coming year. I'll not wade into that. My only prediction is this - that in 2025, the rate of change in key emerging and difficult to humanly understand (nearly black-box) technologies like AI and quantum computing will continue to accelerate, in many cases, beyond our ability to comprehend or predict. This is the so-called singularity, and it's evolving and emerging during a period also marked by political, military, and economic upheaval.<span class=\"white-space-pre\" face=\"var(--artdeco-reset-typography-font-family-sans)\"> </span></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember914\"><span style=\"font-family: verdana;\">Surf's up. Happy New Year. - andy</span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember915\"><br /></p><h2><span style=\"font-family: verdana;\">References</span></h2><p class=\"ember-view reader-text-block__paragraph\" id=\"ember917\"><span style=\"font-family: verdana;\">[0] Photo by Ben Wicks on Unsplash</span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember918\"><span style=\"font-family: verdana;\">[1] SC24 schedule:<span class=\"white-space-pre\" face=\"var(--artdeco-reset-typography-font-family-sans)\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://sc24.conference-program.com/\" target=\"_self\">https://sc24.conference-program.com/</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember919\"><span style=\"font-family: verdana;\">[2] El Capitan hardware overview:<span class=\"white-space-pre\" face=\"var(--artdeco-reset-typography-font-family-sans)\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://hpc.llnl.gov/documentation/user-guides/using-el-capitan-systems/hardware-overview\" target=\"_self\">https://hpc.llnl.gov/documentation/user-guides/using-el-capitan-systems/hardware-overview</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember920\"><span style=\"font-family: verdana;\">[3] \"Breaking the Million-Electron and 1 EFLOP/s Barriers: Biomolecular-Scale Ab Initio Molecular Dynamics Using MP2 Potentials\",<span class=\"white-space-pre\" face=\"var(--artdeco-reset-typography-font-family-sans)\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://dl.acm.org/doi/pdf/10.1109/SC41406.2024.00015\" target=\"_self\">https://dl.acm.org/doi/pdf/10.1109/SC41406.2024.00015</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember921\"><span style=\"font-family: verdana;\">[4] NVIDIA SC24 superchip press release:<span class=\"white-space-pre\" face=\"var(--artdeco-reset-typography-font-family-sans)\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://www.datacenterdynamics.com/en/news/nvidia-announces-new-gb200-nvl4-superchip-at-sc24-but-says-theres-still-value-to-be-found-in-grace-hopper/\" target=\"_self\">https://www.datacenterdynamics.com/en/news/nvidia-announces-new-gb200-nvl4-superchip-at-sc24-but-says-theres-still-value-to-be-found-in-grace-hopper/</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember922\"><span style=\"font-family: verdana;\">[5] Tracking CHIPS Act funding:<span class=\"white-space-pre\" face=\"var(--artdeco-reset-typography-font-family-sans)\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://www.semiconductors.org/chips-incentives-awards/\" target=\"_self\">https://www.semiconductors.org/chips-incentives-awards/</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember923\"><span style=\"font-family: verdana;\">[6] NVIDIA Grace Hopper architecture:<span class=\"white-space-pre\" face=\"var(--artdeco-reset-typography-font-family-sans)\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://developer-blogs.nvidia.com/wp-content/uploads/2022/11/grace-hopper-overview.png\" target=\"_self\">https://developer-blogs.nvidia.com/wp-content/uploads/2022/11/grace-hopper-overview.png</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember924\"><span style=\"font-family: verdana;\">[7] \"Exploring quantum use cases for the aerospace industry\", IBM white paper,<span class=\"white-space-pre\" face=\"var(--artdeco-reset-typography-font-family-sans)\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://www.ibm.com/thought-leadership/institute-business-value/en-us/report/quantum-aerospace\" target=\"_self\">https://www.ibm.com/thought-leadership/institute-business-value/en-us/report/quantum-aerospace</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember925\"><span style=\"font-family: verdana;\">[8] IonQ with NVIDIA SC24 press release:<span class=\"white-space-pre\" face=\"var(--artdeco-reset-typography-font-family-sans)\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://ionq.com/news/ionq-to-advance-hybrid-quantum-computing-with-new-chemistry-application-and\" target=\"_self\">https://ionq.com/news/ionq-to-advance-hybrid-quantum-computing-with-new-chemistry-application-and</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember926\"><span style=\"font-family: verdana;\">[9] Microsoft and Atom quantum press releease:<span class=\"white-space-pre\" face=\"var(--artdeco-reset-typography-font-family-sans)\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://azure.microsoft.com/en-us/blog/quantum/2024/11/19/microsoft-and-atom-computing-offer-a-commercial-quantum-machine-with-the-largest-number-of-entangled-logical-qubits-on-record/\" target=\"_self\">https://azure.microsoft.com/en-us/blog/quantum/2024/11/19/microsoft-and-atom-computing-offer-a-commercial-quantum-machine-with-the-largest-number-of-entangled-logical-qubits-on-record/</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember927\"><span style=\"font-family: verdana;\">[10] Alice &amp; Bob logical qubit lib press release :<span class=\"white-space-pre\" face=\"var(--artdeco-reset-typography-font-family-sans)\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://alice-bob.com/newsroom/logical-qubit-emulator-felis-quantum-cloud-alice-bob/\" target=\"_self\">https://alice-bob.com/newsroom/logical-qubit-emulator-felis-quantum-cloud-alice-bob/</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember928\"><span style=\"font-family: verdana;\">[11] Quantinuum stack press release:<span class=\"white-space-pre\" face=\"var(--artdeco-reset-typography-font-family-sans)\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://www.quantinuum.com/blog/announcing-the-launch-of-quantinuum-nexus-our-all-in-one-quantum-computing-platform\" target=\"_self\">https://www.quantinuum.com/blog/announcing-the-launch-of-quantinuum-nexus-our-all-in-one-quantum-computing-platform</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember929\"><span style=\"font-family: verdana;\">[12] RPI's experiments with HPC and quantum co-scheduling:<span class=\"white-space-pre\" face=\"var(--artdeco-reset-typography-font-family-sans)\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://www.ibm.com/quantum/blog/supercomputing-24\" target=\"_self\">https://www.ibm.com/quantum/blog/supercomputing-24</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember930\"><span style=\"font-family: verdana;\">[13] \"Combining quantum processors with real-time classical communication\", Nature, Nov 2024,<span class=\"white-space-pre\" face=\"var(--artdeco-reset-typography-font-family-sans)\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://www.nature.com/articles/s41586-024-08178-2\" target=\"_self\">https://www.nature.com/articles/s41586-024-08178-2</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember931\"><span style=\"font-family: verdana;\">[14] Algorithm Zoo:<span class=\"white-space-pre\" face=\"var(--artdeco-reset-typography-font-family-sans)\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://quantumalgorithmzoo.org/\" target=\"_self\">https://quantumalgorithmzoo.org/</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember932\"><span style=\"font-family: verdana;\">[15] Pattern match example code:<span class=\"white-space-pre\" face=\"var(--artdeco-reset-typography-font-family-sans)\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://github.com/agallojr/research-notes/blob/02253900f33d784402f0cd0b3ed4d9d360544605/quantum/src/qiskit/pattern_match.py\" target=\"_self\">https://github.com/agallojr/research-notes/blob/02253900f33d784402f0cd0b3ed4d9d360544605/quantum/src/qiskit/pattern_match.py</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember933\"><span style=\"font-family: verdana;\">[16] \"QC — Grover’s algorithm\", J. Hui,<span class=\"white-space-pre\" face=\"var(--artdeco-reset-typography-font-family-sans)\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://jonathan-hui.medium.com/qc-grovers-algorithm-cd81e61cf248\" target=\"_self\">https://jonathan-hui.medium.com/qc-grovers-algorithm-cd81e61cf248</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember934\"><span style=\"font-family: verdana;\">[17] Codeium Windsurf IDE:<span class=\"white-space-pre\" face=\"var(--artdeco-reset-typography-font-family-sans)\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://codeium.com/windsurf\" target=\"_self\">https://codeium.com/windsurf</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember935\"><span style=\"font-family: verdana;\">[18] Cursor IDE:<span class=\"white-space-pre\" face=\"var(--artdeco-reset-typography-font-family-sans)\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://www.cursor.com/\" target=\"_self\">https://www.cursor.com/</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember936\"><span style=\"font-family: verdana;\">[19] Dario Amodei, CEO Anthropic, on Lex Fridman podcast:<span class=\"white-space-pre\" face=\"var(--artdeco-reset-typography-font-family-sans)\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://www.youtube.com/watch?v=ugvHCXCOmm4&amp;t=20s&amp;pp=ygULbGV4IGZyaWRtYW4%3D\" target=\"_self\">https://www.youtube.com/watch?v=ugvHCXCOmm4&amp;t=20s&amp;pp=ygULbGV4IGZyaWRtYW4%3D</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember937\"><span style=\"font-family: verdana;\">[20] SWE-bench:<span class=\"white-space-pre\" face=\"var(--artdeco-reset-typography-font-family-sans)\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://www.swebench.com/\" target=\"_self\">https://www.swebench.com/</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember938\"><span style=\"font-family: verdana;\">[21] IBM Qiskit Code Assistant:<span class=\"white-space-pre\" face=\"var(--artdeco-reset-typography-font-family-sans)\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://www.ibm.com/quantum/blog/qiskit-code-assistant\" target=\"_self\">https://www.ibm.com/quantum/blog/qiskit-code-assistant</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember939\"><span style=\"font-family: verdana;\">[22] \"The Mythical Man-Month\", Fred Brooks, 1975:<span class=\"white-space-pre\" face=\"var(--artdeco-reset-typography-font-family-sans)\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://web.eecs.umich.edu/~weimerw/2018-481/readings/mythical-man-month.pdf\" target=\"_self\">https://web.eecs.umich.edu/~weimerw/2018-481/readings/mythical-man-month.pdf</a></span></p><br class=\"Apple-interchange-newline\" /></div></div></div></div></div>",
            "url": "https://hpc.social/personal-blog/2024/surfing-the-singularity-super-grover/",
            
            
            
            
            
            "date_published": "2024-12-07T17:00:00-07:00",
            "date_modified": "2024-12-07T17:00:00-07:00",
            
                "author": "Surfing the Singularity"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2024/sc-24-recap/",
            "title": "SC'24 recap",
            "summary": null,
            "content_text": "The premiere annual conference of the high-performance computing community, SC24, was held in Atlanta last week, and    it attracted a record-shattering number of attendees--nearly 18,000 registrants, up 28% from last    year! The conference felt big as well, and there seemed to be a lot more running between sessions, meetings,    and the exhibition floor. Despite its objectively bigger size though, the content of the conference felt more diffuse this year, and I was left wondering if this reflected my own biases or was a real effect of the AI industry    beginning to overflow into AI-adjacent technology conferences like SC.Of course, this isn't to say that SC24 was anything short of a great conference. Some exciting new technologies were    announced, a new supercomputer beat out Frontier to become the fastest supercomputer on the Top500 list, and I got    to catch up with a bunch of great people that I only get to see at shows like this. I'll touch on all of these    things below. But this year felt different from previous SC conferences to me, and I'll try to talk about that too.There's no great way to arrange all the things I jotted down in my notes, but I've tried to arrange them by what readers may be interested in. Here's the table of contents:My approach to SC this yearNew technology and announcementsTop500 and a new #1 system#1 - El Capitan#5 - Eni HPC6#16 and #17 - SoftBank CHIE-2 and CHIE-3#18 - Jülich's JUPITER Exascale Transition Instrument (JETI)#32 - Reindeer!Technology on the exhibit floorGB200Slingshot 400Grace-Grace for storage?Microsoft and AMD's new HBM CPUThe HPC industry overallWhat I learned about the average SC technical program attendeePeople think sustainability and energy efficiency are the same thingAI sessions are really scientific computing sessions about AIAI for operations is not yet real in scientific computingSome are beginning to realize that HPC exists outside of scientific computingNSF's broad front vs. DOE's big bets in HPC and AIExhibitor trendsBooths by the numbersProliferation of GPU-as-a-Service providersCommunity and connectionsGetting to know peopleTalking to early career peopleShift in social mediaSo what's the takeaway?Before getting into the details though, I should explain how my perspective shaped what I noticed (and missed) through the conference. And to be clear: these are my own personal opinions and do not necessarily reflect those of my employer. Although Microsoft covered the cost for me to attend SC, I wrote this blog post during my own free time over the Thanksgiving holiday, and nobody had any editorial control over what follows except me.My approach to SC this yearAlthough this is the eleventh SC conference I've attended, it was the first time that I:attended as a practitioner            of hyperscale AI rather than traditional HPC and scientific computingattended as a Microsoft engineer (I represented Microsoft as a product manager at        SC22 and SC23)did not attend SC as a designated storage person (since 2013)Because of these changes in my identity as an attendee, I approached the    conference with a different set of goals in mind:As a hyperscale/AI person, I felt that I should    prioritize attending all the cloud and AI sessions whenever forced to choose between one session or another. I chose to focus on understanding the traditional HPC community's understanding of hyperscale and AI, which meant I had to spend less time in the workshops, panels and BOFs where I built my career.As an engineer rather than a product manager,    it wasn't my primary responsibility to run private briefings and gather HPC customers' requirements and feedback. Instead, I prioritized only those meetings where my first-hand    knowledge of how massive-scale AI training works could have a meaningful impact. This meant I focused on partners and practitioners who also operate in the realm of            hyperscale--think massive, AI-adjacent companies and the HPC centers who have historically    dominated the very top of the Top500 list.One thing I didn't anticipate going into SC24 is that I've inherited a third identity: there are a new cohort of people in HPC who see me as a long-time community            member. This resulted in a surprising amount of my time being spent talking to students and early career practitioners who were looking    for advice.These three identities and goals meant I don't many notes to share on the technical program, but I did capture more observations about broader trends in the HPC industry and community.New technology and announcementsHPC is all about cutting-edge technology, so that's a fine place to start talking about what was new.Top500 and a new #1 systemA cornerstone of every SC conference is the release of the new Top500 list on Monday, and    this is especially true on years when a new #1 supercomputer is announced. As was widely anticipated in the weeks    leading up to SC24, El Capitan unseated Frontier as the new #1 supercomputer this year, posting an impressive 1.74 EFLOPS of FP64. In addition though, Frontier grew a    little (it added 400 nodes), there was a notable new #5 system (Eni's HPC6), and a number of smaller systems appeared that are worth calling    out.#1 - El CapitanThe highlight of the Top500 list was undoubtedly the debut of El Capitan, Lawrence    Livermore National Laboratory's massive new MI300A-based exascale supercomputer. Its 1.74 EF score resulted from a    105-minute HPL run that came in under 30 MW, and a bunch of technical details about the system were disclosed by    Livermore Computing's CTO, Bronis de Supinski, during an invited talk during the Top500 BOF. Plenty of others    summarize the system's speeds and feeds (e.g., see The        Next Platform's article on El Cap), so I won't do that. However, I will comment on how unusual Bronis' talk    was.Foremost, the El Capitan talk seemed haphazard and last-minute. Considering the system took over half a decade of planning and cost at least half a    billion dollars, El Capitan's unveiling was the most unenthusiastic description of a brand-new #1 supercomputer I've    ever seen. I can understand that the Livermore folks have debuted plenty of novel #1 systems in their careers, but El    Capitan is objectively a fascinating system, and running a full-system job for nearly two hours across first-of-a-kind APUs    is an amazing feat. If community leaders don't get excited about their own groundbreaking achievements, what kind of message should the next generation of HPC professionals take home?In sharp contrast to the blasé announcement of this new system was the leading slide that was presented to describe the speeds and feeds of El Capitan:I've never seen a speaker take the main stage and put a photo of himself literally in the center of the slide, in front of the supercomputer they're talking about. I don't know what the communications people at Livermore were trying to do with this graphic, but I don't think it    was intended to be evocative of the first thing that came to my mind:The supercomputer is literally named \"The Captain,\" and there's a photo of one dude (the boss of Livermore Computing,    who is also standing on stage giving the talk) blocking the view of the machine. It wasn't a great look, and it left me feeling very uneasy about what I was witnessing and what message it was sending to the HPC community.In case it needs to be said, HPC is a team sport. The unveiling of El Capitan (or any other #1 system    before it) is always the product of dozens, if not hundreds, of people devoting years of their professional lives to    ensuring it all comes together. It was a big miss, both to those who put in the work, and those who will have    to put in the work on future systems, to suggest that a single, smiling face comes before the success of the system deployment.#5 - Eni HPC6The other notable entrant to the Top 10 list was HPC6, an industry system deployed by Eni (a major Italian energy    company) built on MI250X. Oil and gas companies tend to be conservative in the systems they buy since the seismic    imaging done on their large supercomputers informs hundred-million to billion-dollar investments in drilling a new    well, and they have much less tolerance for weird architectures than federally funded leadership computing does.    Thus, Eni's adoption of AMD GPUs in this #5 system is a strong endorsement of their capability in mission-critical    commercial computing.#16 and #17 - SoftBank CHIE-2 and CHIE-3SoftBank, the Japanese investment conglomerate who, among other things, owns a significant stake in Arm, made its Top500 debut with two identical 256-node DGX H100 SuperPODs. While    not technologically interesting (H100 is getting old), these systems represent significant investment in HPC by    private industry in Japan and signals that SoftBank is following the lead of large American investment groups in        building private AI clusters for the AI startups in their portfolios. In doing this, SoftBank's investments    aren't dependent on third-party cloud providers to supply the GPUs to make these startups successful and reduces    their overall risk.Although I didn't hear anything about these SoftBank systems at the conference, NVIDIA issued a press statement    during the NVIDIA AI Summit Japan during the week prior to SC24 that discussed SoftBank's        investment in large NVIDIA supercomputers. The press statement states that these systems will be used \"for    [SoftBank's] own generative AI development and AI-related business, as well as that of universities, research    institutions and businesses throughout Japan.\" The release also suggests we can expect B200 and GB200 SuperPODs from    SoftBank to appear as those technologies come online.#18 - Jülich's JUPITER Exascale Transition Instrument (JETI)Just below the SoftBank systems was the precursor system to Europe's first exascale system. I was hoping that    JUPITER, the full exascale system being deployed at FRJ, would appear in the Top 10, but it seems like we'll have to    wait for ISC25 for that. Still, the JETI system ran HPL across 480 nodes of BullSequana XH3000, the same node that    will be used in JUPITER, and achieved 83 TFLOPS. By comparison, the full JUPITER system will be over 10x larger (\"roughly 6000 compute nodes\" in the Booster), and    projecting the JETI run (173 TF/node) out to this full JUPITER scale indicates that JUPITER should just squeak over    the 1.0 EFLOPS line.In preparation for JUPITER, Eviden had a couple of these BullSequana XH3000 nodes out on display this year:And if you're interested in more, I've been tracking the technical details of JUPITER in my digital garden.#32 - Reindeer!Waay down the list was Microsoft's sole new Top500 entry this cycle, an NVIDIA H200 system that ran HPL over 120 ND    H200 v5 nodes in Azure. It was one of only two conventional (non-Grace) H200 clusters that appeared in the top 100,    and it had a pretty good efficiency (Rmax/Rpeak &gt; 80%). Microsoft also had a Reindeer node on display at its    booth:An astute observer may note that this node looks an awful lot like the H100 node used in its Eagle supercomputer,    which was on display at SC23 last year. That's    because it's the same chassis, just with an upgraded HGX baseboard.Reindeer was not super exciting, and there were no press releases about it, but I mention it here for a couple    reasons:One of my teammates did the HPL run and submission, and his group got to come up with the name of the system for        the purposes of HPL. As it turns out, generating a public name for a Top500 submission involves a comical amount        of legal and marketing process when it comes from a giant corporation like Microsoft. And as it turns out,        naming a cluster \"Reindeer\" has a low probability of offending anyone.Reindeer is pretty boring--it's a relatively small cluster with a bunch of GPUs. But when you're building out AI        infrastructure at a pace of 5x Eagles (70,000            GPUs!) per month, you want the clusters that those GPUs go into to be as boring, predictable, and        automatable as possible. Seeing as how Reindeer only used 960 GPUs but still got #32, it doesn't require much        math to realize that the big hyperscalers could flood the Top500 list with these cookie-cutter GPU clusters and        (in this case) make any ranking below #32 completely irrelevant. Heaven help the Top500 list if they ever        publish an API for submitting new systems; cloud providers' build validation automation could tack a Top500        submission on at the end of burn-in and permanently ruin the list.On a personal note, the supercomputer grant that gave me my first job in the HPC business debuted at #48. It's mind-boggling that I now work in a place    where standing up a #32 system is just day-to-day business.Technology on the exhibit floorThe exhibit floor had a few new pieces of HPC technology on display this year that are    worthy of mention, but a lot of the most HPC-centric exciting stuff actually had a soft debut at ISC24 in May. For example, even though SC24 was MI300A's big splash due to    the El Capitan announcement, some MI300A nodes (such as the Cray EX255a) were on display in Hamburg. However,    Eviden had their MI300A node (branded XH3406-3) on display at SC24 which was new to me:I'm unaware of anyone who's actually committed to a large Eviden MI300A system, so I was    surprised to see that Eviden already has a full blade design. But as with Eni's HPC6 supercomputer, perhaps this is    a sign that AMD's GPUs (and now APUs) have graduated from being built-to-order science experiments to a technology    ecosystem that people will want to buy off the rack.There was also a ton of GH200 on the exhibit hall floor, but again, these node types were    also on display at ISC24. This wasn't a surprise since a bunch of upcoming European systems have invested in GH200    already; in addition to JUPITER's 6,000 GH200 nodes described above, CSCS Alps has 2,688 GH200 nodes, and Bristol's Isambard-AI will have 1,362 GH200    nodes. All of these systems will have a 1:1 CPU:GPU ratio and an NVL4 domain, suggesting this is the optimal way to    configure GH200 for HPC workloads. I didn't hear a single mention of GH200 NVL32.GB200SC24 was the debut of NVIDIA's Blackwell GPU in the flesh, and a bunch of integrators had    material on GB200 out at their booths. Interestingly, they all followed the same pattern as GH200 with an NVL4    domain size, and just about every smaller HPC integrator followed a similar pattern wheretheir booth had a standard \"NVIDIA Partner\" (or \"Preferred Partner!\") placard on their main deskthey had a bare NVIDIA GB200 baseboard (superchip) on displaythere wasn't much other differentiationFrom this, I gather that not many companies have manufactured GB200 nodes yet, or if they    have, there aren't enough GB200 boards available to waste them on display models. So, we had to settle for these    bare NVIDIA-manufactured, 4-GPU + 2-CPU superchip boards:What struck me is that these are very large FRUs--if a single component (CPU, GPU, voltage    regulator, DRAM chip, or anything else) goes bad, you have to yank and replace four GPUs and two CPUs. And because    all the components are soldered down, someone's going to have to do a lot of work to remanufacture these boards to    avoid throwing out a lot of very expensive, fully functional Blackwell GPUs.There were a few companies who were further along their GB200 journey and had more    integrated nodes on display. The HPE Cray booth had this GB200 NVL4 blade (the Cray EX154n) on display:It looks remarkably sparse compared to the super-dense blades that normally slot into the    Cray EX line, but even with a single NVL4 node per blade, the Cray EX cabinet only supports 56 of these blades,    leaving 8 blade slots empty in the optimal configuration. I assume this is a limitation of power and cooling.The booth collateral around this blade suggested its use case is \"machine learning and    sovereign AI\" rather than traditional HPC, and that makes sense since each node has 768 GB of HBM3e which is enough    to support training some pretty large sovereign models. However, the choice to force all I/O traffic on to the    high-speed network by only leaving room for one piddly node-local NVMe drive (this blade only supports one SSD per    blade) will make training on this platform very sensitive to the quality of the global storage subsystem. This is    great if you bundle this blade with all-flash Lustre (like Cray ClusterStor) or DAOS (handy, since Intel divested the entire DAOS        development team to HPE). But it's not how I would build an AI-optimized system.I suspect the cost-per-FLOP of this Cray GB200 solution is much lower than what a pure-play    GB200 for LLM training would be. And since GB200 is actually a solid platform for FP64 (thanks to Dan Ernst for challenging me on this and sharing    some great resources on the topic), I expect to see this node do well    in situations that are not training frontier LLMs, but rather fine-tuning LLMs, training smaller models, and mixing    in traditional scientific computing on the same general-purpose HPC/AI system.Speaking of pure-play LLM training platforms, though, I was glad that very few exhibitors    were trying to talk up GB200 NVL72 this year. It may have been the case that vendors simply aren't ready to begin    selling NVL72 yet, but I like to be optimistic and instead believe that the exhibitors who show up to SC24 know that    the scientific computing community likely won't get enough value out of a 72-GPU coherence domain to justify the    additional cost and complexity of NVL72. I didn't see a single vendor with a GB200 NVL36 or NVL72 rack on display    (or a GH200 NVL32, for that matter), and not having to think about NVL72 for the week of SC24 was a nice break from    my day job.Perhaps the closest SC24 got to NVL72 was a joint announcement at the beginning of the week    by Dell and CoreWeave, who announced that they have begun bringing        GB200 NVL72 racks online. Dell did have a massive, AI-focused booth on the exhibit floor, and they did talk    up their high-powered, liquid-cooled rack infrastructure. But in addition to supporting GB200 with NVLink Switches,    I'm sure that rack infrastructure would be equally good at supporting nodes geared more squarely at traditional HPC.Slingshot 400HPE Cray also debuted a new 400G Slingshot switch, appropriately named Slingshot 400. I    didn't get a chance to ask anyone any questions about it, but from the marketing material that came out right before    the conference, it sounds like a serdes upgrade without any significant changes to Slingshot's L2 protocol.There was a Slingshot 400 switch for the Cray EX rack on display at their booth, and it    looked pretty amazing:It looks way more dense than the original 200G Rosetta switch, and it introduces    liquid-cooled optics. If you look closely, you can also see a ton of flyover cables connecting the switch ASIC in    the center to the transceivers near the top; similar flyover cables are showing up in all manner of    ultra-high-performance networking equipment, likely reflecting the inability to maintain signal integrity across PCB    traces.The port density on Slingshot 400 remains the same as it was on 200G Slingshot, so there's    still only 64 ports per switch, and the fabric scale limits don't increase. In addition, the media is saying that    Slingshot 400 (and the GB200 blade that will launch with it) won't start appearing until \"Fall        2025.\" Considering 64-port 800G switches (like NVIDIA's SN5600 and Arista's        7060X6) will have already been on the market by then though, Slingshot 400 will be launching with HPE Cray    on its back foot.However, there was a curious statement on the placard accompanying this Slingshot 400    switch:It reads, \"Ultra Ethernet is the future, HPE Slingshot delivers today!\"Does this suggest that Slingshot 400 is just a stopgap until 800G Ultra Ethernet NICs begin    appearing? If so, I look forward to seeing HPE Cray jam third-party 800G switch ASICs into the Cray EX liquid-cooled    form factor at future SC conferences.Grace-Grace for storage?One of the weirder things I saw on the exhibit floor was a scale-out storage server built    on NVIDIA Grace CPUs that the good folks at WEKA had on display at their booth.Manufactured by Supermicro, this \"ARS-121L-NE316R\" server (really rolls off the tongue)    uses a two-socket Grace superchip and its LPDDR5X instead of conventional, socketed CPUs and DDR. The rest of it    seems like a normal scale-out storage server, with sixteen E3.S SSD slots in the front and four 400G ConnectX-7 or    BlueField-3 NICs in the back. No fancy dual-controller failover or anything like that; the presumption is that    whatever storage system you'd install over this server would implement its own erasure coding across drives and    servers.At a glance, this might seem like a neat idea for a compute-intensive storage system like    WEKA or DAOS. However, one thing that you typically want in a storage server is high reliability and repairability,    features which weren't the optimal design point for these Grace superchips. Specifically,The Grace-Grace superchip turn both CPU sockets into a single FRU. This means that if one CPU goes bad, you're        shipping the whole board back to NVIDIA rather than just doing a field-swap of a socket.Grace uses LPDDR5X, whose ECC is not as robust as DDR5. I'm not an expert on memory architecture, but my        understanding is that the ECC scheme on Grace does not provide ChipKill or row failures. And as with CPU        failure, if a single DRAM chip goes back, you're throwing out two CPUs and all the DRAM.There's no way to value-engineer the exact quantity of cores, clock, and DRAM to be optimal for the storage        software installed on top of these servers.On the upside, though, there might be a cost advantage to using this Grace-Grace server    over a beefier AMD- or Intel-based server with a bunch of traditional DIMMs. And if you really like NVIDIA products,    this lets you do NVIDIA storage servers to go with your NVIDIA network and NVIDIA compute. As long as your storage    software can work with the interrupt rates of such a server (e.g., it supports rebuild-on-read) and the 144 Neoverse    V2 cores are a good fit for its computational requirements (e.g., calculating complex erasure codes), this server    makes sense. But building a parallel storage system on LPDDR5X still gives me the willies.I could also see this thing being useful for certain analytics workloads, especially those    which may be upstream of LLM training. I look forward to hearing about where this turns up in the field.Microsoft and AMD's new HBM CPUThe last bit of new and exciting HPC technology that I noted came from my very own employer    in the form of HBv5, a new, monster four-socket node featuring custom-designed AMD CPUs with HBM. STH wrote up an article with        great photos of HBv5 and its speeds and feeds, but in brief, this single node has:384 physical Zen 4 cores (352 accessible from within the VM) that clock up to 4 GHz512 GB of HBM3 (up to 450 GB accessible from the VM) with up to 6.9 TB/s STREAM bandwidth4x NDR InfiniBand NICs clocked at 200G per port200G Azure Boost NIC (160G accessible from the VM)8x 1.84 TB NVMe SSDs with up to 50 GB/s read and 30 GB/s write bandwidthThe node itself looks kind of wacky as well, because there just isn't a lot on it:There are the obvious four sockets of AMD EPYC 9V64H, each with 96 physical cores and 128 GB of HBM3, and giant heat    pipes on top of them since it's 100% air-cooled. But there's no DDR at all, no power converter board (the node is    powered by a DC bus bar), and just a few flyover cables to connect the PCIe add-in-card cages. There is a separate    fan board with just two pairs of power cables connecting to the motherboard, and that's really about it.The front end of the node shows its I/O capabilities which are similarly uncomplicated:There are four NDR InfiniBand cards (one localized to each socket) which are 400G-capable but cabled up at 200G,    eight E1.S NVMe drives, and a brand-new dual-port Azure Boost 200G NIC. Here's a close-up of the right third of the    node's front:This is the first time I've seen an Azure Boost NIC in a server, and it looksmuch better integrated than the previous-generation 100G Azure SmartNIC that put the FPGA and hard NIC on separateboards connected by a funny little pigtail. This older 100G SmartNIC with pigtail was also on display at the Microsoftbooth in an ND MI300X v5 node:And finally, although I am no expert in this new node, I did hang around the people who are all week, and I    repeatedly heard them answer the same few questions:Is this MI300C? It is if you want it to be. You can call it Sally if you want; I don't think it will        care. But Microsoft calls it HBv5, and the processor name will show up as AMD EPYC 9V64H in /proc/cpuinfo.Is its InfiniBand 1x800 port, 2x400 ports, ...? There are four NDR InfiniBand HCA cards, and each card        has one full 400G NDR InfiniBand port. However, each port is only connected up to top-of-rack switching at 200G.        Each InfiniBand HCA hangs off of a different EPYC 9V64H socket so that any memory address can get to        InfiniBand without having to traverse Infinity Fabric. Running four ports of NDR InfiniBand at half speed is an        unusual configuration, but that's what's going on here.How can I buy this CPU? EPYC 9V64H are \"custom            AMD EPYC processors only available in Azure.\" This means the only way to access it is by provisioning an        HBv5 virtual machine in Azure.Amidst all the unrelenting news about new GPUs optimized for AI workloads, it was nice to see something new and    unique launched squarely for the benefit of traditional scientific computing workloads.The HPC industry overallNew technology announcements are always exciting, but one of the main reasons I attend        SC and ISC is to figure out the broader trends shaping the HPC industry. What concerns are top of mind for the        community, and what blind spots remain open across all the conversations happening during the week? Answering        these questions requires more than just walking the exhibit floor; it involves interpreting the subtext of the        discussions happening at panels and BOF sessions. However, identifying where the industry needs more information        or a clearer picture informs a lot of the public-facing talks and activities in which I participate throughout        the year.What I learned about the average SC technical program attendeeThe biggest realization that I confirmed this week is that the SC conference is not an HPC        conference; it is a scientific computing conference. I sat in a few sessions where the phrase \"HPC    workflows\" was clearly a stand-in for \"scientific workflows,\" and \"performance evaluation\" still really means \"MPI    and OpenMP profiling.\" I found myself listening to ideas or hearing about tools that were intellectually    interesting but ultimately not useful to me because they    were so entrenched in the traditions of applying HPC to scientific computing. Let's talk about a few ways in which    this manifested.People think sustainability and energy efficiency are the same thingTake, for example, the topic of sustainability. There were talks, panels, papers, and BOFs    that touched on the environmental impact of HPC throughout the week, but the vast majority of them really weren't    talking about sustainability at all; they were talking about energy efficiency. These talks often use the following    narrative:Energy use from datacenters is predicted to reach some ridiculous number by 2030We must create more energy-efficient algorithms, processors, and scheduling policiesHere is an idea we tested that reduced the energy consumption without impacting the performance of some        application or workflowSustainability achieved! Success!The problem with this approach is that it declares victory when energy consumption is    reduced. This is a great result if all you care about is spending less money on electricity for your supercomputer,    but it completely misses the much greater issue that the electricity required to power an HPC job is often generated    by burning fossil fuels, and that the carbon emissions that are directly attributable to HPC workloads are    contributing to global climate change. This blind spot was exemplified by this slide, presented during a talk titled    \"Towards Sustainable Post-Exascale Leadership Computing\" at the Sustainable Supercomputing workshop:I've written about        this before and I'll write about it again: FLOPS/Watt and PUE are not    meaningful metrics by themselves when talking about sustainability. A PUE of 1.01 is not helpful if the datacenter    that achieves it relies on burning coal for its power. Conversely, a PUE of 1.5 is not bad if all that electricity    comes from a zero-carbon energy source. The biggest issue that I saw being reinforced at SC this year is that    claims of \"sustainable HPC\" are accompanied by the subtext of \"as long as I can keep doing everything else the way I    always have.\"There were glimmers of hope, though. Maciej Cytowski from Pawsey presented the opening talk    at the Sustainable Supercomputing workshop, and he led with the right thing--he acknowledged that 60% of    the fuel mix that powers Pawsey's supercomputers comes from burning fossil fuels:Rather than patting himself on the back at his low PUE, Dr. Cytowski's described on how    they built their datacenter atop a large aquifer from which they draw water at 21°C and return it at 30°C to avoid    using energy-intensive chillers. To further reduce the carbon impact of this water loop, Pawsey also installed over    200 kW of solar panels on its facility roof to power the water pumps. Given the fact that Pawsey cannot relocate to    somewhere with a higher ratio of zero-carbon energy on account of its need to be physically near the Square    Kilometer Array, Cytowski's talk felt like the most substantive discussion on sustainability in HPC that week.Most other talks and panels on the topic really wanted to equate \"sustainability\" to \"FLOPS    per Watt\" and pretend like where one deploys a supercomputer is not a part of the sustainability discussion. The    reality is that, if the HPC industry wanted to take sustainability seriously, it would talk less about watts and    more about tons of CO2. Seeing as how the average watt of electricity in Tennessee produces 2.75x more carbon than a watt of electricity in Washington,    the actual environmental impact of fine-tuning Slurm scheduling or fiddling with CPU frequencies is meaningless when    compared to the benefits that would be gained by deploying that supercomputer next to a hydroelectric dam instead of    a coal-fired power plant.I say all this because there are parts of the HPC industry (namely, the part in which I work)    who are serious about sustainability. And those conversations go beyond simply building supercomputers in    places where energy is low-carbon (thereby reducing Scope 2 emissions). They    include holding suppliers to high standards on reducing the carbon impact of transporting people and material to    these data centers, reducing the carbon impact of all the excess packaging that accompanies components, and being    accountable for the impact of everything in the data center after it reaches end of life (termed Scope 3 emissions).The HPC community--or more precisely, the scientific computing community--is still married    to the idea that the location of a supercomputer is non-negotiable, and \"sustainability\" is a nice-to-have secondary    goal. I was    hoping that the sessions I attended on sustainability would approach this topic at a level where the    non-scientific HPC world has been living. Unfortunately, the discussion at SC24, which spanned workshops, BOFs, and    Green 500, remains largely stuck on the idea that PUE and FLOPS/Watt are the end-all sustainability metrics. Those    metrics are important, but there are global optimizations that have much greater effects on reducing the    environmental impact of the HPC industry.AI sessions are really scientific computing sessions about AIAnother area where \"HPC\" was revealed to really mean \"scientific computing\" was in the    topic of AI. I sat in on a few BOFs and panels around AI topics to get a feel for where this community is in    adopting AI for science, but again, I found the level of discourse to degrade to generic AI banter despite the best    efforts of panelists and moderators. For example, I sat in the \"Foundational Large Language Models for    High-Performance Computing\" BOF session, and Jeff Vetter very clearly defined what a \"foundational large language    model\" was at the outset so we could have a productive discussion about their applicability in HPC (or, really,    scientific computing):The panelists did a good job of outlining their positions. On the upside, LLMs are good for    performing source code conversion, documenting and validating code, and maximizing continuity in application codes    that get passed around as graduate students come and go. On the downside, they have a difficult time creating    efficient parallel code, and they struggle to debug parallel code. And that's probably where the BOF should have    stopped, because LLMs, as defined at the outset of the session, don't actually have a ton of applicability in    scientific computing. But as soon as the session opened up to audience questions, the session went off the rails.The first question was an extremely basic and nonspecific question: \"Is AI a bubble?\"It's fun to ask provocative questions to a panel of experts. I get it. But the question had    nothing to do with LLMs, any of the position statements presented by panelists, or even HPC or scientific computing.    It turned a BOF on \"LLMs for HPC\" into a BOF that might as well have been titled \"Let's just talk about AI!\" A few    panelists tried to get things back on track by talking about the successes of surrogate models to simulate physical    processes, but this reduced the conversation to a point where \"LLMs\" really meant \"any AI model\" and \"HPC\" really    meant \"scientific simulations.\"Perhaps the most productive statement to come out of that panel was when Rio Yokota    asserted that \"we\" (the scientific community) should not train their own LLMs, because doing so would be    \"unproductive for science.\" But I, as well as anyone who understands the difference between LLMs and \"AI,\" already    knew that. And the people who don't understand the difference between an LLM and a surrogate model probably didn't    pick up on Dr. Yokota's statement, so I suspect the meaning of his contribution was completely lost.Walking out of that BOF (and, frankly, the other AI-themed BOFs and panels I attended), I    was disappointed at how superficial the conversation was. This isn't to say these AI sessions were objectively    bad; rather, I think it reflects the general state of understanding of AI amongst SC attendees. Or perhaps it    reflects the demographic that is drawn to these sorts of sessions. If the SC community is not ready to have a    meaningful discussion about AI in the context of HPC or scientific computing, attending BOFs with like-minded peers    is probably a good place to begin getting immersed.But what became clear to me this past week is that SC BOFs and panels with \"AI\" in their    title aren't really meant for practitioners of AI. They're meant for scientific computing people who are beginning    to dabble in AI.AI for operations is not yet real in scientific computingI was invited to sit on a BOF panel called \"Artificial Intelligence and Machine Learning    for HPC Workload Analysis\" following on a successful BOF in which I participated at ISC24. The broad intent was to    have a discussion around the tools, methods, and neat ideas that HPC practitioners have been using to better    understand workloads, and each of us panelists was tasked with talking about a project or idea we had in applying    AI/ML to improve some aspect of workloads.What emerged from us speakers' lightning talks is that applying AI for operations--in this    case, understanding user workloads--is nascent. Rather than talking about how we use AI to affect how we design or    operate supercomputers, all of us seemed to focus more on how we are collecting data and beginning to analyze that    data using ML techniques. And maybe that's OK, because AI won't ever do anything for workload characterization until    you have a solid grasp of the telemetry you can capture about those workloads in the first place.But when we opened the BOF up to discussion with all attendees, despite having a packed    room, there was very little that the audience had. Our BOF lead, Kadidia Konaté, tried to pull discussion out of the    room from a couple of different fronts by asking what tools people were using, what challenges they were facing, and    things along those lines. However, it seemed to me that the majority of the audience was in that room as spectators;    they didn't know where to start applying AI towards understanding the operations of supercomputers. Folks attended    to find out the art of the possible, not talk about their own challenges.As such, the conversation wound up bubbling back up to the safety of traditional topics in    scientific computing--how is LDMS working out, how do you deal with data storage challenges of collecting telemetry,    and all the usual things that monitoring and telemetry folks worry about. It's easy to talk about the topics you    understand, and just as the LLM conversation reverted back to generic AI for science and the sustainability topic    reverted back to FLOPS/Watt, this topic of AI for operations reverted back to standard telemetry collection.Some are beginning to realize that HPC exists outside of scientific computingDespite the pervasive belief at SC24 that \"HPC\" and \"scientific computing\" are the same thing, there are early signs    that the leaders in the community are coming to terms with the reality that there is now a significant amount of    leadership HPC happening outside the scope of the conference. This was most prominent at the part of the Top500 BOF    where Erich Strohmaier typically discusses trends based on the latest publication of the list.In years past, Dr. Strohmaier's talk was full of statements that strongly implied that, if a supercomputer is not    listed on Top500, it simply does not exist. This year was different though: he acknowledged that El Capitan,    Frontier, and Aurora were \"the three exascale systems we are aware of,\" now being    clear that there is room for exascale systems to exist that simply never ran HPL, or never submitted HPL results to    Top500. He explicitly acknowledged again that China has stopped making any Top500 submissions, and although he    didn't name them outright, he spent a few minutes dancing around \"hyperscalers\" who have been deploying exascale    class systems such as Meta's H100        clusters (2x24K H100), xAI's        Colossus (100K H100), and the full system behind Microsoft's Eagle (14K H100 is a \"tiny fraction\").Strohmaier did an interesting analysis that estimated the total power of the Top500 list's supercomputers so he could    compare it to industry buzz around hyperscalers building gigawatt-sized datacenters:It was a fun analysis where he concluded that there are between 500-600 megawatts of supercomputers on the Top500    list, and after you factor in storage, PUE, and other ancillary power sources, the whole Top500 list sums up to what    hyperscalers are talking about sticking into a single datacenter facility.Although he didn't say it outright, I think the implication here is that the Top500 list is rapidly losing relevance    in the broad HPC market, because a significant amount of the world's supercomputing capacity and capability    are absent from the list. Although specific hyperscale supercomputers (like Meta's, xAI's, and Microsoft's) were not    mentioned outright, their absence from the Top500 list suggests that this list might already be more incomplete than    it is complete--the sum of the FLOPS or power on the Top500 supercomputers may be less than the sum of the giant    supercomputers which are known but not listed. This will only get worse as the AI giants keep building systems every    year while the government is stuck on its 3-5 year procurement cycles.It follows that the meaning of the Top500 is sprinting towards a place where it is not representative of HPC so much    as it is representative of the slice of HPC that serves scientific computing. Erich Strohmaier was clearly    aware of this in his talk this year, and I look forward to seeing how the conversation around the Top500 list    continues to morph as the years go on.NSF's broad front vs. DOE's big bets in HPC and AIMy career was started at an NSF HPC center and built up over my years in the        DOE, so I feel like I owe a debt to the people who provided all the opportunities and mentorship that let me    get to the place of privilege in the hyperscale/AI industry that I now enjoy. As a result, I find myself still    spending a lot of my free time thinking about the role of governments in the changing face of        HPC (as evidenced by my critiques of thinktank reports and federal RFIs...) and trying to bridge the gap    in technical understanding between my old colleagues (in DOE, NSF, and European HPC organizations) and whatever they    call what I work on now (hyperscale AI?).To that end, I found myself doing quite a bit of business development (more on this later) with government    types since I think that is where I can    offer the most impact. I used to be government, and I closely follow the state of their thinking in HPC, but I also    know what's going on inside the hyperscale and AI world. I also have enough context in both areas to draw a line    through all the buzzy AI press releases to demonstrate how the momentum of private-sector investment in AI might    affect the way national HPC    efforts do business. So, I did a lot of talking to both my old colleagues in DOE and their industry partners in an    attempt to help them understand how the hyperscale and AI industry thinks about infrastructure, and what they should    expect in the next year.More importantly though, I also sat in on a couple of NSF-themed BOFs to get a better understanding of where their    thinking is, where NAIRR is going, how the NSF's strategy contrasts with DOE's strategy, and where the ambitions of    the Office of Advanced Cyberinfrastructure might intersect with the trajectory of hyperscale AI.What I learned was that NSF leadership is aware of everything that the community should be concerned about: the    growth of data, the increasing need for specialized silicon, the incursion of AI into scientific computing, new    business models and relationships with industry, and broadening the reach of HPC investments to be globally    competitive. But beyond that, I struggled to see a cohesive vision for the future of NSF-funded    supercomputing. A BOF with a broad range of stakeholders probably isn't the best place to lay out a vision for the future of NSF's    HPC efforts, and perhaps NSF's vision is best expressed through its funding opportunities and awards. Whichever the    case may be, it seems like the NSF remains on a path to make incremental progress on a broad front of topics. Its    Advanced Computing Systems and Services (ACSS) program will continue to fund the acquisition of newer    supercomputers, and a smorgasbord of other research programs will continue funding efforts across public access to    open science, cybersecurity, sustainable software, and other areas. My biggest concern is that peanut-buttering    funding across such a broad portfolio will make net forward progress much slower than taking big bets. Perhaps big    bets just aren't in the NSF's mission though.NAIRR was also a topic that came up in every NSF-themed session I attended, but again, I didn't get a clear picture    of the future. Most of the discussion that I heard was around socializing the resources that are available today    through NAIRR, suggesting that the pilot's biggest issue is not a lack of HPC resources donated by industry, but    awareness that NAIRR is a resource that researchers can use. This was reinforced by a survey whose results were    presented in the NAIRR BOF:It seems like the biggest challenges facing the NSF community relying on NAIRR (which has its own sample bias) is    that they don't really know where to start even though they have AI resources (both GPUs and model API services) at    their disposal. In a sense, this is a great position for the NSF sinceits users need intellectual help more than access to GPU resources, and the NSF has been great at promoting        education, training, and workforce development.its users are unlikely to demand the same cutting-edge GPUs that AI industry leaders are snapping up. For        example, the largest pool of GPUs in NAIRR are A100 GPUs that NVIDIA donated via DGX Cloud; the big AI        companies moved off of Ampere a year ago and are about to move off of Hopper.However, it also means that there's not a clear role for partnership with many industry players beyond donating    resources to the NAIRR pilot today in the hopes of selling resources to the full NAIRR tomorrow. I asked what OAC    leadership thought about moving beyond such a transactional relationship between NSF and industry at one of the BOFs    I attended, and while the panelists were eager to explore specific answers to that question, I didn't hear any ideas    that would approach some sort of truly equitable partnership where both parties contributed in-kind.I also walked away from these NSF sessions struck by how different the NSF HPC community's culture is from that of    the DOE. NSF BOF attendees seemed focused on getting answers and guidance from NSF leadership, unlike the typical    DOE gathering, where discussions often revolve around attendees trying to shape priorities to align with their own    agendas. A room full of DOE people tends to feel like everyone thinks they're the smartest person there, while NSF    gatherings appear more diverse in the expertise and areas of depth of its constituents. Neither way is inherently    better or worse, but it will make the full ambition of NAIRR (as an inter-agency collaboration) challenging to    navigate. This is particularly relevant as DOE is now pursuing its own multi-billion-dollar AI infrastructure    effort, FASST, that appears to sidestep NAIRR.Exhibitor trendsThere's no better way to figure out what's going on in the HPC industry than walking the    exhibit floor each year, because booths cost money and reflect the priorities (and budgets) of all participants.    This year's exhibit felt physically huge, and walking from one end to the other was an adventure. You can get a    sense of the scale from this photo I took during the opening gala:Despite having almost 18,000 registrants and the opening gala usually being acrush of people, the gala this year felt and looked very sparse just because people and booths were more spread out.There was also a perceptibly larger number of splashy vendors who have historically never attended before who werepromoting downstream HPC technologies like data center cooling and electrical distribution, and there was healthyspeculation online about whether the hugeness of the exhibit this year was due to these new power and cooling companies.To put these questions to rest, I figured out how to yank down all the exhibitor metadata    from the conference website so I could do some basic analysis on it.Booths by the numbersThe easiest way to find the biggest companies to appear this year was to compare the    exhibitor list and booth sizes from SC23 to this year and see whose booth went from zero to some big square footage.I only took the top twenty new vendors, but they broadly fall into a couple of categories:Power and cooling: Stulz, Delta, Airedale, Valvoline, Boundary Electric, Schneider Electric, Mara        Server manufacturing: Wistron, AMI, PegatronHigher ed: Tennessee Tech, SCRCCThere were a couple other companies that must've just missed last SC but aren't new to        the show (NetApp, Ansys, Samsung, Micron, Broadcom). And curiously, only one new GPU-as-a-Service provider        (Nebius) showed up this year, suggesting last year was the year of the GPU Cloud.But to confirm what others had speculated: yes, a significant amount of the new square        footage of the exhibit floor can be attributed to companies focused on power and cooling. This is an interesting        indicator that HPC is becoming mainstream, largely thanks to AI demanding ultra-high density of power and        cooling. But it's also heartening to see a few new exhibitors in higher education making an appearance. Notably,        SCRCC (South Carolina Research Computing Consortium) is a consortium between Clemon, University of South        Carolina, and Savannah River National Laboratory that just formed last year, and I look forward to seeing what        their combined forces can bring to bear.We can also take a look at whose booths grew the most compared to SC23:This distribution is much more interesting, since the top 20 exhibitors who grew their footprint comprise the        majority of the growth in existing exhibitors. Cherry-picking a few interesting growers:Power and cooling: USystems, Midas, VertivData center/GPUaaS: iM, Iris Energy, and (arguably) OracleSoftware: Arc Compute and CIQCompanies facing serious financial or legal troubles: I count at least three! Impressive that they            are still pouring money into their SC booths.It's also interesting to see HLRS, the German national HPC center, grow so        significantly. I'm not sure what prompted such a great expansion, but I take it to mean that things have been        going well there.Finally, Dell had a massive booth and showing this year. Not only did they grow the        most since SC23, but they had the single largest booth on the exhibit floor at SC24. This was no doubt a result        of their great successes in partnering with NVIDIA to land massive GPU buildout deals at places like xAI and CoreWeave.        They also had \"AI factory\" messaging emblazoned all over their marketing material and debuted a nice 200 kW        liquid-cooled rack that will be the basis for their GB200 NVL72 solution, clearly leaning into the idea that        they are leaders in AI infrastructure. Despite this messaging being off-beat for the SC audience as I've        described earlier, their booth was surprisingly full all the time, and I didn't actually get a chance to get in        there to talk to anyone about what they've been doing.Equally interesting are the vendors who reduced their footprint at SC24 relative to        SC23:Reading too much into any of these big shrinkers is pretty easy; while a reduction in        booth size could suggest business hasn't been as good, it could equally mean that an exhibitor just went        overboard at SC23 and downsized to correct this year. A few noteworthy exhibitors to call out:Penguin and the Korea Semiconductor Industry Association both cut way back from massive 50x50 booths to            30x30. Their booths this year were both big, but they weren't massive. Viridien, formerly known as CGG, also            shrunk from a massive booth to a less-massive 30x40.Juniper still kept an independent booth, but it is in the process                of being absorbed into HPE. Shrinking makes sense.Major cloud providers Google and AWS scaled back, but Microsoft did not.GPU-as-a-Service cloud providers CoreWeave and Lambda both scaled back. Since these GPUaaS providers'            business models typically rely on courting few big customers, it may make sense to cut back on booth volume.        Major AI storage companies DDN, VAST, and (to a lesser degree) Pure also scaled back, while WEKA did not. I            know business for DDN and VAST has been great this past year, so these may just reflect having gone            overboard last year.Overall, almost twice as many vendors grew their booths than scaled back, so I'd        caution anyone against trying to interpret any of this as anything beyond exhibitors right-sizing their booths        after going all-in last year.Finally, there are a handful of vendors who disappeared outright after SC23:It is critical to point out that the largest booths to vanish outright were all on the        smaller size: SUSE, Tenstorrent, and Symbiosys Alliance all disappeared this year, but their booths last year        were only 20x30. I was surprised to see that Tenstorrent and Arm didn't have booths, but the others are either        companies I haven't heard of (suggesting the return on investment of showing at SC might've been low), are easy        to rationalize as only being HPC-adjacent (such as SNIA and DigitalOcean), or simply went bankrupt in the last        year.As we say at the business factory, the net-net of the exhibit hall this year is that        the square footage of booth space increased by 15,000 square feet, so it was in fact bigger, it did take longer        to walk from one end to the other, and there definitely were a bunch of new power and cooling companies filling        out the space. Some exhibitors shrank or vanished, but the industry as a whole appears to be moving in a healthy        direction.And if you're interested in analyzing this data more yourself, please have a look at the data and the Jupyter notebook I used to generate            the above treemaps on GitHub. If you discover anything interesting, please write about it and post it        online!Proliferation of GPU-as-a-Service providersAs an AI infrastructure person working for a major cloud provider, I kept an eye out for all the companies trying        to get into the GPU-as-a-Service game. I described these players last year as            \"pure-play GPU clouds,\" and it seems like the number of options available to customers who want to go        this route is growing. But I found it telling that a lot of them had booths that were completely        indistinguishable from each other. Here's an example of one:As best I can tell, these companies are all NVIDIA preferred partners with    data centers and a willingness to deploy NVIDIA GPUs, NVIDIA SmartNICs, and NVIDIA cloud stack, and sell multi-year    commitments to consume those GPUs. I tried to accost some of these companies' booth staff to ask them my favorite    question (\"What makes you different from everyone else?\"), but most of these companies' booths were staffed by    people more interested in talking to each other than me.These GPUaaS providers tend to freak me out, because, as Microsoft's CEO recently stated, these companies are        often \"just a bunch of            tech companies still using VC money to buy a bunch of GPUs.\" I can't help but feel like this is where        the AI hype will come back to bite companies who have chosen to build houses upon sand. Walking the SC24 exhibit        floor is admittedly a very narrow view of this line of business, but it seemed like some of these companies were        content to buy up huge booths, hang a pretty banner above it, and otherwise leave the booth empty of anything        beyond a few chairs and some generic value propositions. I didn't feel a lot of hunger or enthusiasm from these        companies despite the fact that a bunch of them have hundreds of millions of dollars of GPUs effectively sitting        on credit cards that they are going to have to make payments on for the next five years.That all said, not all the companies in the GPUaaS are kicking back and letting the money pour in. In particular,        I spent a few minutes chatting up someone at the CoreWeave booth, and I was surprised to hear about how much        innovation they're adding on top of their conventional GPUaaS offering. For example, they developed Slurm on Kubernetes            (SUNK) with one of their key customers to close the gap between the fact that CoreWeave exposes its GPU        service through Kubernetes, but many AI customers have built their stack around Slurm, pyxis, and enroot.    In a weird twist of fate, I later ran into an old acquaintance who turned out to be one of the key CoreWeave        customers for whom SUNK was developed. He commented that SUNK is the real deal and does exactly what his users        need which, given the high standards that this person has historically had, is a strong affirmation that SUNK is        more than just toy software that was developed and thrown on to GitHub for an easy press release. CoreWeave is        also developing some interesting high-performance object storage caching software, and all of these software        services are provided at no cost above whatever customers are already paying for their GPU service.I bring this up because it highlights an emerging distinction in the GPUaaS market, which used to be a homogenous        sea of bitcoin-turned-AI providers. Of course, many companies still rely on that simple business model: holding        the bill for rapidly depreciating GPUs that NVIDIA sells and AI startups consume. However, there are now GPUaaS        providers moving up the value chain by taking on the automation and engineering challenges that model developers        don't want to deal with. Investing in uncertain projects like new software or diverse technology stacks is        certainly risky, especially since they may never result in enough revenue to pay for themselves. But having a        strong point of view, taking a stance, and investing in projects that you feel are right deserves recognition.        My hat is off to the GPUaaS providers who are willing to take these risks and raise the tide for all of us        rather than simply sling NVIDIA GPUs to anyone with a bag of money.Community and connectionsAs much as I enjoy increasing shareholder value, the part of SC that gives me the    greatest joy is reconnecting with the HPC community. Knowing I'll get to chat with my favorite people in the    industry (and meet some new favorite people!) makes the long plane rides, upper respiratory infections, and weird    hotel rooms completely worth it.I wound up averaging under six hours of sleep per night this year in large part because 9pm    or 7am were often the only free times I had to meet with people I really wanted to see. I have this unhealthy    mindset where every hour of every day, from the day I land to the day I leave, is too precious to waste, and it's    far too easy for me to rationalize that spending an hour talking to someone interesting is worth losing an hour of    sleep.But like I said at the outset of this blog post, this year felt different for a few    reasons, and a lot of them revolve around the fact that I think I'm getting old. Now, it's always fun to say \"I'm    getting old\" in a mostly braggadocious way, but this feeling manifested in concrete ways that affected the way I    experienced the conference:I hit my limit on Monday night and couldn't get home without spending 15 minutes sitting in an unlit playground        across from the World of Coke. I've always gotten blisters and fatigue, but this was the first time I couldn't        just cowboy up and muscle through it. To avoid a repeat of this, I wound up \"wasting\" (see above) a lot more        time to just get off my feet this year.This year, I reached the point where I need to start time-box how much time I spend chatting up the folks I        bump into. I used to just let the good times roll if I ran into someone I knew, but this year I wound up        spending as much time attending sessions as I did missing sessions because I got caught up in a conversation.        This isn't a bad thing per se, but I did feel a little sour when I realized I'd made a bad bet on choosing to        chat instead of attending a session or vice versa, and this bad feeling lingered in the back of my mind just        about every day.There weren't a lot of surprises for me at the conference this year, and I worry that I am at risk of losing        touch with the technical aspects of the conference that get newer attendees excited. Instead of hearing about,        say, the latest research in interconnects, more of my time was spent mucking it up with the sorts of people in        the HPC community who I used to find intimidating. On the one hand, hooray me for making it into old boys'        clubs. But on the other, I don't want to become some HPC greybeard whose last meaningful contribution to the        industry was twenty years ago.This is the first year where I've had people accost me and ask me for advice. I've long been accosted by        strangers because of my online presence, but those interactions were always lighthearted exchanges of \"I follow        you on Twitter\" and \"Great to meet you. Have an @HPC_Guru pin.\" This year, I had people specifically ask me for        advice on industry versus postdoc, AI versus HPC, and what my master plan was when I left NERSC. Even though I        didn't have any sage advice, I still found it really hard to tell bright-eyed students to go kick rocks just so        I wouldn't be late for yet another mushy panel on AI.If you read this all and think \"boo hoo, poor Glenn is too popular and wise for his own    good,\" yeah, I get it. There are worse problems to have. But this was the first year where I felt like what I put    into the conference was greater than what I got out of it. Presenting at SC used to be at least as good for my    career as it was useful for my audiences, but it just doesn't count for much given my current role and career stage.    It felt like some of the magic was gone this year in a way I've never experienced before. Getting to know peopleAs the years have gone on, I spend an increasing amount of my week having one-on-one    conversations instead of wandering aimlessly. This year though, I came to SC without really having anything to buy    or sell:I am not a researcher, so I don't need to pump up the work I'm doing to impress my fellow researchers.I no longer own a product market segment, so I don't directly influence the customers or vendors with whom my        employer works.I don't have any bandwidth in my day job to support any new customers or partnerships, so I don't have a strong        reason to sell people on partnering with me or my employer. Much to my surprise though, a bunch of my old vendor/partner colleagues still wanted to get    together to chat this year. Reflecting back, I was surprised to realize that it was these conversations--not the    ones about business--that were the most fulfilling this year.I learned about people's hobbies, families, and their philosophies on life, and it was    amazing to get to know some of the people behind the companies with whom I've long dealt. I was reminded that the    person is rarely the same as the company, and even behind some of the most aggressive and blusterous tech companies    are often normal people with the same concerns and moments of self-doubt that everyone else has. I was also reminded    that good engineers appreciate good engineering regardless of whether it's coming from a competitor or not. The    public persona of a tech exec may not openly admire a competitor's product, but that doesn't mean they don't know    good work when they see it.I also surprised a colleague whose career has been in the DOE labs with an anecdote that    amounted to the following: even though two companies may be in fierce competition, the people who work for them    don't have to be. The HPC community is small enough that almost everyone has got a pal at a competing company, and    when there are deals to be made, people looove to gossip. If one salesperson hears a juicy rumor about a prospective    customer, odds are that everyone else on the market will hear about it pretty quickly too. Of course, the boundaries    of confidentiality and professionalism are respected when it matters, but the interpersonal relationships that are    formed between coworkers and friends don't suddenly disappear when people change jobs.And so, I guess it would make sense that people still want to talk to me even though I have    nothing to buy or sell. I love trading gossip just as much as everyone else, and I really enjoyed this aspect of the    week.Talking to early career peopleI also spent an atypically significant amount of my week talking to early career people in    HPC who knew of me one way or another and wanted career advice. This is the first year I recall having the same    career conversations with multiple people, and this new phase of my life was perhaps most apparent during the IEEE    TCHPC/TCPP HPCSC career panel in which I was invited to speak this year.It was an honor to be asked to present on a career panel, but I didn't feel very qualified to give career advice to    up-and-coming computer science graduate students who want to pursue HPC. I am neither a computer scientist nor a    researcher, but fortunately for me, my distinguished co-panelists (Drs. Dewi Yokelson, Olga Pearce, YJ Ji, and    Rabab Alomairy) had plenty of more relevant wisdom to share. And at the end of the panel, there were a few things we    all seemed to agree on as good advice:Knowing stuff is good, but being able to learn things is better. Being eager to learn and naturally curious        makes this much easier as well.The life of a researcher sometimes requires more than working a standard nine-to-five, so it'll be hard to be        really successful if your heart isn't in it.People will forget what you did or what you said,            but they remember how you made them feel. Don't be a jerk, because this community is small.In both this panel the one-on-one conversations I had with early career individuals, the best I could offer was the    truth: I never had a master plan that got me to where I am; I just try out new things until I realize I don't like    doing them anymore. I never knew what I wanted to be when I grew up, and I still don't really, so it now makes me    nervous that people have started approaching me with the assumption that I've got it all figured out. Unless I    torpedo my career and go live on a goat farm though, maybe I should prepare for this to be a significant part of my    SC experiences going forward.Shift in social mediaOne last, big change in the community aspect of SC this year was the mass-migration of a ton of HPC folks from    Twitter to Bluesky during the week prior to the conference. I don't really understand what prompted it so suddenly;    a few of us have been trying for years to get some kind of momentum on other social platforms like Mastodon, but the    general lack of engagement meant that all the excitement around SC always wound up exclusively on Twitter. This year    was different though, and Bluesky hit critical mass with the HPC community.I personally have never experienced an SC conference without Twitter; my first SC was in 2013, and part of what made    that first conference so exciting was being able to pull up my phone and see what other people were seeing,    thinking, and doing across the entire convention center via Twitter. Having the social media component to the    conference made me feel like I was a part of something that first year, and as the years went on, Twitter became an    increasingly indispensable part of the complete SC experience for me.This year, though, I decided to try an        experiment and see what SC would be like if I set Twitter aside and invested my time into Bluesky instead.The verdict? It was actually pretty nice.It felt a lot like the SC13 days, where my day ended and began with me popping open Bluesky to see what new #SC24 posts were made. And because many of the tech companies and HPC    centers hadn't yet made it over, the hashtag wasn't clogged up by a bunch of prescheduled marketing blasts that    buried the posts written by regular old conference attendees who were asking important questions:Which booths at #sc24 have coffee? I noticed oracle do. Anyone else?— Mike Croucher (@walkingrandomly.bsky.social) November 18, 2024 at 3:02 PMOf course, I still clogged Bluesky up with my nonsense during the week, but there was an amazing amount of    engagement by a diversity of thoughtful people--many who came from Twitter, but some whose names and handles I    didn't recognize.The volume of traffic on Bluesky during the week did feel a little lower than what it had been on Twitter in years    past though. I also didn't see as many live posts of technical sessions as they happened, so I couldn't really tell    whether I was missing something interesting in real time. This may have contributed to why I felt a little less    connected to the pulse of the conference this year than I had in the past. It also could've been the fact that    conference was physically smeared out across a massive space though; the sparsity of the convention center was at    least on par with the sparsity on Bluesky.At the end of the week, I didn't regret the experiment. In fact, I'll probably be putting more effort into my Bluesky    account than my Twitter account going forward. To be clear though, this isn't a particularly political decision on    my part, and I pass no judgment on anyone who wants to use one platform over the other. It's just that I like the    way I feel when I scroll through my Bluesky feeds, and I don't get that same feeling when I use Twitter.So what's the takeaway?SC this year was a great conference by almost every measure, as it always is, but it still felt a little different for me. I'm sure that some of that feeling is the result of my own growth, and my role with respect to the conference seems to be evolving from someone who gets a lot out of the conference to someone who is giving more to the conference. That's not to say that I don't get a lot out of it, though; I had no shortage of wonderful interactions with everyone from technology executives to rising stars who are early in their career, and I learned a lot about both them and me as whole people. But SC24, more than any SC before it, is when I realized this change was happening.On the technological front, we saw the debut of a new #1 system (emblazoned with the smiling face of Bronis...) and a growing crop of massive, new clusters deployed for commercial applications. The exhibit floor was quantitatively bigger, in large part due to new power and cooling companies who are suddenly relevant to the HPC world thanks to the momentum of AI. At the same time, the SC technical program is clearly separating itself out as a conference focused on scientific computing; the level of discourse around AI remains largely superficial compared to true AI conferences, the role of hyperscalers in the HPC industry is still cast more as a threat than an opportunity.For my part, I'm still trying to get a grasp on where government agencies like DOE and NSF want to take their AI ambitions so I can try to help build a better mutual understanding between the scientific computing community and the hyperscale AI community. However, it seems like the NSF is progressing slowly on a wide front, while the DOE is doing what DOE does and charging headfirst into a landscape that has changed more than I think they realize.There's a lot of technical content that I know I missed on account of the increasing time I've been spending on the people and community aspect of the conference, and I'm coming to terms with the idea that this just may be the way SC is from now on. And I think I'm okay with that, since the support of the community is what helped me go from being a bored materials science student into someone whose HPC career advice is worth soliciting in the short span of eleven years. Despite any or all of the cynicism that may come out in the things I say about this conference, SC is always the highlight of my year. I always go into it with excitement, gladly burn the candle at both ends all week, and fly home feeling both grateful for and humbled by everything the HPC community has done and continues to do to keep getting me out of bed in the morning.",
            "content_html": "<p>The premiere annual conference of the high-performance computing community, SC24, was held in Atlanta last week, and    it attracted a record-shattering number of attendees--<a href=\"https://www.hpcwire.com/2024/11/20/sc24-half-way-there/\">nearly 18,000 registrants</a>, up 28% from last    year! The conference <i>felt</i> big as well, and there seemed to be a lot more running between sessions, meetings,    and the exhibition floor. Despite its objectively bigger size though, the content of the conference felt more diffuse this year, and I was left wondering if this reflected my own biases or was a real effect of the AI industry    beginning to overflow into AI-adjacent technology conferences like SC.</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure></figure></div><p></p><p>Of course, this isn't to say that SC24 was anything short of a great conference. Some exciting new technologies were    announced, a new supercomputer beat out Frontier to become the fastest supercomputer on the Top500 list, and I got    to catch up with a bunch of great people that I only get to see at shows like this. I'll touch on all of these    things below. But this year felt different from previous SC conferences to me, and I'll try to talk about that too.</p><p>There's no great way to arrange all the things I jotted down in my notes, but I've tried to arrange them by what readers may be interested in. Here's the table of contents:</p><p></p><ol><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#approach\">My approach to SC this year</a></li><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#tech\">New technology and announcements</a><ol><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#tech-top500\">Top500 and a new #1 system</a><ol><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#tech-top500-elcap\">#1 - El Capitan</a></li><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#tech-top500-hpc6\">#5 - Eni HPC6</a></li><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#tech-top500-softbank\">#16 and #17 - SoftBank CHIE-2 and CHIE-3</a></li><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#tech-top500-jeti\">#18 - Jülich's JUPITER Exascale Transition Instrument (JETI)</a></li><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#tech-top500-reindeer\">#32 - Reindeer!</a></li></ol></li><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#tech-expo\">Technology on the exhibit floor</a><ol><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#tech-expo-gb200\">GB200</a></li><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#tech-expo-ss400\">Slingshot 400</a></li><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#tech-expo-gg\">Grace-Grace for storage?</a></li><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#tech-expo-hbv5\">Microsoft and AMD's new HBM CPU</a></li></ol></li></ol></li><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#industry\">The HPC industry overall</a><ol><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#industry-attendee\">What I learned about the average SC technical program attendee</a><ol><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#industry-attendee-sustainability\">People think sustainability and energy efficiency are the same thing</a></li><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#industry-attendee-ai\">AI sessions are really scientific computing sessions about AI</a></li><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#industry-attendee-ops\">AI for operations is not yet real in scientific computing</a></li><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#industry-attendee-hyperscale\">Some are beginning to realize that HPC exists outside of scientific computing</a></li></ol></li><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#industry-nsf\">NSF's broad front vs. DOE's big bets in HPC and AI</a></li><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#industry-expo\">Exhibitor trends</a><ol><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#industry-expo-booths\">Booths by the numbers</a></li><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#industry-expo-gpuaas\">Proliferation of GPU-as-a-Service providers</a></li></ol></li></ol></li><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#community\">Community and connections</a><ol><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#community-people\">Getting to know people</a></li><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#community-career\">Talking to early career people</a></li><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#community-bsky\">Shift in social media</a></li></ol></li><li><a href=\"https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#conclusion\">So what's the takeaway?</a></li></ol><p>Before getting into the details though, I should explain how my perspective shaped what I noticed (and missed) through the conference. And to be clear: <b><i><span style=\"color: #cc0000;\">these are my own personal opinions and do not necessarily reflect those of my employer</span></i></b>. Although Microsoft covered the cost for me to attend SC, I wrote this blog post during my own free time over the Thanksgiving holiday, and nobody had any editorial control over what follows except me.</p><p></p><h2 id=\"approach\">My approach to SC this year</h2><p>Although this is the eleventh SC conference I've attended, it was the first time that I:</p><p></p><ol><li>attended as a <a href=\"https://blog.glennklockwood.com/2024/08/how-has-life-after-leaving-labs-been.html#hpc-ai-development\">practitioner            of hyperscale AI</a> rather than traditional HPC and scientific computing</li><li>attended as a Microsoft engineer (I represented Microsoft as a <a href=\"https://blog.glennklockwood.com/2024/08/how-has-life-after-leaving-labs-been.html#storage-product-management\">product manager</a> at        SC22 and SC23)</li><li>did not attend SC as a designated storage person (since 2013)</li></ol><p>Because of these changes in my <b><span style=\"color: #990000;\">identity</span></b> as an attendee, I approached the    conference with a different set of <b><span style=\"color: #0b5394;\">goals</span></b> in mind:</p><p>As a <b><span style=\"color: #990000;\">hyperscale/AI person</span></b>, I felt that I should    prioritize <b><span style=\"color: #0b5394;\">attending all the cloud and AI sessions</span></b> whenever forced to choose between one session or another. I chose to focus on understanding the traditional HPC community's understanding of hyperscale and AI, which meant I had to spend less time in the workshops, panels and BOFs where I built my career.</p><p>As an <b><span style=\"color: #990000;\">engineer</span></b> rather than a product manager,    it wasn't my primary responsibility to run private briefings and gather HPC customers' requirements and feedback. Instead, I prioritized only those meetings where my first-hand    knowledge of how massive-scale AI training works could have a meaningful impact. This meant I <b><span style=\"color: #0b5394;\">focused on partners and practitioners who also operate in the realm of            hyperscale</span></b>--think massive, AI-adjacent companies and the HPC centers who have historically    dominated the very top of the Top500 list.</p><p>One thing I didn't anticipate going into SC24 is that I've inherited a third identity: there are a new cohort of people in HPC who see me as a <b><span style=\"color: #990000;\">long-time community            member</span></b>. This resulted in a surprising amount of my time being spent <b><span style=\"color: #0b5394;\">talking to students and early career practitioners</span></b> who were looking    for advice.</p><p>These three identities and goals meant I don't many notes to share on the technical program, but I did capture more observations about broader trends in the HPC industry and community.</p><h2 id=\"tech\">New technology and announcements</h2><div>HPC is all about cutting-edge technology, so that's a fine place to start talking about what was new.</div><h3 id=\"tech-top500\">Top500 and a new #1 system</h3><p>A cornerstone of every SC conference is the release of the new Top500 list on Monday, and    this is especially true on years when a new #1 supercomputer is announced. As was widely anticipated in the weeks    leading up to SC24, El Capitan unseated Frontier as the new #1 supercomputer this year, posting an impressive <a href=\"https://www.top500.org/system/180307/\">1.74 EFLOPS</a> of FP64. In addition though, Frontier grew a    little (it added 400 nodes), there was a notable new #5 system (Eni's HPC6), and a number of smaller systems appeared that are worth calling    out.</p><h4 id=\"tech-top500-elcap\">#1 - El Capitan</h4><p>The highlight of the Top500 list was undoubtedly the debut of El Capitan, Lawrence    Livermore National Laboratory's massive new MI300A-based exascale supercomputer. Its 1.74 EF score resulted from a    105-minute HPL run that came in under 30 MW, and a bunch of technical details about the system were disclosed by    Livermore Computing's CTO, Bronis de Supinski, during an invited talk during the Top500 BOF. Plenty of others    summarize the system's speeds and feeds (e.g., see <a href=\"https://www.nextplatform.com/2024/11/18/el-capitan-supercomputer-blazes-the-trail-for-converged-cpu-gpu-compute/\">The        Next Platform's article on El Cap</a>), so I won't do that. However, I will comment on how unusual Bronis' talk    was.</p><p>Foremost, the El Capitan talk seemed haphazard and last-minute. Considering the system took over half a decade of planning and cost at least half a    billion dollars, El Capitan's unveiling was the most unenthusiastic description of a brand-new #1 supercomputer I've    ever seen. I can understand that the Livermore folks have debuted plenty of novel #1 systems in their careers, but El    Capitan is objectively a fascinating system, and running a full-system job for nearly two hours across first-of-a-kind APUs    is an amazing feat. If community leaders don't get excited about their own groundbreaking achievements, what kind of message should the next generation of HPC professionals take home?</p><p>In sharp contrast to the blasé announcement of this new system was the leading slide that was presented to describe the speeds and feeds of El Capitan:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure></figure></div><p></p><p>I've never seen a speaker take the main stage and put <i>a photo of himself</i> literally in the center of the slide, in front of the supercomputer they're talking about. I don't know what the communications people at Livermore were trying to do with this graphic, but I don't think it    was intended to be evocative of the first thing that came to my mind:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure></figure></div><p></p><p>The supercomputer is literally named \"The Captain,\" and there's a photo of one dude (the boss of Livermore Computing,    who is also standing on stage giving the talk) blocking the view of the machine. It wasn't a great look, and it left me feeling very uneasy about what I was witnessing and what message it was sending to the HPC community.</p><p>In case it needs to be said, HPC is a team sport. The unveiling of El Capitan (or any other #1 system    before it) is always the product of dozens, if not hundreds, of people devoting years of their professional lives to    ensuring it all comes together. It was a big miss, both to those who put in the work, and those who will have    to put in the work on future systems, to suggest that a single, smiling face comes before the success of the system deployment.</p><h4 id=\"tech-top500-hpc6\">#5 - Eni HPC6</h4><p>The other notable entrant to the Top 10 list was HPC6, an industry system deployed by Eni (a major Italian energy    company) built on MI250X. Oil and gas companies tend to be conservative in the systems they buy since the seismic    imaging done on their large supercomputers informs hundred-million to billion-dollar investments in drilling a new    well, and they have much less tolerance for weird architectures than federally funded leadership computing does.    Thus, Eni's adoption of AMD GPUs in this #5 system is a strong endorsement of their capability in mission-critical    commercial computing.</p><h4 id=\"tech-top500-softbank\">#16 and #17 - SoftBank CHIE-2 and CHIE-3</h4><p>SoftBank, the Japanese investment conglomerate who, among other things, owns a significant stake in Arm, made its <a href=\"https://www.top500.org/site/51045/\">Top500 debut with two identical 256-node DGX H100 SuperPODs</a>. While    not technologically interesting (H100 is getting old), these systems represent significant investment in HPC by    private industry in Japan and signals that SoftBank is following the lead of large <a href=\"https://www.nytimes.com/2023/08/16/technology/ai-gpu-chips-shortage.html\">American investment groups in        building private AI clusters for the AI startups in their portfolios</a>. In doing this, SoftBank's investments    aren't dependent on third-party cloud providers to supply the GPUs to make these startups successful and reduces    their overall risk.</p><p>Although I didn't hear anything about these SoftBank systems at the conference, NVIDIA issued a press statement    during the NVIDIA AI Summit Japan during the week prior to SC24 that discussed <a href=\"https://nvidianews.nvidia.com/news/nvidia-and-softbank-accelerate-japans-journey-to-global-ai-powerhouse\">SoftBank's        investment in large NVIDIA supercomputers</a>. The press statement states that these systems will be used \"for    [SoftBank's] own generative AI development and AI-related business, as well as that of universities, research    institutions and businesses throughout Japan.\" The release also suggests we can expect B200 and GB200 SuperPODs from    SoftBank to appear as those technologies come online.</p><h4 id=\"tech-top500-jeti\">#18 - Jülich's JUPITER Exascale Transition Instrument (JETI)</h4><p>Just below the SoftBank systems was the precursor system to Europe's first exascale system. I was hoping that    JUPITER, the full exascale system being deployed at FRJ, would appear in the Top 10, but it seems like we'll have to    wait for ISC25 for that. Still, the JETI system ran HPL across 480 nodes of BullSequana XH3000, the same node that    will be used in JUPITER, and achieved 83 TFLOPS. By comparison, the full JUPITER system will be over 10x larger (\"<a href=\"https://www.fz-juelich.de/en/ias/jsc/jupiter/tech\">roughly 6000 compute nodes</a>\" in the Booster), and    projecting the JETI run (173 TF/node) out to this full JUPITER scale indicates that JUPITER should just squeak over    the 1.0 EFLOPS line.</p><p>In preparation for JUPITER, Eviden had a couple of these BullSequana XH3000 nodes out on display this year:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure></figure></div><p></p><p>And if you're interested in more, I've been tracking the technical details of <a href=\"https://glennklockwood.com/garden/systems/jupiter\">JUPITER in my digital garden</a>.</p><h4 id=\"tech-top500-reindeer\">#32 - Reindeer!</h4><p>Waay down the list was Microsoft's sole new Top500 entry this cycle, an NVIDIA H200 system that ran HPL over 120 ND    H200 v5 nodes in Azure. It was one of only two conventional (non-Grace) H200 clusters that appeared in the top 100,    and it had a pretty good efficiency (Rmax/Rpeak &gt; 80%). Microsoft also had a Reindeer node on display at its    booth:</p><p></p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure></figure></div><p></p><p>An astute observer may note that this node looks an awful lot like the H100 node used in its Eagle supercomputer,    which was <a href=\"https://blog.glennklockwood.com/2023/11/sc23-recap.html\">on display at SC23 last year</a>. That's    because it's the same chassis, just with an upgraded HGX baseboard.</p><p>Reindeer was not <i>super</i> exciting, and there were no press releases about it, but I mention it here for a couple    reasons:</p><p></p><ul><li>One of my teammates did the HPL run and submission, and his group got to come up with the name of the system for        the purposes of HPL. As it turns out, generating a public name for a Top500 submission involves a comical amount        of legal and marketing process when it comes from a giant corporation like Microsoft. And as it turns out,        naming a cluster \"Reindeer\" has a low probability of offending anyone.</li><li>Reindeer is pretty boring--it's a relatively small cluster with a bunch of GPUs. But when you're building out AI        infrastructure at a pace of <a href=\"https://build.microsoft.com/en-US/sessions/984ca69a-ffca-4729-bf72-72ea0cd8a5db\">5x Eagles (70,000            GPUs!) per month</a>, you want the clusters that those GPUs go into to be as boring, predictable, and        automatable as possible. Seeing as how Reindeer only used 960 GPUs but still got #32, it doesn't require much        math to realize that the big hyperscalers could flood the Top500 list with these cookie-cutter GPU clusters and        (in this case) make any ranking below #32 completely irrelevant. Heaven help the Top500 list if they ever        publish an API for submitting new systems; cloud providers' build validation automation could tack a Top500        submission on at the end of burn-in and permanently ruin the list.</li></ul><div>On a personal note, the supercomputer grant that gave me my first job in the HPC business <a href=\"https://www.top500.org/system/177455/\">debuted at #48</a>. It's mind-boggling that I now work in a place    where standing up a #32 system is just day-to-day business.</div><p></p><h3 id=\"tech-expo\">Technology on the exhibit floor</h3><p>The exhibit floor had a few new pieces of HPC technology on display this year that are    worthy of mention, but a lot of the most HPC-centric exciting stuff actually had a soft debut at <a href=\"https://blog.glennklockwood.com/2024/05/isc24-recap.html\">ISC24 in May</a>. For example, even though SC24 was MI300A's big splash due to    the El Capitan announcement, some MI300A nodes (such as the <a href=\"https://glennklockwood.com/garden/nodes/cray-ex255a\">Cray EX255a</a>) were on display in Hamburg. However,    Eviden had their MI300A node (branded XH3406-3) on display at SC24 which was new to me:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure></figure></div><p></p><p>I'm unaware of anyone who's actually committed to a large Eviden MI300A system, so I was    surprised to see that Eviden already has a full blade design. But as with Eni's HPC6 supercomputer, perhaps this is    a sign that AMD's GPUs (and now APUs) have graduated from being built-to-order science experiments to a technology    ecosystem that people will want to buy off the rack.</p><p>There was also a ton of GH200 on the exhibit hall floor, but again, these node types were    also on display at ISC24. This wasn't a surprise since a bunch of upcoming European systems have invested in GH200    already; in addition to JUPITER's 6,000 GH200 nodes described above, <a href=\"https://www.cscs.ch/computers/alps\">CSCS Alps</a> has 2,688 GH200 nodes, and <a href=\"https://glennklockwood.com/garden/systems/isambard-ai\">Bristol's Isambard-AI</a> will have 1,362 GH200    nodes. All of these systems will have a 1:1 CPU:GPU ratio and an NVL4 domain, suggesting this is the optimal way to    configure GH200 for HPC workloads. I didn't hear a single mention of GH200 NVL32.</p><h4 id=\"tech-expo-gb200\">GB200</h4><p>SC24 was the debut of NVIDIA's Blackwell GPU in the flesh, and a bunch of integrators had    material on GB200 out at their booths. Interestingly, they all followed the same pattern as GH200 with an NVL4    domain size, and just about every smaller HPC integrator followed a similar pattern where</p><p></p><ul><li>their booth had a standard \"NVIDIA Partner\" (or \"Preferred Partner!\") placard on their main desk</li><li>they had a bare NVIDIA GB200 baseboard (superchip) on display</li><li>there wasn't much other differentiation</li></ul><p>From this, I gather that not many companies have manufactured GB200 nodes yet, or if they    have, there aren't enough GB200 boards available to waste them on display models. So, we had to settle for these    bare NVIDIA-manufactured, 4-GPU + 2-CPU superchip boards:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure></figure></div><p></p><p>What struck me is that these are very large FRUs--if a single component (CPU, GPU, voltage    regulator, DRAM chip, or anything else) goes bad, you have to yank and replace four GPUs and two CPUs. And because    all the components are soldered down, someone's going to have to do a lot of work to remanufacture these boards to    avoid throwing out a lot of very expensive, fully functional Blackwell GPUs.</p><p>There were a few companies who were further along their GB200 journey and had more    integrated nodes on display. The HPE Cray booth had this GB200 NVL4 blade (the Cray EX154n) on display:</p><p></p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure></figure></div><p></p><p>It looks remarkably sparse compared to the super-dense blades that normally slot into the    Cray EX line, but even with a single NVL4 node per blade, the Cray EX cabinet only supports 56 of these blades,    leaving 8 blade slots empty in the optimal configuration. I assume this is a limitation of power and cooling.</p><p>The booth collateral around this blade suggested its use case is \"machine learning and    sovereign AI\" rather than traditional HPC, and that makes sense since each node has 768 GB of HBM3e which is enough    to support training some pretty large sovereign models. However, the choice to force all I/O traffic on to the    high-speed network by only leaving room for one piddly node-local NVMe drive (this blade only supports one SSD per    blade) will make training on this platform very sensitive to the quality of the global storage subsystem. This is    great if you bundle this blade with all-flash Lustre (like Cray ClusterStor) or DAOS (handy, since <a href=\"https://bsky.app/profile/adrianjhpc.bsky.social/post/3lba4yfg5fc2a\">Intel divested the entire DAOS        development team to HPE</a>). But it's not how I would build an AI-optimized system.</p><p>I suspect the cost-per-FLOP of this Cray GB200 solution is much lower than what a pure-play    GB200 for LLM training would be. And since GB200 is actually a solid platform for FP64 (thanks to Dan Ernst for <a href=\"https://bsky.app/profile/ernstdj.bsky.social/post/3lb23ipwnvc26\">challenging me on this</a> and sharing    some <a href=\"https://arxiv.org/abs/2411.12090\">great resources on the topic</a>), I expect to see this node do well    in situations that are not training frontier LLMs, but rather fine-tuning LLMs, training smaller models, and mixing    in traditional scientific computing on the same general-purpose HPC/AI system.</p><p>Speaking of pure-play LLM training platforms, though, I was glad that very few exhibitors    were trying to talk up GB200 NVL72 this year. It may have been the case that vendors simply aren't ready to begin    selling NVL72 yet, but I like to be optimistic and instead believe that the exhibitors who show up to SC24 know that    the scientific computing community likely won't get enough value out of a 72-GPU coherence domain to justify the    additional cost and complexity of NVL72. I didn't see a single vendor with a GB200 NVL36 or NVL72 rack on display    (or a GH200 NVL32, for that matter), and not having to think about NVL72 for the week of SC24 was a nice break from    my day job.</p><p>Perhaps the closest SC24 got to NVL72 was a joint announcement at the beginning of the week    by Dell and CoreWeave, who announced that <a href=\"https://www.coreweave.com/blog/coreweave-pushes-boundaries-with-gb200-and-more\">they have begun bringing        GB200 NVL72 racks online</a>. Dell did have a massive, AI-focused booth on the exhibit floor, and they did talk    up their high-powered, liquid-cooled rack infrastructure. But in addition to supporting GB200 with NVLink Switches,    I'm sure that rack infrastructure would be equally good at supporting nodes geared more squarely at traditional HPC.</p><h4 id=\"tech-expo-ss400\">Slingshot 400</h4><p>HPE Cray also debuted a new 400G Slingshot switch, appropriately named Slingshot 400. I    didn't get a chance to ask anyone any questions about it, but from the marketing material that came out right before    the conference, it sounds like a serdes upgrade without any significant changes to Slingshot's L2 protocol.</p><p>There was a Slingshot 400 switch for the Cray EX rack on display at their booth, and it    looked pretty amazing:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure></figure></div><p></p><p>It looks way more dense than the original 200G Rosetta switch, and it introduces    liquid-cooled optics. If you look closely, you can also see a ton of flyover cables connecting the switch ASIC in    the center to the transceivers near the top; similar flyover cables are showing up in all manner of    ultra-high-performance networking equipment, likely reflecting the inability to maintain signal integrity across PCB    traces.</p><p>The port density on Slingshot 400 remains the same as it was on 200G Slingshot, so there's    still only 64 ports per switch, and the fabric scale limits don't increase. In addition, the media is saying that    Slingshot 400 (and the GB200 blade that will launch with it) won't start appearing until \"<a href=\"https://www.nextplatform.com/2024/11/26/hpe-upgrades-supercomputer-lineup-top-to-bottom-in-2025/\">Fall        2025</a>.\" Considering 64-port 800G switches (like <a href=\"https://nvidianews.nvidia.com/news/networking-switches-gpu-computing-ai\">NVIDIA's SN5600</a> and <a href=\"https://www.arista.com/en/company/news/press-release/19493-arista-unveils-etherlink-ai-networking-platforms\">Arista's        7060X6</a>) will have already been on the market by then though, Slingshot 400 will be launching with HPE Cray    on its back foot.</p><p>However, there was a curious statement on the placard accompanying this Slingshot 400    switch:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure></figure></div><p></p><p>It reads, \"Ultra Ethernet is the future, HPE Slingshot delivers today!\"</p><p>Does this suggest that Slingshot 400 is just a stopgap until 800G Ultra Ethernet NICs begin    appearing? If so, I look forward to seeing HPE Cray jam third-party 800G switch ASICs into the Cray EX liquid-cooled    form factor at future SC conferences.</p><h4 id=\"tech-expo-gg\">Grace-Grace for storage?</h4><p>One of the weirder things I saw on the exhibit floor was a scale-out storage server built    on NVIDIA Grace CPUs that the good folks at WEKA had on display at their booth.</p><p></p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure></figure></div><p></p><p>Manufactured by Supermicro, this \"ARS-121L-NE316R\" server (really rolls off the tongue)    uses a two-socket Grace superchip and its LPDDR5X instead of conventional, socketed CPUs and DDR. The rest of it    seems like a normal scale-out storage server, with sixteen E3.S SSD slots in the front and four 400G ConnectX-7 or    BlueField-3 NICs in the back. No fancy dual-controller failover or anything like that; the presumption is that    whatever storage system you'd install over this server would implement its own erasure coding across drives and    servers.</p><p>At a glance, this might seem like a neat idea for a compute-intensive storage system like    WEKA or DAOS. However, one thing that you typically want in a storage server is high reliability and repairability,    features which weren't the optimal design point for these Grace superchips. Specifically,</p><p></p><ul><li>The Grace-Grace superchip turn both CPU sockets into a single FRU. This means that if one CPU goes bad, you're        shipping the whole board back to NVIDIA rather than just doing a field-swap of a socket.</li><li>Grace uses LPDDR5X, whose ECC is not as robust as DDR5. I'm not an expert on memory architecture, but my        understanding is that the ECC scheme on Grace does not provide ChipKill or row failures. And as with CPU        failure, if a single DRAM chip goes back, you're throwing out two CPUs and all the DRAM.</li><li>There's no way to value-engineer the exact quantity of cores, clock, and DRAM to be optimal for the storage        software installed on top of these servers.</li></ul><p>On the upside, though, there might be a cost advantage to using this Grace-Grace server    over a beefier AMD- or Intel-based server with a bunch of traditional DIMMs. And if you really like NVIDIA products,    this lets you do NVIDIA storage servers to go with your NVIDIA network and NVIDIA compute. As long as your storage    software can work with the interrupt rates of such a server (e.g., it supports rebuild-on-read) and the 144 Neoverse    V2 cores are a good fit for its computational requirements (e.g., calculating complex erasure codes), this server    makes sense. But building a parallel storage system on LPDDR5X still gives me the willies.</p><p>I could also see this thing being useful for certain analytics workloads, especially those    which may be upstream of LLM training. I look forward to hearing about where this turns up in the field.</p><p></p><h4 id=\"tech-expo-hbv5\">Microsoft and AMD's new HBM CPU</h4><p>The last bit of new and exciting HPC technology that I noted came from my very own employer    in the form of HBv5, a new, monster four-socket node featuring custom-designed AMD CPUs with HBM. STH wrote up <a href=\"https://www.servethehome.com/this-is-the-microsoft-azure-hbv5-and-amd-mi300c-nvidia/\">an article with        great photos of HBv5 and its speeds and feeds</a>, but in brief, this single node has:</p><p></p><ul><li>384 physical Zen 4 cores (352 accessible from within the VM) that clock up to 4 GHz</li><li>512 GB of HBM3 (up to 450 GB accessible from the VM) with up to 6.9 TB/s STREAM bandwidth</li><li>4x NDR InfiniBand NICs clocked at 200G per port</li><li>200G Azure Boost NIC (160G accessible from the VM)</li><li>8x 1.84 TB NVMe SSDs with up to 50 GB/s read and 30 GB/s write bandwidth</li></ul><p></p><p>The node itself looks kind of wacky as well, because there just isn't a lot on it:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure></figure></div><p></p><p>There are the obvious four sockets of AMD EPYC 9V64H, each with 96 physical cores and 128 GB of HBM3, and giant heat    pipes on top of them since it's 100% air-cooled. But there's no DDR at all, no power converter board (the node is    powered by a DC bus bar), and just a few flyover cables to connect the PCIe add-in-card cages. There is a separate    fan board with just two pairs of power cables connecting to the motherboard, and that's really about it.</p><p>The front end of the node shows its I/O capabilities which are similarly uncomplicated:</p><p></p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure></figure></div><p></p><p>There are four NDR InfiniBand cards (one localized to each socket) which are 400G-capable but cabled up at 200G,    eight E1.S NVMe drives, and a brand-new dual-port Azure Boost 200G NIC. Here's a close-up of the right third of the    node's front:</p><p></p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure></figure></div><p>This is the first time I've seen an Azure Boost NIC in a server, and it looksmuch better integrated than the previous-generation 100G Azure SmartNIC that put the FPGA and hard NIC on separateboards connected by a funny little pigtail. This older 100G SmartNIC with pigtail was also on display at the Microsoftbooth in an ND MI300X v5 node:</p><p></p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure></figure></div><p></p><p>And finally, although I am no expert in this new node, I did hang around the people who are all week, and I    repeatedly heard them answer the same few questions:</p><p></p><ul><li><b>Is this MI300C?</b> It is if you want it to be. You can call it Sally if you want; I don't think it will        care. But Microsoft calls it HBv5, and the processor name will show up as AMD EPYC 9V64H in /proc/cpuinfo.</li><li><b>Is its InfiniBand 1x800 port, 2x400 ports, ...?</b> There are four NDR InfiniBand HCA cards, and each card        has one full 400G NDR InfiniBand port. However, each port is only connected up to top-of-rack switching at 200G.        Each InfiniBand HCA hangs off of a different EPYC 9V64H socket so that any memory address can get to        InfiniBand without having to traverse Infinity Fabric. Running four ports of NDR InfiniBand at half speed is an        unusual configuration, but that's what's going on here.</li><li><b>How can I buy this CPU?</b> EPYC 9V64H are \"<a href=\"https://techcommunity.microsoft.com/blog/azurehighperformancecomputingblog/announcing-azure-hbv5-virtual-machines-a-breakthrough-in-memory-bandwidth-for-hp/4303504\">custom            AMD EPYC processors only available in Azure</a>.\" This means the only way to access it is by provisioning an        HBv5 virtual machine in Azure.</li></ul><div>Amidst all the unrelenting news about new GPUs optimized for AI workloads, it was nice to see something new and    unique launched squarely for the benefit of traditional scientific computing workloads.</div><p></p><p></p><h2 id=\"industry\">The HPC industry overall</h2><div><p>New technology announcements are always exciting, but one of the main reasons I attend        SC and ISC is to figure out the broader trends shaping the HPC industry. What concerns are top of mind for the        community, and what blind spots remain open across all the conversations happening during the week? Answering        these questions requires more than just walking the exhibit floor; it involves interpreting the subtext of the        discussions happening at panels and BOF sessions. However, identifying where the industry needs more information        or a clearer picture informs a lot of the public-facing talks and activities in which I participate throughout        the year.</p></div><h3 id=\"industry-attendee\">What I learned about the average SC technical program attendee</h3><p>The biggest realization that I confirmed this week is that <b>the SC conference is not an HPC        conference; it is a scientific computing conference</b>. I sat in a few sessions where the phrase \"HPC    workflows\" was clearly a stand-in for \"scientific workflows,\" and \"performance evaluation\" still really means \"MPI    and OpenMP profiling.\" I found myself listening to ideas or hearing about tools that were <em>intellectually</em>    interesting but ultimately not useful to me because they    were so entrenched in the traditions of applying HPC to scientific computing. Let's talk about a few ways in which    this manifested.</p><h4 id=\"industry-attendee-sustainability\">People think sustainability and energy efficiency are the same thing</h4><p>Take, for example, the topic of sustainability. There were talks, panels, papers, and BOFs    that touched on the environmental impact of HPC throughout the week, but the vast majority of them really weren't    talking about sustainability at all; they were talking about energy efficiency. These talks often use the following    narrative:</p><p></p><ol><li>Energy use from datacenters is predicted to reach some ridiculous number by 2030</li><li>We must create more energy-efficient algorithms, processors, and scheduling policies</li><li>Here is an idea we tested that reduced the energy consumption without impacting the performance of some        application or workflow</li><li>Sustainability achieved! Success!</li></ol><p>The problem with this approach is that it declares victory when energy consumption is    reduced. This is a great result if all you care about is spending less money on electricity for your supercomputer,    but it completely misses the much greater issue that the electricity required to power an HPC job is often generated    by burning fossil fuels, and that the carbon emissions that are directly attributable to HPC workloads are    contributing to global climate change. This blind spot was exemplified by this slide, presented during a talk titled    \"Towards Sustainable Post-Exascale Leadership Computing\" at the Sustainable Supercomputing workshop:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure></figure></div><p></p><p>I've <a href=\"https://blog.glennklockwood.com/2024/11/fasst-will-be-does-opportunity-to-adapt.html\">written about        this before</a> and I'll write about it again: FLOPS/Watt and PUE are not    meaningful metrics by themselves when talking about sustainability. A PUE of 1.01 is not helpful if the datacenter    that achieves it relies on burning coal for its power. Conversely, a PUE of 1.5 is not bad if all that electricity    comes from a zero-carbon energy source. The biggest issue that I saw being reinforced at SC this year is that    claims of \"sustainable HPC\" are accompanied by the subtext of \"as long as I can keep doing everything else the way I    always have.\"</p><p>There were glimmers of hope, though. Maciej Cytowski from Pawsey presented the opening talk    at the Sustainable Supercomputing workshop, and he led with the right thing--he acknowledged that 60% of    the fuel mix that powers Pawsey's supercomputers comes from burning fossil fuels:</p><p></p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure></figure></div><p></p><p>Rather than patting himself on the back at his low PUE, Dr. Cytowski's described on how    they built their datacenter atop a large aquifer from which they draw water at 21°C and return it at 30°C to avoid    using energy-intensive chillers. To further reduce the carbon impact of this water loop, Pawsey also installed over    200 kW of solar panels on its facility roof to power the water pumps. Given the fact that Pawsey cannot relocate to    somewhere with a higher ratio of zero-carbon energy on account of its need to be physically near the Square    Kilometer Array, Cytowski's talk felt like the most substantive discussion on sustainability in HPC that week.</p><p>Most other talks and panels on the topic really wanted to equate \"sustainability\" to \"FLOPS    per Watt\" and pretend like where one deploys a supercomputer is not a part of the sustainability discussion. The    reality is that, if the HPC industry wanted to take sustainability seriously, it would talk less about watts and    more about tons of CO<sub>2</sub>. Seeing as how the average watt of electricity in Tennessee produces <a href=\"https://www.epa.gov/egrid/data-explorer\">2.75x more carbon</a> than a watt of electricity in Washington,    the actual environmental impact of fine-tuning Slurm scheduling or fiddling with CPU frequencies is meaningless when    compared to the benefits that would be gained by deploying that supercomputer next to a hydroelectric dam instead of    a coal-fired power plant.</p><p>I say all this because there are parts of the HPC industry (namely, the part in which I work)    who <i>are</i> serious about sustainability. And those conversations go beyond simply building supercomputers in    places where energy is low-carbon (thereby reducing <a href=\"https://www.epa.gov/climateleadership/scope-1-and-scope-2-inventory-guidance\">Scope 2 emissions</a>). They    include holding suppliers to high standards on reducing the carbon impact of transporting people and material to    these data centers, reducing the carbon impact of all the excess packaging that accompanies components, and being    accountable for the impact of everything in the data center after it reaches end of life (termed <a href=\"https://www.epa.gov/climateleadership/scope-3-inventory-guidance\">Scope 3 emissions</a>).</p><p>The HPC community--or more precisely, the scientific computing community--is still married    to the idea that the location of a supercomputer is non-negotiable, and \"sustainability\" is a nice-to-have secondary    goal. I was    hoping that the sessions I attended on sustainability would approach this topic at a level where the    non-scientific HPC world has been living. Unfortunately, the discussion at SC24, which spanned workshops, BOFs, and    Green 500, remains largely stuck on the idea that PUE and FLOPS/Watt are the end-all sustainability metrics. Those    metrics are important, but there are global optimizations that have much greater effects on reducing the    environmental impact of the HPC industry.</p><h4 id=\"industry-attendee-ai\">AI sessions are really scientific computing sessions about AI</h4><p>Another area where \"HPC\" was revealed to really mean \"scientific computing\" was in the    topic of AI. I sat in on a few BOFs and panels around AI topics to get a feel for where this community is in    adopting AI for science, but again, I found the level of discourse to degrade to generic AI banter despite the best    efforts of panelists and moderators. For example, I sat in the \"Foundational Large Language Models for    High-Performance Computing\" BOF session, and Jeff Vetter very clearly defined what a \"foundational large language    model\" was at the outset so we could have a productive discussion about their applicability in HPC (or, really,    scientific computing):</p><p></p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure></figure></div><p></p><p>The panelists did a good job of outlining their positions. On the upside, LLMs are good for    performing source code conversion, documenting and validating code, and maximizing continuity in application codes    that get passed around as graduate students come and go. On the downside, they have a difficult time creating    efficient parallel code, and they struggle to debug parallel code. And that's probably where the BOF should have    stopped, because LLMs, as defined at the outset of the session, don't actually have a ton of applicability in    scientific computing. But as soon as the session opened up to audience questions, the session went off the rails.</p><p>The first question was an extremely basic and nonspecific question: \"Is AI a bubble?\"</p><p>It's fun to ask provocative questions to a panel of experts. I get it. But the question had    nothing to do with LLMs, any of the position statements presented by panelists, or even HPC or scientific computing.    It turned a BOF on \"LLMs for HPC\" into a BOF that might as well have been titled \"Let's just talk about AI!\" A few    panelists tried to get things back on track by talking about the successes of surrogate models to simulate physical    processes, but this reduced the conversation to a point where \"LLMs\" really meant \"any AI model\" and \"HPC\" really    meant \"scientific simulations.\"</p><p>Perhaps the most productive statement to come out of that panel was when Rio Yokota    asserted that \"we\" (the scientific community) should not train their own LLMs, because doing so would be    \"unproductive for science.\" But I, as well as anyone who understands the difference between LLMs and \"AI,\" already    knew that. And the people who don't understand the difference between an LLM and a surrogate model probably didn't    pick up on Dr. Yokota's statement, so I suspect the meaning of his contribution was completely lost.</p><p>Walking out of that BOF (and, frankly, the other AI-themed BOFs and panels I attended), I    was disappointed at how superficial the conversation was. This isn't to say these AI sessions were objectively    <i>bad</i>; rather, I think it reflects the general state of understanding of AI amongst SC attendees. Or perhaps it    reflects the demographic that is drawn to these sorts of sessions. If the SC community is not ready to have a    meaningful discussion about AI in the context of HPC or scientific computing, attending BOFs with like-minded peers    is probably a good place to begin getting immersed.</p><p>But what became clear to me this past week is that SC BOFs and panels with \"AI\" in their    title aren't really meant for practitioners of AI. They're meant for scientific computing people who are beginning    to dabble in AI.</p><h4 id=\"industry-attendee-ops\">AI for operations is not yet real in scientific computing</h4><p>I was invited to sit on a BOF panel called \"Artificial Intelligence and Machine Learning    for HPC Workload Analysis\" following on a successful BOF in which I participated at ISC24. The broad intent was to    have a discussion around the tools, methods, and neat ideas that HPC practitioners have been using to better    understand workloads, and each of us panelists was tasked with talking about a project or idea we had in applying    AI/ML to improve some aspect of workloads.</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure></figure></div><p></p><p>What emerged from us speakers' lightning talks is that applying AI for operations--in this    case, understanding user workloads--is nascent. Rather than talking about how we use AI to affect how we design or    operate supercomputers, all of us seemed to focus more on how we are collecting data and beginning to analyze that    data using ML techniques. And maybe that's OK, because AI won't ever do anything for workload characterization until    you have a solid grasp of the telemetry you can capture about those workloads in the first place.</p><p>But when we opened the BOF up to discussion with all attendees, despite having a packed    room, there was very little that the audience had. Our BOF lead, Kadidia Konaté, tried to pull discussion out of the    room from a couple of different fronts by asking what tools people were using, what challenges they were facing, and    things along those lines. However, it seemed to me that the majority of the audience was in that room as spectators;    they didn't know where to start applying AI towards understanding the operations of supercomputers. Folks attended    to find out the art of the possible, not talk about their own challenges.</p><p>As such, the conversation wound up bubbling back up to the safety of traditional topics in    scientific computing--how is LDMS working out, how do you deal with data storage challenges of collecting telemetry,    and all the usual things that monitoring and telemetry folks worry about. It's easy to talk about the topics you    understand, and just as the LLM conversation reverted back to generic AI for science and the sustainability topic    reverted back to FLOPS/Watt, this topic of AI for operations reverted back to standard telemetry collection.</p><h4 id=\"industry-attendee-hyperscale\">Some are beginning to realize that HPC exists outside of scientific computing</h4><p>Despite the pervasive belief at SC24 that \"HPC\" and \"scientific computing\" are the same thing, there are early signs    that the leaders in the community are coming to terms with the reality that there is now a significant amount of    leadership HPC happening outside the scope of the conference. This was most prominent at the part of the Top500 BOF    where Erich Strohmaier typically discusses trends based on the latest publication of the list.</p><p>In years past, Dr. Strohmaier's talk was full of statements that strongly implied that, if a supercomputer is not    listed on Top500, it simply does not exist. This year was different though: he acknowledged that El Capitan,    Frontier, and Aurora were \"the three exascale systems <u style=\"font-style: italic;\">we are aware of</u>,\" now being    clear that there is room for exascale systems to exist that simply never ran HPL, or never submitted HPL results to    Top500. He explicitly acknowledged again that China has stopped making any Top500 submissions, and although he    didn't name them outright, he spent a few minutes dancing around \"hyperscalers\" who have been deploying exascale    class systems such as <a href=\"https://glennklockwood.com/garden/systems/meta's-h100-clusters\">Meta's H100        clusters</a> (2x24K H100), <a href=\"https://glennklockwood.com/garden/systems/colossus\">xAI's        Colossus</a> (100K H100), and the full system behind <a href=\"https://glennklockwood.com/garden/systems/eagle\">Microsoft's Eagle</a> (14K H100 is a \"<a href=\"https://build.microsoft.com/en-US/sessions/984ca69a-ffca-4729-bf72-72ea0cd8a5db\">tiny fraction</a>\").</p><p>Strohmaier did an interesting analysis that estimated the total power of the Top500 list's supercomputers so he could    compare it to industry buzz around hyperscalers building gigawatt-sized datacenters:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure></figure></div><p></p><p>It was a fun analysis where he concluded that there are between 500-600 megawatts of supercomputers on the Top500    list, and after you factor in storage, PUE, and other ancillary power sources, the whole Top500 list sums up to what    hyperscalers are talking about sticking into a single datacenter facility.</p><p>Although he didn't say it outright, I think the implication here is that the Top500 list is rapidly losing relevance    in the broad HPC market, because a significant amount of the world's supercomputing capacity <i>and capability</i>    are absent from the list. Although specific hyperscale supercomputers (like Meta's, xAI's, and Microsoft's) were not    mentioned outright, their absence from the Top500 list suggests that this list might already be more incomplete than    it is complete--the sum of the FLOPS or power on the Top500 supercomputers may be less than the sum of the giant    supercomputers which are known but not listed. This will only get worse as the AI giants keep building systems every    year while the government is stuck on its 3-5 year procurement cycles.</p><p>It follows that the meaning of the Top500 is sprinting towards a place where it is not representative of HPC so much    as it is representative of <i>the slice of HPC that serves scientific computing</i>. Erich Strohmaier was clearly    aware of this in his talk this year, and I look forward to seeing how the conversation around the Top500 list    continues to morph as the years go on.</p><h3 id=\"industry-nsf\">NSF's broad front vs. DOE's big bets in HPC and AI</h3><p>My career was started at an NSF HPC center and <a href=\"https://blog.glennklockwood.com/2022/05/life-and-leaving-nersc.html\">built up over my years in the        DOE</a>, so I feel like I owe a debt to the people who provided all the opportunities and mentorship that let me    get to the place of privilege in the hyperscale/AI industry that I now enjoy. As a result, I find myself still    spending a lot of my free time thinking about <a href=\"https://glennklockwood.com/garden/government's-role-in-ai\">the role of governments in the changing face of        HPC</a> (as evidenced by my critiques of <a href=\"https://blog.glennklockwood.com/2024/10/a-critique-of-call-for-public-ai.html\">thinktank reports</a> and <a href=\"https://blog.glennklockwood.com/2024/11/fasst-will-be-does-opportunity-to-adapt.html\">federal RFIs</a>...) and trying to bridge the gap    in technical understanding between my old colleagues (in DOE, NSF, and European HPC organizations) and whatever they    call what I work on now (hyperscale AI?).</p><p>To that end, I found myself doing quite a bit of <i>business development</i> (more on this later) with government    types since I think that is where I can    offer the most impact. I used to be government, and I closely follow the state of their thinking in HPC, but I also    know what's going on inside the hyperscale and AI world. I also have enough context in both areas to draw a line    through all the buzzy AI press releases to demonstrate how the momentum of private-sector investment in AI might    affect the way national HPC    efforts do business. So, I did a lot of talking to both my old colleagues in DOE and their industry partners in an    attempt to help them understand how the hyperscale and AI industry thinks about infrastructure, and what they should    expect in the next year.</p><p>More importantly though, I also sat in on a couple of NSF-themed BOFs to get a better understanding of where their    thinking is, where NAIRR is going, how the NSF's strategy contrasts with DOE's strategy, and where the ambitions of    the Office of Advanced Cyberinfrastructure might intersect with the trajectory of hyperscale AI.</p><p>What I learned was that NSF leadership is aware of everything that the community should be concerned about: the    growth of data, the increasing need for specialized silicon, the incursion of AI into scientific computing, new    business models and relationships with industry, and broadening the reach of HPC investments to be globally    competitive. But beyond that, I struggled to see a cohesive vision for the future of NSF-funded    supercomputing. </p><p>A BOF with a broad range of stakeholders probably isn't the best place to lay out a vision for the future of NSF's    HPC efforts, and perhaps NSF's vision is best expressed through its funding opportunities and awards. Whichever the    case may be, it seems like the NSF remains on a path to make incremental progress on a broad front of topics. Its    Advanced Computing Systems and Services (ACSS) program will continue to fund the acquisition of newer    supercomputers, and a smorgasbord of other research programs will continue funding efforts across public access to    open science, cybersecurity, sustainable software, and other areas. My biggest concern is that peanut-buttering    funding across such a broad portfolio will make net forward progress much slower than taking big bets. Perhaps big    bets just aren't in the NSF's mission though.</p><p>NAIRR was also a topic that came up in every NSF-themed session I attended, but again, I didn't get a clear picture    of the future. Most of the discussion that I heard was around socializing the resources that are available today    through NAIRR, suggesting that the pilot's biggest issue is not a lack of HPC resources donated by industry, but    awareness that NAIRR is a resource that researchers can use. This was reinforced by a survey whose results were    presented in the NAIRR BOF:</p><p></p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure></figure></div><p></p><p>It seems like the biggest challenges facing the NSF community relying on NAIRR (which has its own sample bias) is    that they don't really know where to start even though they have AI resources (both GPUs and model API services) at    their disposal. In a sense, this is a great position for the NSF since</p><p></p><ol><li>its users need intellectual help more than access to GPU resources, and the NSF has been great at promoting        education, training, and workforce development.</li><li>its users are unlikely to demand the same cutting-edge GPUs that AI industry leaders are snapping up. For        example, the largest pool of GPUs in NAIRR are A100 GPUs that NVIDIA donated via DGX Cloud; the big AI        companies moved off of Ampere a year ago and are about to move off of Hopper.</li></ol><p></p><p>However, it also means that there's not a clear role for partnership with many industry players beyond donating    resources to the NAIRR pilot today in the hopes of selling resources to the full NAIRR tomorrow. I asked what OAC    leadership thought about moving beyond such a transactional relationship between NSF and industry at one of the BOFs    I attended, and while the panelists were eager to explore specific answers to that question, I didn't hear any ideas    that would approach some sort of truly equitable partnership where both parties contributed in-kind.</p><p>I also walked away from these NSF sessions struck by how different the NSF HPC community's culture is from that of    the DOE. NSF BOF attendees seemed focused on getting answers and guidance from NSF leadership, unlike the typical    DOE gathering, where discussions often revolve around attendees trying to shape priorities to align with their own    agendas. A room full of DOE people tends to feel like everyone thinks they're the smartest person there, while NSF    gatherings appear more diverse in the expertise and areas of depth of its constituents. Neither way is inherently    better or worse, but it will make the full ambition of NAIRR (as an inter-agency collaboration) challenging to    navigate. This is particularly relevant as DOE is now pursuing its own multi-billion-dollar AI infrastructure    effort, FASST, that appears to sidestep NAIRR.</p><h3 id=\"industry-expo\">Exhibitor trends</h3><p>There's no better way to figure out what's going on in the HPC industry than walking the    exhibit floor each year, because booths cost money and reflect the priorities (and budgets) of all participants.    This year's exhibit felt physically huge, and walking from one end to the other was an adventure. You can get a    sense of the scale from this photo I took during the opening gala:</p><p></p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure></figure></div><p></p><p>Despite having almost 18,000 registrants and the opening gala usually being acrush of people, the gala this year felt and looked very sparse just because people and booths were more spread out.There was also a perceptibly larger number of splashy vendors who have historically never attended before who werepromoting downstream HPC technologies like data center cooling and electrical distribution, and there was healthyspeculation online about whether the hugeness of the exhibit this year was due to these new power and cooling companies.</p><p></p><p>To put these questions to rest, I figured out how to yank down all the exhibitor metadata    from the conference website so I could do some basic analysis on it.</p><h4 id=\"industry-expo-booths\">Booths by the numbers</h4><p>The easiest way to find the biggest companies to appear this year was to compare the    exhibitor list and booth sizes from SC23 to this year and see whose booth went from zero to some big square footage.</p><p></p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure></figure></div><p></p><p>I only took the top twenty new vendors, but they broadly fall into a couple of categories:</p><p></p><ul><li><b>Power and cooling</b>: Stulz, Delta, Airedale, Valvoline, Boundary Electric, Schneider Electric, Mara        </li><li><b>Server manufacturing</b>: Wistron, AMI, Pegatron</li><li><b>Higher ed</b>: Tennessee Tech, SCRCC</li></ul><p>There were a couple other companies that must've just missed last SC but aren't new to        the show (NetApp, Ansys, Samsung, Micron, Broadcom). And curiously, only one new GPU-as-a-Service provider        (Nebius) showed up this year, suggesting last year was the year of the GPU Cloud.</p><p>But to confirm what others had speculated: yes, a significant amount of the new square        footage of the exhibit floor can be attributed to companies focused on power and cooling. This is an interesting        indicator that HPC is becoming mainstream, largely thanks to AI demanding ultra-high density of power and        cooling. But it's also heartening to see a few new exhibitors in higher education making an appearance. Notably,        SCRCC (South Carolina Research Computing Consortium) is a consortium between Clemon, University of South        Carolina, and Savannah River National Laboratory that just formed last year, and I look forward to seeing what        their combined forces can bring to bear.</p><p>We can also take a look at whose booths grew the most compared to SC23:</p><p></p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure></figure></div><p></p><p>This distribution is much more interesting, since the top 20 exhibitors who grew their footprint comprise the        majority of the growth in existing exhibitors. Cherry-picking a few interesting growers:</p><p></p><ul><li><b>Power and cooling</b>: USystems, Midas, Vertiv</li><li><b>Data center/GPUaaS</b>: iM, Iris Energy, and (arguably) Oracle</li><li><b>Software</b>: Arc Compute and CIQ</li><li><b>Companies facing serious financial or legal troubles</b>: I count at least three! Impressive that they            are still pouring money into their SC booths.</li></ul><p>It's also interesting to see HLRS, the German national HPC center, grow so        significantly. I'm not sure what prompted such a great expansion, but I take it to mean that things have been        going well there.</p><p>Finally, Dell had a massive booth and showing this year. Not only did they grow the        most since SC23, but they had the single largest booth on the exhibit floor at SC24. This was no doubt a result        of their great successes in partnering with NVIDIA to land massive GPU buildout deals at places like <a href=\"https://qz.com/dell-super-micro-computer-stock-elon-musk-ai-nvidia-1851550428\">xAI</a> and <a href=\"https://www.tomshardware.com/tech-industry/artificial-intelligence/dell-reaches-milestone-with-industrys-first-enterprise-ready-nvidia-blackwell-poweredge-xe9712-server-racks\">CoreWeave</a>.        They also had \"AI factory\" messaging emblazoned all over their marketing material and debuted a nice 200 kW        liquid-cooled rack that will be the basis for their GB200 NVL72 solution, clearly leaning into the idea that        they are leaders in AI infrastructure. Despite this messaging being off-beat for the SC audience as I've        described earlier, their booth was surprisingly full all the time, and I didn't actually get a chance to get in        there to talk to anyone about what they've been doing.</p><p>Equally interesting are the vendors who reduced their footprint at SC24 relative to        SC23:</p><p></p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure></figure></div><p></p><p>Reading too much into any of these big shrinkers is pretty easy; while a reduction in        booth size could suggest business hasn't been as good, it could equally mean that an exhibitor just went        overboard at SC23 and downsized to correct this year. A few noteworthy exhibitors to call out:</p><p></p><ul><li>Penguin and the Korea Semiconductor Industry Association both cut way back from massive 50x50 booths to            30x30. Their booths this year were both big, but they weren't massive. Viridien, formerly known as CGG, also            shrunk from a massive booth to a less-massive 30x40.</li><li>Juniper still kept an independent booth, but it is in the <a href=\"https://www.hpe.com/us/en/newsroom/press-release/2024/01/hpe-to-acquire-juniper-networks-to-accelerate-ai-driven-innovation.html\">process                of being absorbed into HPE</a>. Shrinking makes sense.</li><li>Major cloud providers Google and AWS scaled back, but Microsoft did not.</li><li>GPU-as-a-Service cloud providers CoreWeave and Lambda both scaled back. Since these GPUaaS providers'            business models typically rely on courting few big customers, it may make sense to cut back on booth volume.        </li><li>Major AI storage companies DDN, VAST, and (to a lesser degree) Pure also scaled back, while WEKA did not. I            know business for DDN and VAST has been great this past year, so these may just reflect having gone            overboard last year.</li></ul><p>Overall, almost twice as many vendors grew their booths than scaled back, so I'd        caution anyone against trying to interpret any of this as anything beyond exhibitors right-sizing their booths        after going all-in last year.</p><p>Finally, there are a handful of vendors who disappeared outright after SC23:</p><p></p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure></figure></div><p></p><p>It is critical to point out that the largest booths to vanish outright were all on the        smaller size: SUSE, Tenstorrent, and Symbiosys Alliance all disappeared this year, but their booths last year        were only 20x30. I was surprised to see that Tenstorrent and Arm didn't have booths, but the others are either        companies I haven't heard of (suggesting the return on investment of showing at SC might've been low), are easy        to rationalize as only being HPC-adjacent (such as SNIA and DigitalOcean), or simply went bankrupt in the last        year.</p><p>As we say at the business factory, the net-net of the exhibit hall this year is that        the square footage of booth space increased by 15,000 square feet, so it was in fact bigger, it did take longer        to walk from one end to the other, and there definitely were a bunch of new power and cooling companies filling        out the space. Some exhibitors shrank or vanished, but the industry as a whole appears to be moving in a healthy        direction.</p><p>And if you're interested in analyzing this data more yourself, please have a look at <a href=\"https://github.com/glennklockwood/sc-exhibitors\">the data and the Jupyter notebook I used to generate            the above treemaps on GitHub</a>. If you discover anything interesting, please write about it and post it        online!</p><p></p><p></p><h4 id=\"industry-expo-gpuaas\">Proliferation of GPU-as-a-Service providers</h4><p>As an AI infrastructure person working for a major cloud provider, I kept an eye out for all the companies trying        to get into the GPU-as-a-Service game. <a href=\"https://blog.glennklockwood.com/2023/11/sc23-recap.html\">I described these players last year as            \"pure-play GPU clouds,\"</a> and it seems like the number of options available to customers who want to go        this route is growing. But I found it telling that a lot of them had booths that were completely        indistinguishable from each other. Here's an example of one:</p><p></p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure></figure></div><p></p><p>As best I can tell, these companies are all NVIDIA preferred partners with    data centers and a willingness to deploy NVIDIA GPUs, NVIDIA SmartNICs, and NVIDIA cloud stack, and sell multi-year    commitments to consume those GPUs. I tried to accost some of these companies' booth staff to ask them my favorite    question (\"What makes you different from everyone else?\"), but most of these companies' booths were staffed by    people more interested in talking to each other than me.</p><p>These GPUaaS providers tend to freak me out, because, as Microsoft's CEO recently stated, these companies are        often \"<a href=\"https://www.microsoft.com/en-us/Investor/events/FY-2025/earnings-fy-2025-q1\">just a bunch of            tech companies still using VC money to buy a bunch of GPUs</a>.\" I can't help but feel like this is where        the AI hype will come back to bite companies who have chosen to build houses upon sand. Walking the SC24 exhibit        floor is admittedly a very narrow view of this line of business, but it seemed like some of these companies were        content to buy up huge booths, hang a pretty banner above it, and otherwise leave the booth empty of anything        beyond a few chairs and some generic value propositions. I didn't feel a lot of hunger or enthusiasm from these        companies despite the fact that a bunch of them have hundreds of millions of dollars of GPUs effectively sitting        on credit cards that they are going to have to make payments on for the next five years.</p><p>That all said, not all the companies in the GPUaaS are kicking back and letting the money pour in. In particular,        I spent a few minutes chatting up someone at the CoreWeave booth, and I was surprised to hear about how much        innovation they're adding on top of their conventional GPUaaS offering. For example, they developed <a href=\"https://docs.coreweave.com/coreweave-machine-learning-and-ai/training/sunk\">Slurm on Kubernetes            (SUNK)</a> with one of their key customers to close the gap between the fact that CoreWeave exposes its GPU        service through Kubernetes, but many AI customers have built their stack around Slurm, <a href=\"https://github.com/NVIDIA/pyxis\">pyxis</a>, and <a href=\"https://github.com/NVIDIA/enroot\">enroot</a>.    </p><p>In a weird twist of fate, I later ran into an old acquaintance who turned out to be one of the key CoreWeave        customers for whom SUNK was developed. He commented that SUNK is the real deal and does exactly what his users        need which, given the high standards that this person has historically had, is a strong affirmation that SUNK is        more than just toy software that was developed and thrown on to GitHub for an easy press release. CoreWeave is        also developing some interesting high-performance object storage caching software, and all of these software        services are provided at no cost above whatever customers are already paying for their GPU service.</p><p>I bring this up because it highlights an emerging distinction in the GPUaaS market, which used to be a homogenous        sea of bitcoin-turned-AI providers. Of course, many companies still rely on that simple business model: holding        the bill for rapidly depreciating GPUs that NVIDIA sells and AI startups consume. However, there are now GPUaaS        providers moving up the value chain by taking on the automation and engineering challenges that model developers        don't want to deal with. Investing in uncertain projects like new software or diverse technology stacks is        certainly risky, especially since they may never result in enough revenue to pay for themselves. But having a        strong point of view, taking a stance, and investing in projects that you feel are right deserves recognition.        My hat is off to the GPUaaS providers who are willing to take these risks and raise the tide for all of us        rather than simply sling NVIDIA GPUs to anyone with a bag of money.</p><h2 id=\"community\">Community and connections</h2><p>As much as I enjoy <i>increasing shareholder value</i>, the part of SC that gives me the    greatest joy is reconnecting with the HPC community. Knowing I'll get to chat with my favorite people in the    industry (and meet some new favorite people!) makes the long plane rides, upper respiratory infections, and weird    hotel rooms completely worth it.</p><p></p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure></figure></div><p></p><p>I wound up averaging under six hours of sleep per night this year in large part because 9pm    or 7am were often the only free times I had to meet with people I really wanted to see. I have this unhealthy    mindset where every hour of every day, from the day I land to the day I leave, is too precious to waste, and it's    far too easy for me to rationalize that spending an hour talking to someone interesting is worth losing an hour of    sleep.</p><p>But like I said at the outset of this blog post, this year felt different for a few    reasons, and a lot of them revolve around the fact that I think I'm getting old. Now, it's always fun to say \"I'm    getting old\" in a mostly braggadocious way, but this feeling manifested in concrete ways that affected the way I    experienced the conference:</p><p></p><ol><li>I hit my limit on Monday night and couldn't get home without spending 15 minutes sitting in an unlit playground        across from the World of Coke. I've always gotten blisters and fatigue, but this was the first time I couldn't        just cowboy up and muscle through it. To avoid a repeat of this, I wound up \"wasting\" (see above) a lot more        time to just get off my feet this year.</li><li>This year, I reached the point where I need to start time-box how much time I spend chatting up the folks I        bump into. I used to just let the good times roll if I ran into someone I knew, but this year I wound up        spending as much time attending sessions as I did missing sessions because I got caught up in a conversation.        This isn't a bad thing per se, but I did feel a little sour when I realized I'd made a bad bet on choosing to        chat instead of attending a session or vice versa, and this bad feeling lingered in the back of my mind just        about every day.</li><li>There weren't a lot of surprises for me at the conference this year, and I worry that I am at risk of losing        touch with the technical aspects of the conference that get newer attendees excited. Instead of hearing about,        say, the latest research in interconnects, more of my time was spent mucking it up with the sorts of people in        the HPC community who I used to find intimidating. On the one hand, hooray me for making it into old boys'        clubs. But on the other, I don't want to become some HPC greybeard whose last meaningful contribution to the        industry was twenty years ago.</li><li>This is the first year where I've had people accost me <i>and ask me for advice</i>. I've long been accosted by        strangers because of my online presence, but those interactions were always lighthearted exchanges of \"I follow        you on Twitter\" and \"Great to meet you. Have an @HPC_Guru pin.\" This year, I had people specifically ask me for        advice on industry versus postdoc, AI versus HPC, and what my master plan was when I left NERSC. Even though I        didn't have any sage advice, I still found it really hard to tell bright-eyed students to go kick rocks just so        I wouldn't be late for yet another mushy panel on AI.</li></ol><p>If you read this all and think \"boo hoo, poor Glenn is too popular and wise for his own    good,\" yeah, I get it. There are worse problems to have. But this was the first year where I felt like what I put    into the conference was greater than what I got out of it. Presenting at SC used to be at least as good for my    career as it was useful for my audiences, but it just doesn't count for much given my current role and career stage.    It felt like some of the magic was gone this year in a way I've never experienced before. </p><p></p><h3 id=\"community-people\">Getting to know people</h3><p>As the years have gone on, I spend an increasing amount of my week having one-on-one    conversations instead of wandering aimlessly. This year though, I came to SC without really having anything to buy    or sell:</p><p></p><ul><li>I am not a researcher, so I don't need to pump up the work I'm doing to impress my fellow researchers.</li><li>I no longer own a product market segment, so I don't directly influence the customers or vendors with whom my        employer works.</li><li>I don't have any bandwidth in my day job to support any new customers or partnerships, so I don't have a strong        reason to sell people on partnering with me or my employer. </li></ul><p>Much to my surprise though, a bunch of my old vendor/partner colleagues still wanted to get    together to chat this year. Reflecting back, I was surprised to realize that it was these conversations--not the    ones about business--that were the most fulfilling this year.</p><p>I learned about people's hobbies, families, and their philosophies on life, and it was    amazing to get to know some of the people behind the companies with whom I've long dealt. I was reminded that the    person is rarely the same as the company, and even behind some of the most aggressive and blusterous tech companies    are often normal people with the same concerns and moments of self-doubt that everyone else has. I was also reminded    that good engineers appreciate good engineering regardless of whether it's coming from a competitor or not. The    public persona of a tech exec may not openly admire a competitor's product, but that doesn't mean they don't know    good work when they see it.</p><p>I also surprised a colleague whose career has been in the DOE labs with an anecdote that    amounted to the following: even though two companies may be in fierce competition, the people who work for them    don't have to be. The HPC community is small enough that almost everyone has got a pal at a competing company, and    when there are deals to be made, people looove to gossip. If one salesperson hears a juicy rumor about a prospective    customer, odds are that everyone else on the market will hear about it pretty quickly too. Of course, the boundaries    of confidentiality and professionalism are respected when it matters, but the interpersonal relationships that are    formed between coworkers and friends don't suddenly disappear when people change jobs.</p><p>And so, I guess it would make sense that people still want to talk to me even though I have    nothing to buy or sell. I love trading gossip just as much as everyone else, and I really enjoyed this aspect of the    week.</p><p></p><h3 id=\"community-career\">Talking to early career people</h3><p>I also spent an atypically significant amount of my week talking to early career people in    HPC who knew of me one way or another and wanted career advice. This is the first year I recall having the same    career conversations with multiple people, and this new phase of my life was perhaps most apparent during the IEEE    TCHPC/TCPP HPCSC career panel in which I was invited to speak this year.</p><p></p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure></figure></div><p></p><p>It was an honor to be asked to present on a career panel, but I didn't feel very qualified to give career advice to    up-and-coming computer science graduate students who want to pursue HPC. I am neither a computer scientist nor a    researcher, but fortunately for me, my distinguished co-panelists (Drs. Dewi Yokelson, Olga Pearce, YJ Ji, and    Rabab Alomairy) had plenty of more relevant wisdom to share. And at the end of the panel, there were a few things we    all seemed to agree on as good advice:</p><p></p><ol><li>Knowing stuff is good, but being able to learn things is better. Being eager to learn and naturally curious        makes this much easier as well.</li><li>The life of a researcher sometimes requires more than working a standard nine-to-five, so it'll be hard to be        really successful if your heart isn't in it.</li><li><a href=\"https://quoteinvestigator.com/2014/04/06/they-feel/\">People will forget what you did or what you said,            but they remember how you made them feel</a>. Don't be a jerk, because this community is small.</li></ol><p></p><p>In both this panel the one-on-one conversations I had with early career individuals, the best I could offer was the    truth: I never had a master plan that got me to where I am; I just try out new things until I realize I don't like    doing them anymore. I never knew what I wanted to be when I grew up, and I still don't really, so it now makes me    nervous that people have started approaching me with the assumption that I've got it all figured out. Unless I    torpedo my career and go live on a goat farm though, maybe I should prepare for this to be a significant part of my    SC experiences going forward.</p><h3 id=\"community-bsky\">Shift in social media</h3><p>One last, big change in the community aspect of SC this year was the mass-migration of a ton of HPC folks from    Twitter to Bluesky during the week prior to the conference. I don't really understand what prompted it so suddenly;    a few of us have been trying for years to get some kind of momentum on other social platforms like Mastodon, but the    general lack of engagement meant that all the excitement around SC always wound up exclusively on Twitter. This year    was different though, and Bluesky hit critical mass with the HPC community.</p><p>I personally have never experienced an SC conference without Twitter; my first SC was in 2013, and part of what made    that first conference so exciting was being able to pull up my phone and see what other people were seeing,    thinking, and doing across the entire convention center via Twitter. Having the social media component to the    conference made me feel like I was a part of something that first year, and as the years went on, Twitter became an    increasingly indispensable part of the complete SC experience for me.</p><p>This year, though, I decided to <a href=\"https://x.com/glennklockwood/status/1857571101028790498\">try an        experiment</a> and see what SC would be like if I set Twitter aside and invested my time into Bluesky instead.</p><p>The verdict? <i>It was actually pretty nice.</i></p><p>It felt a lot like the SC13 days, where my day ended and began with me popping open Bluesky to see what new <a href=\"https://bsky.app/hashtag/sc24\">#SC24</a> posts were made. And because many of the tech companies and HPC    centers hadn't yet made it over, the hashtag wasn't clogged up by a bunch of prescheduled marketing blasts that    buried the posts written by regular old conference attendees who were <a href=\"https://bsky.app/profile/walkingrandomly.bsky.social/post/3lbazofprgc2y\">asking important questions</a>:</p><blockquote class=\"bluesky-embed\"><p lang=\"en\">Which booths at #sc24 have coffee? I noticed oracle do. Anyone else?</p>— Mike Croucher (<a href=\"https://bsky.app/profile/did:plc:sd6xejkhcmyehbscxb5lz3uq?ref_src=embed\">@walkingrandomly.bsky.social</a>) <a href=\"https://bsky.app/profile/did:plc:sd6xejkhcmyehbscxb5lz3uq/post/3lbazofprgc2y?ref_src=embed\">November 18, 2024 at 3:02 PM</a></blockquote><p>Of course, I still clogged Bluesky up with my nonsense during the week, but there was an amazing amount of    engagement by a diversity of thoughtful people--many who came from Twitter, but some whose names and handles I    didn't recognize.</p><p>The volume of traffic on Bluesky during the week did feel a little lower than what it had been on Twitter in years    past though. I also didn't see as many live posts of technical sessions as they happened, so I couldn't really tell    whether I was missing something interesting in real time. This may have contributed to why I felt a little less    connected to the pulse of the conference this year than I had in the past. It also could've been the fact that    conference was physically smeared out across a massive space though; the sparsity of the convention center was at    least on par with the sparsity on Bluesky.</p><p>At the end of the week, I didn't regret the experiment. In fact, I'll probably be putting more effort into my Bluesky    account than my Twitter account going forward. To be clear though, this isn't a particularly political decision on    my part, and I pass no judgment on anyone who wants to use one platform over the other. It's just that I like the    way I feel when I scroll through my Bluesky feeds, and I don't get that same feeling when I use Twitter.</p><h2 id=\"conclusion\">So what's the takeaway?</h2><p>SC this year was a great conference by almost every measure, as it always is, but it still felt a little different for me. I'm sure that some of that feeling is the result of my own growth, and my role with respect to the conference seems to be evolving from someone who gets a lot out of the conference to someone who is giving more to the conference. That's not to say that I don't get a lot out of it, though; I had no shortage of wonderful interactions with everyone from technology executives to rising stars who are early in their career, and I learned a lot about both them and me as whole people. But SC24, more than any SC before it, is when I realized this change was happening.</p><p>On the technological front, we saw the debut of a new #1 system (emblazoned with the smiling face of Bronis...) and a growing crop of massive, new clusters deployed for commercial applications. The exhibit floor was quantitatively bigger, in large part due to new power and cooling companies who are suddenly relevant to the HPC world thanks to the momentum of AI. At the same time, the SC technical program is clearly separating itself out as a conference focused on scientific computing; the level of discourse around AI remains largely superficial compared to true AI conferences, the role of hyperscalers in the HPC industry is still cast more as a threat than an opportunity.</p><p>For my part, I'm still trying to get a grasp on where government agencies like DOE and NSF want to take their AI ambitions so I can try to help build a better mutual understanding between the scientific computing community and the hyperscale AI community. However, it seems like the NSF is progressing slowly on a wide front, while the DOE is doing what DOE does and charging headfirst into a landscape that has changed more than I think they realize.</p><p>There's a lot of technical content that I know I missed on account of the increasing time I've been spending on the people and community aspect of the conference, and I'm coming to terms with the idea that this just may be the way SC is from now on. And I think I'm okay with that, since the support of the community is what helped me go from being a bored materials science student into someone whose HPC career advice is worth soliciting in the short span of eleven years. Despite any or all of the cynicism that may come out in the things I say about this conference, SC is always the highlight of my year. I always go into it with excitement, gladly burn the candle at both ends all week, and fly home feeling both grateful for and humbled by everything the HPC community has done and continues to do to keep getting me out of bed in the morning.</p><p></p>",
            "url": "https://hpc.social/personal-blog/2024/sc-24-recap/",
            
            
            
            
            
            "date_published": "2024-12-02T07:30:00-07:00",
            "date_modified": "2024-12-02T07:30:00-07:00",
            
                "author": "Glenn K. Lockwood's Blog"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2024/surfing-the-singularity-the-workflow-is-the-app/",
            "title": "Surfing the Singularity - \"the Workflow is the App\"",
            "summary": null,
            "content_text": "Hello and happy fall holidays to you and yours. As I wrote about in the last blog post [1], as quantum computing hardware matures over the next 5 to 10 years from an experimental toy through to utility and then perhaps advantage over classical (for some applications), it will be included into an already diverse and hybrid computing and applications landscape - on-prem computing, mobile and edge, cloud, and now novel types of computing devices which require new thinking and wholly new means of addressing them. How to deal with the burgeoning heterogeneity of the computing landscape - how to write and run apps which produce and consume data across a widening array of devices- is the topic of this post. Language Landscape The Java programming language, once touted in the glory days of \"the World Wide Web\" as being \"write once, deploy anywhere\", and in its heyday representing 25% of new application development, is now down below 10%. What's hot? Python (23%), and \"the C's\", a collection of C, C++, C# and their kin (&gt;24% in total) which are traditionally recompiled for specific hardware platforms. [2] And while Python provides portability, often for performance in math operations it depends on native libraries, built in, you guessed it, the C's. Into this mix wades the US government which has come out recently with a potentially disruptive statement against the use of the C's, citing security concerns due to their free-wheeling memory management, and in spite of efforts like Safe C++, the government is recommending movement to memory safe languages like Rust, currently with just 1% market share, but \"with a bullet\". [3] Whether it is better to port to Rust or just update to Safe C++ depends on many factors - for example, how good are your docs and test cases - and while there may exist conceptual impedance mismatches between languages, modern AI coding assistants will only increase in capability especially for more rote tasks like porting.Add to this mix the coding of Graphical Processing Units (GPUs) - originally intended for visualizations but now used in applications for almost anything involving matrix math (turns out, lots of stuff). GPUs today are mostly sold by NVIDIA and are programmed in the C's (sometimes with a Python interface) using the NVIDIA CUDA library. These pieces of the application, the \"kernels\", are hardware dependent, and while many attempts have been made to create hardware-portable frameworks for GPU programming (see SYCL for example [4]), nearly always the newest fastest GPU features are available in the native non-portable form first, leading to vendor lock. (This might be a good time to remember that NVIDIA does not themselves manufacture chips - they design chips which others produce.)The manner in which we program GPUs is similar to the way we program quantum computers, i.e. QPUs - we delegate to them the portions of the application to which they are best suited, program them using device-specific instructions, and weave them back into the holistic solution. Rather than wielding the Java hammer where everything is a virtualized nail, we use the best tool for the job. In quantum computing, for example, \"variational\" hybrid algorithms are a common theme, where some part of the work and preparation are performed on classical hardware as a setup for a quantum step, and then post-processing the results back on classical hardware for potential iteration to an optimal solution. Two of several emerging patterns for integrating quantum computing into an application solution. [5]This pattern is analogous to what is also common in classical high performance computing (HPC) for applications like weather modeling and other complex simulations - pre-process on commodity hardware, run an HPC job on the big box, and post-process the results. The introduction into the mix of steerage provided by AI models increases the heterogeneity of the complete solution. A blended computing landscape, enabling for example, quantum computing to produce highly precise data to train AI to steer a classical HPC simulation. [6]All these hardware-dependent application pieces for an ever widening array of hardware means that compilers are cool again, and compiler pipelines like LLVM are critical to application development and deployment. [7] Included in this class of development tools are circuit transpilers for quantum hardware which must take into consideration not only the architectural differences between QPUs (e.g. which gates are supported, what's the inter-qubit connectivity like, etc.), but also the changes which can occur in a quantum data center on a daily basis as these new, noisy, and fragile qubits simply fail and go offline, potentially altering the machine's topology. Just-in-time compilation is needed, and compiler optimization is therefore also cool again. Thank you, Frances Allen. [8] Parts is PartsWhat emerges from this landscape is not a singular executable running on one computer, but rather, multiple application piece parts, written in different languages, running on radically different hardware in sequence and simultaneously, being orchestrated into a complete solution.In other words, a workflow. Back in the day Java's Sun Microsystems (remember them?) asserted \"the network is the computer\". Now we assert \"the workflow is the app\". Or more likely, a workflow of workflows. We like to think of these nested workflows in three types: [9]in-situ: the workflow is running all on the same machine (e.g. a local process, an HPC job)intra-site: the workflow is running on different machines within the same connected enterprise (e.g. within the same data center, virtual network, etc.)inter-site: the workflow is running across different machines in different enterprises (e.g. hybrid on-prem and perhaps multi-vendor cloud)With all these compute types, languages, and locations working together to realize the workflow and solution, loose coupling is key - components connected but not dependent - each part minding its own business. In other words, to paraphrase the poet, good interfaces make good neighbors. [10]We use the convenience term \"Site\" to mean a provider of secure compute and data services. What interfaces must a Site provide? The interface or API can include lots of things, but it must at least provide: 1) authentication and authorization, 2) a means to run components through their lifecycle, 3) a means to manage data being operated on and produced, perhaps being moved into and out of the Site, and 4) some way to get an inventory of the Site's service offerings and provision them for the purposes of running components or holding data. We call these by four functional nicknames: Auth, Run, Repo, and Spin. Four functional pillars of an interoperable computing site.We can see in each of the three types of workflows the need for each of these four functional pillars, albeit some as a no-op or inherited from a higher order workflow. For example, in a \"type 1\" workflow of components running on a single machine or within an HPC allocation the Auth aspects may be implied to be already addressed - i.e. the user is already logged into the machine or authorized to run on the HPC cluster. But a workflow which utilizes compute resources both on-prem and in the cloud will have to interact at runtime with the \"auth\" aspects of the cloud provider prior to being able to \"run\" workloads, or put and get data to various \"repos\". Most cloud providers provide a means to list available computing resources, to \"spin\" them up and down. This provisioning itself can be part of an end-to-end workflow: authenticate, get an inventory of available services, spin some up, run jobs on them storing the results, and spin them down. Stuck in the MiddleMost cloud providers - from Amazon to IBM Quantum cloud - provide a callable API interface which can be viewed through the lens of Auth, Run, Repo, Spin. So do some of the supercomputers and cutting edge resources provided by the Federal government, most notably those provided by the National Energy Research Scientific Computing Center (NERSC). [11] As Sites, these providers expose their offerings to internal and external workflows, however, they do not themselves promote a means to author these cross-site workflows, to manage them, track them, or keep tabs on all that distributed data. What else is needed? First, since cloud and other service providers have no motivation to standardize their interfaces, a framework super-interface could exist with the ability to plug in drivers for specific service providers. This in theory is the Auth, Run, Repo, Spin interface. Second, since each provider defines their own service and runtime component lifecycle (loosely: start, run, and stop with success or fail end states) there needs to be a way to normalize the status terminology - a \"fail\" on one site is the same as an \"error\" on another, \"success\" means the same thing as \"done\". This permits the third aspect of a middleware framework - the ability to track running jobs on Sites and trigger other jobs on any Site to run accordingly - i.e. the control flow of the workflow. What about the data? Commonly we need the ability to put data to a Site and get some back - this is the Repo interface of the Site. And while most (but not all) Sites provide some means to store and retrieve data, be it filesystem or S3 object store or database or something else, it would also be nice to be able to say something \"meta\" about the data - which Site did it come from, what job or application produced it, what other workflow steps on this Site or others consumed it? Some Sites provide storage with metadata (e.g. Amazon S3) but most don't. This metadata comprises the provenance of the data - like a Civil War sword on the Antiques Roadshow, its the paper trail showing where the item came from, proving the item is legit. In a workflow which perhaps produces many pieces of data, perhaps iteratively as it converges on a solution - keeping track of all the data pieces seems, well, important. The acronym FAIR - findable, accessible, interoperable, reusable - seems a good starting point. [12]Open Says MeOur open source project lwfm, the \"local workflow manager\", attempts to render these concepts as a reference implementation. [13] Its small with minimal Python lib dependencies and can be taken anywhere easily as a single runnable component, its provenancial metadata also easily portable and importable. A typical Site driver - a Python class which implements the Site interface - weighs in around 200 lines of code including the whitespace. Armed with a Site driver for a cloud service, you can author long-running workflows which utilize a mix of compute resources, storage, and data infrastructures, and automatically track the provenancial paper trail. The lwfm middleware component provides some very recognizable services:polling of remote job status status normalization and persistencesystem and user metadata persistenceevent handling, control flow and data flow triggeredShould you use this tooling? I wouldn't recommend it. (Huh? Did I hear you correctly?) How many people are working maintaining it? (Two?) What about the community? (Next to none.) The software would fare poorly on a \"spider web\" analysis of its overall quality - you would not want to recommend it to your boss.A convenient multi-axis assessment framework for software model maturity. [14]The lwfm is a reference implementation of a workflow interop framework, at best. Are there alternatives? OMG are there alternatives! The workflow landscape is notoriously rich, fragmented, and super-niched. But portability and interoperability are often neglected as is data provenance. Government or university projects, while well meaning and sometimes directionally correct, quickly go stale when the funding elapses [15], and commercial solutions while often suffering some of the same deficiencies offer the added trap of vendor lock and can come with a hefty price tag.Order, OrderSo its back to committee. [16] Next week the high performance computing community will be meeting again at the SC Conference Series Supercomputing 2024, this year in Atlanta. Hybrid workflows for scientific and engineering applications - involving classical HPC, AI-focused clusters, and now also quantum computers - will be among the very many topics discussed.[17] And we should expect some surprises - in the new rankings for example of top machines on the planet, at least, the ones they want us to know about. [18]Perhaps I'll report back on some of those returns in a future blog. Best regards. - andy References &amp; Amusements [0] Banner photo by Ben Wicks on Unsplash[1] \"Surfing the Singularity: The Universe Computes\", A. Gallo, https://www.linkedin.com/pulse/surfing-singularity-universe-computes-andy-gallo-6fgle[2] TIOBE ranking of programming language popularity: https://www.tiobe.com/tiobe-index/[3] Safe C++, with some chronology of the government statements: https://safecpp.org/[4] SYCL: https://www.khronos.org/sycl/[5] \"Post-variational quantum neural networks\", https://pennylane.ai/qml/demos/tutorial_post-variational_quantum_neural_networks[6] \"Hope Versus Hype: Quantum, AI and the Path to Commercial Advantage\", Matthias Troyer, presentation at IEEE Quantum Week, Montreal, September 2024.[7] LLVM: https://llvm.org/[8] https://amturing.acm.org/award_winners/allen_1012327.cfm[9] \"Industrial Experience Deploying Heterogeneous Platforms for Use in Multi-Modal Power Systems Design Workflows\", A. Gallo et al, https://drive.google.com/file/d/1c3YEVmEAUjbI5urj4PiV2TtjzBUzLlws[10] \"Mending Wall, Robert Frost, https://www.poetryfoundation.org/poems/44266/mending-wall[11] NERSC SuperFacility API: https://docs.nersc.gov/services/sfapi/[12] \"The FAIR GuidingPrinciples for scientific data management and stewardship\", Mark D. Wilkinson et al., https://pmc.ncbi.nlm.nih.gov/articles/PMC4792175/pdf/sdata201618.pdf[13] lwfm, https://github.com/lwfm-proj/lwfm [14] \"Model Maturity Web\", https://richardarthur.medium.com/co-design-web-6f37664ac1e1[15] Them's fighting words, and I expect to be roasted for it. But it seems to me that even the most popular software tool kits (no names) which emerged from the massively government funded ExaScale Computing Project failed to gain traction outside of a narrow community, failed to provide sustainable maintenance in the face of the funded end of the ECP, and would thus fair similarly poorly on a spider web analysis of their sustainability, their recommendability. [16] \"Workflows Community Summit 2024: Future Trends and Challenges in Scientific Workflows\", da Silva et al, \"https://zenodo.org/records/13844759. I participated in the event, as well as the prior in 2022, and you can compare to that report as well: \"Workflows Community Summit 2022: A Roadmap Revolution\", also da Silva et al, https://zenodo.org/records/7750670.[17] SC24, https://sc24.conference-program.com/[18] TOP 500 supercomputers, June 2024, https://top500.org/lists/top500/list/2024/06/ - to be updated again before Thanksgiving. ",
            "content_html": "<p class=\"ember-view reader-text-block__paragraph\" id=\"ember2131\"><span color=\"rgba(255, 255, 255, 0.9)\" style=\"font-family: verdana;\">Hello and happy fall holidays to you and yours.</span><span class=\"white-space-pre\" color=\"rgba(255, 255, 255, 0.9)\"> </span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2131\"><span style=\"font-family: verdana;\">As I wrote about in the last blog post [<a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://www.linkedin.com/pulse/surfing-singularity-universe-computes-andy-gallo-6fgle\" target=\"_self\">1</a>], as quantum computing hardware matures over the next 5 to 10 years from an experimental toy through to utility and then perhaps advantage over classical (for some applications), it will be included into an already diverse and hybrid computing and applications landscape - on-prem computing, mobile and edge, cloud, and now novel types of computing devices which require new thinking and wholly new means of addressing them. How to deal with the burgeoning heterogeneity of the computing landscape - how to write and run apps which produce and consume data across a widening array of devices- is the topic of this post.<span class=\"white-space-pre\"> </span></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2131\"><span style=\"font-family: verdana;\"><span class=\"white-space-pre\"><br /></span></span></p><h2><span style=\"font-family: verdana; font-size: x-large;\">Language Landscape<span class=\"white-space-pre\"> </span></span></h2><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2133\"><span style=\"font-family: verdana;\">The Java programming language, once touted in the glory days of \"the World Wide Web\" as being \"write once, deploy anywhere\", and in its heyday representing 25% of new application development, is now down below 10%. What's hot? Python (23%), and \"the C's\", a collection of C, C++, C# and their kin (&gt;24% in total) which are traditionally recompiled for specific hardware platforms. [2] And while Python provides portability, often for performance in math operations it depends on native libraries, built in, you guessed it, the C's. Into this mix wades the US government which has come out recently with a potentially disruptive statement against the use of the C's, citing security concerns due to their free-wheeling memory management, and in spite of efforts like Safe C++, the government is recommending movement to memory safe languages like Rust, currently with just 1% market share, but \"with a bullet\". [3] Whether it is better to port to Rust or just update to Safe C++ depends on many factors - for example, how good are your docs and test cases - and while there may exist conceptual impedance mismatches between languages, modern AI coding assistants will only increase in capability especially for more rote tasks like porting.</span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2134\"><span style=\"font-family: verdana;\">Add to this mix the coding of Graphical Processing Units (GPUs) - originally intended for visualizations but now used in applications for almost anything involving matrix math (turns out, lots of stuff). GPUs today are mostly sold by NVIDIA and are programmed in the C's (sometimes with a Python interface) using the NVIDIA CUDA library. These pieces of the application, the \"kernels\", are hardware dependent, and while many attempts have been made to create hardware-portable frameworks for GPU programming (see SYCL for example [4]), nearly always the newest fastest GPU features are available in the native non-portable form first, leading to vendor lock. (This might be a good time to remember that<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://www.linkedin.com/company/nvidiausa/\">NVIDIA</a><span class=\"white-space-pre\"> </span>does not themselves manufacture chips - they design chips which others produce.)</span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2135\"><span style=\"font-family: verdana;\">The manner in which we program GPUs is similar to the way we program quantum computers, i.e. QPUs - we delegate to them the portions of the application to which they are best suited, program them using device-specific instructions, and weave them back into the holistic solution. Rather than wielding the Java hammer where everything is a virtualized nail, we use the best tool for the job. In quantum computing, for example, \"variational\" hybrid algorithms are a common theme, where some part of the work and preparation are performed on classical hardware as a setup for a quantum step, and then post-processing the results back on classical hardware for potential iteration to an optimal solution.<span class=\"white-space-pre\"> </span></span></p><div class=\"reader-image-block reader-image-block--full-width\"><figure class=\"reader-image-block__figure\"><div class=\"ivm-image-view-model\"><div class=\"ivm-view-attr__img-wrapper\"><img alt=\"\" class=\"ivm-view-attr__img--centered reader-image-block__img evi-image lazy-image ember-view\" id=\"ember2136\" src=\"https://media.licdn.com/dms/image/v2/D4E12AQHHLPGy-FVQ7g/article-inline_image-shrink_1500_2232/article-inline_image-shrink_1500_2232/0/1731379488723?e=1740009600&amp;v=beta&amp;t=rBfADA-gBrGoIdiIzfxE-P90i-WNLV2EFP7uYQzam20\" /></div></div><figcaption class=\"reader-image-block__figure-image-caption display-block full-width text-body-small-open t-sans text-align-center t-black--light\"><span style=\"font-family: verdana;\">Two of several emerging patterns for integrating quantum computing into an application solution. [5]</span></figcaption></figure></div><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2137\"><span style=\"font-family: verdana;\">This pattern is analogous to what is also common in classical high performance computing (HPC) for applications like weather modeling and other complex simulations - pre-process on commodity hardware, run an HPC job on the big box, and post-process the results. The introduction into the mix of steerage provided by AI models increases the heterogeneity of the complete solution.<span class=\"white-space-pre\"> </span></span></p><div class=\"reader-image-block reader-image-block--full-width\"><figure class=\"reader-image-block__figure\"><div class=\"ivm-image-view-model\"><div class=\"ivm-view-attr__img-wrapper\"><img alt=\"\" class=\"ivm-view-attr__img--centered reader-image-block__img evi-image lazy-image ember-view\" id=\"ember2138\" src=\"https://media.licdn.com/dms/image/v2/D4E12AQGVu8gKIzDRLg/article-inline_image-shrink_1000_1488/article-inline_image-shrink_1000_1488/0/1731379555676?e=1740009600&amp;v=beta&amp;t=Bt4u6m8pi0i6xVNmfWxkxGK4bXBGkWuP3_zxrab1R3M\" /></div></div><figcaption class=\"reader-image-block__figure-image-caption display-block full-width text-body-small-open t-sans text-align-center t-black--light\"><span style=\"font-family: verdana;\">A blended computing landscape, enabling for example, quantum computing to produce highly precise data to train AI to steer a classical HPC simulation. [6]</span></figcaption></figure></div><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2139\"><span style=\"font-family: verdana;\">All these hardware-dependent application pieces for an ever widening array of hardware means that compilers are cool again, and compiler pipelines like LLVM are critical to application development and deployment. [7] Included in this class of development tools are circuit transpilers for quantum hardware which must take into consideration not only the architectural differences between QPUs (e.g. which gates are supported, what's the inter-qubit connectivity like, etc.), but also the changes which can occur in a quantum data center on a daily basis as these new, noisy, and fragile qubits simply fail and go offline, potentially altering the machine's topology. Just-in-time compilation is needed, and compiler optimization is therefore also cool again. Thank you, Frances Allen. [8]<span class=\"white-space-pre\"> </span></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2139\"><span style=\"font-family: verdana;\"><span class=\"white-space-pre\"><br /></span></span></p><h3 class=\"ember-view reader-text-block__heading-3\" id=\"ember2140\"><span style=\"font-family: verdana; font-size: x-large;\">Parts is Parts</span></h3><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2141\"><span style=\"font-family: verdana;\">What emerges from this landscape is not a singular executable running on one computer, but rather, multiple application piece parts, written in different languages, running on radically different hardware in sequence and simultaneously, being orchestrated into a complete solution.</span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2142\"><span style=\"font-family: verdana;\">In other words, a workflow. Back in the day Java's Sun Microsystems (remember them?) asserted \"the network is the computer\". Now we assert \"the workflow is the app\".<span class=\"white-space-pre\"> </span></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2143\"><span style=\"font-family: verdana;\">Or more likely, a workflow of workflows. We like to think of these nested workflows in three types: [9]</span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2144\"></p><ol><li><span style=\"font-family: verdana;\"><span>in-situ</span>: the workflow is running all on the same machine (e.g. a local process, an HPC job)</span></li><li><span style=\"font-family: verdana;\"><span>intra-site</span>: the workflow is running on different machines within the same connected enterprise (e.g. within the same data center, virtual network, etc.)</span></li><li><span style=\"font-family: verdana;\"><span>inter-site</span>: the workflow is running across different machines in different enterprises (e.g. hybrid on-prem and perhaps multi-vendor cloud)</span></li></ol><p></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2145\"><span style=\"font-family: verdana;\">With all these compute types, languages, and locations working together to realize the workflow and solution, loose coupling is key - components connected but not dependent - each part minding its own business. In other words, to paraphrase the poet, good interfaces make good neighbors. [10]</span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2146\"><span style=\"font-family: verdana;\">We use the convenience term \"Site\" to mean a provider of secure compute and data services. What interfaces must a Site provide? The interface or API can include lots of things, but it must at least provide: 1) authentication and authorization, 2) a means to run components through their lifecycle, 3) a means to manage data being operated on and produced, perhaps being moved into and out of the Site, and 4) some way to get an inventory of the Site's service offerings and provision them for the purposes of running components or holding data. We call these by four functional nicknames: Auth, Run, Repo, and Spin.<span class=\"white-space-pre\"> </span></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2147\"><span style=\"font-family: verdana;\"><br /></span></p><div class=\"reader-image-block reader-image-block--resize\"><figure class=\"reader-image-block__figure\"><div class=\"ivm-image-view-model\"><div class=\"ivm-view-attr__img-wrapper\"><img alt=\"\" class=\"ivm-view-attr__img--centered reader-image-block__img evi-image lazy-image ember-view\" id=\"ember2148\" src=\"https://media.licdn.com/dms/image/v2/D4E12AQGVO-K51OaPPw/article-inline_image-shrink_1500_2232/article-inline_image-shrink_1500_2232/0/1731379758535?e=1740009600&amp;v=beta&amp;t=t0p27qST2ZLG6ETfkhu-0OppATVBXdcfX7Mgnw1Qk2A\" /></div></div><figcaption class=\"reader-image-block__figure-image-caption display-block full-width text-body-small-open t-sans text-align-center t-black--light\"><span style=\"font-family: verdana;\">Four functional pillars of an interoperable computing site.</span></figcaption></figure></div><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2149\"><span style=\"font-family: verdana;\">We can see in each of the three types of workflows the need for each of these four functional pillars, albeit some as a no-op or inherited from a higher order workflow. For example, in a \"type 1\" workflow of components running on a single machine or within an HPC allocation the Auth aspects may be implied to be already addressed - i.e. the user is already logged into the machine or authorized to run on the HPC cluster. But a workflow which utilizes compute resources both on-prem and in the cloud will have to interact at runtime with the \"auth\" aspects of the cloud provider prior to being able to \"run\" workloads, or put and get data to various \"repos\". Most cloud providers provide a means to list available computing resources, to \"spin\" them up and down. This provisioning itself can be part of an end-to-end workflow: authenticate, get an inventory of available services, spin some up, run jobs on them storing the results, and spin them down.</span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2149\"><span style=\"font-family: verdana;\"><span class=\"white-space-pre\"> </span></span></p><h3 class=\"ember-view reader-text-block__heading-3\" id=\"ember2150\"><span style=\"font-family: verdana; font-size: x-large;\">Stuck in the Middle</span></h3><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2151\"><span style=\"font-family: verdana;\">Most cloud providers - from Amazon to IBM Quantum cloud - provide a callable API interface which can be viewed through the lens of Auth, Run, Repo, Spin. So do some of the supercomputers and cutting edge resources provided by the Federal government, most notably those provided by the<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://www.linkedin.com/company/national-energy-research-scientific-computing-center/\">National Energy Research Scientific Computing Center (NERSC)</a>. [11]<span class=\"white-space-pre\"> </span></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2152\"><span style=\"font-family: verdana;\">As Sites, these providers expose their offerings to internal and external workflows, however, they do not themselves promote a means to author these cross-site workflows, to manage them, track them, or keep tabs on all that distributed data. What else is needed? First, since cloud and other service providers have no motivation to standardize their interfaces, a framework super-interface could exist with the ability to plug in drivers for specific service providers. This in theory is the Auth, Run, Repo, Spin interface. Second, since each provider defines their own service and runtime component lifecycle (loosely: start, run, and stop with success or fail end states) there needs to be a way to normalize the status terminology - a \"fail\" on one site is the same as an \"error\" on another, \"success\" means the same thing as \"done\". This permits the third aspect of a middleware framework - the ability to track running jobs on Sites and trigger other jobs on any Site to run accordingly - i.e. the control flow of the workflow.<span class=\"white-space-pre\"> </span></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2153\"><span style=\"font-family: verdana;\">What about the data? Commonly we need the ability to put data to a Site and get some back - this is the Repo interface of the Site. And while most (but not all) Sites provide some means to store and retrieve data, be it filesystem or S3 object store or database or something else, it would also be nice to be able to say something \"meta\" about the data - which Site did it come from, what job or application produced it, what other workflow steps on this Site or others consumed it? Some Sites provide storage with metadata (e.g. Amazon S3) but most don't. This metadata comprises the provenance of the data - like a Civil War sword on the Antiques Roadshow, its the paper trail showing where the item came from, proving the item is legit. In a workflow which perhaps produces many pieces of data, perhaps iteratively as it converges on a solution - keeping track of all the data pieces seems, well, important. The acronym FAIR - findable, accessible, interoperable, reusable - seems a good starting point. [12]</span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2153\"><span style=\"font-family: verdana;\"><br /></span></p><h3 class=\"ember-view reader-text-block__heading-3\" id=\"ember2154\"><span style=\"font-family: verdana; font-size: x-large;\">Open Says Me</span></h3><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2155\"><span style=\"font-family: verdana;\">Our open source project lwfm, the \"local workflow manager\", attempts to render these concepts as a reference implementation. [13] Its small with minimal Python lib dependencies and can be taken anywhere easily as a single runnable component, its provenancial metadata also easily portable and importable. A typical Site driver - a Python class which implements the Site interface - weighs in around 200 lines of code including the whitespace. Armed with a Site driver for a cloud service, you can author long-running workflows which utilize a mix of compute resources, storage, and data infrastructures, and automatically track the provenancial paper trail.<span class=\"white-space-pre\"> </span></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2156\"><span style=\"font-family: verdana;\">The lwfm middleware component provides some very recognizable services:</span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2157\"></p><ul><li><span style=\"font-family: verdana;\">polling of remote job status<span class=\"white-space-pre\"> </span></span></li><li><span style=\"font-family: verdana;\">status normalization and persistence</span></li><li><span style=\"font-family: verdana;\">system and user metadata persistence</span></li><li><span style=\"font-family: verdana;\">event handling, control flow and data flow triggered</span></li></ul><p></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2158\"><span style=\"font-family: verdana;\">Should you use this tooling? I wouldn't recommend it. (Huh? Did I hear you correctly?) How many people are working maintaining it? (Two?) What about the community? (Next to none.) The software would fare poorly on a \"spider web\" analysis of its overall quality - you would not want to recommend it to your boss.</span></p><div class=\"reader-image-block reader-image-block--resize\"><figure class=\"reader-image-block__figure\"><div class=\"ivm-image-view-model\"><div class=\"ivm-view-attr__img-wrapper\"><img alt=\"\" class=\"ivm-view-attr__img--centered reader-image-block__img evi-image lazy-image ember-view\" id=\"ember2159\" src=\"https://media.licdn.com/dms/image/v2/D4E12AQEs4kk5AG5gIA/article-inline_image-shrink_1500_2232/article-inline_image-shrink_1500_2232/0/1731379896216?e=1740009600&amp;v=beta&amp;t=AMlOWYzQl60i9SQsBL77teawfvRVlFW_7l_yHoT4Nnk\" /></div></div><figcaption class=\"reader-image-block__figure-image-caption display-block full-width text-body-small-open t-sans text-align-center t-black--light\"><span style=\"font-family: verdana;\">A convenient multi-axis assessment framework for software model maturity. [14]</span></figcaption></figure></div><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2160\"><span style=\"font-family: verdana;\">The lwfm is a reference implementation of a workflow interop framework, at best. Are there alternatives? OMG are there alternatives! The workflow landscape is notoriously rich, fragmented, and super-niched. But portability and interoperability are often neglected as is data provenance. Government or university projects, while well meaning and sometimes directionally correct, quickly go stale when the funding elapses [15], and commercial solutions while often suffering some of the same deficiencies offer the added trap of vendor lock and can come with a hefty price tag.</span></p><h3 class=\"ember-view reader-text-block__heading-3\" id=\"ember2161\"><span style=\"font-family: verdana;\">Order, Order</span></h3><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2162\"><span style=\"font-family: verdana;\">So its back to committee. [16] Next week the high performance computing community will be meeting again at the<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://www.linkedin.com/company/sc-conference/\">SC Conference Series</a><span class=\"white-space-pre\"> </span>Supercomputing 2024, this year in Atlanta. Hybrid workflows for scientific and engineering applications - involving classical HPC, AI-focused clusters, and now also quantum computers - will be among the very many topics discussed.[17] And we should expect some surprises - in the new rankings for example of top machines on the planet, at least, the ones they want us to know about. [18]</span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2163\"><span style=\"font-family: verdana;\">Perhaps I'll report back on some of those returns in a future blog. Best regards. - andy<span class=\"white-space-pre\"> </span></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2164\"><span style=\"font-family: verdana;\"><br /></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2165\"><span><span style=\"font-family: verdana; font-size: x-large;\">References &amp; Amusements<span class=\"white-space-pre\"> </span></span></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2166\"><span style=\"font-family: verdana;\">[0] Banner photo by Ben Wicks on Unsplash</span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2167\"><span style=\"font-family: verdana;\">[1] \"Surfing the Singularity: The Universe Computes\", A. Gallo,<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://www.linkedin.com/pulse/surfing-singularity-universe-computes-andy-gallo-6fgle\" target=\"_self\">https://www.linkedin.com/pulse/surfing-singularity-universe-computes-andy-gallo-6fgle</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2168\"><span style=\"font-family: verdana;\">[2] TIOBE ranking of programming language popularity:<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://www.tiobe.com/tiobe-index/\" target=\"_self\">https://www.tiobe.com/tiobe-index/</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2169\"><span style=\"font-family: verdana;\">[3] Safe C++, with some chronology of the government statements:<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://safecpp.org/\" target=\"_self\">https://safecpp.org/</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2170\"><span style=\"font-family: verdana;\">[4] SYCL:<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://www.khronos.org/sycl/\" target=\"_self\">https://www.khronos.org/sycl/</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2171\"><span style=\"font-family: verdana;\">[5] \"Post-variational quantum neural networks\",<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://pennylane.ai/qml/demos/tutorial_post-variational_quantum_neural_networks\" target=\"_self\">https://pennylane.ai/qml/demos/tutorial_post-variational_quantum_neural_networks</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2172\"><span style=\"font-family: verdana;\">[6] \"Hope Versus Hype: Quantum, AI and the Path to Commercial Advantage\", Matthias Troyer, presentation at IEEE Quantum Week, Montreal, September 2024.</span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2173\"><span style=\"font-family: verdana;\">[7] LLVM:<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://llvm.org/\" target=\"_self\">https://llvm.org/</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2174\"><span style=\"font-family: verdana;\">[8]<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://amturing.acm.org/award_winners/allen_1012327.cfm\" target=\"_self\">https://amturing.acm.org/award_winners/allen_1012327.cfm</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2175\"><span style=\"font-family: verdana;\">[9] \"Industrial Experience Deploying Heterogeneous Platforms for Use in Multi-Modal Power Systems Design Workflows\", A. Gallo et al,<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://drive.google.com/file/d/1c3YEVmEAUjbI5urj4PiV2TtjzBUzLlws\" target=\"_self\">https://drive.google.com/file/d/1c3YEVmEAUjbI5urj4PiV2TtjzBUzLlws</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2176\"><span style=\"font-family: verdana;\">[10] \"Mending Wall, Robert Frost,<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://www.poetryfoundation.org/poems/44266/mending-wall\" target=\"_self\">https://www.poetryfoundation.org/poems/44266/mending-wall</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2177\"><span style=\"font-family: verdana;\">[11] NERSC SuperFacility API:<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://docs.nersc.gov/services/sfapi/\" target=\"_self\">https://docs.nersc.gov/services/sfapi/</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2178\"><span style=\"font-family: verdana;\">[12] \"The FAIR Guiding</span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2179\"><span style=\"font-family: verdana;\">Principles for scientific data management and stewardship\", Mark D. Wilkinson et al.,<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://pmc.ncbi.nlm.nih.gov/articles/PMC4792175/pdf/sdata201618.pdf\" target=\"_self\">https://pmc.ncbi.nlm.nih.gov/articles/PMC4792175/pdf/sdata201618.pdf</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2180\"><span style=\"font-family: verdana;\">[13] lwfm,<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://github.com/lwfm-proj/lwfm\" target=\"_self\">https://github.com/lwfm-proj/lwfm</a><span class=\"white-space-pre\"> </span></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2181\"><span style=\"font-family: verdana;\">[14] \"Model Maturity Web\",<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://richardarthur.medium.com/co-design-web-6f37664ac1e1\" target=\"_self\">https://richardarthur.medium.com/co-design-web-6f37664ac1e1</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2182\"><span style=\"font-family: verdana;\">[15] Them's fighting words, and I expect to be roasted for it. But it seems to me that even the most popular software tool kits (no names) which emerged from the massively government funded ExaScale Computing Project failed to gain traction outside of a narrow community, failed to provide sustainable maintenance in the face of the funded end of the ECP, and would thus fair similarly poorly on a spider web analysis of their sustainability, their recommendability.<span class=\"white-space-pre\"> </span></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2183\"><span style=\"font-family: verdana;\">[16] \"Workflows Community Summit 2024: Future Trends and Challenges in Scientific Workflows\", da Silva et al, \"<a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://zenodo.org/records/13844759\" target=\"_self\">https://zenodo.org/records/13844759</a>. I participated in the event, as well as the prior in 2022, and you can compare to that report as well: \"Workflows Community Summit 2022: A Roadmap Revolution\", also da Silva et al,<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://zenodo.org/records/7750670\" target=\"_self\">https://zenodo.org/records/7750670</a>.</span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2184\"><span style=\"font-family: verdana;\">[17] SC24,<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://sc24.conference-program.com/\" target=\"_self\">https://sc24.conference-program.com/</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember2185\"><span style=\"font-family: verdana;\">[18] TOP 500 supercomputers, June 2024,<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo\" href=\"https://top500.org/lists/top500/list/2024/06/\" target=\"_self\">https://top500.org/lists/top500/list/2024/06/</a><span class=\"white-space-pre\"> </span>- to be updated again before Thanksgiving.<span class=\"white-space-pre\"> </span></span></p>",
            "url": "https://hpc.social/personal-blog/2024/surfing-the-singularity-the-workflow-is-the-app/",
            
            
            
            
            
            "date_published": "2024-11-12T17:00:00-07:00",
            "date_modified": "2024-11-12T17:00:00-07:00",
            
                "author": "Surfing the Singularity"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2024/surfing-the-singularity-the-universe-computes/",
            "title": "Surfing the Singularity - The Universe Computes",
            "summary": null,
            "content_text": "Just back from the IEEE Computer Society Quantum Week in Montreal, and besides eating my weight in pastry and bagels [1], it was a great conference. The collective hardware roadmaps from the major players leaves us thinking the big wave in quantum computing is not here yet, but soon - perhaps in 5 years time for scientific applications, and within a decade for commercial utility. While the current software continues to be sparse and low-level, there are inklings of software engineers starting to build up a stack in anticipation of needing one. But besides that, and at the risk of sounding like the other hot topic - AI marketeer hype [2] - there is the sense with quantum of being present at a new phase in computing at least, if not something larger still. The concept of the computing universe is still just a hypothesis; nothing has been proved. However, I am confident that this idea can help unveil the secrets of nature. - Konrad Zuse, 1969 [3]It seems, at its core, that the universe computes. Conch shells grow in logarithmic spirals, bees and orb weavers understand structural geometry. Many animals - not including Mr. Ed or Clever Hans [4], but including primates, fish, and rats - have been shown to use simple arithmetic or approximations, and in other cases can show ability to order objects in a list. There are lizards and sea shells with surface patterns constructed by cellular automata processes - simple rules which can produce complex structures, like Conway's \"Game of Life\". Ducks, using an inherently quantum mechanical biological process, can see magnetic fields giving them a kind of \"heads up\" display when migrating. [5] A variation on Conway's \"Game of Life\" [6]Our own eyes are themselves literally photon detectors, our retinas and optic nerves pre-processing the signals before they even get to the brain. DNA stores vast amounts of information in simple patterns of four \"letters\" (effectively two classical bits), recipes to manufacture from the raw material of the universe a wide array of proteins for all manner of biological purposes, including of course growing your own brain. Humans can themselves perform the manual calculations necessary to build bridges and other structures which can span and withstand the forces of nature for centuries. Its also not hard to see computation of a kind in plants and their systemic networks of roots. Is the universe computing, or is the universe doing the only thing it can based on the rules? Water flowing downhill. Lightning finding its own path of least resistance. Entangled electrons separated at distance flipping their spin in response to their partner, in real time. [7]&lt;div class=\"reader-embed-block__iframe-embed\"&gt;&lt;/div&gt;An Entangled HistoryNature isn't classical, dammit, and if you want to make a simulation of nature, you'd better make it quantum mechanical. - Richard Feynman [8] I think I can safely say that nobody understands quantum mechanics. - Richard Feynman [9]In the 1920s physicists like Heisenberg, Born, Pauli, and Schrödinger convinced their peers of the validity of a new formulation of the laws of physics they called (in German) \"quantum mechanics\", describing the behavior of nature and the universe even below the scale of atoms. This then led computing pioneer John Von Neumann in the 1930s to solidify some of the necessary maths to perform discrete quantum mechanical calculations. Von Neumann, a member of the Manhattan Project, would go on to formulate the hardware architecture for today's \"classical\" computers, the approach to computing being challenged by quantum computing today. Since then, mostly notably in the 1940s with the Manhattan Project, humans have shown an increasing ability to harness the basic quantum physics into a new range of applications. We learned how to make superconductors, and how to park and manipulate individual atoms like tinkertoys [10], and now we've learned how to use these skills to make computers, with most immediate applications in modeling quantum systems like atoms and molecules, as Nobel laureate and Manhattan Project member Feynman predicted more than 40 years ago. We learned how to make lasers and LEDs, and we can now also harness photons for computing. The scientists have had nearly a century to refine their theories, and are now handing off to the engineers to prove the depth of human understanding by building things in the real world, with the business people eagerly waiting on the sidelines in anticipation (think: MRI machines). It seems that the universe computes - we now endeavor to make use of that knowledge for our own human purposes. Qubits, Gates, and Error EverywhereWhat is a qubit? Like the early video game Q*bert which showed a simulated 3D world on a 2D screen back in 1982, a qubit is a little hard to visualize as it goes deeper than a classical binary bit - much deeper. While a bit can be either a zero or a one but nothing else, a qubit can model the probability of a zero or a one and probabilistically anything in between. It might be a zero, or it might be a one, with some probability of each. It might start off a zero, and then noise from the environment might cause it to drift, or it might move off the zero purposefully as a result of acting on the qubit with one of several kinds of single and multi-qubit gates. Like gates in classical computing, a quantum gate can flip the qubit, or unique to quantum just nudge it a little. As in classical computing, its the acting on the qubit by gates which results in the computing. A native quantum program is a circuit, a directed graph, composed of gates. Visualizing the effect of various quantum gates on a single qubit. [11] How fast can we flip a qubit? Heisenberg 100 years ago gave us a way to compute the lower bound on the time to flip a spin given an energy - in short, its fast. But this doesn't even tell the whole performance story because of superposition and the ability of wide circuits to act on multiple qubits simultaneously - the speed advantage of quantum over classical can be exponential, albeit application-specific, making a class of problems which would be classically uncomputable in any human lifetime now well within reach. The main challenge in realizing these potentials is the noise. Qubits are noisy, meaning they don't stay fixed where you think you last left them, and the seemingly magical entanglement also can show decoherence over time and distance. The gates necessary for computation can themselves introduce noisy error, as can the act of measuring the qubit to sample the solution result. Software algorithms can also just be estimates, and thus introduce their own error term relative to experiment. The prevalence of error in quantum computing means the algorithms themselves must be aware the results might be unpredictable, might need to be computed more then once to improve confidence, and might need to allocate and use a good number of precious qubits just to help mitigate the errors. We call the period we are in today the \"NISQ era\", meaning, noisy intermediate scale quantum computing. Beyond the noise, how do we quantify \"scale\" or other metrics for sizing up the capabilities of quantum computers, now, in future, and as compared to classical? One aspect of the problem is that when comparing quantum to classical we're not comparing apples to apples, and even within \"quantum apples\" as we have seen, there are different kinds. [12] In one simple measure we can count qubits, but we must also know their error rate, and we must notice something about their connectedness - some quantum hardware use an all-to-all grid, and others use other topologies - racetracks and the like. And qubits can fail. Because of the limitations of qubits in the NISQ era the connectedness matters, is necessary to be known at time of circuit transpilation, and may result in swaps or other strategies employed by the transpiler toolchain to minimize errors due to the physical layout. Qubit coherence in superposition can be measured and reported as a hardware spec. Quantum volume is a number which expresses the size of a circuit N qubits wide by d gates deep which can be executed on a given machine. Gate errors especially for 2-qubit gates can be reported by the vendor. CLOPS - circuit layer operations per second - is another proposed metric which takes into consideration the time to prepare the qubits, execute the gates, and take the measurement of the result. [13] The US government in the form of Defense Advanced Research Projects Agency (DARPA) has gotten into the game of studying this varied performance landscape, towards being able to help pick winners and losers and accelerate innovation with funding awards. [14] Quantum Hardware RoadmapI build quantum computers that store information on individual atoms and then massage the normal interactions between atoms to make them compute. - Seth Lloyd [15]This year's IEEE Quantum Week was an opportunity to see and hear from most of the major players in quantum computing R&amp;D - those focused on quantum processors, systems control, networking, and software. The software topic we'll leave as a topic of a future blog, but focusing on hardware, the vendors collectively represented multiple distinct technical mechanisms to making a quantum computing machine. There's superconducting qubits from US companies like IBM Quantum , Google , and Rigetti Computing, which refrigerate and maintain the qubit between a ground and an excited state. Trapped ion computers from companies like IonQ and Quantinuum , and neutral atom computers from QuEra Computing Inc. use novel methods to again cool the system qubits to near absolute zero. But there are quantum computers which also operate quite differently. Quantum annealing, or algorithmically simulating an adiabatic process for slowly evolving a system to an optimal state, could be simulated on one of the above general quantum machines, or shown more directly on a specialized quantum machine from a Canadian company like D-Wave. Xanadu, also based in Canada, performs its quantum tricks with photonics. And while Google may jump the gun on announcing successes from time to time, they and Microsoft and others are working on a \"topological qubit\" based on previously only-hypothesized Majorana particles which provide the great advantage relative to other qubit implementations of being able to be controlled digitally. [16, 17] Staying within the NISQ era as the machines scale up, a good chunk of the available qubits will continue to be allocated to error correction schemes, a task which may later as these systems mature be allocated to a software layer. At 100s of useful error-corrected qubits we can start to gain real scientific utility from quantum computing - begin to do research with quantum rather than research about quantum. Vendors such as Quantinuum promise a fully connected machine of that size in 5 years. In 10 years, vendors expect to deliver machines with 1000s of QEC, which will usher in commercial utility, and the era of \"cryptographically relevant\" quantum computing (i.e. DARPA wants the US to get there first [18]). In the meantime, certain scientific domains, those which study things most closely associated with real quantum systems, will be early adopters of the technology. Molecular biology. Chemistry, for example, studying better ways to perform synthetic nitrogen fixation (think: energy-costly ammonia production for fertilizers). Conclusion The quantum hardware industry is in its infancy. From this gaggle of eager go-getters it’s reasonable to assume there will be technical and business winners and losers. For reasons of national security, governments will ramp up their involvement. But current machines are small, flaky, and limited in usefulness. It will be 5 to 10 years before there are quantum computers being used more commonly. A new but also a familiar approach to software will be needed - more on that in a future blog. Until utility some industries will be early leaders, ready to capitalize on an exponential increase in computing capability, one which promises to get us closer to harnessing the grand computing engine of the universe which is all around and within.References &amp; Trivia[0] Photo by Ben Wicks on Unsplash[1] Montreal bagels: https://www.mtl.org/en/experience/the-famous-montreal-bagel[2] Gartner AI Hype Cycle 2024 explained: https://www.youtube.com/watch?v=qXKYOR3KqxQ [3] \"Calculating Space\", Konrad Zuse, 1969, https://philpapers.org/archive/ZUSRR.pdf Its worth noting that while having worked for Ford Motor Co. in his early career, and like Von Neumann doing very important early work on computers, Zuse was a conscripted employee of the German Nazi government from 1939-1945.[4] Clever Hans: https://www.horsejournals.com/popular/history-heritage/clever-hans [5] \"How Migrating Birds Use Quantum Effects to Navigate\", Scientific American, April 2022, https://www.scientificamerican.com/article/how-migrating-birds-use-quantum-effects-to-navigate/[6] A variation on Conway's Game of Life, https://stackoverflow.com/questions/70019538/simple-animation-for-conways-game-of-life-with-funcanimation[7] \"Real-Time Imaging of Quantum Entanglement\", 2013, https://youtu.be/wGkx1MUw2TU?si=mnIExRs2ZOwv46Bh, but not strangely enough \"Entanglement between superconducting qubits and a tardigrade\", https://arxiv.org/pdf/2112.07978 [8] \"Simulating Physics with Computers\", International Journal of Theoretical Physics vol 21, transcript of a talk at MIT by Richard Feynman, 1981, https://s2.smu.edu/~mitch/class/5395/papers/feynman-quantum-1981.pdf[9] \"The Character of Physical Law\", transcript of lectures by Richard Feynman at Cornell U, 1967, https://archive.org/details/characterofphysi0000feyn/page/12/mode/2up[10] \"2 Researchers Spell 'I.B.M.' Atom by Atom\", New York Times, April 5, 1990,https://timesmachine.nytimes.com/timesmachine/1990/04/05/356490.html?pageNumber=41[11] \"Qubit Bloch Sphere Visualization\", Casey Duckering, https://raw.githubusercontent.com/cduck/bloch_sphere/master/examples/xyss_gate.gif[12] Its apple picking season here in New York: https://www.applesfromny.com/varieties/[13] \"Driving quantum performance: more qubits, higher Quantum Volume, and now a proper measure of speed\", https://www.ibm.com/quantum/blog/circuit-layer-operations-per-second[14] DARPA Quantum Benchmarking Initiative, https://www.darpa.mil/work-with-us/quantum-benchmarking-initiative[15] \"The Computational Universe\", Seth Lloyd, 2002, https://www.edge.org/conversation/seth_lloyd-the-computational-universe[16] \"Google Claims To Achieve Quantum Supremacy — IBM Pushes Back\", https://www.npr.org/2019/10/23/772710977/google-claims-to-achieve-quantum-supremacy-ibm-pushes-back[17] \"A route to scalable Majorana qubits\", https://phys.org/news/2024-06-route-scalable-majorana-qubits.html[18] \"DARPA's quantum computing is powered by ... FOMO\", https://www.theregister.com/2023/02/02/darpa_quantum_microsoft/",
            "content_html": "<p><span style=\"font-family: verdana;\"><span>Just back from the</span><span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://www.linkedin.com/company/ieee-computer-society/\" target=\"_self\">IEEE Computer Society</a><span class=\"white-space-pre\"> </span><span>Quantum Week in Montreal, and besides eating my weight in pastry and bagels [1], it was a great conference. The collective hardware roadmaps from the major players leaves us thinking the big wave in quantum computing is not here yet, but soon - perhaps in 5 years time for scientific applications, and within a decade for commercial utility. While the current software continues to be sparse and low-level, there are inklings of software engineers starting to build up a stack in anticipation of needing one. But besides that, and at the risk of sounding like the other hot topic - AI marketeer hype [2] - there is the sense with quantum of being present at a new phase in computing at least, if not something larger still.</span><span class=\"white-space-pre\"> </span></span></p><blockquote class=\"ember-view reader-text-block__blockquote\" id=\"ember3175\"><span style=\"color: #04ff00; font-family: verdana;\">The concept of the computing universe is still just a hypothesis; nothing has been proved. However, I am confident that this idea can help unveil the secrets of nature. - Konrad Zuse, 1969 [3]</span></blockquote><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3176\"><span style=\"font-family: verdana;\">It seems, at its core, that the universe computes. Conch shells grow in logarithmic spirals, bees and orb weavers understand structural geometry. Many animals - not including Mr. Ed or Clever Hans [4], but including primates, fish, and rats - have been shown to use simple arithmetic or approximations, and in other cases can show ability to order objects in a list. There are lizards and sea shells with surface patterns constructed by cellular automata processes - simple rules which can produce complex structures, like Conway's \"Game of Life\". Ducks, using an inherently quantum mechanical biological process, can see magnetic fields giving them a kind of \"heads up\" display when migrating. [5]<span class=\"white-space-pre\"> </span></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3177\"><span style=\"font-family: verdana;\"><br /></span></p><div class=\"reader-image-block reader-image-block--full-width\"><figure class=\"reader-image-block__figure\"><div class=\"ivm-image-view-model   \"><div class=\"ivm-view-attr__img-wrapper                \"><img alt=\"\" class=\"ivm-view-attr__img--centered  reader-image-block__img evi-image lazy-image ember-view\" id=\"ember3178\" src=\"https://media.licdn.com/dms/image/v2/D4E12AQF2LHaealtiYA/article-inline_image-shrink_1000_1488/article-inline_image-shrink_1000_1488/0/1727727559513?e=1740009600&amp;v=beta&amp;t=vLl0WFGaEuH2xJfhffuvIBSRwDXRnMtGU8m-kADT2c4\" /></div></div><figcaption class=\"reader-image-block__figure-image-caption display-block full-width text-body-small-open t-sans text-align-center t-black--light\"><span style=\"font-family: verdana;\">A variation on Conway's \"Game of Life\" [6]</span></figcaption></figure></div><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3179\"><span style=\"font-family: verdana;\">Our own eyes are themselves literally photon detectors, our retinas and optic nerves pre-processing the signals before they even get to the brain. DNA stores vast amounts of information in simple patterns of four \"letters\" (effectively two classical bits), recipes to manufacture from the raw material of the universe a wide array of proteins for all manner of biological purposes, including of course growing your own brain. Humans can themselves perform the manual calculations necessary to build bridges and other structures which can span and withstand the forces of nature for centuries. Its also not hard to see computation of a kind in plants and their systemic networks of roots. Is the universe computing, or is the universe doing the only thing it can based on the rules? Water flowing downhill. Lightning finding its own path of least resistance. Entangled electrons separated at distance flipping their spin in response to their partner, in real time. [7]</span></p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p><br />&lt;div class=\"reader-embed-block__iframe-embed\"&gt;<br />&lt;/div&gt;</p><div class=\"reader-embed-block__iframe-embed\"><br /></div><div class=\"reader-embed-block__iframe-embed\"></div><h3 class=\"ember-view reader-text-block__heading-3\" id=\"ember3181\"><span style=\"font-family: verdana; font-size: x-large;\">An Entangled History</span></h3><blockquote class=\"ember-view reader-text-block__blockquote\" id=\"ember3182\"><span style=\"color: #04ff00; font-family: verdana;\">Nature isn't classical, dammit, and if you want to make a simulation of nature, you'd better make it quantum mechanical. - Richard Feynman [8]<span class=\"white-space-pre\"> </span></span></blockquote><blockquote class=\"ember-view reader-text-block__blockquote\" id=\"ember3183\"><span style=\"color: #04ff00; font-family: verdana;\">I think I can safely say that nobody understands quantum mechanics. - Richard Feynman [9]</span></blockquote><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3184\"><span style=\"font-family: verdana;\">In the 1920s physicists like Heisenberg, Born, Pauli, and Schrödinger convinced their peers of the validity of a new formulation of the laws of physics they called (in German) \"quantum mechanics\", describing the behavior of nature and the universe even below the scale of atoms. This then led computing pioneer John Von Neumann in the 1930s to solidify some of the necessary maths to perform discrete quantum mechanical calculations. Von Neumann, a member of the Manhattan Project, would go on to formulate the hardware architecture for today's \"classical\" computers, the approach to computing being challenged by quantum computing today.<span class=\"white-space-pre\"> </span></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3185\"><span style=\"font-family: verdana;\">Since then, mostly notably in the 1940s with the Manhattan Project, humans have shown an increasing ability to harness the basic quantum physics into a new range of applications. We learned how to make superconductors, and how to park and manipulate individual atoms like tinkertoys [10], and now we've learned how to use these skills to make computers, with most immediate applications in modeling quantum systems like atoms and molecules, as Nobel laureate and Manhattan Project member Feynman predicted more than 40 years ago. We learned how to make lasers and LEDs, and we can now also harness photons for computing. The scientists have had nearly a century to refine their theories, and are now handing off to the engineers to prove the depth of human understanding by building things in the real world, with the business people eagerly waiting on the sidelines in anticipation (think: MRI machines). It seems that the universe computes - we now endeavor to make use of that knowledge for our own human purposes.<span class=\"white-space-pre\"> </span></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3186\"><span style=\"font-family: verdana;\"><br /></span></p><h3 class=\"ember-view reader-text-block__heading-3\" id=\"ember3187\"><span style=\"font-family: verdana; font-size: x-large;\">Qubits, Gates, and Error Everywhere</span></h3><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3188\"><span style=\"font-family: verdana;\">What is a qubit? Like the early video game Q*bert which showed a simulated 3D world on a 2D screen back in 1982, a qubit is a little hard to visualize as it goes deeper than a classical binary bit - much deeper. While a bit can be either a zero or a one but nothing else, a qubit can model the<span class=\"white-space-pre\"> </span><span>probability</span><span class=\"white-space-pre\"> </span>of a zero or a one and probabilistically anything in between. It might be a zero, or it might be a one, with some probability of each. It might start off a zero, and then noise from the environment might cause it to drift, or it might move off the zero purposefully as a result of acting on the qubit with one of several kinds of single and multi-qubit gates. Like gates in classical computing, a quantum gate can flip the qubit, or unique to quantum just nudge it a little. As in classical computing, its the acting on the qubit by gates which results in the computing. A native quantum program is a circuit, a directed graph, composed of gates.<span class=\"white-space-pre\"> </span></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3189\"><span style=\"font-family: verdana;\"><br /></span></p><div class=\"reader-image-block reader-image-block--full-width\"><figure class=\"reader-image-block__figure\"><div class=\"ivm-image-view-model   \"><div class=\"ivm-view-attr__img-wrapper                \"><img alt=\"\" class=\"ivm-view-attr__img--centered  reader-image-block__img evi-image lazy-image ember-view\" id=\"ember3190\" src=\"https://media.licdn.com/dms/image/v2/D4E12AQFAklbbJReWZA/article-inline_image-shrink_1500_2232/article-inline_image-shrink_1500_2232/0/1727726980638?e=1740009600&amp;v=beta&amp;t=vnbWE-YUgsGxlQf5M2FnYJ7DLtJl-P7NvK_b0j3YVw0\" /></div></div><figcaption class=\"reader-image-block__figure-image-caption display-block full-width text-body-small-open t-sans text-align-center t-black--light\"><span style=\"font-family: verdana;\">Visualizing the effect of various quantum gates on a single qubit. [11]</span></figcaption></figure></div><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3191\"><span class=\"white-space-pre\"><span style=\"font-family: verdana;\"> </span></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3192\"><span style=\"font-family: verdana;\">How fast can we flip a qubit? Heisenberg 100 years ago gave us a way to compute the lower bound on the time to flip a spin given an energy - in short, its fast. But this doesn't even tell the whole performance story because of superposition and the ability of wide circuits to act on multiple qubits simultaneously - the speed advantage of quantum over classical can be exponential, albeit application-specific, making a class of problems which would be classically uncomputable in any human lifetime now well within reach.<span class=\"white-space-pre\"> </span></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3193\"><span style=\"font-family: verdana;\">The main challenge in realizing these potentials is the noise. Qubits are noisy, meaning they don't stay fixed where you think you last left them, and the seemingly magical entanglement also can show decoherence over time and distance. The gates necessary for computation can themselves introduce noisy error, as can the act of measuring the qubit to sample the solution result. Software algorithms can also just be estimates, and thus introduce their own error term relative to experiment. The prevalence of error in quantum computing means the algorithms themselves must be aware the results might be unpredictable, might need to be computed more then once to improve confidence, and might need to allocate and use a good number of precious qubits just to help mitigate the errors. We call the period we are in today the \"NISQ era\", meaning, noisy intermediate scale quantum computing.<span class=\"white-space-pre\"> </span></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3194\"><span style=\"font-family: verdana;\">Beyond the noise, how do we quantify \"scale\" or other metrics for sizing up the capabilities of quantum computers, now, in future, and as compared to classical? One aspect of the problem is that when comparing quantum to classical we're not comparing apples to apples, and even within \"quantum apples\" as we have seen, there are different kinds. [12] In one simple measure we can count qubits, but we must also know their error rate, and we must notice something about their connectedness - some quantum hardware use an all-to-all grid, and others use other topologies - racetracks and the like. And qubits can fail. Because of the limitations of qubits in the NISQ era the connectedness matters, is necessary to be known at time of circuit transpilation, and may result in swaps or other strategies employed by the transpiler toolchain to minimize errors due to the physical layout. Qubit coherence in superposition can be measured and reported as a hardware spec. Quantum<span class=\"white-space-pre\"> </span><span>volume</span><span class=\"white-space-pre\"> </span>is a number which expresses the size of a circuit N qubits wide by d gates deep which can be executed on a given machine. Gate errors especially for 2-qubit gates can be reported by the vendor. CLOPS - circuit layer operations per second - is another proposed metric which takes into consideration the time to prepare the qubits, execute the gates, and take the measurement of the result. [13] The US government in the form of<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://www.linkedin.com/company/darpa/\">Defense Advanced Research Projects Agency (DARPA)</a><span class=\"white-space-pre\"> </span>has gotten into the game of studying this varied performance landscape, towards being able to help pick winners and losers and accelerate innovation with funding awards. [14]<span class=\"white-space-pre\"> </span></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3195\"><span style=\"font-family: verdana;\"><br /></span></p><h3 class=\"ember-view reader-text-block__heading-3\" id=\"ember3196\"><span style=\"font-family: verdana; font-size: x-large;\">Quantum Hardware Roadmap</span></h3><blockquote class=\"ember-view reader-text-block__blockquote\" id=\"ember3197\"><span style=\"color: #04ff00; font-family: verdana;\">I build quantum computers that store information on individual atoms and then massage the normal interactions between atoms to make them compute. - Seth Lloyd [15]</span></blockquote><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3198\"><span style=\"font-family: verdana;\">This year's IEEE Quantum Week was an opportunity to see and hear from most of the major players in quantum computing R&amp;D - those focused on quantum processors, systems control, networking, and software. The software topic we'll leave as a topic of a future blog, but focusing on hardware, the vendors collectively represented multiple distinct technical mechanisms to making a quantum computing machine. There's superconducting qubits from US companies like<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://www.linkedin.com/showcase/ibm-quantum/\">IBM Quantum</a><span class=\"white-space-pre\"> </span>,<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://www.linkedin.com/company/google/\">Google</a><span class=\"white-space-pre\"> </span>, and<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://www.linkedin.com/company/rigetti-computing/\">Rigetti Computing</a>, which refrigerate and maintain the qubit between a ground and an excited state. Trapped ion computers from companies like<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://www.linkedin.com/company/ionq-co/\">IonQ</a><span class=\"white-space-pre\"> </span>and<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://www.linkedin.com/company/quantinuumqc/\">Quantinuum</a><span class=\"white-space-pre\"> </span>, and neutral atom computers from<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://www.linkedin.com/company/quera-computing-inc/\">QuEra Computing Inc.</a><span class=\"white-space-pre\"> </span>use novel methods to again cool the system qubits to near absolute zero. But there are quantum computers which also operate quite differently. Quantum annealing, or algorithmically simulating an adiabatic process for slowly evolving a system to an optimal state, could be simulated on one of the above general quantum machines, or shown more directly on a specialized quantum machine from a Canadian company like<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://www.linkedin.com/company/d-wave-quantum/\">D-Wave</a>.<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://www.linkedin.com/company/xanaduai/\">Xanadu</a>, also based in Canada, performs its quantum tricks with photonics. And while Google may jump the gun on announcing successes from time to time, they and<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://www.linkedin.com/company/microsoft/\">Microsoft</a><span class=\"white-space-pre\"> </span>and others are working on a \"topological qubit\" based on previously only-hypothesized Majorana particles which provide the great advantage relative to other qubit implementations of being able to be controlled digitally. [16, 17]<span class=\"white-space-pre\"> </span></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3199\"><span style=\"font-family: verdana;\">Staying within the NISQ era as the machines scale up, a good chunk of the available qubits will continue to be allocated to error correction schemes, a task which may later as these systems mature be allocated to a software layer. At 100s of useful error-corrected qubits we can start to gain real scientific utility from quantum computing - begin to do research<span class=\"white-space-pre\"> </span><span>with</span><span class=\"white-space-pre\"> </span>quantum rather than research<span class=\"white-space-pre\"> </span><span>about</span><span class=\"white-space-pre\"> </span>quantum. Vendors such as Quantinuum promise a fully connected machine of that size in 5 years. In 10 years, vendors expect to deliver machines with 1000s of QEC, which will usher in commercial utility, and the era of \"cryptographically relevant\" quantum computing (i.e. DARPA wants the US to get there first [18]).<span class=\"white-space-pre\"> </span></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3200\"><span style=\"font-family: verdana;\">In the meantime, certain scientific domains, those which study things most closely associated with real quantum systems, will be early adopters of the technology. Molecular biology. Chemistry, for example, studying better ways to perform synthetic nitrogen fixation (think: energy-costly ammonia production for fertilizers).<span class=\"white-space-pre\"> </span></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3201\"><span style=\"font-family: verdana;\"><br /></span></p><h3 class=\"ember-view reader-text-block__heading-3\" id=\"ember3202\"><span style=\"font-family: verdana; font-size: x-large;\">Conclusion<span class=\"white-space-pre\"> </span></span></h3><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3203\"><span style=\"font-family: verdana;\">The quantum hardware industry is in its infancy. From this gaggle of eager go-getters it’s reasonable to assume there will be technical and business winners and losers. For reasons of national security, governments will ramp up their involvement. But current machines are small, flaky, and limited in usefulness. It will be 5 to 10 years before there are quantum computers being used more commonly. A new but also a familiar approach to software will be needed - more on that in a future blog. Until utility some industries will be early leaders, ready to capitalize on an exponential increase in computing capability, one which promises to get us closer to harnessing the grand computing engine of the universe which is all around and within.</span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3204\"><span style=\"font-family: verdana;\"><br /></span></p><h3 class=\"ember-view reader-text-block__heading-3\" id=\"ember3205\"><span style=\"font-family: verdana; font-size: x-large;\">References &amp; Trivia</span></h3><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3206\"><span style=\"font-family: verdana;\">[0] Photo by Ben Wicks on Unsplash</span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3207\"><span style=\"font-family: verdana;\">[1] Montreal bagels:<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://www.mtl.org/en/experience/the-famous-montreal-bagel\" target=\"_self\">https://www.mtl.org/en/experience/the-famous-montreal-bagel</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3208\"><span style=\"font-family: verdana;\">[2] Gartner AI Hype Cycle 2024 explained:<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://www.youtube.com/watch?v=qXKYOR3KqxQ\" target=\"_self\">https://www.youtube.com/watch?v=qXKYOR3KqxQ</a><span class=\"white-space-pre\"> </span></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3209\"><span style=\"font-family: verdana;\">[3] \"Calculating Space\", Konrad Zuse, 1969,<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://philpapers.org/archive/ZUSRR.pdf\" target=\"_self\">https://philpapers.org/archive/ZUSRR.pdf</a><span class=\"white-space-pre\"> </span>Its worth noting that while having worked for Ford Motor Co. in his early career, and like Von Neumann doing very important early work on computers, Zuse was a conscripted employee of the German Nazi government from 1939-1945.</span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3210\"><span style=\"font-family: verdana;\">[4] Clever Hans:<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://www.horsejournals.com/popular/history-heritage/clever-hans\" target=\"_self\">https://www.horsejournals.com/popular/history-heritage/clever-hans</a><span class=\"white-space-pre\"> </span></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3211\"><span style=\"font-family: verdana;\">[5] \"How Migrating Birds Use Quantum Effects to Navigate\", Scientific American, April 2022,<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://www.scientificamerican.com/article/how-migrating-birds-use-quantum-effects-to-navigate/\" target=\"_self\">https://www.scientificamerican.com/article/how-migrating-birds-use-quantum-effects-to-navigate/</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3212\"><span style=\"font-family: verdana;\">[6] A variation on Conway's Game of Life,<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://stackoverflow.com/questions/70019538/simple-animation-for-conways-game-of-life-with-funcanimation\" target=\"_self\">https://stackoverflow.com/questions/70019538/simple-animation-for-conways-game-of-life-with-funcanimation</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3213\"><span style=\"font-family: verdana;\">[7] \"Real-Time Imaging of Quantum Entanglement\", 2013,<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://youtu.be/wGkx1MUw2TU?si=mnIExRs2ZOwv46Bh\" target=\"_self\">https://youtu.be/wGkx1MUw2TU?si=mnIExRs2ZOwv46Bh</a>, but not strangely enough \"Entanglement between superconducting qubits and a tardigrade\",<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://arxiv.org/pdf/2112.07978\" target=\"_self\">https://arxiv.org/pdf/2112.07978</a><span class=\"white-space-pre\"> </span></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3214\"><span style=\"font-family: verdana;\">[8] \"Simulating Physics with Computers\", International Journal of Theoretical Physics vol 21, transcript of a talk at MIT by Richard Feynman, 1981,<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://s2.smu.edu/~mitch/class/5395/papers/feynman-quantum-1981.pdf\" target=\"_self\">https://s2.smu.edu/~mitch/class/5395/papers/feynman-quantum-1981.pdf</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3215\"><span style=\"font-family: verdana;\">[9] \"The Character of Physical Law\", transcript of lectures by Richard Feynman at Cornell U, 1967,<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://archive.org/details/characterofphysi0000feyn/page/12/mode/2up\" target=\"_self\">https://archive.org/details/characterofphysi0000feyn/page/12/mode/2up</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3216\"><span style=\"font-family: verdana;\">[10] \"2 Researchers Spell 'I.B.M.' Atom by Atom\", New York Times, April 5, 1990,<a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://timesmachine.nytimes.com/timesmachine/1990/04/05/356490.html?pageNumber=41\" target=\"_self\">https://timesmachine.nytimes.com/timesmachine/1990/04/05/356490.html?pageNumber=41</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3217\"><span style=\"font-family: verdana;\">[11] \"Qubit Bloch Sphere Visualization\", Casey Duckering,<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://raw.githubusercontent.com/cduck/bloch_sphere/master/examples/xyss_gate.gif\" target=\"_self\">https://raw.githubusercontent.com/cduck/bloch_sphere/master/examples/xyss_gate.gif</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3218\"><span style=\"font-family: verdana;\">[12] Its apple picking season here in New York:<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://www.applesfromny.com/varieties/\" target=\"_self\">https://www.applesfromny.com/varieties/</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3219\"><span style=\"font-family: verdana;\">[13] \"Driving quantum performance: more qubits, higher Quantum Volume, and now a proper measure of speed\",<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://www.ibm.com/quantum/blog/circuit-layer-operations-per-second\" target=\"_self\">https://www.ibm.com/quantum/blog/circuit-layer-operations-per-second</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3220\"><span style=\"font-family: verdana;\">[14] DARPA Quantum Benchmarking Initiative,<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://www.darpa.mil/work-with-us/quantum-benchmarking-initiative\" target=\"_self\">https://www.darpa.mil/work-with-us/quantum-benchmarking-initiative</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3221\"><span style=\"font-family: verdana;\">[15] \"The Computational Universe\", Seth Lloyd, 2002,<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://www.edge.org/conversation/seth_lloyd-the-computational-universe\" target=\"_self\">https://www.edge.org/conversation/seth_lloyd-the-computational-universe</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3222\"><span style=\"font-family: verdana;\">[16] \"Google Claims To Achieve Quantum Supremacy — IBM Pushes Back\",<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://www.npr.org/2019/10/23/772710977/google-claims-to-achieve-quantum-supremacy-ibm-pushes-back\" target=\"_self\">https://www.npr.org/2019/10/23/772710977/google-claims-to-achieve-quantum-supremacy-ibm-pushes-back</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3223\"><span style=\"font-family: verdana;\">[17] \"A route to scalable Majorana qubits\",<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://phys.org/news/2024-06-route-scalable-majorana-qubits.html\" target=\"_self\">https://phys.org/news/2024-06-route-scalable-majorana-qubits.html</a></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3224\"><span style=\"font-family: verdana;\">[18] \"DARPA's quantum computing is powered by ... FOMO\",<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://www.theregister.com/2023/02/02/darpa_quantum_microsoft/\" target=\"_self\">https://www.theregister.com/2023/02/02/darpa_quantum_microsoft/</a></span></p><div><br /></div>",
            "url": "https://hpc.social/personal-blog/2024/surfing-the-singularity-the-universe-computes/",
            
            
            
            
            
            "date_published": "2024-09-30T16:00:00-06:00",
            "date_modified": "2024-09-30T16:00:00-06:00",
            
                "author": "Surfing the Singularity"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2024/the-hpc-cluster-as-a-reflection-of-values/",
            "title": "The HPC cluster as a reflection of values",
            "summary": null,
            "content_text": "Yesterday while I was cooking dinner, I happened to re-watch Bryan Cantrill&#8217;s talk on &#8220;Platform as a Reflection of Values&#8220;. (I watch a lot tech talks while cooking or baking &#8212; I often have trouble focusing on a video unless I&#8217;m doing something with my hands, but if I know a recipe well I can often make it on autopilot.)If you haven&#8217;t watched this talk before, I encourage checking it out. Cantrill gave it in part to talk about why the node.js community and Joyent didn&#8217;t work well together, but I thought he had some good insights into how values get built into a technical artifact itself, as well as how the community around those artifacts will prioritize certain values.While I was watching the talk (and chopping some vegetables), I started thinking about what values are most important in the &#8220;HPC cluster platform&#8221;.Technical valuesThis slide from the talk shows some examples of what Cantrill thinks of as platform values:A key point from the talk is that all of these are good things! Ideally you want to have all of these things when you build a new platform, whether that&#8217;s a programming language, a cloud platform, or whatever. But any given platform will choose to prioritize some set of values over others. You want them all, but when they come into tension, which ones will win?One example that Cantrill gives in the talk is the original Unix out of Bell Labs, which prioritized simplicity, composability, and portability. Certainly Unix wanted other features, like performance and maintainability, but if forced into a choice like performance vs simplicity, it would generally choose simplicity. Similarly, he talked about how JavaScript and node.js are built around values like approachability, expressiveness, and velocity, and how that contrasted with values like robustness and debuggability that Joyent valued as a cloud provider.The HPC cluster platformWhen I saw &#8220;HPC cluster platform&#8221;, I&#8217;m loosely talking about the collection of hardware and software that is most often used to build high-performance computing clusters for workloads like scientific research or machine learning training.This generic platform consists of a large collection of identical compute nodes, orchestrated by a batch scheduler like Slurm or PBS, and with one or more &#8220;login nodes&#8221; serving as a front-end where users SSH in to prepare and run jobs on the cluster. For multi-node jobs and high-speed storage access, the compute nodes are connected by a very high-speed network, like 100Gb Ethernet or InfiniBand, which needs specific libraries to use effectively. Users on the cluster have access to command-line editors and development tools like compilers and scientific libraries, but mostly interact with the platform in a purely command line environment.See also, this really ugly Google Draw diagram:What values does this platform prioritize? In general, I tend to think that HPC platforms prioritize performance, portability, and approachability.Performance: This might seem obvious given the name &#8220;HPC&#8221;, but it&#8217;s worth thinking a little more about. When faced with a choice between performance and some other value, HPC clusters almost always choose performance. Performance is generally performance above cost, with most clusters using expensive compute and networking hardware. It&#8217;s prioritized over observability (&#8220;measurability&#8221; on Cantrill&#8217;s slide?), with most HPC clusters I&#8217;m aware of disabling most active monitoring features if they have a performance cost. It&#8217;s even prioritized above security, often turning off security features if they lead to lower performance or even measurable performance variability.Portability: Mindful of the difficulty in writing high-performance, correct scientific code, the HPC platform works reasonably hard to maintain portability to new hardware and software over time. A lot of this is due to a robust ecosystem of libraries and middleware. Most applications that scale across multiple nodes still use MPI; code doing linear algebra still depends on long-lived libraries like LAPACK and BLAS; and platform tools like the scheduler tend to be remarkably stable over time. New hardware features are often abstracted by middleware, especially at the networking level where support is built into your MPI library of choice.This story isn&#8217;t perfect &#8212; applications usually need recompilation on a new cluster, and still often need major changes to take advantages of new features. That&#8217;s why I chose &#8220;portability&#8221; instead of &#8220;compatibility&#8221;. But as a cluster admin, I&#8217;ve worked with many researchers who have maintained the same app on many different clusters for 10, 20, or even 30 years, which is a pretty impressive portability story.Approachability: This one may be controversial! The average HPC cluster can seem pretty arcane, especially for someone new to the platform. But I do think that HPC prioritizes a particular kind of approachability, which is that it is designed to onboard scientific researchers who are not themselves expert developers.A new user onboarding to a research HPC cluster frequently needs to understand three main tools:The Linux shell: Most HPC cluster environments are entirely command-line oriented (though Open OnDemand is helping change this!). You log in with SSH; edit using nano, vim, or emacs; and interact with the system entirely using a shell.The cluster scheduler: When you have your application ready to go, you submit your job to a queue using a cluster scheduler like Slurm and wait for it to complete. Cluster schedulers have a lot of moving parts and a user can often find endless knobs to tune, but it&#8217;s easy to get started with just a few commands. (And interestingly, almost all HPC cluster schedulers define their jobs as&#8230; shell scripts! You&#8217;re back to needing to know the shell. Annoying, sure, but at least it ain&#8217;t YAML!)Environment modules: This tool allows the cluster admins to provide a large library of libraries and tools, with specific versions, such that a cluster user just needs to type &#8220;module load openmpi/3&#8221;. While the tool munges the shell environment variables as needed to set up PATH, LD_LIBRARY_PATH, etc just so.Now if this doesn&#8217;t sound like a robust software engineering environment&#8230; it isn&#8217;t! There are endless things that can go wrong, especially with environment modules interacting with the user&#8217;s own shell rc files and who knows what else. And there&#8217;s very little in this environment to encourage best practices like linting, pinned library versions, or even version control at all!But this environment is approachable&#8230; if you&#8217;re a graduate student in a field like physics or biology, running an existing application or writing your own simulation or data processing code. But who never got to take a class on software engineering, and where the code itself is not a first class deliverable. The deliverable is the published paper.But what about all those other values?They&#8217;re still important! But the point of this exercise is to think about which values are will &#8220;win&#8221; when they come into tension. And I do think that, if you look at HPC clusters in general, this is the set of values that will win.Availability is important, but not if that work costs us (much) performance. Velocity is great, but we&#8217;ll de-prioritize it in the name of workload portability. Security is essential &#8212; but we don&#8217;t want to make it harder to onboard new grad students&#8230;You cluster is not the generic platform (and neither is mine)A last point I want to make is that there&#8217;s actually no such thing as the &#8220;generic HPC cluster platform&#8221;. Each individual cluster, at a university or company or government lab, is often configured in a unique way based on the hardware, performance goals, and whims of the person setting it up.Because of this, each individual HPC cluster may prioritize different values. A cluster at a national lab may choose security at the expense of approachability; or a different cluster may choose to sacrifice portability in the name of velocity if they&#8217;re developing on a new hardware or software system.(Also, the systems I build as part of my day job also make very different choices than the &#8220;generic&#8221; cluster would. To a first approximation, I think I&#8217;d say we choose performance/debuggability/portability/security&#8230; but we also make different choices depending on what we&#8217;re building!)But I still think that performance, portability, and approachability represent the most common platform values I&#8217;ve seen in the HPC field as a whole. And I think the tools and practices we use bias towards those values.However&#8230; all of that is what I thought about while making dinner! If you think a different set of values makes more sense, feel free to send me an email and let me know. ",
            "content_html": "<p>Yesterday while I was cooking dinner, I happened to re-watch Bryan Cantrill&#8217;s talk on &#8220;<a href=\"https://www.youtube.com/watch?v=Xhx970_JKX4\">Platform as a Reflection of Values</a>&#8220;. (I watch a lot tech talks while cooking or baking &#8212; I often have trouble focusing on a video unless I&#8217;m doing something with my hands, but if I know a recipe well I can often make it on autopilot.)</p><p>If you haven&#8217;t watched this talk before, I encourage checking it out. Cantrill gave it in part to talk about why the node.js community and Joyent didn&#8217;t work well together, but I thought he had some good insights into how values get built into a technical artifact itself, as well as how the community around those artifacts will prioritize certain values.</p><p>While I was watching the talk (and chopping some vegetables), I started thinking about what values are most important in the &#8220;HPC cluster platform&#8221;.</p><p><span id=\"more-339\"></span></p><h2 class=\"wp-block-heading\">Technical values</h2><p>This slide from the talk shows some examples of what Cantrill thinks of as platform values:</p><figure class=\"wp-block-image size-full\"><img alt=\"A slide with the title &quot;Some platform values&quot;. The list includes approachability, availability, compatibility, composability, debuggability, expressiveness, extensibility, interoperability, integrity, maintainability, operability, performance, portability, resiliency, rigor, robustness, safety, security, simplicity, thoroughness, transparency, and velocity.\" class=\"wp-image-340\" height=\"538\" src=\"https://thinking.ajdecon.org/wp-content/uploads/2024/09/image.png\" width=\"969\" /></figure><p>A key point from the talk is that all of these are good things! Ideally you want to have <em>all</em> of these things when you build a new platform, whether that&#8217;s a programming language, a cloud platform, or whatever. But any given platform will choose to<em> </em>prioritize some set of values over others. You want them all, but when they come into tension, which ones will win?</p><p>One example that Cantrill gives in the talk is the original Unix out of Bell Labs, which prioritized simplicity, composability, and portability. Certainly Unix wanted other features, like performance and maintainability, but if forced into a choice like performance vs simplicity, it would generally choose simplicity. Similarly, he talked about how JavaScript and node.js are built around values like approachability, expressiveness, and velocity, and how that contrasted with values like robustness and debuggability that Joyent valued as a cloud provider.</p><h2 class=\"wp-block-heading\">The HPC cluster platform</h2><p>When I saw &#8220;HPC cluster platform&#8221;, I&#8217;m loosely talking about the collection of hardware and software that is most often used to build high-performance computing clusters for workloads like scientific research or machine learning training.</p><p>This generic platform consists of a large collection of identical compute nodes, orchestrated by a batch scheduler like <a href=\"https://github.com/SchedMD/slurm\">Slurm</a> or <a href=\"https://github.com/openpbs/openpbs\">PBS</a>, and with one or more &#8220;login nodes&#8221; serving as a front-end where users SSH in to prepare and run jobs on the cluster. For multi-node jobs and high-speed storage access, the compute nodes are connected by a very high-speed network, like 100Gb Ethernet or InfiniBand, which needs specific libraries to use effectively. Users on the cluster have access to command-line editors and development tools like compilers and scientific libraries, but mostly interact with the platform in a purely command line environment.</p><p>See also, this really ugly Google Draw diagram:</p><figure class=\"wp-block-image size-large\"><img alt=\"A simple diagram showing a login node, a set of compute nodes, and network storage. The login node is connected to compute nodes by a management network. The storage is connected to compute nodes by a high-speed network.\" class=\"wp-image-348\" height=\"488\" src=\"https://thinking.ajdecon.org/wp-content/uploads/2024/09/image-1-1024x488.png\" width=\"1024\" /></figure><p>What values does this platform prioritize? In general, I tend to think that HPC platforms prioritize <em>performance</em>, <em>portability</em>, and <em>approachability</em>.</p><p><strong>Performance: </strong>This might seem obvious given the name &#8220;HPC&#8221;, but it&#8217;s worth thinking a little more about. When faced with a choice between performance and some other value, HPC clusters <em>almost always</em> choose performance. <br /><br />Performance is generally performance above cost, with most clusters using expensive compute and networking hardware. It&#8217;s prioritized over observability (&#8220;measurability&#8221; on Cantrill&#8217;s slide?), with most HPC clusters I&#8217;m aware of disabling most active monitoring features if they have a performance cost. It&#8217;s even prioritized above security, often turning off security features if they lead to lower performance or even measurable performance <em>variability</em>.</p><p><strong>Portability: </strong>Mindful of the difficulty in writing high-performance, correct scientific code, the HPC platform works reasonably hard to maintain portability to new hardware and software over time. </p><p>A lot of this is due to a robust ecosystem of libraries and middleware. Most applications that scale across multiple nodes still use <a href=\"https://en.wikipedia.org/wiki/Message_Passing_Interface\">MPI</a>; code doing linear algebra still depends on long-lived libraries like <a href=\"https://www.netlib.org/lapack/\">LAPACK</a> and <a href=\"https://www.netlib.org/blas/\">BLAS</a>; and platform tools like the scheduler tend to be remarkably stable over time. New hardware features are often abstracted by middleware, especially at the networking level where support is built into your MPI library of choice.</p><p>This story isn&#8217;t perfect &#8212; applications usually need recompilation on a new cluster, and still often need major changes to take advantages of new features. That&#8217;s why I chose &#8220;portability&#8221; instead of &#8220;compatibility&#8221;. But as a cluster admin, I&#8217;ve worked with many researchers who have maintained the same app on many different clusters for 10, 20, or even 30 years, which is a pretty impressive portability story.</p><p><strong>Approachability: </strong>This one may be controversial! The average HPC cluster can seem pretty arcane, especially for someone new to the platform. But I do think that HPC prioritizes a particular <em>kind</em> of approachability, which is that it is designed to onboard scientific researchers who are not themselves expert developers.</p><p>A new user onboarding to a research HPC cluster frequently needs to understand three main tools:</p><ul class=\"wp-block-list\"><li><strong>The Linux shell:</strong> Most HPC cluster environments are entirely command-line oriented (though <a href=\"https://openondemand.org/\">Open OnDemand</a> is helping change this!). You log in with SSH; edit using nano, vim, or emacs; and interact with the system entirely using a shell.</li><li><strong>The cluster scheduler: </strong>When you have your application ready to go, you submit your job to a queue using a cluster scheduler like Slurm and wait for it to complete. Cluster schedulers have a lot of moving parts and a user can often find endless knobs to tune, but it&#8217;s easy to get started with just a few commands. (And interestingly, almost all HPC cluster schedulers define their jobs as&#8230; shell scripts! You&#8217;re back to needing to know the shell. Annoying, sure, but at least it ain&#8217;t YAML!)</li><li><a href=\"https://modules.readthedocs.io/en/latest/\"><strong>Environment modules</strong></a>: This tool allows the cluster admins to provide a large library of libraries and tools, with specific versions, such that a cluster user just needs to type &#8220;module load openmpi/3&#8221;. While the tool munges the shell environment variables as needed to set up PATH, LD_LIBRARY_PATH, etc just so.</li></ul><p>Now if this doesn&#8217;t sound like a robust software engineering environment&#8230; it isn&#8217;t! There are endless things that can go wrong, especially with environment modules interacting with the user&#8217;s own shell rc files and who knows what else. And there&#8217;s very little in this environment to encourage best practices like linting, pinned library versions, or even version control at all!</p><p>But this environment is <em>approachable</em>&#8230; if you&#8217;re a graduate student in a field like physics or biology, running an existing application or writing your own simulation or data processing code. But who never got to take a class on software engineering, and where the code itself is not a first class deliverable. The deliverable is the published paper.</p><h2 class=\"wp-block-heading\">But what about all those other values?</h2><p>They&#8217;re still important! But the point of this exercise is to think about which values are will &#8220;win&#8221; when they come into tension. And I do think that, if you look at HPC clusters in general, this is the set of values that will win.</p><p>Availability is important, but not if that work costs us (much) performance. Velocity is great, but we&#8217;ll de-prioritize it in the name of workload portability. Security is essential &#8212; but we don&#8217;t want to make it harder to onboard new grad students&#8230;</p><h2 class=\"wp-block-heading\">You cluster is not the generic platform (and neither is mine)</h2><p>A last point I want to make is that there&#8217;s actually <em>no such thing</em> as the &#8220;generic HPC cluster platform&#8221;. Each individual cluster, at a university or company or government lab, is often configured in a unique way based on the hardware, performance goals, and whims of the person setting it up.</p><p>Because of this, each <em>individual</em> HPC cluster may prioritize different values. A cluster at a national lab may choose security at the expense of approachability; or a different cluster may choose to sacrifice portability in the name of velocity if they&#8217;re developing on a new hardware or software system.</p><p>(Also, the systems I build as part of my day job also make <em>very</em> different choices than the &#8220;generic&#8221; cluster would. To a first approximation, I think I&#8217;d say we choose performance/debuggability/portability/security&#8230; but we also make different choices depending on what we&#8217;re building!)</p><p>But I still think that <em>performance</em>, <em>portability</em>, and <em>approachability</em> represent the most common platform values I&#8217;ve seen in the HPC field as a whole. And I think the tools and practices we use bias towards those values.</p><p>However&#8230; all of that is what I thought about while making dinner! If you think a different set of values makes more sense, feel free to <a href=\"mailto:ajdecon@ajdecon.org\">send me an email</a> and let me know. <img alt=\"😉\" class=\"wp-smiley\" src=\"https://s.w.org/images/core/emoji/15.0.3/72x72/1f609.png\" style=\"height: 1em;\" /></p>",
            "url": "https://hpc.social/personal-blog/2024/the-hpc-cluster-as-a-reflection-of-values/",
            
            
            
            
            
            "date_published": "2024-09-29T22:22:51-06:00",
            "date_modified": "2024-09-29T22:22:51-06:00",
            
                "author": "Thinking Out Loud"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2024/surfing-the-singularity-staying-relevant-in-a-time-of-rapid-change/",
            "title": "Surfing the Singularity - Staying Relevant in a Time of Rapid Change",
            "summary": null,
            "content_text": "The If you've been tracking the technology industry, and the software space in particular, for any amount of time you've witnessed the accelerating rate of technical change - it was always there, but now its become impossible to miss. The rate of technological change has seemed exponential for a while now, but recent advancements in AI have pushed this curve to new heights. An Accenture report released for Davos 2024 suggests that technical rate of change is seen by C-level leaders as the number one most impactful force on their business - more than financial or geopolitical matters - largely as a result of advances in various forms of AI tooling. [1] Of those surveyed, 88% see the rate of change increasing even further, and half say their organizations are not ready, even though 70% see it as a revenue opportunity. Dinosaur Developers? Today staying alive in business, especially the business of software engineering, means surfing increasingly turbulent and potentially disruptive waters. Consider the leaked recent remarks of Amazon Web Services CEO Matt Garman, wherein he suggested that a mere 2 years from now most AWS programmers wouldn't be coding. [2] In their Q2 investor call, Amazon cited 4,500 person-years of savings through the use of AI assistants on mostly mundane programming tasks like porting and hardening code with patterns of best practices. [3] While the International Monetary Fund suggests AI will impact 60% of jobs and increase wealth inequality, the jobs impacted are more likely to be skewed to higher income countries. [4] These remarks from influential leaders in the industry suggest that the impact of AI will be felt most acutely among software practitioners. Those of us who use integrated development environments (IDEs) to write code (and documents, like this one) with AI assist already are familiar with the benefits. For those unwilling to adapt, to retool and upscale their skills, the future might be bleak. Growing might mean zooming out from code to a more soup-to-nuts view of the software engineering process, especially specification and validation - the need to clearly state requirements and validate results without an immediate need to focus on implementation details. Notice that in the below diagram, taken from the Federal Aviation Administration which is increasingly interested in software engineering and model validation, traditional coding sits only at the bottom of the process rendered as a \"V\". [5]Development in the context of verification and validation, as seen by the FAA.So how to stay relevant in a rapidly changing world, to stay one step ahead of AI and the algorithmic job reaper? A recent LinkedIn survey of technologists suggests the number one thing a person can do is to learn new technologies. [6]A recent Gartner report [7] of the 30 most impactful technologies lists quantum computing as a weighty albeit distant critical enabler. Why? For starters, the existence of Shor's quantum-based numerical factoring algorithm means its a matter of when, not if, quantum computers will be used to crack existing military-grade encryption. In the hands of an adversary, especially when unknown as with the Enigma machine in WWII, the results could be catastrophic, and this is a good part of what is fueling the current government interest in quantum computing.  Off to Quantum Summer School So for me, it was back to school. Summer school. First I hit the stacks, brushed my very stale self up on the fundamentals of the necessary calculus and linear algebra, the quantum mechanics to at least an undergraduate level of understanding, read several texts on the subject of quantum computing including the K&amp;R of quantum \"Mike &amp; Ike\", consumed mass quantities of videos from companies like IBM and Xanadu, and kicked the tires on their programming tool kits. Next I traveled a short distance from my home office to the Griffiss Institute in Rome NY at their now annual \"Quantum 4 International\" conference. This consisted of an impressive array of researchers and government administrators presenting their latest findings and laboratory results, often in a sort of national inventory of funded priority projects. The US Air Force, which maintains a research presence in Rome NY, is particularly interested in quantum computing and networking, for example, scaling up to a larger quantum computer by networking (entangling) a set of smaller ones. The Army and Navy are more focused on other non-computing aspects of quantum technology - sensing, magnetics, material defect identification, and as radio receivers. The Canadian delegation was focused on many of the same research topics, as well as a national emphasis on quantum technology education - to be impactful in quantum computing, one must be able to meld a variety of maths, physics, and programming skills with an unusual level of creativity to design novel and efficient algorithms which take advantage of the power of the quantum qubit - as a former college adjunct, this is no small educational challenge. Finally, researchers from the EU demonstrated new upper bounds on entanglement at a distance for wider area networking, and the use of novel estimation techniques to scale up quantum simulators in this \"NISQ\" era where real quantum computers are still small, noisy, fragile, and scarce. What was noticeably lacking was the demonstration of any current industrial utility for quantum computing applications, and the head of DARPA saw none emerging until we collectively move beyond the NISQ era. Pack a Remote Lunch While some industrial domains like chemistry will likely gain utility first, the head of DARPA suggests that utility in my own current application area - computational fluid dynamics (CFD) - will not emerge until we move into the \"cryptographically relevant\" era. It was with this in mind that I remotely attended the von Karman Institute for Fluid Dynamics in Belgium for a week-long course called \"Introduction to Quantum Computing in Fluid Dynamics\" funded by NATO. Entirely civilian in nature, the training was aimed at CFD researchers who might take advantage of one of the quantum facilities being installed at national laboratories in the US and EU, often collocated with their existing high performance computing (HPC) clusters. Not being a physicist, for me much of the class was consumed for general domain literacy, and the \"tl;dr\" is the re-emergence of particle-based methods like Lattice Boltzmann as a focus of research over finite volume methods and solving the Navier-Stokes equations, as is currently dominant in HPC-based CFD. With mind fully blown by the Von Karman experience, I next took two weeks and attended the IBM Global Quantum Summer School, 2024 edition, consisting of 10 lectures on a variety of topics and 4 labs. The videos are now posted on YouTube [8] and while I personally enjoyed the lecture on Hamiltonian simulation, there was a distinct and unfortunately NISQ-era necessity to focus on error correction and compensating for noise, and on the inner workings of the IBM Qiskit transpiler. In the latter case, because of the diverse nature of the emerging quantum hardware, because inter-qubit connectivity is often not N-way, and because at this stage things often break, it becomes common to mess with the compiler, and to adopt a toolchain with an eye to portability. Qiskit, a library and tool set for Python, is one of a couple frameworks (another being PennyLane) which currently meet this need, and the labs went to length to expose the student to the various topological mapping, translation, and optimization stages which are present in the quantum programming toolchain. And we got to play with a Hamiltonian simulation up to 50 qubits on real hardware, as most classical machines would have a hard time managing the simultaneous behavior of 50 spins.Next Up: AI Assistants &amp; Hybrid Quantum Computing During the Qiskit labs, naturally I was using LLM assist in my IDE, at minimum for tedious or repetitive tasks. But it was remarkable how often the AI assistant was helpful, even for a seemingly niche programming task such as using a quantum computing framework. I intend to delve into this topic more in a future blog and share my experiences with the various emerging AI tools for code and document assist, as well as in the broader end-to-end software engineering context. In addition, I intend to share future blog installments as my quantum education in search of industrial utility continues through the fall conference season. As a software engineer, I'll be particularly on the lookout for frameworks, including those which leverage AI, which allow the programmer to rise above the level of 1950s-like qubits and gates to higher and portable constructs. I'll also be sharing learnings on the rise of classical-quantum hybrids, especially in HPC contexts, as today's quantum approaches such as variational algorithms which converge on solutions require it. Here is another place where toolchains will play a major role, and where heterogeneous workflows which utilize AI tools will likely be impactful. Until next time, enjoy these last few weeks of summer.- andy References: 0. Photo by Ben Wicks on Unsplash1. https://www.accenture.com/us-en/about/company/pulse-of-change2. https://www.businessinsider.com/aws-ceo-developers-stop-coding-ai-takes-over-2024-83. https://accelerationeconomy.com/cloud-wars/amazon-genai-slashes-260-million-in-costs-saves-4500-years/4. https://nypost.com/2024/01/15/business/ai-will-affect-60-of-us-jobs-imf-warns/5. https://www.faa.gov/sites/faa.gov/files/2024-07/d_VVFlow_2024Mar21.jpg6. https://www.linkedin.com/advice/0/how-can-you-stay-relevant-software-development-skills-it-services-kxeme7. https://www.gartner.com/en/articles/30-emerging-technologies-that-will-guide-your-business-decisions8. https://www.youtube.com/playlist?list=PLOFEBzvs-Vvr-GzDWlZpAcDpki5jUqYJu",
            "content_html": "<p class=\"ember-view reader-text-block__paragraph\" id=\"ember3428\">The If you've been tracking the technology industry, and the software space in particular, for any amount of time you've witnessed the<span class=\"white-space-pre\"> </span><span>accelerating</span><span class=\"white-space-pre\"> </span>rate of technical change - it was always there, but now its become impossible to miss.<span class=\"white-space-pre\"> </span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3428\">The rate of technological change has seemed exponential for a while now, but recent advancements in AI have pushed this curve to new heights.<span class=\"white-space-pre\"> </span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3429\">An Accenture report released for Davos 2024 suggests that technical rate of change is seen by C-level leaders as the number one most impactful force on their business - more than financial or geopolitical matters - largely as a result of advances in various forms of AI tooling. [1] Of those surveyed, 88% see the rate of change increasing even further, and half say their organizations are not ready, even though 70% see it as a revenue opportunity.<span class=\"white-space-pre\"> </span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3430\"><br /></p><h3 class=\"ember-view reader-text-block__heading-3\" id=\"ember3431\"><span style=\"font-size: x-large;\">Dinosaur Developers?<span class=\"white-space-pre\"> </span></span></h3><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3432\">Today staying alive in business, especially the business of software engineering, means surfing increasingly turbulent and potentially disruptive waters. Consider the leaked recent remarks of Amazon Web Services CEO Matt Garman, wherein he suggested that a mere 2 years from now most AWS programmers wouldn't be coding. [2] In their Q2 investor call, Amazon cited 4,500<span class=\"white-space-pre\"> </span><span>person-years</span><span class=\"white-space-pre\"> </span>of savings through the use of AI assistants on mostly mundane programming tasks like porting and hardening code with patterns of best practices. [3]<span class=\"white-space-pre\"> </span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3433\">While the International Monetary Fund suggests AI will impact 60% of jobs and increase wealth inequality, the jobs impacted are more likely to be skewed to higher income countries. [4] These remarks from influential leaders in the industry suggest that the impact of AI will be felt most acutely among software practitioners. Those of us who use integrated development environments (IDEs) to write code (and documents, like this one) with AI assist already are familiar with the benefits. For those unwilling to adapt, to retool and upscale their skills, the future might be bleak. Growing might mean zooming out from code to a more soup-to-nuts view of the software engineering process, especially specification and validation - the need to clearly state requirements and validate results without an immediate need to focus on implementation details. Notice that in the below diagram, taken from the Federal Aviation Administration which is increasingly interested in software engineering and model validation, traditional coding sits only at the bottom of the process rendered as a \"V\". [5]</p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3434\"><br /></p><div class=\"reader-image-block reader-image-block--full-width\"><figure class=\"reader-image-block__figure\"><div class=\"ivm-image-view-model   \"><div class=\"ivm-view-attr__img-wrapper                \"><img alt=\"\" class=\"ivm-view-attr__img--centered  reader-image-block__img evi-image lazy-image ember-view\" id=\"ember3435\" src=\"https://media.licdn.com/dms/image/v2/D4E12AQEOZqWZEle0-Q/article-inline_image-shrink_1500_2232/article-inline_image-shrink_1500_2232/0/1725982434333?e=1740009600&amp;v=beta&amp;t=IY_I5FqIuF9dAu5ikJgAtZat6UOQ67wPDShSYD2loeU\" /></div></div><figcaption class=\"reader-image-block__figure-image-caption display-block full-width text-body-small-open t-sans text-align-center t-black--light\">Development in the context of verification and validation, as seen by the FAA.</figcaption></figure></div><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3436\">So how to stay relevant in a rapidly changing world, to stay one step ahead of AI and the algorithmic job reaper? A recent LinkedIn survey of technologists suggests the number one thing a person can do is to learn new technologies. [6]</p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3437\">A recent Gartner report [7] of the 30 most impactful technologies lists quantum computing as a weighty albeit distant critical enabler. Why? For starters, the existence of Shor's quantum-based numerical factoring algorithm means its a matter of when, not if, quantum computers will be used to crack existing military-grade encryption. In the hands of an adversary, especially when unknown as with the Enigma machine in WWII, the results could be catastrophic, and this is a good part of what is fueling the current government interest in quantum computing.<span class=\"white-space-pre\"> </span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3438\"><span class=\"white-space-pre\"> </span></p><h3 class=\"ember-view reader-text-block__heading-3\" id=\"ember3439\"><span style=\"font-size: x-large;\">Off to Quantum Summer School<span class=\"white-space-pre\"> </span></span></h3><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3440\">So for me, it was back to school. Summer school. First I hit the stacks, brushed my very stale self up on the fundamentals of the necessary calculus and linear algebra, the quantum mechanics to at least an undergraduate level of understanding, read several texts on the subject of quantum computing including the K&amp;R of quantum \"Mike &amp; Ike\", consumed mass quantities of videos from companies like IBM and Xanadu, and kicked the tires on their programming tool kits.<span class=\"white-space-pre\"> </span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3441\">Next I traveled a short distance from my home office to the<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://www.linkedin.com/company/griffiss-institute/\">Griffiss Institute</a><span class=\"white-space-pre\"> </span>in Rome NY at their now annual \"Quantum 4 International\" conference. This consisted of an impressive array of researchers and government administrators presenting their latest findings and laboratory results, often in a sort of national inventory of funded priority projects. The US Air Force, which maintains a research presence in Rome NY, is particularly interested in quantum computing and networking, for example, scaling up to a larger quantum computer by networking (entangling) a set of smaller ones. The Army and Navy are more focused on other non-computing aspects of quantum technology - sensing, magnetics, material defect identification, and as radio receivers. The Canadian delegation was focused on many of the same research topics, as well as a national emphasis on quantum technology education - to be impactful in quantum computing, one must be able to meld a variety of maths, physics, and programming skills with an unusual level of creativity to design novel and efficient algorithms which take advantage of the power of the quantum qubit - as a former college adjunct, this is no small educational challenge. Finally, researchers from the EU demonstrated new upper bounds on entanglement at a distance for wider area networking, and the use of novel estimation techniques to scale up quantum simulators in this \"NISQ\" era where real quantum computers are still small, noisy, fragile, and scarce. What was noticeably lacking was the demonstration of any current industrial utility for quantum computing applications, and the head of DARPA saw none emerging until we collectively move beyond the NISQ era.<span class=\"white-space-pre\"> </span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3442\"><br /></p><h3 class=\"ember-view reader-text-block__heading-3\" id=\"ember3443\"><span style=\"font-size: x-large;\">Pack a Remote Lunch<span class=\"white-space-pre\"> </span></span></h3><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3444\">While some industrial domains like chemistry will likely gain utility first, the head of DARPA suggests that utility in my own current application area - computational fluid dynamics (CFD) - will not emerge until we move into the \"cryptographically relevant\" era. It was with this in mind that I remotely attended the<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://www.linkedin.com/company/vki-vonkarmaninstitute/\">von Karman Institute for Fluid Dynamics</a><span class=\"white-space-pre\"> </span>in Belgium for a week-long course called \"Introduction to Quantum Computing in Fluid Dynamics\" funded by NATO. Entirely civilian in nature, the training was aimed at CFD researchers who might take advantage of one of the quantum facilities being installed at national laboratories in the US and EU, often collocated with their existing high performance computing (HPC) clusters. Not being a physicist, for me much of the class was consumed for general domain literacy, and the \"tl;dr\" is the re-emergence of particle-based methods like Lattice Boltzmann as a focus of research over finite volume methods and solving the Navier-Stokes equations, as is currently dominant in HPC-based CFD.<span class=\"white-space-pre\"> </span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3445\">With mind fully blown by the Von Karman experience, I next took two weeks and attended the IBM Global Quantum Summer School, 2024 edition, consisting of 10 lectures on a variety of topics and 4 labs. The videos are now posted on YouTube [8] and while I personally enjoyed the lecture on Hamiltonian simulation, there was a distinct and unfortunately NISQ-era necessity to focus on error correction and compensating for noise, and on the inner workings of the IBM<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://www.linkedin.com/company/qiskit/\">Qiskit</a><span class=\"white-space-pre\"> </span>transpiler. In the latter case, because of the diverse nature of the emerging quantum hardware, because inter-qubit connectivity is often not N-way, and because at this stage things often break, it becomes common to mess with the compiler, and to adopt a toolchain with an eye to portability. Qiskit, a library and tool set for Python, is one of a couple frameworks (another being<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://www.linkedin.com/company/pennylaneai/\">PennyLane</a>) which currently meet this need, and the labs went to length to expose the student to the various topological mapping, translation, and optimization stages which are present in the quantum programming toolchain. And we got to play with a Hamiltonian simulation up to 50 qubits on real hardware, as most classical machines would have a hard time managing the simultaneous behavior of 50 spins.</p><div class=\"reader-embed-block__iframe-embed\"></div><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3446\"><br /></p><h3 class=\"ember-view reader-text-block__heading-3\" id=\"ember3447\"><span style=\"font-size: x-large;\">Next Up: AI Assistants &amp; Hybrid Quantum Computing<span class=\"white-space-pre\"> </span></span></h3><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3448\">During the Qiskit labs, naturally I was using LLM assist in my IDE, at minimum for tedious or repetitive tasks. But it was remarkable how often the AI assistant was helpful, even for a seemingly niche programming task such as using a quantum computing framework. I intend to delve into this topic more in a future blog and share my experiences with the various emerging AI tools for code and document assist, as well as in the broader end-to-end software engineering context.<span class=\"white-space-pre\"> </span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3449\">In addition, I intend to share future blog installments as my quantum education in search of industrial utility continues through the fall conference season. As a software engineer, I'll be particularly on the lookout for frameworks, including those which leverage AI, which allow the programmer to rise above the level of 1950s-like qubits and gates to higher and portable constructs. I'll also be sharing learnings on the rise of classical-quantum hybrids, especially in HPC contexts, as today's quantum approaches such as variational algorithms which converge on solutions require it. Here is another place where toolchains will play a major role, and where heterogeneous workflows which utilize AI tools will likely be impactful.<span class=\"white-space-pre\"> </span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3450\">Until next time, enjoy these last few weeks of summer.</p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3451\">- andy<span class=\"white-space-pre\"> </span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3452\"><br /></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3453\"><span style=\"font-size: x-large;\">References:<span class=\"white-space-pre\"> </span></span></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3454\">0. Photo by<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://unsplash.com/@profwicks?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash\" target=\"_self\">Ben Wicks</a><span class=\"white-space-pre\"> </span>on<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://unsplash.com/photos/green-and-blue-light-bokeh-Ia-qPL-HQdA?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash\" target=\"_self\">Unsplash</a></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3455\">1.<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://www.accenture.com/us-en/about/company/pulse-of-change\" target=\"_self\">https://www.accenture.com/us-en/about/company/pulse-of-change</a></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3456\">2.<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://www.businessinsider.com/aws-ceo-developers-stop-coding-ai-takes-over-2024-8\" target=\"_self\">https://www.businessinsider.com/aws-ceo-developers-stop-coding-ai-takes-over-2024-8</a></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3457\">3.<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://accelerationeconomy.com/cloud-wars/amazon-genai-slashes-260-million-in-costs-saves-4500-years/\" target=\"_self\">https://accelerationeconomy.com/cloud-wars/amazon-genai-slashes-260-million-in-costs-saves-4500-years/</a></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3458\">4.<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://nypost.com/2024/01/15/business/ai-will-affect-60-of-us-jobs-imf-warns/\" target=\"_self\">https://nypost.com/2024/01/15/business/ai-will-affect-60-of-us-jobs-imf-warns/</a></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3459\">5.<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://www.faa.gov/sites/faa.gov/files/2024-07/d_VVFlow_2024Mar21.jpg\" target=\"_self\">https://www.faa.gov/sites/faa.gov/files/2024-07/d_VVFlow_2024Mar21.jpg</a></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3460\">6.<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://www.linkedin.com/advice/0/how-can-you-stay-relevant-software-development-skills-it-services-kxeme\" target=\"_self\">https://www.linkedin.com/advice/0/how-can-you-stay-relevant-software-development-skills-it-services-kxeme</a></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3461\">7.<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://www.gartner.com/en/articles/30-emerging-technologies-that-will-guide-your-business-decisions\" target=\"_self\">https://www.gartner.com/en/articles/30-emerging-technologies-that-will-guide-your-business-decisions</a></p><p class=\"ember-view reader-text-block__paragraph\" id=\"ember3462\">8.<span class=\"white-space-pre\"> </span><a class=\"bpCpipVrrjRHIWtfjEjtbNsDescTJyo \" href=\"https://www.youtube.com/playlist?list=PLOFEBzvs-Vvr-GzDWlZpAcDpki5jUqYJu\" target=\"_self\">https://www.youtube.com/playlist?list=PLOFEBzvs-Vvr-GzDWlZpAcDpki5jUqYJu</a></p>",
            "url": "https://hpc.social/personal-blog/2024/surfing-the-singularity-staying-relevant-in-a-time-of-rapid-change/",
            
            
            
            
            
            "date_published": "2024-09-10T16:00:00-06:00",
            "date_modified": "2024-09-10T16:00:00-06:00",
            
                "author": "Surfing the Singularity"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2024/how-has-life-after-leaving-the-labs-been-going/",
            "title": "How has life after leaving the Labs been going?",
            "summary": null,
            "content_text": "June 2024 marked two years since I left my job at one of the world's most prestigious government HPC centers for a job in one of the world's largest technology corporations. In that time, the world of HPC has changeddramatically; just six months after I started, ChatGPT was released and triggered a gold rush in AI that is now overshadowing traditional scientific computing. This shift brought about massive HPCdeployments led by hyperscalers, challenging the long-held belief that only national governments coulddeploy and operate world-leading supercomputers. My experiences atISC'24 this past summer made clear to me that the traditional HPC community is now rethinking their role the industry, and some individuals who built their careers in public HPC are revisiting theirassumption that world-class HPC systems are limited to the public institutions that havehistorically dominated the top of the Top500 list. Ihad no idea things would unfold this way when I left my job at NERSC back in 2022, and I've been remarkably lucky tonow be a part of the largest forces driving this huge shift in HPC.One of my new offices. Nicer than my old government office, and it has free food, but it's a ninety-minute drive each way.In the spirit of openness and helping others who are facing similar career decisions, I thought I would follow up on my Life and leavingNERSC post by sharing how my professional journey from DOE HPC into cloud HPC has been going. I'll first explain the path I've traveled over these past two years, then answer some of the mostcommon questions I've been asked about this transition.As a forewarning, this is not a typical technology-focused post, and most of this might be obvious to people who already work in Big Tech. Here are the questions on which I reflected:What happened during my first two years in Corporate America?So what do I actually do?Storage product managementHPC/AI developmentAm I happy with my decision and the new job?Broadly, yesBut for a long time, noFinally, yesWhat does industry do better than the Labs?AccountabilityPace and decision makingRelevanceTechnically: securityBut the pay is good, right?How's work-life balance?Do you miss anything about working at the lab?Freedom to have an off dayTravelOpennessWould you still have left NERSC knowing what you know now?What happened during my first two years in Corporate America?I published my Life and leaving NERSC blog post on aThursday, which was my last day working at NERSC. The following Monday was my first day at the new job, and beinghired as 100% remote, it didn't feel that different; I was just booting up a Lenovo laptop (yuck) instead of aMacBook, using Teams and Outlook instead of Slack, GSuite, and Zoom, and that sort of thing.However, the job was undeniably different; whereas I used to be an engineer at NERSC, I was hired to be a \"Principal ProductManager\" within the cloud storage organization which was responsible for all object, disk, and file storageservices. Although my title was \"product manager,\" I wasn't a people manager, andI didn't manage any specific storage products. Rather, my responsibility was to act as an HPC-focused overlayacross all cloud storage services, and my job was to represent the interests of HPC users to all the people who did manage specific storage products. I didn't define product or feature roadmaps myself, but I could help those responsible for each product or service understand how to shape their roadmaps to benefit HPC workloads.I struggled in this position for a variety of reasons, so after I gave the new rolean honest six to nine months, I decided that being a storage product manager just wasn't a good fit for me.Unfortunately, I reached this decision after the yield curve inverted andmass-layoffs and hiringfreezes were implemented, so there weren't a lot of places to go other than back to a government lab.Although I wasn't thriving as a storage product manager, I did have allies that helped me navigate myday-to-day struggles, and I decided to wait until more opportunities opened up and learn as much aboutproduct management as I could in the meantime.The yield curve inverted a month after I started my new job. Not great timing.After a little over a year as a storage product manager, a new engineering role opened up within a sister team in ourHPC/AI infrastructure organization. After discussing the needs and nature of the work with the hiring manager, I applied for the job, went through theinterview process, and was eventually given a verbal offer to join his team in June 2023. Unfortunately, the globaleconomic outlook was still uncertain, and I wound up sitting in a holding pattern (as a storage productmanager) from June 2023 to November 2023. It wasn't until the week of SC'23 that I finally got the written offerletter, and I spent December wrapping up loose ends within the storage organization.On January 2, 2024, I began my new (and current) role within the company. The move was completely lateral, but I changed job titles from \"Product Manager\" to \"Software Engineer,\" and Ichanged organizations from storage to specialized compute.I say all this because my experiences in making the professional transition from government HPC to cloud HPC are colored by the fact that I really changed jobs twice. I've had both product management and engineering/development roles, and I've been in both storage and HPCorganizations.So what do I actually do?I've had two very different roles within the same orbit of HPC/AI infrastructure, so I'lldescribe them separately to give you a sense of the breadth of HPC roles possible.Storage product managementAs a storage product manager (PM), I was an inch deep but a mile wide on every storage service, every commercial HPC workload, andall the ways in which those two could touch each other. I'd guess that only 25% of my day-to-day work required deepexpertise in HPC; the remainder was either business-centric or required only understanding HPC in broad strokes. Thiswas quite unlike the things I'd done earlier in my career in the public sector, since there's not an equivalent towhat a product manager does within the DOE Labs.For example, I spent a lot of my time as a storage PM explaining the basics of HPC I/O to different teams within thecompany. When most cloud people think \"storage,\" they are really thinking about either enterprise storage (thingslike virtual disks for virtual machines) or content distribution (think serving up content for web apps). Theconcept of hundreds or thousands of VMs all writing to the same place at the same time is standard practice in theHPC world, but in the cloud world, this is a DDoSattack. Since my organization was responsible for all storage, not just HPC storage, there were a lot ofpeople who simply never had to think about the challenges that HPC people take for granted, and it could be challenging (as the new guy) to convince seasoned cloud storage PMs that some workloads legitimately need hundreds of gigabytes per second of bandwidth.As a PM, I also wound up doing a fair amount of business reporting. For example, object storage is used by all manner of cloudcustomers, so prioritizing features that specifically help HPC customers required understanding how many HPCcustomers actually used it. How do you define whether a workload is really an HPC workloads or not? InDOE, we'd waste hours debating stuff like this for no real purpose, but when I became a product manager, I had todefine this to make the business case that we needed to develop a certain feature that would only be used by HPC workloads.Finally, I did a fair amount of actual product and project management work. Get on the phone with a customer,write down what they do, and turn those into requirements. Do that a bunch of times, then synthesize a more generalrequirements document. Review it with leadership. Get approval to assign developers to work on the features to meetthose requirements. Ask other teams to develop features you need for your feature. Negotiate with everyone ondevelopment priorities in the next six months. Track progress of the development team. Produce demos to show thatprogress is being made. Present progress to leadership. That sort of thing. It's similar to being a PI on a researchgrant, except I had customers, dependencies, and ultimate accountability.As far as technical work, a lot of it revolved around meeting customers and internal partner teams where they were interms of their knowledge of HPC. I did a fair amount of technical marketing; I would come up withthe ways people should think about combining storage services together in their HPC workflows, then figure outhow to communicate that to audiences with vastly different levels of technical understanding. For example, I didn'town our Lustre product, object storageproduct, or HPC CPUnode product, but I owned thestory around how we envisioned all three services worked well together. This meant I would create slides andnarratives around this, then present them to anyone from our sales teams (who often had limited HPC-specific experience) tothe world's leading HPC centers.I also sometimes helped development teams accurately test their storage systems against HPC workloads. For example,when ChatGPT exploded, everyone wanted to know how well their storage service worked for training large languagemodels. I would talk to the engineers who trained LLMs, infer what their I/O patterns would be based ontheir description of how they did training, then design a benchmark that our developers could follow toemulate that LLM training workflow. Since I understood both the workload and the storage technology, it was often faster for me to translate between AI engineers and storage engineers rather than have them speak directly.HPC/AI developmentAs an HPC/AI engineer, my work is a lot more technical and focused. I'm on a \"white-glove support team\"that works directly with large, strategic customers in HPC and AI, so rather than working with dozens of customers and connect them to dozens of storage technologies, I work with one or two customers and the specific technologies on which they build their HPC or AI clusters. Because of this, I'd wager 95% of my day-to-day work istechnical.I don't spend much time in a terminal by virtue of my relative seniority. Instead, I sit in on a lot of internal meetings and represent the perspective of our strategic HPC and AI customers. For example, if we are trying to decide which CPU toinclude in our next HPC-optimized CPU node, I might work with our benchmarking engineers to develop a representativebenchmark and then interpret the results with the node's product managers. I'm not the person running the benchmarkmyself; instead, I might ask hard questions that the customer might ask, help decide the next experiments to run,and backstop our engineers if the customer starts poking too many holes in the work.I also function as a system architect at times; if a customer shows up with unusually large or complexHPC system requirements, I'll help translate the customer requirement (e.g., \"We need 10 TB/s of storage bandwidth)for individual product teams (e.g., \"they will be using N compute nodes and accessing storage via a network withthis topology and tapering, likely running an application that has this pattern, ...\"). This often requires understanding what the compute, network, and storage product teams are doing and being able to explain it all inwhatever terms each team understands. I also wind up sitting in on customer meetings and asking critical questionsso that we can make informed design tradeoffs.I do write code, but no more than I did when I was a system architect at NERSC. For example, I might pull PDUtelemetry from across a data center to help determine if oversubscribing the power for a future cluster wouldimpact workloads. The code itself is pretty straightforward statistical analysis, but interpreting it requires anunderstanding of a bunch of things ranging from the workload running on the nodes to how nodes are distributedacross PDUs, racks, rows, halls, and buildings.The remaining 5% of my work is not very technical and involves things I opt into because it's interesting orthe right thing to do. This might be spending time providing historical context for a business strategy document or showing up at a meeting to help explain the customer perspective to a finance or sales team.Am I happy with my decision and the new job?Yes, no, and yes.Broadly, yesI am glad I made the decision to leave NERSC and take on a job in Big Tech for a couple ofhigh-level reasons.As a product manager, I learned a lot about how businesses and corporations work to adegree that I never did when I worked at a startup and I never would have if I stayed with the government. Not onlydo I now know what the difference between gross and operating margin is, but I get it because I've had tobuild COGS and pricing models that could sustain and grow a new product. I know exactly how toprice cloud services (or any product or service, really) and where that money goes. I now pay much more attention toquarterly earnings reports, and I have a more confident opinion on what different elements of these reports sayabout a technology company's trajectory. This has equipped me with what feels like a much more completeunderstanding of the HPC industry overall.I'm also glad to work at a company that generally tries to do the right things. Itis investing heavily towards being carbonnegative (rather than just buying carbon offsets) while others are burning gas inefficiently in a race to be #1. It also matchesevery donation I make to 501(c)3 nonprofits which is a huge benefit that matches up with the ways inwhich I try to share my good fortune with others. And it beats employees over the heads with a strong, positive corporate culture which holds managers and leadersaccountable for the wellness of their employees. These sorts of things don't meaningfully exist in government, andthere are a lot of big corporations out prioritize short-term profits over the longer-term benefits that come from investing in sustainability and philanthropy.But for a long time, noHowever, I was unhappy for my first eighteen months.I took a gamble on storage product management being as interesting and fulfilling asengineering when I decided to step into this new job, and I lost that bet. I quickly came to realize that there's a big difference betweenbeing a storage person in an HPC organization and being an HPC person in a storage organization.When I worked in an HPC organization like NERSC, I was used to being the odd man outbecause parallel storage is a complicated topic that most HPC folks don't really understand. Despite that, everyone is still generally like-minded and appreciates the same things; everyone knows what MPI and InfiniBandare, and everybody knows what a checkpoint and restart might look like.Conversely, when I worked in a storage organization, I was an odd man out because nobodyreally understood HPC. The average engineer only had a vague notion of what MPI orInfiniBand accomplished. If you don't understand that MPI is what lets hundreds of servers all work on the samedistributed problem at once, it's easy to forget that an MPI application will also cause hundreds of servers toall write data at once. And if you've never used an MPI barrier, it's hard to internalize the fact that the wholeapplication stops until the slowest process finishes writing.Instead of worrying about tightly coupled applications, I realized that storage peopleworry about data availability and durability above all else. After all, storage's #1 job is to not lose data. Incontrast, it's not unusual for an HPC user to have hundreds of terabytes of data vanish because they forgot to copyit off of scratch before it got purged. This sharp difference in priorities--data durability versusperformance--causes friction, because at the end of the day, what's good for HPC (high bandwidth and low latency) isusually bad for storage (high durability and availability).The landscape of storage for HPC and storage for enterprises as I see it. If you care about one but work with people who care about the other, expect friction.These are technological differences, but they result in a persistent, elevated level of latent stress that never goes away. People tend to worry about the things they understand, and people tend to ask for helpabout the things that worry them. What this meant for me is that I spent a lot of time focusing on things thateveryone understood (like market trends, revenue, and general indicators of performance) instead of hard problemsunique to large-scale HPC. And because I was never solving the hard problems, I never got the gratification of feeling like I accomplished something that, as I learned, is an important motivator to me.To be clear, I realize that I made the decision to focus on problems that other people brought merather than carve out a space for me to work on the problems I felt were important. I'm sure that someone who wasmore tenacious and unafraid to pursue challenges that nobody else understood would have a very different experience as a PM. But after about a year, I realized that what I value and enjoy doing just isn'taligned with what a successful storage PM needs to be successful. I realized I didn't want to keep doing what I was doingfor another five years, so I decided to stop.Finally, yesI quite enjoy my role in HPC/AI engineering and development now, as it's similar towhat I used to do in the DOE. I have to learn about how different hardware, software,and systems work, and I have a lot of room to focus on challenges that play to my strengths and interests.For example, I love engaging with the HPC community, and my job still allows me to go out to the big HPC conferences to do that. At the same time, I also like getting into the guts of system behavior, and I still get to spend at least an hour or two a week doing something quantitative.My day-to-day is also steeped in that familiar feel of working in an HPC organization.Every cluster has a name that gets bandied about in meetings, and they have the same familiar challenges--fabric disruptions, firmwareupgrades, flaky nodes, and the like. The standard responsibilities are also all there; some teams perform systemadministration, others support users, and some of us focus on future system designs. But the cluster names aren't nearly as creative as those in the public sector (Eagle's real name sounds like a serial number). And they look pretty boring too; there are no fancy rack graphics.Five racks of a cloud GPU cluster that runs ND H100 v5-series VMs. SourceThere are also teams that have no analogue in the traditional HPC world, like those whoare responsible for things ranging from the smart NICs and software-defined networks to profits and losses. This iswhat keeps things interesting; I can just as easily spend an hour reviewing benchmark results from the latest GPUwith my teammates as I can learning how the control systems for liquidheat exchangers affect system reliability or data centersafety. When things are quiet and no fires are burning, going to work can sometimes feel like going to a bigplayground full of HPC and HPC-adjacent technology.Don't get me wrong; it's still a job, and there are still unpleasant tasks anduncomfortable situations. Working at a cloud provider means a lot of processes are designed to be slow and steady,and some teams struggle to understand why anyone would want to reboot every node in a cluster at once--such an event would be a massive outage in general-purpose cloud! But working in an HPC organization means that when thesesituations arise, I'm no longer the odd HPC guy--I'm on the odd HPC team.What does industry do better than the Labs?AccountabilityOrganizational planning happens twice a year,and this planning is the time when teams all get on the same page about whatwork to prioritize in the next six months (a semester). Teams coordinate dependent work with each other,trades horses on what the priority of each request is, and at the end of planning, have committed agreements about what workwill be done in the next semester. The progress on that work is tracked throughout the semester, delays andinterrupts are accounted, and there's an escalation path up through the ranks of management and leadership ifpriorities cannot be agreed upon by individual teams.The DOE Labs operate much more loosely in my experience. There, people tendto work on whatever pet projects they want until they lose interest. If a project is funded by a research grant, there are loosedeliverables and timelines (write X papers per year), but at the end of the day, nothing really bad happens if thework progresses slowly or its quality is poor. There's no penalty if a research grant results in a piece of softwarethat nobody uses or a paper that nobody reads. The value of the work is largely intellectual, and as a result, it's perfectlypossible to have a long career at a DOE lab, churning out papers and software, that lacks anylasting impact.Tying money to the value of work can make accountability much more black and white. If you pay a team of engineers amillion dollars a year to develop a new service that only increases revenue by a million dollars a year, thatservice is going to be scrutinized every time prioritization happens. Is there a way to increase its revenue throughbetter features or better positioning? It'll be a product manager's job to go figure that out. If the answer comesback as \"no,\" then that service might be put on a shelf and its engineering team reassigned to work on somethingthat has a greater impact. Those engineers don't get to decide that they keep wanting to work on the service thathas limited demonstrable value.At the same time, managers are accountable for the wellbeing of their team and the teams underneath them. Allemployees fill out regular, semi-anonymized surveys on different aspects of job satisfaction, and the results ofthese surveys roll up all the way to the top of the company. If employees are disgruntled, their managers know it,and those managers' managers know it, and everyone up the chain is accountable for improving those scores. Sometimes that resultsin increased hiring so engineers don't feel overworked. Other times it means reorganizing people and teams to alignthem with the work they are good at performing. And if nothing works and a team's morale keeps declining, maybe it's because ofthe manager--and the manager gets replaced.Pace and decision makingBecause managers and leaders are accountable, I've also found them to be much more empowered to just do what theyfeel is the right thing to do. Whereas no big decision in the DOE Labs can be made without reviews, panels,strategic offsites, more reviews, and presentations to headquarters--all of which could add months oryears to a project--the direction can move on a dime because all it takes is one executive to sign off and acceptfull responsibility for the consequences of their decision. Getting the approval to staff up and pursue a good ideaoften requires only winning over one or two key people, not an army of feds in Germantown or an anonymous reviewpanel who isn't conversant in what you're proposing.And again, sometimes money makes decisions much easier to make. For example, a few people at ISC'24 asked me why wedidn't re-do the Top500 run for Eagle to beat Aurora since theSC'23 scoring was so close. The decision process can be as simple as this:According to the Top500 list's rawdata, Eagle achieved 561,200 TFlop/s using an Nmax of 11,796,480.Knowing that HPL's walltime is (flop count / Rmax) and HPL's flop count is (2/3 * Nmax^3), you can calculatethat the HPL walltime for this run was 1,950 seconds or 0.512 hours.The public list price for an Eaglenode (ND96isr H100 v5) is something like $60 an hour.The HPL run used 1,800 such nodes.Give the above, during the half hour it would take to run HPL, those same nodes could berunning a production workload which would have resulted in $58,000 in revenue. That is, the opportunity costof re-running HPL is at least $58,000 in lost revenue. In reality, it would take time to boot up and configure thecluster of virtual machines and do a few scale-up runs which would tie up the nodes for a couple hours, makingthis opportunity cost closer to a couple hundred thousand dollars.Is getting a marginally higher Top500 score worth a couple hundred thousand dollars if yourmachine is already listed and had its day in the sun? I don't need an executive to answer that question. But in thepublic HPC space, who's to say what the opportunity cost is? If HPL wasn't running twice a year on Frontier, are thedozen or so lattice QCD jobs that would be running instead worth a couple hundred thousand dollars?RelevanceI might be more vain than I thought when I worked for the government, because I really enjoy being able to talk about the work that I dowith the general public now. When people ask, \"What work do you do?\" and I respond with, \"Have you ever heard of Copilot orChatGPT?\" there is almost always a conversation that follows. People may not really understand how artificial intelligence andlarge language models work, but they've played with those technologies and have opinions and questions. Sometimes the conversation is about big-picture stuff like \"will AI take over the world?\" At other times it's specific like \"what do you think aboutAI's effect on global climate change?\" Because I am steeped in all aspects of AI in my day-to-day work, I canusually speak intelligently about any dimension of the AI industry when my neighbors ask.Every blog post these days needs at least one AI-generated picture, so here is a picture generated by DALL-E that \"captures the essence of explaining AI concepts to neighbors in a friendly, approachable setting.\" But more poignantly, my team directly supports the supercomputers that trained the model that generates these pictures.This was a much bigger challenge when I worked in the public sector. When I told people that I worked at Lawrence BerkeleyNational Lab, nobody knew what I was talking about half of the time. The other half of the time, people would think I worked onnuclear weapons because Lawrence Livermore National Lab has a confusingly similar name and geography. And if theconversation ever got as far as what people did on the supercomputers I supported, it would rapidly tail off onceall parties (including me) realized that cosmological hydrodynamics and quantum Monte Carlo don't really make for great conversation since they don't touch people's everyday lives.This isn't to say that the work done at the Labs isn't important. But the general public doesn't understand it, andto a large degree, doesn't really care about it. I realize that being able to impress your neighbors with what youdo isn't at the top of the list of most people's job requirements, but I get a lot of satisfaction out of it.Technically: securityHPC doesn't really worry about cybersecurity. Every HPC center has a security group and does scans and threatmodeling, but at the end of the day, the security practices on all the largest supercomputers in the public sectorare roughly the same as they were twenty years ago. Users ssh into a login node, and once you're inside, you haveaccess to everything. You can see everyone else who's logged in, you can see everyone who chmodded their homedirectory to be +777, and the only thing separating you from everyone else is the Linux kernel. Passwordless ssh iseverywhere, and often times, passwordless ssh for the root user is everywhere.This does not fly with paying commercial HPC and AI customers in the cloud who use supercomputing to develop betterproducts faster than their competitors. For example, both Arm and AMD have publiclystated that they perform a lot of their silicon design simulations using HPC in the cloud. What would happenif both AMD and Arm used the same cluster and one accidentally made their project directory world-readable? Shoulddomain scientists' understanding of how POSIX file permissions work really be the last line of defense against anext-generation CPU or GPU's specs being leaked to the competition?I had to quickly learn about modern security practices when I started doing HPC in the commercial cloud out ofnecessity. I'm still nowhere close to being a security expert, but two years has been long enough for me to nowcringe when I talk to my colleagues in the traditional HPC community about how they protect against threats. It'snot really their fault that most of the HPC community hasn't adopted modern practices, because the tools andpractices required to do it right aren't easy to set up, automate, and maintain from scratch.For example, basic LDAP is a short path to allowing users to log into a cluster's nodes, but if those users also needto authenticate themselves to REST services that support an HPC workflow across multiple clusters, you have to start building a Rube Goldberg machine of software on top of LDAP. Similarly, sticking every user on their own overlay network is great to limit the blast radius of acompromised account. However, automating the configuration of VXLAN tunnel endpoints as nodes get allocated and deallocated tojobs requires a lot of fancy orchestration that is either very complicated to build and maintain yourself or veryexpensive to buy and maintain. As a result, HPC just accepts the risk. Cloud hasfigured all this out though, and the price of providing this security infrastructure is included in the cost ofcloud-based supercomputers.But the pay is good, right?Like I said before I left the public sector, my base salary iscomparable to what I got at the lab. It's actually gotten less competitive because all salaries were frozen when I was first eligible for a raise. So, after considering the effects of inflation, my paycheck is a little lower than what it was in the government two years ago.What's different is the bonus structure which simply does not exist in the government or university world. For thosewho aren't familiar with how bonuses work in the tech industry, I'll share how it works for me:In the first year, I was awarded two signing bonuses: one in cash, one in stock. Half of the cash bonus was paidout up-front, and the other half was paid out after I had been there a year. The stock grant cannot be touchedduring the first year because it had a one-year \"cliff.\"On my one-year anniversary, I got the second half of my cash signing bonus, and my signing stock grant began\"vesting.\"After a year, I was also eligible for an annual performance-based raise, cash bonus, and stock bonus.Because of the economy, my annual raise was zero.The cash bonus was paid out in a lump sum, similar to my cash signing bonus.The stock bonus was awarded all at once but follows a multi-year \"vesting schedule\" which means I am onlyactually given fractions of the total award over time. However, these bonuses don't have a \"cliff\" and beginvesting immediately.Every year thereafter, I am eligible for an annual raise, cash bonus, and another stock bonus.The way stock bonuses work was the least intuitive part to me, but since it's such a significant part of total compensation, it's worth spellingout for anyone who's considering an offer that includes this:Stock bonuses are defined in terms of dollar values. For example, let's say I got a signing stock bonus of $1000with a one-year cliff that vests quarterly (every three months) over five years.On the day that stock bonus is awarded, my employer converts that $1000 value into company stock based on themarket value that day. If stocks are $50 per share, I am awarded 20 shares. My employer hangs on to those shareson my behalf, so I can't actually do anything with them yet.Since I have a five-year vesting schedule and the award vests quarterly, my shares will vest twenty times (fourquarters, five years). Coincidentally, since I have 20 shares, I will get one share per quarter.However, because I have a one-year cliff, I get all four quarters of my first year at my one-year anniversary.So, four shares should appear in my brokerage account on my one-year anniversary. Once a share is in mybrokerage account, I can do whatever I want with it (like sell it immediately!)Every quarter thereafter, one more share vests and appears in my brokerage account.Assuming I get a stock bonus as part of my overall annual bonus, this means that stockawards pile up and vest every year. This is tricky for two reasons:Although my initial stock award was $1,000 in the above example, that amount was converted to stock the day itwas awarded. Assuming I am doing a good job and increasing the value of my employer's stock, the value ofthose shares will increase while they're vesting. This means by the time the first four shares of my awardvested at my one-year anniversary, they were worth more than the $50 per share they represented when they wereawarded. More broadly, the value of a stock bonus tends to increase over time, making the true cash value of a$1000 stock bonus worth a lot more than $1000 by the time it completely vests.Every year's stock award comes with its own multi-year vesting period, which means at any given time, I havemultiple years' bonuses all vesting at once. This also means that at any given time, I have a bunch of unvestedstock that's worth a lot of money that I can't yet spend. If I quit my job though, all these unvested sharesvanish into thin air.These two factors make up the golden handcuffs that people often talk about in industry.The longer I stick around, the more unvested stock I have hanging over my head, and it usually becomes increasinglyvaluable (yet inaccessible!) over time. The reality is that if you've put in a few years in Big Tech, you might haveyears' worth of base salary tied up in unvested stock that all goes away if you quit.The end result is that although base salary is competitive with what you can make in a government HPC facility, there's a significant cash bonus that falls out of the sky once a year, andan appreciable amount of stock appears in your brokerage account every couple of months which you can turn aroundand sell for more cash. Depending on seniority and performance, these bonuses can add up to a significant fractionof base salary.Finally, the above is consistent with what I've seen firsthand at two companies in Big Tech but may be different based on the role and the company. For example, field-facing roles in sales and support may be completely different beasts, and private companies and startups load things differently due to the value of equity.How's work-life balance?It hasn't been different than working in the government. Just like at a lab or university, some peoplework around the clock while others stick pretty close to the standard workday. There may be a higher concentrationof Type A personalities who put in a lot of time in Big Tech, and this may pressure others to keep up and also putin long hours, but there's rarely been an occasion where a manager expects staff to routinely work nights andweekends. Doing so would probably result in negative employee satisfaction scores which would roll up and eventuallyhave to be addressed.Of course, there are cases where working odd hours is required to get the job done. BecauseI work for a global organization, I've had to get up early to meet with teams or customers in Europe. I've also hadto stay up late to meet with teams or customers in Asia. And in some particularly annoying days, I've had to do bothand wind up working from 5am to 8pm. But I never felt that I had no choice in the matter; I pulled these hoursbecause it was the right thing to do at the time. And I don't see this as being too different from the days when I'dwork sixteen-hour days, seven days a week, for the entire month of March to put together a paper for SC. Or dayswhen I'm at SC and am preparing talks, meeting with partners, and otherwise hustling from 8am to 1am for five daysstraight.One big difference is the fact that my employer offers discretionary time off (\"unlimited vacation\"). This is a divisive topic in industry, but I see it as a positive for work-life balancebecause it underscores an emphasis on outcomes rather than output. I can take an afternoonoff or enjoy a long weekend with little fanfare, because productivity is infinitely more valuable that presence. Aslong as I do what needs to get done, I don't have to worry about timing vacations to ensure I am banking enough timeoff in between.Do you miss anything about working at the lab?Absolutely. There are a bunch of appealing things about working in a DOE lab (or an NSF center)that I've had to give up since coming to industry.Freedom to have an off dayRight before I finished graduate school, I hada conversation with Professor Edmund Webb soonafter he became a professor after a decade-long career at Sandia National Labs about life at the Labs. He said that,after becoming a professor, he lost the ability to just close the door to his office and focus onsomething he needed to get done for a day. I didn't really grasp what this meant at the time, but I totally get it now. TheDOE might be one of the few places where you can take a day--maybe even a week--and just close your door toeverything else that's going on around you to focus on what you want to do. In the case of professorship, there's always students requiring attention; in industry, it's customers and partners.I think this difference results from two factors: very few things in publicHPC are very urgent, and the Labs are stocked full of independent, free-thinking Ph.D. types. There's rarely apenalty if something is late by a day (or two years! Remember when Aurorawas called \"A21?\"), but there can be huge payoff in prestige if one of your wacky side projects turns out to besomething useful (this is how Shifter came to be). By comparison, working at a giant corporation often means there are a bunch of interdependencieson others, and the odds of any one of your 200,000 coworkers sending you a Teams message asking for help is just a lot higher than it is at a 70-person supercomputer center. The culture is much more team-oriented, and being a one-person army isn't incentivized as much.TravelPart of my job within the DOE complex was to go around the country (and the world) and be smart, and secondarily,show that my lab hired smart people and did smart things. If headquarters wanted to make sure that the supercomputerthey were about to spend $500M on was technically sound, I'd sometimes get invited to go sit in on a review and tryto poke holes in the design. If a European HPC project wanted to ensure they were including a global perspective onsome dimension of future HPC strategy, I'd sometimes get invited to give a talk about how I view the world of data.And if these reviews and workshops happened to be in awesome places around the world--oh well!I feel a lot more self-conscious about requesting approval to attend these sorts of boondoggles as an engineer nowbecause the first question I have to answer is, \"Is this trip business critical?\" If there's a direct line of sightbetween me giving a talk at a workshop and a specific business strategy, I can say \"yes\" with a straight face. But it'shard to accept an invitation to fly off to Switzerland to give a 30-minute talk when I know that my attendance isn'tgoing to move any needles.OpennessJust like it's no longer my job to travel the world and just be smart, it's not my job to write about the work that I(or my team) does. I miss writing papers and giving technical talks, because the process of putting togethercoherent thoughts around a technical topic is one of the ways I really come to understand it. There's also a lot ofreally wild ideas that we're pursuing at scale that the scientific computingcommunity has never considered, but there are two factors that work against being open about these things:In terms of prioritization, my time is always better spent solving problems, or at least documenting them forinternal audiences who fully grasp the context around them, than writing about them in a way that the rest ofthe world can understand. It's hard to justify the time to write a retrospective or a study unless there's astrategic advantage behind it.The customers I support typically do not want the world knowing what they're doing. There is an AI arms racehappening right now, and having the technical sophistication to utilize massive-scale supercomputers effectivelyis a competitive advantage. In the traditional HPC community, only national security is comparable to the levelof secrecy involved, and none of the intelligence agencies are openly contributing to the state of the art inHPC either.So instead of making conference papers and presentations, these days I make more internal papers and presentations.I'm trying to figure out ways to publish interesting technical anecdotes on my website (for example, I maintain a collection of LLM training requirements as I am exposed to them), but it's a lot of extra work to disentangle the proprietary bits from my work notes to do this.Related to openness is also freedom to speak my mind in public forums. I had the most latitude to blast myopinions out on to the Internet when I was still early in my career and nobody listened to me, but I've had to getprogressively less opinionated over the years. At this point, I abide by a written corporate social media policywhich, although very reasonable in what it requests (don't slander competitors, always be transparent about who employs you), it stops me from commenting on news as much as I used to since so many techcompanies qualify as competitors in some dimension.Would you still have left knowing what you know now?Yes. I still stand by just about everything I wrote in my original blog post; at the time, I just needed a change, and Ifound the change that I was looking for. Without immersing myself in the world of cloud, I would havenever learned about virtualization, physical infrastructure, or modern security to the degree that I have. And the fact that Istumbled into what has become one of the leading companies in AI at the dawn of generative AI was an extremely luckycoincidence.However, this doesn't mean that I now turn my nose up at doing HPC in the public sector.There are many unique aspects to working at a DOE lab or NSF center that have no parallel in industry. I also believe that I amthe sum of the experiences that led me to where I work today, and I would never have gotten the opportunity to writethis retrospective if I didn't learn everything I did working in the DOE and NSF.And perhaps above all else, there is something attractive about public service that I haven't beenable to shake in the last two years. I still dial in to ASCACmeetings to see what the world of public HPC and scientific computing is thinking and doing, and I still tryto contribute time and attention to working groups like NITRD's MAGIC. I write lengthy blog posts in a futile attempt to caution the leaders in public-sector HPC againstrejecting AI workloads in commercial clouds as HPC. And every time I learn some slick way we deal with hard technological or sociological issues at work, I still file it away in the \"good ideas for when I goback\" folder in the back of my mind.I don't have any near-term plans on going anywhere though. Like I said before, there arestill plenty of days when dialing into work is like going to the playground. Amazing things are happening in theworld of HPC infrastructure at scale now that the world is pouring money into AI, and the rate of scale andinnovation is no longer constrained to 40 MWand $500M persupercomputer like it was when public-sector HPC was setting the bar for leadership. There is a whole new exciting world of challenges and possibilities when you start thinking about building supercomputers that consume hundreds of megawatts of power.Like I wrote two years ago, I don't think any government has the appetite to build data centers for scientific computing that are larger than today's 50 MW exascale facilities. This means that government HPC centers will never have a reason to explore the exciting world of 100+ MW supercomputers or work on the wacky problems that arise at that scale. Consequently, the biggest and most challenging problems in HPC--at least in terms of infrastructure and systems design at scale--are becoming unique to industry, not public HPC.I got into HPC because I enjoy working on large, complex systems. Considering where I am at this stage of my life, what I want to accomplish in the rest of my career, and what gets me out of bed in the morning, I feel like I wound up in the right place for now. I have no regrets.",
            "content_html": "<p>June 2024 marked two years since I <a href=\"http://blog.glennklockwood.com/2022/05/life-and-leaving-nersc.html\">left my job at one of the world's most prestigious government HPC centers for a job in one of the world's largest technology corporations</a>. In that time, the world of HPC has changeddramatically; just six months after I started, ChatGPT was released and triggered a gold rush in AI that is now overshadowing traditional scientific computing. This shift brought about massive HPCdeployments led by hyperscalers, challenging the long-held belief that only national governments coulddeploy and operate world-leading supercomputers. <a href=\"http://blog.glennklockwood.com/2024/05/isc24-recap.html\">My experiences atISC'24 this past summer</a> made clear to me that the traditional HPC community is now rethinking their role the industry, and some individuals who built their careers in public HPC are revisiting theirassumption that world-class HPC systems are limited to the public institutions that havehistorically dominated the <a href=\"https://www.top500.org/lists/top500/list/2024/06/\">top of the Top500 list</a>. Ihad no idea things would unfold this way when I left my job at NERSC back in 2022, and I've been remarkably lucky tonow be a part of the largest forces driving this huge shift in HPC.</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure style=\"display: inline-block; margin-left: 1em; margin-right: 1em;\"><figcaption style=\"font-size: 14px; margin-top: 5px;\">One of my new offices. Nicer than my old government office, and it has free food, but it's a ninety-minute drive each way.</figcaption></figure></div><p>In the spirit of openness and helping others who are facing similar career decisions, I thought I would follow up on my <a href=\"http://blog.glennklockwood.com/2022/05/life-and-leaving-nersc.html\">Life and leavingNERSC</a> post by sharing how my professional journey from DOE HPC into cloud HPC has been going. I'll first explain the path I've traveled over these past two years, then answer some of the mostcommon questions I've been asked about this transition.</p><p>As a forewarning, this is not a typical technology-focused post, and most of this might be obvious to people who already work in Big Tech. Here are the questions on which I reflected:</p><ol><li><a href=\"http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#first-two-years\">What happened during my first two years in Corporate America?</a></li><li><a href=\"http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#what-i-do\">So what do I actually do?</a><ol><li><a href=\"http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#storage-product-management\">Storage product management</a></li><li><a href=\"http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#hpc-ai-development\">HPC/AI development</a></li></ol></li><li><a href=\"http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#am-i-happy\">Am I happy with my decision and the new job?</a><ol><li><a href=\"http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#am-i-happy-broadly-yes\">Broadly, yes</a></li><li><a href=\"http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#long-time-no\">But for a long time, no</a></li><li><a href=\"http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#finally-yes\">Finally, yes</a></li></ol></li><li><a href=\"http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#industry-does-better\">What does industry do better than the Labs?</a><ol><li><a href=\"http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#accountability\">Accountability</a></li><li><a href=\"http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#pace-and-decision-making\">Pace and decision making</a></li><li><a href=\"http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#relevance\">Relevance</a></li><li><a href=\"http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#technically-security\">Technically: security</a></li><li><a href=\"http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#pay-good\">But the pay is good, right?</a></li><li><a href=\"http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#work-life-balance\">How's work-life balance?</a></li></ol></li><li><a href=\"http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#what-i-miss\">Do you miss anything about working at the lab?</a><ol><li><a href=\"http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#freedom-to-have-an-off-day\">Freedom to have an off day</a></li><li><a href=\"http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#travel\">Travel</a></li><li><a href=\"http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#openness\">Openness</a></li></ol></li><li><a href=\"http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#regret-decision\">Would you still have left NERSC knowing what you know now?</a></li></ol><h2 id=\"first-two-years\">What happened during my first two years in Corporate America?</h2><p>I published my <a href=\"http://blog.glennklockwood.com/2022/05/life-and-leaving-nersc.html\">Life and leaving NERSC</a> blog post on aThursday, which was my last day working at NERSC. The following Monday was my first day at the new job, and beinghired as 100% remote, it didn't feel that different; I was just booting up a Lenovo laptop (yuck) instead of aMacBook, using Teams and Outlook instead of Slack, GSuite, and Zoom, and that sort of thing.</p><p>However, the job was undeniably different; whereas I used to be an engineer at NERSC, I was hired to be a \"Principal ProductManager\" within the cloud storage organization which was responsible for all object, disk, and file storageservices. Although my title was \"product manager,\" I wasn't a people manager, andI didn't manage any specific storage products. Rather, my responsibility was to act as an HPC-focused overlayacross all cloud storage services, and my job was to represent the interests of HPC users to all the people who did manage specific storage products. I didn't define product or feature roadmaps myself, but I could help those responsible for each product or service understand how to shape their roadmaps to benefit HPC workloads.</p><p>I struggled in this position for a variety of reasons, so after I gave the new rolean honest six to nine months, I decided that being a storage product manager just wasn't a good fit for me.Unfortunately, I reached this decision after the <a href=\"https://www.nytimes.com/2022/07/21/business/yield-curve-inversion.html\">yield curve inverted</a> and<a href=\"https://www.theverge.com/2023/1/18/23560315/microsoft-job-cuts-layoffs-2023-tech\">mass-layoffs and hiringfreezes</a> were implemented, so there weren't a lot of places to go other than back to a government lab.Although I wasn't thriving as a storage product manager, I did have allies that helped me navigate myday-to-day struggles, and I decided to wait until more opportunities opened up and learn as much aboutproduct management as I could in the meantime.</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure style=\"display: inline-block; margin-left: 1em; margin-right: 1em;\"><figcaption style=\"font-size: 14px; margin-top: 5px;\">The yield curve inverted a month after I started my new job. Not great timing.</figcaption></figure></div><p>After a little over a year as a storage product manager, a new engineering role opened up within a sister team in ourHPC/AI infrastructure organization. After discussing the needs and nature of the work with the hiring manager, I applied for the job, went through theinterview process, and was eventually given a verbal offer to join his team in June 2023. Unfortunately, the globaleconomic outlook was still uncertain, and I wound up sitting in a holding pattern (as a storage productmanager) from June 2023 to November 2023. It wasn't until the week of SC'23 that I finally got the written offerletter, and I spent December wrapping up loose ends within the storage organization.</p><p>On January 2, 2024, I began my new (and current) role within the company. The move was completely lateral, but I changed job titles from \"Product Manager\" to \"Software Engineer,\" and Ichanged organizations from storage to specialized compute.</p><p>I say all this because my experiences in making the professional transition from government HPC to cloud HPC are colored by the fact that I really changed jobs twice. I've had both product management and engineering/development roles, and I've been in both storage and HPCorganizations.</p><h2 id=\"what-i-do\">So what do I actually do?</h2><p>I've had two very different roles within the same orbit of HPC/AI infrastructure, so I'lldescribe them separately to give you a sense of the breadth of HPC roles possible.</p><h3 id=\"storage-product-management\">Storage product management</h3><p>As a <b>storage product manager</b> (PM), I was an inch deep but a mile wide on every storage service, every commercial HPC workload, andall the ways in which those two could touch each other. I'd guess that only 25% of my day-to-day work required deepexpertise in HPC; the remainder was either business-centric or required only understanding HPC in broad strokes. Thiswas quite unlike the things I'd done earlier in my career in the public sector, since there's not an equivalent towhat a product manager does within the DOE Labs.</p><p>For example, I spent a lot of my time as a storage PM explaining the basics of HPC I/O to different teams within thecompany. When most cloud people think \"storage,\" they are really thinking about either enterprise storage (thingslike virtual disks for virtual machines) or content distribution (think serving up content for web apps). Theconcept of hundreds or thousands of VMs all writing to the same place at the same time is standard practice in theHPC world, but in the cloud world, this is a <a href=\"https://www.microsoft.com/en-us/security/business/security-101/what-is-a-ddos-attack?msockid=2008901357a56c4518b3840856e96dad\">DDoSattack</a>. Since my organization was responsible for all storage, not just HPC storage, there were a lot ofpeople who simply never had to think about the challenges that HPC people take for granted, and it could be challenging (as the new guy) to convince seasoned cloud storage PMs that some workloads legitimately need hundreds of gigabytes per second of bandwidth.</p><p>As a PM, I also wound up doing a fair amount of business reporting. For example, object storage is used by all manner of cloudcustomers, so prioritizing features that specifically help HPC customers required understanding how many HPCcustomers actually used it. How do you define whether a workload is really an HPC workloads or not? InDOE, we'd waste hours debating stuff like this for no real purpose, but when I became a product manager, I had todefine this to make the business case that we needed to develop a certain feature that would only be used by HPC workloads.</p><p>Finally, I did a fair amount of actual product and project management work. Get on the phone with a customer,write down what they do, and turn those into requirements. Do that a bunch of times, then synthesize a more generalrequirements document. Review it with leadership. Get approval to assign developers to work on the features to meetthose requirements. Ask other teams to develop features you need for your feature. Negotiate with everyone ondevelopment priorities in the next six months. Track progress of the development team. Produce demos to show thatprogress is being made. Present progress to leadership. That sort of thing. It's similar to being a PI on a researchgrant, except I had customers, dependencies, and ultimate accountability.</p><p>As far as technical work, a lot of it revolved around meeting customers and internal partner teams where they were interms of their knowledge of HPC. I did a fair amount of technical marketing; I would come up withthe ways people should think about combining storage services together in their HPC workflows, then figure outhow to communicate that to audiences with vastly different levels of technical understanding. For example, I didn'town our <a href=\"https://learn.microsoft.com/en-us/azure/azure-managed-lustre/amlfs-overview\">Lustre product</a>, <a href=\"https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction\">object storageproduct</a>, or <a href=\"https://learn.microsoft.com/en-us/azure/virtual-machines/sizes/high-performance-compute/hb-family\">HPC CPUnode product</a>, but I owned <a href=\"https://techcommunity.microsoft.com/t5/azure-high-performance-computing/azure-managed-lustre-not-your-grandparents-parallel-file-system/ba-p/3889946\">thestory around how we envisioned all three services worked well together</a>. This meant I would create slides andnarratives around this, then present them to anyone from our sales teams (who often had limited HPC-specific experience) tothe world's leading HPC centers.</p><p>I also sometimes helped development teams accurately test their storage systems against HPC workloads. For example,when ChatGPT exploded, everyone wanted to know how well their storage service worked for training large languagemodels. I would talk to the engineers who trained LLMs, infer what their I/O patterns would be based ontheir description of how they did training, then design a benchmark that our developers could follow toemulate that LLM training workflow. Since I understood both the workload and the storage technology, it was often faster for me to translate between AI engineers and storage engineers rather than have them speak directly.</p><h3 id=\"hpc-ai-development\">HPC/AI development</h3><p>As an <b>HPC/AI engineer</b>, my work is a lot more technical and focused. I'm on a \"white-glove support team\"that works directly with large, strategic customers in HPC and AI, so rather than working with dozens of customers and connect them to dozens of storage technologies, I work with one or two customers and the specific technologies on which they build their HPC or AI clusters. Because of this, I'd wager 95% of my day-to-day work istechnical.</p><p>I don't spend much time in a terminal by virtue of my relative seniority. Instead, I sit in on a lot of internal meetings and represent the perspective of our strategic HPC and AI customers. For example, if we are trying to decide which CPU toinclude in our next HPC-optimized CPU node, I might work with our benchmarking engineers to develop a representativebenchmark and then interpret the results with the node's product managers. I'm not the person running the benchmarkmyself; instead, I might ask hard questions that the customer might ask, help decide the next experiments to run,and backstop our engineers if the customer starts poking too many holes in the work.</p><p>I also function as a system architect at times; if a customer shows up with unusually large or complexHPC system requirements, I'll help translate the customer requirement (e.g., \"We need 10 TB/s of storage bandwidth)for individual product teams (e.g., \"they will be using N compute nodes and accessing storage via a network withthis topology and tapering, likely running an application that has this pattern, ...\"). This often requires understanding what the compute, network, <i>and</i> storage product teams are doing and being able to explain it all inwhatever terms each team understands. I also wind up sitting in on customer meetings and asking critical questionsso that we can make informed design tradeoffs.</p><p>I do write code, but no more than I did when I was a system architect at NERSC. For example, I might pull PDUtelemetry from across a data center to help determine if oversubscribing the power for a future cluster wouldimpact workloads. The code itself is pretty straightforward statistical analysis, but interpreting it requires anunderstanding of a bunch of things ranging from the workload running on the nodes to how nodes are distributedacross PDUs, racks, rows, halls, and buildings.</p><p>The remaining 5% of my work is not very technical and involves things I opt into because it's interesting orthe right thing to do. This might be spending time providing historical context for a business strategy document or showing up at a meeting to help explain the customer perspective to a finance or sales team.</p><h2 id=\"am-i-happy\">Am I happy with my decision and the new job?</h2><p>Yes, no, and yes.</p><h3 id=\"am-i-happy-broadly-yes\">Broadly, yes</h3><p>I am glad I made the decision to leave NERSC and take on a job in Big Tech for a couple ofhigh-level reasons.</p><p>As a product manager, I learned a lot about how businesses and corporations work to adegree that I never did when I worked at a startup and I never would have if I stayed with the government. Not onlydo I now know what the difference between gross and operating margin is, but I <i>get</i> it because I've had tobuild <a href=\"https://www.investopedia.com/terms/c/cogs.asp\">COGS</a> and pricing models that could sustain and grow a new product. I know exactly how toprice cloud services (or any product or service, really) and where that money goes. I now pay much more attention toquarterly earnings reports, and I have a more confident opinion on what different elements of these reports sayabout a technology company's trajectory. This has equipped me with what feels like a much more completeunderstanding of the HPC industry overall.</p><p>I'm also glad to work at a company that generally tries to do the right things. Itis investing heavily towards being <a href=\"https://blogs.microsoft.com/blog/2020/01/16/microsoft-will-be-carbon-negative-by-2030/\">carbonnegative</a> (rather than just buying carbon offsets) while others are <a href=\"https://www.tomshardware.com/tech-industry/artificial-intelligence/elon-musks-new-worlds-fastest-ai-data-center-is-powered-by-massive-portable-power-generators-to-sidestep-electricity-supply-constraints\">burning gas inefficiently</a> in a race to be #1. It also <a href=\"https://givebutter.com/blog/companies-that-match-donations\">matchesevery donation I make to 501(c)3 nonprofits</a> which is a huge benefit that matches up with the ways inwhich I try to share my good fortune with others. And it beats employees over the heads with a strong, positive <a href=\"https://careers.microsoft.com/v2/global/en/culture\">corporate culture</a> which holds managers and leadersaccountable for the wellness of their employees. These sorts of things don't meaningfully exist in government, andthere are a lot of big corporations out prioritize short-term profits over the longer-term benefits that come from investing in sustainability and philanthropy.</p><h3 id=\"long-time-no\">But for a long time, no</h3><p>However, I was unhappy for my first eighteen months.</p><p>I took a gamble on storage product management being as interesting and fulfilling asengineering when I decided to step into this new job, and I lost that bet. I quickly came to realize that there's a big difference betweenbeing a <u>storage person in an HPC organization</u> and being an <u>HPC person in a storage organization</u>.</p><p>When I worked in an HPC organization like NERSC, I was used to being the odd man outbecause parallel storage is a complicated topic that most HPC folks don't <i>really</i> understand. Despite that, everyone is still generally like-minded and appreciates the same things; everyone knows what MPI and InfiniBandare, and everybody knows what a checkpoint and restart might look like.</p><p>Conversely, when I worked in a storage organization, I was an odd man out because nobodyreally understood HPC. The average engineer only had a vague notion of what MPI orInfiniBand accomplished. If you don't understand that MPI is what lets hundreds of servers all work on the samedistributed problem at once, it's easy to forget that an MPI application will also cause hundreds of servers toall write data at once. And if you've never used an MPI barrier, it's hard to internalize the fact that the wholeapplication stops until the slowest process finishes writing.</p><p>Instead of worrying about tightly coupled applications, I realized that storage peopleworry about data availability and durability above all else. After all, storage's #1 job is to not lose data. Incontrast, it's not unusual for an HPC user to have hundreds of terabytes of data vanish because they forgot to copyit off of scratch before it got purged. This sharp difference in priorities--data durability versusperformance--causes friction, because at the end of the day, what's good for HPC (high bandwidth and low latency) isusually bad for storage (high durability and availability).</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure style=\"display: inline-block; margin-left: 1em; margin-right: 1em;\"><figcaption style=\"font-size: 14px; margin-top: 5px;\">The landscape of storage for HPC and storage for enterprises as I see it. If you care about one but work with people who care about the other, expect friction.</figcaption></figure></div><p>These are technological differences, but they result in a persistent, elevated level of latent stress that never goes away. People tend to worry about the things they understand, and people tend to ask for helpabout the things that worry them. What this meant for me is that I spent a lot of time focusing on things thateveryone understood (like market trends, revenue, and general indicators of performance) instead of hard problemsunique to large-scale HPC. And because I was never solving the hard problems, I never got the gratification of feeling like I accomplished something that, as I learned, is an important motivator to me.</p><p>To be clear, I realize that I made the decision to focus on problems that other people brought merather than carve out a space for me to work on the problems I felt were important. I'm sure that someone who wasmore tenacious and unafraid to pursue challenges that nobody else understood would have a very different experience as a PM. But after about a year, I realized that what I value and enjoy doing just isn'taligned with what a successful storage PM needs to be successful. I realized I didn't want to keep doing what I was doingfor another five years, so I decided to stop.</p><h3 id=\"finally-yes\">Finally, yes</h3><p>I quite enjoy my role in HPC/AI engineering and development now, as it's similar towhat I used to do in the DOE. I have to learn about how different hardware, software,and systems work, and I have a lot of room to focus on challenges that play to my strengths and interests.For example, I love engaging with the HPC community, and my job still allows me to go out to the big HPC conferences to do that. At the same time, I also like getting into the guts of system behavior, and I still get to spend at least an hour or two a week doing something quantitative.</p><p>My day-to-day is also steeped in that familiar feel of working in an HPC organization.Every cluster has a name that gets bandied about in meetings, and they have the same familiar challenges--fabric disruptions, firmwareupgrades, flaky nodes, and the like. The standard responsibilities are also all there; some teams perform systemadministration, others support users, and some of us focus on future system designs. But the cluster names aren't nearly as creative as those in the public sector (<a href=\"https://www.top500.org/system/180236/\">Eagle's</a> real name sounds like a serial number). And they look pretty boring too; there are no fancy rack graphics.</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure style=\"display: inline-block; margin-left: 1em; margin-right: 1em;\"><figcaption style=\"font-size: 14px; margin-top: 5px;\">Five racks of a cloud GPU cluster that runs ND H100 v5-series VMs. <a href=\"https://www.youtube.com/watch?v=ntKZ5CibuIQ\">Source</a></figcaption></figure></div><p>There are also teams that have no analogue in the traditional HPC world, like those whoare responsible for things ranging from the <a href=\"https://techcommunity.microsoft.com/t5/azure-infrastructure-blog/announcing-the-general-availability-of-azure-boost/ba-p/3981384\">smart NICs</a> and software-defined networks to profits and losses. This iswhat keeps things interesting; I can just as easily spend an hour reviewing benchmark results from the latest GPUwith my teammates as I can learning how the control systems for <a href=\"https://news.microsoft.com/source/features/ai/in-house-chips-silicon-to-service-to-meet-ai-demand/\">liquidheat exchangers</a> affect system reliability or <a href=\"https://www.osha.gov/noise/standards\">data centersafety</a>. When things are quiet and no fires are burning, going to work can sometimes feel like going to a bigplayground full of HPC and HPC-adjacent technology.</p><p>Don't get me wrong; it's still a job, and there are still unpleasant tasks anduncomfortable situations. Working at a cloud provider means a lot of processes are designed to be slow and steady,and some teams struggle to understand why anyone would want to reboot every node in a cluster at once--such an event would be a massive outage in general-purpose cloud! But working in an HPC organization means that when thesesituations arise, I'm no longer the <i>odd HPC guy</i>--I'm on the <i>odd HPC team</i>.</p><h2 id=\"industry-does-better\">What does industry do better than the Labs?</h2><h3 id=\"accountability\">Accountability</h3><p>Organizational planning happens <a href=\"https://devblogs.microsoft.com/azure-sdk/planning-2021/\">twice a year</a>,and this planning is the time when teams all get on the same page about whatwork to prioritize in the next six months (a <i>semester</i>). Teams coordinate dependent work with each other,trades horses on what the priority of each request is, and at the end of planning, have committed agreements about what workwill be done in the next semester. The progress on that work is tracked throughout the semester, delays andinterrupts are accounted, and there's an escalation path up through the ranks of management and leadership ifpriorities cannot be agreed upon by individual teams.</p><p>The DOE Labs operate much more loosely in my experience. There, people tendto work on whatever pet projects they want until they lose interest. If a project is funded by a research grant, there are loosedeliverables and timelines (write X papers per year), but at the end of the day, nothing really bad happens if thework progresses slowly or its quality is poor. There's no penalty if a research grant results in a piece of softwarethat nobody uses or a paper that nobody reads. The value of the work is largely intellectual, and as a result, it's perfectlypossible to have a long career at a DOE lab, churning out papers and software, that lacks anylasting impact.</p><p>Tying money to the value of work can make accountability much more black and white. If you pay a team of engineers amillion dollars a year to develop a new service that only increases revenue by a million dollars a year, thatservice is going to be scrutinized every time prioritization happens. Is there a way to increase its revenue throughbetter features or better positioning? It'll be a product manager's job to go figure that out. If the answer comesback as \"no,\" then that service might be put on a shelf and its engineering team reassigned to work on somethingthat has a greater impact. Those engineers don't get to decide that they keep wanting to work on the service thathas limited demonstrable value.</p><p>At the same time, managers are accountable for the wellbeing of their team and the teams underneath them. Allemployees fill out regular, semi-anonymized surveys on different aspects of job satisfaction, and the results ofthese surveys roll up all the way to the top of the company. If employees are disgruntled, their managers know it,and those managers' managers know it, and everyone up the chain is accountable for improving those scores. Sometimes that resultsin increased hiring so engineers don't feel overworked. Other times it means reorganizing people and teams to alignthem with the work they are good at performing. And if nothing works and a team's morale keeps declining, maybe it's because ofthe manager--and the manager gets replaced.</p><h3 id=\"pace-and-decision-making\">Pace and decision making</h3><p>Because managers and leaders are accountable, I've also found them to be much more empowered to just do what theyfeel is the right thing to do. Whereas no big decision in the DOE Labs can be made without reviews, panels,strategic offsites, more reviews, and presentations to headquarters--all of which could add months oryears to a project--the direction can move on a dime because all it takes is one executive to sign off and acceptfull responsibility for the consequences of their decision. Getting the approval to staff up and pursue a good ideaoften requires only winning over one or two key people, not an army of feds in Germantown or an anonymous reviewpanel who isn't conversant in what you're proposing.</p><p>And again, sometimes money makes decisions much easier to make. For example, a few people at ISC'24 asked me why wedidn't re-do the <a href=\"https://www.top500.org/system/180236/\">Top500 run for Eagle</a> to beat Aurora since theSC'23 scoring was so close. The decision process can be as simple as this:</p><p></p><ul><li>According to the <a href=\"https://top500.org/lists/top500/2024/06/download/TOP500_202406.xlsx\">Top500 list's rawdata</a>, Eagle achieved 561,200 TFlop/s using an Nmax of 11,796,480.</li><li>Knowing that HPL's walltime is (flop count / Rmax) and HPL's flop count is (2/3 * Nmax^3), you can calculatethat the HPL walltime for this run was 1,950 seconds or 0.512 hours.</li><li>The <a href=\"https://azure.microsoft.com/en-us/pricing/calculator/\">public list price</a> for an Eaglenode (ND96isr H100 v5) is something like $60 an hour.</li><li>The HPL run used 1,800 such nodes.</li></ul><p>Give the above, during the half hour it would take to run HPL, those same nodes could berunning a production workload which would have resulted in $58,000 in revenue. That is, the <i>opportunity cost</i>of re-running HPL is at least $58,000 in lost revenue. In reality, it would take time to boot up and configure thecluster of virtual machines and do a few scale-up runs which would tie up the nodes for a couple hours, makingthis opportunity cost closer to a couple hundred thousand dollars.</p><p>Is getting a marginally higher Top500 score worth a couple hundred thousand dollars if yourmachine is already listed and had its day in the sun? I don't need an executive to answer that question. But in thepublic HPC space, who's to say what the opportunity cost is? If HPL wasn't running twice a year on Frontier, are thedozen or so lattice QCD jobs that would be running instead worth a couple hundred thousand dollars?</p><p></p><h3 id=\"relevance\">Relevance</h3><p>I might be more vain than I thought when I worked for the government, because I really enjoy being able to talk about the work that I dowith the general public now. When people ask, \"What work do you do?\" and I respond with, \"Have you ever heard of Copilot orChatGPT?\" there is almost always a conversation that follows. People may not really understand how artificial intelligence andlarge language models work, but they've played with those technologies and have opinions and questions. Sometimes the conversation is about big-picture stuff like \"will AI take over the world?\" At other times it's specific like \"what do you think aboutAI's effect on global climate change?\" Because I am steeped in all aspects of AI in my day-to-day work, I canusually speak intelligently about any dimension of the AI industry when my neighbors ask.</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><figure style=\"display: inline-block; margin-left: 1em; margin-right: 1em;\"><figcaption style=\"font-size: 14px; margin-top: 5px;\">Every blog post these days needs at least one AI-generated picture, so here is a picture generated by DALL-E that \"captures the essence of explaining AI concepts to neighbors in a friendly, approachable setting.\" But more poignantly, my team directly supports the supercomputers that trained the model that generates these pictures.</figcaption></figure></div><p>This was a much bigger challenge when I worked in the public sector. When I told people that I worked at Lawrence BerkeleyNational Lab, nobody knew what I was talking about half of the time. The other half of the time, people would think I worked onnuclear weapons because Lawrence Livermore National Lab has a confusingly similar name and geography. And if theconversation ever got as far as what people did on the supercomputers I supported, it would rapidly tail off onceall parties (including me) realized that cosmological hydrodynamics and quantum Monte Carlo don't really make for great conversation since they don't touch people's everyday lives.</p><p>This isn't to say that the work done at the Labs isn't important. But the general public doesn't understand it, andto a large degree, doesn't really care about it. I realize that being able to impress your neighbors with what youdo isn't at the top of the list of most people's job requirements, but I get a lot of satisfaction out of it.</p><h3 id=\"technically-security\">Technically: security</h3><p>HPC doesn't really worry about cybersecurity. Every HPC center has a security group and does scans and threatmodeling, but at the end of the day, the security practices on all the largest supercomputers in the public sectorare roughly the same as they were twenty years ago. Users ssh into a login node, and once you're inside, you haveaccess to everything. You can see everyone else who's logged in, you can see everyone who chmodded their homedirectory to be +777, and the only thing separating you from everyone else is the Linux kernel. Passwordless ssh iseverywhere, and often times, passwordless ssh for the root user is everywhere.</p><p>This does not fly with paying commercial HPC and AI customers in the cloud who use supercomputing to develop betterproducts faster than their competitors. For example, both <a href=\"https://www.synopsys.com/blogs/chip-design/eda-in-the-cloud-snug-2023.html\">Arm and AMD have publiclystated that they perform a lot of their silicon design simulations using HPC in the cloud</a>. What would happenif both AMD and Arm used the same cluster and one accidentally made their project directory world-readable? Shoulddomain scientists' understanding of how POSIX file permissions work really be the last line of defense against anext-generation CPU or GPU's specs being leaked to the competition?</p><p>I had to quickly learn about modern security practices when I started doing HPC in the commercial cloud out ofnecessity. I'm still nowhere close to being a security expert, but two years has been long enough for me to nowcringe when I talk to my colleagues in the traditional HPC community about how they protect against threats. It'snot really their fault that most of the HPC community hasn't adopted modern practices, because the tools andpractices required to do it right aren't easy to set up, automate, and maintain from scratch.</p><p>For example, basic LDAP is a short path to allowing users to log into a cluster's nodes, but if those users also needto authenticate themselves to REST services that support an HPC workflow across multiple clusters, you have to start building a Rube Goldberg machine of software on top of LDAP. Similarly, sticking every user on their own overlay network is great to limit the blast radius of acompromised account. However, automating the configuration of VXLAN tunnel endpoints as nodes get allocated and deallocated tojobs requires a lot of fancy orchestration that is either very complicated to build and maintain yourself or veryexpensive to buy and maintain. As a result, HPC just accepts the risk. Cloud hasfigured all this out though, and the price of providing this security infrastructure is included in the cost ofcloud-based supercomputers.</p><h3 id=\"pay-good\">But the pay is good, right?</h3><p>Like I said before I left the public sector, <a href=\"http://blog.glennklockwood.com/2022/05/life-and-leaving-nersc.html\">my base salary iscomparable to what I got at the lab</a>. It's actually gotten less competitive because <a href=\"https://www.theregister.com/2023/05/11/microsoft_pay_freeze/\">all salaries were frozen</a> when I was first eligible for a raise. So, after considering the effects of inflation, my paycheck is a little lower than what it was in the government two years ago.</p><p>What's different is the bonus structure which simply does not exist in the government or university world. For thosewho aren't familiar with how bonuses work in the tech industry, I'll share how it works for me:</p><p></p><ul><li>In the first year, I was awarded two signing bonuses: one in cash, one in stock. Half of the cash bonus was paidout up-front, and the other half was paid out after I had been there a year. The stock grant cannot be touchedduring the first year because it had a one-year \"cliff.\"</li><li>On my one-year anniversary, I got the second half of my cash signing bonus, and my signing stock grant began\"vesting.\"</li><li>After a year, I was also eligible for an annual performance-based raise, cash bonus, and stock bonus.</li><ul><li>Because of the economy, my annual raise was zero.</li><li>The cash bonus was paid out in a lump sum, similar to my cash signing bonus.</li><li>The stock bonus was awarded all at once but follows a multi-year \"vesting schedule\" which means I am onlyactually given fractions of the total award over time. However, these bonuses don't have a \"cliff\" and beginvesting immediately.</li></ul><li>Every year thereafter, I am eligible for an annual raise, cash bonus, and another stock bonus.</li></ul><p>The way stock bonuses work was the least intuitive part to me, but since it's such a significant part of total compensation, it's worth spellingout for anyone who's considering an offer that includes this:</p><p></p><ul><li>Stock bonuses are defined in terms of dollar values. For example, let's say I got a signing stock bonus of $1000with a one-year cliff that vests quarterly (every three months) over five years.</li><li>On the day that stock bonus is awarded, my employer converts that $1000 value into company stock based on themarket value that day. If stocks are $50 per share, I am awarded 20 shares. My employer hangs on to those shareson my behalf, so I can't actually do anything with them yet.</li><li>Since I have a five-year vesting schedule and the award vests quarterly, my shares will vest twenty times (fourquarters, five years). Coincidentally, since I have 20 shares, I will get one share per quarter.</li><li>However, because I have a one-year cliff, I get all four quarters of my first year at my one-year anniversary.So, four shares should appear in my brokerage account on my one-year anniversary. Once a share is in mybrokerage account, I can do whatever I want with it (like sell it immediately!)</li><li>Every quarter thereafter, one more share vests and appears in my brokerage account.</li></ul><p>Assuming I get a stock bonus as part of my overall annual bonus, this means that stockawards pile up and vest every year. This is tricky for two reasons:</p><p></p><ol><li>Although my initial stock award was $1,000 in the above example, that amount was converted to stock the day itwas awarded. <i>Assuming I am doing a good job and increasing the value of my employer's stock</i>, the value ofthose shares will increase while they're vesting. This means by the time the first four shares of my awardvested at my one-year anniversary, they were worth more than the $50 per share they represented when they wereawarded. More broadly, the value of a stock bonus tends to increase over time, making the true cash value of a$1000 stock bonus worth a lot more than $1000 by the time it completely vests.</li><li>Every year's stock award comes with its own multi-year vesting period, which means at any given time, I havemultiple years' bonuses all vesting at once. This also means that at any given time, I have a bunch of unvestedstock that's worth a lot of money that I can't yet spend. If I quit my job though, all these unvested sharesvanish into thin air.</li></ol><p>These two factors make up the golden handcuffs that people often talk about in industry.The longer I stick around, the more unvested stock I have hanging over my head, and it usually becomes increasinglyvaluable (yet inaccessible!) over time. The reality is that if you've put in a few years in Big Tech, you might haveyears' worth of base salary tied up in unvested stock that all goes away if you quit.</p><p>The end result is that although base salary is competitive with what you can make in a government HPC facility, there's a significant cash bonus that falls out of the sky once a year, andan appreciable amount of stock appears in your brokerage account every couple of months which you can turn aroundand sell for more cash. Depending on seniority and performance, these bonuses can add up to a significant fractionof base salary.</p><p>Finally, the above is consistent with what I've seen firsthand at two companies in Big Tech but may be different based on the role and the company. For example, field-facing roles in sales and support may be completely different beasts, and private companies and startups load things differently due to the value of equity.</p><h3 id=\"work-life-balance\">How's work-life balance?</h3><p>It hasn't been different than working in the government. Just like at a lab or university, some peoplework around the clock while others stick pretty close to the standard workday. There may be a higher concentrationof Type A personalities who put in a lot of time in Big Tech, and this may pressure others to keep up and also putin long hours, but there's rarely been an occasion where a manager expects staff to routinely work nights andweekends. Doing so would probably result in negative employee satisfaction scores which would roll up and eventuallyhave to be addressed.</p><p>Of course, there are cases where working odd hours is required to get the job done. BecauseI work for a global organization, I've had to get up early to meet with teams or customers in Europe. I've also hadto stay up late to meet with teams or customers in Asia. And in some particularly annoying days, I've had to do bothand wind up working from 5am to 8pm. But I never felt that I had no choice in the matter; I pulled these hoursbecause it was the right thing to do at the time. And I don't see this as being too different from the days when I'dwork sixteen-hour days, seven days a week, for the entire month of March to put together a paper for SC. Or dayswhen I'm at SC and am preparing talks, meeting with partners, and otherwise hustling from 8am to 1am for five daysstraight.</p><p>One big difference is the fact that my employer offers discretionary time off (\"unlimited vacation\"). This is a divisive topic in industry, but I see it as a positive for work-life balancebecause it underscores an emphasis on <i>outcomes</i> rather than <i>output</i>. I can take an afternoonoff or enjoy a long weekend with little fanfare, because <i>productivity</i> is infinitely more valuable that <i>presence</i>. Aslong as I do what needs to get done, I don't have to worry about timing vacations to ensure I am banking enough timeoff in between.</p><h2 id=\"what-i-miss\">Do you miss anything about working at the lab?</h2><div>Absolutely. There are a bunch of appealing things about working in a DOE lab (or an NSF center)that I've had to give up since coming to industry.</div><h3 id=\"freedom-to-have-an-off-day\">Freedom to have an off day</h3><p>Right before I finished graduate school, I hada conversation with <a href=\"https://engineering.lehigh.edu/faculty/edmund-webb-iii\">Professor Edmund Webb</a> soonafter he became a professor after a decade-long career at Sandia National Labs about life at the Labs. He said that,after becoming a professor, he lost the ability to just close the door to his office and focus onsomething he needed to get done for a day. I didn't really grasp what this meant at the time, but I totally get it now. TheDOE might be one of the few places where you can take a day--maybe even a week--and just close your door toeverything else that's going on around you to focus on what you want to do. In the case of professorship, there's always students requiring attention; in industry, it's customers and partners.</p><p>I think this difference results from two factors: very few things in publicHPC are very urgent, and the Labs are stocked full of independent, free-thinking Ph.D. types. There's rarely apenalty if something is late by a day (or two years! Remember when <a href=\"https://insidehpc.com/2020/10/doe-under-secretary-for-science-dabbars-exascale-update-frontier-to-be-first-aurora-to-be-monitored/\">Aurorawas called \"A21?\"</a>), but there can be huge payoff in prestige if one of your wacky side projects turns out to besomething useful (this is how <a href=\"https://docs.nersc.gov/development/containers/shifter/\">Shifter</a> came to be). By comparison, working at a giant corporation often means there are a bunch of interdependencieson others, and the odds of any one of your 200,000 coworkers sending you a Teams message asking for help is just a lot higher than it is at a 70-person supercomputer center. The culture is much more team-oriented, and being a one-person army isn't incentivized as much.</p><h3 id=\"travel\">Travel</h3><p>Part of my job within the DOE complex was to go around the country (and the world) and be smart, and secondarily,show that my lab hired smart people and did smart things. If headquarters wanted to make sure that the supercomputerthey were about to spend $500M on was technically sound, I'd sometimes get invited to go sit in on a review and tryto poke holes in the design. If a European HPC project wanted to ensure they were including a global perspective onsome dimension of future HPC strategy, I'd sometimes get invited to give a talk about how I view the world of data.And if these reviews and workshops happened to be in awesome places around the world--oh well!</p><p>I feel a lot more self-conscious about requesting approval to attend these sorts of boondoggles as an engineer nowbecause the first question I have to answer is, \"Is this trip business critical?\" If there's a direct line of sightbetween me giving a talk at a workshop and a specific business strategy, I can say \"yes\" with a straight face. But it'shard to accept an invitation to fly off to Switzerland to give a 30-minute talk when I know that my attendance isn'tgoing to move any needles.</p><h3 id=\"openness\">Openness</h3><p>Just like it's no longer my job to travel the world and just be smart, it's not my job to write about the work that I(or my team) does. I miss writing papers and giving technical talks, because the process of putting togethercoherent thoughts around a technical topic is one of the ways I really come to understand it. There's also a lot ofreally wild ideas that we're pursuing at scale that the scientific computingcommunity has never considered, but there are two factors that work against being open about these things:</p><p></p><ol><li>In terms of prioritization, my time is always better spent solving problems, or at least documenting them forinternal audiences who fully grasp the context around them, than writing about them in a way that the rest ofthe world can understand. It's hard to justify the time to write a retrospective or a study unless there's astrategic advantage behind it.</li><li>The customers I support typically do not want the world knowing what they're doing. There is an AI arms racehappening right now, and having the technical sophistication to utilize massive-scale supercomputers effectivelyis a competitive advantage. In the traditional HPC community, only national security is comparable to the levelof secrecy involved, and none of the intelligence agencies are openly contributing to the state of the art inHPC either.</li></ol><div>So instead of making conference papers and presentations, these days I make more internal papers and presentations.I'm trying to figure out ways to publish interesting technical anecdotes on my website (for example, I maintain <a href=\"https://www.glennklockwood.com/ai/ai-requirements.html\">a collection of LLM training requirements as I am exposed to them</a>), but it's a lot of extra work to disentangle the proprietary bits from my work notes to do this.</div><p></p><p>Related to openness is also freedom to speak my mind in public forums. I had the most latitude to blast myopinions out on to the Internet when I was still early in my career and nobody listened to me, but I've had to getprogressively less opinionated over the years. At this point, I abide by a written corporate social media policywhich, although very reasonable in what it requests (don't slander competitors, always be transparent about who employs you), it stops me from commenting on news as much as I used to since so many techcompanies qualify as competitors in some dimension.</p><h2 id=\"regret-decision\">Would you still have left knowing what you know now?</h2><p>Yes. I still stand by just about everything I wrote in my <a href=\"http://blog.glennklockwood.com/2022/05/life-and-leaving-nersc.html\">original blog post</a>; at the time, I just needed a change, and Ifound the change that I was looking for. Without immersing myself in the world of cloud, I would havenever learned about virtualization, physical infrastructure, or modern security to the degree that I have. And the fact that Istumbled into what has become one of the leading companies in AI at the dawn of generative AI was an extremely luckycoincidence.</p><p>However, this doesn't mean that I now turn my nose up at doing HPC in the public sector.There are many unique aspects to working at a DOE lab or NSF center that have no parallel in industry. I also believe that I amthe sum of the experiences that led me to where I work today, and I would never have gotten the opportunity to writethis retrospective if I didn't learn everything I did working in the DOE and NSF.</p><p>And perhaps above all else, there is something attractive about public service that I haven't beenable to shake in the last two years. I still dial in to <a href=\"https://science.osti.gov/ascr/ascac\">ASCACmeetings</a> to see what the world of public HPC and scientific computing is thinking and doing, and I still tryto contribute time and attention to working groups like <a href=\"https://www.nitrd.gov/coordination-areas/lsn/magic/\">NITRD's MAGIC</a>. I write lengthy blog posts in a <a href=\"http://blog.glennklockwood.com/2024/05/isc24-recap.html\">futile attempt to caution the leaders in public-sector HPC</a> againstrejecting AI workloads in commercial clouds as HPC. And every time I learn some slick way we deal with hard technological or sociological issues at work, I still file it away in the \"good ideas for when I goback\" folder in the back of my mind.</p><p>I don't have any near-term plans on going anywhere though. Like I said before, there arestill plenty of days when dialing into work is like going to the playground. Amazing things are happening in theworld of HPC infrastructure at scale now that the world is pouring money into AI, and the rate of scale andinnovation is no longer constrained to <a href=\"https://www.llnl.gov/article/48101/powering-llnl-prepares-exascale-massive-energy-water-upgrade\">40 MW</a>and <a href=\"https://www.olcf.ornl.gov/wp-content/uploads/OLCF-6-RFP-Cover-Letter-07-19-2024.pdf\">$500M</a> persupercomputer like it was when public-sector HPC was setting the bar for leadership. There is a whole new exciting world of challenges and possibilities when you start thinking about building supercomputers that consume <a href=\"https://www.datacenterdynamics.com/en/news/aws-acquires-talens-nuclear-data-center-campus-in-pennsylvania/\">hundreds of megawatts of power</a>.</p><p>Like I wrote two years ago, I don't think any government has the appetite to build data centers for scientific computing that are larger than today's 50 MW exascale facilities. This means that government HPC centers will never have a reason to explore the exciting world of 100+ MW supercomputers or work on the wacky problems that arise at that scale. Consequently, the biggest and most challenging problems in HPC--at least in terms of infrastructure and systems design at scale--are becoming unique to industry, not public HPC.</p><p>I got into HPC because I enjoy working on large, complex systems. Considering where I am at this stage of my life, what I want to accomplish in the rest of my career, and what gets me out of bed in the morning, I feel like I wound up in the right place for now. I have no regrets.</p>",
            "url": "https://hpc.social/personal-blog/2024/how-has-life-after-leaving-the-labs-been-going/",
            
            
            
            
            
            "date_published": "2024-08-04T20:21:00-06:00",
            "date_modified": "2024-08-04T20:21:00-06:00",
            
                "author": "Glenn K. Lockwood's Blog"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2024/advanced-lsf-resource-connector-configuration-on-ibm-cloud-part-iii/",
            "title": "Advanced LSF resource connector configuration on IBM Cloud - part III",
            "summary": null,
            "content_text": "OverviewThis is the third instalment in a series of blogs covering advanced configuration topics for LSF resource connector. The earlier parts in the series can be found here: part I, part II.As hinted in the closing of part II, this instalment will cover running Docker workloads on cloud instances which are dynamically managed by the LSF resource connector. The cloud environment in this example is IBM Cloud. To understand more about LSF resource connector, please read the earlier parts in the blog series.LSF provides a framework for the management and execution of containerized workloads. It supports the following container runtimes: Docker, NVIDIA Docker, Shifter, Singularity, Podman and Enroot. The LSF documentation provides configuration steps for the supported container runtimes. Once configured, this capability is effectively transparent from the end user perspective.Enable Docker supportFirst we need to enable support in LSF to run Docker containers. This is covered in detail in the LSF documentation and also something which I wrote about previously in the blog post Jack of all containers. The following steps will assume that the configuration steps have been completed.LSF uses a Boolean resource named docker to identify hosts where the Docker runtime is available. This Boolean resource needs to be set on the compute nodes which are dynamically started by LSF resource connector.In our example, an insecure Docker repository (using http) has been setup on the LSF manager host in the cluster with hostname lsf-mgmt-host. This will serve as the repository to host an OpenFOAM Docker container which has been prepared according to the procedures documented here. This blog will not go into detail on the creation of the insecure Docker registry. On the LSF management node, below is the output showing the available images. We see the OpenFoam image is available both locally and via http on port 5000.# docker image lsREPOSITORY                   TAG           IMAGE ID      CREATED        SIZElocalhost/openfoam/openfoam  v1912_update  bce4eb059f36  11 days ago    6.71 GBlocalhost:5000/openfoam      v1912_update  bce4eb059f36  11 days ago    6.71 GBdocker.io/library/registry   2             6a3edb1d5eb6  10 months ago  26 MBNote An insecure Docker registry was used in this example for simplicity and is not recommended in production.As was the case in part II of the blog series, the user_data.sh script will be used for multiple purposes here:Set docker Boolean variable on dynamic compute nodesInstall Docker CE runtime and relevant support packagesAdd user(s) to the docker group (/etc/group)Configuration to point to insecure Docker registry on LSF management hostlsf-mgmt-hostThe following updates were made to the user_data.sh script. See comments inline for details.$ diff -u4 ./user_data.sh ./user_data_sh.org--- ./user_data.sh\t2024-07-29 18:44:24.483146000 +0000+++ ./user_data_sh.org\t2024-07-11 14:34:47.688341000 +0000@@ -29,25 +29,8 @@  #!/bin/bash # shellcheck disable=all -# -# The following steps will add the Docker CE repo, install the latest Docker CE-# version along with supporting packages. It will create a Docker Linux group-# and add the lsfadmin user to that group. Furthermore, it will create-# the /etc/docker/daemon.json file pointing to the insecure Docker registry-# which has been configured on the LSF management host. Finally it will-# start Docker. Note that the hostname lsf-mgmt-host for the insecure-registries-# configuration of Docker needs to be updated accordingly. -# -yum-config-manager --add-repo https://download.docker.com/linux/rhel/docker-ce.repo -y -dnf install htop hwloc hwloc-libs libevent stress stress-ng python36 docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin -y  &gt;&gt; $logfile 2&gt;&amp;1-ln -s /usr/bin/python3 /usr/bin/python-groupadd docker &gt;&gt; $logfile 2&gt;&amp;1-usermod -aG docker lsfadmin  &gt;&gt; $logfile 2&gt;&amp;1 -echo -e \"{\\n \\\"insecure-registries\\\" : [ \\”lsf-mgmt-host:5000\\\" ]\\n }\" &gt;&gt; /etc/docker/daemon.json -systemctl start docker &gt;&gt; $logfile 2&gt;&amp;1  - if [ \"$compute_user_data_vars_ok\" != \"1\" ]; then   echo 2&gt;&amp;1 \"fatal: vars block is missing\"   exit 1 fi@@ -225,15 +208,8 @@ else   echo \"Can not get instance ID\" &gt;&gt; $logfile fi -# -# Add the docker Boolean variable to the LSF_LOCAL_RESOURCES variable in-# the lsf.conf file on the compute hosts. This will ensure that the host-# is tagged with the docker variable. -# -sed -i \"s/\\(LSF_LOCAL_RESOURCES=.*\\)\\\"/\\1 [resource docker]\\\"/\" $LSF_CONF_FILE &gt;&gt; $logfile 2&gt;&amp;1 - #Update LSF Tuning on dynamic hosts LSF_TUNABLES=\"etc/sysctl.conf\" echo 'vm.overcommit_memory=1' &gt;&gt; $LSF_TUNABLES echo 'net.core.rmem_max=26214400' &gt;&gt; $LSF_TUNABLESApplication profile configurationNext, we configure the LSF application profile for the OpenFOAM Docker container which has been loaded into the insecure Docker registry on the LSF management host. LSF application profiles can be used to define common job parameters for the same job type. This includes the container and container runtime definition. Learn more about LSF application profiles here.On the LSF management node, the following application profile is defined in $LSF_ENVDIR/lsbatch/&lt;clustername&gt;/configdir/lsb.applications. Note that the hostname lsf-mgmt-host must point to the hostname where the insecure Docker repository has been setup in your environment. Additionally the volume specification -v /mnt/vpcstorage/data is specific to this environment and can be adjusted or removed as needed.….….Begin ApplicationNAME = openfoamDESCRIPTION = Example OpenFOAM applicationCONTAINER = docker[image(lsf-mgmt-host:5000/openfoam:v1912_update) \\   options(--rm --net=host --ipc=host \\   --cap-add=SYS_PTRACE \\   -v /etc/passwd:/etc/passwd \\   -v /etc/group:/etc/group \\   -v /mnt/vpcstorage/data:/mnt/vpcstorage/data \\    ) starter(root)]   EXEC_DRIVER = context[user(lsfadmin)] \\   starter[/opt/ibm/lsf_worker/10.1/linux3.10-glibc2.17-x86_64/etc/docker-starter.py] \\   controller[/opt/ibm/lsf_worker/10.1/linux3.10-glibc2.17-x86_64/etc/docker-control.py] \\   monitor[/opt/ibm/lsf_worker/10.1/linux3.10-glibc2.17-x86_64/etc/docker-monitor.py]End Application….….In order to make the above change take effect, run the badmin reconfig command as the defined LSF administrator. The LSF bapp command can be used to check the newly defined configuration for LSF application profile openfoam.$ badmin reconfigChecking configuration files ...No errors found.Reconfiguration initiated$ bapp -l openfoamAPPLICATION NAME: openfoam -- Example OpenFOAM applicationSTATISTICS:   NJOBS     PEND      RUN    SSUSP    USUSP      RSV        0        0        0        0        0        0PARAMETERS:CONTAINER: docker[image(lsf-mgmt-host:5000/openfoam:v1912_update)    options(--rm --net=host --ipc=host    --cap-add=SYS_PTRACE    -v /etc/passwd:/etc/passwd    -v /etc/group:/etc/group    -v /mnt/vpcstorage/data:/mnt/vpcstorage/data    ) starter(root)]EXEC_DRIVER:     context[user(lsfadmin)]    starter[/opt/ibm/lsf_worker/10.1/linux3.10-glibc2.17-x86_64/etc/docker-starter.py]    controller[/opt/ibm/lsf_worker/10.1/linux3.10-glibc2.17-x86_64/etc/docker-control.py]    monitor[/opt/ibm/lsf_worker/10.1/linux3.10-glibc2.17-x86_64/etc/docker-monitor.py]Submitting workloadWith all of the configuration in place, it’s now time to submit an OpenFOAM workload. For this, LSF Application Center is used. The OpenFOAM application template is available on the Spectrum Computing github here. The OpenFOAM application template is configured to use the openfoam application profile. An example job is submitted and it runs to completion successfully. In the screenshot below, we see that the openfoam Docker container is executed.The LSF bjobs and bhist output from the job follows below:$ bjobs -l 2613Job &lt;2613&gt;, Job Name &lt;myOpenFoam_run_motorBike&gt;, User &lt;lsfadmin&gt;, Project &lt;defa                     ult&gt;, Application &lt;openfoam&gt;, Status &lt;RUN&gt;, Queue &lt;normal&gt;                     , Command &lt;/mnt/lsf/repository-path/lsfadmin/myOpenFoam_ru                     n_1722358195200AuWDY/motorBike/bsub.myOpenFoam_run&gt;, Share                      group charged &lt;/lsfadmin&gt;Tue Jul 30 16:49:55: Submitted from host &lt;gsamu-hpc-demo-mgmt-1-a844-001&gt;, CWD                      &lt;/mnt/lsf/repository-path/lsfadmin/myOpenFoam_run_17223581                     95200AuWDY/motorBike&gt;, Specified CWD &lt;/mnt/lsf/repository-                     path/lsfadmin/myOpenFoam_run_1722358195200AuWDY/motorBike&gt;                     , Output File &lt;/mnt/lsf/repository-path/lsfadmin/myOpenFoa                     m_run_1722358195200AuWDY/motorBike/output.lsfadmin.txt&gt;, E                     rror File &lt;/mnt/lsf/repository-path/lsfadmin/myOpenFoam_ru                     n_1722358195200AuWDY/motorBike/error.lsfadmin.txt&gt;, Notify                      when job begins/ends, 6 Task(s), Requested Resources &lt;spa                     n[hosts=1]&gt;;Tue Jul 30 16:55:23: Started 6 Task(s) on Host(s) &lt;gsamu-hpc-demo-10-241-0-137&gt;                     &lt;gsamu-hpc-demo-10-241-0-137&gt; &lt;gsamu-hpc-demo-10-241-0-137                     &gt; &lt;gsamu-hpc-demo-10-241-0-137&gt; &lt;gsamu-hpc-demo-10-241-0-1                     37&gt; &lt;gsamu-hpc-demo-10-241-0-137&gt;, Allocated 6 Slot(s) on                      Host(s) &lt;gsamu-hpc-demo-10-241-0-137&gt; &lt;gsamu-hpc-demo-10-2                     41-0-137&gt; &lt;gsamu-hpc-demo-10-241-0-137&gt; &lt;gsamu-hpc-demo-10                     -241-0-137&gt; &lt;gsamu-hpc-demo-10-241-0-137&gt; &lt;gsamu-hpc-demo-                     10-241-0-137&gt;, Execution Home &lt;/home/lsfadmin&gt;, Execution                      CWD &lt;/mnt/lsf/repository-path/lsfadmin/myOpenFoam_run_1722                     358195200AuWDY/motorBike&gt;;Tue Jul 30 17:03:33: Resource usage collected.                     The CPU time used is 1411 seconds.                     MEM: 928 Mbytes;  SWAP: 0 Mbytes;  NTHREAD: 41                     PGID: 18426;  PIDs: 18426 18427 18428 20088                      PGID: 20374;  PIDs: 20374 20388 20800 21385                      PGID: 21389;  PIDs: 21389                      PGID: 21390;  PIDs: 21390                      PGID: 21391;  PIDs: 21391                      PGID: 21392;  PIDs: 21392                      PGID: 21393;  PIDs: 21393                      PGID: 21394;  PIDs: 21394  MEMORY USAGE: MAX MEM: 982 Mbytes;  AVG MEM: 422 Mbytes; MEM Efficiency: 0.00% CPU USAGE: CPU PEAK: 5.89 ;  CPU PEAK DURATION: 63 second(s) CPU AVERAGE EFFICIENCY: 42.81% ;  CPU PEAK EFFICIENCY: 98.15% SCHEDULING PARAMETERS:           r15s   r1m  r15m   ut      pg    io   ls    it    tmp    swp    mem loadSched   -     -     -     -       -     -    -     -     -      -      -   loadStop    -     -     -     -       -     -    -     -     -      -      -   RESOURCE REQUIREMENT DETAILS: Combined: select[(docker) &amp;&amp; (type == any)] order[r15s:pg] span[hosts=1] Effective: select[(docker) &amp;&amp; (type == any)] order[r15s:pg] span[hosts=1] $ bhist -l 2613Job &lt;2613&gt;, Job Name &lt;myOpenFoam_run_motorBike&gt;, User &lt;lsfadmin&gt;, Project &lt;defa                     ult&gt;, Application &lt;openfoam&gt;, Command &lt;/mnt/lsf/repository                     -path/lsfadmin/myOpenFoam_run_1722358195200AuWDY/motorBike                     /bsub.myOpenFoam_run&gt;Tue Jul 30 16:49:55: Submitted from host &lt;gsamu-hpc-demo-mgmt-1-a844-001&gt;, to Q                     ueue &lt;normal&gt;, CWD &lt;/mnt/lsf/repository-path/lsfadmin/myOp                     enFoam_run_1722358195200AuWDY/motorBike&gt;, Specified CWD &lt;/                     mnt/lsf/repository-path/lsfadmin/myOpenFoam_run_1722358195                     200AuWDY/motorBike&gt;, Output File &lt;/mnt/lsf/repository-path                     /lsfadmin/myOpenFoam_run_1722358195200AuWDY/motorBike/outp                     ut.lsfadmin.txt&gt;, Error File &lt;/mnt/lsf/repository-path/lsf                     admin/myOpenFoam_run_1722358195200AuWDY/motorBike/error.ls                     fadmin.txt&gt;, Notify when job begins/ends, 6 Task(s), Reque                     sted Resources &lt;span[hosts=1]&gt;;Tue Jul 30 16:55:23: Dispatched 6 Task(s) on Host(s) &lt;gsamu-hpc-demo-10-241-0-1                     37&gt; &lt;gsamu-hpc-demo-10-241-0-137&gt; &lt;gsamu-hpc-demo-10-241-0                     -137&gt; &lt;gsamu-hpc-demo-10-241-0-137&gt; &lt;gsamu-hpc-demo-10-241                     -0-137&gt; &lt;gsamu-hpc-demo-10-241-0-137&gt;, Allocated 6 Slot(s)                      on Host(s) &lt;gsamu-hpc-demo-10-241-0-137&gt; &lt;gsamu-hpc-demo-                     10-241-0-137&gt; &lt;gsamu-hpc-demo-10-241-0-137&gt; &lt;gsamu-hpc-dem                     o-10-241-0-137&gt; &lt;gsamu-hpc-demo-10-241-0-137&gt; &lt;gsamu-hpc-d                     emo-10-241-0-137&gt;, Effective RES_REQ &lt;select[(docker) &amp;&amp; (                     type == any)] order[r15s:pg] span[hosts=1] &gt;;Tue Jul 30 16:55:23: Starting (Pid 18426);Tue Jul 30 16:55:24: Running with execution home &lt;/home/lsfadmin&gt;, Execution CW                     D &lt;/mnt/lsf/repository-path/lsfadmin/myOpenFoam_run_172235                     8195200AuWDY/motorBike&gt;, Execution Pid &lt;18426&gt;;Tue Jul 30 17:04:01: Done successfully. The CPU time used is 1535.1 seconds;Tue Jul 30 17:04:02: Post job process done successfully;MEMORY USAGE:MAX MEM: 982 Mbytes;  AVG MEM: 431 Mbytes; MEM Efficiency: 0.00%CPU USAGE:CPU PEAK: 5.92 ;  CPU PEAK DURATION: 63 second(s)CPU AVERAGE EFFICIENCY: 50.67% ;  CPU PEAK EFFICIENCY: 98.68%Summary of time in seconds spent in various states by  Tue Jul 30 17:04:02  PEND     PSUSP    RUN      USUSP    SSUSP    UNKWN    TOTAL  328      0        518      0        0        0        846         ConclusionThe user_data.sh script of LSF resource connector allows a high degree of customization for cloud compute resources that dynamically join the LSF cluster. We’ve demonstrated how it can be used to tag cloud compute resources with a specific LSF Boolean resource in addition to the ability to install specific packages and do configuration customization. This is a simplified example, but illustrates this point.",
            "content_html": "<p><strong>Overview</strong></p><p>This is the third instalment in a series of blogs covering advanced configuration topics for LSF resource connector. The earlier parts in the series can be found here: <a href=\"https://community.ibm.com/community/user/cloud/blogs/gbor-samu/2023/11/09/advanced-resource-connector-configuration-on-ibm-c\">part I</a>, <a href=\"https://community.ibm.com/community/user/cloud/blogs/gbor-samu/2024/03/20/advanced-lsf-resource-connector-configuration-on-i\">part II</a>.</p><p>As hinted in the closing of part II, this instalment will cover running Docker workloads on cloud instances which are dynamically managed by the LSF resource connector. The cloud environment in this example is <a href=\"https://cloud.ibm.com/catalog/content/terraform-1623200063-71606cab-c6e1-4f95-a47a-2ce541dcbed8-global\">IBM Cloud</a>. To understand more about LSF resource connector, please read the earlier parts in the blog series.</p><p><a href=\"https://www.ibm.com/products/hpc-workload-management\">LSF</a> provides a framework for the management and execution of containerized workloads. It supports the following container runtimes: Docker, NVIDIA Docker, Shifter, Singularity, Podman and Enroot. The LSF <a href=\"https://www.ibm.com/docs/en/spectrum-lsf/10.1.0?topic=lsf-configuring-containers\">documentation</a> provides configuration steps for the supported container runtimes. Once configured, this capability is effectively transparent from the end user perspective.</p><p><strong>Enable Docker support</strong></p><p>First we need to enable support in LSF to run Docker containers. This is covered in detail in the LSF <a href=\"https://www.ibm.com/docs/en/spectrum-lsf/10.1.0?topic=containers-lsf-docker\">documentation</a> and also something which I wrote about previously in the blog post <a href=\"https://medium.com/ibm-data-ai/jack-of-all-containers-e0d7fd0633b3\">Jack of all containers</a>. The following steps will assume that the configuration steps have been completed.</p><p>LSF uses a Boolean resource named <em>docker</em> to identify hosts where the Docker runtime is available. This Boolean resource needs to be set on the compute nodes which are dynamically started by LSF resource connector.</p><p>In our example, an insecure Docker repository (using http) has been setup on the LSF manager host in the cluster with hostname <em>lsf-mgmt-host</em>. This will serve as the repository to host an OpenFOAM Docker container which has been prepared according to the procedures documented <a href=\"https://community.ibm.com/community/user/cloud/blogs/john-welch/2020/02/12/building-an-openfoam-ready-container-for-lsf\">here</a>. This blog will not go into detail on the creation of the insecure Docker registry. On the LSF management node, below is the output showing the available images. We see the OpenFoam image is available both locally and via http on port 5000.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\"># docker image lsREPOSITORY                   TAG           IMAGE ID      CREATED        SIZElocalhost/openfoam/openfoam  v1912_update  bce4eb059f36  11 days ago    6.71 GBlocalhost:5000/openfoam      v1912_update  bce4eb059f36  11 days ago    6.71 GBdocker.io/library/registry   2             6a3edb1d5eb6  10 months ago  26 MB</code></pre></div><p><strong>Note</strong> An insecure Docker registry was used in this example for simplicity and is not recommended in production.</p><p>As was the case in part II of the blog series, the <em>user_data.sh</em> script will be used for multiple purposes here:</p><ul><li>Set <em>docker</em> Boolean variable on dynamic compute nodes</li><li>Install Docker CE runtime and relevant support packages</li><li>Add user(s) to the docker group (<em>/etc/group</em>)</li><li>Configuration to point to insecure Docker registry on LSF management host<em>lsf-mgmt-host</em></li></ul><p>The following updates were made to the <em>user_data.sh</em> script. See comments inline for details.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ diff -u4 ./user_data.sh ./user_data_sh.org--- ./user_data.sh\t2024-07-29 18:44:24.483146000 +0000+++ ./user_data_sh.org\t2024-07-11 14:34:47.688341000 +0000@@ -29,25 +29,8 @@  #!/bin/bash # shellcheck disable=all -# -# The following steps will add the Docker CE repo, install the latest Docker CE-# version along with supporting packages. It will create a Docker Linux group-# and add the lsfadmin user to that group. Furthermore, it will create-# the /etc/docker/daemon.json file pointing to the insecure Docker registry-# which has been configured on the LSF management host. Finally it will-# start Docker. Note that the hostname lsf-mgmt-host for the insecure-registries-# configuration of Docker needs to be updated accordingly. -# -yum-config-manager --add-repo https://download.docker.com/linux/rhel/docker-ce.repo -y -dnf install htop hwloc hwloc-libs libevent stress stress-ng python36 docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin -y  &gt;&gt; $logfile 2&gt;&amp;1-ln -s /usr/bin/python3 /usr/bin/python-groupadd docker &gt;&gt; $logfile 2&gt;&amp;1-usermod -aG docker lsfadmin  &gt;&gt; $logfile 2&gt;&amp;1 -echo -e \"{\\n \\\"insecure-registries\\\" : [ \\”lsf-mgmt-host:5000\\\" ]\\n }\" &gt;&gt; /etc/docker/daemon.json -systemctl start docker &gt;&gt; $logfile 2&gt;&amp;1  - if [ \"$compute_user_data_vars_ok\" != \"1\" ]; then   echo 2&gt;&amp;1 \"fatal: vars block is missing\"   exit 1 fi@@ -225,15 +208,8 @@ else   echo \"Can not get instance ID\" &gt;&gt; $logfile fi -# -# Add the docker Boolean variable to the LSF_LOCAL_RESOURCES variable in-# the lsf.conf file on the compute hosts. This will ensure that the host-# is tagged with the docker variable. -# -sed -i \"s/\\(LSF_LOCAL_RESOURCES=.*\\)\\\"/\\1 [resource docker]\\\"/\" $LSF_CONF_FILE &gt;&gt; $logfile 2&gt;&amp;1 - #Update LSF Tuning on dynamic hosts LSF_TUNABLES=\"etc/sysctl.conf\" echo 'vm.overcommit_memory=1' &gt;&gt; $LSF_TUNABLES echo 'net.core.rmem_max=26214400' &gt;&gt; $LSF_TUNABLES</code></pre></div><p><strong>Application profile configuration</strong></p><p>Next, we configure the LSF application profile for the OpenFOAM Docker container which has been loaded into the insecure Docker registry on the LSF management host. LSF application profiles can be used to define common job parameters for the same job type. This includes the container and container runtime definition. Learn more about LSF application profiles <a href=\"https://www.ibm.com/docs/en/spectrum-lsf/10.1.0?topic=lsf-application-profiles\">here</a>.</p><p>On the LSF management node, the following application profile is defined in <em>$LSF_ENVDIR/lsbatch/&lt;clustername&gt;/configdir/lsb.applications</em>. Note that the hostname <em>lsf-mgmt-host</em> must point to the hostname where the insecure Docker repository has been setup in your environment. Additionally the volume specification <em>-v /mnt/vpcstorage/data</em> is specific to this environment and can be adjusted or removed as needed.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">….….Begin ApplicationNAME = openfoamDESCRIPTION = Example OpenFOAM applicationCONTAINER = docker[image(lsf-mgmt-host:5000/openfoam:v1912_update) \\   options(--rm --net=host --ipc=host \\   --cap-add=SYS_PTRACE \\   -v /etc/passwd:/etc/passwd \\   -v /etc/group:/etc/group \\   -v /mnt/vpcstorage/data:/mnt/vpcstorage/data \\    ) starter(root)]   EXEC_DRIVER = context[user(lsfadmin)] \\   starter[/opt/ibm/lsf_worker/10.1/linux3.10-glibc2.17-x86_64/etc/docker-starter.py] \\   controller[/opt/ibm/lsf_worker/10.1/linux3.10-glibc2.17-x86_64/etc/docker-control.py] \\   monitor[/opt/ibm/lsf_worker/10.1/linux3.10-glibc2.17-x86_64/etc/docker-monitor.py]End Application….….</code></pre></div><p>In order to make the above change take effect, run the <em>badmin reconfig</em> command as the defined LSF administrator. The LSF <em>bapp</em> command can be used to check the newly defined configuration for LSF application profile <em>openfoam</em>.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ badmin reconfigChecking configuration files ...No errors found.Reconfiguration initiated$ bapp -l openfoamAPPLICATION NAME: openfoam -- Example OpenFOAM applicationSTATISTICS:   NJOBS     PEND      RUN    SSUSP    USUSP      RSV        0        0        0        0        0        0PARAMETERS:CONTAINER: docker[image(lsf-mgmt-host:5000/openfoam:v1912_update)    options(--rm --net=host --ipc=host    --cap-add=SYS_PTRACE    -v /etc/passwd:/etc/passwd    -v /etc/group:/etc/group    -v /mnt/vpcstorage/data:/mnt/vpcstorage/data    ) starter(root)]EXEC_DRIVER:     context[user(lsfadmin)]    starter[/opt/ibm/lsf_worker/10.1/linux3.10-glibc2.17-x86_64/etc/docker-starter.py]    controller[/opt/ibm/lsf_worker/10.1/linux3.10-glibc2.17-x86_64/etc/docker-control.py]    monitor[/opt/ibm/lsf_worker/10.1/linux3.10-glibc2.17-x86_64/etc/docker-monitor.py]</code></pre></div><p><strong>Submitting workload</strong></p><p>With all of the configuration in place, it’s now time to submit an OpenFOAM workload. For this, LSF Application Center is used. The OpenFOAM application template is available on the Spectrum Computing github <a href=\"https://github.com/IBMSpectrumComputing/lsf-integrations\">here</a>. The OpenFOAM application template is configured to use the <em>openfoam</em> application profile. An example job is submitted and it runs to completion successfully. In the screenshot below, we see that the openfoam Docker container is executed.</p><figure><img src=\"https://www.gaborsamu.com/images/openfoam_job.jpg\" /></figure><p>The LSF <em>bjobs</em> and <em>bhist</em> output from the job follows below:</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ bjobs -l 2613Job &lt;2613&gt;, Job Name &lt;myOpenFoam_run_motorBike&gt;, User &lt;lsfadmin&gt;, Project &lt;defa                     ult&gt;, Application &lt;openfoam&gt;, Status &lt;RUN&gt;, Queue &lt;normal&gt;                     , Command &lt;/mnt/lsf/repository-path/lsfadmin/myOpenFoam_ru                     n_1722358195200AuWDY/motorBike/bsub.myOpenFoam_run&gt;, Share                      group charged &lt;/lsfadmin&gt;Tue Jul 30 16:49:55: Submitted from host &lt;gsamu-hpc-demo-mgmt-1-a844-001&gt;, CWD                      &lt;/mnt/lsf/repository-path/lsfadmin/myOpenFoam_run_17223581                     95200AuWDY/motorBike&gt;, Specified CWD &lt;/mnt/lsf/repository-                     path/lsfadmin/myOpenFoam_run_1722358195200AuWDY/motorBike&gt;                     , Output File &lt;/mnt/lsf/repository-path/lsfadmin/myOpenFoa                     m_run_1722358195200AuWDY/motorBike/output.lsfadmin.txt&gt;, E                     rror File &lt;/mnt/lsf/repository-path/lsfadmin/myOpenFoam_ru                     n_1722358195200AuWDY/motorBike/error.lsfadmin.txt&gt;, Notify                      when job begins/ends, 6 Task(s), Requested Resources &lt;spa                     n[hosts=1]&gt;;Tue Jul 30 16:55:23: Started 6 Task(s) on Host(s) &lt;gsamu-hpc-demo-10-241-0-137&gt;                     &lt;gsamu-hpc-demo-10-241-0-137&gt; &lt;gsamu-hpc-demo-10-241-0-137                     &gt; &lt;gsamu-hpc-demo-10-241-0-137&gt; &lt;gsamu-hpc-demo-10-241-0-1                     37&gt; &lt;gsamu-hpc-demo-10-241-0-137&gt;, Allocated 6 Slot(s) on                      Host(s) &lt;gsamu-hpc-demo-10-241-0-137&gt; &lt;gsamu-hpc-demo-10-2                     41-0-137&gt; &lt;gsamu-hpc-demo-10-241-0-137&gt; &lt;gsamu-hpc-demo-10                     -241-0-137&gt; &lt;gsamu-hpc-demo-10-241-0-137&gt; &lt;gsamu-hpc-demo-                     10-241-0-137&gt;, Execution Home &lt;/home/lsfadmin&gt;, Execution                      CWD &lt;/mnt/lsf/repository-path/lsfadmin/myOpenFoam_run_1722                     358195200AuWDY/motorBike&gt;;Tue Jul 30 17:03:33: Resource usage collected.                     The CPU time used is 1411 seconds.                     MEM: 928 Mbytes;  SWAP: 0 Mbytes;  NTHREAD: 41                     PGID: 18426;  PIDs: 18426 18427 18428 20088                      PGID: 20374;  PIDs: 20374 20388 20800 21385                      PGID: 21389;  PIDs: 21389                      PGID: 21390;  PIDs: 21390                      PGID: 21391;  PIDs: 21391                      PGID: 21392;  PIDs: 21392                      PGID: 21393;  PIDs: 21393                      PGID: 21394;  PIDs: 21394  MEMORY USAGE: MAX MEM: 982 Mbytes;  AVG MEM: 422 Mbytes; MEM Efficiency: 0.00% CPU USAGE: CPU PEAK: 5.89 ;  CPU PEAK DURATION: 63 second(s) CPU AVERAGE EFFICIENCY: 42.81% ;  CPU PEAK EFFICIENCY: 98.15% SCHEDULING PARAMETERS:           r15s   r1m  r15m   ut      pg    io   ls    it    tmp    swp    mem loadSched   -     -     -     -       -     -    -     -     -      -      -   loadStop    -     -     -     -       -     -    -     -     -      -      -   RESOURCE REQUIREMENT DETAILS: Combined: select[(docker) &amp;&amp; (type == any)] order[r15s:pg] span[hosts=1] Effective: select[(docker) &amp;&amp; (type == any)] order[r15s:pg] span[hosts=1] </code></pre></div><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ bhist -l 2613Job &lt;2613&gt;, Job Name &lt;myOpenFoam_run_motorBike&gt;, User &lt;lsfadmin&gt;, Project &lt;defa                     ult&gt;, Application &lt;openfoam&gt;, Command &lt;/mnt/lsf/repository                     -path/lsfadmin/myOpenFoam_run_1722358195200AuWDY/motorBike                     /bsub.myOpenFoam_run&gt;Tue Jul 30 16:49:55: Submitted from host &lt;gsamu-hpc-demo-mgmt-1-a844-001&gt;, to Q                     ueue &lt;normal&gt;, CWD &lt;/mnt/lsf/repository-path/lsfadmin/myOp                     enFoam_run_1722358195200AuWDY/motorBike&gt;, Specified CWD &lt;/                     mnt/lsf/repository-path/lsfadmin/myOpenFoam_run_1722358195                     200AuWDY/motorBike&gt;, Output File &lt;/mnt/lsf/repository-path                     /lsfadmin/myOpenFoam_run_1722358195200AuWDY/motorBike/outp                     ut.lsfadmin.txt&gt;, Error File &lt;/mnt/lsf/repository-path/lsf                     admin/myOpenFoam_run_1722358195200AuWDY/motorBike/error.ls                     fadmin.txt&gt;, Notify when job begins/ends, 6 Task(s), Reque                     sted Resources &lt;span[hosts=1]&gt;;Tue Jul 30 16:55:23: Dispatched 6 Task(s) on Host(s) &lt;gsamu-hpc-demo-10-241-0-1                     37&gt; &lt;gsamu-hpc-demo-10-241-0-137&gt; &lt;gsamu-hpc-demo-10-241-0                     -137&gt; &lt;gsamu-hpc-demo-10-241-0-137&gt; &lt;gsamu-hpc-demo-10-241                     -0-137&gt; &lt;gsamu-hpc-demo-10-241-0-137&gt;, Allocated 6 Slot(s)                      on Host(s) &lt;gsamu-hpc-demo-10-241-0-137&gt; &lt;gsamu-hpc-demo-                     10-241-0-137&gt; &lt;gsamu-hpc-demo-10-241-0-137&gt; &lt;gsamu-hpc-dem                     o-10-241-0-137&gt; &lt;gsamu-hpc-demo-10-241-0-137&gt; &lt;gsamu-hpc-d                     emo-10-241-0-137&gt;, Effective RES_REQ &lt;select[(docker) &amp;&amp; (                     type == any)] order[r15s:pg] span[hosts=1] &gt;;Tue Jul 30 16:55:23: Starting (Pid 18426);Tue Jul 30 16:55:24: Running with execution home &lt;/home/lsfadmin&gt;, Execution CW                     D &lt;/mnt/lsf/repository-path/lsfadmin/myOpenFoam_run_172235                     8195200AuWDY/motorBike&gt;, Execution Pid &lt;18426&gt;;Tue Jul 30 17:04:01: Done successfully. The CPU time used is 1535.1 seconds;Tue Jul 30 17:04:02: Post job process done successfully;MEMORY USAGE:MAX MEM: 982 Mbytes;  AVG MEM: 431 Mbytes; MEM Efficiency: 0.00%CPU USAGE:CPU PEAK: 5.92 ;  CPU PEAK DURATION: 63 second(s)CPU AVERAGE EFFICIENCY: 50.67% ;  CPU PEAK EFFICIENCY: 98.68%Summary of time in seconds spent in various states by  Tue Jul 30 17:04:02  PEND     PSUSP    RUN      USUSP    SSUSP    UNKWN    TOTAL  328      0        518      0        0        0        846         </code></pre></div><p><strong>Conclusion</strong></p><p>The <em>user_data.sh</em> script of LSF resource connector allows a high degree of customization for cloud compute resources that dynamically join the LSF cluster. We’ve demonstrated how it can be used to tag cloud compute resources with a specific LSF Boolean resource in addition to the ability to install specific packages and do configuration customization. This is a simplified example, but illustrates this point.</p>",
            "url": "https://hpc.social/personal-blog/2024/advanced-lsf-resource-connector-configuration-on-ibm-cloud-part-iii/",
            
            
            
            
            
            "date_published": "2024-08-01T13:08:20-06:00",
            "date_modified": "2024-08-01T13:08:20-06:00",
            
                "author": "Ramblings of a supercomputing enthusiast."
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2024/sustain-is-a-verb-code-is-sustained-or-not-sustained-not-sustainable/",
            "title": "Sustain is a Verb- Code is Sustained or Not Sustained, not 'Sustainable'",
            "summary": null,
            "content_text": "(Note: This post is adapted from #182 of the Research Computing Teams Newsletter)So you, reader, will already understand that software is not “sustainable”.  There’s no sustainability linter you can run over the code to highlight possible sustainability issues, no test suite you can run to check for sustainability regressions.   Sustainability is not an inherent property of a piece of software.Same with a computing system, or a curated database, or..Instead, these efforts are sustained, or not, by people or organizations who pay for it to be sustained.Those people or organizations do this sustaining because they (or a community they support) need that software or other effort to do their jobs.  Because there are users who advocate for sustaining the effort, or sustain it themselves.In other words, sustaining is something a community does, and to the extent that “sustainability” in this sense is a thing that exists at all, one enhances an efforts’ “sustainability” by nurturing and supporting that community, and making it easy for them to effectively advocate for continued sustenance.Even so, the need the community has for that effort is going to wax and wane over time, as will the sustaining.   Eventually, at some point, the community will move on or dissolve entirely, and the sustaining will come to an end.So a tool goes from a prototype or something bespoke for one problem, and grows in technological readiness (#91) to become RCD development, not just research (#119), and over time gathers a community which, with luck, will sustain the effort for as long as the community exists and needs it.It may not find or build such a community - in startup speak it may not find “product-market fit”, and fade away (as with Sochat’s article on updated software in #172).  That’s disappointing for the individuals involved, but it’s very much the nature of research - not every idea or effort pans out.This understanding of sustaining vs sustainability is finally starting to gain wider acceptance, which heartens me.  We need to understand that passively hoping that by checking off a list of criteria we’ve proved ourselves worthy, and that therefore sustaining funding will somehwo happen, is to let down our community and our users.   We have to actively create the communities that will sustain the work we start.",
            "content_html": "<p>(Note: This post is adapted from <a href=\"https://www.researchcomputingteams.org/newsletter_issues/0182\">#182</a> of the <a href=\"https://www.researchcomputingteams.org\">Research Computing Teams Newsletter</a>)</p><p>So you, reader, will already understand that software is not “sustainable”.  There’s no sustainability linter you can run over the code to highlight possible sustainability issues, no test suite you can run to check for sustainability regressions.   Sustainability is not an inherent property of a piece of software.</p><p>Same with a computing system, or a curated database, or..</p><p>Instead, these efforts are sustained, or not, by people or organizations who pay for it to be sustained.</p><p>Those people or organizations do this sustaining because they (or a community they support) need that software or other effort to do their jobs.  Because there are users who advocate for sustaining the effort, or sustain it themselves.</p><p>In other words, sustaining is something a <strong>community</strong> does, and to the extent that “sustainability” in this sense is a thing that exists at all, one enhances an efforts’ “sustainability” by nurturing and supporting that community, and making it easy for them to effectively advocate for continued sustenance.</p><p>Even so, the need the community has for that effort is going to wax and wane over time, as will the sustaining.   Eventually, at some point, the community will move on or dissolve entirely, and the sustaining will come to an end.</p><p>So a tool goes from a prototype or something bespoke for one problem, and grows in <a href=\"https://www.researchcomputingteams.org/newsletter_issues/0091\">technological readiness</a> (#91) to become <a href=\"https://www.researchcomputingteams.org/newsletter_issues/0119\">RCD development, not just research</a> (#119), and over time gathers a community which, with luck, will sustain the effort for as long as the community exists and needs it.</p><p>It may not find or build such a community - in startup speak it may not find “product-market fit”, and fade away (as with Sochat’s article on updated software in #<a href=\"https://www.researchcomputingteams.org/newsletter_issues/0172\">172</a>).  That’s disappointing for the individuals involved, but it’s very much the nature of research - not every idea or effort pans out.</p><p>This understanding of sustaining vs sustainability is finally <em>starting</em> to gain wider acceptance, which heartens me.  We need to understand that passively hoping that by checking off a list of criteria we’ve proved ourselves worthy, and that therefore sustaining funding will somehwo happen, is to let down our community and our users.   We have to actively create the communities that will sustain the work we start.</p>",
            "url": "https://hpc.social/personal-blog/2024/sustain-is-a-verb-code-is-sustained-or-not-sustained-not-sustainable/",
            
            
            
            
            
            "date_published": "2024-06-02T00:00:00-06:00",
            "date_modified": "2024-06-02T00:00:00-06:00",
            
                "author": "Jonathan Dursi's Blog"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2024/isc-24-recap/",
            "title": "ISC’24 recap",
            "summary": null,
            "content_text": "I had the great pleasure of attending the ISC High Performance conference this month, marking the fifth time I've attended what has become one of my top must-attend industry conferences of the year. This year was particularly meaningful to me because it is the first time that:I attended ISC as a Microsoft employee. This is also the first time I've attended any HPC conference since I changed my focus from storage into AI infrastructure.I attended ISC in-person since before the pandemic. It's also the first time I've visited Hamburg which turned out to be an absolute delight.Although registrations have been lower since the pandemic, this year's final registration count was over 3,400 attendees, and there was no shortage of old and new colleagues to bump into walking between the sessions at the beautiful Congress Center Hamburg.&lt;p&gt;This year’s theme was “Reinvent HPC,” and that idea—that HPC needs to reinvent itself—was pervasive throughout the program. The whole industry had been pulling towards exascale for the better part of a decade, and now that there are two exaflop systems on Top500 and the dust is settling, it feels like everyone is struggling to figure out what’s next. Is it quantum? AI?&lt;/p&gt;It was difficult for me to draw a line through all the topics worth reviewing at this year's ISC, as it was a very dense four days packed with a variety of topics, discussions, vendors, and events. I only experienced a fraction of everything there was to be seen since so many interesting sessions overlapped, but I thought it might be worthwhile to share my perspective of the conference and encourage others to do the same.Table of ContentsReinventing HPC (and blast those hyperscalers!)Kathy Yelick's opening keynoteClosing keynotes on the futureTop500 and Aurora#1 - Frontier#2 - Aurora#3 - EagleOther notable tidbitsEveryone is an AI expert!The Exascale AI Synergies LLM Workflows BOFAI Systems for Science and ZettascaleReal applications of generative AI for scienceHigh Performance Software FoundationQuantum computingReinvent HPC to include urgent computing?The Urgent Computing focus sessionThe Interactive and Urgent HPC workshopConcluding thoughtsReinventing HPC (and blast those hyperscalers!)The need to reinvent HPC was the prevailing theme of the conference from the very first session; with the listing of Aurora as the second system on Top500 to break the 1 exaflops barrier, the community is in search of a new milestone to drive research (and funding!). At the same time, commercial AI has rapidly risen up largely in an independent, parallel effort with a speed and scale that begs the question: how important was the decade-long drive to break the exaflops barrier if the AI industry could catch up so quickly without the help of the institutions that have historically posted the top HPL scores? If the commercial AI industry overtakes scientific computing as the world leader in deploying at scale, how can “HPC” be reinvented so it can continue to claim leadership in another dimension?Kathy Yelick's opening keynoteISC’s opening keynote was given by Kathy Yelick, where she provided commentary on two recent government-commissioned reports on the future of HPC:Charting a Path in a Shifting Technical and Geopolitical Landscape: Post-Exascale Computing for the National Nuclear Security Administration, commissioned by the National AcademiesCan the United States Maintain Its Leadership in High-Performance Computing?, commissioned by the US Department of Energy’s Advanced Scientific Computing Research programLiving up to her reputation, Dr. Yelick’s talk was fast and insightful, describing the insatiable demand for computing driven by scientific research, the struggle to expose continuing amounts of parallelism to make use of newer processors, and some promising directions to address that disconnect. However, her talk started in a direction that I didn’t like when she went into describing the disruptors that necessitate reinventing HPC:The above slide implied that AI, quantum, or cloud may pose an existential threat to the HPC community gathered at ISC this year; this immediately raised my hackles, as it cast the relationship between “HPC” and “AI”/“cloud” as having some sort of adversarial tension. As the talk went on, I realized that “HPC” didn’t really mean “high-performance computing” to her. Rather, it was used to refer to something much more narrowly scoped—high-performance computing to solve scientific problems. Slide after slide, the presentation kept doubling down on this idea that “HPC” as the audience knows it is being threatened. For example, Yelick talked through this slide:The picture she painted is that “HPC” (denoted by companies with blue bars) no longer has influence over technology providers because the “hyperscalers” (green bars) have such an outsized amount of investment. She then used this to call on the audience to think about ways “we” could influence “them” to produce technologies that are useful for both scientific computing and low-precision AI workloads.Her talk culminated in this slide:Which was accompanied by this conclusion:\"So what’s a post-exascale strategic for the scientific community? It's the beat 'em or join 'em strategy. The beat 'em strategy says we’re going to design our own processors. [...] The join 'em strategy says let's leverage the AI hardware that's out there. [...] The sort of sneaky way of doing this is getting embedded in the AI community and trying to convince them that in order to make AI better for commercial AI applications, you really want to have certain features. Like don't throw away your 64-bit arithmetic and things like that.\"I found myself getting increasingly unsettled through the keynote, because this \"us versus them\" mentality put me, a long-standing member of this HPC community, in the camp of \"them.\" It was as if I was suddenly an outsider in a conference that I've been attending for years just because I no longer work for an organization that has been doing HPC since the early days of computing. Even though the clusters I support use the same NVIDIA and AMD GPUs, the same InfiniBand fabrics, and the same Lustre file systems that \"HPC\" uses, I am no longer in \"HPC\" because I am \"hyperscale\" or \"cloud\" or \"AI.\"The underlying message is one I get; GPUs are trending in a direction that favors massive gains in lower-precision computation over FP64 performance. And the cost of HBM is driving the overall value (in FP64 FLOPS per dollar) of accelerators backwards for the first time in the history of scientific computing. But the thesis that the scientific computing community needs to be sneaky to influence the hyperscale or AI players seemed way off the mark to me. What seemed absent was the recognition that many of the \"hyperscalers\" are her former coworkers and remain her colleagues, and \"they\" sit in the same audiences at the same conferences and share the same stages as the \"HPC\" community. All that is true because \"HPC\" is not somehow different than \"cloud\" or \"AI\" or \"hyperscale.\" If there really is a desire to influence the hyperscale and AI industry, the first step should be to internalize that there is no \"us\" and \"them.\"Closing keynotes on the futureJust as the conference was opened with a talk about this \"us versus them\" mentality, it was closed with a talk about \"us versus them\" in a keynote session titled, \"Reinventing HPC with Specialized Architectures and New Applications Workflows\" which had two speakers followed by Q&amp;A.Chiplets for modular HPCJohn Shalf gave one half of the closing keynote, where he gave his usual rally for investments in chiplets and specialized processors for HPC:He gives a variant of this talk at every ISC, but this year he lasered in on this notion that the \"HPC\" community needs to do what the \"hyperscalers\" do and use chiplets to develop custom ASICs. It was an energetic and impassioned talk, but this notion that hyperscalers are already executing on his idea for the future sounded a little funny to me seeing as how I now work for one of these hyperscalers and his message didn't resonate.If you really follow the money, as Shalf suggested, a huge amount of it is flowing into GPUs, not specialized processors. It wasn't clear to me what specialization he was thinking of when he referred to custom silicon being developed by the likes of Meta, Google, AWS, and Microsoft; it's true that these companies are developing their own silicon, but those efforts are largely addressing cost, risk, and supply, not improving performance beyond more general-purpose silicon like GPUs. And it turns out that a significant fraction of the (non-US) HPC community is already developing custom silicon for the same reasons as the hyperscalers; Japan, China, and Europe are all developing their own indigenous processors or accelerators for scientific computing at leadership scales. In that sense, Shalf was preaching to the choir given that, on the international stage, his government is the odd one out of the custom silicon game.He also suggested a dichotomy where the HPC community would either have to just (1) make every scientific problem an AI problem or (2) join this journey towards making domain-specific accelerators, ignoring the significant, unexplored runway offered by using mixed precision arithmetic in scientific applications. He called for partnering with hyperscalers, but his examples of implementing a RISC-V-based stencil accelerator and a SambaNova-based DFT processor didn't draw a clear line to the core missions of the large hyperscalers he extolled. He briefly said that partnering would benefit hyperscalers by addressing some capital cost challenges, but seeing as how the annual capital expenditures of the hyperscalers outstrips those of the US national HPC effort by orders of magnitude, I couldn't understand what the hyperscalers would stand to gain by partnering in this way.Integrating HPC, AI, and workflowsRosa Badia gave the second half of the closing keynote where she proposed ideas around complex scientific workflows and the novel requirements to support them. This talk felt a lot more familiar, as the focus was squarely on solving scientific computing challenges by connecting traditional HPC resources together in nontraditional ways using software whose focus goes beyond cranking out floating point arithmetic.As she spoke, I couldn't help but see parallels between the challenges she presented and the sort of technologies we live and breathe every day in cloud services.  For example, she showed this slide:Dr. Badia obviously wanted to make a cloud-tie in by calling this \"HPC Workflows as a Service,\" but what I'm not sure she realized is that this model almost exactly describes platform-as-a-service frameworks that already exist in commercial clouds. For example,What she calls a \"Data Catalog\" is a public or private object storage account (a blob container, an S3 bucket) or a PaaS abstraction built atop themWhat she calls a \"Software Catalog\" is a container registry (Azure Container Registry, Amazon Elastic Container Registry) or an abstraction built atop themA \"Workflow Description\" is something like an AzureML pipeline or SageMaker pipelineA \"Workflow Registry\" is just a Github repository containing pipelinesThe \"Portal\" is the web UI provided by AzureML or SageMakerI don't think there's anything truly new here; the challenges she described lie in wedging these workflows into HPC infrastructure which lacks the platform features like robust identity and access management (i.e., something better than LDAP that supports more modern authentication and authorization flows and finer-grained access controls) and data management (i.e., something better than a parallel file system that depends on POSIX users, groups, and permissions and implicit trust of clients).She went on to describe a workflow data management system that reinvented a bunch of infrastructure that is already baked into commercial cloud object stores like Azure Blob and AWS S3:As she was describing the requirements for such a workflow data management layer, it struck me that what the scientific data community calls \"FAIR principles\" are the same basic requirements for operating in commercial environments where data may be subject to strict privacy and compliance regulations. The notion of findable data may be aspirational for scientific datasets, but when a company is having to find datasets because it's being sued or subpoenaed, findability is a bare-minimum requirement for any data management system. Similarly, tracking the provenance of data may be a nice-to-have for scientific data, but it is a hard requirement when establishing a secure software supply chain. Cloud storage systems solved many of these challenges a long time ago, and I can't help but wonder if this idea that workflows in HPC pose a new set of challenges is another manifestation of \"us\" not realizing \"they\" might have done something useful and applicable for science.Badia's final slide had a particularly poignant statement which read, \"Systems can only be justified if we have applications that need them.\" I think she was trying to call for more investment in application development to exploit new systems, but I think the inverse is also true. If modern scientific applications truly require more complex orchestration of compute and data, maybe the scientific computing community should stop building computing platforms that make it really difficult to integrate different systems.Again, \"HPC\" is not the opposite of \"cloud;\" it's not an either/or decision. There are technologies and tools that were designed from the beginning to simplify the secure connection of services and resources; they just weren't invented by the HPC community.Top500 and AuroraOne of the cornerstones of ISC is the semiannual release of the Top500 list, and unlike at SC, the Top500 announcements and awards do not overlap with any other sessions, so it tends to have a higher profile and draw all attendees. This go-around, there were no dramatic changes in the Top 10; the new Alps system at CSCS was the only new entry, and the order of the top five systems remained the same. Notably, though, Aurora posted a significantly higher score than at SC'23 and broke through the exaflops barrier using 87% of the system, cementing its place as the second exascale system listed. But let's start at the top.#1 - FrontierFrontier at Oak Ridge remained #1, but it squeezed twelve more petaflops out of the same node count and is now just over 1.2 EF. Nothing groundbreaking, but it's clear evidence that ORNL is continuing to tune the performance of Frontier at full system scale.#2 - AuroraAurora, on the other hand, finally eked over the exaflops line with 1.012 EF using 87% of the system's total 63,744 GPUs. Rick Stevens gave a short talk about the achievement which is summed up on this slide:I was a little surprised by how honest Stevens was in this talk; the typical game that is played is that you stand up on stage, talk about how great of a partnership you had with your partners to realize this achievement, extol the virtues of the technologies on which your system was built, and talk about how this HPL score is just the start of a lot of great science.Stevens didn't do that though.He started out by telling the conference that Intel had bad product names, then explained that their low Graph500 and HPCG scores were the result of their exclusive focus on breaking the exaflops barrier with HPL, implying they didn't have time or ability to run Graph500 or HPCG at the same 87%-89% scale as their HPL and HPL-MxP runs. Based on this, it sounds like Aurora is still a ways away from being stable at scale, and we're unlikely to see any Gordon Bell-nominated papers at SC'24 this November.After this session, folks seemed to relish in dunking on Aurora; its window to be #1 is likely to have closed and it has some power efficiency issues. But I don't think anyone involved with the Aurora project needs to be told that; if what Stevens implied is true, the folks at ALCF, Intel, and HPE have been struggling for a long time now, and topping out over 1018 was a hard-sought, major milestone to be celebrated. The Aurora project has been thrown more curveballs than I would have ever guessed a single HPC project could have, so all parties deserve credit for sticking it through all this way rather than just walking away. With any luck, Aurora will stabilize in the next six months, and we'll see full-scale runs of Top500, Graph500, HPCG, and science apps by November.#3 - EagleThe third highest system on the list was Eagle, whose HPL score was not updated since the system was first listed at SC'23 last year. Through a few twists of fate, I wound up being the person who accepted the award on-stage, and I now have a Top500 award for the #3 system sitting in my home office. Here's a photo of me goofing around with it:It's not entirely inappropriate that I was the one to accept it since my teammates are the ones carrying pagers for the on-call rotation of that system, and we were also the hands-on-keyboard when that HPL run was conducted. Still, it was a bit surreal to walk on-stage to pick up such a noteworthy award immediately following two actually important people (both of whom have \"director\" in their titles) accepting the same award. By comparison, most of my career highlights to date have been just trolling HPC people on Twitter (as the esteemed Horst Simon actually said out loud as I was leaving the stage!)It was weird.That said, I take this to mean that it is now my duty to be the friendly face from Microsoft who can speak intelligently about the #3 system on Top500. To that end, I'll answer some questions that I was asked at ISC about the system and Azure HPC clusters in general below. None of this is new or secret information!Why didn't you run HPL again and post a higher score to beat Aurora? Because the day after that HPL run completed, that system was put into production. Once systems are in production, people are paying to use them, and taking a time-out to re-run HPL costs a ton of money in either real dollars (if a customer runs it) or lost revenue (if the HPL run is blocking customer workloads). This is quite different from public-sector HPC systems which never have to pay for themselves.Can I get access to Eagle for a Gordon Bell run or to test software? That's not really how it works. Whereas a traditional supercomputer might allow users to ssh in and submit jobs to a Slurm queue, cloud-based supercomputers allow users to deploy virtual machines through a REST API. Those virtual machines can allow ssh, run Slurm, and support MPI jobs like HPL, but that OS environment is managed by Azure users, not Azure itself. You can get a taste for what's required to run a basic MPI job by reading some instructions I wrote on provisioning an MPI cluster on Azure.Is it just a bunch of GPU nodes scattered around a bunch of data centers? No, all the nodes on any given Azure HPC cluster (like Eagle) share an InfiniBand fabric. There are countless InfiniBand clusters in Azure, but each one is a real supercomputer by any definition of a supercomputer, and they are designed to run tightly coupled job across all their GPUs.What parallel file system does it use? Don't think about it that way. You can provision a Lustre file system and mount that to any or all cluster nodes if you want to, or you can access data directly from object storage.Are there any photos of it? You can see a photo of one of the Microsoft-designed nodes that comprise the system on my SC'23 recap blog post. Beyond that, there's not much to look at because Azure HPC clusters are not meant to be photogenic like, say, Cray supercomputers. There's no rack graphics (or even rack doors!). It's just tons and tons of air-cooled racks with InfiniBand optics coming out of each one. Maybe the only unique thing is that the racks are painted white instead of the typical black. Not sure why.Getting back to that false separation between \"HPC\" and \"cloud,\" Eagle is strong evidence that they aren't different. What the \"hyperscalers\" do is not that different from what traditional HPC centers do. Perhaps the biggest difference is that cloud supercomputers get all the benefits of cloud infrastructure like software-defined infrastructure like virtual machines and virtual networking, integration with identity and access management that transcends simple Linux UIDs/GIDs, and the flexibility to integrate with whatever storage systems or ancillary services you want from any compute node.Other notable tidbitsIt is tradition for Erich Strohmaier to talk through some highlights and trends of the latest Top500 list every time a new one is announced, and in the past, I've been critical of how he's presented conclusions from the list with this implicit assumption that computers that never post to Top500 simply don't exist. This year felt different, because Dr. Strohmaier made the explicit statement that China has completely stopped submitting to Top500. Their exascale systems aren't listed, but neither are any new systems in the past three years at the bottom. They simply don't play the game anymore, making it undeniable that Top500 is no longer an authoritative list.Just as the whole conference's theme was reinventing HPC, I felt a sense that even the most stalwart proponents of Top500 are now recognizing the need to reinvent the Top500 list. Kathy Yelick said as much during her keynote (\"Shall we replace Top500? What are the metrics in post-exascale computing that are important?\"), and Erich implored the audience to help expand the HPL-MxP (formerly HPL-AI; an HPL-like benchmark that can use the mixed-precision capabilities of tensor cores) list. Nobody seems to know how to quantify what makes a leadership supercomputer nowadays, but accepting that HPL scores (or appearing on the Top500 list!) won't cut it is a good first step.That all said, Top500 is still a valuable way to track technology trends in the industry. For example, this edition of the list where NVIDIA's new Grace-Hopper node started appearing in force. The only new entrant in the Top 10 was the 270 PF GH200 component of CSCS's Alps system, and HPEhad these EX254n GH200 blades on display on the show floor.To HPE/Cray's credit, they seem to have gotten the system up and running with Slingshot without the delays that plagued early Cray EX systems like Frontier and Aurora. Hopefully this is a sign that the Cray EX platform and Slingshot-11 have graduated from being risky and not-quite-production-ready.The other notable entrants on this year's Top500 are a trio of early MI300A APU-based Cray systems being built around the El Capitan program at Lawrence Livermore National Laboratory. This is a positive sign that MI300A is up and running at modest scale, and HPE also had one of these EX255a blades on display at their booth:The strong showing of MI300A suggests that we may see El Capitan take the top spot in the next edition of the Top500 list coming in November.Everyone is an AI expert!Since I now work on a team responsible for AI infrastructure, I tried attending as many of the AI-focused talks and panels as I could this year. Unsurprisingly, these sessions largely carried the same undertones of \"reinventing HPC,\" and speakers opined on how AI would affect scientific computing and offered examples of what their institutions were doing to extend their leadership in the HPC space into the AI space. There was a fair amount of grasping going on (as there always is when AI is discussed at non-AI conferences), but this year I was struck by how confused so many speakers and attendees were about concepts related to applying AI.To be clear: I am no expert in AI. However, my day job requires that I be steeped in some of the largest AI training workloads on the largest AI supercomputers on the planet, and I have to have a cursory understanding of the latest model architectures and techniques to anticipate how future system designs will have to evolve. It's from this perspective that I made the following observation: there are a lot of HPC people speaking very confidently about AI based on an outdated understanding of the state of the art. The AI industry generally moves much faster than the government-funded research community, and I couldn't help but wonder if some community leaders assumed that the AI industry today is the same as it was the last time they wrote their AI grant proposal.Of course, there were also some really insightful perspectives on AI for science shared as well. Let's talk through some examples of both.The Exascale AI Synergies LLM Workflows BOFThis realization that the ISC community is not keeping up with the AI community first slapped me in the face when I ducked into a BOF session titled, \"Tales of Exascales – AI and HPC Supercomputing Platforms Synergies for Large Language Models (LLMs) and Scientific Workflows.\" I sometimes wonder if the organizers who propose titles like that are intentionally creating word salad, but in this case, it was apt session name; the discourse around HPC and AI was all over the board throughout the hour.The session started on a strong, positive note by Simon McIntosh-Smith describing Bristol's new Isambard-AI system, a GH200-based Cray supercomputer funded under the broad charge of \"AI research.\" While I'm usually skeptical of such nebulously defined \"AI research\" machines, Dr. McIntosh-Smith's description of the project quickly checked a bunch of boxes on how a real AI research platform should be developed. In particular,Isambard-AI was developed and deployed at the pace of AI rather than HPC for scientific computing. Whereas government-funded, large-scale HPC systems typically take years to procure, Simon said that the first discussions started in August 2023, and in the nine months that followed, they had built the site, the team, and the system itself to the degree that a piece of the final system is already on Top500. By comparison, LLNL's El Capitan supercomputer also debuted on Top500 this month, but its contract was signed five years ago, and its procurement began at least two years before that. The AI industry would not exist if the systems it trains on took seven years to procure.Isambard-AI deliberately avoided exotic AI accelerators to remain future-proof. Simon rightly pointed out that the AI industry moves too quickly to anticipate whether a bespoke AI accelerator would even be relevant to whatever the hottest model architecture will be in a year. GPUs were chosen because they are the most flexible way to accelerate the widest range of AI workloads, regardless of if they are dense models, sparse models, inferencing, training, and whatever level of quantization makes sense. The reality is that cutting-edge research is done on GPUs, so aligning an AI supercomputer on the same technology will ensure that the algorithms developed by industry are immediately usable for scientific research.A reasonable definition of \"AI for science\" was defined from the outset. Rather than blurting out \"we need to research AI!\" and asking for a sack of money to buy GPUs, Simon outlined a vision of training AI models using data generated by physical simulation on a more conventional HPC system. Training models on models to create surrogate models is not particularly new, but it does establish a few reasonable architectural decisions such as having a robust data management and sharing platform, close coupling to the HPC system performing simulation, and aligning software stacks and programming environments as closely as possible.Simon's contribution to the discussion stood out to me as the most impressive, and the discourse seemed to fall into a trap of familiarity following. Rather than focusing on the new and exciting prospects of AI, some panelists and audience members wanted to focus on the aspects of AI they understood. For example, an uncomfortable time was spent on a back-and-forth on how HPC centers can support Kubernetes and random I/O (which is what defines AI vs. HPC?) instead of Slurm and Lustre. If your biggest challenge in delivering infrastructure to support AI workloads is figuring out how to deploy both Kubernetes and Slurm, you haven’t even reached the starting line. This is a trivial issue in cloud environments, where entire AI clusters can be built up and torn down in minutes. Again, this is evidence that the scientific computing community isn’t ready to keep pace with the AI industry.I jotted down a few of the questions and comments that I heard during this BOF that seem to reflect the level of familiarity the average ISC attendee has with AI:\"Would be nice if there were more models for science.\" I wasn't sure sure what this means. All the leading LLMs are pretty good at \"science,\" and domain-specific models aren't readily transferable between different science domains or problems.Scientific problems \"have to validate outputs for correctness, unlike LLMs.\" I think the speaker was making a sidelong reference to hallucinations, but like with any model (large language or physics-based), validating outputs for correctness is certainly necessary and readily possible.\"The demands of inference of LLMs are completely different from those for training. How do you buy inference infrastructure?\" I wonder where this notion came from. If your infrastructure can train a model, it can definitely inference that model. Cost-optimizing infrastructure for inferencing is a separate matter (you can cut corners for inferencing that you wouldn't want to cut for training), as is building the service infrastructure around inferencing to deliver inferencing as a service. But I don't think that's what this question was about.\"Working safely with sensitive data / isolating workloads on big shared clusters.\" This is a problem that arises only when you try to wedge AI workloads into infrastructure designed for traditional physics-based simulation. If you have sensitive data, don't use big shared clusters. Provision separate clusters for each security domain on a shared, zero-trust infrastructure.\"How different are the files and filesystem access while training for LLMs, image generation models, reinforcement learning?\" This question reflects a general misunderstanding of data and storage in HPC overall; how data is organized into files and how that data is accessed by a workload is an arbitrary decision made by the application developer. You can organize piles of text into one giant file or a million little files.There were a few questions that came up that touched on deeper issues on which the HPC community should reflect:\"What are the first steps for scientific groups wanting to get ready for using AI in the future?\" This is probably the purest question raised in the entire session, and I think this is something the scientific computing community as a whole needs to figure out. What does \"using AI\" really mean for scientific groups? Is it training models? Fine-tuning models? Inferencing using pre-trained models on HPC infrastructure? Is it integrating simulation applications with separately managed inferencing services? Who manages those inferencing services? Does inferencing even require HPC resources, or can suitable models run on a few CPU cores? I think the first step to answering this question is ensuring that the scientific computing community reaches a common baseline level of understanding of \"using AI\" means. And a lot of that probably means ignoring what some self-professed AI experts in the HPC community claim is the future.\"Care to predict what that ChatGPT moment will be for AI for Science? Had it already happened?\" This question was addressed directly by panelist Séverine Habert who rightly pointed out that the ChatGPT moment occurred when a complex and esoteric topic was suddenly put in the hands of hundreds of millions of laypeople across the world. It was the moment that the common person walking on the street could suddenly interact with the most cutting-edge technology that had been previously understandable only to the headiest of researchers in industry and academia. That will likely never happen in AI for science because science, by definition, requires a higher baseline of education and understanding than the average layperson has.\"How to effectively train the existing workforce when we are already struggling to retain talent in research/academia?\" This question strikes at the same theme that Kathy Yelick's opening keynote confronted: what is the role of the scientific computing community now that it turns out that you don't need decades of institutional experience to deploy and use HPC resources at leadership scale? As offensive as it may sound, perhaps the public-sector HPC community should accept that their role is not training future researchers and academics, but training future practitioners of AI in industry. This is how the wider tech industry generally works; neither startups nor tech giants make hires assuming those people will still be around in ten years. Why does the public-sector HPC industry think otherwise?Finally, I was also struck but how fiercely the discourse clung to the idea that large language models are the answer to all AI problems in science. I get that this panel was focused on exascale, and LLM training is one of the rare cases where AI requires exascale computing capabilities. But there was no acknowledgment that trillion-parameter models are not actually a good idea for most scientific applications.AI Systems for Science and ZettascaleThis singular focus on creating massive LLMs for science was front-and-center in a talk given by Rick Stevens titled \"The Decade Ahead: Building Frontier AI Systems for Science and the Path to Zettascale.\" The overall thesis that I heard was something like...Science needs its own trillion-parameter foundation modelsTraining trillion-parameter foundation models requires a lot of GPUsWe need $25 billion from the U.S. governmentHowever, Stevens never answered a very basic question: what does a foundation model for science do that any other foundation model cannot do?He showed slides like this which really don't sound like foundation models for science as much as a generic AI assistants:Is the scientific computing HPC community really the most qualified bunch to reinvent what existing foundation models like GPT-4 or Claude 3 have already done? Even if you argue that these proprietary models aren't as good at \"science\" as they could be, who would have a better chance of addressing this with a billion dollars of federal funding: the companies who developed GPT or Claude, or a collection of government scientists starting from scratch?I think the answer to this question was in other parts of Stevens' talk. For example, he started with this slide:While robust requirements are good when there's no urgency, this slide is also a tacit admission that the government takes years to general a perspective on AI. Do you think the creators of Llama-3 or Mistral Large gathered wide community input from over 1,300 researchers before deciding to build a supercomputer and train a model? Even if science needs its own foundation models, this slide is strong evidence that, by the time the scientific HPC community agrees on a path forward, that path will be years out of date relative to what the commercial AI industry is doing.A great example of this already happening is the basic premise that creating a foundation model with a trillion parameters is the best way to apply AI to solve science problems. This certainly was the leading thought two years ago, when transformer scaling laws were published that suggested that the best way to get better-performing LLMs was to simply add more parameters to your transformer and train on more data. But there's a reason all the leading models have stopped advertising how many parameters they use.Dealing with massive transformers is really expensive. They're not only really expensive to train, but they're really expensive to use for inferencing too. This has led to a bunch of innovation to develop model architectures and approaches to training that result in dramatically higher quality outputs from a fixed parameter count. Dense transformer architectures with a trillion parameters have become the blunt instrument in developing foundation models since 2022, so it took me by surprise to hear Stevens put so much stock into this notion that the need for a trillion-parameter model is essential for science.To repeat myself, I am no expert in AI. I've never been called in front of Congress to talk about AI or been invited to give talks on the topic at ISC. There might be something basic that I am missing here. But when I look at the science drivers for AI:I know that you do not need to train your own trillion-parameter model to do most of this stuff. Even the use cases that do require generative AI, like code generation and math theory, don't actually require trillions of parameters. Small language models, such as that described in Textbooks Are All You Need (published in 2023, after the reports Stevens cited in his talk), can produce amazing results with very small models when you train them using high-quality data instead of garbage from Reddit. And when you create or fine-tune a small language model for a specific science domain, not only do you save yourself from having to buy a billion-dollar supercomputer for training, but you get a model that is much more accessible to scientists around the world because they won't need a million dollars' worth of GPUs to inference with it.So, if there's one question that was never answered across any of the AI-themed sessions at ISC this year, it is this: Why does science need to train its own large language models? My intuition is that either fine-tuning existing large language models or training small language models for domain-specific applications, would be a better investment in actually advancing science. However, if we cynically assume the real goal of LLMs-for-science is to justify buying massive GPU systems, suddenly a lot of the talks given at ISC on this topic make a lot more sense.Real applications of generative AI for scienceAs frustrated as I got sitting through sessions on AI where it sometimes felt like the blind leading the blind, there was one really good session on actual applications of generative AI for science.Mohamed Wahib of RIKEN gave an insightful presentation on the unique challenges of using generative AI in science. His summary slide touched on a lot of the key challenges:And his actual talk focused largely on the model and data aspects of generative AI. What struck me is that the challenges he described reflected the experience of someone who has actually tried to do what many other AI experts at the conference were claiming would be the future. For example,He recognized the importance of training scientific models with high-quality datasets, not just garbage scraped off of social media. This means not only scraping or generating high quality data for training, but curating and attributing that data and applying reinforcement learning with human feedback as the model is being trained. This is uniquely challenging when creating models for scientific applications, as managing the quality of scientific data requires deep domain expertise. This contrasts with a generic chat bot whose inputs and outputs can often be assessed by anyone with a basic education.He also talked about the tendency of scientific data to be highly multimodal and multidimensional. Whereas multimodal chatbots may combine text and vision, scientific data often contains observations of the same phenomenon from many different sensors (for example, pressure, temperature, density, strain fields, ...), and the output of a generative model for science may require multiple modalities as well.  These capabilities are not well developed in LLMs designed for human language.Dr. Wahib also pointed out that scientific datasets tend to be huge compared to text and images, and this may require developing ways for models to have context windows can fit multi-petabyte datasets' tokens to identify long-range correlations. Relatedly, he also pointed out that tokenization of scientific data is a new set of challenges unique to this community, since industry has been focused on tokenizing low-dimensional data such as text, audio, and images.The good news is that industry's quest towards both commercializing generative AI and achieving AGI will touch on some of these challenges soon. For example, training domain-specific models using high-quality datasets is an essential component of the small language models I described in the previous section, and these small language models are what will enable privacy-preserving and cost-effective generative AI on laptops and phones. Effectively infinite context windows are also a major hurdle on the path to AGI, as industry is hard at work developing AI agents that can remember every conversation you've ever had with them. Finding more scalable approaches to attention that do not sacrifice accuracy are a part of this.François Lanusse, currently at the Flatiron Institute, also gave a nice presentation that clearly explained how generative AI can be used to solve inverse problems—that is, figuring out the causes or conditions that resulted in a collection of measurements. A precise example he used applied generative AI to figure out what an image distorted by gravitational lensing might look like in the absence of those distortions. As I understood it, he trained a diffusion model to understand the relationship between images that are affected by gravitational lensing and the masses that cause lensing through simulation. He then used that model instead of an oversimplified Gaussian model as part of a larger method to solve the inverse problem of un-distorting the image.The details of exactly what he did were a little over my head, but the insight piece for me is that combining generative AI and science in practice is not as straightforward as asking ChatGPT what the undistorted version of a telescope image is. Rather, almost all of the standard, science-informed approach to solving the inverse problem remained the same; the role of generative AI was simply to replace an oversimplified part of the iterative process (the Annealed Hamiltonian Monte Carlo method) to help it converge on better answers. It really is a combination of simulation and AI, rather than an outright substitution or surrogate model.Dr. Lanusse also showed this slide which demonstrated how this approach can be generalized to other scientific domains:The general approach of pretraining, fine-tuning (\"adapt\"), and combining foundation models with other physics-based models seems reasonable, although I admit I have a difficult time wrapping my head around exactly how broadly scoped he envisions any given pretrained foundation model to be. I can see such a model trained on extensive sky survey data being useful for a number of astrophysical and cosmological tasks, but it's less clear to me how such a model might be useful in unrelated domains like, say, genomics.You might also ask why I think this vision of foundation models for science is reasonable while Rick Stevens' vision didn't ring true; the difference is in scale! The foundation models cited on Lanusse's slide are vision transformers which have many orders of magnitude fewer parameters than the trillion-parameter models that others talk about. Whereas a trillion-parameter model might need to be distributed over dozens of H100 GPUs just to produce one inference result, the largest of the vision transformers can probably be squeezed on to a single high-end desktop GPU. Again, you don't need billion-dollar supercomputers to train these models for science.Frank Noé from Microsoft Research then talked about how generative AI can be applied to solve problems in simulating biological systems. Like the talk before his, Dr. Noé followed this pattern where a larger, physics-based framework had one statistical technique replaced by a method based on generative AI, and then a physics-based model is used to quantify the likelihood that the result is reasonable. He contrasted this with convention approaches (to, say, protein folding) where you just simulate for really long times in the hopes that your simulation randomly wanders into a situation where you capture a rare event.His talk wasn't about generative AI as much as the previous speakers, but he offered a litany of ways in which AI models can be useful to molecular modeling:Markov state models provide a statistical framework that lets you replace one long simulation (that hopefully captures every possible scenario) with a bunch of short, chopped-up simulations that hopefully capture every possible in parallel. He cited an example that took 20,000 GPU-days on V100 GPUs that would've otherwise taken a million GPU-years if done in one long simulation.Coarse-grained models use machine learning to develop surrogate models to simulate the physics of relatively uninteresting parts of molecular systems. The example he used was simulating the water molecules surrounding a biomolecule; water can be very difficult to accurately model, and the example he cited led to a surrogate model that was 100x faster than directly simulating water molecules.Boltzmann generators can generate 3D molecular structures based on a known probability distribution defined by the energy states of the system. This is another fast way to find rare but stable molecular configurations without having to throw darts at a dartboard.What struck me is that, in all these cases, the AI model is never generating results that are blindly trusted. Instead, they generate molecular configurations which are then fed into physics-based models which can quantify how likely they are to be valid.Both Lanusse's and Noé's examples of combining AI and simulation painted a picture to me where generative AI can be really useful in solving problems where a researcher would otherwise have to make educated guesses about what physical phenomenon is really happening based on incomplete information. So long as there is a way to apply a physics-based model to check the accuracy of each guess, generative AI can be trained to predict the relationships between incomplete information and what's really going on and get to probable answers much faster than relying on physics alone.More broadly, I couldn't help but think about the Sora video showing pirate ships battling in a cup of coffee as I left this session. Like that video, these talks demonstrated that it's possible to train generative AI models to reproduce physical phenomena (like the fluid dynamics of coffee) without explicitly embedding any laws of physics (like the Navier-Stokes equations) into the model itself and still get really compelling results. The part of this that was lacking from the Sora video—but was present in these talks—was closing the loop between generated results and the laws of physics by feeding those generated results back into the laws of physics to figure out if they are probable.High Performance Software FoundationISC'24 wasn't all about AI though! I wound up attending the launch of the High Performance Software Foundation (HPSF), a new Linux Foundation effort spearheaded by Todd Gamblin and Christian Trott (from Livermore and Sandia, respectively) aimed to promote the sustainability of the software packages relied upon within the high-performance computing community.I haven't paid close attention to HPC software in a long time since most of my work was in platform architecture and storage systems, so a lot of the background context remains a little murky to me. That said, it seems like HPSF was formed to be like the Cloud Native Computing Foundation for the HPC community in that:it will serve as a neutral home for software projects that aren't tied to any single university or government institutionit provides mechanisms to ensure that critical HPC software can continue to be maintained if its original author gets hit by a busit will help with the marketing, promotion, and marketing of HPC softwareIts governance seems pretty reasonable, with different levels of membership being accompanied by different levels of rights and obligations: There is a Governing Board is comprised of paying members (and predominantly those who pay the most), while the Technical Advisory Council carries out the more technical tasks of forming working groups and onboarding projects.There are three levels of membership, and the highest (premier) has a $175,000 per year buy-in and comes with a seat on the Governing Board. Right now, the founding seats are held by AWS, HPE, LLNL, and Sandia.Below that is a general membership tier whose cost is on a sliding scale based on the organization size, and AMD, Intel, NVIDIA, Kitware, ORNL, LANL, and Argonne have all committed at this level.  The associate tier is below that, and it is free to nonprofits but comes with no voting rights.It seemed like the exact functions that HPSF will have beyond this governing structure are not fully baked yet, though there were six \"prospective\" working groups that provide a general scope of what the HPSF will be doing:My read of the description of these working groups is thatCI/testing will supply resources (GPUs) on which HPSF projects' code can be automatically tested.Software stacks will maintain E4S.User engagement sounds like it will figure out what users of HPSF projects' software are looking for. It sounds like this will provide some product management-like support for projects.Facility engagement is probably like user engagement, but for the sites deploying code on behalf of their users. Again, this sounds like product management functions.Security sounded like stewarding SBOM-like stuff for member projects' software.Benchmarking would make a framework for benchmarking HPC applications.That all said, it still wasn't clear what exactly HPSF would do; what would all those membership dues go towards supporting? Based on some Q&amp;A during this BOF and follow-up afterwards, I pieced together the following:HPSF will not be funding developers, much in the same way that OpenSFS doesn't fund Lustre development. That said, Todd Gamblin later said that not funding software development was a financial constraint more than a policy one, with the implication that if more members join, there may be opportunity for HPSF to fund projects.HPSF likely will be hosting events and conferences (perhaps like the CNCF hosts KubeCon), providing scholarships, developing and providing training related to member projects, and \"increasing collaboration\" (whatever that may mean!).HPSF also has some influence and ownership over its member projects:HPSF will co-own its projects' GitHub repos to ensure continuity in case the other repo owner abandons it.HPSF will own the domain for the project for the same reasons as above.Member projects still manage their own software development, roadmaps, releases, and the like. The HPSF won't dictate the technical direction of projects.HPSF will own the trademark and logos of its member projects so it can prevent corporations from profiting off of repackaging products without respecting trademark.This establishes an interesting new direction for the sorts of software projects that are likely to become member projects. Historically, such projects developed by the member organizations (i.e., DOE labs) have been wholly controlled by the labs that funded the work, and those software projects lived and died at the whims of the government funding. The HPSF offers a new vehicle for software projects to live on beyond the end of the grants that created them, but at the same time, it requires that the DOE surrender control of the work that it sponsored.I left the session still wondering a few pretty major things, likely borne out of my own ignorance of how similar organizations (like CNCF or the Apache Foundation) work:How does a software project actually become a member project? The HPSF folks said that the Technical Advisory Committee onboards new projects, but what is the bar if I have an open-source project used by the community that I no longer want to maintain myself? I assume it's not a pay-to-play arrangement since that defeats the purpose of sustaining software after its seed funding runs out.What do stakeholders actually get out of joining HPSF? I see obvious value for organizations (like the DOE labs) who develop open-source software but may not want to be exclusively responsible for sustaining it forever. But would an HPC facility get any obvious benefit from joining and paying dues if it is simply a consumer of member projects' software? What does a cloud vendor like AWS get by being a premiere member? Is HPSF just a way to get someone else to cover the overheads of maintaining open-source software that comes out of, say, R&amp;D organizations rather than product organizations?Hopefully the answers to these questions become clearer as the foundation gets off the ground and we get to see what member organizations contribute under the HPSF banner.Ultimately though, I see this as a really positive direction for the HPC software community that might help resolve some uncertainty around key pieces of HPC software that have uncertain ownership. For example, I wound up as a maintainer of the IOR and mdtest benchmark because I was the last one to touch it when its previous maintainer lost interest/funding. I don't even work in I/O performance anymore, but the community still uses this benchmark in virtually every procurement of parallel file systems either directly or through IO500. It would be wonderful if such an important tool didn't rest on my shoulders and had a more concrete governance structure given how important it is.Quantum computingBesides AI and cloud, quantum computing was cited in Kathy Yelick's opening keynote as the third disruptor to HPC for scientific computing. At the time, I thought citing quantum was just an obligation of any opening keynote speaker, but quantum computing was particularly high-profile at ISC this year. I was surprised to see over a dozen quantum computing companies on the vendor exhibition floor, many of whom were Europe-based startups.In addition, this year's Hans Meuer award (for best research paper) was given to a paper on quantum computing by Camps et al. This is particularly notable since this is the first time that the Meuer award has ever been given to a paper on a topic that isn't some hardcore traditional HPC like MPI or OpenMP advancements; by comparison, this award has never been given to any papers on AI topics. Granted, the winning paper was specifically about how to use conventional HPC to solve quantum problems, but this recognition of research in quantum computing makes a powerful statement: quantum computing research is high-performance computing research.Reinvent HPC to include urgent computing?I was invited to give a lightning talk at the Workshop on Interactive and Urgent High-Performance Computing on Thursday, and urgent/interactive HPC is not something I'd really paid attention to in the past. So as not to sound like an ignorant fool going into that workshop, I opted to sit in on a focus session titled \"Urgent Computing\" on Tuesday. I had two goals:Make sure I understood the HPC problems that fall under urgent and interactive computing so I could hold an intelligent conversation on this topic at the Thursday workshop, andSee if there are any opportunities for cloud HPC to provide unique value to the challenges faced by folks working in urgent HPCI'll describe what I came away with through these lenses.The Urgent Computing focus sessionWhat I learned from the focus session is that urgent computing is not a very well-defined set of application areas and challenges. Rather, it's another manifestation of reinventing HPC to include any kind of computation for scientific purposes.Much to my surprise, this \"Urgent Computing\" focus session was actually a session on IoT and edge computing for science. Several speakers spoke about getting data from edge sensors on drones or telephone poles into some centralized location for lightweight data analysis, and the \"urgent\" part of the problem came from the hypothetical use cases of analyzing this sensor data to respond to natural disasters. There wasn't much mention of anything requiring HPC-like computing resources; at best, a few talks made unclear references to using AI models for data analysis, but it felt like grasping:The above conclusion slide was presented by one of the speakers, and to be honest, I don't understand what any of it means. Granted, I know very little about urgent computing, IoT, or edge computing so there may be some domain jargon here that's throwing me off. But based on this, as someone working in the area of HPC and AI in the cloud, I don't think I have a role to play here. I'm sure cloud computing can help, but the challenges would be in general-purpose cloud rather than HPC.The Interactive and Urgent HPC workshopFortunately for me, the Thursday workshop on Interactive and Urgent HPC was much less about edge/IoT and more about developing software infrastructure and workflows that allow scientific data analysis of large datasets to happen before the results become obsolete. It was a fascinating workshop for learning about specific science drivers that require fast access to HPC resources, and how different HPC providers are enabling that through non-traditional services and policies. Below are a few highlights.Sam Welborn (NERSC) presented his team's efforts to convert a streaming data workflow from its current file-based approach into one that streamed directly into compute node memory. The specific use case was the initial data processing for image information coming off of a scanning transmission electron microscope at 480 Gbps, totaling 750 GB per shot. As he described it, the current technique involves streaming those data to files at the microscope, then copying those files to the parallel file system of a remote supercomputer, then reading, processing, and writing that data within the HPC environment to prepare it for downstream analysis tasks. And for what it's worth, this is how I've always seen \"streaming\" HPC workflows actually work; they're actually using file transfers, and the performance of both the file system at the source and destination are in the critical path.The problem with this approach is that parallel file systems on HPC systems tend to be super flaky, and there's no real reason to bounce data through a storage system if you're just going to pick it up and process it. So, Dr. Welborn showed a true streaming workflow that skipped this file step and used ZeroMQ push sockets at the microscope and pull sockets on the HPC compute nodes to do a direct memory-to-memory transfer:Seeing software like ZeroMQ used to enable communication in an HPC environment instead of forcing this workflow to fit into the MPI paradigm is an encouraging sign in my eyes. ZeroMQ, despite not using purpose-built HPC technology like RDMA, is the right tool for this sort of job since it supports much better resilience characteristics than messaging libraries designed for tightly coupled HPC jobs. Workflows like this that combine beefy GPU nodes with software developed in the commercial tech space suggest that the world of HPC is willing to abandon not-invented-here ideology.It wasn't clear to me that there's a great opportunity for cloud HPC to be uniquely useful in use cases like this; while you certainly can provision beefy CPU and GPU nodes with InfiniBand in Azure, cloud services can't obviously simplify this ZeroMQ-based workflow beyond just supplying general-purpose VMs on which the orchestration services can run. Had this team stuck with a file-based streaming mechanism, the performance SLAs on cloud storage (like object or ephemeral Lustre) would provide a more reliable experience to ensure the data transfer happened in near-real-time. But the better solution to unpredictable file system performance is to do exactly what was done here: skip the file system entirely.Just to keep the speaker honest, I asked why this computation couldn't simply be done at the same place as the telescope generating the data. After all, if the telescope always generates 750 GB per shot, you should be able to buy a couple GPU servers that are ideally sized to process that exact workload in the time between images. There were actually two answers: one from Sam and one from an audience member:Sam said that you can process this workflow locally, but that the goal of this work was to prepare for a future microscope (or another instrument) that could not. He also insightfully pointed out that there's tremendous value in getting the data into the HPC environment because of all the services that can be used to work on that data later. I envisioned doing things like using a Jupyter notebook to further process the data, serve it up through a web UI, and similar tasks that cannot be done if the data is stuck inside a microscope room.An audience member also pointed out that sticking GPU nodes in the same room as electron microscopes can result in enough noise and vibration to disrupt the actual scope. This was a great point! In the days before I started working in HPC, I was training to become an electron microscopist, and I worked in a lab where we had water-cooled walls to avoid the problems that would be caused by air conditioning breezes. There's no way a loud server would've worked in there.Toshio Endo (Tokyo Tech) gave an interesting talk on how they enable urgent/interactive compute jobs on their batch-scheduled TSUBAME4.0 supercomputer by doing, frankly, unnatural things. Rather than holding aside some nodes for interactive use as is common practice, his work found that a lot of user jobs do not completely use all resources on each compute node they reserve:I had to do a double-take when I saw this: even though 65%-80% of the nodes on the supercomputer were allocated to user jobs, less than 7% of the GPUs were actually being utilized.Dr. Endo's hypothesis was that if nodes were suitably subdivided and jobs were allowed to oversubscribe CPUs, GPUs, and memory on a compute node without impacting performance too much, they could deliver real-time access to HPC resources without having to create a separate pool of nodes only for interactive uses. He defined success as the slowdown of a shared job being 1/k if k jobs shared the same node; for example, if four jobs were all running on the same node, each one taking four times as long to complete would be acceptable, but any longer would not. He then went on to show that the best way to accomplish this is using Slurm's gang scheduling, where each job takes turns having exclusive access to all the CPUs and GPUs on a node. The alternative (just letting the OS context switch) was no good.While a fascinating study in how to provide zero wait time to jobs in exchange for reduced performance, this whole mechanism of using gang scheduling to exploit low resource utilization seems like jamming a square peg into a round hole. If a workload doesn't (or can't) use all the GPUs on a node, then that's not the right node for the job; I feel like a more appealing solution would simply be to offer a heterogeneous mix of nodes based on the demands of the workload mix. This is hard to do if you're buying monolithic supercomputers since you're stuck with whatever node mix you've got for five years, but there is another way to buy supercomputers!I won't pretend like dynamically provisioning different flavors of CPU- and GPU-based nodes interconnected with InfiniBand in the cloud doesn't come with a cost; the convenience of being able to slosh a cluster makeup between CPU-heavy and GPU-heavy nodes will be more expensive than committing to use the same makeup of node flavors for multiple years. But if you're paying for GPUs that are only being used 7% of the time, surely it's cheaper to pay a higher cost for GPUs when you need them if it also allows you to not pay for them 93% of the time when they're idle.Bjoern Enders (NERSC) gave the first lightning talk where he presented the exploration they're making into enabling real-time and urgent computation. They're currently going in three parallel directions to provide this capability:Reservations, a process by which a user can request a specific number of nodes for a specific period of time, and Slurm ensures that many nodes are available for the exclusive use of that user by the time the reservation starts. He said that implementing this at NERSC is costly and rigid because it requires a human administrator to perform manual steps to register the reservation with Slurm. Realtime queues, where a few nodes are held from the regular batch queue and only special real-time users can submit jobs to them. Dr. Enders said that NERSC is extremely selective about who can access this queue for obvious reasons: if too many people use it, it will back up just like the regular batch queues do.Jupyter Hub, which utilizes job preemption and backfill under the hood. If a user requests a Jupyter job, Slurm will pre-empt a job that was submitted to a preemptible queue to satisfy the Jupyter request. However, if there are no preemptible jobs running, the Jupyter job will fail to launch after waiting for ten minutes.To provide compute resources to back up these scheduling capabilities, they also deployed a new set of compute nodes that can be dynamically attached to different supercomputers they have to support urgent workloads even during downtimes.  Called \"Perlmutter on Demand\" (POD), it sounded like a separate set of Cray EX racks that can be assigned to either the Perlmutter supercomputer, or if Perlmutter is down for maintenance, either their smaller Alvarez or Muller supercomputers which share the same Cray EX architecture. What wasn't clear to me is how the Slingshot fabrics of these nodes interact; perhaps POD has its own fabric, and only the control plane owning those racks are what changes.He showed a slide of explorations they're doing with this POD infrastructure, but as with Dr. Endo's talk, this seemed a bit like a square peg in a round hole:All of this sounds aligned with the strengths of what HPC in a cloud environment can deliver, and some of the big challenges (like figuring out the ideal node count to reserve for interactive jobs) are problems specific to Slurm and its mechanism for scheduling. There's a lot more flexibility to rapidly provision HPC resources in cloud environments because, unlike the case where Slurm is scheduling jobs on a single cluster, cloud resource managers can schedule across any number of clusters independently. For example, if an urgent workload needing only four GPU nodes suddenly appears, it doesn't necessarily have to be scheduled on the same InfiniBand fabric that a large hero job is running on. Since the urgent job and the hero job don't need to talk to each other, cloud resource managers can go find a GPU cluster with a little more flex in them to provision those resources quickly.Automating the process of reservations is also a bit of a game of catch-up, though my guess is that this is more a matter of someone having a weekend to sit down and write the REST service that manages incoming reservation requests. Although there's not a direct analog for reservations like this in Azure, AWS has a feature called AWS Capacity Blocks that does exactly this: if you know you'll want a certain number of GPU nodes sometime in the future, Capacity Blocks let you reserve them ahead of time through an API.Finally, I represented Microsoft and gave a lightning talk that riffed on a lot of what I've been writing about in this blog post: HPC seems to be reinventing a lot of things that the cloud has already figured out how to do. The illustrious Nick Brown was kind enough to snap a photo of one of my slides and post it on Twitter:My thesis was that the way urgent HPC workflows are triggered, scheduled, run, and reported on follows the same pattern that inferencing-as-a-service services (like Copilot and ChatGPT) are implemented under the hood, right down to executing multi-node jobs on InfiniBand clusters. The difference is that these cloud workflows are built on the foundation of really nice cloud services that provide security, scalability, monitoring, and hands-free management that were originally developed for commercial (not HPC!) customers. My argument was that, even if you don't want to pay cloud providers to run urgent HPC workflows as a managed service, you can use these services (and the software infrastructure on which they're built) as a blueprint for how to build these capabilities in your own HPC environments.Concluding thoughtsThe ISC'24 conference was fantastic, and I am glad it has not lost the unique elements that made me want to attend in the years prior to the pandemic. It's still that smaller, intimate, and focused HPC conference that brings the community together. Although a lot of my synopsis above may sound critical of the content presented over the four days I attended, the fact that I've had so much to write down in this blog post is a testament to the value I really get out of attending: it makes me sit down and think critically about the way the HPC community is evolving, what the leading minds in the field are thinking, and where I might be able to contribute the most in the coming year.I never much paid attention to the annual taglines of conferences like ISC, but this year's \"Reinvent HPC\" really resonated. The HPC community is at a crossroads. Exascale computing for science is now in the rear-view mirror, and large-scale AI is all the rage across the computing industry at large. But for the first time ever, this new direction in at-scale computing is happening without the inclusion of the people and organizations who've historically driven innovation in HPC. Whereas institutions like Oak Ridge, RIKEN, Cray, and Fujitsu defined the future of computing for decades, hundred-person startups like OpenAI and Anthropic are now paving the way in partnership with companies like Microsoft and Amazon.HPC needs to be reinvented, if for no other reason than to decide whether the HPC community wants to be inclusive of new frontiers in computing that they do not lead. Does the HPC community want AI to be considered a part of HPC?Judging from many speakers and panelists, the answer may be \"no.\" To many, it sounded like AI is just another industry that's sucking all the air (and GPUs) out of the room; it's a distraction that is pulling funding and public interest away from solving real problems. It's not something worth understanding, it's not something that uses the familiar tools and libraries, and it's not the product of decades of steady, government-funded improvements. AI is \"them\" and HPC is \"us.\"Personally, I'd like the answer to be \"yes\" though. Now that I'm on the other side of the table, supporting AI for a cloud provider, I can say that the technical challenges I face at Microsoft are the same technical challenges I faced in the DOE. The desire to deeply understand systems, optimize applications, and put world-class computing infrastructure in the hands of people who do amazing things is the same. And as the days go by, many of the faces I see are the same; instead of wearing DOE or Cray badges, my lifelong colleagues are now wearing NVIDIA or Microsoft badges.All this applies equally to whether cloud is HPC or not. The HPC community needs to reinvent itself to be inclusive of everyone working towards solving the same problems of computing at scale. Stop talking about people who work on commercial AI in cloud-based supercomputers as if they aren't in the room. They are in the room. Often near the front row, snapping photos, and angrily posting commentary on Twitter about how you're getting it all wrong.HPC has historically been used to solve scientific problems, whether to expand our understanding of the university, to find the next best place to drill an oil well, or to model the safety of aging nuclear weapons. The fact that HPC is now being used to solve squishier problems related to natural language or image generation does not change the essence of HPC. And whether that HPC is delivered through physical nodes and networks or virtualized nodes and networks is irrelevant, as long as those resources are still delivering high performance. AI is just as much HPC as scientific computing is, and cloud is just as much HPC as OLCF, R-CCS, or CSCS is.So perhaps HPC doesn't need to be reinvented as much as the mindset of its community does.That all said, I am genuinely impressed by how quickly ISC'24 has been reinventing itself in recent years. It wasn't too long ago that all its keynote speakers were greybeards from a predictable pool of public HPC centers all saying the same things year after year. It's wonderful to see a greater diversity of perspectives on the main stage and torches passing on to the next generation of leading figures in the field. And it was not lost on me that, for the first time in the history of this conference, Thomas Sterling did not deliver the closing keynote. As much fun as I had poking fun at his meandering and often-off-the-mark conjectures every year, it was delightful to be exposed to something new this year.I'm hopeful that ISC will continue to get better year over year, and ISC'25 will feel more inclusive of me despite the fact that I am now one of those hyperscale cloud AI people. So long as I still feel like it's my community, though, I will keep showing up in Germany every summer.",
            "content_html": "<p>I had the great pleasure of attending the ISC High Performance conference this month, marking the fifth time I've attended what has become one of my top must-attend industry conferences of the year. This year was particularly meaningful to me because it is the first time that:</p><p></p><ol style=\"text-align: left;\"><li>I attended ISC as a Microsoft employee. This is also the first time I've attended any HPC conference since I changed my focus from storage into AI infrastructure.</li><li>I attended ISC in-person since before the pandemic. It's also the first time I've visited Hamburg which turned out to be an absolute delight.</li></ol><p></p><p>Although registrations have been lower since the pandemic, this year's final registration count was over 3,400 attendees, and there was no shortage of old and new colleagues to bump into walking between the sessions at the beautiful Congress Center Hamburg.</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p><br />&lt;p&gt;This year’s theme was “Reinvent HPC,” and that idea—that HPC needs to reinvent itself—was pervasive throughout the program. The whole industry had been pulling towards exascale for the better part of a decade, and now that there are two exaflop systems on Top500 and the dust is settling, it feels like everyone is struggling to figure out what’s next. Is it quantum? AI?&lt;/p&gt;</p><p>It was difficult for me to draw a line through all the topics worth reviewing at this year's ISC, as it was a very dense four days packed with a variety of topics, discussions, vendors, and events. I only experienced a fraction of everything there was to be seen since so many interesting sessions overlapped, but I thought it might be worthwhile to share my perspective of the conference and encourage others to do the same.<span></span></p><p></p><div id=\"toc\"><h2>Table of Contents</h2><ul><li><a href=\"http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#section1\">Reinventing HPC (and blast those hyperscalers!)</a><ul><li><a href=\"http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#section11\">Kathy Yelick's opening keynote</a></li><li><a href=\"http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#section12\">Closing keynotes on the future</a></li></ul></li><li><a href=\"http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#section2\">Top500 and Aurora</a><ul><li><a href=\"http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#section21\">#1 - Frontier</a></li><li><a href=\"http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#section22\">#2 - Aurora</a></li><li><a href=\"http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#section23\">#3 - Eagle</a></li><li><a href=\"http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#section24\">Other notable tidbits</a></li></ul></li><li><a href=\"http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#section3\">Everyone is an AI expert!</a><ul><li><a href=\"http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#section31\">The Exascale AI Synergies LLM Workflows BOF</a></li><li><a href=\"http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#section32\">AI Systems for Science and Zettascale</a></li><li><a href=\"http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#section33\">Real applications of generative AI for science</a></li></ul></li><li><a href=\"http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#section4\">High Performance Software Foundation</a></li><li><a href=\"http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#section5\">Quantum computing</a></li><li><a href=\"http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#section6\">Reinvent HPC to include urgent computing?</a><ul><li><a href=\"http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#section61\">The Urgent Computing focus session</a></li><li><a href=\"http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#section62\">The Interactive and Urgent HPC workshop</a></li></ul></li><li><a href=\"http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#section7\">Concluding thoughts</a></li></ul></div><h2 id=\"section1\">Reinventing HPC (and blast those hyperscalers!)</h2><p>The need to reinvent HPC was the prevailing theme of the conference from the very first session; with the listing of Aurora as the second system on Top500 to break the 1 exaflops barrier, the community is in search of a new milestone to drive research (and funding!). At the same time, commercial AI has rapidly risen up largely in an independent, parallel effort with a speed and scale that begs the question: how important was the decade-long drive to break the exaflops barrier if the AI industry could catch up so quickly without the help of the institutions that have historically posted the top HPL scores? If the commercial AI industry overtakes scientific computing as the world leader in deploying at scale, how can “HPC” be reinvented so it can continue to claim leadership in another dimension?</p><h3 id=\"section11\">Kathy Yelick's opening keynote</h3><p>ISC’s opening keynote was given by Kathy Yelick, where she provided commentary on two recent government-commissioned reports on the future of HPC:</p><p></p><ol style=\"text-align: left;\"><li><a href=\"https://nap.nationalacademies.org/catalog/26916/charting-a-path-in-a-shifting-technical-and-geopolitical-landscape\">Charting a Path in a Shifting Technical and Geopolitical Landscape: Post-Exascale Computing for the National Nuclear Security Administration</a>, commissioned by the National Academies</li><li><a href=\"https://www.osti.gov/biblio/1989107\">Can the United States Maintain Its Leadership in High-Performance Computing?</a>, commissioned by the US Department of Energy’s Advanced Scientific Computing Research program</li></ol><p style=\"text-align: left;\">Living up to her reputation, Dr. Yelick’s talk was fast and insightful, describing the insatiable demand for computing driven by scientific research, the struggle to expose continuing amounts of parallelism to make use of newer processors, and some promising directions to address that disconnect. However, her talk started in a direction that I didn’t like when she went into describing the disruptors that necessitate reinventing HPC:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p style=\"text-align: left;\">The above slide implied that AI, quantum, or cloud may pose an existential threat to the HPC community gathered at ISC this year; this immediately raised my hackles, as it cast the relationship between “HPC” and “AI”/“cloud” as having some sort of adversarial tension. As the talk went on, I realized that “HPC” didn’t really mean “high-performance computing” to her. Rather, it was used to refer to something much more narrowly scoped—high-performance computing <i>to solve scientific problems</i>. Slide after slide, the presentation kept doubling down on this idea that “HPC” as the audience knows it is being threatened. For example, Yelick talked through this slide:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p style=\"text-align: left;\">The picture she painted is that “HPC” (denoted by companies with blue bars) no longer has influence over technology providers because the “hyperscalers” (green bars) have such an outsized amount of investment. She then used this to call on the audience to think about ways “we” could influence “them” to produce technologies that are useful for both scientific computing and low-precision AI workloads.</p><p style=\"text-align: left;\">Her talk culminated in this slide:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p style=\"text-align: left;\">Which was accompanied by this conclusion:</p><p style=\"text-align: left;\"></p><blockquote>\"So what’s a post-exascale strategic for the scientific community? It's the beat 'em or join 'em strategy. The beat 'em strategy says we’re going to design our own processors. [...] The join 'em strategy says let's leverage the AI hardware that's out there. [...] The sort of sneaky way of doing this is getting embedded in the AI community and trying to convince them that in order to make AI better for commercial AI applications, you really want to have certain features. Like don't throw away your 64-bit arithmetic and things like that.\"</blockquote><p></p><p>I found myself getting increasingly unsettled through the keynote, because this \"us versus them\" mentality put me, a long-standing member of this HPC community, in the camp of \"them.\" It was as if I was suddenly an outsider in a conference that I've been attending for years just because I no longer work for an organization that has been doing HPC since the early days of computing. Even though the clusters I support use the same NVIDIA and AMD GPUs, the same InfiniBand fabrics, and the same Lustre file systems that \"HPC\" uses, I am no longer in \"HPC\" because I am \"hyperscale\" or \"cloud\" or \"AI.\"</p><p>The underlying message is one I get; GPUs are trending in a direction that favors massive gains in lower-precision computation over FP64 performance. And the cost of HBM is driving the overall value (in FP64 FLOPS per dollar) of accelerators backwards for the first time in the history of scientific computing. But the thesis that the scientific computing community needs to be sneaky to influence the hyperscale or AI players seemed way off the mark to me. What seemed absent was the recognition that many of the \"hyperscalers\" are her former coworkers and remain her colleagues, and \"they\" sit in the same audiences at the same conferences and share the same stages as the \"HPC\" community. All that is true because \"HPC\" is not somehow different than \"cloud\" or \"AI\" or \"hyperscale.\" If there really is a desire to influence the hyperscale and AI industry, the first step should be to internalize that there is no \"us\" and \"them.\"</p><h3 id=\"section12\">Closing keynotes on the future</h3><p>Just as the conference was opened with a talk about this \"us versus them\" mentality, it was closed with a talk about \"us versus them\" in a keynote session titled, \"Reinventing HPC with Specialized Architectures and New Applications Workflows\" which had two speakers followed by Q&amp;A.</p><h4>Chiplets for modular HPC</h4><p>John Shalf gave one half of the closing keynote, where he gave his usual rally for investments in chiplets and specialized processors for HPC:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p style=\"text-align: left;\">He gives a variant of this talk at every ISC, but this year he lasered in on this notion that the \"HPC\" community needs to do what the \"hyperscalers\" do and use chiplets to develop custom ASICs. It was an energetic and impassioned talk, but this notion that hyperscalers are already executing on his idea for the future sounded a little funny to me seeing as how I now work for one of these hyperscalers and his message didn't resonate.</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p style=\"text-align: left;\">If you really follow the money, as Shalf suggested, a huge amount of it is flowing into GPUs, not specialized processors. It wasn't clear to me what specialization he was thinking of when he referred to custom silicon being developed by the likes of Meta, Google, AWS, and Microsoft; it's true that these companies are developing their own silicon, but those efforts are largely addressing cost, risk, and supply, not improving performance beyond more general-purpose silicon like GPUs. And it turns out that a significant fraction of the (non-US) HPC community is already developing custom silicon for the same reasons as the hyperscalers; Japan, China, and Europe are all developing their own indigenous processors or accelerators for scientific computing at leadership scales. In that sense, Shalf was preaching to the choir given that, on the international stage, his government is the odd one out of the custom silicon game.</p><p style=\"text-align: left;\">He also suggested a dichotomy where the HPC community would either have to just (1) make every scientific problem an AI problem or (2) join this journey towards making domain-specific accelerators, ignoring the significant, unexplored runway offered by using mixed precision arithmetic in scientific applications. He called for partnering with hyperscalers, but his examples of implementing a RISC-V-based stencil accelerator and a SambaNova-based DFT processor didn't draw a clear line to the core missions of the large hyperscalers he extolled. He briefly said that partnering would benefit hyperscalers by addressing some capital cost challenges, but seeing as how the annual capital expenditures of the hyperscalers outstrips those of the US national HPC effort by orders of magnitude, I couldn't understand what the hyperscalers would stand to gain by partnering in this way.</p><h4 style=\"text-align: left;\">Integrating HPC, AI, and workflows</h4><p style=\"text-align: left;\">Rosa Badia gave the second half of the closing keynote where she proposed ideas around complex scientific workflows and the novel requirements to support them. This talk felt a lot more familiar, as the focus was squarely on solving scientific computing challenges by connecting traditional HPC resources together in nontraditional ways using software whose focus goes beyond cranking out floating point arithmetic.</p><p style=\"text-align: left;\">As she spoke, I couldn't help but see parallels between the challenges she presented and the sort of technologies we live and breathe every day in cloud services.  For example, she showed this slide:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p style=\"text-align: left;\">Dr. Badia obviously wanted to make a cloud-tie in by calling this \"HPC Workflows as a Service,\" but what I'm not sure she realized is that this model almost exactly describes platform-as-a-service frameworks that already exist in commercial clouds. For example,</p><p style=\"text-align: left;\"></p><ul style=\"text-align: left;\"><li>What she calls a \"Data Catalog\" is a public or private object storage account (a blob container, an S3 bucket) or a PaaS abstraction built atop them</li><li>What she calls a \"Software Catalog\" is a container registry (Azure Container Registry, Amazon Elastic Container Registry) or an abstraction built atop them</li><li>A \"Workflow Description\" is something like an <a href=\"https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-component-pipeline-python\">AzureML pipeline</a> or <a href=\"https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_model_building_pipeline.html\">SageMaker pipeline</a></li><li>A \"Workflow Registry\" is just a Github repository containing pipelines</li><li>The \"Portal\" is the web UI provided by AzureML or SageMaker</li></ul><p></p><p style=\"text-align: left;\">I don't think there's anything truly new here; the challenges she described lie in wedging these workflows into HPC infrastructure which lacks the platform features like robust identity and access management (i.e., something better than LDAP that supports more modern authentication and authorization flows and finer-grained access controls) and data management (i.e., something better than a parallel file system that depends on POSIX users, groups, and permissions and implicit trust of clients).</p><p style=\"text-align: left;\">She went on to describe a workflow data management system that reinvented a bunch of infrastructure that is already baked into commercial cloud object stores like Azure Blob and AWS S3:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p style=\"text-align: left;\">As she was describing the requirements for such a workflow data management layer, it struck me that what the scientific data community calls \"<a href=\"https://en.wikipedia.org/wiki/FAIR_data\">FAIR principles</a>\" are the same basic requirements for operating in commercial environments where data may be subject to strict privacy and compliance regulations. The notion of findable data may be aspirational for scientific datasets, but when a company is having to find datasets because it's being sued or subpoenaed, findability is a bare-minimum requirement for any data management system. Similarly, tracking the provenance of data may be a nice-to-have for scientific data, but it is a hard requirement when establishing a secure software supply chain. Cloud storage systems solved many of these challenges a long time ago, and I can't help but wonder if this idea that workflows in HPC pose a new set of challenges is another manifestation of \"us\" not realizing \"they\" might have done something useful and applicable for science.</p><p style=\"text-align: left;\">Badia's final slide had a particularly poignant statement which read, \"Systems can only be justified if we have applications that need them.\" I think she was trying to call for more investment in application development to exploit new systems, but I think the inverse is also true. If modern scientific applications truly require more complex orchestration of compute and data, maybe the scientific computing community should stop building computing platforms that make it really difficult to integrate different systems.</p><p style=\"text-align: left;\">Again, \"HPC\" is not the opposite of \"cloud;\" it's not an either/or decision. There are technologies and tools that were designed from the beginning to simplify the secure connection of services and resources; they just weren't invented by the HPC community.</p><h2 id=\"section2\">Top500 and Aurora</h2><p>One of the cornerstones of ISC is the semiannual release of the Top500 list, and unlike at SC, the Top500 announcements and awards do not overlap with any other sessions, so it tends to have a higher profile and draw all attendees. This go-around, there were no dramatic changes in the Top 10; the new Alps system at CSCS was the only new entry, and the order of the top five systems remained the same. Notably, though, Aurora posted a significantly higher score than at SC'23 and broke through the exaflops barrier using 87% of the system, cementing its place as the second exascale system listed. But let's start at the top.</p><h3 id=\"section21\">#1 - Frontier</h3><p>Frontier at Oak Ridge remained #1, but it squeezed twelve more petaflops out of the same node count and is now just over 1.2 EF. Nothing groundbreaking, but it's clear evidence that ORNL is continuing to tune the performance of Frontier at full system scale.</p><h3 id=\"section22\">#2 - Aurora</h3><p>Aurora, on the other hand, finally eked over the exaflops line with 1.012 EF using 87% of the system's total 63,744 GPUs. Rick Stevens gave a short talk about the achievement which is summed up on this slide:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>I was a little surprised by how honest Stevens was in this talk; the typical game that is played is that you stand up on stage, talk about how great of a partnership you had with your partners to realize this achievement, extol the virtues of the technologies on which your system was built, and talk about how this HPL score is just the start of a lot of great science.</p><p>Stevens didn't do that though.</p><p>He started out by telling the conference that Intel had bad product names, then explained that their low Graph500 and HPCG scores were the result of their exclusive focus on breaking the exaflops barrier with HPL, implying they didn't have time or ability to run Graph500 or HPCG at the same 87%-89% scale as their HPL and HPL-MxP runs. Based on this, it sounds like Aurora is still a ways away from being stable at scale, and we're unlikely to see any Gordon Bell-nominated papers at SC'24 this November.</p><p>After this session, folks seemed to relish in <a href=\"https://x.com/hpc_guru/status/1792018127464874176?s=61\">dunking on Aurora</a>; its <a href=\"https://x.com/hpc_guru/status/1790248333120000500?s=61\">window to be #1 is likely to have closed</a> and it has <a href=\"https://x.com/hpc_guru/status/1790273734730985865?s=61\">some power efficiency issues</a>. But I don't think anyone involved with the Aurora project needs to be told that; if what Stevens implied is true, the folks at ALCF, Intel, and HPE have been struggling for a long time now, and topping out over 10<sup>18</sup> was a hard-sought, major milestone to be celebrated. The Aurora project has been thrown more curveballs than I would have ever guessed a single HPC project could have, so all parties deserve credit for sticking it through all this way rather than just walking away. With any luck, Aurora will stabilize in the next six months, and we'll see full-scale runs of Top500, Graph500, HPCG, and science apps by November.</p><h3 id=\"section23\">#3 - Eagle</h3><p style=\"text-align: left;\">The third highest system on the list was Eagle, whose HPL score was not updated since the system was first listed at SC'23 last year. Through a few twists of fate, I wound up being the person who accepted the award on-stage, and I now have a Top500 award for the #3 system sitting in my home office. Here's a photo of me goofing around with it:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p style=\"text-align: left;\">It's not entirely inappropriate that I was the one to accept it since my teammates are the ones carrying pagers for the on-call rotation of that system, and we were also the hands-on-keyboard when that HPL run was conducted. Still, it was a bit surreal to walk on-stage to pick up such a noteworthy award immediately following two actually important people (both of whom have \"director\" in their titles) accepting the same award. By comparison, most of my career highlights to date have been just trolling HPC people on Twitter (as the esteemed Horst Simon actually said out loud as I was leaving the stage!)</p><p style=\"text-align: left;\">It was weird.</p><p style=\"text-align: left;\">That said, I take this to mean that it is now my duty to be the friendly face from Microsoft who can speak intelligently about the #3 system on Top500. To that end, I'll answer some questions that I was asked at ISC about the system and Azure HPC clusters in general below. <i>None of this is new or secret information!</i></p><p style=\"text-align: left;\"></p><ul style=\"text-align: left;\"><li><b>Why didn't you run HPL again and post a higher score to beat Aurora?</b> Because the day after that HPL run completed, that system was put into production. Once systems are in production, people are paying to use them, and taking a time-out to re-run HPL costs a ton of money in either real dollars (if a customer runs it) or lost revenue (if the HPL run is blocking customer workloads). This is quite different from public-sector HPC systems which never have to pay for themselves.</li><li><b>Can I get access to Eagle for a Gordon Bell run or to test software?</b> That's not really how it works. Whereas a traditional supercomputer might allow users to ssh in and submit jobs to a Slurm queue, cloud-based supercomputers allow users to deploy virtual machines through a REST API. Those virtual machines can allow ssh, run Slurm, and support MPI jobs like HPL, but that OS environment is managed by Azure users, not Azure itself. You can get a taste for what's required to run a basic MPI job by reading some instructions I wrote on <a href=\"https://www.glennklockwood.com/cloud/mpi-cluster.html\">provisioning an MPI cluster on Azure</a>.</li><li><b>Is it just a bunch of GPU nodes scattered around a bunch of data centers?</b> No, all the nodes on any given Azure HPC cluster (like Eagle) share an InfiniBand fabric. There are countless InfiniBand clusters in Azure, but each one is a real supercomputer by any definition of a supercomputer, and they are designed to run tightly coupled job across all their GPUs.</li><li><b>What parallel file system does it use?</b> Don't think about it that way. You can provision a Lustre file system and mount that to any or all cluster nodes if you want to, or you can access data directly from object storage.</li><li><b>Are there any photos of it?</b> You can see a photo of one of the Microsoft-designed nodes that comprise the system on my <a href=\"https://blog.glennklockwood.com/2023/11/sc23-recap.html\">SC'23 recap blog post</a>. Beyond that, there's not much to look at because Azure HPC clusters are not meant to be photogenic like, say, Cray supercomputers. There's no rack graphics (or even rack doors!). It's just tons and tons of air-cooled racks with InfiniBand optics coming out of each one. Maybe the only unique thing is that the racks are painted white instead of the typical black. Not sure why.</li></ul><div>Getting back to that false separation between \"HPC\" and \"cloud,\" Eagle is strong evidence that they aren't different. What the \"hyperscalers\" do is not that different from what traditional HPC centers do. Perhaps the biggest difference is that cloud supercomputers get all the benefits of cloud infrastructure like software-defined infrastructure like virtual machines and virtual networking, integration with identity and access management that transcends simple Linux UIDs/GIDs, and the flexibility to integrate with whatever storage systems or ancillary services you want from any compute node.</div><p></p><h3 id=\"section24\">Other notable tidbits</h3><p>It is tradition for Erich Strohmaier to talk through some highlights and trends of the latest Top500 list every time a new one is announced, and in the past, <a href=\"https://x.com/glennklockwood/status/1140637729182683136?s=61\">I've been critical</a> of how he's presented conclusions from the list with this implicit assumption that computers that never post to Top500 simply don't exist. This year felt different, because Dr. Strohmaier made the explicit statement that China has completely stopped submitting to Top500. Their exascale systems aren't listed, but neither are any new systems in the past three years at the bottom. They simply don't play the game anymore, making it undeniable that Top500 is no longer an authoritative list.</p><p>Just as the whole conference's theme was reinventing HPC, I felt a sense that even the most stalwart proponents of Top500 are now recognizing the need to reinvent the Top500 list. Kathy Yelick said as much during her keynote (\"Shall we replace Top500? What are the metrics in post-exascale computing that are important?\"), and Erich implored the audience to help expand the <a href=\"https://hpl-mxp.org/\">HPL-MxP</a> (formerly HPL-AI; an HPL-like benchmark that can use the mixed-precision capabilities of tensor cores) list. Nobody seems to know how to quantify what makes a leadership supercomputer nowadays, but accepting that HPL scores (or appearing on the Top500 list!) won't cut it is a good first step.</p><p>That all said, Top500 is still a valuable way to track technology trends in the industry. For example, this edition of the list where NVIDIA's new Grace-Hopper node started appearing in force. The only new entrant in the Top 10 was the <a href=\"https://www.top500.org/system/180259/\">270 PF GH200</a> component of <a href=\"https://www.cscs.ch/computers/alps\">CSCS's Alps system</a>, and HPEhad these EX254n GH200 blades on display on the show floor.</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>To HPE/Cray's credit, they seem to have gotten the system up and running with Slingshot without the delays that plagued early Cray EX systems like Frontier and Aurora. Hopefully this is a sign that the Cray EX platform and Slingshot-11 have graduated from being risky and not-quite-production-ready.</p><p>The other notable entrants on this year's Top500 are a trio of <a href=\"https://www.top500.org/system/180283/\">early MI300A APU-based Cray systems</a> being built around the El Capitan program at Lawrence Livermore National Laboratory. This is a positive sign that MI300A is up and running at modest scale, and HPE also had one of these EX255a blades on display at their booth:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>The strong showing of MI300A suggests that we may see El Capitan take the top spot in the next edition of the Top500 list coming in November.</p><h2 id=\"section3\">Everyone is an AI expert!</h2><p>Since I now work on a team responsible for AI infrastructure, I tried attending as many of the AI-focused talks and panels as I could this year. Unsurprisingly, these sessions largely carried the same undertones of \"reinventing HPC,\" and speakers opined on how AI would affect scientific computing and offered examples of what their institutions were doing to extend their leadership in the HPC space into the AI space. There was a fair amount of grasping going on (as there always is when AI is discussed at non-AI conferences), but this year I was struck by how confused so many speakers and attendees were about concepts related to applying AI.</p><p>To be clear: I am no expert in AI. However, my day job requires that I be steeped in some of the largest AI training workloads on the largest AI supercomputers on the planet, and I have to have a cursory understanding of the latest model architectures and techniques to anticipate how future system designs will have to evolve. It's from this perspective that I made the following observation: there are a lot of HPC people speaking very confidently about AI based on an outdated understanding of the state of the art. The AI industry generally moves much faster than the government-funded research community, and I couldn't help but wonder if some community leaders assumed that the AI industry today is the same as it was the last time they wrote their AI grant proposal.</p><p>Of course, there were also some really insightful perspectives on AI for science shared as well. Let's talk through some examples of both.</p><h3 id=\"section31\">The Exascale AI Synergies LLM Workflows BOF</h3><p>This realization that the ISC community is not keeping up with the AI community first slapped me in the face when I ducked into a BOF session titled, \"<a href=\"https://isc.app.swapcard.com/event/isc-high-performance-2024/planning/UGxhbm5pbmdfMTgyNjgxMQ==\">Tales of Exascales – AI and HPC Supercomputing Platforms Synergies for Large Language Models (LLMs) and Scientific Workflows</a>.\" I sometimes wonder if the organizers who propose titles like that are intentionally creating word salad, but in this case, it was apt session name; the discourse around HPC and AI was all over the board throughout the hour.</p><p>The session started on a strong, positive note by Simon McIntosh-Smith describing Bristol's new <a href=\"https://www.bristol.ac.uk/news/2023/september/isambard-ai.html\">Isambard-AI system</a>, a GH200-based Cray supercomputer funded under the broad charge of \"AI research.\" While I'm usually skeptical of such nebulously defined \"AI research\" machines, Dr. McIntosh-Smith's description of the project quickly checked a bunch of boxes on how a real AI research platform should be developed. In particular,</p><p><b>Isambard-AI was developed and deployed at the pace of AI rather than HPC for scientific computing</b>. Whereas government-funded, large-scale HPC systems typically take years to procure, Simon said that the first discussions started in August 2023, and in the nine months that followed, they had built the site, the team, and the system itself to the degree that <a href=\"https://www.top500.org/system/180257/\">a piece of the final system is already on Top500</a>. By comparison, LLNL's El Capitan supercomputer also debuted on Top500 this month, but <a href=\"https://www.energy.gov/articles/does-nnsa-signs-600-million-contract-build-its-first-exascale-supercomputer\">its contract was signed five years ago</a>, and its procurement began <a href=\"https://web.archive.org/web/20200605114639/https://asc.llnl.gov/coral-2-benchmarks/\">at least two years before that</a>. The AI industry would not exist if the systems it trains on took seven years to procure.</p><p><b>Isambard-AI deliberately avoided exotic AI accelerators to remain future-proof</b>. Simon rightly pointed out that the AI industry moves too quickly to anticipate whether a bespoke AI accelerator would even be relevant to whatever the hottest model architecture will be in a year. GPUs were chosen because they are the most flexible way to accelerate the widest range of AI workloads, regardless of if they are dense models, sparse models, inferencing, training, and whatever level of quantization makes sense. The reality is that cutting-edge research is done on GPUs, so aligning an AI supercomputer on the same technology will ensure that the algorithms developed by industry are immediately usable for scientific research.</p><p><b>A reasonable definition of \"AI for science\" was defined from the outset</b>. Rather than blurting out \"we need to research AI!\" and asking for a sack of money to buy GPUs, Simon outlined a vision of training AI models using data generated by physical simulation on a more conventional HPC system. Training models on models to create surrogate models is not particularly new, but it does establish a few reasonable architectural decisions such as having a robust data management and sharing platform, close coupling to the HPC system performing simulation, and aligning software stacks and programming environments as closely as possible.</p><p>Simon's contribution to the discussion stood out to me as the most impressive, and the discourse seemed to fall into a trap of familiarity following. Rather than focusing on the new and exciting prospects of AI, some panelists and audience members wanted to focus on the aspects of AI they understood. For example, an uncomfortable time was spent on a back-and-forth on how HPC centers can support Kubernetes and random I/O (which is what defines AI vs. HPC?) instead of Slurm and Lustre. If your biggest challenge in delivering infrastructure to support AI workloads is figuring out how to deploy both Kubernetes and Slurm, you haven’t even reached the starting line. This is a trivial issue in cloud environments, where entire AI clusters can be built up and torn down in minutes. Again, this is evidence that the scientific computing community isn’t ready to keep pace with the AI industry.</p><p>I jotted down a few of the questions and comments that I heard during this BOF that seem to reflect the level of familiarity the average ISC attendee has with AI:</p><p></p><ul style=\"text-align: left;\"><li><b>\"Would be nice if there were more models for science.\"</b> I wasn't sure sure what this means. All the leading LLMs are pretty good at \"science,\" and domain-specific models aren't readily transferable between different science domains or problems.</li><li>Scientific problems <b>\"have to validate outputs for correctness, unlike LLMs.\"</b> I think the speaker was making a sidelong reference to hallucinations, but like with any model (large language or physics-based), validating outputs for correctness is certainly necessary and readily possible.</li><li><b>\"The demands of inference of LLMs are completely different from those for training. How do you buy inference infrastructure?\"</b> I wonder where this notion came from. If your infrastructure can train a model, it can definitely inference that model. Cost-optimizing infrastructure for inferencing is a separate matter (you can cut corners for inferencing that you wouldn't want to cut for training), as is building the service infrastructure around inferencing to deliver inferencing as a service. But I don't think that's what this question was about.</li><li><b>\"Working safely with sensitive data / isolating workloads on big shared clusters.\"</b> This is a problem that arises only when you try to wedge AI workloads into infrastructure designed for traditional physics-based simulation. If you have sensitive data, don't use big shared clusters. Provision separate clusters for each security domain on a shared, zero-trust infrastructure.</li><li><b>\"How different are the files and filesystem access while training for LLMs, image generation models, reinforcement learning?\"</b> This question reflects a general misunderstanding of data and storage in HPC overall; how data is organized into files and how that data is accessed by a workload is an arbitrary decision made by the application developer. You can organize piles of text into one giant file or a million little files.</li></ul><p></p><p>There were a few questions that came up that touched on deeper issues on which the HPC community should reflect:</p><ul><li><b>\"What are the first steps for scientific groups wanting to get ready for using AI in the future?\"</b> This is probably the purest question raised in the entire session, and I think this is something the scientific computing community as a whole needs to figure out. What does \"using AI\" really mean for scientific groups? Is it training models? Fine-tuning models? Inferencing using pre-trained models on HPC infrastructure? Is it integrating simulation applications with separately managed inferencing services? Who manages those inferencing services? Does inferencing even require HPC resources, or can suitable models run on a few CPU cores? I think the first step to answering this question is ensuring that the scientific computing community reaches a common baseline level of understanding of \"using AI\" means. And a lot of that probably means ignoring what some self-professed AI experts in the HPC community claim is the future.</li><li><b>\"Care to predict what that ChatGPT moment will be for AI for Science? Had it already happened?\"</b> This question was addressed directly by panelist Séverine Habert who rightly pointed out that the ChatGPT moment occurred when a complex and esoteric topic was suddenly put in the hands of hundreds of millions of laypeople across the world. It was the moment that the common person walking on the street could suddenly interact with the most cutting-edge technology that had been previously understandable only to the headiest of researchers in industry and academia. That will likely never happen in AI for science because science, by definition, requires a higher baseline of education and understanding than the average layperson has.</li><li><b>\"How to effectively train the existing workforce when we are already struggling to retain talent in research/academia?\"</b> This question strikes at the same theme that Kathy Yelick's opening keynote confronted: what is the role of the scientific computing community now that it turns out that you don't need decades of institutional experience to deploy and use HPC resources at leadership scale? As offensive as it may sound, perhaps the public-sector HPC community should accept that their role is not training future researchers and academics, but training future practitioners of AI in industry. This is how the wider tech industry generally works; neither startups nor tech giants make hires assuming those people will still be around in ten years. Why does the public-sector HPC industry think otherwise?</li></ul><p>Finally, I was also struck but how fiercely the discourse clung to the idea that large language models are the answer to all AI problems in science. I get that this panel was focused on exascale, and LLM training is one of the rare cases where AI requires exascale computing capabilities. But there was no acknowledgment that trillion-parameter models are not actually a good idea for most scientific applications.</p><h3 id=\"section32\">AI Systems for Science and Zettascale</h3><p style=\"text-align: left;\">This singular focus on creating massive LLMs for science was front-and-center in a talk given by Rick Stevens titled \"<a href=\"https://isc.app.swapcard.com/event/isc-high-performance-2024/planning/UGxhbm5pbmdfMTg4MTE0Mg==\">The Decade Ahead: Building Frontier AI Systems for Science and the Path to Zettascale</a>.\" The overall thesis that I heard was something like...</p><div><ol style=\"text-align: left;\"><li>Science needs its own trillion-parameter foundation models</li><li>Training trillion-parameter foundation models requires a lot of GPUs</li><li>We need $25 billion from the U.S. government</li></ol><p style=\"text-align: left;\">However, Stevens never answered a very basic question: what does a foundation model for science do that any other foundation model cannot do?</p><p style=\"text-align: left;\">He showed slides like this which really don't sound like foundation models for science as much as a generic AI assistants:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p style=\"text-align: left;\">Is the scientific computing HPC community really the most qualified bunch to reinvent what existing foundation models like GPT-4 or Claude 3 have already done? Even if you argue that these proprietary models aren't as good at \"science\" as they could be, who would have a better chance of addressing this with a billion dollars of federal funding: the companies who developed GPT or Claude, or a collection of government scientists starting from scratch?</p><p style=\"text-align: left;\">I think the answer to this question was in other parts of Stevens' talk. For example, he started with this slide:</p></div><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>While robust requirements are good when there's no urgency, this slide is also a tacit admission that the government takes years to general a perspective on AI. Do you think the creators of Llama-3 or Mistral Large gathered wide community input from over 1,300 researchers before deciding to build a supercomputer and train a model? Even if science needs its own foundation models, this slide is strong evidence that, by the time the scientific HPC community agrees on a path forward, that path will be years out of date relative to what the commercial AI industry is doing.</p><p>A great example of this already happening is the basic premise that creating a foundation model with a trillion parameters is the best way to apply AI to solve science problems. This certainly was the leading thought two years ago, when transformer scaling laws were published that suggested that the best way to get better-performing LLMs was to simply add more parameters to your transformer and train on more data. But there's a reason all the leading models have stopped advertising how many parameters they use.</p><p>Dealing with massive transformers is really expensive. They're not only really expensive to train, but they're really expensive to use for inferencing too. This has led to a bunch of innovation to develop model architectures and approaches to training that result in dramatically higher quality outputs from a fixed parameter count. Dense transformer architectures with a trillion parameters have become the blunt instrument in developing foundation models since 2022, so it took me by surprise to hear Stevens put so much stock into this notion that the need for a trillion-parameter model is essential for science.</p><p>To repeat myself, I am no expert in AI. I've never been <a href=\"https://www.energy.senate.gov/services/files/CF8309D8-C0A1-40C7-944F-CF71EF523FF8\">called in front of Congress to talk about AI</a> or been <a href=\"https://isc.app.swapcard.com/event/isc-high-performance-2024/planning/UGxhbm5pbmdfMTg4MTE0Mg==\">invited to give talks on the topic at ISC</a>. There might be something basic that I am missing here. But when I look at the science drivers for AI:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>I <i>know</i> that you do not need to train your own trillion-parameter model to do most of this stuff. Even the use cases that do require generative AI, like code generation and math theory, don't actually require trillions of parameters. Small language models, such as that described in <a href=\"https://arxiv.org/abs/2306.11644\">Textbooks Are All You Need</a> (published in 2023, after the reports Stevens cited in his talk), can produce amazing results with very small models when you train them using high-quality data instead of garbage from Reddit. And when you create or fine-tune a small language model for a specific science domain, not only do you save yourself from having to buy a billion-dollar supercomputer for training, but you get a model that is much more accessible to scientists around the world because they won't need a million dollars' worth of GPUs to inference with it.</p><p>So, if there's one question that was never answered across any of the AI-themed sessions at ISC this year, it is this: Why does science need to train its own large language models? My intuition is that either fine-tuning existing large language models or training small language models for domain-specific applications, would be a better investment in actually advancing science. However, if we cynically assume the real goal of LLMs-for-science is to justify buying massive GPU systems, suddenly a lot of the talks given at ISC on this topic make a lot more sense.</p><h3 id=\"section33\">Real applications of generative AI for science</h3><p>As frustrated as I got sitting through sessions on AI where it sometimes felt like the blind leading the blind, there was one really good session on actual applications of generative AI for science.</p><p><b>Mohamed Wahib </b>of RIKEN gave an insightful presentation on the unique challenges of using generative AI in science. His summary slide touched on a lot of the key challenges:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>And his actual talk focused largely on the model and data aspects of generative AI. What struck me is that the challenges he described reflected the experience of someone who has actually tried to do what many other AI experts at the conference were claiming would be the future. For example,</p><p></p><ul style=\"text-align: left;\"><li>He recognized the importance of <b>training scientific models with high-quality datasets</b>, not just garbage scraped off of social media. This means not only scraping or generating high quality data for training, but curating and attributing that data and applying reinforcement learning with human feedback as the model is being trained. This is uniquely challenging when creating models for scientific applications, as managing the quality of scientific data requires deep domain expertise. This contrasts with a generic chat bot whose inputs and outputs can often be assessed by anyone with a basic education.</li><li>He also talked about the tendency of <b>scientific data to be highly multimodal and multidimensional</b>. Whereas multimodal chatbots may combine text and vision, scientific data often contains observations of the same phenomenon from many different sensors (for example, pressure, temperature, density, strain fields, ...), and the output of a generative model for science may require multiple modalities as well.  These capabilities are not well developed in LLMs designed for human language.</li><li>Dr. Wahib also pointed out that scientific datasets tend to be huge compared to text and images, and this may require developing ways for models to have <b>context windows can fit multi-petabyte datasets' tokens</b> to identify long-range correlations. Relatedly, he also pointed out that <b>tokenization of scientific data</b> is a new set of challenges unique to this community, since industry has been focused on tokenizing low-dimensional data such as text, audio, and images.</li></ul><p></p><p>The good news is that industry's quest towards both commercializing generative AI and achieving AGI will touch on some of these challenges soon. For example, training domain-specific models using high-quality datasets is an essential component of the small language models I described in the previous section, and these small language models are what will enable privacy-preserving and cost-effective generative AI on laptops and phones. Effectively infinite context windows are also a major hurdle on the path to AGI, as industry is hard at work developing AI agents that can remember every conversation you've ever had with them. Finding more scalable approaches to attention that do not sacrifice accuracy are a part of this.</p><p><b>François Lanusse</b>, currently at the Flatiron Institute, also gave a nice presentation that clearly explained how generative AI can be used to solve inverse problems—that is, figuring out the causes or conditions that resulted in a collection of measurements. A precise example he used applied generative AI to figure out what an image distorted by gravitational lensing might look like in the absence of those distortions. As I understood it, he trained a diffusion model to understand the relationship between images that are affected by gravitational lensing and the masses that cause lensing through simulation. He then used that model instead of an oversimplified Gaussian model as part of a larger method to solve the inverse problem of un-distorting the image.</p><p>The details of exactly what he did were a little over my head, but the insight piece for me is that combining generative AI and science in practice is not as straightforward as asking ChatGPT what the undistorted version of a telescope image is. Rather, almost all of the standard, science-informed approach to solving the inverse problem remained the same; the role of generative AI was simply to replace an oversimplified part of the iterative process (the Annealed Hamiltonian Monte Carlo method) to help it converge on better answers. It really is a combination of simulation and AI, rather than an outright substitution or surrogate model.</p><p>Dr. Lanusse also showed this slide which demonstrated how this approach can be generalized to other scientific domains:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>The general approach of pretraining, fine-tuning (\"adapt\"), and combining foundation models with other physics-based models seems reasonable, although I admit I have a difficult time wrapping my head around exactly how broadly scoped he envisions any given pretrained foundation model to be. I can see such a model trained on extensive sky survey data being useful for a number of astrophysical and cosmological tasks, but it's less clear to me how such a model might be useful in unrelated domains like, say, genomics.</p><p>You might also ask why I think this vision of foundation models for science is reasonable while Rick Stevens' vision didn't ring true; the difference is in scale! The foundation models cited on Lanusse's slide are vision transformers which have many orders of magnitude fewer parameters than the trillion-parameter models that others talk about. Whereas a trillion-parameter model might need to be distributed over dozens of H100 GPUs just to produce one inference result, the largest of the vision transformers can probably be squeezed on to a single high-end desktop GPU. Again, <i>you don't need billion-dollar supercomputers to train these models for science</i>.</p><p><b>Frank Noé</b> from Microsoft Research then talked about how generative AI can be applied to solve problems in simulating biological systems. Like the talk before his, Dr. Noé followed this pattern where a larger, physics-based framework had one statistical technique replaced by a method based on generative AI, and then a physics-based model is used to quantify the likelihood that the result is reasonable. He contrasted this with convention approaches (to, say, protein folding) where you just simulate for really long times in the hopes that your simulation randomly wanders into a situation where you capture a rare event.</p><p>His talk wasn't about generative AI as much as the previous speakers, but he offered a litany of ways in which AI models can be useful to molecular modeling:</p><p></p><ul style=\"text-align: left;\"><li><b>Markov state models</b> provide a statistical framework that lets you replace one long simulation (that hopefully captures every possible scenario) with a bunch of short, chopped-up simulations that hopefully capture every possible in parallel. He cited an example that took 20,000 GPU-days on V100 GPUs that would've otherwise taken a million GPU-years if done in one long simulation.</li><li><b>Coarse-grained models</b> use machine learning to develop surrogate models to simulate the physics of relatively uninteresting parts of molecular systems. The example he used was simulating the water molecules surrounding a biomolecule; water can be very difficult to accurately model, and the example he cited led to a surrogate model that was 100x faster than directly simulating water molecules.</li><li><b>Boltzmann generators</b> can generate 3D molecular structures based on a known probability distribution defined by the energy states of the system. This is another fast way to find rare but stable molecular configurations without having to throw darts at a dartboard.</li></ul><p></p><p>What struck me is that, in all these cases, the AI model is never generating results that are blindly trusted. Instead, they generate molecular configurations which are then fed into physics-based models which can quantify how likely they are to be valid.</p><p>Both Lanusse's and Noé's examples of combining AI and simulation painted a picture to me where generative AI can be really useful in solving problems where a researcher would otherwise have to make educated guesses about what physical phenomenon is really happening based on incomplete information. So long as there is a way to apply a physics-based model to check the accuracy of each guess, generative AI can be trained to predict the relationships between incomplete information and what's really going on and get to probable answers much faster than relying on physics alone.</p><p>More broadly, I couldn't help but think about the <a href=\"https://www.youtube.com/watch?v=Jfv5XCMj2c0\">Sora video showing pirate ships battling in a cup of coffee</a> as I left this session. Like that video, these talks demonstrated that it's possible to train generative AI models to reproduce physical phenomena (like the fluid dynamics of coffee) without explicitly embedding any laws of physics (like the Navier-Stokes equations) into the model itself and still get really compelling results. The part of this that was lacking from the Sora video—but was present in these talks—was closing the loop between generated results and the laws of physics by feeding those generated results back into the laws of physics to figure out if they are probable.</p><h2 id=\"section4\">High Performance Software Foundation</h2><p style=\"text-align: left;\">ISC'24 wasn't all about AI though! I wound up attending the launch of the <a href=\"https://www.hpsf.io/\">High Performance Software Foundation</a> (HPSF), a new Linux Foundation effort spearheaded by Todd Gamblin and Christian Trott (from Livermore and Sandia, respectively) aimed to promote the sustainability of the software packages relied upon within the high-performance computing community.</p><p style=\"text-align: left;\">I haven't paid close attention to HPC software in a long time since most of my work was in platform architecture and storage systems, so a lot of the background context remains a little murky to me. That said, it seems like HPSF was formed to be like the Cloud Native Computing Foundation for the HPC community in that:</p><p></p><ul style=\"text-align: left;\"><li>it will serve as a neutral home for software projects that aren't tied to any single university or government institution</li><li>it provides mechanisms to ensure that critical HPC software can continue to be maintained if its original author gets hit by a bus</li><li>it will help with the marketing, promotion, and marketing of HPC software</li></ul><p></p><p>Its governance seems pretty reasonable, with different levels of membership being accompanied by different levels of rights and obligations:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><span style=\"text-align: left;\"> </span></div><p>There is a Governing Board is comprised of paying members (and predominantly those who pay the most), while the Technical Advisory Council carries out the more technical tasks of forming working groups and onboarding projects.</p><p>There are three levels of membership, and the highest (premier) has a $175,000 per year buy-in and comes with a seat on the Governing Board. Right now, the founding seats are held by AWS, HPE, LLNL, and Sandia.</p><p>Below that is a general membership tier whose cost is on a sliding scale based on the organization size, and AMD, Intel, NVIDIA, Kitware, ORNL, LANL, and Argonne have all committed at this level.  The associate tier is below that, and it is free to nonprofits but comes with no voting rights.</p><p>It seemed like the exact functions that HPSF will have beyond this governing structure are not fully baked yet, though there were six \"prospective\" working groups that provide a general scope of what the HPSF will be doing:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>My read of the description of these working groups is that</p><p></p><ul style=\"text-align: left;\"><li><b>CI/testing</b> will supply resources (GPUs) on which HPSF projects' code can be automatically tested.</li><li><b>Software stacks</b> will maintain E4S.</li><li><b>User engagement</b> sounds like it will figure out what users of HPSF projects' software are looking for. It sounds like this will provide some product management-like support for projects.</li><li><b>Facility engagement</b> is probably like user engagement, but for the sites deploying code on behalf of their users. Again, this sounds like product management functions.</li><li><b>Security</b> sounded like stewarding SBOM-like stuff for member projects' software.</li><li><b>Benchmarking</b> would make a framework for benchmarking HPC applications.</li></ul><p></p><p>That all said, it still wasn't clear what exactly HPSF would do; what would all those membership dues go towards supporting? Based on some Q&amp;A during this BOF and follow-up afterwards, I pieced together the following:</p><p></p><ul style=\"text-align: left;\"><li>HPSF will <i>not</i> be funding developers, much in the same way that OpenSFS doesn't fund Lustre development. That said, <a href=\"https://x.com/tgamblin/status/1790018859816153327\">Todd Gamblin later said</a> that not funding software development was a financial constraint more than a policy one, with the implication that if more members join, there may be opportunity for HPSF to fund projects.</li><li>HPSF likely will be hosting events and conferences (perhaps like the CNCF hosts KubeCon), providing scholarships, developing and providing training related to member projects, and \"increasing collaboration\" (whatever that may mean!).</li></ul><div>HPSF also has some influence and ownership over its member projects:</div><p></p><ul style=\"text-align: left;\"><li>HPSF will co-own its projects' GitHub repos to ensure continuity in case the other repo owner abandons it.</li><li>HPSF will own the domain for the project for the same reasons as above.</li><li>Member projects still manage their own software development, roadmaps, releases, and the like. The HPSF won't dictate the technical direction of projects.</li><li>HPSF will own the trademark and logos of its member projects so it can prevent corporations from profiting off of repackaging products without respecting trademark.</li></ul><p style=\"text-align: left;\">This establishes an interesting new direction for the sorts of software projects that are likely to become member projects. Historically, such projects developed by the member organizations (i.e., DOE labs) have been wholly controlled by the labs that funded the work, and those software projects lived and died at the whims of the government funding. The HPSF offers a new vehicle for software projects to live on beyond the end of the grants that created them, but at the same time, it requires that the DOE surrender control of the work that it sponsored.</p><p style=\"text-align: left;\">I left the session still wondering a few pretty major things, likely borne out of my own ignorance of how similar organizations (like CNCF or the Apache Foundation) work:</p><p style=\"text-align: left;\"></p><ol style=\"text-align: left;\"><li>How does a software project actually become a member project? The HPSF folks said that the Technical Advisory Committee onboards new projects, but what is the bar if I have an open-source project used by the community that I no longer want to maintain myself? I assume it's not a pay-to-play arrangement since that defeats the purpose of sustaining software after its seed funding runs out.</li><li>What do stakeholders actually get out of joining HPSF? I see obvious value for organizations (like the DOE labs) who develop open-source software but may not want to be exclusively responsible for sustaining it forever. But would an HPC facility get any obvious benefit from joining and paying dues if it is simply a consumer of member projects' software? What does a cloud vendor like AWS get by being a premiere member? Is HPSF just a way to get someone else to cover the overheads of maintaining <a href=\"https://github.com/awslabs\">open-source software that comes out of, say, R&amp;D organizations</a> rather than product organizations?</li></ol><p></p><p></p><p>Hopefully the answers to these questions become clearer as the foundation gets off the ground and we get to see what member organizations contribute under the HPSF banner.</p><p>Ultimately though, I see this as a really positive direction for the HPC software community that might help resolve some uncertainty around key pieces of HPC software that have uncertain ownership. For example, I wound up as a maintainer of the IOR and mdtest benchmark because I was the last one to touch it when its previous maintainer lost interest/funding. I don't even work in I/O performance anymore, but the community still uses this benchmark in virtually every procurement of parallel file systems either directly or through IO500. It would be wonderful if such an important tool didn't rest on my shoulders and had a more concrete governance structure given how important it is.</p><h2 id=\"section5\">Quantum computing</h2><p style=\"text-align: left;\">Besides AI and cloud, quantum computing was cited in Kathy Yelick's opening keynote as the third disruptor to HPC for scientific computing. At the time, I thought citing quantum was just an obligation of any opening keynote speaker, but quantum computing was particularly high-profile at ISC this year. I was surprised to see over a dozen quantum computing companies on the vendor exhibition floor, many of whom were Europe-based startups.</p><p style=\"text-align: left;\">In addition, this year's Hans Meuer award (for best research paper) was given to a paper on quantum computing by Camps et al. This is particularly notable since this is the first time that the Meuer award has ever been given to a paper on a topic that isn't some hardcore traditional HPC like MPI or OpenMP advancements; by comparison, this award has never been given to any papers on AI topics. Granted, the winning paper was specifically about how to use conventional HPC to solve quantum problems, but this recognition of research in quantum computing makes a powerful statement: quantum computing research is high-performance computing research.</p><h2 id=\"section6\">Reinvent HPC to include urgent computing?</h2><p style=\"text-align: left;\">I was invited to give a lightning talk at the <a href=\"https://www.interactivehpc.com\">Workshop on Interactive and Urgent High-Performance Computing</a> on Thursday, and urgent/interactive HPC is not something I'd really paid attention to in the past. So as not to sound like an ignorant fool going into that workshop, I opted to sit in on a focus session titled \"Urgent Computing\" on Tuesday. I had two goals:</p><p style=\"text-align: left;\"></p><ol style=\"text-align: left;\"><li>Make sure I understood the HPC problems that fall under urgent and interactive computing so I could hold an intelligent conversation on this topic at the Thursday workshop, and</li><li>See if there are any opportunities for cloud HPC to provide unique value to the challenges faced by folks working in urgent HPC</li></ol><div>I'll describe what I came away with through these lenses.</div><p></p><h3 id=\"section61\">The Urgent Computing focus session</h3><p style=\"text-align: left;\">What I learned from the focus session is that urgent computing is not a very well-defined set of application areas and challenges. Rather, it's another manifestation of reinventing HPC to include any kind of computation for scientific purposes.</p><p style=\"text-align: left;\">Much to my surprise, this \"Urgent Computing\" focus session was actually a session on IoT and edge computing for science. Several speakers spoke about getting data from edge sensors on drones or telephone poles into some centralized location for lightweight data analysis, and the \"urgent\" part of the problem came from the hypothetical use cases of analyzing this sensor data to respond to natural disasters. There wasn't much mention of anything requiring HPC-like computing resources; at best, a few talks made unclear references to using AI models for data analysis, but it felt like grasping:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p style=\"text-align: left;\">The above conclusion slide was presented by one of the speakers, and to be honest, I don't understand what any of it means. Granted, I know very little about urgent computing, IoT, or edge computing so there may be some domain jargon here that's throwing me off. But based on this, as someone working in the area of HPC and AI in the cloud, I don't think I have a role to play here. I'm sure <i>cloud computing</i> can help, but the challenges would be in general-purpose cloud rather than HPC.</p><h3 id=\"section62\">The Interactive and Urgent HPC workshop</h3><p style=\"text-align: left;\">Fortunately for me, the Thursday workshop on Interactive and Urgent HPC was much less about edge/IoT and more about developing software infrastructure and workflows that allow scientific data analysis of large datasets to happen before the results become obsolete. It was a fascinating workshop for learning about specific science drivers that require fast access to HPC resources, and how different HPC providers are enabling that through non-traditional services and policies. Below are a few highlights.</p><p style=\"text-align: left;\"><b>Sam Welborn (NERSC)</b> presented his team's efforts to convert a streaming data workflow from its current file-based approach into one that streamed directly into compute node memory. The specific use case was the initial data processing for image information coming off of a scanning transmission electron microscope at 480 Gbps, totaling 750 GB per shot. As he described it, the current technique involves streaming those data to files at the microscope, then copying those files to the parallel file system of a remote supercomputer, then reading, processing, and writing that data within the HPC environment to prepare it for downstream analysis tasks. And for what it's worth, this is how I've always seen \"streaming\" HPC workflows actually work; they're actually using file transfers, and the performance of both the file system at the source and destination are in the critical path.</p><p style=\"text-align: left;\">The problem with this approach is that parallel file systems on HPC systems tend to be super flaky, and there's no real reason to bounce data through a storage system if you're just going to pick it up and process it. So, Dr. Welborn showed a true streaming workflow that skipped this file step and used ZeroMQ push sockets at the microscope and pull sockets on the HPC compute nodes to do a direct memory-to-memory transfer:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p style=\"text-align: left;\">Seeing software like ZeroMQ used to enable communication in an HPC environment instead of forcing this workflow to fit into the MPI paradigm is an encouraging sign in my eyes. ZeroMQ, despite not using purpose-built HPC technology like RDMA, is the right tool for this sort of job since it supports much better resilience characteristics than messaging libraries designed for tightly coupled HPC jobs. Workflows like this that combine beefy GPU nodes with software developed in the commercial tech space suggest that the world of HPC is willing to abandon not-invented-here ideology.</p><p style=\"text-align: left;\">It wasn't clear to me that there's a great opportunity for cloud HPC to be uniquely useful in use cases like this; while you certainly can provision beefy CPU and GPU nodes with InfiniBand in Azure, cloud services can't obviously simplify this ZeroMQ-based workflow beyond just supplying general-purpose VMs on which the orchestration services can run. Had this team stuck with a file-based streaming mechanism, the performance SLAs on cloud storage (like object or ephemeral Lustre) would provide a more reliable experience to ensure the data transfer happened in near-real-time. But the better solution to unpredictable file system performance is to do exactly what was done here: skip the file system entirely.</p><p style=\"text-align: left;\">Just to keep the speaker honest, I asked why this computation couldn't simply be done at the same place as the telescope generating the data. After all, if the telescope always generates 750 GB per shot, you should be able to buy a couple GPU servers that are ideally sized to process that exact workload in the time between images. There were actually two answers: one from Sam and one from an audience member:</p><p style=\"text-align: left;\"></p><ol style=\"text-align: left;\"><li>Sam said that you can process this workflow locally, but that the goal of this work was to prepare for a future microscope (or another instrument) that could not. He also insightfully pointed out that there's tremendous value in getting the data into the HPC environment because of all the services that can be used to work on that data later. I envisioned doing things like using a Jupyter notebook to further process the data, serve it up through a web UI, and similar tasks that cannot be done if the data is stuck inside a microscope room.</li><li>An audience member also pointed out that sticking GPU nodes in the same room as electron microscopes can result in enough noise and vibration to disrupt the actual scope. This was a great point! In the days before I started working in HPC, I was training to become an electron microscopist, and I worked in a lab where we had <a href=\"https://ifmd.lehigh.edu/research-stem\">water-cooled walls</a> to avoid the problems that would be caused by air conditioning breezes. There's no way a loud server would've worked in there.</li></ol><p style=\"text-align: left;\"><b>Toshio Endo (Tokyo Tech)</b> gave an interesting talk on how they enable urgent/interactive compute jobs on their batch-scheduled TSUBAME4.0 supercomputer by doing, frankly, unnatural things. Rather than holding aside some nodes for interactive use as is common practice, his work found that a lot of user jobs do not completely use all resources on each compute node they reserve:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p style=\"text-align: left;\">I had to do a double-take when I saw this: even though 65%-80% of the nodes on the supercomputer were allocated to user jobs, less than 7% of the GPUs were actually being utilized.</p><p style=\"text-align: left;\">Dr. Endo's hypothesis was that if nodes were suitably subdivided and jobs were allowed to oversubscribe CPUs, GPUs, and memory on a compute node without impacting performance too much, they could deliver real-time access to HPC resources without having to create a separate pool of nodes only for interactive uses. He defined success as the slowdown of a shared job being 1/k if k jobs shared the same node; for example, if four jobs were all running on the same node, each one taking four times as long to complete would be acceptable, but any longer would not. He then went on to show that the best way to accomplish this is using <a href=\"https://slurm.schedmd.com/gang_scheduling.html\">Slurm's gang scheduling</a>, where each job takes turns having exclusive access to all the CPUs and GPUs on a node. The alternative (just letting the OS context switch) was no good.</p><p style=\"text-align: left;\">While a fascinating study in how to provide zero wait time to jobs in exchange for reduced performance, this whole mechanism of using gang scheduling to exploit low resource utilization seems like jamming a square peg into a round hole. If a workload doesn't (or can't) use all the GPUs on a node, then that's not the right node for the job; I feel like a more appealing solution would simply be to offer a heterogeneous mix of nodes based on the demands of the workload mix. This is hard to do if you're buying monolithic supercomputers since you're stuck with whatever node mix you've got for five years, but there is another way to buy supercomputers!</p><p style=\"text-align: left;\">I won't pretend like dynamically provisioning different flavors of CPU- and GPU-based nodes interconnected with InfiniBand in the cloud doesn't come with a cost; the convenience of being able to slosh a cluster makeup between CPU-heavy and GPU-heavy nodes will be more expensive than committing to use the same makeup of node flavors for multiple years. But if you're paying for GPUs that are only being used 7% of the time, surely it's cheaper to pay a higher cost for GPUs when you need them if it also allows you to not pay for them 93% of the time when they're idle.</p><p style=\"text-align: left;\">Bjoern Enders (NERSC) gave the first lightning talk where he presented the exploration they're making into enabling real-time and urgent computation. They're currently going in three parallel directions to provide this capability:</p><p style=\"text-align: left;\"></p><ol style=\"text-align: left;\"><li>Reservations, a process by which a user can request a specific number of nodes for a specific period of time, and Slurm ensures that many nodes are available for the exclusive use of that user by the time the reservation starts. He said that implementing this at NERSC is costly and rigid because it requires a human administrator to perform manual steps to register the reservation with Slurm. </li><li>Realtime queues, where a few nodes are held from the regular batch queue and only special real-time users can submit jobs to them. Dr. Enders said that NERSC is extremely selective about who can access this queue for obvious reasons: if too many people use it, it will back up just like the regular batch queues do.</li><li>Jupyter Hub, which utilizes job preemption and backfill under the hood. If a user requests a Jupyter job, Slurm will pre-empt a job that was submitted to a preemptible queue to satisfy the Jupyter request. However, if there are no preemptible jobs running, the Jupyter job will fail to launch after waiting for ten minutes.</li></ol><p style=\"text-align: left;\">To provide compute resources to back up these scheduling capabilities, they also deployed a new set of compute nodes that can be dynamically attached to different supercomputers they have to support urgent workloads even during downtimes.  Called \"Perlmutter on Demand\" (POD), it sounded like a separate set of Cray EX racks that can be assigned to either the Perlmutter supercomputer, or if Perlmutter is down for maintenance, either their smaller Alvarez or Muller supercomputers which share the same Cray EX architecture. What wasn't clear to me is how the Slingshot fabrics of these nodes interact; perhaps POD has its own fabric, and only the control plane owning those racks are what changes.</p><p style=\"text-align: left;\">He showed a slide of explorations they're doing with this POD infrastructure, but as with Dr. Endo's talk, this seemed a bit like a square peg in a round hole:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p style=\"text-align: left;\">All of this sounds aligned with the strengths of what HPC in a cloud environment can deliver, and some of the big challenges (like figuring out the ideal node count to reserve for interactive jobs) are problems specific to Slurm and its mechanism for scheduling. There's a lot more flexibility to rapidly provision HPC resources in cloud environments because, unlike the case where Slurm is scheduling jobs on a single cluster, cloud resource managers can schedule across any number of clusters independently. For example, if an urgent workload needing only four GPU nodes suddenly appears, it doesn't necessarily have to be scheduled on the same InfiniBand fabric that a large hero job is running on. Since the urgent job and the hero job don't need to talk to each other, cloud resource managers can go find a GPU cluster with a little more flex in them to provision those resources quickly.</p><p style=\"text-align: left;\">Automating the process of reservations is also a bit of a game of catch-up, though my guess is that this is more a matter of someone having a weekend to sit down and write the REST service that manages incoming reservation requests. Although there's not a direct analog for reservations like this in Azure, AWS has a feature called <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-capacity-blocks.html\">AWS Capacity Blocks</a> that does exactly this: if you know you'll want a certain number of GPU nodes sometime in the future, Capacity Blocks let you reserve them ahead of time through an API.</p><p></p><p></p><p>Finally, <b>I represented Microsoft</b> and gave a lightning talk that riffed on a lot of what I've been writing about in this blog post: HPC seems to be reinventing a lot of things that the cloud has already figured out how to do. The illustrious Nick Brown was kind enough to <a href=\"https://twitter.com/nickbrownhpc/status/1791129551218590207?s=21&amp;t=7LM0hNWEuk95n8Z_CNZvPg\">snap a photo of one of my slides and post it on Twitter</a>:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>My thesis was that the way urgent HPC workflows are triggered, scheduled, run, and reported on follows the same pattern that inferencing-as-a-service services (like Copilot and ChatGPT) are implemented under the hood, right down to executing multi-node jobs on InfiniBand clusters. The difference is that these cloud workflows are built on the foundation of really nice cloud services that provide security, scalability, monitoring, and hands-free management that were originally developed for commercial (not HPC!) customers. My argument was that, even if you don't want to pay cloud providers to run urgent HPC workflows as a managed service, you can use these services (and the software infrastructure on which they're built) as a blueprint for how to build these capabilities in your own HPC environments.</p><h2 id=\"section7\">Concluding thoughts</h2><p>The ISC'24 conference was fantastic, and I am glad it has not lost the unique elements that made me want to attend in the years prior to the pandemic. It's still that smaller, intimate, and focused HPC conference that brings the community together. Although a lot of my synopsis above may sound critical of the content presented over the four days I attended, the fact that I've had so much to write down in this blog post is a testament to the value I really get out of attending: it makes me sit down and think critically about the way the HPC community is evolving, what the leading minds in the field are thinking, and where I might be able to contribute the most in the coming year.</p><p>I never much paid attention to the annual taglines of conferences like ISC, but this year's \"Reinvent HPC\" really resonated. The HPC community is at a crossroads. Exascale computing for science is now in the rear-view mirror, and large-scale AI is all the rage across the computing industry at large. But for the first time ever, this new direction in at-scale computing is happening without the inclusion of the people and organizations who've historically driven innovation in HPC. Whereas institutions like Oak Ridge, RIKEN, Cray, and Fujitsu defined the future of computing for decades, hundred-person startups like OpenAI and Anthropic are now paving the way in partnership with companies like Microsoft and Amazon.</p><p>HPC needs to be reinvented, if for no other reason than to decide whether the HPC community wants to be inclusive of new frontiers in computing that they do not lead. Does the HPC community want AI to be considered a part of HPC?</p><p>Judging from many speakers and panelists, the answer may be \"no.\" To many, it sounded like AI is just another industry that's sucking all the air (and GPUs) out of the room; it's a distraction that is pulling funding and public interest away from solving real problems. It's not something worth understanding, it's not something that uses the familiar tools and libraries, and it's not the product of decades of steady, government-funded improvements. AI is \"them\" and HPC is \"us.\"</p><p>Personally, I'd like the answer to be \"yes\" though. Now that I'm on the other side of the table, supporting AI for a cloud provider, I can say that the technical challenges I face at Microsoft are the same technical challenges I faced in the DOE. The desire to deeply understand systems, optimize applications, and put world-class computing infrastructure in the hands of people who do amazing things is the same. And as the days go by, many of the faces I see are the same; instead of wearing DOE or Cray badges, my lifelong colleagues are now wearing NVIDIA or Microsoft badges.</p><p>All this applies equally to whether cloud is HPC or not. The HPC community needs to reinvent itself to be inclusive of <i>everyone</i> working towards solving the same problems of computing at scale. Stop talking about people who work on commercial AI in cloud-based supercomputers as if they aren't in the room. They are in the room. Often near the front row, snapping photos, and angrily posting commentary on Twitter about how you're getting it all wrong.</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p style=\"text-align: left;\">HPC has historically been used to solve scientific problems, whether to expand our understanding of the university, to find the next best place to drill an oil well, or to model the safety of aging nuclear weapons. The fact that HPC is now being used to solve squishier problems related to natural language or image generation does not change the essence of HPC. And whether that HPC is delivered through physical nodes and networks or virtualized nodes and networks is irrelevant, as long as those resources are still delivering high performance. AI is just as much HPC as scientific computing is, and cloud is just as much HPC as OLCF, R-CCS, or CSCS is.</p><p style=\"text-align: left;\">So perhaps HPC doesn't need to be reinvented as much as the mindset of its community does.</p><p style=\"text-align: left;\">That all said, I am genuinely impressed by how quickly ISC'24 has been reinventing itself in recent years. It wasn't too long ago that all its keynote speakers were greybeards from a predictable pool of public HPC centers all saying the same things year after year. It's wonderful to see a greater diversity of perspectives on the main stage and torches passing on to the next generation of leading figures in the field. And it was not lost on me that, for the first time in the history of this conference, Thomas Sterling did not deliver the closing keynote. As much fun as I had poking fun at his meandering and often-off-the-mark conjectures every year, it was delightful to be exposed to something new this year.</p><p style=\"text-align: left;\">I'm hopeful that ISC will continue to get better year over year, and ISC'25 will feel more inclusive of me despite the fact that I am now one of those hyperscale cloud AI people. So long as I still feel like it's my community, though, I will keep showing up in Germany every summer.</p>",
            "url": "https://hpc.social/personal-blog/2024/isc-24-recap/",
            
            
            
            
            
            "date_published": "2024-05-28T05:24:00-06:00",
            "date_modified": "2024-05-28T05:24:00-06:00",
            
                "author": "Glenn K. Lockwood's Blog"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2024/centralized-system-and-lsf-logging-on-a-turing-pi-system/",
            "title": "Centralized system and LSF logging on a Turing Pi system",
            "summary": null,
            "content_text": "Logs are one of those indispensable things in IT when things go wrong. Having worked in technical support for software products in a past life, I’ve likely looked at hundreds (or more) logs over the years, helping to identify issues. So, I really appreciate the importance of logs, but I can honestly say that I never really thought about a logging strategy for the systems on my home network - primarily those running Linux.One of my longtime friends, Peter Czanik, who also works in IT, happens to be a logging guru as well as an IBM Champion for Power Systems (yeah!). So it’s only natural that we get to talking about logging. He is often complaining that even at IT security conferences people are unaware of the importance of central logging. So, why is it so important? For security it’s obvious: logs are stored independently from the compromised system, so they cannot be modified or deleted by the attacker. But central logging is beneficial for the HPC operator as well. First of all, it’s availability. You can read the logs even if one of your nodes becomes unreachable. Instead of trying to breath life into the failed node, you can just take a look at the logs and see a broken hard drive, or a similar deadly problem. And it is also convenience, as all logs are available at a single location. Logging into each node on the 3 node cluster to check locally saved logs is inconvenient but doable. On a 10 node cluster it takes a long time. On a 100 node cluster a couple of working days. While, if your logs are collected to a central location, maybe a single grep command, or search in a Kibana or similar web interface.Those who follow my blog will know that I’ve been tinkering with a Turing Pi V1 system lately. You can read my latest post here. For me, the Turing Pi has always been a cluster in a box. My Turing Pi is fully populated with 7 compute modules. I’ve designed Node 1 to be the NFS server and LSF manager for the cluster. LSF is a workload scheduler for high-performance computing (HPC) from IBM. Naturally I turned to Peter for his guidance on this, and the result is this blog. Peter recommended that I  use syslog-ng for log aggregation and also helped me through some of my first steps with syslog-ng. And the goal was to aggregate both the system (syslog) as well as LSF logs on Node 1. TL;DR it was easy to get it all working. But I encourage you to read on to better understand the nuances and necessary configuration both syslog-ng and LSF that was needed.The environmentThe following software has been deployed on the Turing Pi:Raspberry Pi OS (2023-02-21-raspios-bullseye-arm64-lite.img)syslog-ng 3 – (3.28.1 as supplied with Raspberry Pi OS)IBM LSF Standard Edition V10.1.0.13The Turing Pi system is configured as follows:Node 1 (turingpi) is the manager node of this cluster in a box and has by far the most storage. Naturally we want to use that as the centralized logging server.NodeHostnameHardwareNotes1turingpiCM3+LSF manager, NFS server, 128GB SDcard2kemenyCM34GB eMMC flash3neumannCM3+8GB SDcard4szilardCM3+8GB SDcard5tellerCM3+8GB SDcard6vonkarmanCM3+8GB SDcard7wignerCM3+8GB SDcardSyslog-ng &amp; LSF setupRaspberry Pi OS configures rsyslog out of the box. The first step is to install syslog-ng on Node 1 in the environment. Note that installing syslog-ng automatically disables rsyslog on the nodes.  Output of apt update; apt-get install syslog-ng -y. Click to expand  root@turingpi:~# apt update; apt-get install syslog-ng -y Hit:1 http://security.debian.org/debian-security bullseye-security InReleaseHit:2 http://deb.debian.org/debian bullseye InRelease                                                        Hit:3 http://deb.debian.org/debian bullseye-updates InRelease                                                Hit:4 https://repos.influxdata.com/debian stable InRelease                                                   Hit:5 https://repos.influxdata.com/debian bullseye InRelease                                                 Hit:6 http://archive.raspberrypi.org/debian bullseye InRelease                                  Hit:7 https://packagecloud.io/ookla/speedtest-cli/debian bullseye InRelease                     Reading package lists... DoneBuilding dependency tree... DoneReading state information... DoneAll packages are up to date.Reading package lists... DoneBuilding dependency tree... DoneReading state information... DoneThe following additional packages will be installed:  libbson-1.0-0 libdbi1 libesmtp6 libhiredis0.14 libivykis0 libmaxminddb0 libmongoc-1.0-0 libmongocrypt0  libnet1 libprotobuf-c1 librabbitmq4 librdkafka1 libriemann-client0 libsnappy1v5 libsnmp-base libsnmp40  syslog-ng-core syslog-ng-mod-add-contextual-data syslog-ng-mod-amqp syslog-ng-mod-examples  syslog-ng-mod-extra syslog-ng-mod-geoip2 syslog-ng-mod-getent syslog-ng-mod-graphite syslog-ng-mod-http  syslog-ng-mod-map-value-pairs syslog-ng-mod-mongodb syslog-ng-mod-python syslog-ng-mod-rdkafka  syslog-ng-mod-redis syslog-ng-mod-riemann syslog-ng-mod-slog syslog-ng-mod-smtp syslog-ng-mod-snmp  syslog-ng-mod-sql syslog-ng-mod-stardate syslog-ng-mod-stomp syslog-ng-mod-xml-parserSuggested packages:  mmdb-bin snmp-mibs-downloader rabbitmq-server graphite-web mongodb-server libdbd-mysql libdbd-pgsql  libdbd-sqlite3 activemqThe following packages will be REMOVED:  rsyslogThe following NEW packages will be installed:  libbson-1.0-0 libdbi1 libesmtp6 libhiredis0.14 libivykis0 libmaxminddb0 libmongoc-1.0-0 libmongocrypt0  libnet1 libprotobuf-c1 librabbitmq4 librdkafka1 libriemann-client0 libsnappy1v5 libsnmp-base libsnmp40  syslog-ng syslog-ng-core syslog-ng-mod-add-contextual-data syslog-ng-mod-amqp syslog-ng-mod-examples  syslog-ng-mod-extra syslog-ng-mod-geoip2 syslog-ng-mod-getent syslog-ng-mod-graphite syslog-ng-mod-http  syslog-ng-mod-map-value-pairs syslog-ng-mod-mongodb syslog-ng-mod-python syslog-ng-mod-rdkafka  syslog-ng-mod-redis syslog-ng-mod-riemann syslog-ng-mod-slog syslog-ng-mod-smtp syslog-ng-mod-snmp  syslog-ng-mod-sql syslog-ng-mod-stardate syslog-ng-mod-stomp syslog-ng-mod-xml-parser0 upgraded, 39 newly installed, 1 to remove and 0 not upgraded.Need to get 7,015 kB of archives.After this operation, 15.1 MB of additional disk space will be used.Get:1 http://deb.debian.org/debian bullseye/main arm64 libbson-1.0-0 arm64 1.17.6-1 [69.7 kB]Get:2 http://deb.debian.org/debian bullseye/main arm64 libmongocrypt0 arm64 1.1.0-1 [114 kB]Get:3 http://deb.debian.org/debian bullseye/main arm64 libsnappy1v5 arm64 1.1.8-1 [17.2 kB]Get:4 http://deb.debian.org/debian bullseye/main arm64 libmongoc-1.0-0 arm64 1.17.6-1 [257 kB]Get:5 http://deb.debian.org/debian bullseye/main arm64 libivykis0 arm64 0.42.4-1 [25.3 kB]Get:6 http://deb.debian.org/debian bullseye/main arm64 libnet1 arm64 1.1.6+dfsg-3.1 [56.8 kB]Get:7 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-core arm64 3.28.1-2+deb11u1 [591 kB]Get:8 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-mongodb arm64 3.28.1-2+deb11u1 [37.9 kB]Get:9 http://deb.debian.org/debian bullseye/main arm64 libdbi1 arm64 0.9.0-6 [27.8 kB]Get:10 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-sql arm64 3.28.1-2+deb11u1 [41.5 kB]Get:11 http://deb.debian.org/debian bullseye/main arm64 libesmtp6 arm64 1.0.6-4.3 [52.0 kB]Get:12 http://deb.debian.org/debian bullseye/main arm64 libhiredis0.14 arm64 0.14.1-1 [33.7 kB]Get:13 http://deb.debian.org/debian bullseye/main arm64 libmaxminddb0 arm64 1.5.2-1 [29.6 kB]Get:14 http://deb.debian.org/debian bullseye/main arm64 libprotobuf-c1 arm64 1.3.3-1+b2 [26.8 kB]Get:15 http://deb.debian.org/debian bullseye/main arm64 librabbitmq4 arm64 0.10.0-1 [39.7 kB]Get:16 http://deb.debian.org/debian bullseye/main arm64 librdkafka1 arm64 1.6.0-1 [515 kB]Get:17 http://deb.debian.org/debian bullseye/main arm64 libriemann-client0 arm64 1.10.4-2+b2 [21.9 kB]Get:18 http://deb.debian.org/debian bullseye/main arm64 libsnmp-base all 5.9+dfsg-4+deb11u1 [1,736 kB]Get:19 http://deb.debian.org/debian bullseye/main arm64 libsnmp40 arm64 5.9+dfsg-4+deb11u1 [2,497 kB]Get:20 http://deb.debian.org/debian bullseye/main arm64 syslog-ng all 3.28.1-2+deb11u1 [25.9 kB]Get:21 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-add-contextual-data arm64 3.28.1-2+deb11u1 [40.5 kB]Get:22 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-amqp arm64 3.28.1-2+deb11u1 [48.8 kB]Get:23 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-examples arm64 3.28.1-2+deb11u1 [57.3 kB]Get:24 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-extra all 3.28.1-2+deb11u1 [35.7 kB]Get:25 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-geoip2 arm64 3.28.1-2+deb11u1 [36.9 kB]Get:26 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-graphite arm64 3.28.1-2+deb11u1 [29.4 kB]Get:27 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-http arm64 3.28.1-2+deb11u1 [50.5 kB]Get:28 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-python arm64 3.28.1-2+deb11u1 [69.9 kB]Get:29 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-rdkafka arm64 3.28.1-2+deb11u1 [41.5 kB]Get:30 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-redis arm64 3.28.1-2+deb11u1 [37.6 kB]Get:31 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-riemann arm64 3.28.1-2+deb11u1 [40.1 kB]Get:32 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-slog arm64 3.28.1-2+deb11u1 [63.3 kB]Get:33 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-smtp arm64 3.28.1-2+deb11u1 [38.0 kB]Get:34 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-snmp arm64 3.28.1-2+deb11u1 [42.5 kB]Get:35 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-stomp arm64 3.28.1-2+deb11u1 [39.1 kB]Get:36 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-xml-parser arm64 3.28.1-2+deb11u1 [34.7 kB]Get:37 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-getent arm64 3.28.1-2+deb11u1 [29.5 kB]Get:38 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-map-value-pairs arm64 3.28.1-2+deb11u1 [34.0 kB]Get:39 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-stardate arm64 3.28.1-2+deb11u1 [28.6 kB]Fetched 7,015 kB in 5s (1,311 kB/s)           Extracting templates from packages: 100%(Reading database ... 90182 files and directories currently installed.)Removing rsyslog (8.2102.0-2+deb11u1) ...Selecting previously unselected package libbson-1.0-0.(Reading database ... 90124 files and directories currently installed.)Preparing to unpack .../00-libbson-1.0-0_1.17.6-1_arm64.deb ...Unpacking libbson-1.0-0 (1.17.6-1) ...Selecting previously unselected package libmongocrypt0:arm64.Preparing to unpack .../01-libmongocrypt0_1.1.0-1_arm64.deb ...Unpacking libmongocrypt0:arm64 (1.1.0-1) ...Selecting previously unselected package libsnappy1v5:arm64.Preparing to unpack .../02-libsnappy1v5_1.1.8-1_arm64.deb ...Unpacking libsnappy1v5:arm64 (1.1.8-1) ...Selecting previously unselected package libmongoc-1.0-0.Preparing to unpack .../03-libmongoc-1.0-0_1.17.6-1_arm64.deb ...Unpacking libmongoc-1.0-0 (1.17.6-1) ...Selecting previously unselected package libivykis0:arm64.Preparing to unpack .../04-libivykis0_0.42.4-1_arm64.deb ...Unpacking libivykis0:arm64 (0.42.4-1) ...Selecting previously unselected package libnet1:arm64.Preparing to unpack .../05-libnet1_1.1.6+dfsg-3.1_arm64.deb ...Unpacking libnet1:arm64 (1.1.6+dfsg-3.1) ...Selecting previously unselected package syslog-ng-core.Preparing to unpack .../06-syslog-ng-core_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-core (3.28.1-2+deb11u1) ...Selecting previously unselected package syslog-ng-mod-mongodb.Preparing to unpack .../07-syslog-ng-mod-mongodb_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-mod-mongodb (3.28.1-2+deb11u1) ...Selecting previously unselected package libdbi1:arm64.Preparing to unpack .../08-libdbi1_0.9.0-6_arm64.deb ...Unpacking libdbi1:arm64 (0.9.0-6) ...Selecting previously unselected package syslog-ng-mod-sql.Preparing to unpack .../09-syslog-ng-mod-sql_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-mod-sql (3.28.1-2+deb11u1) ...Selecting previously unselected package libesmtp6.Preparing to unpack .../10-libesmtp6_1.0.6-4.3_arm64.deb ...Unpacking libesmtp6 (1.0.6-4.3) ...Selecting previously unselected package libhiredis0.14:arm64.Preparing to unpack .../11-libhiredis0.14_0.14.1-1_arm64.deb ...Unpacking libhiredis0.14:arm64 (0.14.1-1) ...Selecting previously unselected package libmaxminddb0:arm64.Preparing to unpack .../12-libmaxminddb0_1.5.2-1_arm64.deb ...Unpacking libmaxminddb0:arm64 (1.5.2-1) ...Selecting previously unselected package libprotobuf-c1:arm64.Preparing to unpack .../13-libprotobuf-c1_1.3.3-1+b2_arm64.deb ...Unpacking libprotobuf-c1:arm64 (1.3.3-1+b2) ...Selecting previously unselected package librabbitmq4:arm64.Preparing to unpack .../14-librabbitmq4_0.10.0-1_arm64.deb ...Unpacking librabbitmq4:arm64 (0.10.0-1) ...Selecting previously unselected package librdkafka1:arm64.Preparing to unpack .../15-librdkafka1_1.6.0-1_arm64.deb ...Unpacking librdkafka1:arm64 (1.6.0-1) ...Selecting previously unselected package libriemann-client0:arm64.Preparing to unpack .../16-libriemann-client0_1.10.4-2+b2_arm64.deb ...Unpacking libriemann-client0:arm64 (1.10.4-2+b2) ...Selecting previously unselected package libsnmp-base.Preparing to unpack .../17-libsnmp-base_5.9+dfsg-4+deb11u1_all.deb ...Unpacking libsnmp-base (5.9+dfsg-4+deb11u1) ...Selecting previously unselected package libsnmp40:arm64.Preparing to unpack .../18-libsnmp40_5.9+dfsg-4+deb11u1_arm64.deb ...Unpacking libsnmp40:arm64 (5.9+dfsg-4+deb11u1) ...Selecting previously unselected package syslog-ng.Preparing to unpack .../19-syslog-ng_3.28.1-2+deb11u1_all.deb ...Unpacking syslog-ng (3.28.1-2+deb11u1) ...Selecting previously unselected package syslog-ng-mod-add-contextual-data.Preparing to unpack .../20-syslog-ng-mod-add-contextual-data_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-mod-add-contextual-data (3.28.1-2+deb11u1) ...Selecting previously unselected package syslog-ng-mod-amqp.Preparing to unpack .../21-syslog-ng-mod-amqp_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-mod-amqp (3.28.1-2+deb11u1) ...Selecting previously unselected package syslog-ng-mod-examples.Preparing to unpack .../22-syslog-ng-mod-examples_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-mod-examples (3.28.1-2+deb11u1) ...Selecting previously unselected package syslog-ng-mod-extra.Preparing to unpack .../23-syslog-ng-mod-extra_3.28.1-2+deb11u1_all.deb ...Unpacking syslog-ng-mod-extra (3.28.1-2+deb11u1) ...Selecting previously unselected package syslog-ng-mod-geoip2.Preparing to unpack .../24-syslog-ng-mod-geoip2_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-mod-geoip2 (3.28.1-2+deb11u1) ...Selecting previously unselected package syslog-ng-mod-graphite.Preparing to unpack .../25-syslog-ng-mod-graphite_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-mod-graphite (3.28.1-2+deb11u1) ...Selecting previously unselected package syslog-ng-mod-http.Preparing to unpack .../26-syslog-ng-mod-http_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-mod-http (3.28.1-2+deb11u1) ...Selecting previously unselected package syslog-ng-mod-python.Preparing to unpack .../27-syslog-ng-mod-python_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-mod-python (3.28.1-2+deb11u1) ...Selecting previously unselected package syslog-ng-mod-rdkafka.Preparing to unpack .../28-syslog-ng-mod-rdkafka_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-mod-rdkafka (3.28.1-2+deb11u1) ...Selecting previously unselected package syslog-ng-mod-redis.Preparing to unpack .../29-syslog-ng-mod-redis_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-mod-redis (3.28.1-2+deb11u1) ...Selecting previously unselected package syslog-ng-mod-riemann.Preparing to unpack .../30-syslog-ng-mod-riemann_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-mod-riemann (3.28.1-2+deb11u1) ...Selecting previously unselected package syslog-ng-mod-slog.Preparing to unpack .../31-syslog-ng-mod-slog_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-mod-slog (3.28.1-2+deb11u1) ...Selecting previously unselected package syslog-ng-mod-smtp.Preparing to unpack .../32-syslog-ng-mod-smtp_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-mod-smtp (3.28.1-2+deb11u1) ...Selecting previously unselected package syslog-ng-mod-snmp.Preparing to unpack .../33-syslog-ng-mod-snmp_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-mod-snmp (3.28.1-2+deb11u1) ...Selecting previously unselected package syslog-ng-mod-stomp.Preparing to unpack .../34-syslog-ng-mod-stomp_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-mod-stomp (3.28.1-2+deb11u1) ...Selecting previously unselected package syslog-ng-mod-xml-parser.Preparing to unpack .../35-syslog-ng-mod-xml-parser_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-mod-xml-parser (3.28.1-2+deb11u1) ...Selecting previously unselected package syslog-ng-mod-getent.Preparing to unpack .../36-syslog-ng-mod-getent_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-mod-getent (3.28.1-2+deb11u1) ...Selecting previously unselected package syslog-ng-mod-map-value-pairs.Preparing to unpack .../37-syslog-ng-mod-map-value-pairs_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-mod-map-value-pairs (3.28.1-2+deb11u1) ...Selecting previously unselected package syslog-ng-mod-stardate.Preparing to unpack .../38-syslog-ng-mod-stardate_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-mod-stardate (3.28.1-2+deb11u1) ...Setting up librabbitmq4:arm64 (0.10.0-1) ...Setting up libdbi1:arm64 (0.9.0-6) ...Setting up libsnmp-base (5.9+dfsg-4+deb11u1) ...Setting up libmaxminddb0:arm64 (1.5.2-1) ...Setting up libesmtp6 (1.0.6-4.3) ...Setting up libnet1:arm64 (1.1.6+dfsg-3.1) ...Setting up libprotobuf-c1:arm64 (1.3.3-1+b2) ...Setting up libsnappy1v5:arm64 (1.1.8-1) ...Setting up libsnmp40:arm64 (5.9+dfsg-4+deb11u1) ...Setting up libbson-1.0-0 (1.17.6-1) ...Setting up libivykis0:arm64 (0.42.4-1) ...Setting up libriemann-client0:arm64 (1.10.4-2+b2) ...Setting up librdkafka1:arm64 (1.6.0-1) ...Setting up libhiredis0.14:arm64 (0.14.1-1) ...Setting up libmongocrypt0:arm64 (1.1.0-1) ...Setting up libmongoc-1.0-0 (1.17.6-1) ...Setting up syslog-ng-core (3.28.1-2+deb11u1) ...Created symlink /etc/systemd/system/multi-user.target.wants/syslog-ng.service → /lib/systemd/system/syslog-ng.service.Setting up syslog-ng-mod-examples (3.28.1-2+deb11u1) ...Setting up syslog-ng-mod-xml-parser (3.28.1-2+deb11u1) ...Setting up syslog-ng-mod-stomp (3.28.1-2+deb11u1) ...Setting up syslog-ng-mod-riemann (3.28.1-2+deb11u1) ...Setting up syslog-ng-mod-stardate (3.28.1-2+deb11u1) ...Setting up syslog-ng-mod-geoip2 (3.28.1-2+deb11u1) ...Setting up syslog-ng-mod-getent (3.28.1-2+deb11u1) ...Setting up syslog-ng-mod-amqp (3.28.1-2+deb11u1) ...Setting up syslog-ng-mod-python (3.28.1-2+deb11u1) ...Setting up syslog-ng-mod-smtp (3.28.1-2+deb11u1) ...Setting up syslog-ng-mod-snmp (3.28.1-2+deb11u1) ...Setting up syslog-ng-mod-extra (3.28.1-2+deb11u1) ...Setting up syslog-ng-mod-rdkafka (3.28.1-2+deb11u1) ...Setting up syslog-ng-mod-graphite (3.28.1-2+deb11u1) ...Setting up syslog-ng-mod-add-contextual-data (3.28.1-2+deb11u1) ...Setting up syslog-ng-mod-mongodb (3.28.1-2+deb11u1) ...Setting up syslog-ng-mod-http (3.28.1-2+deb11u1) ...Setting up syslog-ng-mod-slog (3.28.1-2+deb11u1) ...Setting up syslog-ng-mod-map-value-pairs (3.28.1-2+deb11u1) ...Setting up syslog-ng-mod-sql (3.28.1-2+deb11u1) ...Setting up syslog-ng-mod-redis (3.28.1-2+deb11u1) ...Setting up syslog-ng (3.28.1-2+deb11u1) ...Processing triggers for man-db (2.9.4-2) ...Processing triggers for libc-bin (2.31-13+rpt2+rpi1+deb11u8) ...Scanning processes...                                                                                         Scanning processor microcode...                                                                               Scanning linux images...                                                                                      Running kernel seems to be up-to-date.Failed to check for processor microcode upgrades.No services need to be restarted.No containers need to be restarted.No user sessions are running outdated binaries.2. With syslog-ng installed, it’s now time to build the configuration for it. A new configuration file fromnet.conf is shown below, in which a syslog-ng destination is created which will aggregate logs from the Turing Pi nodes in /var/log/fromnet in plain text format. Additionally, the logs will be written in JSON format to the file /var/log/fromnet.json.root@turingpi:~# cat /etc/syslog-ng/fromnet.conf # sourcesource s_fromnet {  syslog(port(601));};# destination destination d_fromnet {  file(\"/var/log/fromnet\");  file(\"/var/log/fromnet.json\" template(\"$(format-json --scope rfc5424 --scope dot-nv-pairs        --rekey .* --shift 1 --scope nv-pairs)\\n\") );};# log pathlog {  source(s_fromnet);  destination(d_fromnet);}; Unless we only want to see source IP addresses in the collected logs, it’s necessary to update the syslog-ng configuration file /etc/syslog-ng/syslog-ng.conf to record the hostnames from which the log messages have originated. This is done by adding the keep_hostname(yes) parameter to the options section as follows:........# First, set some global options. options { chain_hostnames(off); flush_lines(0); use_dns(no); use_fqdn(no);                  keep_hostname(yes);dns_cache(no); owner(\"root\"); group(\"adm\"); perm(0640);         stats_freq(0); bad_hostname(\"^gconfd$\"); };........Next, the IBM LSF configuration is updated to prevent the creation of local logfiles for the LSF daemons. This is done by commenting the LSF_LOGDIR option in the configuration file $LSF_ENVDIR/lsf.conf. At the same time, we also set LSF_LOG_MASK=LOG_DEBUG for testing purposes to enable verbose logging for the LSF daemons.........# Daemon log messages# LSF_LOGDIR=/opt/ibm/lsf/logLSF_LOG_MASK=LOG_DEBUG........Finally, to make the changes take effect, both syslog-ng and LSF are restarted.root@turingpi:~# systemctl restart syslog-ng root@turingpi:~# . /opt/ibm/lsf/conf/profile.lsf  root@turingpi:~# lsf_daemons restart Stopping the LSF subsystem Starting the LSF subsystemWith the configuration ready on the centralized logging server, host turingpi, we now turn our attention to Nodes 2-7 in the cluster. Here we’ll use the parallel-ssh tool to streamline some operations. We start with the installation of syslog-ng across Nodes 2-7. Note that the output of the installation of syslog-ng across the compute nodes has been truncated.  Truncated output of parallel-ssh -h /opt/workers -i &ldquo;apt-get install syslog-ng -y&rdquo;. Click to expand  root@turingpi:~# parallel-ssh -h /opt/workers -i \"apt-get install syslog-ng -y\" [1] 13:57:07 [SUCCESS] kemenyReading package lists...Building dependency tree...Reading state information...The following additional packages will be installed:  libbson-1.0-0 libdbi1 libesmtp6 libhiredis0.14 libivykis0 libmaxminddb0  libmongoc-1.0-0 libmongocrypt0 libnet1 libprotobuf-c1 librabbitmq4  librdkafka1 libriemann-client0 libsensors-config libsensors5 libsnappy1v5  libsnmp-base libsnmp40 syslog-ng-core syslog-ng-mod-add-contextual-data  syslog-ng-mod-amqp syslog-ng-mod-examples syslog-ng-mod-extra  syslog-ng-mod-geoip2 syslog-ng-mod-getent syslog-ng-mod-graphite  syslog-ng-mod-http syslog-ng-mod-map-value-pairs syslog-ng-mod-mongodb  syslog-ng-mod-python syslog-ng-mod-rdkafka syslog-ng-mod-redis  syslog-ng-mod-riemann syslog-ng-mod-slog syslog-ng-mod-smtp  syslog-ng-mod-snmp syslog-ng-mod-sql syslog-ng-mod-stardate  syslog-ng-mod-stomp syslog-ng-mod-xml-parserSuggested packages:  mmdb-bin lm-sensors snmp-mibs-downloader rabbitmq-server graphite-web  mongodb-server libdbd-mysql libdbd-pgsql libdbd-sqlite3 activemqThe following packages will be REMOVED:  rsyslogThe following NEW packages will be installed:  libbson-1.0-0 libdbi1 libesmtp6 libhiredis0.14 libivykis0 libmaxminddb0  libmongoc-1.0-0 libmongocrypt0 libnet1 libprotobuf-c1 librabbitmq4  librdkafka1 libriemann-client0 libsensors-config libsensors5 libsnappy1v5  libsnmp-base libsnmp40 syslog-ng syslog-ng-core  syslog-ng-mod-add-contextual-data syslog-ng-mod-amqp syslog-ng-mod-examples  syslog-ng-mod-extra syslog-ng-mod-geoip2 syslog-ng-mod-getent  syslog-ng-mod-graphite syslog-ng-mod-http syslog-ng-mod-map-value-pairs  syslog-ng-mod-mongodb syslog-ng-mod-python syslog-ng-mod-rdkafka  syslog-ng-mod-redis syslog-ng-mod-riemann syslog-ng-mod-slog  syslog-ng-mod-smtp syslog-ng-mod-snmp syslog-ng-mod-sql  syslog-ng-mod-stardate syslog-ng-mod-stomp syslog-ng-mod-xml-parser0 upgraded, 41 newly installed, 1 to remove and 0 not upgraded.Need to get 7,098 kB of archives.After this operation, 15.3 MB of additional disk space will be used.Get:1 http://deb.debian.org/debian bullseye/main arm64 libbson-1.0-0 arm64 1.17.6-1 [69.7 kB]Get:2 http://deb.debian.org/debian bullseye/main arm64 libmongocrypt0 arm64 1.1.0-1 [114 kB]Get:3 http://deb.debian.org/debian bullseye/main arm64 libsnappy1v5 arm64 1.1.8-1 [17.2 kB]Get:4 http://deb.debian.org/debian bullseye/main arm64 libmongoc-1.0-0 arm64 1.17.6-1 [257 kB]Get:5 http://deb.debian.org/debian bullseye/main arm64 libivykis0 arm64 0.42.4-1 [25.3 kB]Get:6 http://deb.debian.org/debian bullseye/main arm64 libnet1 arm64 1.1.6+dfsg-3.1 [56.8 kB]Get:7 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-core arm64 3.28.1-2+deb11u1 [591 kB]Get:8 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-mongodb arm64 3.28.1-2+deb11u1 [37.9 kB]Get:9 http://deb.debian.org/debian bullseye/main arm64 libdbi1 arm64 0.9.0-6 [27.8 kB]Get:10 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-sql arm64 3.28.1-2+deb11u1 [41.5 kB]Get:11 http://deb.debian.org/debian bullseye/main arm64 libesmtp6 arm64 1.0.6-4.3 [52.0 kB]Get:12 http://deb.debian.org/debian bullseye/main arm64 libhiredis0.14 arm64 0.14.1-1 [33.7 kB]Get:13 http://deb.debian.org/debian bullseye/main arm64 libmaxminddb0 arm64 1.5.2-1 [29.6 kB]Get:14 http://deb.debian.org/debian bullseye/main arm64 libprotobuf-c1 arm64 1.3.3-1+b2 [26.8 kB]Get:15 http://deb.debian.org/debian bullseye/main arm64 librabbitmq4 arm64 0.10.0-1 [39.7 kB]Get:16 http://deb.debian.org/debian bullseye/main arm64 librdkafka1 arm64 1.6.0-1 [515 kB]Get:17 http://deb.debian.org/debian bullseye/main arm64 libriemann-client0 arm64 1.10.4-2+b2 [21.9 kB]Get:18 http://deb.debian.org/debian bullseye/main arm64 libsensors-config all 1:3.6.0-7 [32.3 kB]Get:19 http://deb.debian.org/debian bullseye/main arm64 libsensors5 arm64 1:3.6.0-7 [51.2 kB]Get:20 http://deb.debian.org/debian bullseye/main arm64 libsnmp-base all 5.9+dfsg-4+deb11u1 [1,736 kB]Get:21 http://deb.debian.org/debian bullseye/main arm64 libsnmp40 arm64 5.9+dfsg-4+deb11u1 [2,497 kB]Get:22 http://deb.debian.org/debian bullseye/main arm64 syslog-ng all 3.28.1-2+deb11u1 [25.9 kB]Get:23 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-add-contextual-data arm64 3.28.1-2+deb11u1 [40.5 kB]Get:24 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-amqp arm64 3.28.1-2+deb11u1 [48.8 kB]Get:25 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-examples arm64 3.28.1-2+deb11u1 [57.3 kB]Get:26 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-extra all 3.28.1-2+deb11u1 [35.7 kB]Get:27 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-geoip2 arm64 3.28.1-2+deb11u1 [36.9 kB]Get:28 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-graphite arm64 3.28.1-2+deb11u1 [29.4 kB]Get:29 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-http arm64 3.28.1-2+deb11u1 [50.5 kB]Get:30 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-python arm64 3.28.1-2+deb11u1 [69.9 kB]Get:31 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-rdkafka arm64 3.28.1-2+deb11u1 [41.5 kB]Get:32 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-redis arm64 3.28.1-2+deb11u1 [37.6 kB]Get:33 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-riemann arm64 3.28.1-2+deb11u1 [40.1 kB]Get:34 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-slog arm64 3.28.1-2+deb11u1 [63.3 kB]Get:35 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-smtp arm64 3.28.1-2+deb11u1 [38.0 kB]Get:36 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-snmp arm64 3.28.1-2+deb11u1 [42.5 kB]Get:37 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-stomp arm64 3.28.1-2+deb11u1 [39.1 kB]Get:38 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-xml-parser arm64 3.28.1-2+deb11u1 [34.7 kB]Get:39 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-getent arm64 3.28.1-2+deb11u1 [29.5 kB]Get:40 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-map-value-pairs arm64 3.28.1-2+deb11u1 [34.0 kB]Get:41 http://deb.debian.org/debian bullseye/main arm64 syslog-ng-mod-stardate arm64 3.28.1-2+deb11u1 [28.6 kB]Fetched 7,098 kB in 2s (3,566 kB/s)(Reading database ... 37650 files and directories currently installed.)Removing rsyslog (8.2102.0-2+deb11u1) ...Selecting previously unselected package libbson-1.0-0.(Reading database ... 37592 files and directories currently installed.)Preparing to unpack .../00-libbson-1.0-0_1.17.6-1_arm64.deb ...Unpacking libbson-1.0-0 (1.17.6-1) ...Selecting previously unselected package libmongocrypt0:arm64.Preparing to unpack .../01-libmongocrypt0_1.1.0-1_arm64.deb ...Unpacking libmongocrypt0:arm64 (1.1.0-1) ...Selecting previously unselected package libsnappy1v5:arm64.Preparing to unpack .../02-libsnappy1v5_1.1.8-1_arm64.deb ...Unpacking libsnappy1v5:arm64 (1.1.8-1) ...Selecting previously unselected package libmongoc-1.0-0.Preparing to unpack .../03-libmongoc-1.0-0_1.17.6-1_arm64.deb ...Unpacking libmongoc-1.0-0 (1.17.6-1) ...Selecting previously unselected package libivykis0:arm64.Preparing to unpack .../04-libivykis0_0.42.4-1_arm64.deb ...Unpacking libivykis0:arm64 (0.42.4-1) ...Selecting previously unselected package libnet1:arm64.Preparing to unpack .../05-libnet1_1.1.6+dfsg-3.1_arm64.deb ...Unpacking libnet1:arm64 (1.1.6+dfsg-3.1) ...Selecting previously unselected package syslog-ng-core.Preparing to unpack .../06-syslog-ng-core_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-core (3.28.1-2+deb11u1) ...Selecting previously unselected package syslog-ng-mod-mongodb.Preparing to unpack .../07-syslog-ng-mod-mongodb_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-mod-mongodb (3.28.1-2+deb11u1) ...Selecting previously unselected package libdbi1:arm64.Preparing to unpack .../08-libdbi1_0.9.0-6_arm64.deb ...Unpacking libdbi1:arm64 (0.9.0-6) ...Selecting previously unselected package syslog-ng-mod-sql.Preparing to unpack .../09-syslog-ng-mod-sql_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-mod-sql (3.28.1-2+deb11u1) ...Selecting previously unselected package libesmtp6.Preparing to unpack .../10-libesmtp6_1.0.6-4.3_arm64.deb ...Unpacking libesmtp6 (1.0.6-4.3) ...Selecting previously unselected package libhiredis0.14:arm64.Preparing to unpack .../11-libhiredis0.14_0.14.1-1_arm64.deb ...Unpacking libhiredis0.14:arm64 (0.14.1-1) ...Selecting previously unselected package libmaxminddb0:arm64.Preparing to unpack .../12-libmaxminddb0_1.5.2-1_arm64.deb ...Unpacking libmaxminddb0:arm64 (1.5.2-1) ...Selecting previously unselected package libprotobuf-c1:arm64.Preparing to unpack .../13-libprotobuf-c1_1.3.3-1+b2_arm64.deb ...Unpacking libprotobuf-c1:arm64 (1.3.3-1+b2) ...Selecting previously unselected package librabbitmq4:arm64.Preparing to unpack .../14-librabbitmq4_0.10.0-1_arm64.deb ...Unpacking librabbitmq4:arm64 (0.10.0-1) ...Selecting previously unselected package librdkafka1:arm64.Preparing to unpack .../15-librdkafka1_1.6.0-1_arm64.deb ...Unpacking librdkafka1:arm64 (1.6.0-1) ...Selecting previously unselected package libriemann-client0:arm64.Preparing to unpack .../16-libriemann-client0_1.10.4-2+b2_arm64.deb ...Unpacking libriemann-client0:arm64 (1.10.4-2+b2) ...Selecting previously unselected package libsensors-config.Preparing to unpack .../17-libsensors-config_1%3a3.6.0-7_all.deb ...Unpacking libsensors-config (1:3.6.0-7) ...Selecting previously unselected package libsensors5:arm64.Preparing to unpack .../18-libsensors5_1%3a3.6.0-7_arm64.deb ...Unpacking libsensors5:arm64 (1:3.6.0-7) ...Selecting previously unselected package libsnmp-base.Preparing to unpack .../19-libsnmp-base_5.9+dfsg-4+deb11u1_all.deb ...Unpacking libsnmp-base (5.9+dfsg-4+deb11u1) ...Selecting previously unselected package libsnmp40:arm64.Preparing to unpack .../20-libsnmp40_5.9+dfsg-4+deb11u1_arm64.deb ...Unpacking libsnmp40:arm64 (5.9+dfsg-4+deb11u1) ...Selecting previously unselected package syslog-ng.Preparing to unpack .../21-syslog-ng_3.28.1-2+deb11u1_all.deb ...Unpacking syslog-ng (3.28.1-2+deb11u1) ...Selecting previously unselected package syslog-ng-mod-add-contextual-data.Preparing to unpack .../22-syslog-ng-mod-add-contextual-data_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-mod-add-contextual-data (3.28.1-2+deb11u1) ...Selecting previously unselected package syslog-ng-mod-amqp.Preparing to unpack .../23-syslog-ng-mod-amqp_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-mod-amqp (3.28.1-2+deb11u1) ...Selecting previously unselected package syslog-ng-mod-examples.Preparing to unpack .../24-syslog-ng-mod-examples_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-mod-examples (3.28.1-2+deb11u1) ...Selecting previously unselected package syslog-ng-mod-extra.Preparing to unpack .../25-syslog-ng-mod-extra_3.28.1-2+deb11u1_all.deb ...Unpacking syslog-ng-mod-extra (3.28.1-2+deb11u1) ...Selecting previously unselected package syslog-ng-mod-geoip2.Preparing to unpack .../26-syslog-ng-mod-geoip2_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-mod-geoip2 (3.28.1-2+deb11u1) ...Selecting previously unselected package syslog-ng-mod-graphite.Preparing to unpack .../27-syslog-ng-mod-graphite_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-mod-graphite (3.28.1-2+deb11u1) ...Selecting previously unselected package syslog-ng-mod-http.Preparing to unpack .../28-syslog-ng-mod-http_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-mod-http (3.28.1-2+deb11u1) ...Selecting previously unselected package syslog-ng-mod-python.Preparing to unpack .../29-syslog-ng-mod-python_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-mod-python (3.28.1-2+deb11u1) ...Selecting previously unselected package syslog-ng-mod-rdkafka.Preparing to unpack .../30-syslog-ng-mod-rdkafka_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-mod-rdkafka (3.28.1-2+deb11u1) ...Selecting previously unselected package syslog-ng-mod-redis.Preparing to unpack .../31-syslog-ng-mod-redis_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-mod-redis (3.28.1-2+deb11u1) ...Selecting previously unselected package syslog-ng-mod-riemann.Preparing to unpack .../32-syslog-ng-mod-riemann_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-mod-riemann (3.28.1-2+deb11u1) ...Selecting previously unselected package syslog-ng-mod-slog.Preparing to unpack .../33-syslog-ng-mod-slog_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-mod-slog (3.28.1-2+deb11u1) ...Selecting previously unselected package syslog-ng-mod-smtp.Preparing to unpack .../34-syslog-ng-mod-smtp_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-mod-smtp (3.28.1-2+deb11u1) ...Selecting previously unselected package syslog-ng-mod-snmp.Preparing to unpack .../35-syslog-ng-mod-snmp_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-mod-snmp (3.28.1-2+deb11u1) ...Selecting previously unselected package syslog-ng-mod-stomp.Preparing to unpack .../36-syslog-ng-mod-stomp_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-mod-stomp (3.28.1-2+deb11u1) ...Selecting previously unselected package syslog-ng-mod-xml-parser.Preparing to unpack .../37-syslog-ng-mod-xml-parser_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-mod-xml-parser (3.28.1-2+deb11u1) ...Selecting previously unselected package syslog-ng-mod-getent.Preparing to unpack .../38-syslog-ng-mod-getent_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-mod-getent (3.28.1-2+deb11u1) ...Selecting previously unselected package syslog-ng-mod-map-value-pairs.Preparing to unpack .../39-syslog-ng-mod-map-value-pairs_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-mod-map-value-pairs (3.28.1-2+deb11u1) ...Selecting previously unselected package syslog-ng-mod-stardate.Preparing to unpack .../40-syslog-ng-mod-stardate_3.28.1-2+deb11u1_arm64.deb ...Unpacking syslog-ng-mod-stardate (3.28.1-2+deb11u1) ...Setting up librabbitmq4:arm64 (0.10.0-1) ...Setting up libdbi1:arm64 (0.9.0-6) ...Setting up libsnmp-base (5.9+dfsg-4+deb11u1) ...Setting up libmaxminddb0:arm64 (1.5.2-1) ...Setting up libsensors-config (1:3.6.0-7) ...Setting up libesmtp6 (1.0.6-4.3) ...Setting up libnet1:arm64 (1.1.6+dfsg-3.1) ...Setting up libprotobuf-c1:arm64 (1.3.3-1+b2) ...Setting up libsnappy1v5:arm64 (1.1.8-1) ...Setting up libbson-1.0-0 (1.17.6-1) ...Setting up libivykis0:arm64 (0.42.4-1) ...Setting up libriemann-client0:arm64 (1.10.4-2+b2) ...Setting up libsensors5:arm64 (1:3.6.0-7) ...Setting up librdkafka1:arm64 (1.6.0-1) ...Setting up libhiredis0.14:arm64 (0.14.1-1) ...Setting up libmongocrypt0:arm64 (1.1.0-1) ...Setting up libsnmp40:arm64 (5.9+dfsg-4+deb11u1) ...Setting up libmongoc-1.0-0 (1.17.6-1) ...Setting up syslog-ng-core (3.28.1-2+deb11u1) ...Created symlink /etc/systemd/system/multi-user.target.wants/syslog-ng.service → /lib/systemd/system/syslog-ng.service.Setting up syslog-ng-mod-examples (3.28.1-2+deb11u1) ...Setting up syslog-ng-mod-xml-parser (3.28.1-2+deb11u1) ...Setting up syslog-ng-mod-stomp (3.28.1-2+deb11u1) ...Setting up syslog-ng-mod-riemann (3.28.1-2+deb11u1) ...Setting up syslog-ng-mod-stardate (3.28.1-2+deb11u1) ...Setting up syslog-ng-mod-geoip2 (3.28.1-2+deb11u1) ...Setting up syslog-ng-mod-getent (3.28.1-2+deb11u1) ...Setting up syslog-ng-mod-amqp (3.28.1-2+deb11u1) ...Setting up syslog-ng-mod-python (3.28.1-2+deb11u1) ...Setting up syslog-ng-mod-smtp (3.28.1-2+deb11u1) ...Setting up syslog-ng-mod-snmp (3.28.1-2+deb11u1) ...Setting up syslog-ng-mod-extra (3.28.1-2+deb11u1) ...Setting up syslog-ng-mod-rdkafka (3.28.1-2+deb11u1) ...Setting up syslog-ng-mod-graphite (3.28.1-2+deb11u1) ...Setting up syslog-ng-mod-add-contextual-data (3.28.1-2+deb11u1) ...Setting up syslog-ng-mod-mongodb (3.28.1-2+deb11u1) ...Setting up syslog-ng-mod-http (3.28.1-2+deb11u1) ...Setting up syslog-ng-mod-slog (3.28.1-2+deb11u1) ...Setting up syslog-ng-mod-map-value-pairs (3.28.1-2+deb11u1) ...Setting up syslog-ng-mod-sql (3.28.1-2+deb11u1) ...Setting up syslog-ng-mod-redis (3.28.1-2+deb11u1) ...Setting up syslog-ng (3.28.1-2+deb11u1) ...Processing triggers for man-db (2.9.4-2) ...Processing triggers for libc-bin (2.31-13+rpt2+rpi1+deb11u8) ...Stderr: debconf: unable to initialize frontend: Dialogdebconf: (TERM is not set, so the dialog frontend is not usable.)debconf: falling back to frontend: Readlinedebconf: unable to initialize frontend: Readlinedebconf: (This frontend requires a controlling tty.)debconf: falling back to frontend: Teletypedpkg-preconfigure: unable to re-open stdin: ........7. Following the installation of syslog-ng across Nodes 2-7. We verify that the installation was successful by checking the syslog-ng service status.  Output of parallel-ssh -h /opt/workers -i &ldquo;systemctl status syslog-ng&rdquo;. Click to expand  root@turingpi:~# parallel-ssh -h /opt/workers -i \"systemctl status syslog-ng\" [1] 14:03:46 [SUCCESS] kemeny● syslog-ng.service - System Logger Daemon     Loaded: loaded (/lib/systemd/system/syslog-ng.service; enabled; vendor preset: enabled)     Active: active (running) since Thu 2024-03-28 13:57:01 EDT; 6min ago       Docs: man:syslog-ng(8)   Main PID: 28694 (syslog-ng)      Tasks: 2 (limit: 779)        CPU: 40.228s     CGroup: /system.slice/syslog-ng.service             └─28694 /usr/sbin/syslog-ng -FMar 28 13:57:00 kemeny systemd[1]: Starting System Logger Daemon...Mar 28 13:57:01 kemeny syslog-ng[28694]: DIGEST-MD5 common mech freeMar 28 13:57:01 kemeny systemd[1]: Started System Logger Daemon.[2] 14:03:50 [SUCCESS] vonkarman● syslog-ng.service - System Logger Daemon     Loaded: loaded (/lib/systemd/system/syslog-ng.service; enabled; vendor preset: enabled)     Active: active (running) since Thu 2024-03-28 13:57:49 EDT; 5min ago       Docs: man:syslog-ng(8)   Main PID: 27486 (syslog-ng)      Tasks: 2 (limit: 779)        CPU: 2min 5.540s     CGroup: /system.slice/syslog-ng.service             └─27486 /usr/sbin/syslog-ng -FMar 28 13:57:44 vonkarman systemd[1]: Starting System Logger Daemon...Mar 28 13:57:46 vonkarman syslog-ng[27486]: DIGEST-MD5 common mech freeMar 28 13:57:49 vonkarman systemd[1]: Started System Logger Daemon.[3] 14:03:51 [SUCCESS] teller● syslog-ng.service - System Logger Daemon     Loaded: loaded (/lib/systemd/system/syslog-ng.service; enabled; vendor preset: enabled)     Active: active (running) since Thu 2024-03-28 13:57:39 EDT; 6min ago       Docs: man:syslog-ng(8)   Main PID: 24821 (syslog-ng)      Tasks: 2 (limit: 779)        CPU: 2min 262ms     CGroup: /system.slice/syslog-ng.service             └─24821 /usr/sbin/syslog-ng -FMar 28 13:57:38 teller systemd[1]: Starting System Logger Daemon...Mar 28 13:57:38 teller syslog-ng[24821]: DIGEST-MD5 common mech freeMar 28 13:57:39 teller systemd[1]: Started System Logger Daemon.[4] 14:03:53 [SUCCESS] neumann● syslog-ng.service - System Logger Daemon     Loaded: loaded (/lib/systemd/system/syslog-ng.service; enabled; vendor preset: enabled)     Active: active (running) since Thu 2024-03-28 13:57:39 EDT; 6min ago       Docs: man:syslog-ng(8)   Main PID: 27734 (syslog-ng)      Tasks: 2 (limit: 779)        CPU: 1min 43.504s     CGroup: /system.slice/syslog-ng.service             └─27734 /usr/sbin/syslog-ng -FMar 28 13:57:38 neumann systemd[1]: Starting System Logger Daemon...Mar 28 13:57:38 neumann syslog-ng[27734]: DIGEST-MD5 common mech freeMar 28 13:57:39 neumann systemd[1]: Started System Logger Daemon.[5] 14:03:53 [SUCCESS] wigner● syslog-ng.service - System Logger Daemon     Loaded: loaded (/lib/systemd/system/syslog-ng.service; enabled; vendor preset: enabled)     Active: active (running) since Thu 2024-03-28 13:57:37 EDT; 6min ago       Docs: man:syslog-ng(8)   Main PID: 27512 (syslog-ng)      Tasks: 2 (limit: 779)        CPU: 1min 49.643s     CGroup: /system.slice/syslog-ng.service             └─27512 /usr/sbin/syslog-ng -FMar 28 13:57:36 wigner systemd[1]: Starting System Logger Daemon...Mar 28 13:57:36 wigner syslog-ng[27512]: DIGEST-MD5 common mech freeMar 28 13:57:37 wigner systemd[1]: Started System Logger Daemon.[6] 14:03:57 [SUCCESS] szilard● syslog-ng.service - System Logger Daemon     Loaded: loaded (/lib/systemd/system/syslog-ng.service; enabled; vendor preset: enabled)     Active: active (running) since Thu 2024-03-28 13:57:35 EDT; 6min ago       Docs: man:syslog-ng(8)   Main PID: 24136 (syslog-ng)      Tasks: 5 (limit: 779)        CPU: 2min 10.257s     CGroup: /system.slice/syslog-ng.service             └─24136 /usr/sbin/syslog-ng -FMar 28 13:57:34 szilard systemd[1]: Starting System Logger Daemon...Mar 28 13:57:34 szilard syslog-ng[24136]: DIGEST-MD5 common mech freeMar 28 13:57:35 szilard systemd[1]: Started System Logger Daemon.8. Create the  configuration file send.conf in /opt on host turingpi. Note that /opt is an NFS export on turingpi and is NFS mounted by all of the compute nodes. This file will set the HOST field to the local hostname for log messages that are sent. This in done in the subsequent steps where “placeholder” will be replaced using a sed operation with the local hostname. Additionally, a data source s_hpc is defined which will scan /opt/ibm/lsf/log for the presence of LSF daemon logfiles. root@turingpi:/# cat /opt/send.confrewrite r_host { set(\"placeholder\", value(\"HOST\")); };destination d_net {  syslog(\"turingpi\" port(601));};source s_hpc {  wildcard-file(      base-dir(\"/opt/ibm/lsf/log\")      filename-pattern(\"*.log.*\")      recursive(no)      follow-freq(1)  );};log {  source(s_src);  source(s_hpc);  rewrite(r_host);   destination(d_net);};On Nodes 2-7, copy the file /opt/send.conf to /etc/syslog-ng/conf.d/send.conf. root@turingpi:/# parallel-ssh -h /opt/workers -i \"cp /opt/send.conf /etc/syslog-ng/conf.d\" [1] 14:19:29 [SUCCESS] kemeny[2] 14:19:30 [SUCCESS] vonkarman[3] 14:19:30 [SUCCESS] wigner[4] 14:19:30 [SUCCESS] szilard[5] 14:19:30 [SUCCESS] teller[6] 14:19:31 [SUCCESS] neumannUsing sed, replace the “placeholder” string in /etc/syslog-ng/conf.d/send.conf with the local hostname. And we also double check that the change was correctly made. root@turingpi:/# parallel-ssh -h /opt/workers -i 'HOST=`hostname`; sed -i \"s/placeholder/$HOST/g\" /etc/syslog-ng/conf.d/send.conf' [1] 14:38:09 [SUCCESS] kemeny[2] 14:38:09 [SUCCESS] teller[3] 14:38:09 [SUCCESS] vonkarman[4] 14:38:09 [SUCCESS] wigner[5] 14:38:09 [SUCCESS] neumann[6] 14:38:09 [SUCCESS] szilard  Output of parallel-ssh -h /opt/workers -i &ldquo;cat /etc/syslog-ng/conf.d/send.conf&rdquo;. Click to expand  root@turingpi:/# parallel-ssh -h /opt/workers -i \"cat /etc/syslog-ng/conf.d/send.conf\" [1] 14:38:33 [SUCCESS] kemenyrewrite r_host { set(\"kemeny\", value(\"HOST\")); };destination d_net {  syslog(\"turingpi\" port(601));};source s_hpc {  wildcard-file(      base-dir(\"/opt/ibm/lsf/log\")      filename-pattern(\"*.log.*\")      recursive(no)      follow-freq(1)  );};log {  source(s_sys);  source(s_hpc);  rewrite(r_host);   destination(d_net);};[2] 14:38:33 [SUCCESS] tellerrewrite r_host { set(\"teller\", value(\"HOST\")); };destination d_net {  syslog(\"turingpi\" port(601));};source s_hpc {  wildcard-file(      base-dir(\"/opt/ibm/lsf/log\")      filename-pattern(\"*.log.*\")      recursive(no)      follow-freq(1)  );};log {  source(s_sys);  source(s_hpc);  rewrite(r_host);   destination(d_net);};[3] 14:38:33 [SUCCESS] neumannrewrite r_host { set(\"neumann\", value(\"HOST\")); };destination d_net {  syslog(\"turingpi\" port(601));};source s_hpc {  wildcard-file(      base-dir(\"/opt/ibm/lsf/log\")      filename-pattern(\"*.log.*\")      recursive(no)      follow-freq(1)  );};log {  source(s_sys);  source(s_hpc);  rewrite(r_host);   destination(d_net);};[4] 14:38:33 [SUCCESS] szilardrewrite r_host { set(\"szilard\", value(\"HOST\")); };destination d_net {  syslog(\"turingpi\" port(601));};source s_hpc {  wildcard-file(      base-dir(\"/opt/ibm/lsf/log\")      filename-pattern(\"*.log.*\")      recursive(no)      follow-freq(1)  );};log {  source(s_sys);  source(s_hpc);  rewrite(r_host);   destination(d_net);};[5] 14:38:33 [SUCCESS] wignerrewrite r_host { set(\"wigner\", value(\"HOST\")); };destination d_net {  syslog(\"turingpi\" port(601));};source s_hpc {  wildcard-file(      base-dir(\"/opt/ibm/lsf/log\")      filename-pattern(\"*.log.*\")      recursive(no)      follow-freq(1)  );};log {  source(s_sys);  source(s_hpc);  rewrite(r_host);   destination(d_net);};[6] 14:38:33 [SUCCESS] vonkarmanrewrite r_host { set(\"vonkarman\", value(\"HOST\")); };destination d_net {  syslog(\"turingpi\" port(601));};source s_hpc {  wildcard-file(      base-dir(\"/opt/ibm/lsf/log\")      filename-pattern(\"*.log.*\")      recursive(no)      follow-freq(1)  );};log {  source(s_sys);  source(s_hpc);  rewrite(r_host);   destination(d_net);};11. Finally, syslog-ng is restarted on Nodes 2-7 and the status of the service is checked to ensure that there are no errors. root@turingpi:/opt# parallel-ssh -h /opt/workers -i \"systemctl restart syslog-ng\" [1] 14:49:03 [SUCCESS] kemeny[2] 14:49:05 [SUCCESS] szilard[3] 14:49:06 [SUCCESS] vonkarman[4] 14:49:06 [SUCCESS] neumann[5] 14:49:06 [SUCCESS] teller[6] 14:49:07 [SUCCESS] wigner  Output of parallel-ssh -h /opt/workers -i &ldquo;systemctl status syslog-ng&rdquo;. Click to expand  root@turingpi:/opt# parallel-ssh -h /opt/workers -i \"systemctl status syslog-ng\" [1] 14:49:31 [SUCCESS] kemeny● syslog-ng.service - System Logger Daemon     Loaded: loaded (/lib/systemd/system/syslog-ng.service; enabled; vendor preset: enabled)     Active: active (running) since Thu 2024-03-28 14:49:03 EDT; 28s ago       Docs: man:syslog-ng(8)   Main PID: 34982 (syslog-ng)      Tasks: 2 (limit: 779)        CPU: 398ms     CGroup: /system.slice/syslog-ng.service             └─34982 /usr/sbin/syslog-ng -FMar 28 14:49:02 kemeny systemd[1]: Starting System Logger Daemon...Mar 28 14:49:02 kemeny syslog-ng[34982]: DIGEST-MD5 common mech freeMar 28 14:49:03 kemeny systemd[1]: Started System Logger Daemon.[2] 14:49:33 [SUCCESS] vonkarman● syslog-ng.service - System Logger Daemon     Loaded: loaded (/lib/systemd/system/syslog-ng.service; enabled; vendor preset: enabled)     Active: active (running) since Thu 2024-03-28 14:49:06 EDT; 25s ago       Docs: man:syslog-ng(8)   Main PID: 33710 (syslog-ng)      Tasks: 2 (limit: 779)        CPU: 934ms     CGroup: /system.slice/syslog-ng.service             └─33710 /usr/sbin/syslog-ng -FMar 28 14:49:03 vonkarman systemd[1]: Starting System Logger Daemon...Mar 28 14:49:03 vonkarman syslog-ng[33710]: DIGEST-MD5 common mech freeMar 28 14:49:06 vonkarman systemd[1]: Started System Logger Daemon.[3] 14:49:33 [SUCCESS] neumann● syslog-ng.service - System Logger Daemon     Loaded: loaded (/lib/systemd/system/syslog-ng.service; enabled; vendor preset: enabled)     Active: active (running) since Thu 2024-03-28 14:49:06 EDT; 25s ago       Docs: man:syslog-ng(8)   Main PID: 34000 (syslog-ng)      Tasks: 2 (limit: 779)        CPU: 959ms     CGroup: /system.slice/syslog-ng.service             └─34000 /usr/sbin/syslog-ng -FMar 28 14:49:03 neumann systemd[1]: Starting System Logger Daemon...Mar 28 14:49:03 neumann syslog-ng[34000]: DIGEST-MD5 common mech freeMar 28 14:49:06 neumann systemd[1]: Started System Logger Daemon.[4] 14:49:33 [SUCCESS] wigner● syslog-ng.service - System Logger Daemon     Loaded: loaded (/lib/systemd/system/syslog-ng.service; enabled; vendor preset: enabled)     Active: active (running) since Thu 2024-03-28 14:49:07 EDT; 25s ago       Docs: man:syslog-ng(8)   Main PID: 33941 (syslog-ng)      Tasks: 2 (limit: 779)        CPU: 1.115s     CGroup: /system.slice/syslog-ng.service             └─33941 /usr/sbin/syslog-ng -FMar 28 14:49:03 wigner systemd[1]: Starting System Logger Daemon...Mar 28 14:49:04 wigner syslog-ng[33941]: DIGEST-MD5 common mech freeMar 28 14:49:07 wigner systemd[1]: Started System Logger Daemon.[5] 14:49:34 [SUCCESS] szilard● syslog-ng.service - System Logger Daemon     Loaded: loaded (/lib/systemd/system/syslog-ng.service; enabled; vendor preset: enabled)     Active: active (running) since Thu 2024-03-28 14:49:05 EDT; 26s ago       Docs: man:syslog-ng(8)   Main PID: 30348 (syslog-ng)      Tasks: 2 (limit: 779)        CPU: 816ms     CGroup: /system.slice/syslog-ng.service             └─30348 /usr/sbin/syslog-ng -FMar 28 14:49:03 szilard systemd[1]: Starting System Logger Daemon...Mar 28 14:49:03 szilard syslog-ng[30348]: DIGEST-MD5 common mech freeMar 28 14:49:05 szilard systemd[1]: Started System Logger Daemon.[6] 14:49:34 [SUCCESS] teller● syslog-ng.service - System Logger Daemon     Loaded: loaded (/lib/systemd/system/syslog-ng.service; enabled; vendor preset: enabled)     Active: active (running) since Thu 2024-03-28 14:49:06 EDT; 25s ago       Docs: man:syslog-ng(8)   Main PID: 31034 (syslog-ng)      Tasks: 2 (limit: 779)        CPU: 965ms     CGroup: /system.slice/syslog-ng.service             └─31034 /usr/sbin/syslog-ng -FDoes it work?The answer to this question is an emphatic YES!Let’s begin with a simple test running the logger command on all of the compute nodes, while monitoring /var/log/fromnet on host turingpi. root@turingpi:/home/lsfadmin# date; parallel-ssh -h /opt/workers -i 'HOST=`hostname`; logger This is a test from node $HOST. Do not panic!' Wed  3 Apr 21:41:45 EDT 2024 [1] 21:41:46 [SUCCESS] teller [2] 21:41:46 [SUCCESS] neumann [3] 21:41:46 [SUCCESS] wigner [4] 21:41:46 [SUCCESS] kemeny [5] 21:41:46 [SUCCESS] szilard [6] 21:41:46 [SUCCESS] vonkarmanroot@turingpi:/var/log# tail -f fromnet |grep panic Apr  3 21:41:46 szilard root[10918]: This is a test from node szilard. Do not panic! Apr  3 21:41:46 wigner root[11011]: This is a test from node wigner. Do not panic! Apr  3 21:41:46 neumann root[11121]: This is a test from node neumann. Do not panic! Apr  3 21:41:46 kemeny root[11029]: This is a test from node kemeny. Do not panic! Apr  3 21:41:46 teller root[10875]: This is a test from node teller. Do not panic! Apr  3 21:41:46 vonkarman root[10805]: This is a test from node vonkarman. Do not panic!Next, let’s look at whether the LSF logging is also captured. Here we simply restart the LSF daemons on Nodes 2-7 and monitor the /var/log/fromnet file. The full output can be viewed below.  Output of tail -f /var/log/fromnet. Click to expand  root@turingpi:/var/log# tail -f fromnet Apr  3 21:41:57 vonkarman systemd[10786]: systemd-exit.service: Succeeded. Apr  3 21:41:57 vonkarman systemd[10786]: Finished Exit the Session. Apr  3 21:41:57 vonkarman systemd[10786]: Reached target Exit the Session. Apr  3 21:41:57 vonkarman systemd[1]: user@0.service: Succeeded. Apr  3 21:41:57 vonkarman systemd[1]: Stopped User Manager for UID 0. Apr  3 21:41:57 vonkarman systemd[1]: Stopping User Runtime Directory /run/user/0... Apr  3 21:41:57 vonkarman systemd[1]: run-user-0.mount: Succeeded. Apr  3 21:41:57 vonkarman systemd[1]: user-runtime-dir@0.service: Succeeded. Apr  3 21:41:57 vonkarman systemd[1]: Stopped User Runtime Directory /run/user/0. Apr  3 21:41:57 vonkarman systemd[1]: Removed slice User Slice of UID 0. Apr  3 21:44:30 wigner dhcpcd[493]: eth0: Router Advertisement from fe80::da58:d7ff:fe00:6d83 Apr  3 21:44:57 szilard sshd[11234]: Accepted publickey for root from 192.168.1.172 port 52600 ssh2: ED25519 SHA256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Apr  3 21:44:57 szilard sshd[11234]: pam_unix(sshd:session): session opened for user root(uid=0) by (uid=0) Apr  3 21:44:58 szilard systemd[1]: Created slice User Slice of UID 0. Apr  3 21:44:58 szilard systemd[1]: Starting User Runtime Directory /run/user/0... Apr  3 21:44:58 szilard systemd-logind[382]: New session 30 of user root. Apr  3 21:44:58 szilard systemd[1]: Finished User Runtime Directory /run/user/0. Apr  3 21:44:58 szilard systemd[1]: Starting User Manager for UID 0... Apr  3 21:44:58 szilard systemd[11237]: pam_unix(systemd-user:session): session opened for user root(uid=0) by(uid=0) Apr  3 21:44:57 wigner sshd[11342]: Accepted publickey for root from 192.168.1.172 port 60388 ssh2: ED25519 SHA256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Apr  3 21:44:57 wigner sshd[11342]: pam_unix(sshd:session): session opened for user root(uid=0) by (uid=0) Apr  3 21:44:58 wigner systemd[1]: Created slice User Slice of UID 0. Apr  3 21:44:58 wigner systemd[1]: Starting User Runtime Directory /run/user/0... Apr  3 21:44:58 wigner systemd-logind[383]: New session 30 of user root. Apr  3 21:44:58 wigner systemd[1]: Finished User Runtime Directory /run/user/0. Apr  3 21:44:58 wigner systemd[1]: Starting User Manager for UID 0... Apr  3 21:44:58 wigner systemd[11345]: pam_unix(systemd-user:session): session opened for user root(uid=0) by (uid=0) Apr  3 21:44:57 neumann sshd[11436]: Accepted publickey for root from 192.168.1.172 port 55144 ssh2: ED25519 SHA256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Apr  3 21:44:57 neumann sshd[11436]: pam_unix(sshd:session): session opened for user root(uid=0) by (uid=0) Apr  3 21:44:57 neumann systemd[1]: Created slice User Slice of UID 0. Apr  3 21:44:57 neumann systemd[1]: Starting User Runtime Directory /run/user/0... Apr  3 21:44:58 neumann systemd-logind[398]: New session 30 of user root. Apr  3 21:44:58 neumann systemd[1]: Finished User Runtime Directory /run/user/0. Apr  3 21:44:58 neumann systemd[1]: Starting User Manager for UID 0... Apr  3 21:44:58 neumann systemd[11439]: pam_unix(systemd-user:session): session opened for user root(uid=0) by(uid=0) Apr  3 21:44:57 kemeny sshd[11345]: Accepted publickey for root from 192.168.1.172 port 59830 ssh2: ED25519 SHA256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Apr  3 21:44:57 kemeny sshd[11345]: pam_unix(sshd:session): session opened for user root(uid=0) by (uid=0) Apr  3 21:44:58 kemeny systemd[1]: Created slice User Slice of UID 0. Apr  3 21:44:58 kemeny systemd[1]: Starting User Runtime Directory /run/user/0... Apr  3 21:44:58 kemeny systemd-logind[386]: New session 30 of user root. Apr  3 21:44:58 kemeny systemd[1]: Finished User Runtime Directory /run/user/0. Apr  3 21:44:58 kemeny systemd[1]: Starting User Manager for UID 0... Apr  3 21:44:58 kemeny systemd[11348]: pam_unix(systemd-user:session): session opened for user root(uid=0) by (uid=0) Apr  3 21:44:57 teller sshd[11189]: Accepted publickey for root from 192.168.1.172 port 35310 ssh2: ED25519 SHA256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Apr  3 21:44:57 teller sshd[11189]: pam_unix(sshd:session): session opened for user root(uid=0) by (uid=0) Apr  3 21:44:58 teller systemd[1]: Created slice User Slice of UID 0. Apr  3 21:44:58 teller systemd[1]: Starting User Runtime Directory /run/user/0... Apr  3 21:44:58 teller systemd-logind[382]: New session 30 of user root. Apr  3 21:44:58 teller systemd[1]: Finished User Runtime Directory /run/user/0. Apr  3 21:44:58 teller systemd[1]: Starting User Manager for UID 0... Apr  3 21:44:58 teller systemd[11192]: pam_unix(systemd-user:session): session opened for user root(uid=0) by (uid=0) Apr  3 21:44:57 vonkarman sshd[11118]: Accepted publickey for root from 192.168.1.172 port 48654 ssh2: ED25519SHA256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Apr  3 21:44:58 vonkarman sshd[11118]: pam_unix(sshd:session): session opened for user root(uid=0) by (uid=0) Apr  3 21:44:58 vonkarman systemd[1]: Created slice User Slice of UID 0. Apr  3 21:44:58 vonkarman systemd[1]: Starting User Runtime Directory /run/user/0... Apr  3 21:44:58 vonkarman systemd-logind[382]: New session 29 of user root. Apr  3 21:44:58 vonkarman systemd[1]: Finished User Runtime Directory /run/user/0. Apr  3 21:44:58 vonkarman systemd[1]: Starting User Manager for UID 0... Apr  3 21:44:58 vonkarman systemd[11121]: pam_unix(systemd-user:session): session opened for user root(uid=0) by (uid=0) Apr  3 21:44:58 neumann systemd[11439]: Queued start job for default target Main User Target. Apr  3 21:44:58 neumann systemd[11439]: Created slice User Application Slice. Apr  3 21:44:58 neumann systemd[11439]: Reached target Paths. Apr  3 21:44:58 neumann systemd[11439]: Reached target Timers. Apr  3 21:44:58 neumann systemd[11439]: Listening on GnuPG network certificate management daemon. Apr  3 21:44:58 neumann systemd[11439]: Listening on GnuPG cryptographic agent and passphrase cache (access for web browsers). Apr  3 21:44:58 neumann systemd[11439]: Listening on GnuPG cryptographic agent and passphrase cache (restricted). Apr  3 21:44:58 neumann systemd[11439]: Listening on GnuPG cryptographic agent (ssh-agent emulation). Apr  3 21:44:58 neumann systemd[11439]: Listening on GnuPG cryptographic agent and passphrase cache. Apr  3 21:44:58 neumann systemd[11439]: Reached target Sockets. Apr  3 21:44:58 neumann systemd[11439]: Reached target Basic System. Apr  3 21:44:58 neumann systemd[11439]: Reached target Main User Target. Apr  3 21:44:58 neumann systemd[11439]: Startup finished in 379ms. Apr  3 21:44:58 neumann systemd[1]: Started User Manager for UID 0. Apr  3 21:44:58 neumann systemd[1]: Started Session 30 of user root. Apr  3 21:44:58 teller systemd[11192]: Queued start job for default target Main User Target. Apr  3 21:44:58 teller systemd[11192]: Created slice User Application Slice. Apr  3 21:44:58 teller systemd[11192]: Reached target Paths. Apr  3 21:44:58 teller systemd[11192]: Reached target Timers. Apr  3 21:44:58 teller systemd[11192]: Listening on GnuPG network certificate management daemon. Apr  3 21:44:58 teller systemd[11192]: Listening on GnuPG cryptographic agent and passphrase cache (access forweb browsers). Apr  3 21:44:58 teller systemd[11192]: Listening on GnuPG cryptographic agent and passphrase cache (restricted). Apr  3 21:44:58 teller systemd[11192]: Listening on GnuPG cryptographic agent (ssh-agent emulation). Apr  3 21:44:58 teller systemd[11192]: Listening on GnuPG cryptographic agent and passphrase cache. Apr  3 21:44:58 teller systemd[11192]: Reached target Sockets. Apr  3 21:44:58 teller systemd[11192]: Reached target Basic System. Apr  3 21:44:58 teller systemd[11192]: Reached target Main User Target. Apr  3 21:44:58 teller systemd[11192]: Startup finished in 373ms. Apr  3 21:44:58 teller systemd[1]: Started User Manager for UID 0. Apr  3 21:44:58 teller systemd[1]: Started Session 30 of user root. Apr  3 21:44:58 vonkarman systemd[11121]: Queued start job for default target Main User Target. Apr  3 21:44:58 vonkarman systemd[11121]: Created slice User Application Slice. Apr  3 21:44:58 vonkarman systemd[11121]: Reached target Paths. Apr  3 21:44:58 vonkarman systemd[11121]: Reached target Timers. Apr  3 21:44:58 vonkarman systemd[11121]: Listening on GnuPG network certificate management daemon. Apr  3 21:44:58 vonkarman systemd[11121]: Listening on GnuPG cryptographic agent and passphrase cache (access for web browsers). Apr  3 21:44:58 vonkarman systemd[11121]: Listening on GnuPG cryptographic agent and passphrase cache (restricted). Apr  3 21:44:58 vonkarman systemd[11121]: Listening on GnuPG cryptographic agent (ssh-agent emulation). Apr  3 21:44:58 vonkarman systemd[11121]: Listening on GnuPG cryptographic agent and passphrase cache. Apr  3 21:44:58 vonkarman systemd[11121]: Reached target Sockets. Apr  3 21:44:58 vonkarman systemd[11121]: Reached target Basic System. Apr  3 21:44:58 vonkarman systemd[11121]: Reached target Main User Target. Apr  3 21:44:58 vonkarman systemd[11121]: Startup finished in 392ms. Apr  3 21:44:58 vonkarman systemd[1]: Started User Manager for UID 0. Apr  3 21:44:58 vonkarman systemd[1]: Started Session 29 of user root. Apr  3 21:44:58 szilard systemd[11237]: Queued start job for default target Main User Target. Apr  3 21:44:58 szilard systemd[11237]: Created slice User Application Slice. Apr  3 21:44:58 szilard systemd[11237]: Reached target Paths. Apr  3 21:44:58 szilard systemd[11237]: Reached target Timers. Apr  3 21:44:58 szilard systemd[11237]: Listening on GnuPG network certificate management daemon. Apr  3 21:44:58 szilard systemd[11237]: Listening on GnuPG cryptographic agent and passphrase cache (access for web browsers). Apr  3 21:44:58 szilard systemd[11237]: Listening on GnuPG cryptographic agent and passphrase cache (restricted). Apr  3 21:44:58 szilard systemd[11237]: Listening on GnuPG cryptographic agent (ssh-agent emulation). Apr  3 21:44:58 szilard systemd[11237]: Listening on GnuPG cryptographic agent and passphrase cache. Apr  3 21:44:58 szilard systemd[11237]: Reached target Sockets. Apr  3 21:44:58 szilard systemd[11237]: Reached target Basic System. Apr  3 21:44:58 szilard systemd[11237]: Reached target Main User Target. Apr  3 21:44:58 szilard systemd[11237]: Startup finished in 385ms. Apr  3 21:44:58 szilard systemd[1]: Started User Manager for UID 0. Apr  3 21:44:58 szilard systemd[1]: Started Session 30 of user root. Apr  3 21:44:58 wigner systemd[11345]: Queued start job for default target Main User Target. Apr  3 21:44:58 wigner systemd[11345]: Created slice User Application Slice. Apr  3 21:44:58 wigner systemd[11345]: Reached target Paths. Apr  3 21:44:58 wigner systemd[11345]: Reached target Timers. Apr  3 21:44:58 wigner systemd[11345]: Listening on GnuPG network certificate management daemon. Apr  3 21:44:58 wigner systemd[11345]: Listening on GnuPG cryptographic agent and passphrase cache (access forweb browsers). Apr  3 21:44:58 wigner systemd[11345]: Listening on GnuPG cryptographic agent and passphrase cache (restricted). Apr  3 21:44:58 wigner systemd[11345]: Listening on GnuPG cryptographic agent (ssh-agent emulation). Apr  3 21:44:58 wigner systemd[11345]: Listening on GnuPG cryptographic agent and passphrase cache. Apr  3 21:44:58 wigner systemd[11345]: Reached target Sockets. Apr  3 21:44:58 wigner systemd[11345]: Reached target Basic System. Apr  3 21:44:58 wigner systemd[11345]: Reached target Main User Target. Apr  3 21:44:58 wigner systemd[11345]: Startup finished in 375ms. Apr  3 21:44:58 wigner systemd[1]: Started User Manager for UID 0. Apr  3 21:44:58 wigner systemd[1]: Started Session 30 of user root. Apr  3 21:44:58 kemeny systemd[11348]: Queued start job for default target Main User Target. Apr  3 21:44:58 kemeny systemd[11348]: Created slice User Application Slice. Apr  3 21:44:58 kemeny systemd[11348]: Reached target Paths. Apr  3 21:44:58 kemeny systemd[11348]: Reached target Timers. Apr  3 21:44:58 kemeny systemd[11348]: Listening on GnuPG network certificate management daemon. Apr  3 21:44:58 kemeny systemd[11348]: Listening on GnuPG cryptographic agent and passphrase cache (access forweb browsers). Apr  3 21:44:58 kemeny systemd[11348]: Listening on GnuPG cryptographic agent and passphrase cache (restricted). Apr  3 21:44:58 kemeny systemd[11348]: Listening on GnuPG cryptographic agent (ssh-agent emulation). Apr  3 21:44:58 kemeny systemd[11348]: Listening on GnuPG cryptographic agent and passphrase cache. Apr  3 21:44:58 kemeny systemd[11348]: Reached target Sockets. Apr  3 21:44:58 kemeny systemd[11348]: Reached target Basic System. Apr  3 21:44:58 kemeny systemd[11348]: Reached target Main User Target. Apr  3 21:44:58 kemeny systemd[11348]: Startup finished in 400ms. Apr  3 21:44:58 kemeny systemd[1]: Started User Manager for UID 0. Apr  3 21:44:58 kemeny systemd[1]: Started Session 30 of user root. Apr  3 21:44:59 kemeny res[691]: term_handler: Received signal 15, exiting Apr  3 21:44:59 kemeny lim[688]: term_handler: Received signal 15, exiting Apr  3 21:44:59 kemeny sbatchd[693]: Daemon on host &lt;kemeny&gt; received signal &lt;15&gt;; exiting Apr  3 21:44:59 kemeny lsf_daemons[11434]: Stopping the LSF subsystem Apr  3 21:44:59 kemeny systemd[1]: lsfd.service: Succeeded. Apr  3 21:44:59 kemeny systemd[1]: lsfd.service: Consumed 11min 56.744s CPU time. Apr  3 21:44:59 szilard lim[685]: term_handler: Received signal 15, exiting Apr  3 21:44:59 szilard res[687]: term_handler: Received signal 15, exiting Apr  3 21:44:59 szilard sbatchd[689]: Daemon on host &lt;szilard&gt; received signal &lt;15&gt;; exiting Apr  3 21:44:59 vonkarman lim[686]: term_handler: Received signal 15, exiting Apr  3 21:44:59 vonkarman sbatchd[690]: Daemon on host &lt;vonkarman&gt; received signal &lt;15&gt;; exiting Apr  3 21:44:59 vonkarman res[688]: term_handler: Received signal 15, exiting Apr  3 21:44:59 teller lim[683]: term_handler: Received signal 15, exiting Apr  3 21:44:59 teller res[689]: term_handler: Received signal 15, exiting Apr  3 21:44:59 teller sbatchd[691]: Daemon on host &lt;teller&gt; received signal &lt;15&gt;; exiting Apr  3 21:44:59 teller lsf_daemons[11294]: Stopping the LSF subsystem Apr  3 21:44:59 wigner lim[719]: term_handler: Received signal 15, exiting Apr  3 21:44:59 wigner res[722]: term_handler: Received signal 15, exiting Apr  3 21:44:59 wigner sbatchd[724]: Daemon on host &lt;wigner&gt; received signal &lt;15&gt;; exiting Apr  3 21:44:59 wigner lsf_daemons[11438]: Stopping the LSF subsystem Apr  3 21:44:59 neumann res[713]: term_handler: Received signal 15, exiting Apr  3 21:44:59 neumann sbatchd[715]: Daemon on host &lt;neumann&gt; received signal &lt;15&gt;; exiting Apr  3 21:44:59 neumann lim[711]: term_handler: Received signal 15, exiting Apr  3 21:44:59 neumann lsf_daemons[11540]: Stopping the LSF subsystem Apr  3 21:44:59 neumann sshd[11436]: Received disconnect from 192.168.1.172 port 55144:11: disconnected by user Apr  3 21:44:59 neumann sshd[11436]: Disconnected from user root 192.168.1.172 port 55144 Apr  3 21:44:59 szilard lsf_daemons[11331]: Stopping the LSF subsystem Apr  3 21:44:59 szilard sshd[11234]: Received disconnect from 192.168.1.172 port 52600:11: disconnected by user Apr  3 21:44:59 szilard sshd[11234]: Disconnected from user root 192.168.1.172 port 52600 Apr  3 21:44:59 szilard sshd[11234]: pam_unix(sshd:session): session closed for user root Apr  3 21:44:59 szilard res[11357]: res/get_hostInfo: ls_gethostinfo() failed. Server host LIM configuration is not ready yet. Apr  3 21:44:59 szilard systemd-logind[382]: Session 30 logged out. Waiting for processes to exit. Apr  3 21:44:59 szilard res[11357]: cg_load_hierarchies: Please use the LSF package with higher glibc version to enable LSF cgroup v2 support. Apr  3 21:44:59 szilard systemd[1]: lsfd.service: Succeeded. Apr  3 21:44:59 szilard systemd[1]: lsfd.service: Consumed 1h 17min 44.040s CPU time. Apr  3 21:44:59 neumann sshd[11436]: pam_unix(sshd:session): session closed for user root Apr  3 21:44:59 neumann systemd-logind[398]: Session 30 logged out. Waiting for processes to exit. Apr  3 21:44:59 neumann res[11559]: res/get_hostInfo: ls_gethostinfo() failed. Server host LIM configuration is not ready yet. Apr  3 21:44:59 neumann res[11559]: cg_load_hierarchies: Please use the LSF package with higher glibc version to enable LSF cgroup v2 support. Apr  3 21:44:59 neumann systemd[1]: lsfd.service: Succeeded. Apr  3 21:44:59 neumann systemd[1]: lsfd.service: Consumed 1h 17min 21.135s CPU time. Apr  3 21:44:59 teller sshd[11189]: Received disconnect from 192.168.1.172 port 35310:11: disconnected by user Apr  3 21:44:59 teller sshd[11189]: Disconnected from user root 192.168.1.172 port 35310 Apr  3 21:44:59 teller sshd[11189]: pam_unix(sshd:session): session closed for user root Apr  3 21:44:59 teller systemd-logind[382]: Session 30 logged out. Waiting for processes to exit. Apr  3 21:44:59 teller res[11307]: res/get_hostInfo: ls_gethostinfo() failed. Server host LIM configuration isnot ready yet. Apr  3 21:44:59 teller res[11307]: cg_load_hierarchies: Please use the LSF package with higher glibc version to enable LSF cgroup v2 support. Apr  3 21:44:59 teller res[11307]: term_handler: Received signal 15, exiting Apr  3 21:44:59 teller lim[11305]: term_handler: Received signal 15, exiting Apr  3 21:44:59 teller systemd[1]: lsfd.service: Succeeded. Apr  3 21:44:59 teller systemd[1]: lsfd.service: Consumed 1h 17min 47.675s CPU time. Apr  3 21:44:59 teller sbatchd[11309]: cg_load_hierarchies: Please use the LSF package with higher glibc version to enable LSF cgroup v2 support. Apr  3 21:44:59 kemeny sshd[11345]: Received disconnect from 192.168.1.172 port 59830:11: disconnected by user Apr  3 21:44:59 kemeny sshd[11345]: Disconnected from user root 192.168.1.172 port 59830 Apr  3 21:44:59 kemeny sshd[11345]: pam_unix(sshd:session): session closed for user root Apr  3 21:44:59 kemeny systemd-logind[386]: Session 30 logged out. Waiting for processes to exit. Apr  3 21:44:59 kemeny res[11467]: res/get_hostInfo: ls_gethostinfo() failed. Server host LIM configuration isnot ready yet. Apr  3 21:44:59 kemeny res[11467]: cg_load_hierarchies: Please use the LSF package with higher glibc version to enable LSF cgroup v2 support. Apr  3 21:44:59 vonkarman lsf_daemons[11215]: Stopping the LSF subsystem Apr  3 21:44:59 vonkarman sshd[11118]: Received disconnect from 192.168.1.172 port 48654:11: disconnected by user Apr  3 21:44:59 vonkarman sshd[11118]: Disconnected from user root 192.168.1.172 port 48654 Apr  3 21:44:59 vonkarman sshd[11118]: pam_unix(sshd:session): session closed for user root Apr  3 21:44:59 vonkarman systemd-logind[382]: Session 29 logged out. Waiting for processes to exit. Apr  3 21:44:59 vonkarman res[11241]: res/get_hostInfo: ls_gethostinfo() failed. Server host LIM configurationis not ready yet. Apr  3 21:44:59 vonkarman res[11241]: cg_load_hierarchies: Please use the LSF package with higher glibc version to enable LSF cgroup v2 support. Apr  3 21:44:59 vonkarman systemd[1]: lsfd.service: Succeeded. Apr  3 21:44:59 vonkarman systemd[1]: lsfd.service: Consumed 1h 17min 34.650s CPU time. Apr  3 21:44:59 wigner sshd[11342]: Received disconnect from 192.168.1.172 port 60388:11: disconnected by user Apr  3 21:44:59 wigner sshd[11342]: Disconnected from user root 192.168.1.172 port 60388 Apr  3 21:44:59 wigner sshd[11342]: pam_unix(sshd:session): session closed for user root Apr  3 21:44:59 wigner res[11464]: res/get_hostInfo: ls_gethostinfo() failed. Server host LIM configuration isnot ready yet. Apr  3 21:44:59 wigner systemd-logind[383]: Session 30 logged out. Waiting for processes to exit. Apr  3 21:44:59 wigner res[11464]: cg_load_hierarchies: Please use the LSF package with higher glibc version to enable LSF cgroup v2 support. Apr  3 21:44:59 wigner systemd[1]: lsfd.service: Succeeded. Apr  3 21:44:59 wigner systemd[1]: lsfd.service: Consumed 1h 17min 44.610s CPU time.As expected, we observed that LSF log messages are written to the fromnet file. And importantly each entry contains the hostname, so that we can identify the origin of the message.ConclusionWhat started out as a chat about logging, grew into an idea of a blog, for which I am thankful for the collaboration of Peter. We’ve illustrated an example here of how to setup centralized logging on a Turing Pi system with syslog-ng to collect system and LSF logs.Of course collecting log messages centrally is just the start of a journey. It is an important step as it allows for significantly easier debugging and troubleshooting. You can store logs to databases for easier search. And once you better understand which log messages are important, you can even potentially parse those and generate alersts from them or dashboards. All of these help you to make sure that your HPC system runs smoothly and with minimal downtime. For me this was a learning experience and I&rsquo;ll be looking how I can implement more broadly centralized logging in my home network.",
            "content_html": "<p>Logs are one of those indispensable things in IT when things go wrong. Having worked in technical support for software products in a past life, I’ve likely looked at hundreds (or more) logs over the years, helping to identify issues. So, I really appreciate the importance of logs, but I can honestly say that I never really thought about a logging strategy for the systems on my home network - primarily those running Linux.</p><p>One of my longtime friends, <a href=\"https://peter.czanik.hu/\">Peter Czanik</a>, who also works in IT, happens to be a logging guru as well as an IBM Champion for Power Systems (yeah!). So it’s only natural that we get to talking about logging. He is often complaining that even at IT security conferences people are unaware of the importance of central logging. So, why is it so important? For security it’s obvious: logs are stored independently from the compromised system, so they cannot be modified or deleted by the attacker. But central logging is beneficial for the HPC operator as well. First of all, it’s availability. You can read the logs even if one of your nodes becomes unreachable. Instead of trying to breath life into the failed node, you can just take a look at the logs and see a broken hard drive, or a similar deadly problem. And it is also convenience, as all logs are available at a single location. Logging into each node on the 3 node cluster to check locally saved logs is inconvenient but doable. On a 10 node cluster it takes a long time. On a 100 node cluster a couple of working days. While, if your logs are collected to a central location, maybe a single grep command, or search in a Kibana or similar web interface.</p><p>Those who follow my blog will know that I’ve been tinkering with a Turing Pi V1 system lately. You can read my latest post <a href=\"https://www.gaborsamu.com/blog/turingpi_noctua/\">here</a>. For me, the Turing Pi has always been a cluster in a box. My Turing Pi is fully populated with 7 compute modules. I’ve designed Node 1 to be the NFS server and LSF manager for the cluster. LSF is a workload scheduler for high-performance computing (HPC) from IBM. Naturally I turned to Peter for his guidance on this, and the result is this blog. Peter recommended that I  use <a href=\"https://www.syslog-ng.com/\">syslog-ng</a> for log aggregation and also helped me through some of my first steps with <em>syslog-ng</em>. And the goal was to aggregate both the system (syslog) as well as LSF logs on Node 1. TL;DR it was easy to get it all working. But I encourage you to read on to better understand the nuances and necessary configuration both syslog-ng and LSF that was needed.</p><p><strong>The environment</strong></p><p>The following software has been deployed on the Turing Pi:</p><ul><li>Raspberry Pi OS (<em>2023-02-21-raspios-bullseye-arm64-lite.img</em>)</li><li>syslog-ng 3 – (3.28.1 as supplied with Raspberry Pi OS)</li><li>IBM LSF Standard Edition V10.1.0.13</li></ul><p>The Turing Pi system is configured as follows:</p><p>Node 1 (<em>turingpi</em>) is the manager node of this cluster in a box and has by far the most storage. Naturally we want to use that as the centralized logging server.</p><hr /><table><thead><tr><th><strong>Node</strong></th><th><strong>Hostname</strong></th><th><strong>Hardware</strong></th><th><strong>Notes</strong></th></tr></thead><tbody><tr><td>1</td><td>turingpi</td><td>CM3+</td><td>LSF manager, NFS server, 128GB SDcard</td></tr><tr><td>2</td><td>kemeny</td><td>CM3</td><td>4GB eMMC flash</td></tr><tr><td>3</td><td>neumann</td><td>CM3+</td><td>8GB SDcard</td></tr><tr><td>4</td><td>szilard</td><td>CM3+</td><td>8GB SDcard</td></tr><tr><td>5</td><td>teller</td><td>CM3+</td><td>8GB SDcard</td></tr><tr><td>6</td><td>vonkarman</td><td>CM3+</td><td>8GB SDcard</td></tr><tr><td>7</td><td>wigner</td><td>CM3+</td><td>8GB SDcard</td></tr></tbody></table><hr /><p><strong>Syslog-ng &amp; LSF setup</strong></p><ol><li>Raspberry Pi OS configures <em>rsyslog</em> out of the box. The first step is to install <em>syslog-ng</em> on Node 1 in the environment. Note that installing syslog-ng automatically disables <em>rsyslog</em> on the nodes.</li></ol><p><details>  <strong>Output of <em>apt update; apt-get install syslog-ng -y</em>. Click to expand</strong>  <div class=\"highlight\"><pre><code class=\"language-python\">root<span style=\"color: #a6e22e;\">@turingpi</span>:<span style=\"color: #f92672;\">~</span><span style=\"color: #75715e;\"># apt update; apt-get install syslog-ng -y </span>Hit:<span style=\"color: #ae81ff;\">1</span> http:<span style=\"color: #f92672;\">//</span>security<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian<span style=\"color: #f92672;\">-</span>security bullseye<span style=\"color: #f92672;\">-</span>security InReleaseHit:<span style=\"color: #ae81ff;\">2</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye InRelease                                                        Hit:<span style=\"color: #ae81ff;\">3</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">-</span>updates InRelease                                                Hit:<span style=\"color: #ae81ff;\">4</span> https:<span style=\"color: #f92672;\">//</span>repos<span style=\"color: #f92672;\">.</span>influxdata<span style=\"color: #f92672;\">.</span>com<span style=\"color: #f92672;\">/</span>debian stable InRelease                                                   Hit:<span style=\"color: #ae81ff;\">5</span> https:<span style=\"color: #f92672;\">//</span>repos<span style=\"color: #f92672;\">.</span>influxdata<span style=\"color: #f92672;\">.</span>com<span style=\"color: #f92672;\">/</span>debian bullseye InRelease                                                 Hit:<span style=\"color: #ae81ff;\">6</span> http:<span style=\"color: #f92672;\">//</span>archive<span style=\"color: #f92672;\">.</span>raspberrypi<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye InRelease                                  Hit:<span style=\"color: #ae81ff;\">7</span> https:<span style=\"color: #f92672;\">//</span>packagecloud<span style=\"color: #f92672;\">.</span>io<span style=\"color: #f92672;\">/</span>ookla<span style=\"color: #f92672;\">/</span>speedtest<span style=\"color: #f92672;\">-</span>cli<span style=\"color: #f92672;\">/</span>debian bullseye InRelease                     Reading package lists<span style=\"color: #f92672;\">...</span> DoneBuilding dependency tree<span style=\"color: #f92672;\">...</span> DoneReading state information<span style=\"color: #f92672;\">...</span> DoneAll packages are up to date<span style=\"color: #f92672;\">.</span>Reading package lists<span style=\"color: #f92672;\">...</span> DoneBuilding dependency tree<span style=\"color: #f92672;\">...</span> DoneReading state information<span style=\"color: #f92672;\">...</span> DoneThe following additional packages will be installed:  libbson<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">0</span> libdbi1 libesmtp6 libhiredis0<span style=\"color: #ae81ff;\">.14</span> libivykis0 libmaxminddb0 libmongoc<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">0</span> libmongocrypt0  libnet1 libprotobuf<span style=\"color: #f92672;\">-</span>c1 librabbitmq4 librdkafka1 libriemann<span style=\"color: #f92672;\">-</span>client0 libsnappy1v5 libsnmp<span style=\"color: #f92672;\">-</span>base libsnmp40  syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>core syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>add<span style=\"color: #f92672;\">-</span>contextual<span style=\"color: #f92672;\">-</span>data syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>amqp syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>examples  syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>extra syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>geoip2 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>getent syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>graphite syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>http  syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>map<span style=\"color: #f92672;\">-</span>value<span style=\"color: #f92672;\">-</span>pairs syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>mongodb syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>python syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>rdkafka  syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>redis syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>riemann syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>slog syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>smtp syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>snmp  syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>sql syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>stardate syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>stomp syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>xml<span style=\"color: #f92672;\">-</span>parserSuggested packages:  mmdb<span style=\"color: #f92672;\">-</span>bin snmp<span style=\"color: #f92672;\">-</span>mibs<span style=\"color: #f92672;\">-</span>downloader rabbitmq<span style=\"color: #f92672;\">-</span>server graphite<span style=\"color: #f92672;\">-</span>web mongodb<span style=\"color: #f92672;\">-</span>server libdbd<span style=\"color: #f92672;\">-</span>mysql libdbd<span style=\"color: #f92672;\">-</span>pgsql  libdbd<span style=\"color: #f92672;\">-</span>sqlite3 activemqThe following packages will be REMOVED:  rsyslogThe following NEW packages will be installed:  libbson<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">0</span> libdbi1 libesmtp6 libhiredis0<span style=\"color: #ae81ff;\">.14</span> libivykis0 libmaxminddb0 libmongoc<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">0</span> libmongocrypt0  libnet1 libprotobuf<span style=\"color: #f92672;\">-</span>c1 librabbitmq4 librdkafka1 libriemann<span style=\"color: #f92672;\">-</span>client0 libsnappy1v5 libsnmp<span style=\"color: #f92672;\">-</span>base libsnmp40  syslog<span style=\"color: #f92672;\">-</span>ng syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>core syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>add<span style=\"color: #f92672;\">-</span>contextual<span style=\"color: #f92672;\">-</span>data syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>amqp syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>examples  syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>extra syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>geoip2 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>getent syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>graphite syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>http  syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>map<span style=\"color: #f92672;\">-</span>value<span style=\"color: #f92672;\">-</span>pairs syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>mongodb syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>python syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>rdkafka  syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>redis syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>riemann syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>slog syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>smtp syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>snmp  syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>sql syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>stardate syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>stomp syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>xml<span style=\"color: #f92672;\">-</span>parser<span style=\"color: #ae81ff;\">0</span> upgraded, <span style=\"color: #ae81ff;\">39</span> newly installed, <span style=\"color: #ae81ff;\">1</span> to remove <span style=\"color: #f92672;\">and</span> <span style=\"color: #ae81ff;\">0</span> <span style=\"color: #f92672;\">not</span> upgraded<span style=\"color: #f92672;\">.</span>Need to get <span style=\"color: #ae81ff;\">7</span>,<span style=\"color: #ae81ff;\">015</span> kB of archives<span style=\"color: #f92672;\">.</span>After this operation, <span style=\"color: #ae81ff;\">15.1</span> MB of additional disk space will be used<span style=\"color: #f92672;\">.</span>Get:<span style=\"color: #ae81ff;\">1</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 libbson<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">0</span> arm64 <span style=\"color: #ae81ff;\">1.17.6</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span> [<span style=\"color: #ae81ff;\">69.7</span> kB]Get:<span style=\"color: #ae81ff;\">2</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 libmongocrypt0 arm64 <span style=\"color: #ae81ff;\">1.1.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span> [<span style=\"color: #ae81ff;\">114</span> kB]Get:<span style=\"color: #ae81ff;\">3</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 libsnappy1v5 arm64 <span style=\"color: #ae81ff;\">1.1.8</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span> [<span style=\"color: #ae81ff;\">17.2</span> kB]Get:<span style=\"color: #ae81ff;\">4</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 libmongoc<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">0</span> arm64 <span style=\"color: #ae81ff;\">1.17.6</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span> [<span style=\"color: #ae81ff;\">257</span> kB]Get:<span style=\"color: #ae81ff;\">5</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 libivykis0 arm64 <span style=\"color: #ae81ff;\">0.42.4</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span> [<span style=\"color: #ae81ff;\">25.3</span> kB]Get:<span style=\"color: #ae81ff;\">6</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 libnet1 arm64 <span style=\"color: #ae81ff;\">1.1.6</span><span style=\"color: #f92672;\">+</span>dfsg<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">3.1</span> [<span style=\"color: #ae81ff;\">56.8</span> kB]Get:<span style=\"color: #ae81ff;\">7</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>core arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">591</span> kB]Get:<span style=\"color: #ae81ff;\">8</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>mongodb arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">37.9</span> kB]Get:<span style=\"color: #ae81ff;\">9</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 libdbi1 arm64 <span style=\"color: #ae81ff;\">0.9.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">6</span> [<span style=\"color: #ae81ff;\">27.8</span> kB]Get:<span style=\"color: #ae81ff;\">10</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>sql arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">41.5</span> kB]Get:<span style=\"color: #ae81ff;\">11</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 libesmtp6 arm64 <span style=\"color: #ae81ff;\">1.0.6</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">4.3</span> [<span style=\"color: #ae81ff;\">52.0</span> kB]Get:<span style=\"color: #ae81ff;\">12</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 libhiredis0<span style=\"color: #ae81ff;\">.14</span> arm64 <span style=\"color: #ae81ff;\">0.14.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span> [<span style=\"color: #ae81ff;\">33.7</span> kB]Get:<span style=\"color: #ae81ff;\">13</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 libmaxminddb0 arm64 <span style=\"color: #ae81ff;\">1.5.2</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span> [<span style=\"color: #ae81ff;\">29.6</span> kB]Get:<span style=\"color: #ae81ff;\">14</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 libprotobuf<span style=\"color: #f92672;\">-</span>c1 arm64 <span style=\"color: #ae81ff;\">1.3.3</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span><span style=\"color: #f92672;\">+</span>b2 [<span style=\"color: #ae81ff;\">26.8</span> kB]Get:<span style=\"color: #ae81ff;\">15</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 librabbitmq4 arm64 <span style=\"color: #ae81ff;\">0.10.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span> [<span style=\"color: #ae81ff;\">39.7</span> kB]Get:<span style=\"color: #ae81ff;\">16</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 librdkafka1 arm64 <span style=\"color: #ae81ff;\">1.6.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span> [<span style=\"color: #ae81ff;\">515</span> kB]Get:<span style=\"color: #ae81ff;\">17</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 libriemann<span style=\"color: #f92672;\">-</span>client0 arm64 <span style=\"color: #ae81ff;\">1.10.4</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>b2 [<span style=\"color: #ae81ff;\">21.9</span> kB]Get:<span style=\"color: #ae81ff;\">18</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 libsnmp<span style=\"color: #f92672;\">-</span>base all <span style=\"color: #ae81ff;\">5.9</span><span style=\"color: #f92672;\">+</span>dfsg<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">4</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">1</span>,<span style=\"color: #ae81ff;\">736</span> kB]Get:<span style=\"color: #ae81ff;\">19</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 libsnmp40 arm64 <span style=\"color: #ae81ff;\">5.9</span><span style=\"color: #f92672;\">+</span>dfsg<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">4</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">2</span>,<span style=\"color: #ae81ff;\">497</span> kB]Get:<span style=\"color: #ae81ff;\">20</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng all <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">25.9</span> kB]Get:<span style=\"color: #ae81ff;\">21</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>add<span style=\"color: #f92672;\">-</span>contextual<span style=\"color: #f92672;\">-</span>data arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">40.5</span> kB]Get:<span style=\"color: #ae81ff;\">22</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>amqp arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">48.8</span> kB]Get:<span style=\"color: #ae81ff;\">23</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>examples arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">57.3</span> kB]Get:<span style=\"color: #ae81ff;\">24</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>extra all <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">35.7</span> kB]Get:<span style=\"color: #ae81ff;\">25</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>geoip2 arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">36.9</span> kB]Get:<span style=\"color: #ae81ff;\">26</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>graphite arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">29.4</span> kB]Get:<span style=\"color: #ae81ff;\">27</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>http arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">50.5</span> kB]Get:<span style=\"color: #ae81ff;\">28</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>python arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">69.9</span> kB]Get:<span style=\"color: #ae81ff;\">29</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>rdkafka arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">41.5</span> kB]Get:<span style=\"color: #ae81ff;\">30</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>redis arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">37.6</span> kB]Get:<span style=\"color: #ae81ff;\">31</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>riemann arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">40.1</span> kB]Get:<span style=\"color: #ae81ff;\">32</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>slog arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">63.3</span> kB]Get:<span style=\"color: #ae81ff;\">33</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>smtp arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">38.0</span> kB]Get:<span style=\"color: #ae81ff;\">34</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>snmp arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">42.5</span> kB]Get:<span style=\"color: #ae81ff;\">35</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>stomp arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">39.1</span> kB]Get:<span style=\"color: #ae81ff;\">36</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>xml<span style=\"color: #f92672;\">-</span>parser arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">34.7</span> kB]Get:<span style=\"color: #ae81ff;\">37</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>getent arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">29.5</span> kB]Get:<span style=\"color: #ae81ff;\">38</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>map<span style=\"color: #f92672;\">-</span>value<span style=\"color: #f92672;\">-</span>pairs arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">34.0</span> kB]Get:<span style=\"color: #ae81ff;\">39</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>stardate arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">28.6</span> kB]Fetched <span style=\"color: #ae81ff;\">7</span>,<span style=\"color: #ae81ff;\">015</span> kB <span style=\"color: #f92672;\">in</span> <span style=\"color: #ae81ff;\">5</span>s (<span style=\"color: #ae81ff;\">1</span>,<span style=\"color: #ae81ff;\">311</span> kB<span style=\"color: #f92672;\">/</span>s)           Extracting templates <span style=\"color: #f92672;\">from</span> packages: <span style=\"color: #ae81ff;\">100</span><span style=\"color: #f92672;\">%</span>(Reading database <span style=\"color: #f92672;\">...</span> <span style=\"color: #ae81ff;\">90182</span> files <span style=\"color: #f92672;\">and</span> directories currently installed<span style=\"color: #f92672;\">.</span>)Removing rsyslog (<span style=\"color: #ae81ff;\">8.2102.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package libbson<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">0.</span>(Reading database <span style=\"color: #f92672;\">...</span> <span style=\"color: #ae81ff;\">90124</span> files <span style=\"color: #f92672;\">and</span> directories currently installed<span style=\"color: #f92672;\">.</span>)Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">00</span><span style=\"color: #f92672;\">-</span>libbson<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">0_1.17.6</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking libbson<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">0</span> (<span style=\"color: #ae81ff;\">1.17.6</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package libmongocrypt0:arm64<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">01</span><span style=\"color: #f92672;\">-</span>libmongocrypt0_1<span style=\"color: #ae81ff;\">.1.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking libmongocrypt0:arm64 (<span style=\"color: #ae81ff;\">1.1.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package libsnappy1v5:arm64<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">02</span><span style=\"color: #f92672;\">-</span>libsnappy1v5_1<span style=\"color: #ae81ff;\">.1.8</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking libsnappy1v5:arm64 (<span style=\"color: #ae81ff;\">1.1.8</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package libmongoc<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">0.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">03</span><span style=\"color: #f92672;\">-</span>libmongoc<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">0_1.17.6</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking libmongoc<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">0</span> (<span style=\"color: #ae81ff;\">1.17.6</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package libivykis0:arm64<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">04</span><span style=\"color: #f92672;\">-</span>libivykis0_0<span style=\"color: #ae81ff;\">.42.4</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking libivykis0:arm64 (<span style=\"color: #ae81ff;\">0.42.4</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package libnet1:arm64<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">05</span><span style=\"color: #f92672;\">-</span>libnet1_1<span style=\"color: #ae81ff;\">.1.6</span><span style=\"color: #f92672;\">+</span>dfsg<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">3.1</span>_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking libnet1:arm64 (<span style=\"color: #ae81ff;\">1.1.6</span><span style=\"color: #f92672;\">+</span>dfsg<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">3.1</span>) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>core<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">06</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>core_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>core (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>mongodb<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">07</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>mongodb_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>mongodb (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package libdbi1:arm64<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">08</span><span style=\"color: #f92672;\">-</span>libdbi1_0<span style=\"color: #ae81ff;\">.9.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">6</span>_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking libdbi1:arm64 (<span style=\"color: #ae81ff;\">0.9.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">6</span>) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>sql<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">09</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>sql_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>sql (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package libesmtp6<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">10</span><span style=\"color: #f92672;\">-</span>libesmtp6_1<span style=\"color: #ae81ff;\">.0.6</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">4.3</span>_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking libesmtp6 (<span style=\"color: #ae81ff;\">1.0.6</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">4.3</span>) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package libhiredis0<span style=\"color: #ae81ff;\">.14</span>:arm64<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">11</span><span style=\"color: #f92672;\">-</span>libhiredis0<span style=\"color: #ae81ff;\">.14_0.14.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking libhiredis0<span style=\"color: #ae81ff;\">.14</span>:arm64 (<span style=\"color: #ae81ff;\">0.14.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package libmaxminddb0:arm64<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">12</span><span style=\"color: #f92672;\">-</span>libmaxminddb0_1<span style=\"color: #ae81ff;\">.5.2</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking libmaxminddb0:arm64 (<span style=\"color: #ae81ff;\">1.5.2</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package libprotobuf<span style=\"color: #f92672;\">-</span>c1:arm64<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">13</span><span style=\"color: #f92672;\">-</span>libprotobuf<span style=\"color: #f92672;\">-</span>c1_1<span style=\"color: #ae81ff;\">.3.3</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span><span style=\"color: #f92672;\">+</span>b2_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking libprotobuf<span style=\"color: #f92672;\">-</span>c1:arm64 (<span style=\"color: #ae81ff;\">1.3.3</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span><span style=\"color: #f92672;\">+</span>b2) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package librabbitmq4:arm64<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">14</span><span style=\"color: #f92672;\">-</span>librabbitmq4_0<span style=\"color: #ae81ff;\">.10.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking librabbitmq4:arm64 (<span style=\"color: #ae81ff;\">0.10.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package librdkafka1:arm64<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">15</span><span style=\"color: #f92672;\">-</span>librdkafka1_1<span style=\"color: #ae81ff;\">.6.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking librdkafka1:arm64 (<span style=\"color: #ae81ff;\">1.6.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package libriemann<span style=\"color: #f92672;\">-</span>client0:arm64<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">16</span><span style=\"color: #f92672;\">-</span>libriemann<span style=\"color: #f92672;\">-</span>client0_1<span style=\"color: #ae81ff;\">.10.4</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>b2_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking libriemann<span style=\"color: #f92672;\">-</span>client0:arm64 (<span style=\"color: #ae81ff;\">1.10.4</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>b2) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package libsnmp<span style=\"color: #f92672;\">-</span>base<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">17</span><span style=\"color: #f92672;\">-</span>libsnmp<span style=\"color: #f92672;\">-</span>base_5<span style=\"color: #ae81ff;\">.9</span><span style=\"color: #f92672;\">+</span>dfsg<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">4</span><span style=\"color: #f92672;\">+</span>deb11u1_all<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking libsnmp<span style=\"color: #f92672;\">-</span>base (<span style=\"color: #ae81ff;\">5.9</span><span style=\"color: #f92672;\">+</span>dfsg<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">4</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package libsnmp40:arm64<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">18</span><span style=\"color: #f92672;\">-</span>libsnmp40_5<span style=\"color: #ae81ff;\">.9</span><span style=\"color: #f92672;\">+</span>dfsg<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">4</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking libsnmp40:arm64 (<span style=\"color: #ae81ff;\">5.9</span><span style=\"color: #f92672;\">+</span>dfsg<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">4</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">19</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_all<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>add<span style=\"color: #f92672;\">-</span>contextual<span style=\"color: #f92672;\">-</span>data<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">20</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>add<span style=\"color: #f92672;\">-</span>contextual<span style=\"color: #f92672;\">-</span>data_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>add<span style=\"color: #f92672;\">-</span>contextual<span style=\"color: #f92672;\">-</span>data (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>amqp<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">21</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>amqp_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>amqp (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>examples<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">22</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>examples_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>examples (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>extra<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">23</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>extra_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_all<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>extra (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>geoip2<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">24</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>geoip2_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>geoip2 (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>graphite<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">25</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>graphite_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>graphite (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>http<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">26</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>http_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>http (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>python<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">27</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>python_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>python (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>rdkafka<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">28</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>rdkafka_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>rdkafka (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>redis<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">29</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>redis_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>redis (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>riemann<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">30</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>riemann_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>riemann (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>slog<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">31</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>slog_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>slog (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>smtp<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">32</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>smtp_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>smtp (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>snmp<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">33</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>snmp_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>snmp (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>stomp<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">34</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>stomp_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>stomp (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>xml<span style=\"color: #f92672;\">-</span>parser<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">35</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>xml<span style=\"color: #f92672;\">-</span>parser_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>xml<span style=\"color: #f92672;\">-</span>parser (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>getent<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">36</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>getent_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>getent (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>map<span style=\"color: #f92672;\">-</span>value<span style=\"color: #f92672;\">-</span>pairs<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">37</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>map<span style=\"color: #f92672;\">-</span>value<span style=\"color: #f92672;\">-</span>pairs_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>map<span style=\"color: #f92672;\">-</span>value<span style=\"color: #f92672;\">-</span>pairs (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>stardate<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">38</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>stardate_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>stardate (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up librabbitmq4:arm64 (<span style=\"color: #ae81ff;\">0.10.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>) <span style=\"color: #f92672;\">...</span>Setting up libdbi1:arm64 (<span style=\"color: #ae81ff;\">0.9.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">6</span>) <span style=\"color: #f92672;\">...</span>Setting up libsnmp<span style=\"color: #f92672;\">-</span>base (<span style=\"color: #ae81ff;\">5.9</span><span style=\"color: #f92672;\">+</span>dfsg<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">4</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up libmaxminddb0:arm64 (<span style=\"color: #ae81ff;\">1.5.2</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>) <span style=\"color: #f92672;\">...</span>Setting up libesmtp6 (<span style=\"color: #ae81ff;\">1.0.6</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">4.3</span>) <span style=\"color: #f92672;\">...</span>Setting up libnet1:arm64 (<span style=\"color: #ae81ff;\">1.1.6</span><span style=\"color: #f92672;\">+</span>dfsg<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">3.1</span>) <span style=\"color: #f92672;\">...</span>Setting up libprotobuf<span style=\"color: #f92672;\">-</span>c1:arm64 (<span style=\"color: #ae81ff;\">1.3.3</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span><span style=\"color: #f92672;\">+</span>b2) <span style=\"color: #f92672;\">...</span>Setting up libsnappy1v5:arm64 (<span style=\"color: #ae81ff;\">1.1.8</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>) <span style=\"color: #f92672;\">...</span>Setting up libsnmp40:arm64 (<span style=\"color: #ae81ff;\">5.9</span><span style=\"color: #f92672;\">+</span>dfsg<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">4</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up libbson<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">0</span> (<span style=\"color: #ae81ff;\">1.17.6</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>) <span style=\"color: #f92672;\">...</span>Setting up libivykis0:arm64 (<span style=\"color: #ae81ff;\">0.42.4</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>) <span style=\"color: #f92672;\">...</span>Setting up libriemann<span style=\"color: #f92672;\">-</span>client0:arm64 (<span style=\"color: #ae81ff;\">1.10.4</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>b2) <span style=\"color: #f92672;\">...</span>Setting up librdkafka1:arm64 (<span style=\"color: #ae81ff;\">1.6.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>) <span style=\"color: #f92672;\">...</span>Setting up libhiredis0<span style=\"color: #ae81ff;\">.14</span>:arm64 (<span style=\"color: #ae81ff;\">0.14.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>) <span style=\"color: #f92672;\">...</span>Setting up libmongocrypt0:arm64 (<span style=\"color: #ae81ff;\">1.1.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>) <span style=\"color: #f92672;\">...</span>Setting up libmongoc<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">0</span> (<span style=\"color: #ae81ff;\">1.17.6</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>core (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Created symlink <span style=\"color: #f92672;\">/</span>etc<span style=\"color: #f92672;\">/</span>systemd<span style=\"color: #f92672;\">/</span>system<span style=\"color: #f92672;\">/</span>multi<span style=\"color: #f92672;\">-</span>user<span style=\"color: #f92672;\">.</span>target<span style=\"color: #f92672;\">.</span>wants<span style=\"color: #f92672;\">/</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>service <span style=\"color: #960050; background-color: #1e0010;\">→</span> <span style=\"color: #f92672;\">/</span>lib<span style=\"color: #f92672;\">/</span>systemd<span style=\"color: #f92672;\">/</span>system<span style=\"color: #f92672;\">/</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>service<span style=\"color: #f92672;\">.</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>examples (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>xml<span style=\"color: #f92672;\">-</span>parser (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>stomp (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>riemann (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>stardate (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>geoip2 (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>getent (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>amqp (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>python (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>smtp (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>snmp (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>extra (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>rdkafka (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>graphite (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>add<span style=\"color: #f92672;\">-</span>contextual<span style=\"color: #f92672;\">-</span>data (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>mongodb (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>http (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>slog (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>map<span style=\"color: #f92672;\">-</span>value<span style=\"color: #f92672;\">-</span>pairs (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>sql (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>redis (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Processing triggers <span style=\"color: #66d9ef;\">for</span> man<span style=\"color: #f92672;\">-</span>db (<span style=\"color: #ae81ff;\">2.9.4</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span>) <span style=\"color: #f92672;\">...</span>Processing triggers <span style=\"color: #66d9ef;\">for</span> libc<span style=\"color: #f92672;\">-</span>bin (<span style=\"color: #ae81ff;\">2.31</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">13</span><span style=\"color: #f92672;\">+</span>rpt2<span style=\"color: #f92672;\">+</span>rpi1<span style=\"color: #f92672;\">+</span>deb11u8) <span style=\"color: #f92672;\">...</span>Scanning processes<span style=\"color: #f92672;\">...</span>                                                                                         Scanning processor microcode<span style=\"color: #f92672;\">...</span>                                                                               Scanning linux images<span style=\"color: #f92672;\">...</span>                                                                                      Running kernel seems to be up<span style=\"color: #f92672;\">-</span>to<span style=\"color: #f92672;\">-</span>date<span style=\"color: #f92672;\">.</span>Failed to check <span style=\"color: #66d9ef;\">for</span> processor microcode upgrades<span style=\"color: #f92672;\">.</span>No services need to be restarted<span style=\"color: #f92672;\">.</span>No containers need to be restarted<span style=\"color: #f92672;\">.</span>No user sessions are running outdated binaries<span style=\"color: #f92672;\">.</span></code></pre></div></details><br /><!-- raw HTML omitted -->2. With <em>syslog-ng</em> installed, it’s now time to build the configuration for it. A new configuration file <em>fromnet.conf</em> is shown below, in which a <em>syslog-ng</em> destination is created which will aggregate logs from the Turing Pi nodes in <em>/var/log/fromnet</em> in plain text format. Additionally, the logs will be written in JSON format to the file <em>/var/log/fromnet.json</em>.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">root@turingpi:~# cat /etc/syslog-ng/fromnet.conf # sourcesource s_fromnet {  syslog(port(601));};# destination destination d_fromnet {  file(\"/var/log/fromnet\");  file(\"/var/log/fromnet.json\" template(\"$(format-json --scope rfc5424 --scope dot-nv-pairs        --rekey .* --shift 1 --scope nv-pairs)\\n\") );};# log pathlog {  source(s_fromnet);  destination(d_fromnet);}; </code></pre></div><ol start=\"3\"><li>Unless we only want to see source IP addresses in the collected logs, it’s necessary to update the <em>syslog-ng</em> configuration file <em>/etc/syslog-ng/syslog-ng.conf</em> to record the hostnames from which the log messages have originated. This is done by adding the <em>keep_hostname(yes)</em> parameter to the options section as follows:</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\">........# First, set some global options. options { chain_hostnames(off); flush_lines(0); use_dns(no); use_fqdn(no);                  keep_hostname(yes);dns_cache(no); owner(\"root\"); group(\"adm\"); perm(0640);         stats_freq(0); bad_hostname(\"^gconfd$\"); };........</code></pre></div><ol start=\"4\"><li>Next, the IBM LSF configuration is updated to prevent the creation of local logfiles for the LSF daemons. This is done by commenting the <em>LSF_LOGDIR</em> option in the configuration file <em>$LSF_ENVDIR/lsf.conf</em>. At the same time, we also set <em>LSF_LOG_MASK=LOG_DEBUG</em> for testing purposes to enable verbose logging for the LSF daemons.</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\">........# Daemon log messages# LSF_LOGDIR=/opt/ibm/lsf/logLSF_LOG_MASK=LOG_DEBUG........</code></pre></div><ol start=\"5\"><li>Finally, to make the changes take effect, both syslog-ng and LSF are restarted.</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\">root@turingpi:~# systemctl restart syslog-ng root@turingpi:~# . /opt/ibm/lsf/conf/profile.lsf  root@turingpi:~# lsf_daemons restart Stopping the LSF subsystem Starting the LSF subsystem</code></pre></div><ol start=\"6\"><li>With the configuration ready on the centralized logging server, host <em>turingpi</em>, we now turn our attention to Nodes 2-7 in the cluster. Here we’ll use the <em>parallel-ssh</em> tool to streamline some operations. We start with the installation of <em>syslog-ng</em> across Nodes 2-7. Note that the output of the installation of <em>syslog-ng</em> across the compute nodes has been truncated.</li></ol><p><details>  <strong>Truncated output of <em>parallel-ssh -h /opt/workers -i &ldquo;apt-get install syslog-ng -y&rdquo;</em>. Click to expand</strong>  <div class=\"highlight\"><pre><code class=\"language-python\">root<span style=\"color: #a6e22e;\">@turingpi</span>:<span style=\"color: #f92672;\">~</span><span style=\"color: #75715e;\"># parallel-ssh -h /opt/workers -i \"apt-get install syslog-ng -y\" </span>[<span style=\"color: #ae81ff;\">1</span>] <span style=\"color: #ae81ff;\">13</span>:<span style=\"color: #ae81ff;\">57</span>:<span style=\"color: #ae81ff;\">07</span> [SUCCESS] kemenyReading package lists<span style=\"color: #f92672;\">...</span>Building dependency tree<span style=\"color: #f92672;\">...</span>Reading state information<span style=\"color: #f92672;\">...</span>The following additional packages will be installed:  libbson<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">0</span> libdbi1 libesmtp6 libhiredis0<span style=\"color: #ae81ff;\">.14</span> libivykis0 libmaxminddb0  libmongoc<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">0</span> libmongocrypt0 libnet1 libprotobuf<span style=\"color: #f92672;\">-</span>c1 librabbitmq4  librdkafka1 libriemann<span style=\"color: #f92672;\">-</span>client0 libsensors<span style=\"color: #f92672;\">-</span>config libsensors5 libsnappy1v5  libsnmp<span style=\"color: #f92672;\">-</span>base libsnmp40 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>core syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>add<span style=\"color: #f92672;\">-</span>contextual<span style=\"color: #f92672;\">-</span>data  syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>amqp syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>examples syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>extra  syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>geoip2 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>getent syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>graphite  syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>http syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>map<span style=\"color: #f92672;\">-</span>value<span style=\"color: #f92672;\">-</span>pairs syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>mongodb  syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>python syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>rdkafka syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>redis  syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>riemann syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>slog syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>smtp  syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>snmp syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>sql syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>stardate  syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>stomp syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>xml<span style=\"color: #f92672;\">-</span>parserSuggested packages:  mmdb<span style=\"color: #f92672;\">-</span>bin lm<span style=\"color: #f92672;\">-</span>sensors snmp<span style=\"color: #f92672;\">-</span>mibs<span style=\"color: #f92672;\">-</span>downloader rabbitmq<span style=\"color: #f92672;\">-</span>server graphite<span style=\"color: #f92672;\">-</span>web  mongodb<span style=\"color: #f92672;\">-</span>server libdbd<span style=\"color: #f92672;\">-</span>mysql libdbd<span style=\"color: #f92672;\">-</span>pgsql libdbd<span style=\"color: #f92672;\">-</span>sqlite3 activemqThe following packages will be REMOVED:  rsyslogThe following NEW packages will be installed:  libbson<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">0</span> libdbi1 libesmtp6 libhiredis0<span style=\"color: #ae81ff;\">.14</span> libivykis0 libmaxminddb0  libmongoc<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">0</span> libmongocrypt0 libnet1 libprotobuf<span style=\"color: #f92672;\">-</span>c1 librabbitmq4  librdkafka1 libriemann<span style=\"color: #f92672;\">-</span>client0 libsensors<span style=\"color: #f92672;\">-</span>config libsensors5 libsnappy1v5  libsnmp<span style=\"color: #f92672;\">-</span>base libsnmp40 syslog<span style=\"color: #f92672;\">-</span>ng syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>core  syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>add<span style=\"color: #f92672;\">-</span>contextual<span style=\"color: #f92672;\">-</span>data syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>amqp syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>examples  syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>extra syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>geoip2 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>getent  syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>graphite syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>http syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>map<span style=\"color: #f92672;\">-</span>value<span style=\"color: #f92672;\">-</span>pairs  syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>mongodb syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>python syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>rdkafka  syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>redis syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>riemann syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>slog  syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>smtp syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>snmp syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>sql  syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>stardate syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>stomp syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>xml<span style=\"color: #f92672;\">-</span>parser<span style=\"color: #ae81ff;\">0</span> upgraded, <span style=\"color: #ae81ff;\">41</span> newly installed, <span style=\"color: #ae81ff;\">1</span> to remove <span style=\"color: #f92672;\">and</span> <span style=\"color: #ae81ff;\">0</span> <span style=\"color: #f92672;\">not</span> upgraded<span style=\"color: #f92672;\">.</span>Need to get <span style=\"color: #ae81ff;\">7</span>,<span style=\"color: #ae81ff;\">098</span> kB of archives<span style=\"color: #f92672;\">.</span>After this operation, <span style=\"color: #ae81ff;\">15.3</span> MB of additional disk space will be used<span style=\"color: #f92672;\">.</span>Get:<span style=\"color: #ae81ff;\">1</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 libbson<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">0</span> arm64 <span style=\"color: #ae81ff;\">1.17.6</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span> [<span style=\"color: #ae81ff;\">69.7</span> kB]Get:<span style=\"color: #ae81ff;\">2</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 libmongocrypt0 arm64 <span style=\"color: #ae81ff;\">1.1.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span> [<span style=\"color: #ae81ff;\">114</span> kB]Get:<span style=\"color: #ae81ff;\">3</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 libsnappy1v5 arm64 <span style=\"color: #ae81ff;\">1.1.8</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span> [<span style=\"color: #ae81ff;\">17.2</span> kB]Get:<span style=\"color: #ae81ff;\">4</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 libmongoc<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">0</span> arm64 <span style=\"color: #ae81ff;\">1.17.6</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span> [<span style=\"color: #ae81ff;\">257</span> kB]Get:<span style=\"color: #ae81ff;\">5</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 libivykis0 arm64 <span style=\"color: #ae81ff;\">0.42.4</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span> [<span style=\"color: #ae81ff;\">25.3</span> kB]Get:<span style=\"color: #ae81ff;\">6</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 libnet1 arm64 <span style=\"color: #ae81ff;\">1.1.6</span><span style=\"color: #f92672;\">+</span>dfsg<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">3.1</span> [<span style=\"color: #ae81ff;\">56.8</span> kB]Get:<span style=\"color: #ae81ff;\">7</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>core arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">591</span> kB]Get:<span style=\"color: #ae81ff;\">8</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>mongodb arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">37.9</span> kB]Get:<span style=\"color: #ae81ff;\">9</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 libdbi1 arm64 <span style=\"color: #ae81ff;\">0.9.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">6</span> [<span style=\"color: #ae81ff;\">27.8</span> kB]Get:<span style=\"color: #ae81ff;\">10</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>sql arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">41.5</span> kB]Get:<span style=\"color: #ae81ff;\">11</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 libesmtp6 arm64 <span style=\"color: #ae81ff;\">1.0.6</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">4.3</span> [<span style=\"color: #ae81ff;\">52.0</span> kB]Get:<span style=\"color: #ae81ff;\">12</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 libhiredis0<span style=\"color: #ae81ff;\">.14</span> arm64 <span style=\"color: #ae81ff;\">0.14.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span> [<span style=\"color: #ae81ff;\">33.7</span> kB]Get:<span style=\"color: #ae81ff;\">13</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 libmaxminddb0 arm64 <span style=\"color: #ae81ff;\">1.5.2</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span> [<span style=\"color: #ae81ff;\">29.6</span> kB]Get:<span style=\"color: #ae81ff;\">14</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 libprotobuf<span style=\"color: #f92672;\">-</span>c1 arm64 <span style=\"color: #ae81ff;\">1.3.3</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span><span style=\"color: #f92672;\">+</span>b2 [<span style=\"color: #ae81ff;\">26.8</span> kB]Get:<span style=\"color: #ae81ff;\">15</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 librabbitmq4 arm64 <span style=\"color: #ae81ff;\">0.10.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span> [<span style=\"color: #ae81ff;\">39.7</span> kB]Get:<span style=\"color: #ae81ff;\">16</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 librdkafka1 arm64 <span style=\"color: #ae81ff;\">1.6.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span> [<span style=\"color: #ae81ff;\">515</span> kB]Get:<span style=\"color: #ae81ff;\">17</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 libriemann<span style=\"color: #f92672;\">-</span>client0 arm64 <span style=\"color: #ae81ff;\">1.10.4</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>b2 [<span style=\"color: #ae81ff;\">21.9</span> kB]Get:<span style=\"color: #ae81ff;\">18</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 libsensors<span style=\"color: #f92672;\">-</span>config all <span style=\"color: #ae81ff;\">1</span>:<span style=\"color: #ae81ff;\">3.6.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">7</span> [<span style=\"color: #ae81ff;\">32.3</span> kB]Get:<span style=\"color: #ae81ff;\">19</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 libsensors5 arm64 <span style=\"color: #ae81ff;\">1</span>:<span style=\"color: #ae81ff;\">3.6.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">7</span> [<span style=\"color: #ae81ff;\">51.2</span> kB]Get:<span style=\"color: #ae81ff;\">20</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 libsnmp<span style=\"color: #f92672;\">-</span>base all <span style=\"color: #ae81ff;\">5.9</span><span style=\"color: #f92672;\">+</span>dfsg<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">4</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">1</span>,<span style=\"color: #ae81ff;\">736</span> kB]Get:<span style=\"color: #ae81ff;\">21</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 libsnmp40 arm64 <span style=\"color: #ae81ff;\">5.9</span><span style=\"color: #f92672;\">+</span>dfsg<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">4</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">2</span>,<span style=\"color: #ae81ff;\">497</span> kB]Get:<span style=\"color: #ae81ff;\">22</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng all <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">25.9</span> kB]Get:<span style=\"color: #ae81ff;\">23</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>add<span style=\"color: #f92672;\">-</span>contextual<span style=\"color: #f92672;\">-</span>data arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">40.5</span> kB]Get:<span style=\"color: #ae81ff;\">24</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>amqp arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">48.8</span> kB]Get:<span style=\"color: #ae81ff;\">25</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>examples arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">57.3</span> kB]Get:<span style=\"color: #ae81ff;\">26</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>extra all <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">35.7</span> kB]Get:<span style=\"color: #ae81ff;\">27</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>geoip2 arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">36.9</span> kB]Get:<span style=\"color: #ae81ff;\">28</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>graphite arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">29.4</span> kB]Get:<span style=\"color: #ae81ff;\">29</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>http arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">50.5</span> kB]Get:<span style=\"color: #ae81ff;\">30</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>python arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">69.9</span> kB]Get:<span style=\"color: #ae81ff;\">31</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>rdkafka arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">41.5</span> kB]Get:<span style=\"color: #ae81ff;\">32</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>redis arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">37.6</span> kB]Get:<span style=\"color: #ae81ff;\">33</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>riemann arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">40.1</span> kB]Get:<span style=\"color: #ae81ff;\">34</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>slog arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">63.3</span> kB]Get:<span style=\"color: #ae81ff;\">35</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>smtp arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">38.0</span> kB]Get:<span style=\"color: #ae81ff;\">36</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>snmp arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">42.5</span> kB]Get:<span style=\"color: #ae81ff;\">37</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>stomp arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">39.1</span> kB]Get:<span style=\"color: #ae81ff;\">38</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>xml<span style=\"color: #f92672;\">-</span>parser arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">34.7</span> kB]Get:<span style=\"color: #ae81ff;\">39</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>getent arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">29.5</span> kB]Get:<span style=\"color: #ae81ff;\">40</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>map<span style=\"color: #f92672;\">-</span>value<span style=\"color: #f92672;\">-</span>pairs arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">34.0</span> kB]Get:<span style=\"color: #ae81ff;\">41</span> http:<span style=\"color: #f92672;\">//</span>deb<span style=\"color: #f92672;\">.</span>debian<span style=\"color: #f92672;\">.</span>org<span style=\"color: #f92672;\">/</span>debian bullseye<span style=\"color: #f92672;\">/</span>main arm64 syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>stardate arm64 <span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1 [<span style=\"color: #ae81ff;\">28.6</span> kB]Fetched <span style=\"color: #ae81ff;\">7</span>,<span style=\"color: #ae81ff;\">098</span> kB <span style=\"color: #f92672;\">in</span> <span style=\"color: #ae81ff;\">2</span>s (<span style=\"color: #ae81ff;\">3</span>,<span style=\"color: #ae81ff;\">566</span> kB<span style=\"color: #f92672;\">/</span>s)(Reading database <span style=\"color: #f92672;\">...</span> <span style=\"color: #ae81ff;\">37650</span> files <span style=\"color: #f92672;\">and</span> directories currently installed<span style=\"color: #f92672;\">.</span>)Removing rsyslog (<span style=\"color: #ae81ff;\">8.2102.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package libbson<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">0.</span>(Reading database <span style=\"color: #f92672;\">...</span> <span style=\"color: #ae81ff;\">37592</span> files <span style=\"color: #f92672;\">and</span> directories currently installed<span style=\"color: #f92672;\">.</span>)Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">00</span><span style=\"color: #f92672;\">-</span>libbson<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">0_1.17.6</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking libbson<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">0</span> (<span style=\"color: #ae81ff;\">1.17.6</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package libmongocrypt0:arm64<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">01</span><span style=\"color: #f92672;\">-</span>libmongocrypt0_1<span style=\"color: #ae81ff;\">.1.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking libmongocrypt0:arm64 (<span style=\"color: #ae81ff;\">1.1.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package libsnappy1v5:arm64<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">02</span><span style=\"color: #f92672;\">-</span>libsnappy1v5_1<span style=\"color: #ae81ff;\">.1.8</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking libsnappy1v5:arm64 (<span style=\"color: #ae81ff;\">1.1.8</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package libmongoc<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">0.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">03</span><span style=\"color: #f92672;\">-</span>libmongoc<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">0_1.17.6</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking libmongoc<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">0</span> (<span style=\"color: #ae81ff;\">1.17.6</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package libivykis0:arm64<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">04</span><span style=\"color: #f92672;\">-</span>libivykis0_0<span style=\"color: #ae81ff;\">.42.4</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking libivykis0:arm64 (<span style=\"color: #ae81ff;\">0.42.4</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package libnet1:arm64<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">05</span><span style=\"color: #f92672;\">-</span>libnet1_1<span style=\"color: #ae81ff;\">.1.6</span><span style=\"color: #f92672;\">+</span>dfsg<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">3.1</span>_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking libnet1:arm64 (<span style=\"color: #ae81ff;\">1.1.6</span><span style=\"color: #f92672;\">+</span>dfsg<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">3.1</span>) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>core<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">06</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>core_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>core (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>mongodb<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">07</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>mongodb_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>mongodb (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package libdbi1:arm64<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">08</span><span style=\"color: #f92672;\">-</span>libdbi1_0<span style=\"color: #ae81ff;\">.9.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">6</span>_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking libdbi1:arm64 (<span style=\"color: #ae81ff;\">0.9.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">6</span>) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>sql<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">09</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>sql_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>sql (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package libesmtp6<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">10</span><span style=\"color: #f92672;\">-</span>libesmtp6_1<span style=\"color: #ae81ff;\">.0.6</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">4.3</span>_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking libesmtp6 (<span style=\"color: #ae81ff;\">1.0.6</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">4.3</span>) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package libhiredis0<span style=\"color: #ae81ff;\">.14</span>:arm64<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">11</span><span style=\"color: #f92672;\">-</span>libhiredis0<span style=\"color: #ae81ff;\">.14_0.14.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking libhiredis0<span style=\"color: #ae81ff;\">.14</span>:arm64 (<span style=\"color: #ae81ff;\">0.14.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package libmaxminddb0:arm64<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">12</span><span style=\"color: #f92672;\">-</span>libmaxminddb0_1<span style=\"color: #ae81ff;\">.5.2</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking libmaxminddb0:arm64 (<span style=\"color: #ae81ff;\">1.5.2</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package libprotobuf<span style=\"color: #f92672;\">-</span>c1:arm64<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">13</span><span style=\"color: #f92672;\">-</span>libprotobuf<span style=\"color: #f92672;\">-</span>c1_1<span style=\"color: #ae81ff;\">.3.3</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span><span style=\"color: #f92672;\">+</span>b2_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking libprotobuf<span style=\"color: #f92672;\">-</span>c1:arm64 (<span style=\"color: #ae81ff;\">1.3.3</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span><span style=\"color: #f92672;\">+</span>b2) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package librabbitmq4:arm64<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">14</span><span style=\"color: #f92672;\">-</span>librabbitmq4_0<span style=\"color: #ae81ff;\">.10.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking librabbitmq4:arm64 (<span style=\"color: #ae81ff;\">0.10.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package librdkafka1:arm64<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">15</span><span style=\"color: #f92672;\">-</span>librdkafka1_1<span style=\"color: #ae81ff;\">.6.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking librdkafka1:arm64 (<span style=\"color: #ae81ff;\">1.6.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package libriemann<span style=\"color: #f92672;\">-</span>client0:arm64<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">16</span><span style=\"color: #f92672;\">-</span>libriemann<span style=\"color: #f92672;\">-</span>client0_1<span style=\"color: #ae81ff;\">.10.4</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>b2_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking libriemann<span style=\"color: #f92672;\">-</span>client0:arm64 (<span style=\"color: #ae81ff;\">1.10.4</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>b2) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package libsensors<span style=\"color: #f92672;\">-</span>config<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">17</span><span style=\"color: #f92672;\">-</span>libsensors<span style=\"color: #f92672;\">-</span>config_1<span style=\"color: #f92672;\">%</span><span style=\"color: #ae81ff;\">3</span>a3<span style=\"color: #ae81ff;\">.6.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">7</span>_all<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking libsensors<span style=\"color: #f92672;\">-</span>config (<span style=\"color: #ae81ff;\">1</span>:<span style=\"color: #ae81ff;\">3.6.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">7</span>) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package libsensors5:arm64<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">18</span><span style=\"color: #f92672;\">-</span>libsensors5_1<span style=\"color: #f92672;\">%</span><span style=\"color: #ae81ff;\">3</span>a3<span style=\"color: #ae81ff;\">.6.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">7</span>_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking libsensors5:arm64 (<span style=\"color: #ae81ff;\">1</span>:<span style=\"color: #ae81ff;\">3.6.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">7</span>) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package libsnmp<span style=\"color: #f92672;\">-</span>base<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">19</span><span style=\"color: #f92672;\">-</span>libsnmp<span style=\"color: #f92672;\">-</span>base_5<span style=\"color: #ae81ff;\">.9</span><span style=\"color: #f92672;\">+</span>dfsg<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">4</span><span style=\"color: #f92672;\">+</span>deb11u1_all<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking libsnmp<span style=\"color: #f92672;\">-</span>base (<span style=\"color: #ae81ff;\">5.9</span><span style=\"color: #f92672;\">+</span>dfsg<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">4</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package libsnmp40:arm64<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">20</span><span style=\"color: #f92672;\">-</span>libsnmp40_5<span style=\"color: #ae81ff;\">.9</span><span style=\"color: #f92672;\">+</span>dfsg<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">4</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking libsnmp40:arm64 (<span style=\"color: #ae81ff;\">5.9</span><span style=\"color: #f92672;\">+</span>dfsg<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">4</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">21</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_all<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>add<span style=\"color: #f92672;\">-</span>contextual<span style=\"color: #f92672;\">-</span>data<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">22</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>add<span style=\"color: #f92672;\">-</span>contextual<span style=\"color: #f92672;\">-</span>data_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>add<span style=\"color: #f92672;\">-</span>contextual<span style=\"color: #f92672;\">-</span>data (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>amqp<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">23</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>amqp_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>amqp (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>examples<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">24</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>examples_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>examples (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>extra<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">25</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>extra_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_all<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>extra (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>geoip2<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">26</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>geoip2_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>geoip2 (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>graphite<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">27</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>graphite_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>graphite (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>http<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">28</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>http_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>http (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>python<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">29</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>python_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>python (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>rdkafka<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">30</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>rdkafka_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>rdkafka (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>redis<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">31</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>redis_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>redis (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>riemann<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">32</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>riemann_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>riemann (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>slog<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">33</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>slog_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>slog (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>smtp<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">34</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>smtp_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>smtp (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>snmp<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">35</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>snmp_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>snmp (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>stomp<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">36</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>stomp_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>stomp (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>xml<span style=\"color: #f92672;\">-</span>parser<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">37</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>xml<span style=\"color: #f92672;\">-</span>parser_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>xml<span style=\"color: #f92672;\">-</span>parser (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>getent<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">38</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>getent_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>getent (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>map<span style=\"color: #f92672;\">-</span>value<span style=\"color: #f92672;\">-</span>pairs<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">39</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>map<span style=\"color: #f92672;\">-</span>value<span style=\"color: #f92672;\">-</span>pairs_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>map<span style=\"color: #f92672;\">-</span>value<span style=\"color: #f92672;\">-</span>pairs (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Selecting previously unselected package syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>stardate<span style=\"color: #f92672;\">.</span>Preparing to unpack <span style=\"color: #f92672;\">.../</span><span style=\"color: #ae81ff;\">40</span><span style=\"color: #f92672;\">-</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>stardate_3<span style=\"color: #ae81ff;\">.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1_arm64<span style=\"color: #f92672;\">.</span>deb <span style=\"color: #f92672;\">...</span>Unpacking syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>stardate (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up librabbitmq4:arm64 (<span style=\"color: #ae81ff;\">0.10.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>) <span style=\"color: #f92672;\">...</span>Setting up libdbi1:arm64 (<span style=\"color: #ae81ff;\">0.9.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">6</span>) <span style=\"color: #f92672;\">...</span>Setting up libsnmp<span style=\"color: #f92672;\">-</span>base (<span style=\"color: #ae81ff;\">5.9</span><span style=\"color: #f92672;\">+</span>dfsg<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">4</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up libmaxminddb0:arm64 (<span style=\"color: #ae81ff;\">1.5.2</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>) <span style=\"color: #f92672;\">...</span>Setting up libsensors<span style=\"color: #f92672;\">-</span>config (<span style=\"color: #ae81ff;\">1</span>:<span style=\"color: #ae81ff;\">3.6.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">7</span>) <span style=\"color: #f92672;\">...</span>Setting up libesmtp6 (<span style=\"color: #ae81ff;\">1.0.6</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">4.3</span>) <span style=\"color: #f92672;\">...</span>Setting up libnet1:arm64 (<span style=\"color: #ae81ff;\">1.1.6</span><span style=\"color: #f92672;\">+</span>dfsg<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">3.1</span>) <span style=\"color: #f92672;\">...</span>Setting up libprotobuf<span style=\"color: #f92672;\">-</span>c1:arm64 (<span style=\"color: #ae81ff;\">1.3.3</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span><span style=\"color: #f92672;\">+</span>b2) <span style=\"color: #f92672;\">...</span>Setting up libsnappy1v5:arm64 (<span style=\"color: #ae81ff;\">1.1.8</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>) <span style=\"color: #f92672;\">...</span>Setting up libbson<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">0</span> (<span style=\"color: #ae81ff;\">1.17.6</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>) <span style=\"color: #f92672;\">...</span>Setting up libivykis0:arm64 (<span style=\"color: #ae81ff;\">0.42.4</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>) <span style=\"color: #f92672;\">...</span>Setting up libriemann<span style=\"color: #f92672;\">-</span>client0:arm64 (<span style=\"color: #ae81ff;\">1.10.4</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>b2) <span style=\"color: #f92672;\">...</span>Setting up libsensors5:arm64 (<span style=\"color: #ae81ff;\">1</span>:<span style=\"color: #ae81ff;\">3.6.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">7</span>) <span style=\"color: #f92672;\">...</span>Setting up librdkafka1:arm64 (<span style=\"color: #ae81ff;\">1.6.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>) <span style=\"color: #f92672;\">...</span>Setting up libhiredis0<span style=\"color: #ae81ff;\">.14</span>:arm64 (<span style=\"color: #ae81ff;\">0.14.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>) <span style=\"color: #f92672;\">...</span>Setting up libmongocrypt0:arm64 (<span style=\"color: #ae81ff;\">1.1.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>) <span style=\"color: #f92672;\">...</span>Setting up libsnmp40:arm64 (<span style=\"color: #ae81ff;\">5.9</span><span style=\"color: #f92672;\">+</span>dfsg<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">4</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up libmongoc<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1.0</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">0</span> (<span style=\"color: #ae81ff;\">1.17.6</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span>) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>core (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Created symlink <span style=\"color: #f92672;\">/</span>etc<span style=\"color: #f92672;\">/</span>systemd<span style=\"color: #f92672;\">/</span>system<span style=\"color: #f92672;\">/</span>multi<span style=\"color: #f92672;\">-</span>user<span style=\"color: #f92672;\">.</span>target<span style=\"color: #f92672;\">.</span>wants<span style=\"color: #f92672;\">/</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>service <span style=\"color: #960050; background-color: #1e0010;\">→</span> <span style=\"color: #f92672;\">/</span>lib<span style=\"color: #f92672;\">/</span>systemd<span style=\"color: #f92672;\">/</span>system<span style=\"color: #f92672;\">/</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>service<span style=\"color: #f92672;\">.</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>examples (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>xml<span style=\"color: #f92672;\">-</span>parser (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>stomp (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>riemann (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>stardate (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>geoip2 (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>getent (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>amqp (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>python (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>smtp (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>snmp (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>extra (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>rdkafka (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>graphite (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>add<span style=\"color: #f92672;\">-</span>contextual<span style=\"color: #f92672;\">-</span>data (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>mongodb (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>http (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>slog (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>map<span style=\"color: #f92672;\">-</span>value<span style=\"color: #f92672;\">-</span>pairs (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>sql (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">-</span>mod<span style=\"color: #f92672;\">-</span>redis (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Setting up syslog<span style=\"color: #f92672;\">-</span>ng (<span style=\"color: #ae81ff;\">3.28.1</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span><span style=\"color: #f92672;\">+</span>deb11u1) <span style=\"color: #f92672;\">...</span>Processing triggers <span style=\"color: #66d9ef;\">for</span> man<span style=\"color: #f92672;\">-</span>db (<span style=\"color: #ae81ff;\">2.9.4</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span>) <span style=\"color: #f92672;\">...</span>Processing triggers <span style=\"color: #66d9ef;\">for</span> libc<span style=\"color: #f92672;\">-</span>bin (<span style=\"color: #ae81ff;\">2.31</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">13</span><span style=\"color: #f92672;\">+</span>rpt2<span style=\"color: #f92672;\">+</span>rpi1<span style=\"color: #f92672;\">+</span>deb11u8) <span style=\"color: #f92672;\">...</span>Stderr: debconf: unable to initialize frontend: Dialogdebconf: (TERM <span style=\"color: #f92672;\">is</span> <span style=\"color: #f92672;\">not</span> set, so the dialog frontend <span style=\"color: #f92672;\">is</span> <span style=\"color: #f92672;\">not</span> usable<span style=\"color: #f92672;\">.</span>)debconf: falling back to frontend: Readlinedebconf: unable to initialize frontend: Readlinedebconf: (This frontend requires a controlling tty<span style=\"color: #f92672;\">.</span>)debconf: falling back to frontend: Teletypedpkg<span style=\"color: #f92672;\">-</span>preconfigure: unable to re<span style=\"color: #f92672;\">-</span>open stdin: <span style=\"color: #f92672;\">....</span><span style=\"color: #f92672;\">....</span></code></pre></div></details><br /><!-- raw HTML omitted -->7. Following the installation of <em>syslog-ng</em> across Nodes 2-7. We verify that the installation was successful by checking the <em>syslog-ng</em> service status.</p><p><details>  <strong>Output of <em>parallel-ssh -h /opt/workers -i &ldquo;systemctl status syslog-ng&rdquo;</em>. Click to expand</strong>  <div class=\"highlight\"><pre><code class=\"language-python\">root<span style=\"color: #a6e22e;\">@turingpi</span>:<span style=\"color: #f92672;\">~</span><span style=\"color: #75715e;\"># parallel-ssh -h /opt/workers -i \"systemctl status syslog-ng\" </span>[<span style=\"color: #ae81ff;\">1</span>] <span style=\"color: #ae81ff;\">14</span>:<span style=\"color: #ae81ff;\">03</span>:<span style=\"color: #ae81ff;\">46</span> [SUCCESS] kemeny<span style=\"color: #960050; background-color: #1e0010;\">●</span> syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>service <span style=\"color: #f92672;\">-</span> System Logger Daemon     Loaded: loaded (<span style=\"color: #f92672;\">/</span>lib<span style=\"color: #f92672;\">/</span>systemd<span style=\"color: #f92672;\">/</span>system<span style=\"color: #f92672;\">/</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>service; enabled; vendor preset: enabled)     Active: active (running) since Thu <span style=\"color: #ae81ff;\">2024</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">03</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">13</span>:<span style=\"color: #ae81ff;\">57</span>:<span style=\"color: #ae81ff;\">01</span> EDT; <span style=\"color: #ae81ff;\">6</span>min ago       Docs: man:syslog<span style=\"color: #f92672;\">-</span>ng(<span style=\"color: #ae81ff;\">8</span>)   Main PID: <span style=\"color: #ae81ff;\">28694</span> (syslog<span style=\"color: #f92672;\">-</span>ng)      Tasks: <span style=\"color: #ae81ff;\">2</span> (limit: <span style=\"color: #ae81ff;\">779</span>)        CPU: <span style=\"color: #ae81ff;\">40.228</span>s     CGroup: <span style=\"color: #f92672;\">/</span>system<span style=\"color: #f92672;\">.</span>slice<span style=\"color: #f92672;\">/</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>service             <span style=\"color: #960050; background-color: #1e0010;\">└─</span><span style=\"color: #ae81ff;\">28694</span> <span style=\"color: #f92672;\">/</span>usr<span style=\"color: #f92672;\">/</span>sbin<span style=\"color: #f92672;\">/</span>syslog<span style=\"color: #f92672;\">-</span>ng <span style=\"color: #f92672;\">-</span>FMar <span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">13</span>:<span style=\"color: #ae81ff;\">57</span>:<span style=\"color: #ae81ff;\">00</span> kemeny systemd[<span style=\"color: #ae81ff;\">1</span>]: Starting System Logger Daemon<span style=\"color: #f92672;\">...</span>Mar <span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">13</span>:<span style=\"color: #ae81ff;\">57</span>:<span style=\"color: #ae81ff;\">01</span> kemeny syslog<span style=\"color: #f92672;\">-</span>ng[<span style=\"color: #ae81ff;\">28694</span>]: DIGEST<span style=\"color: #f92672;\">-</span>MD5 common mech freeMar <span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">13</span>:<span style=\"color: #ae81ff;\">57</span>:<span style=\"color: #ae81ff;\">01</span> kemeny systemd[<span style=\"color: #ae81ff;\">1</span>]: Started System Logger Daemon<span style=\"color: #f92672;\">.</span>[<span style=\"color: #ae81ff;\">2</span>] <span style=\"color: #ae81ff;\">14</span>:<span style=\"color: #ae81ff;\">03</span>:<span style=\"color: #ae81ff;\">50</span> [SUCCESS] vonkarman<span style=\"color: #960050; background-color: #1e0010;\">●</span> syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>service <span style=\"color: #f92672;\">-</span> System Logger Daemon     Loaded: loaded (<span style=\"color: #f92672;\">/</span>lib<span style=\"color: #f92672;\">/</span>systemd<span style=\"color: #f92672;\">/</span>system<span style=\"color: #f92672;\">/</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>service; enabled; vendor preset: enabled)     Active: active (running) since Thu <span style=\"color: #ae81ff;\">2024</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">03</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">13</span>:<span style=\"color: #ae81ff;\">57</span>:<span style=\"color: #ae81ff;\">49</span> EDT; <span style=\"color: #ae81ff;\">5</span>min ago       Docs: man:syslog<span style=\"color: #f92672;\">-</span>ng(<span style=\"color: #ae81ff;\">8</span>)   Main PID: <span style=\"color: #ae81ff;\">27486</span> (syslog<span style=\"color: #f92672;\">-</span>ng)      Tasks: <span style=\"color: #ae81ff;\">2</span> (limit: <span style=\"color: #ae81ff;\">779</span>)        CPU: <span style=\"color: #ae81ff;\">2</span>min <span style=\"color: #ae81ff;\">5.540</span>s     CGroup: <span style=\"color: #f92672;\">/</span>system<span style=\"color: #f92672;\">.</span>slice<span style=\"color: #f92672;\">/</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>service             <span style=\"color: #960050; background-color: #1e0010;\">└─</span><span style=\"color: #ae81ff;\">27486</span> <span style=\"color: #f92672;\">/</span>usr<span style=\"color: #f92672;\">/</span>sbin<span style=\"color: #f92672;\">/</span>syslog<span style=\"color: #f92672;\">-</span>ng <span style=\"color: #f92672;\">-</span>FMar <span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">13</span>:<span style=\"color: #ae81ff;\">57</span>:<span style=\"color: #ae81ff;\">44</span> vonkarman systemd[<span style=\"color: #ae81ff;\">1</span>]: Starting System Logger Daemon<span style=\"color: #f92672;\">...</span>Mar <span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">13</span>:<span style=\"color: #ae81ff;\">57</span>:<span style=\"color: #ae81ff;\">46</span> vonkarman syslog<span style=\"color: #f92672;\">-</span>ng[<span style=\"color: #ae81ff;\">27486</span>]: DIGEST<span style=\"color: #f92672;\">-</span>MD5 common mech freeMar <span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">13</span>:<span style=\"color: #ae81ff;\">57</span>:<span style=\"color: #ae81ff;\">49</span> vonkarman systemd[<span style=\"color: #ae81ff;\">1</span>]: Started System Logger Daemon<span style=\"color: #f92672;\">.</span>[<span style=\"color: #ae81ff;\">3</span>] <span style=\"color: #ae81ff;\">14</span>:<span style=\"color: #ae81ff;\">03</span>:<span style=\"color: #ae81ff;\">51</span> [SUCCESS] teller<span style=\"color: #960050; background-color: #1e0010;\">●</span> syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>service <span style=\"color: #f92672;\">-</span> System Logger Daemon     Loaded: loaded (<span style=\"color: #f92672;\">/</span>lib<span style=\"color: #f92672;\">/</span>systemd<span style=\"color: #f92672;\">/</span>system<span style=\"color: #f92672;\">/</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>service; enabled; vendor preset: enabled)     Active: active (running) since Thu <span style=\"color: #ae81ff;\">2024</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">03</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">13</span>:<span style=\"color: #ae81ff;\">57</span>:<span style=\"color: #ae81ff;\">39</span> EDT; <span style=\"color: #ae81ff;\">6</span>min ago       Docs: man:syslog<span style=\"color: #f92672;\">-</span>ng(<span style=\"color: #ae81ff;\">8</span>)   Main PID: <span style=\"color: #ae81ff;\">24821</span> (syslog<span style=\"color: #f92672;\">-</span>ng)      Tasks: <span style=\"color: #ae81ff;\">2</span> (limit: <span style=\"color: #ae81ff;\">779</span>)        CPU: <span style=\"color: #ae81ff;\">2</span>min <span style=\"color: #ae81ff;\">262</span>ms     CGroup: <span style=\"color: #f92672;\">/</span>system<span style=\"color: #f92672;\">.</span>slice<span style=\"color: #f92672;\">/</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>service             <span style=\"color: #960050; background-color: #1e0010;\">└─</span><span style=\"color: #ae81ff;\">24821</span> <span style=\"color: #f92672;\">/</span>usr<span style=\"color: #f92672;\">/</span>sbin<span style=\"color: #f92672;\">/</span>syslog<span style=\"color: #f92672;\">-</span>ng <span style=\"color: #f92672;\">-</span>FMar <span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">13</span>:<span style=\"color: #ae81ff;\">57</span>:<span style=\"color: #ae81ff;\">38</span> teller systemd[<span style=\"color: #ae81ff;\">1</span>]: Starting System Logger Daemon<span style=\"color: #f92672;\">...</span>Mar <span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">13</span>:<span style=\"color: #ae81ff;\">57</span>:<span style=\"color: #ae81ff;\">38</span> teller syslog<span style=\"color: #f92672;\">-</span>ng[<span style=\"color: #ae81ff;\">24821</span>]: DIGEST<span style=\"color: #f92672;\">-</span>MD5 common mech freeMar <span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">13</span>:<span style=\"color: #ae81ff;\">57</span>:<span style=\"color: #ae81ff;\">39</span> teller systemd[<span style=\"color: #ae81ff;\">1</span>]: Started System Logger Daemon<span style=\"color: #f92672;\">.</span>[<span style=\"color: #ae81ff;\">4</span>] <span style=\"color: #ae81ff;\">14</span>:<span style=\"color: #ae81ff;\">03</span>:<span style=\"color: #ae81ff;\">53</span> [SUCCESS] neumann<span style=\"color: #960050; background-color: #1e0010;\">●</span> syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>service <span style=\"color: #f92672;\">-</span> System Logger Daemon     Loaded: loaded (<span style=\"color: #f92672;\">/</span>lib<span style=\"color: #f92672;\">/</span>systemd<span style=\"color: #f92672;\">/</span>system<span style=\"color: #f92672;\">/</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>service; enabled; vendor preset: enabled)     Active: active (running) since Thu <span style=\"color: #ae81ff;\">2024</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">03</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">13</span>:<span style=\"color: #ae81ff;\">57</span>:<span style=\"color: #ae81ff;\">39</span> EDT; <span style=\"color: #ae81ff;\">6</span>min ago       Docs: man:syslog<span style=\"color: #f92672;\">-</span>ng(<span style=\"color: #ae81ff;\">8</span>)   Main PID: <span style=\"color: #ae81ff;\">27734</span> (syslog<span style=\"color: #f92672;\">-</span>ng)      Tasks: <span style=\"color: #ae81ff;\">2</span> (limit: <span style=\"color: #ae81ff;\">779</span>)        CPU: <span style=\"color: #ae81ff;\">1</span>min <span style=\"color: #ae81ff;\">43.504</span>s     CGroup: <span style=\"color: #f92672;\">/</span>system<span style=\"color: #f92672;\">.</span>slice<span style=\"color: #f92672;\">/</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>service             <span style=\"color: #960050; background-color: #1e0010;\">└─</span><span style=\"color: #ae81ff;\">27734</span> <span style=\"color: #f92672;\">/</span>usr<span style=\"color: #f92672;\">/</span>sbin<span style=\"color: #f92672;\">/</span>syslog<span style=\"color: #f92672;\">-</span>ng <span style=\"color: #f92672;\">-</span>FMar <span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">13</span>:<span style=\"color: #ae81ff;\">57</span>:<span style=\"color: #ae81ff;\">38</span> neumann systemd[<span style=\"color: #ae81ff;\">1</span>]: Starting System Logger Daemon<span style=\"color: #f92672;\">...</span>Mar <span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">13</span>:<span style=\"color: #ae81ff;\">57</span>:<span style=\"color: #ae81ff;\">38</span> neumann syslog<span style=\"color: #f92672;\">-</span>ng[<span style=\"color: #ae81ff;\">27734</span>]: DIGEST<span style=\"color: #f92672;\">-</span>MD5 common mech freeMar <span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">13</span>:<span style=\"color: #ae81ff;\">57</span>:<span style=\"color: #ae81ff;\">39</span> neumann systemd[<span style=\"color: #ae81ff;\">1</span>]: Started System Logger Daemon<span style=\"color: #f92672;\">.</span>[<span style=\"color: #ae81ff;\">5</span>] <span style=\"color: #ae81ff;\">14</span>:<span style=\"color: #ae81ff;\">03</span>:<span style=\"color: #ae81ff;\">53</span> [SUCCESS] wigner<span style=\"color: #960050; background-color: #1e0010;\">●</span> syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>service <span style=\"color: #f92672;\">-</span> System Logger Daemon     Loaded: loaded (<span style=\"color: #f92672;\">/</span>lib<span style=\"color: #f92672;\">/</span>systemd<span style=\"color: #f92672;\">/</span>system<span style=\"color: #f92672;\">/</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>service; enabled; vendor preset: enabled)     Active: active (running) since Thu <span style=\"color: #ae81ff;\">2024</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">03</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">13</span>:<span style=\"color: #ae81ff;\">57</span>:<span style=\"color: #ae81ff;\">37</span> EDT; <span style=\"color: #ae81ff;\">6</span>min ago       Docs: man:syslog<span style=\"color: #f92672;\">-</span>ng(<span style=\"color: #ae81ff;\">8</span>)   Main PID: <span style=\"color: #ae81ff;\">27512</span> (syslog<span style=\"color: #f92672;\">-</span>ng)      Tasks: <span style=\"color: #ae81ff;\">2</span> (limit: <span style=\"color: #ae81ff;\">779</span>)        CPU: <span style=\"color: #ae81ff;\">1</span>min <span style=\"color: #ae81ff;\">49.643</span>s     CGroup: <span style=\"color: #f92672;\">/</span>system<span style=\"color: #f92672;\">.</span>slice<span style=\"color: #f92672;\">/</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>service             <span style=\"color: #960050; background-color: #1e0010;\">└─</span><span style=\"color: #ae81ff;\">27512</span> <span style=\"color: #f92672;\">/</span>usr<span style=\"color: #f92672;\">/</span>sbin<span style=\"color: #f92672;\">/</span>syslog<span style=\"color: #f92672;\">-</span>ng <span style=\"color: #f92672;\">-</span>FMar <span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">13</span>:<span style=\"color: #ae81ff;\">57</span>:<span style=\"color: #ae81ff;\">36</span> wigner systemd[<span style=\"color: #ae81ff;\">1</span>]: Starting System Logger Daemon<span style=\"color: #f92672;\">...</span>Mar <span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">13</span>:<span style=\"color: #ae81ff;\">57</span>:<span style=\"color: #ae81ff;\">36</span> wigner syslog<span style=\"color: #f92672;\">-</span>ng[<span style=\"color: #ae81ff;\">27512</span>]: DIGEST<span style=\"color: #f92672;\">-</span>MD5 common mech freeMar <span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">13</span>:<span style=\"color: #ae81ff;\">57</span>:<span style=\"color: #ae81ff;\">37</span> wigner systemd[<span style=\"color: #ae81ff;\">1</span>]: Started System Logger Daemon<span style=\"color: #f92672;\">.</span>[<span style=\"color: #ae81ff;\">6</span>] <span style=\"color: #ae81ff;\">14</span>:<span style=\"color: #ae81ff;\">03</span>:<span style=\"color: #ae81ff;\">57</span> [SUCCESS] szilard<span style=\"color: #960050; background-color: #1e0010;\">●</span> syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>service <span style=\"color: #f92672;\">-</span> System Logger Daemon     Loaded: loaded (<span style=\"color: #f92672;\">/</span>lib<span style=\"color: #f92672;\">/</span>systemd<span style=\"color: #f92672;\">/</span>system<span style=\"color: #f92672;\">/</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>service; enabled; vendor preset: enabled)     Active: active (running) since Thu <span style=\"color: #ae81ff;\">2024</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">03</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">13</span>:<span style=\"color: #ae81ff;\">57</span>:<span style=\"color: #ae81ff;\">35</span> EDT; <span style=\"color: #ae81ff;\">6</span>min ago       Docs: man:syslog<span style=\"color: #f92672;\">-</span>ng(<span style=\"color: #ae81ff;\">8</span>)   Main PID: <span style=\"color: #ae81ff;\">24136</span> (syslog<span style=\"color: #f92672;\">-</span>ng)      Tasks: <span style=\"color: #ae81ff;\">5</span> (limit: <span style=\"color: #ae81ff;\">779</span>)        CPU: <span style=\"color: #ae81ff;\">2</span>min <span style=\"color: #ae81ff;\">10.257</span>s     CGroup: <span style=\"color: #f92672;\">/</span>system<span style=\"color: #f92672;\">.</span>slice<span style=\"color: #f92672;\">/</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>service             <span style=\"color: #960050; background-color: #1e0010;\">└─</span><span style=\"color: #ae81ff;\">24136</span> <span style=\"color: #f92672;\">/</span>usr<span style=\"color: #f92672;\">/</span>sbin<span style=\"color: #f92672;\">/</span>syslog<span style=\"color: #f92672;\">-</span>ng <span style=\"color: #f92672;\">-</span>FMar <span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">13</span>:<span style=\"color: #ae81ff;\">57</span>:<span style=\"color: #ae81ff;\">34</span> szilard systemd[<span style=\"color: #ae81ff;\">1</span>]: Starting System Logger Daemon<span style=\"color: #f92672;\">...</span>Mar <span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">13</span>:<span style=\"color: #ae81ff;\">57</span>:<span style=\"color: #ae81ff;\">34</span> szilard syslog<span style=\"color: #f92672;\">-</span>ng[<span style=\"color: #ae81ff;\">24136</span>]: DIGEST<span style=\"color: #f92672;\">-</span>MD5 common mech freeMar <span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">13</span>:<span style=\"color: #ae81ff;\">57</span>:<span style=\"color: #ae81ff;\">35</span> szilard systemd[<span style=\"color: #ae81ff;\">1</span>]: Started System Logger Daemon<span style=\"color: #f92672;\">.</span></code></pre></div></details><br /><!-- raw HTML omitted -->8. Create the  configuration file <em>send.conf</em> in <em>/opt</em> on host <em>turingpi</em>. Note that <em>/opt</em> is an NFS export on <em>turingpi</em> and is NFS mounted by all of the compute nodes. This file will set the HOST field to the local hostname for log messages that are sent. This in done in the subsequent steps where <em>“placeholder”</em> will be replaced using a <em>sed</em> operation with the local hostname. Additionally, a data source <em>s_hpc</em> is defined which will scan <em>/opt/ibm/lsf/log</em> for the presence of LSF daemon logfiles.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\"> root@turingpi:/# cat /opt/send.confrewrite r_host { set(\"placeholder\", value(\"HOST\")); };destination d_net {  syslog(\"turingpi\" port(601));};source s_hpc {  wildcard-file(      base-dir(\"/opt/ibm/lsf/log\")      filename-pattern(\"*.log.*\")      recursive(no)      follow-freq(1)  );};log {  source(s_src);  source(s_hpc);  rewrite(r_host);   destination(d_net);};</code></pre></div><ol start=\"9\"><li>On Nodes 2-7, copy the file <em>/opt/send.conf</em> to <em>/etc/syslog-ng/conf.d/send.conf</em>.</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\"> root@turingpi:/# parallel-ssh -h /opt/workers -i \"cp /opt/send.conf /etc/syslog-ng/conf.d\" [1] 14:19:29 [SUCCESS] kemeny[2] 14:19:30 [SUCCESS] vonkarman[3] 14:19:30 [SUCCESS] wigner[4] 14:19:30 [SUCCESS] szilard[5] 14:19:30 [SUCCESS] teller[6] 14:19:31 [SUCCESS] neumann</code></pre></div><ol start=\"10\"><li>Using <em>sed</em>, replace the <em>“placeholder”</em> string in <em>/etc/syslog-ng/conf.d/send.conf</em> with the local hostname. And we also double check that the change was correctly made.</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\"> root@turingpi:/# parallel-ssh -h /opt/workers -i 'HOST=`hostname`; sed -i \"s/placeholder/$HOST/g\" /etc/syslog-ng/conf.d/send.conf' [1] 14:38:09 [SUCCESS] kemeny[2] 14:38:09 [SUCCESS] teller[3] 14:38:09 [SUCCESS] vonkarman[4] 14:38:09 [SUCCESS] wigner[5] 14:38:09 [SUCCESS] neumann[6] 14:38:09 [SUCCESS] szilard</code></pre></div><p><details>  <strong>Output of <em>parallel-ssh -h /opt/workers -i &ldquo;cat /etc/syslog-ng/conf.d/send.conf&rdquo;</em>. Click to expand</strong>  <div class=\"highlight\"><pre><code class=\"language-python\">root<span style=\"color: #a6e22e;\">@turingpi</span>:<span style=\"color: #f92672;\">/</span><span style=\"color: #75715e;\"># parallel-ssh -h /opt/workers -i \"cat /etc/syslog-ng/conf.d/send.conf\" [1] 14:38:33 [SUCCESS] kemeny</span>rewrite r_host { set(<span style=\"color: #e6db74;\">\"kemeny\"</span>, value(<span style=\"color: #e6db74;\">\"HOST\"</span>)); };destination d_net {  syslog(<span style=\"color: #e6db74;\">\"turingpi\"</span> port(<span style=\"color: #ae81ff;\">601</span>));};source s_hpc {  wildcard<span style=\"color: #f92672;\">-</span>file(      base<span style=\"color: #f92672;\">-</span>dir(<span style=\"color: #e6db74;\">\"/opt/ibm/lsf/log\"</span>)      filename<span style=\"color: #f92672;\">-</span>pattern(<span style=\"color: #e6db74;\">\"*.log.*\"</span>)      recursive(no)      follow<span style=\"color: #f92672;\">-</span>freq(<span style=\"color: #ae81ff;\">1</span>)  );};log {  source(s_sys);  source(s_hpc);  rewrite(r_host);   destination(d_net);};[<span style=\"color: #ae81ff;\">2</span>] <span style=\"color: #ae81ff;\">14</span>:<span style=\"color: #ae81ff;\">38</span>:<span style=\"color: #ae81ff;\">33</span> [SUCCESS] tellerrewrite r_host { set(<span style=\"color: #e6db74;\">\"teller\"</span>, value(<span style=\"color: #e6db74;\">\"HOST\"</span>)); };destination d_net {  syslog(<span style=\"color: #e6db74;\">\"turingpi\"</span> port(<span style=\"color: #ae81ff;\">601</span>));};source s_hpc {  wildcard<span style=\"color: #f92672;\">-</span>file(      base<span style=\"color: #f92672;\">-</span>dir(<span style=\"color: #e6db74;\">\"/opt/ibm/lsf/log\"</span>)      filename<span style=\"color: #f92672;\">-</span>pattern(<span style=\"color: #e6db74;\">\"*.log.*\"</span>)      recursive(no)      follow<span style=\"color: #f92672;\">-</span>freq(<span style=\"color: #ae81ff;\">1</span>)  );};log {  source(s_sys);  source(s_hpc);  rewrite(r_host);   destination(d_net);};[<span style=\"color: #ae81ff;\">3</span>] <span style=\"color: #ae81ff;\">14</span>:<span style=\"color: #ae81ff;\">38</span>:<span style=\"color: #ae81ff;\">33</span> [SUCCESS] neumannrewrite r_host { set(<span style=\"color: #e6db74;\">\"neumann\"</span>, value(<span style=\"color: #e6db74;\">\"HOST\"</span>)); };destination d_net {  syslog(<span style=\"color: #e6db74;\">\"turingpi\"</span> port(<span style=\"color: #ae81ff;\">601</span>));};source s_hpc {  wildcard<span style=\"color: #f92672;\">-</span>file(      base<span style=\"color: #f92672;\">-</span>dir(<span style=\"color: #e6db74;\">\"/opt/ibm/lsf/log\"</span>)      filename<span style=\"color: #f92672;\">-</span>pattern(<span style=\"color: #e6db74;\">\"*.log.*\"</span>)      recursive(no)      follow<span style=\"color: #f92672;\">-</span>freq(<span style=\"color: #ae81ff;\">1</span>)  );};log {  source(s_sys);  source(s_hpc);  rewrite(r_host);   destination(d_net);};[<span style=\"color: #ae81ff;\">4</span>] <span style=\"color: #ae81ff;\">14</span>:<span style=\"color: #ae81ff;\">38</span>:<span style=\"color: #ae81ff;\">33</span> [SUCCESS] szilardrewrite r_host { set(<span style=\"color: #e6db74;\">\"szilard\"</span>, value(<span style=\"color: #e6db74;\">\"HOST\"</span>)); };destination d_net {  syslog(<span style=\"color: #e6db74;\">\"turingpi\"</span> port(<span style=\"color: #ae81ff;\">601</span>));};source s_hpc {  wildcard<span style=\"color: #f92672;\">-</span>file(      base<span style=\"color: #f92672;\">-</span>dir(<span style=\"color: #e6db74;\">\"/opt/ibm/lsf/log\"</span>)      filename<span style=\"color: #f92672;\">-</span>pattern(<span style=\"color: #e6db74;\">\"*.log.*\"</span>)      recursive(no)      follow<span style=\"color: #f92672;\">-</span>freq(<span style=\"color: #ae81ff;\">1</span>)  );};log {  source(s_sys);  source(s_hpc);  rewrite(r_host);   destination(d_net);};[<span style=\"color: #ae81ff;\">5</span>] <span style=\"color: #ae81ff;\">14</span>:<span style=\"color: #ae81ff;\">38</span>:<span style=\"color: #ae81ff;\">33</span> [SUCCESS] wignerrewrite r_host { set(<span style=\"color: #e6db74;\">\"wigner\"</span>, value(<span style=\"color: #e6db74;\">\"HOST\"</span>)); };destination d_net {  syslog(<span style=\"color: #e6db74;\">\"turingpi\"</span> port(<span style=\"color: #ae81ff;\">601</span>));};source s_hpc {  wildcard<span style=\"color: #f92672;\">-</span>file(      base<span style=\"color: #f92672;\">-</span>dir(<span style=\"color: #e6db74;\">\"/opt/ibm/lsf/log\"</span>)      filename<span style=\"color: #f92672;\">-</span>pattern(<span style=\"color: #e6db74;\">\"*.log.*\"</span>)      recursive(no)      follow<span style=\"color: #f92672;\">-</span>freq(<span style=\"color: #ae81ff;\">1</span>)  );};log {  source(s_sys);  source(s_hpc);  rewrite(r_host);   destination(d_net);};[<span style=\"color: #ae81ff;\">6</span>] <span style=\"color: #ae81ff;\">14</span>:<span style=\"color: #ae81ff;\">38</span>:<span style=\"color: #ae81ff;\">33</span> [SUCCESS] vonkarmanrewrite r_host { set(<span style=\"color: #e6db74;\">\"vonkarman\"</span>, value(<span style=\"color: #e6db74;\">\"HOST\"</span>)); };destination d_net {  syslog(<span style=\"color: #e6db74;\">\"turingpi\"</span> port(<span style=\"color: #ae81ff;\">601</span>));};source s_hpc {  wildcard<span style=\"color: #f92672;\">-</span>file(      base<span style=\"color: #f92672;\">-</span>dir(<span style=\"color: #e6db74;\">\"/opt/ibm/lsf/log\"</span>)      filename<span style=\"color: #f92672;\">-</span>pattern(<span style=\"color: #e6db74;\">\"*.log.*\"</span>)      recursive(no)      follow<span style=\"color: #f92672;\">-</span>freq(<span style=\"color: #ae81ff;\">1</span>)  );};log {  source(s_sys);  source(s_hpc);  rewrite(r_host);   destination(d_net);};</code></pre></div></details><br /><!-- raw HTML omitted -->11. Finally, <em>syslog-ng</em> is restarted on Nodes 2-7 and the status of the service is checked to ensure that there are no errors.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\"> root@turingpi:/opt# parallel-ssh -h /opt/workers -i \"systemctl restart syslog-ng\" [1] 14:49:03 [SUCCESS] kemeny[2] 14:49:05 [SUCCESS] szilard[3] 14:49:06 [SUCCESS] vonkarman[4] 14:49:06 [SUCCESS] neumann[5] 14:49:06 [SUCCESS] teller[6] 14:49:07 [SUCCESS] wigner</code></pre></div><p><details>  <strong>Output of <em>parallel-ssh -h /opt/workers -i &ldquo;systemctl status syslog-ng&rdquo;</em>. Click to expand</strong>  <div class=\"highlight\"><pre><code class=\"language-python\">root<span style=\"color: #a6e22e;\">@turingpi</span>:<span style=\"color: #f92672;\">/</span>opt<span style=\"color: #75715e;\"># parallel-ssh -h /opt/workers -i \"systemctl status syslog-ng\" </span>[<span style=\"color: #ae81ff;\">1</span>] <span style=\"color: #ae81ff;\">14</span>:<span style=\"color: #ae81ff;\">49</span>:<span style=\"color: #ae81ff;\">31</span> [SUCCESS] kemeny<span style=\"color: #960050; background-color: #1e0010;\">●</span> syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>service <span style=\"color: #f92672;\">-</span> System Logger Daemon     Loaded: loaded (<span style=\"color: #f92672;\">/</span>lib<span style=\"color: #f92672;\">/</span>systemd<span style=\"color: #f92672;\">/</span>system<span style=\"color: #f92672;\">/</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>service; enabled; vendor preset: enabled)     Active: active (running) since Thu <span style=\"color: #ae81ff;\">2024</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">03</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">14</span>:<span style=\"color: #ae81ff;\">49</span>:<span style=\"color: #ae81ff;\">03</span> EDT; <span style=\"color: #ae81ff;\">28</span>s ago       Docs: man:syslog<span style=\"color: #f92672;\">-</span>ng(<span style=\"color: #ae81ff;\">8</span>)   Main PID: <span style=\"color: #ae81ff;\">34982</span> (syslog<span style=\"color: #f92672;\">-</span>ng)      Tasks: <span style=\"color: #ae81ff;\">2</span> (limit: <span style=\"color: #ae81ff;\">779</span>)        CPU: <span style=\"color: #ae81ff;\">398</span>ms     CGroup: <span style=\"color: #f92672;\">/</span>system<span style=\"color: #f92672;\">.</span>slice<span style=\"color: #f92672;\">/</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>service             <span style=\"color: #960050; background-color: #1e0010;\">└─</span><span style=\"color: #ae81ff;\">34982</span> <span style=\"color: #f92672;\">/</span>usr<span style=\"color: #f92672;\">/</span>sbin<span style=\"color: #f92672;\">/</span>syslog<span style=\"color: #f92672;\">-</span>ng <span style=\"color: #f92672;\">-</span>FMar <span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">14</span>:<span style=\"color: #ae81ff;\">49</span>:<span style=\"color: #ae81ff;\">02</span> kemeny systemd[<span style=\"color: #ae81ff;\">1</span>]: Starting System Logger Daemon<span style=\"color: #f92672;\">...</span>Mar <span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">14</span>:<span style=\"color: #ae81ff;\">49</span>:<span style=\"color: #ae81ff;\">02</span> kemeny syslog<span style=\"color: #f92672;\">-</span>ng[<span style=\"color: #ae81ff;\">34982</span>]: DIGEST<span style=\"color: #f92672;\">-</span>MD5 common mech freeMar <span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">14</span>:<span style=\"color: #ae81ff;\">49</span>:<span style=\"color: #ae81ff;\">03</span> kemeny systemd[<span style=\"color: #ae81ff;\">1</span>]: Started System Logger Daemon<span style=\"color: #f92672;\">.</span>[<span style=\"color: #ae81ff;\">2</span>] <span style=\"color: #ae81ff;\">14</span>:<span style=\"color: #ae81ff;\">49</span>:<span style=\"color: #ae81ff;\">33</span> [SUCCESS] vonkarman<span style=\"color: #960050; background-color: #1e0010;\">●</span> syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>service <span style=\"color: #f92672;\">-</span> System Logger Daemon     Loaded: loaded (<span style=\"color: #f92672;\">/</span>lib<span style=\"color: #f92672;\">/</span>systemd<span style=\"color: #f92672;\">/</span>system<span style=\"color: #f92672;\">/</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>service; enabled; vendor preset: enabled)     Active: active (running) since Thu <span style=\"color: #ae81ff;\">2024</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">03</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">14</span>:<span style=\"color: #ae81ff;\">49</span>:<span style=\"color: #ae81ff;\">06</span> EDT; <span style=\"color: #ae81ff;\">25</span>s ago       Docs: man:syslog<span style=\"color: #f92672;\">-</span>ng(<span style=\"color: #ae81ff;\">8</span>)   Main PID: <span style=\"color: #ae81ff;\">33710</span> (syslog<span style=\"color: #f92672;\">-</span>ng)      Tasks: <span style=\"color: #ae81ff;\">2</span> (limit: <span style=\"color: #ae81ff;\">779</span>)        CPU: <span style=\"color: #ae81ff;\">934</span>ms     CGroup: <span style=\"color: #f92672;\">/</span>system<span style=\"color: #f92672;\">.</span>slice<span style=\"color: #f92672;\">/</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>service             <span style=\"color: #960050; background-color: #1e0010;\">└─</span><span style=\"color: #ae81ff;\">33710</span> <span style=\"color: #f92672;\">/</span>usr<span style=\"color: #f92672;\">/</span>sbin<span style=\"color: #f92672;\">/</span>syslog<span style=\"color: #f92672;\">-</span>ng <span style=\"color: #f92672;\">-</span>FMar <span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">14</span>:<span style=\"color: #ae81ff;\">49</span>:<span style=\"color: #ae81ff;\">03</span> vonkarman systemd[<span style=\"color: #ae81ff;\">1</span>]: Starting System Logger Daemon<span style=\"color: #f92672;\">...</span>Mar <span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">14</span>:<span style=\"color: #ae81ff;\">49</span>:<span style=\"color: #ae81ff;\">03</span> vonkarman syslog<span style=\"color: #f92672;\">-</span>ng[<span style=\"color: #ae81ff;\">33710</span>]: DIGEST<span style=\"color: #f92672;\">-</span>MD5 common mech freeMar <span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">14</span>:<span style=\"color: #ae81ff;\">49</span>:<span style=\"color: #ae81ff;\">06</span> vonkarman systemd[<span style=\"color: #ae81ff;\">1</span>]: Started System Logger Daemon<span style=\"color: #f92672;\">.</span>[<span style=\"color: #ae81ff;\">3</span>] <span style=\"color: #ae81ff;\">14</span>:<span style=\"color: #ae81ff;\">49</span>:<span style=\"color: #ae81ff;\">33</span> [SUCCESS] neumann<span style=\"color: #960050; background-color: #1e0010;\">●</span> syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>service <span style=\"color: #f92672;\">-</span> System Logger Daemon     Loaded: loaded (<span style=\"color: #f92672;\">/</span>lib<span style=\"color: #f92672;\">/</span>systemd<span style=\"color: #f92672;\">/</span>system<span style=\"color: #f92672;\">/</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>service; enabled; vendor preset: enabled)     Active: active (running) since Thu <span style=\"color: #ae81ff;\">2024</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">03</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">14</span>:<span style=\"color: #ae81ff;\">49</span>:<span style=\"color: #ae81ff;\">06</span> EDT; <span style=\"color: #ae81ff;\">25</span>s ago       Docs: man:syslog<span style=\"color: #f92672;\">-</span>ng(<span style=\"color: #ae81ff;\">8</span>)   Main PID: <span style=\"color: #ae81ff;\">34000</span> (syslog<span style=\"color: #f92672;\">-</span>ng)      Tasks: <span style=\"color: #ae81ff;\">2</span> (limit: <span style=\"color: #ae81ff;\">779</span>)        CPU: <span style=\"color: #ae81ff;\">959</span>ms     CGroup: <span style=\"color: #f92672;\">/</span>system<span style=\"color: #f92672;\">.</span>slice<span style=\"color: #f92672;\">/</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>service             <span style=\"color: #960050; background-color: #1e0010;\">└─</span><span style=\"color: #ae81ff;\">34000</span> <span style=\"color: #f92672;\">/</span>usr<span style=\"color: #f92672;\">/</span>sbin<span style=\"color: #f92672;\">/</span>syslog<span style=\"color: #f92672;\">-</span>ng <span style=\"color: #f92672;\">-</span>FMar <span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">14</span>:<span style=\"color: #ae81ff;\">49</span>:<span style=\"color: #ae81ff;\">03</span> neumann systemd[<span style=\"color: #ae81ff;\">1</span>]: Starting System Logger Daemon<span style=\"color: #f92672;\">...</span>Mar <span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">14</span>:<span style=\"color: #ae81ff;\">49</span>:<span style=\"color: #ae81ff;\">03</span> neumann syslog<span style=\"color: #f92672;\">-</span>ng[<span style=\"color: #ae81ff;\">34000</span>]: DIGEST<span style=\"color: #f92672;\">-</span>MD5 common mech freeMar <span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">14</span>:<span style=\"color: #ae81ff;\">49</span>:<span style=\"color: #ae81ff;\">06</span> neumann systemd[<span style=\"color: #ae81ff;\">1</span>]: Started System Logger Daemon<span style=\"color: #f92672;\">.</span>[<span style=\"color: #ae81ff;\">4</span>] <span style=\"color: #ae81ff;\">14</span>:<span style=\"color: #ae81ff;\">49</span>:<span style=\"color: #ae81ff;\">33</span> [SUCCESS] wigner<span style=\"color: #960050; background-color: #1e0010;\">●</span> syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>service <span style=\"color: #f92672;\">-</span> System Logger Daemon     Loaded: loaded (<span style=\"color: #f92672;\">/</span>lib<span style=\"color: #f92672;\">/</span>systemd<span style=\"color: #f92672;\">/</span>system<span style=\"color: #f92672;\">/</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>service; enabled; vendor preset: enabled)     Active: active (running) since Thu <span style=\"color: #ae81ff;\">2024</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">03</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">14</span>:<span style=\"color: #ae81ff;\">49</span>:<span style=\"color: #ae81ff;\">07</span> EDT; <span style=\"color: #ae81ff;\">25</span>s ago       Docs: man:syslog<span style=\"color: #f92672;\">-</span>ng(<span style=\"color: #ae81ff;\">8</span>)   Main PID: <span style=\"color: #ae81ff;\">33941</span> (syslog<span style=\"color: #f92672;\">-</span>ng)      Tasks: <span style=\"color: #ae81ff;\">2</span> (limit: <span style=\"color: #ae81ff;\">779</span>)        CPU: <span style=\"color: #ae81ff;\">1.115</span>s     CGroup: <span style=\"color: #f92672;\">/</span>system<span style=\"color: #f92672;\">.</span>slice<span style=\"color: #f92672;\">/</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>service             <span style=\"color: #960050; background-color: #1e0010;\">└─</span><span style=\"color: #ae81ff;\">33941</span> <span style=\"color: #f92672;\">/</span>usr<span style=\"color: #f92672;\">/</span>sbin<span style=\"color: #f92672;\">/</span>syslog<span style=\"color: #f92672;\">-</span>ng <span style=\"color: #f92672;\">-</span>FMar <span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">14</span>:<span style=\"color: #ae81ff;\">49</span>:<span style=\"color: #ae81ff;\">03</span> wigner systemd[<span style=\"color: #ae81ff;\">1</span>]: Starting System Logger Daemon<span style=\"color: #f92672;\">...</span>Mar <span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">14</span>:<span style=\"color: #ae81ff;\">49</span>:<span style=\"color: #ae81ff;\">04</span> wigner syslog<span style=\"color: #f92672;\">-</span>ng[<span style=\"color: #ae81ff;\">33941</span>]: DIGEST<span style=\"color: #f92672;\">-</span>MD5 common mech freeMar <span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">14</span>:<span style=\"color: #ae81ff;\">49</span>:<span style=\"color: #ae81ff;\">07</span> wigner systemd[<span style=\"color: #ae81ff;\">1</span>]: Started System Logger Daemon<span style=\"color: #f92672;\">.</span>[<span style=\"color: #ae81ff;\">5</span>] <span style=\"color: #ae81ff;\">14</span>:<span style=\"color: #ae81ff;\">49</span>:<span style=\"color: #ae81ff;\">34</span> [SUCCESS] szilard<span style=\"color: #960050; background-color: #1e0010;\">●</span> syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>service <span style=\"color: #f92672;\">-</span> System Logger Daemon     Loaded: loaded (<span style=\"color: #f92672;\">/</span>lib<span style=\"color: #f92672;\">/</span>systemd<span style=\"color: #f92672;\">/</span>system<span style=\"color: #f92672;\">/</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>service; enabled; vendor preset: enabled)     Active: active (running) since Thu <span style=\"color: #ae81ff;\">2024</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">03</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">14</span>:<span style=\"color: #ae81ff;\">49</span>:<span style=\"color: #ae81ff;\">05</span> EDT; <span style=\"color: #ae81ff;\">26</span>s ago       Docs: man:syslog<span style=\"color: #f92672;\">-</span>ng(<span style=\"color: #ae81ff;\">8</span>)   Main PID: <span style=\"color: #ae81ff;\">30348</span> (syslog<span style=\"color: #f92672;\">-</span>ng)      Tasks: <span style=\"color: #ae81ff;\">2</span> (limit: <span style=\"color: #ae81ff;\">779</span>)        CPU: <span style=\"color: #ae81ff;\">816</span>ms     CGroup: <span style=\"color: #f92672;\">/</span>system<span style=\"color: #f92672;\">.</span>slice<span style=\"color: #f92672;\">/</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>service             <span style=\"color: #960050; background-color: #1e0010;\">└─</span><span style=\"color: #ae81ff;\">30348</span> <span style=\"color: #f92672;\">/</span>usr<span style=\"color: #f92672;\">/</span>sbin<span style=\"color: #f92672;\">/</span>syslog<span style=\"color: #f92672;\">-</span>ng <span style=\"color: #f92672;\">-</span>FMar <span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">14</span>:<span style=\"color: #ae81ff;\">49</span>:<span style=\"color: #ae81ff;\">03</span> szilard systemd[<span style=\"color: #ae81ff;\">1</span>]: Starting System Logger Daemon<span style=\"color: #f92672;\">...</span>Mar <span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">14</span>:<span style=\"color: #ae81ff;\">49</span>:<span style=\"color: #ae81ff;\">03</span> szilard syslog<span style=\"color: #f92672;\">-</span>ng[<span style=\"color: #ae81ff;\">30348</span>]: DIGEST<span style=\"color: #f92672;\">-</span>MD5 common mech freeMar <span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">14</span>:<span style=\"color: #ae81ff;\">49</span>:<span style=\"color: #ae81ff;\">05</span> szilard systemd[<span style=\"color: #ae81ff;\">1</span>]: Started System Logger Daemon<span style=\"color: #f92672;\">.</span>[<span style=\"color: #ae81ff;\">6</span>] <span style=\"color: #ae81ff;\">14</span>:<span style=\"color: #ae81ff;\">49</span>:<span style=\"color: #ae81ff;\">34</span> [SUCCESS] teller<span style=\"color: #960050; background-color: #1e0010;\">●</span> syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>service <span style=\"color: #f92672;\">-</span> System Logger Daemon     Loaded: loaded (<span style=\"color: #f92672;\">/</span>lib<span style=\"color: #f92672;\">/</span>systemd<span style=\"color: #f92672;\">/</span>system<span style=\"color: #f92672;\">/</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>service; enabled; vendor preset: enabled)     Active: active (running) since Thu <span style=\"color: #ae81ff;\">2024</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">03</span><span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">28</span> <span style=\"color: #ae81ff;\">14</span>:<span style=\"color: #ae81ff;\">49</span>:<span style=\"color: #ae81ff;\">06</span> EDT; <span style=\"color: #ae81ff;\">25</span>s ago       Docs: man:syslog<span style=\"color: #f92672;\">-</span>ng(<span style=\"color: #ae81ff;\">8</span>)   Main PID: <span style=\"color: #ae81ff;\">31034</span> (syslog<span style=\"color: #f92672;\">-</span>ng)      Tasks: <span style=\"color: #ae81ff;\">2</span> (limit: <span style=\"color: #ae81ff;\">779</span>)        CPU: <span style=\"color: #ae81ff;\">965</span>ms     CGroup: <span style=\"color: #f92672;\">/</span>system<span style=\"color: #f92672;\">.</span>slice<span style=\"color: #f92672;\">/</span>syslog<span style=\"color: #f92672;\">-</span>ng<span style=\"color: #f92672;\">.</span>service             <span style=\"color: #960050; background-color: #1e0010;\">└─</span><span style=\"color: #ae81ff;\">31034</span> <span style=\"color: #f92672;\">/</span>usr<span style=\"color: #f92672;\">/</span>sbin<span style=\"color: #f92672;\">/</span>syslog<span style=\"color: #f92672;\">-</span>ng <span style=\"color: #f92672;\">-</span>F</code></pre></div></details><br /><!-- raw HTML omitted --><strong>Does it work?</strong></p><p>The answer to this question is an emphatic YES!</p><p>Let’s begin with a simple test running the <em>logger</em> command on all of the compute nodes, while monitoring <em>/var/log/fromnet</em> on host <em>turingpi</em>.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\"> root@turingpi:/home/lsfadmin# date; parallel-ssh -h /opt/workers -i 'HOST=`hostname`; logger This is a test from node $HOST. Do not panic!' Wed  3 Apr 21:41:45 EDT 2024 [1] 21:41:46 [SUCCESS] teller [2] 21:41:46 [SUCCESS] neumann [3] 21:41:46 [SUCCESS] wigner [4] 21:41:46 [SUCCESS] kemeny [5] 21:41:46 [SUCCESS] szilard [6] 21:41:46 [SUCCESS] vonkarmanroot@turingpi:/var/log# tail -f fromnet |grep panic Apr  3 21:41:46 szilard root[10918]: This is a test from node szilard. Do not panic! Apr  3 21:41:46 wigner root[11011]: This is a test from node wigner. Do not panic! Apr  3 21:41:46 neumann root[11121]: This is a test from node neumann. Do not panic! Apr  3 21:41:46 kemeny root[11029]: This is a test from node kemeny. Do not panic! Apr  3 21:41:46 teller root[10875]: This is a test from node teller. Do not panic! Apr  3 21:41:46 vonkarman root[10805]: This is a test from node vonkarman. Do not panic!</code></pre></div><p>Next, let’s look at whether the LSF logging is also captured. Here we simply restart the LSF daemons on Nodes 2-7 and monitor the <em>/var/log/fromnet</em> file. The full output can be viewed below.</p><p><details>  <strong>Output of <em>tail -f /var/log/fromnet</em>. Click to expand</strong>  <div class=\"highlight\"><pre><code class=\"language-python\">root<span style=\"color: #a6e22e;\">@turingpi</span>:<span style=\"color: #f92672;\">/</span>var<span style=\"color: #f92672;\">/</span>log<span style=\"color: #75715e;\"># tail -f fromnet </span>Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">41</span>:<span style=\"color: #ae81ff;\">57</span> vonkarman systemd[<span style=\"color: #ae81ff;\">10786</span>]: systemd<span style=\"color: #f92672;\">-</span>exit<span style=\"color: #f92672;\">.</span>service: Succeeded<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">41</span>:<span style=\"color: #ae81ff;\">57</span> vonkarman systemd[<span style=\"color: #ae81ff;\">10786</span>]: Finished Exit the Session<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">41</span>:<span style=\"color: #ae81ff;\">57</span> vonkarman systemd[<span style=\"color: #ae81ff;\">10786</span>]: Reached target Exit the Session<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">41</span>:<span style=\"color: #ae81ff;\">57</span> vonkarman systemd[<span style=\"color: #ae81ff;\">1</span>]: user<span style=\"color: #f92672;\">@</span><span style=\"color: #ae81ff;\">0.</span>service: Succeeded<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">41</span>:<span style=\"color: #ae81ff;\">57</span> vonkarman systemd[<span style=\"color: #ae81ff;\">1</span>]: Stopped User Manager <span style=\"color: #66d9ef;\">for</span> UID <span style=\"color: #ae81ff;\">0.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">41</span>:<span style=\"color: #ae81ff;\">57</span> vonkarman systemd[<span style=\"color: #ae81ff;\">1</span>]: Stopping User Runtime Directory <span style=\"color: #f92672;\">/</span>run<span style=\"color: #f92672;\">/</span>user<span style=\"color: #f92672;\">/</span><span style=\"color: #ae81ff;\">0.</span><span style=\"color: #f92672;\">..</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">41</span>:<span style=\"color: #ae81ff;\">57</span> vonkarman systemd[<span style=\"color: #ae81ff;\">1</span>]: run<span style=\"color: #f92672;\">-</span>user<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">0.</span>mount: Succeeded<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">41</span>:<span style=\"color: #ae81ff;\">57</span> vonkarman systemd[<span style=\"color: #ae81ff;\">1</span>]: user<span style=\"color: #f92672;\">-</span>runtime<span style=\"color: #f92672;\">-</span>dir<span style=\"color: #f92672;\">@</span><span style=\"color: #ae81ff;\">0.</span>service: Succeeded<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">41</span>:<span style=\"color: #ae81ff;\">57</span> vonkarman systemd[<span style=\"color: #ae81ff;\">1</span>]: Stopped User Runtime Directory <span style=\"color: #f92672;\">/</span>run<span style=\"color: #f92672;\">/</span>user<span style=\"color: #f92672;\">/</span><span style=\"color: #ae81ff;\">0.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">41</span>:<span style=\"color: #ae81ff;\">57</span> vonkarman systemd[<span style=\"color: #ae81ff;\">1</span>]: Removed slice User Slice of UID <span style=\"color: #ae81ff;\">0.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">30</span> wigner dhcpcd[<span style=\"color: #ae81ff;\">493</span>]: eth0: Router Advertisement <span style=\"color: #f92672;\">from</span> fe80::da58:d7ff:fe00:<span style=\"color: #ae81ff;\">6</span>d83 Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">57</span> szilard sshd[<span style=\"color: #ae81ff;\">11234</span>]: Accepted publickey <span style=\"color: #66d9ef;\">for</span> root <span style=\"color: #f92672;\">from</span> <span style=\"color: #ae81ff;\">192.168.1.172</span> port <span style=\"color: #ae81ff;\">52600</span> ssh2: ED25519 SHA256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">57</span> szilard sshd[<span style=\"color: #ae81ff;\">11234</span>]: pam_unix(sshd:session): session opened <span style=\"color: #66d9ef;\">for</span> user root(uid<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>) by (uid<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>) Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> szilard systemd[<span style=\"color: #ae81ff;\">1</span>]: Created slice User Slice of UID <span style=\"color: #ae81ff;\">0.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> szilard systemd[<span style=\"color: #ae81ff;\">1</span>]: Starting User Runtime Directory <span style=\"color: #f92672;\">/</span>run<span style=\"color: #f92672;\">/</span>user<span style=\"color: #f92672;\">/</span><span style=\"color: #ae81ff;\">0.</span><span style=\"color: #f92672;\">..</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> szilard systemd<span style=\"color: #f92672;\">-</span>logind[<span style=\"color: #ae81ff;\">382</span>]: New session <span style=\"color: #ae81ff;\">30</span> of user root<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> szilard systemd[<span style=\"color: #ae81ff;\">1</span>]: Finished User Runtime Directory <span style=\"color: #f92672;\">/</span>run<span style=\"color: #f92672;\">/</span>user<span style=\"color: #f92672;\">/</span><span style=\"color: #ae81ff;\">0.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> szilard systemd[<span style=\"color: #ae81ff;\">1</span>]: Starting User Manager <span style=\"color: #66d9ef;\">for</span> UID <span style=\"color: #ae81ff;\">0.</span><span style=\"color: #f92672;\">..</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> szilard systemd[<span style=\"color: #ae81ff;\">11237</span>]: pam_unix(systemd<span style=\"color: #f92672;\">-</span>user:session): session opened <span style=\"color: #66d9ef;\">for</span> user root(uid<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>) by(uid<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>) Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">57</span> wigner sshd[<span style=\"color: #ae81ff;\">11342</span>]: Accepted publickey <span style=\"color: #66d9ef;\">for</span> root <span style=\"color: #f92672;\">from</span> <span style=\"color: #ae81ff;\">192.168.1.172</span> port <span style=\"color: #ae81ff;\">60388</span> ssh2: ED25519 SHA256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">57</span> wigner sshd[<span style=\"color: #ae81ff;\">11342</span>]: pam_unix(sshd:session): session opened <span style=\"color: #66d9ef;\">for</span> user root(uid<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>) by (uid<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>) Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> wigner systemd[<span style=\"color: #ae81ff;\">1</span>]: Created slice User Slice of UID <span style=\"color: #ae81ff;\">0.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> wigner systemd[<span style=\"color: #ae81ff;\">1</span>]: Starting User Runtime Directory <span style=\"color: #f92672;\">/</span>run<span style=\"color: #f92672;\">/</span>user<span style=\"color: #f92672;\">/</span><span style=\"color: #ae81ff;\">0.</span><span style=\"color: #f92672;\">..</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> wigner systemd<span style=\"color: #f92672;\">-</span>logind[<span style=\"color: #ae81ff;\">383</span>]: New session <span style=\"color: #ae81ff;\">30</span> of user root<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> wigner systemd[<span style=\"color: #ae81ff;\">1</span>]: Finished User Runtime Directory <span style=\"color: #f92672;\">/</span>run<span style=\"color: #f92672;\">/</span>user<span style=\"color: #f92672;\">/</span><span style=\"color: #ae81ff;\">0.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> wigner systemd[<span style=\"color: #ae81ff;\">1</span>]: Starting User Manager <span style=\"color: #66d9ef;\">for</span> UID <span style=\"color: #ae81ff;\">0.</span><span style=\"color: #f92672;\">..</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> wigner systemd[<span style=\"color: #ae81ff;\">11345</span>]: pam_unix(systemd<span style=\"color: #f92672;\">-</span>user:session): session opened <span style=\"color: #66d9ef;\">for</span> user root(uid<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>) by (uid<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>) Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">57</span> neumann sshd[<span style=\"color: #ae81ff;\">11436</span>]: Accepted publickey <span style=\"color: #66d9ef;\">for</span> root <span style=\"color: #f92672;\">from</span> <span style=\"color: #ae81ff;\">192.168.1.172</span> port <span style=\"color: #ae81ff;\">55144</span> ssh2: ED25519 SHA256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">57</span> neumann sshd[<span style=\"color: #ae81ff;\">11436</span>]: pam_unix(sshd:session): session opened <span style=\"color: #66d9ef;\">for</span> user root(uid<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>) by (uid<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>) Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">57</span> neumann systemd[<span style=\"color: #ae81ff;\">1</span>]: Created slice User Slice of UID <span style=\"color: #ae81ff;\">0.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">57</span> neumann systemd[<span style=\"color: #ae81ff;\">1</span>]: Starting User Runtime Directory <span style=\"color: #f92672;\">/</span>run<span style=\"color: #f92672;\">/</span>user<span style=\"color: #f92672;\">/</span><span style=\"color: #ae81ff;\">0.</span><span style=\"color: #f92672;\">..</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> neumann systemd<span style=\"color: #f92672;\">-</span>logind[<span style=\"color: #ae81ff;\">398</span>]: New session <span style=\"color: #ae81ff;\">30</span> of user root<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> neumann systemd[<span style=\"color: #ae81ff;\">1</span>]: Finished User Runtime Directory <span style=\"color: #f92672;\">/</span>run<span style=\"color: #f92672;\">/</span>user<span style=\"color: #f92672;\">/</span><span style=\"color: #ae81ff;\">0.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> neumann systemd[<span style=\"color: #ae81ff;\">1</span>]: Starting User Manager <span style=\"color: #66d9ef;\">for</span> UID <span style=\"color: #ae81ff;\">0.</span><span style=\"color: #f92672;\">..</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> neumann systemd[<span style=\"color: #ae81ff;\">11439</span>]: pam_unix(systemd<span style=\"color: #f92672;\">-</span>user:session): session opened <span style=\"color: #66d9ef;\">for</span> user root(uid<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>) by(uid<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>) Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">57</span> kemeny sshd[<span style=\"color: #ae81ff;\">11345</span>]: Accepted publickey <span style=\"color: #66d9ef;\">for</span> root <span style=\"color: #f92672;\">from</span> <span style=\"color: #ae81ff;\">192.168.1.172</span> port <span style=\"color: #ae81ff;\">59830</span> ssh2: ED25519 SHA256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">57</span> kemeny sshd[<span style=\"color: #ae81ff;\">11345</span>]: pam_unix(sshd:session): session opened <span style=\"color: #66d9ef;\">for</span> user root(uid<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>) by (uid<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>) Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> kemeny systemd[<span style=\"color: #ae81ff;\">1</span>]: Created slice User Slice of UID <span style=\"color: #ae81ff;\">0.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> kemeny systemd[<span style=\"color: #ae81ff;\">1</span>]: Starting User Runtime Directory <span style=\"color: #f92672;\">/</span>run<span style=\"color: #f92672;\">/</span>user<span style=\"color: #f92672;\">/</span><span style=\"color: #ae81ff;\">0.</span><span style=\"color: #f92672;\">..</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> kemeny systemd<span style=\"color: #f92672;\">-</span>logind[<span style=\"color: #ae81ff;\">386</span>]: New session <span style=\"color: #ae81ff;\">30</span> of user root<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> kemeny systemd[<span style=\"color: #ae81ff;\">1</span>]: Finished User Runtime Directory <span style=\"color: #f92672;\">/</span>run<span style=\"color: #f92672;\">/</span>user<span style=\"color: #f92672;\">/</span><span style=\"color: #ae81ff;\">0.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> kemeny systemd[<span style=\"color: #ae81ff;\">1</span>]: Starting User Manager <span style=\"color: #66d9ef;\">for</span> UID <span style=\"color: #ae81ff;\">0.</span><span style=\"color: #f92672;\">..</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> kemeny systemd[<span style=\"color: #ae81ff;\">11348</span>]: pam_unix(systemd<span style=\"color: #f92672;\">-</span>user:session): session opened <span style=\"color: #66d9ef;\">for</span> user root(uid<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>) by (uid<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>) Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">57</span> teller sshd[<span style=\"color: #ae81ff;\">11189</span>]: Accepted publickey <span style=\"color: #66d9ef;\">for</span> root <span style=\"color: #f92672;\">from</span> <span style=\"color: #ae81ff;\">192.168.1.172</span> port <span style=\"color: #ae81ff;\">35310</span> ssh2: ED25519 SHA256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">57</span> teller sshd[<span style=\"color: #ae81ff;\">11189</span>]: pam_unix(sshd:session): session opened <span style=\"color: #66d9ef;\">for</span> user root(uid<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>) by (uid<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>) Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> teller systemd[<span style=\"color: #ae81ff;\">1</span>]: Created slice User Slice of UID <span style=\"color: #ae81ff;\">0.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> teller systemd[<span style=\"color: #ae81ff;\">1</span>]: Starting User Runtime Directory <span style=\"color: #f92672;\">/</span>run<span style=\"color: #f92672;\">/</span>user<span style=\"color: #f92672;\">/</span><span style=\"color: #ae81ff;\">0.</span><span style=\"color: #f92672;\">..</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> teller systemd<span style=\"color: #f92672;\">-</span>logind[<span style=\"color: #ae81ff;\">382</span>]: New session <span style=\"color: #ae81ff;\">30</span> of user root<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> teller systemd[<span style=\"color: #ae81ff;\">1</span>]: Finished User Runtime Directory <span style=\"color: #f92672;\">/</span>run<span style=\"color: #f92672;\">/</span>user<span style=\"color: #f92672;\">/</span><span style=\"color: #ae81ff;\">0.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> teller systemd[<span style=\"color: #ae81ff;\">1</span>]: Starting User Manager <span style=\"color: #66d9ef;\">for</span> UID <span style=\"color: #ae81ff;\">0.</span><span style=\"color: #f92672;\">..</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> teller systemd[<span style=\"color: #ae81ff;\">11192</span>]: pam_unix(systemd<span style=\"color: #f92672;\">-</span>user:session): session opened <span style=\"color: #66d9ef;\">for</span> user root(uid<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>) by (uid<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>) Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">57</span> vonkarman sshd[<span style=\"color: #ae81ff;\">11118</span>]: Accepted publickey <span style=\"color: #66d9ef;\">for</span> root <span style=\"color: #f92672;\">from</span> <span style=\"color: #ae81ff;\">192.168.1.172</span> port <span style=\"color: #ae81ff;\">48654</span> ssh2: ED25519SHA256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> vonkarman sshd[<span style=\"color: #ae81ff;\">11118</span>]: pam_unix(sshd:session): session opened <span style=\"color: #66d9ef;\">for</span> user root(uid<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>) by (uid<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>) Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> vonkarman systemd[<span style=\"color: #ae81ff;\">1</span>]: Created slice User Slice of UID <span style=\"color: #ae81ff;\">0.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> vonkarman systemd[<span style=\"color: #ae81ff;\">1</span>]: Starting User Runtime Directory <span style=\"color: #f92672;\">/</span>run<span style=\"color: #f92672;\">/</span>user<span style=\"color: #f92672;\">/</span><span style=\"color: #ae81ff;\">0.</span><span style=\"color: #f92672;\">..</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> vonkarman systemd<span style=\"color: #f92672;\">-</span>logind[<span style=\"color: #ae81ff;\">382</span>]: New session <span style=\"color: #ae81ff;\">29</span> of user root<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> vonkarman systemd[<span style=\"color: #ae81ff;\">1</span>]: Finished User Runtime Directory <span style=\"color: #f92672;\">/</span>run<span style=\"color: #f92672;\">/</span>user<span style=\"color: #f92672;\">/</span><span style=\"color: #ae81ff;\">0.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> vonkarman systemd[<span style=\"color: #ae81ff;\">1</span>]: Starting User Manager <span style=\"color: #66d9ef;\">for</span> UID <span style=\"color: #ae81ff;\">0.</span><span style=\"color: #f92672;\">..</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> vonkarman systemd[<span style=\"color: #ae81ff;\">11121</span>]: pam_unix(systemd<span style=\"color: #f92672;\">-</span>user:session): session opened <span style=\"color: #66d9ef;\">for</span> user root(uid<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>) by (uid<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>) Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> neumann systemd[<span style=\"color: #ae81ff;\">11439</span>]: Queued start job <span style=\"color: #66d9ef;\">for</span> default target Main User Target<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> neumann systemd[<span style=\"color: #ae81ff;\">11439</span>]: Created slice User Application Slice<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> neumann systemd[<span style=\"color: #ae81ff;\">11439</span>]: Reached target Paths<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> neumann systemd[<span style=\"color: #ae81ff;\">11439</span>]: Reached target Timers<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> neumann systemd[<span style=\"color: #ae81ff;\">11439</span>]: Listening on GnuPG network certificate management daemon<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> neumann systemd[<span style=\"color: #ae81ff;\">11439</span>]: Listening on GnuPG cryptographic agent <span style=\"color: #f92672;\">and</span> passphrase cache (access for web browsers)<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> neumann systemd[<span style=\"color: #ae81ff;\">11439</span>]: Listening on GnuPG cryptographic agent <span style=\"color: #f92672;\">and</span> passphrase cache (restricted)<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> neumann systemd[<span style=\"color: #ae81ff;\">11439</span>]: Listening on GnuPG cryptographic agent (ssh<span style=\"color: #f92672;\">-</span>agent emulation)<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> neumann systemd[<span style=\"color: #ae81ff;\">11439</span>]: Listening on GnuPG cryptographic agent <span style=\"color: #f92672;\">and</span> passphrase cache<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> neumann systemd[<span style=\"color: #ae81ff;\">11439</span>]: Reached target Sockets<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> neumann systemd[<span style=\"color: #ae81ff;\">11439</span>]: Reached target Basic System<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> neumann systemd[<span style=\"color: #ae81ff;\">11439</span>]: Reached target Main User Target<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> neumann systemd[<span style=\"color: #ae81ff;\">11439</span>]: Startup finished <span style=\"color: #f92672;\">in</span> <span style=\"color: #ae81ff;\">379</span>ms<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> neumann systemd[<span style=\"color: #ae81ff;\">1</span>]: Started User Manager <span style=\"color: #66d9ef;\">for</span> UID <span style=\"color: #ae81ff;\">0.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> neumann systemd[<span style=\"color: #ae81ff;\">1</span>]: Started Session <span style=\"color: #ae81ff;\">30</span> of user root<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> teller systemd[<span style=\"color: #ae81ff;\">11192</span>]: Queued start job <span style=\"color: #66d9ef;\">for</span> default target Main User Target<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> teller systemd[<span style=\"color: #ae81ff;\">11192</span>]: Created slice User Application Slice<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> teller systemd[<span style=\"color: #ae81ff;\">11192</span>]: Reached target Paths<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> teller systemd[<span style=\"color: #ae81ff;\">11192</span>]: Reached target Timers<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> teller systemd[<span style=\"color: #ae81ff;\">11192</span>]: Listening on GnuPG network certificate management daemon<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> teller systemd[<span style=\"color: #ae81ff;\">11192</span>]: Listening on GnuPG cryptographic agent <span style=\"color: #f92672;\">and</span> passphrase cache (access <span style=\"color: #66d9ef;\">for</span>web browsers)<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> teller systemd[<span style=\"color: #ae81ff;\">11192</span>]: Listening on GnuPG cryptographic agent <span style=\"color: #f92672;\">and</span> passphrase cache (restricted)<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> teller systemd[<span style=\"color: #ae81ff;\">11192</span>]: Listening on GnuPG cryptographic agent (ssh<span style=\"color: #f92672;\">-</span>agent emulation)<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> teller systemd[<span style=\"color: #ae81ff;\">11192</span>]: Listening on GnuPG cryptographic agent <span style=\"color: #f92672;\">and</span> passphrase cache<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> teller systemd[<span style=\"color: #ae81ff;\">11192</span>]: Reached target Sockets<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> teller systemd[<span style=\"color: #ae81ff;\">11192</span>]: Reached target Basic System<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> teller systemd[<span style=\"color: #ae81ff;\">11192</span>]: Reached target Main User Target<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> teller systemd[<span style=\"color: #ae81ff;\">11192</span>]: Startup finished <span style=\"color: #f92672;\">in</span> <span style=\"color: #ae81ff;\">373</span>ms<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> teller systemd[<span style=\"color: #ae81ff;\">1</span>]: Started User Manager <span style=\"color: #66d9ef;\">for</span> UID <span style=\"color: #ae81ff;\">0.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> teller systemd[<span style=\"color: #ae81ff;\">1</span>]: Started Session <span style=\"color: #ae81ff;\">30</span> of user root<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> vonkarman systemd[<span style=\"color: #ae81ff;\">11121</span>]: Queued start job <span style=\"color: #66d9ef;\">for</span> default target Main User Target<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> vonkarman systemd[<span style=\"color: #ae81ff;\">11121</span>]: Created slice User Application Slice<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> vonkarman systemd[<span style=\"color: #ae81ff;\">11121</span>]: Reached target Paths<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> vonkarman systemd[<span style=\"color: #ae81ff;\">11121</span>]: Reached target Timers<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> vonkarman systemd[<span style=\"color: #ae81ff;\">11121</span>]: Listening on GnuPG network certificate management daemon<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> vonkarman systemd[<span style=\"color: #ae81ff;\">11121</span>]: Listening on GnuPG cryptographic agent <span style=\"color: #f92672;\">and</span> passphrase cache (access <span style=\"color: #66d9ef;\">for</span> web browsers)<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> vonkarman systemd[<span style=\"color: #ae81ff;\">11121</span>]: Listening on GnuPG cryptographic agent <span style=\"color: #f92672;\">and</span> passphrase cache (restricted)<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> vonkarman systemd[<span style=\"color: #ae81ff;\">11121</span>]: Listening on GnuPG cryptographic agent (ssh<span style=\"color: #f92672;\">-</span>agent emulation)<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> vonkarman systemd[<span style=\"color: #ae81ff;\">11121</span>]: Listening on GnuPG cryptographic agent <span style=\"color: #f92672;\">and</span> passphrase cache<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> vonkarman systemd[<span style=\"color: #ae81ff;\">11121</span>]: Reached target Sockets<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> vonkarman systemd[<span style=\"color: #ae81ff;\">11121</span>]: Reached target Basic System<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> vonkarman systemd[<span style=\"color: #ae81ff;\">11121</span>]: Reached target Main User Target<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> vonkarman systemd[<span style=\"color: #ae81ff;\">11121</span>]: Startup finished <span style=\"color: #f92672;\">in</span> <span style=\"color: #ae81ff;\">392</span>ms<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> vonkarman systemd[<span style=\"color: #ae81ff;\">1</span>]: Started User Manager <span style=\"color: #66d9ef;\">for</span> UID <span style=\"color: #ae81ff;\">0.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> vonkarman systemd[<span style=\"color: #ae81ff;\">1</span>]: Started Session <span style=\"color: #ae81ff;\">29</span> of user root<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> szilard systemd[<span style=\"color: #ae81ff;\">11237</span>]: Queued start job <span style=\"color: #66d9ef;\">for</span> default target Main User Target<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> szilard systemd[<span style=\"color: #ae81ff;\">11237</span>]: Created slice User Application Slice<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> szilard systemd[<span style=\"color: #ae81ff;\">11237</span>]: Reached target Paths<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> szilard systemd[<span style=\"color: #ae81ff;\">11237</span>]: Reached target Timers<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> szilard systemd[<span style=\"color: #ae81ff;\">11237</span>]: Listening on GnuPG network certificate management daemon<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> szilard systemd[<span style=\"color: #ae81ff;\">11237</span>]: Listening on GnuPG cryptographic agent <span style=\"color: #f92672;\">and</span> passphrase cache (access for web browsers)<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> szilard systemd[<span style=\"color: #ae81ff;\">11237</span>]: Listening on GnuPG cryptographic agent <span style=\"color: #f92672;\">and</span> passphrase cache (restricted)<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> szilard systemd[<span style=\"color: #ae81ff;\">11237</span>]: Listening on GnuPG cryptographic agent (ssh<span style=\"color: #f92672;\">-</span>agent emulation)<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> szilard systemd[<span style=\"color: #ae81ff;\">11237</span>]: Listening on GnuPG cryptographic agent <span style=\"color: #f92672;\">and</span> passphrase cache<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> szilard systemd[<span style=\"color: #ae81ff;\">11237</span>]: Reached target Sockets<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> szilard systemd[<span style=\"color: #ae81ff;\">11237</span>]: Reached target Basic System<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> szilard systemd[<span style=\"color: #ae81ff;\">11237</span>]: Reached target Main User Target<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> szilard systemd[<span style=\"color: #ae81ff;\">11237</span>]: Startup finished <span style=\"color: #f92672;\">in</span> <span style=\"color: #ae81ff;\">385</span>ms<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> szilard systemd[<span style=\"color: #ae81ff;\">1</span>]: Started User Manager <span style=\"color: #66d9ef;\">for</span> UID <span style=\"color: #ae81ff;\">0.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> szilard systemd[<span style=\"color: #ae81ff;\">1</span>]: Started Session <span style=\"color: #ae81ff;\">30</span> of user root<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> wigner systemd[<span style=\"color: #ae81ff;\">11345</span>]: Queued start job <span style=\"color: #66d9ef;\">for</span> default target Main User Target<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> wigner systemd[<span style=\"color: #ae81ff;\">11345</span>]: Created slice User Application Slice<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> wigner systemd[<span style=\"color: #ae81ff;\">11345</span>]: Reached target Paths<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> wigner systemd[<span style=\"color: #ae81ff;\">11345</span>]: Reached target Timers<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> wigner systemd[<span style=\"color: #ae81ff;\">11345</span>]: Listening on GnuPG network certificate management daemon<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> wigner systemd[<span style=\"color: #ae81ff;\">11345</span>]: Listening on GnuPG cryptographic agent <span style=\"color: #f92672;\">and</span> passphrase cache (access <span style=\"color: #66d9ef;\">for</span>web browsers)<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> wigner systemd[<span style=\"color: #ae81ff;\">11345</span>]: Listening on GnuPG cryptographic agent <span style=\"color: #f92672;\">and</span> passphrase cache (restricted)<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> wigner systemd[<span style=\"color: #ae81ff;\">11345</span>]: Listening on GnuPG cryptographic agent (ssh<span style=\"color: #f92672;\">-</span>agent emulation)<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> wigner systemd[<span style=\"color: #ae81ff;\">11345</span>]: Listening on GnuPG cryptographic agent <span style=\"color: #f92672;\">and</span> passphrase cache<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> wigner systemd[<span style=\"color: #ae81ff;\">11345</span>]: Reached target Sockets<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> wigner systemd[<span style=\"color: #ae81ff;\">11345</span>]: Reached target Basic System<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> wigner systemd[<span style=\"color: #ae81ff;\">11345</span>]: Reached target Main User Target<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> wigner systemd[<span style=\"color: #ae81ff;\">11345</span>]: Startup finished <span style=\"color: #f92672;\">in</span> <span style=\"color: #ae81ff;\">375</span>ms<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> wigner systemd[<span style=\"color: #ae81ff;\">1</span>]: Started User Manager <span style=\"color: #66d9ef;\">for</span> UID <span style=\"color: #ae81ff;\">0.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> wigner systemd[<span style=\"color: #ae81ff;\">1</span>]: Started Session <span style=\"color: #ae81ff;\">30</span> of user root<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> kemeny systemd[<span style=\"color: #ae81ff;\">11348</span>]: Queued start job <span style=\"color: #66d9ef;\">for</span> default target Main User Target<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> kemeny systemd[<span style=\"color: #ae81ff;\">11348</span>]: Created slice User Application Slice<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> kemeny systemd[<span style=\"color: #ae81ff;\">11348</span>]: Reached target Paths<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> kemeny systemd[<span style=\"color: #ae81ff;\">11348</span>]: Reached target Timers<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> kemeny systemd[<span style=\"color: #ae81ff;\">11348</span>]: Listening on GnuPG network certificate management daemon<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> kemeny systemd[<span style=\"color: #ae81ff;\">11348</span>]: Listening on GnuPG cryptographic agent <span style=\"color: #f92672;\">and</span> passphrase cache (access <span style=\"color: #66d9ef;\">for</span>web browsers)<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> kemeny systemd[<span style=\"color: #ae81ff;\">11348</span>]: Listening on GnuPG cryptographic agent <span style=\"color: #f92672;\">and</span> passphrase cache (restricted)<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> kemeny systemd[<span style=\"color: #ae81ff;\">11348</span>]: Listening on GnuPG cryptographic agent (ssh<span style=\"color: #f92672;\">-</span>agent emulation)<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> kemeny systemd[<span style=\"color: #ae81ff;\">11348</span>]: Listening on GnuPG cryptographic agent <span style=\"color: #f92672;\">and</span> passphrase cache<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> kemeny systemd[<span style=\"color: #ae81ff;\">11348</span>]: Reached target Sockets<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> kemeny systemd[<span style=\"color: #ae81ff;\">11348</span>]: Reached target Basic System<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> kemeny systemd[<span style=\"color: #ae81ff;\">11348</span>]: Reached target Main User Target<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> kemeny systemd[<span style=\"color: #ae81ff;\">11348</span>]: Startup finished <span style=\"color: #f92672;\">in</span> <span style=\"color: #ae81ff;\">400</span>ms<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> kemeny systemd[<span style=\"color: #ae81ff;\">1</span>]: Started User Manager <span style=\"color: #66d9ef;\">for</span> UID <span style=\"color: #ae81ff;\">0.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">58</span> kemeny systemd[<span style=\"color: #ae81ff;\">1</span>]: Started Session <span style=\"color: #ae81ff;\">30</span> of user root<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> kemeny res[<span style=\"color: #ae81ff;\">691</span>]: term_handler: Received signal <span style=\"color: #ae81ff;\">15</span>, exiting Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> kemeny lim[<span style=\"color: #ae81ff;\">688</span>]: term_handler: Received signal <span style=\"color: #ae81ff;\">15</span>, exiting Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> kemeny sbatchd[<span style=\"color: #ae81ff;\">693</span>]: Daemon on host <span style=\"color: #f92672;\">&lt;</span>kemeny<span style=\"color: #f92672;\">&gt;</span> received signal <span style=\"color: #f92672;\">&lt;</span><span style=\"color: #ae81ff;\">15</span><span style=\"color: #f92672;\">&gt;</span>; exiting Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> kemeny lsf_daemons[<span style=\"color: #ae81ff;\">11434</span>]: Stopping the LSF subsystem Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> kemeny systemd[<span style=\"color: #ae81ff;\">1</span>]: lsfd<span style=\"color: #f92672;\">.</span>service: Succeeded<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> kemeny systemd[<span style=\"color: #ae81ff;\">1</span>]: lsfd<span style=\"color: #f92672;\">.</span>service: Consumed <span style=\"color: #ae81ff;\">11</span>min <span style=\"color: #ae81ff;\">56.744</span>s CPU time<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> szilard lim[<span style=\"color: #ae81ff;\">685</span>]: term_handler: Received signal <span style=\"color: #ae81ff;\">15</span>, exiting Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> szilard res[<span style=\"color: #ae81ff;\">687</span>]: term_handler: Received signal <span style=\"color: #ae81ff;\">15</span>, exiting Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> szilard sbatchd[<span style=\"color: #ae81ff;\">689</span>]: Daemon on host <span style=\"color: #f92672;\">&lt;</span>szilard<span style=\"color: #f92672;\">&gt;</span> received signal <span style=\"color: #f92672;\">&lt;</span><span style=\"color: #ae81ff;\">15</span><span style=\"color: #f92672;\">&gt;</span>; exiting Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> vonkarman lim[<span style=\"color: #ae81ff;\">686</span>]: term_handler: Received signal <span style=\"color: #ae81ff;\">15</span>, exiting Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> vonkarman sbatchd[<span style=\"color: #ae81ff;\">690</span>]: Daemon on host <span style=\"color: #f92672;\">&lt;</span>vonkarman<span style=\"color: #f92672;\">&gt;</span> received signal <span style=\"color: #f92672;\">&lt;</span><span style=\"color: #ae81ff;\">15</span><span style=\"color: #f92672;\">&gt;</span>; exiting Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> vonkarman res[<span style=\"color: #ae81ff;\">688</span>]: term_handler: Received signal <span style=\"color: #ae81ff;\">15</span>, exiting Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> teller lim[<span style=\"color: #ae81ff;\">683</span>]: term_handler: Received signal <span style=\"color: #ae81ff;\">15</span>, exiting Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> teller res[<span style=\"color: #ae81ff;\">689</span>]: term_handler: Received signal <span style=\"color: #ae81ff;\">15</span>, exiting Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> teller sbatchd[<span style=\"color: #ae81ff;\">691</span>]: Daemon on host <span style=\"color: #f92672;\">&lt;</span>teller<span style=\"color: #f92672;\">&gt;</span> received signal <span style=\"color: #f92672;\">&lt;</span><span style=\"color: #ae81ff;\">15</span><span style=\"color: #f92672;\">&gt;</span>; exiting Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> teller lsf_daemons[<span style=\"color: #ae81ff;\">11294</span>]: Stopping the LSF subsystem Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> wigner lim[<span style=\"color: #ae81ff;\">719</span>]: term_handler: Received signal <span style=\"color: #ae81ff;\">15</span>, exiting Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> wigner res[<span style=\"color: #ae81ff;\">722</span>]: term_handler: Received signal <span style=\"color: #ae81ff;\">15</span>, exiting Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> wigner sbatchd[<span style=\"color: #ae81ff;\">724</span>]: Daemon on host <span style=\"color: #f92672;\">&lt;</span>wigner<span style=\"color: #f92672;\">&gt;</span> received signal <span style=\"color: #f92672;\">&lt;</span><span style=\"color: #ae81ff;\">15</span><span style=\"color: #f92672;\">&gt;</span>; exiting Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> wigner lsf_daemons[<span style=\"color: #ae81ff;\">11438</span>]: Stopping the LSF subsystem Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> neumann res[<span style=\"color: #ae81ff;\">713</span>]: term_handler: Received signal <span style=\"color: #ae81ff;\">15</span>, exiting Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> neumann sbatchd[<span style=\"color: #ae81ff;\">715</span>]: Daemon on host <span style=\"color: #f92672;\">&lt;</span>neumann<span style=\"color: #f92672;\">&gt;</span> received signal <span style=\"color: #f92672;\">&lt;</span><span style=\"color: #ae81ff;\">15</span><span style=\"color: #f92672;\">&gt;</span>; exiting Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> neumann lim[<span style=\"color: #ae81ff;\">711</span>]: term_handler: Received signal <span style=\"color: #ae81ff;\">15</span>, exiting Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> neumann lsf_daemons[<span style=\"color: #ae81ff;\">11540</span>]: Stopping the LSF subsystem Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> neumann sshd[<span style=\"color: #ae81ff;\">11436</span>]: Received disconnect <span style=\"color: #f92672;\">from</span> <span style=\"color: #ae81ff;\">192.168.1.172</span> port <span style=\"color: #ae81ff;\">55144</span>:<span style=\"color: #ae81ff;\">11</span>: disconnected by user Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> neumann sshd[<span style=\"color: #ae81ff;\">11436</span>]: Disconnected <span style=\"color: #f92672;\">from</span> user root <span style=\"color: #ae81ff;\">192.168.1.172</span> port <span style=\"color: #ae81ff;\">55144</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> szilard lsf_daemons[<span style=\"color: #ae81ff;\">11331</span>]: Stopping the LSF subsystem Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> szilard sshd[<span style=\"color: #ae81ff;\">11234</span>]: Received disconnect <span style=\"color: #f92672;\">from</span> <span style=\"color: #ae81ff;\">192.168.1.172</span> port <span style=\"color: #ae81ff;\">52600</span>:<span style=\"color: #ae81ff;\">11</span>: disconnected by user Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> szilard sshd[<span style=\"color: #ae81ff;\">11234</span>]: Disconnected <span style=\"color: #f92672;\">from</span> user root <span style=\"color: #ae81ff;\">192.168.1.172</span> port <span style=\"color: #ae81ff;\">52600</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> szilard sshd[<span style=\"color: #ae81ff;\">11234</span>]: pam_unix(sshd:session): session closed <span style=\"color: #66d9ef;\">for</span> user root Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> szilard res[<span style=\"color: #ae81ff;\">11357</span>]: res<span style=\"color: #f92672;\">/</span>get_hostInfo: ls_gethostinfo() failed<span style=\"color: #f92672;\">.</span> Server host LIM configuration is <span style=\"color: #f92672;\">not</span> ready yet<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> szilard systemd<span style=\"color: #f92672;\">-</span>logind[<span style=\"color: #ae81ff;\">382</span>]: Session <span style=\"color: #ae81ff;\">30</span> logged out<span style=\"color: #f92672;\">.</span> Waiting <span style=\"color: #66d9ef;\">for</span> processes to exit<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> szilard res[<span style=\"color: #ae81ff;\">11357</span>]: cg_load_hierarchies: Please use the LSF package <span style=\"color: #66d9ef;\">with</span> higher glibc version to enable LSF cgroup v2 support<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> szilard systemd[<span style=\"color: #ae81ff;\">1</span>]: lsfd<span style=\"color: #f92672;\">.</span>service: Succeeded<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> szilard systemd[<span style=\"color: #ae81ff;\">1</span>]: lsfd<span style=\"color: #f92672;\">.</span>service: Consumed <span style=\"color: #ae81ff;\">1</span>h <span style=\"color: #ae81ff;\">17</span>min <span style=\"color: #ae81ff;\">44.040</span>s CPU time<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> neumann sshd[<span style=\"color: #ae81ff;\">11436</span>]: pam_unix(sshd:session): session closed <span style=\"color: #66d9ef;\">for</span> user root Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> neumann systemd<span style=\"color: #f92672;\">-</span>logind[<span style=\"color: #ae81ff;\">398</span>]: Session <span style=\"color: #ae81ff;\">30</span> logged out<span style=\"color: #f92672;\">.</span> Waiting <span style=\"color: #66d9ef;\">for</span> processes to exit<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> neumann res[<span style=\"color: #ae81ff;\">11559</span>]: res<span style=\"color: #f92672;\">/</span>get_hostInfo: ls_gethostinfo() failed<span style=\"color: #f92672;\">.</span> Server host LIM configuration is <span style=\"color: #f92672;\">not</span> ready yet<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> neumann res[<span style=\"color: #ae81ff;\">11559</span>]: cg_load_hierarchies: Please use the LSF package <span style=\"color: #66d9ef;\">with</span> higher glibc version to enable LSF cgroup v2 support<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> neumann systemd[<span style=\"color: #ae81ff;\">1</span>]: lsfd<span style=\"color: #f92672;\">.</span>service: Succeeded<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> neumann systemd[<span style=\"color: #ae81ff;\">1</span>]: lsfd<span style=\"color: #f92672;\">.</span>service: Consumed <span style=\"color: #ae81ff;\">1</span>h <span style=\"color: #ae81ff;\">17</span>min <span style=\"color: #ae81ff;\">21.135</span>s CPU time<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> teller sshd[<span style=\"color: #ae81ff;\">11189</span>]: Received disconnect <span style=\"color: #f92672;\">from</span> <span style=\"color: #ae81ff;\">192.168.1.172</span> port <span style=\"color: #ae81ff;\">35310</span>:<span style=\"color: #ae81ff;\">11</span>: disconnected by user Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> teller sshd[<span style=\"color: #ae81ff;\">11189</span>]: Disconnected <span style=\"color: #f92672;\">from</span> user root <span style=\"color: #ae81ff;\">192.168.1.172</span> port <span style=\"color: #ae81ff;\">35310</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> teller sshd[<span style=\"color: #ae81ff;\">11189</span>]: pam_unix(sshd:session): session closed <span style=\"color: #66d9ef;\">for</span> user root Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> teller systemd<span style=\"color: #f92672;\">-</span>logind[<span style=\"color: #ae81ff;\">382</span>]: Session <span style=\"color: #ae81ff;\">30</span> logged out<span style=\"color: #f92672;\">.</span> Waiting <span style=\"color: #66d9ef;\">for</span> processes to exit<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> teller res[<span style=\"color: #ae81ff;\">11307</span>]: res<span style=\"color: #f92672;\">/</span>get_hostInfo: ls_gethostinfo() failed<span style=\"color: #f92672;\">.</span> Server host LIM configuration <span style=\"color: #f92672;\">is</span><span style=\"color: #f92672;\">not</span> ready yet<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> teller res[<span style=\"color: #ae81ff;\">11307</span>]: cg_load_hierarchies: Please use the LSF package <span style=\"color: #66d9ef;\">with</span> higher glibc version to enable LSF cgroup v2 support<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> teller res[<span style=\"color: #ae81ff;\">11307</span>]: term_handler: Received signal <span style=\"color: #ae81ff;\">15</span>, exiting Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> teller lim[<span style=\"color: #ae81ff;\">11305</span>]: term_handler: Received signal <span style=\"color: #ae81ff;\">15</span>, exiting Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> teller systemd[<span style=\"color: #ae81ff;\">1</span>]: lsfd<span style=\"color: #f92672;\">.</span>service: Succeeded<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> teller systemd[<span style=\"color: #ae81ff;\">1</span>]: lsfd<span style=\"color: #f92672;\">.</span>service: Consumed <span style=\"color: #ae81ff;\">1</span>h <span style=\"color: #ae81ff;\">17</span>min <span style=\"color: #ae81ff;\">47.675</span>s CPU time<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> teller sbatchd[<span style=\"color: #ae81ff;\">11309</span>]: cg_load_hierarchies: Please use the LSF package <span style=\"color: #66d9ef;\">with</span> higher glibc version to enable LSF cgroup v2 support<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> kemeny sshd[<span style=\"color: #ae81ff;\">11345</span>]: Received disconnect <span style=\"color: #f92672;\">from</span> <span style=\"color: #ae81ff;\">192.168.1.172</span> port <span style=\"color: #ae81ff;\">59830</span>:<span style=\"color: #ae81ff;\">11</span>: disconnected by user Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> kemeny sshd[<span style=\"color: #ae81ff;\">11345</span>]: Disconnected <span style=\"color: #f92672;\">from</span> user root <span style=\"color: #ae81ff;\">192.168.1.172</span> port <span style=\"color: #ae81ff;\">59830</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> kemeny sshd[<span style=\"color: #ae81ff;\">11345</span>]: pam_unix(sshd:session): session closed <span style=\"color: #66d9ef;\">for</span> user root Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> kemeny systemd<span style=\"color: #f92672;\">-</span>logind[<span style=\"color: #ae81ff;\">386</span>]: Session <span style=\"color: #ae81ff;\">30</span> logged out<span style=\"color: #f92672;\">.</span> Waiting <span style=\"color: #66d9ef;\">for</span> processes to exit<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> kemeny res[<span style=\"color: #ae81ff;\">11467</span>]: res<span style=\"color: #f92672;\">/</span>get_hostInfo: ls_gethostinfo() failed<span style=\"color: #f92672;\">.</span> Server host LIM configuration <span style=\"color: #f92672;\">is</span><span style=\"color: #f92672;\">not</span> ready yet<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> kemeny res[<span style=\"color: #ae81ff;\">11467</span>]: cg_load_hierarchies: Please use the LSF package <span style=\"color: #66d9ef;\">with</span> higher glibc version to enable LSF cgroup v2 support<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> vonkarman lsf_daemons[<span style=\"color: #ae81ff;\">11215</span>]: Stopping the LSF subsystem Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> vonkarman sshd[<span style=\"color: #ae81ff;\">11118</span>]: Received disconnect <span style=\"color: #f92672;\">from</span> <span style=\"color: #ae81ff;\">192.168.1.172</span> port <span style=\"color: #ae81ff;\">48654</span>:<span style=\"color: #ae81ff;\">11</span>: disconnected by user Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> vonkarman sshd[<span style=\"color: #ae81ff;\">11118</span>]: Disconnected <span style=\"color: #f92672;\">from</span> user root <span style=\"color: #ae81ff;\">192.168.1.172</span> port <span style=\"color: #ae81ff;\">48654</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> vonkarman sshd[<span style=\"color: #ae81ff;\">11118</span>]: pam_unix(sshd:session): session closed <span style=\"color: #66d9ef;\">for</span> user root Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> vonkarman systemd<span style=\"color: #f92672;\">-</span>logind[<span style=\"color: #ae81ff;\">382</span>]: Session <span style=\"color: #ae81ff;\">29</span> logged out<span style=\"color: #f92672;\">.</span> Waiting <span style=\"color: #66d9ef;\">for</span> processes to exit<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> vonkarman res[<span style=\"color: #ae81ff;\">11241</span>]: res<span style=\"color: #f92672;\">/</span>get_hostInfo: ls_gethostinfo() failed<span style=\"color: #f92672;\">.</span> Server host LIM configuration<span style=\"color: #f92672;\">is</span> <span style=\"color: #f92672;\">not</span> ready yet<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> vonkarman res[<span style=\"color: #ae81ff;\">11241</span>]: cg_load_hierarchies: Please use the LSF package <span style=\"color: #66d9ef;\">with</span> higher glibc version to enable LSF cgroup v2 support<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> vonkarman systemd[<span style=\"color: #ae81ff;\">1</span>]: lsfd<span style=\"color: #f92672;\">.</span>service: Succeeded<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> vonkarman systemd[<span style=\"color: #ae81ff;\">1</span>]: lsfd<span style=\"color: #f92672;\">.</span>service: Consumed <span style=\"color: #ae81ff;\">1</span>h <span style=\"color: #ae81ff;\">17</span>min <span style=\"color: #ae81ff;\">34.650</span>s CPU time<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> wigner sshd[<span style=\"color: #ae81ff;\">11342</span>]: Received disconnect <span style=\"color: #f92672;\">from</span> <span style=\"color: #ae81ff;\">192.168.1.172</span> port <span style=\"color: #ae81ff;\">60388</span>:<span style=\"color: #ae81ff;\">11</span>: disconnected by user Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> wigner sshd[<span style=\"color: #ae81ff;\">11342</span>]: Disconnected <span style=\"color: #f92672;\">from</span> user root <span style=\"color: #ae81ff;\">192.168.1.172</span> port <span style=\"color: #ae81ff;\">60388</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> wigner sshd[<span style=\"color: #ae81ff;\">11342</span>]: pam_unix(sshd:session): session closed <span style=\"color: #66d9ef;\">for</span> user root Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> wigner res[<span style=\"color: #ae81ff;\">11464</span>]: res<span style=\"color: #f92672;\">/</span>get_hostInfo: ls_gethostinfo() failed<span style=\"color: #f92672;\">.</span> Server host LIM configuration <span style=\"color: #f92672;\">is</span><span style=\"color: #f92672;\">not</span> ready yet<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> wigner systemd<span style=\"color: #f92672;\">-</span>logind[<span style=\"color: #ae81ff;\">383</span>]: Session <span style=\"color: #ae81ff;\">30</span> logged out<span style=\"color: #f92672;\">.</span> Waiting <span style=\"color: #66d9ef;\">for</span> processes to exit<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> wigner res[<span style=\"color: #ae81ff;\">11464</span>]: cg_load_hierarchies: Please use the LSF package <span style=\"color: #66d9ef;\">with</span> higher glibc version to enable LSF cgroup v2 support<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> wigner systemd[<span style=\"color: #ae81ff;\">1</span>]: lsfd<span style=\"color: #f92672;\">.</span>service: Succeeded<span style=\"color: #f92672;\">.</span> Apr  <span style=\"color: #ae81ff;\">3</span> <span style=\"color: #ae81ff;\">21</span>:<span style=\"color: #ae81ff;\">44</span>:<span style=\"color: #ae81ff;\">59</span> wigner systemd[<span style=\"color: #ae81ff;\">1</span>]: lsfd<span style=\"color: #f92672;\">.</span>service: Consumed <span style=\"color: #ae81ff;\">1</span>h <span style=\"color: #ae81ff;\">17</span>min <span style=\"color: #ae81ff;\">44.610</span>s CPU time<span style=\"color: #f92672;\">.</span></code></pre></div></details><br /><!-- raw HTML omitted -->As expected, we observed that LSF log messages are written to the fromnet file. And importantly each entry contains the hostname, so that we can identify the origin of the message.</p><p><strong>Conclusion</strong></p><p>What started out as a chat about logging, grew into an idea of a blog, for which I am thankful for the collaboration of Peter. We’ve illustrated an example here of how to setup centralized logging on a Turing Pi system with syslog-ng to collect system and LSF logs.</p><p>Of course collecting log messages centrally is just the start of a journey. It is an important step as it allows for significantly easier debugging and troubleshooting. You can store logs to databases for easier search. And once you better understand which log messages are important, you can even potentially parse those and generate alersts from them or dashboards. All of these help you to make sure that your HPC system runs smoothly and with minimal downtime. For me this was a learning experience and I&rsquo;ll be looking how I can implement more broadly centralized logging in my home network.</p>",
            "url": "https://hpc.social/personal-blog/2024/centralized-system-and-lsf-logging-on-a-turing-pi-system/",
            
            
            
            
            
            "date_published": "2024-04-05T12:34:38-06:00",
            "date_modified": "2024-04-05T12:34:38-06:00",
            
                "author": "Ramblings of a supercomputing enthusiast."
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2024/4-turning-and-7-chilling/",
            "title": "4 turning and 7 chilling",
            "summary": null,
            "content_text": "How to keep your coolI&rsquo;m back again and revisiting the Turing Pi V1 board. This time the focus isn&rsquo;t on software, but rather cooling. In my previous write-up Pi in the sky? A compute cluster in mini ITX form factor, I used a USB fan I had at hand to keep the temperature of the compute modules in check during the Linpack runs. Although the fan was a seriously sketchy one, it did the job, and prevented throttling of the compute modules under high load, albeit with much noise. Clearly not content with this mediocre setup I pondered what other solutions I could quickly come up with.Looking in my electronics spare parts bin, I came across 2 spare Noctua 40x40x20mm fans, part number NF-A4x20 PWM. I found that these fans fit well on the Turing Pi board perpendicular to the compute modules. I measured that for full cooling coverage of the compute modules I&rsquo;d need 4 such fans, side by side. However before investing in two more fans, I needed to confirm that they had enough oomph (yes that&rsquo;s a technical term) to keep things cool.So my plan was to first test two fans cooling half of the modules. However, to test these fans out, I first needed to get a hold of some USB to 3/4-pin fan power adapter cables. Once I had these adapters, I used a thick elastic band to bind the 2 fans together, and connected them to the USB for power using the adapters and give them a whirl - pun intended. Of course I fell back on Linpack to get the compute modules busy.The results were promising enough that I immediately ordered two more fans and adapters to complete the setup which is shown in the photo below. A thick elastic band was used again to fasten the remaining 2 fans together. Of course, the setup will be made more robust to ensure that fans will stay in place. And I&rsquo;ll do a bit of work on cable management.Totally chillThe view of the dashboard (see below) speaks for itself. Under heavy load running Linpack, the compute modules don&rsquo;t exceed 50C. This is about 10 degrees cooler than what I saw with that USB desk fan. So I&rsquo;d consider that a result. Plus the Noctua fans are so much quieter and will be much more durable in the long run.ConclusionSo where does the title of my blog come from? It&rsquo;s inspired by the slogan &ldquo;6 turning and 4 burning&rdquo; of the B-36 Peacemaker strategic bomber! You can see the B-36 in all it&rsquo;s glory in this short excerpt from the 1955 film Strategic Air Command starring Jimmy Stewart. You could say I have eclectic taste in films. Plus the B-36 has always fascinated me with it&rsquo;s combination of jet and piston engines. As for this blog, 4 turning obviously refers to the 4 Noctua fans turning. And 7 chilling refers to the 7 CM3 modules that now keep their cool under pressure. With a more suitable cooling solution in place, especially as the warmer days arrive, I can now refocus my attention to the software side of things. And as always, stay cool!",
            "content_html": "<p><strong>How to keep your cool</strong></p><p>I&rsquo;m back again and revisiting the Turing Pi V1 board. This time the focus isn&rsquo;t on software, but rather cooling. In my previous write-up <a href=\"https://www.gaborsamu.com/blog/turingpi_hpl/\">Pi in the sky? A compute cluster in mini ITX form factor</a>, I used a USB fan I had at hand to keep the temperature of the compute modules in check during the Linpack runs. Although the fan was a seriously sketchy one, it did the job, and prevented throttling of the compute modules under high load, albeit with much noise. Clearly not content with this mediocre setup I pondered what other solutions I could quickly come up with.</p><p>Looking in my electronics spare parts bin, I came across 2 spare Noctua 40x40x20mm fans, part number <em>NF-A4x20 PWM</em>. I found that these fans fit well on the Turing Pi board perpendicular to the compute modules. I measured that for full cooling coverage of the compute modules I&rsquo;d need 4 such fans, side by side. However before investing in two more fans, I needed to confirm that they had enough oomph (yes that&rsquo;s a technical term) to keep things cool.</p><p>So my plan was to first test two fans cooling half of the modules. However, to test these fans out, I first needed to get a hold of some USB to 3/4-pin fan power adapter cables. Once I had these adapters, I used a thick elastic band to bind the 2 fans together, and connected them to the USB for power using the adapters and give them a whirl - pun intended. Of course I fell back on Linpack to get the compute modules busy.</p><p>The results were promising enough that I immediately ordered two more fans and adapters to complete the setup which is shown in the photo below. A thick elastic band was used again to fasten the remaining 2 fans together. Of course, the setup will be made more robust to ensure that fans will stay in place. And I&rsquo;ll do a bit of work on cable management.</p><figure><img src=\"https://www.gaborsamu.com/images/turingpi_noctua.jpg\" /></figure><p><strong>Totally chill</strong></p><p>The view of the dashboard (see below) speaks for itself. Under heavy load running Linpack, the compute modules don&rsquo;t exceed 50C. This is about 10 degrees cooler than what I saw with that USB desk fan. So I&rsquo;d consider that a result. Plus the Noctua fans are so much quieter and will be much more durable in the long run.</p><figure><img src=\"https://www.gaborsamu.com/images/turingpi_dashboard_noctua.png\" /></figure><p><strong>Conclusion</strong></p><p>So where does the title of my blog come from? It&rsquo;s inspired by the slogan &ldquo;6 turning and 4 burning&rdquo; of the B-36 Peacemaker strategic bomber! You can see the B-36 in all it&rsquo;s glory in this short <a href=\"https://youtu.be/9kQ2X84PRvY?si=q9FZmWFavXHbPcE8\">excerpt</a> from the 1955 film <em>Strategic Air Command</em> starring Jimmy Stewart. You could say I have eclectic taste in films. Plus the B-36 has always fascinated me with it&rsquo;s combination of jet and piston engines. As for this blog, 4 turning obviously refers to the 4 Noctua fans turning. And 7 chilling refers to the 7 CM3 modules that now keep their cool under pressure. With a more suitable cooling solution in place, especially as the warmer days arrive, I can now refocus my attention to the software side of things. And as always, stay cool!</p>",
            "url": "https://hpc.social/personal-blog/2024/4-turning-and-7-chilling/",
            
            
            
            
            
            "date_published": "2024-03-21T18:09:30-06:00",
            "date_modified": "2024-03-21T18:09:30-06:00",
            
                "author": "Ramblings of a supercomputing enthusiast."
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2024/advanced-lsf-resource-connector-configuration-on-ibm-cloud-part-ii/",
            "title": "Advanced LSF resource connector configuration on IBM Cloud - part II",
            "summary": null,
            "content_text": "OverviewBack in November 2023 I authored a blog titled Advanced LSF resource connector configuration on IBM Cloud - part I. As I signed off in that post, I mentioned that there would be a follow-on post to cover some more advanced configuration topics on LSF resource connector.  And that’s the topic of this article today.To recap, the IBM LSF resource connector functionality enables LSF clusters to dynamically spin-up cloud instances from supported resource providers in the cloud based on workload demand, and to destroy those instances when no longer required.The LSF resource connector intelligently choses the most appropriate cloud instance type for a given job from the templates that have been defined by the administrator. This is done automatically and is transparent from the end user perspective. What if the job requires to run on a very specific instance type? In this post we’ll show you how this is feasible by defining an LSF string resource, along with the necessary configuration of LSF resource connector and a supporting script. This will allow users to submit jobs to LSF with a resource requirement string specifying the cloud instance type desired.For the example below, we’ll be using an IBM LSF environment which has been deployed on IBM Cloud using the extensive deployment automation that is available via the IBM Cloud catalog for IBM LSF. Using this automation, you can deploy an LSF cluster in about 10 minutes time including the creation of the virtual private cloud (VPC), networking, security, bastion node, NFS server node, LSF management nodes and optionally LSF Application Center.What is user_data.sh?We’ll start with a brief description of the LSF resource connector user_data.sh script. This script will play an important part in the configuration of the compute servers as we’ll see. The user_data.sh script is used to start-up the LSF daemons on the compute instances launched by LSF resource connector. It also crucially enables admins to configure settings, including LSF settings, which is what we’ll be using the in example below.Specifying the cloud instance typeBy default, the LSF resource connector intelligently chooses the cloud instance profile type based upon the job submission parameters. For example, it considers things like the number of processor requested, the memory requested just to name of few. And it will startup the compute instance or instances from the available configured templates which most closely matches the job requirement.What if you need to request a very specific compute instance type for the work that you’ve submitted based upon other, site-specific needs?  Here we will show exactly how you can achieve this.Let the configuration begin!We begin with updating the LSF configuration to create a new string resource called profile. In the configuration file $LSF_ENVDIR/lsf.shared, define the new string resource profile in the Resource section.….….Begin ResourceRESOURCENAME\tTYPE\t    INTERVAL\tINCREASING\tDESCRIPTION        # Keywordsprofile \t  String   ()       ()             (IBM Cloud Gen2 profile type)End Resource….….To make the change take effect, reconfigure the LSF cluster with the LSF command lsadmin reconfig.# lsadmin reconfig -vChecking configuration files ...EGO 3.4.0 build 1599999, Jan 04 2023Copyright International Business Machines Corp. 1992, 2016.US Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM Corp.  binary type: linux3.10-glibc2.17-x86_64Reading configuration from /opt/ibm/lsf/conf/lsf.confMar 13 16:57:25 2024 1478621 6 3.4.0 Lim starting...Mar 13 16:57:25 2024 1478621 6 3.4.0 LIM is running in advanced workload execution mode.Mar 13 16:57:25 2024 1478621 6 3.4.0 Master LIM is not running in EGO_DISABLE_UNRESOLVABLE_HOST mode.Mar 13 16:57:25 2024 1478621 5 3.4.0 /opt/ibm/lsf/10.1/linux3.10-glibc2.17-x86_64/etc/lim -CMar 13 16:57:26 2024 1478621 6 3.4.0 LIM is running as IBM Spectrum LSF Standard Edition.Mar 13 16:57:26 2024 1478621 6 3.4.0 reCheckClass: numhosts 1 so reset exchIntvl to 15.00Mar 13 16:57:26 2024 1478621 6 3.4.0 Checking Done.---------------------------------------------------------No errors found.Restart only the master candidate hosts? [y/n] nDo you really want to restart LIMs on all hosts? [y/n] yRestart LIM on &lt;icgen2host-AAA-BBB-CCC-DDD&gt; ...... doneNow, check that the profile variable has been setup properly. This can be done using the LSF lsinfo command.# lsinfo |grep profileprofile        String   N/A   IBM Cloud Gen2 profile typeNow we’re ready to update the LSF resource connector templates to add the profile string variable. For this example, there are two templates defined for IBM Cloud profile types bx2-4x16 and mx2-16x128 in the configuration file $LSF_ENVDIR/resource_connector/ibmcloudgen2/conf. Within the template definition, the profile string variable is defined, and a value is set for each respective profile type. Note that the “-“ character cannot be used in the LSF string variables, and in place of that the “_” character is used. The specified profile string for each respective template is defined and used as the selection criteria by LSF resource connector. Then the userData field is used to ensure that this value gets passed and set in the compute instance that is started by the LSF resource connector when the user_data.sh script is run.Instance typeLSF profile variable string valuebx2-4x16bx2_4x16mx2-16x128mx2_16x128ibmcloudgen2_templates.json, with profile configured (obfuscated){    \"templates\": [        {            \"templateId\": \"Template-1\",            \"maxNumber\": 2,            \"attributes\": {                \"type\": [\"String\", \"X86_64\"],                \"ncores\": [\"Numeric\", \"2\"],                \"ncpus\": [\"Numeric\", \"4\"],                \"mem\": [\"Numeric\", \"16384\"],                \"icgen2host\": [\"Boolean\", \"1\"],                 \"profile\":[\"String\",\"bx2_4x16\"]            },            \"imageId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"subnetId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"vpcId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"vmType\": \"bx2-4x16\",            \"userData\":\"profile=bx2_4x16\",            \"securityGroupIds\": [\"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\"],            \"resourceGroupId\": \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\",            \"sshkey_id\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"priority\": \"10\",             \"region\": \"us-east\",            \"zone\": \"us-east-1\"         },       {            \"templateId\": \"Template-2\",            \"maxNumber\": 2,            \"attributes\": {                \"type\": [\"String\", \"X86_64\"],                \"ncores\": [\"Numeric\", \"8\"],                \"ncpus\": [\"Numeric\", \"16\"],                \"mem\": [\"Numeric\", \"131072\"],                \"icgen2host\": [\"Boolean\", \"1\"],\t\t       \"profile\":[\"String\",\"mx2_16x128\"]            },            \"imageId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"subnetId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"vpcId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"vmType\": \"mx2-16x128\",            \"userData\":\"profile=mx2_16x128\",            \"securityGroupIds\": [\"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\"],            \"resourceGroupId\": \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\",            \"sshkey_id\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"priority\": \"5\",            \"region\": \"us-east\",            \"zone\": \"us-east-1\"        }    ]}Now the user_data.sh script is required to be updated in order to set the value of the profile variable in the LSF resourcemap based upon what was requested by the user. This will be added to the LSF configuration during the bootup of the dynamic cloud instances. For more information about the LSF resourcemap read here.user_data.sh script portion….….# Set value of profile variable in the LSF resourcemap. This is based# on the profile value requested at job submission time. if [ -n \"$profile\" ]; thensed -i \"s/\\(LSF_LOCAL_RESOURCES=.*\\)\\\"/\\1 [resourcemap $profile*profile]\\\"/\" $LSF_CONF_FILEecho \"update LSF_LOCAL_RESOURCES in $LSF_CONF_FILE successfully, add [resourcemap ${profile}*profile]\" &gt;&gt; $logfileelseecho \"profile doesn't exist in environment variable\" &gt;&gt; $logfilefi….….With all of the configuration in place, it’s now time to test things out. Initially, a stress job is submitted requesting 4 cores is submitted without requesting a specific compute profile. In this case, the LSF resource connector will chose the most appropriate instance type from the configured templates. In our configuration the templates for instance types bx2-4x16 and mx2-16x128 are configured. Given this, we expect the LSF resource connector to startup a bx2-4x16 instance to satisfy the requirements for this example job.$ bsub -n 4 -q normal -o /mnt/data/%J.out /usr/bin/stress --cpu 4 --vm-bytes 8192MB --timeout 60s Job &lt;3811&gt; is submitted to queue &lt;normal&gt;.After a few moments, we see that a new host, icgen2host-XXX-YYY-ZZZ-44 joins the LSF cluster and the job enters run state. We note that this is a host with the characteristics of 4 cores, and 16GB RAM, which matches the type bx2-4x16.$ lsload -wHOST_NAME               status  r15s   r1m  r15m   ut    pg  ls    it   tmp   swp   memicgen2host-XXX-YYY-ZZZ-37      ok   0.4   0.1   0.1   2%   0.0   1    16   40G    0icgen2host-XXX-YYY-ZZZ-44      ok   0.9   0.2   0.1  19%   0.0   0     0   88G    0M 14.9G$ lshosts -wHOST_NAME                       type       model  cpuf ncpus maxmem maxswp server RESOURCESicgen2host-XXX-YYY-ZZZ-37        X86_64    Intel_E5  12.5     4  15.4G      -    Yes (mg docker ParaView)icgen2host-XXX-YYY-ZZZ-44\t\t X86_64    Intel_E5  12.5     4  15.5G      -    Dyn (icgen2host docker)$ bjobs -l -rJob &lt;3811&gt;, User &lt;lsfadmin&gt;, Project &lt;default&gt;, Status &lt;RUN&gt;, Queue &lt;normal&gt;, C                     ommand &lt;/usr/bin/stress --cpu 4 --vm-bytes 8192MB --timeou                     t 60s&gt;, Share group charged &lt;/lsfadmin&gt;Mon Mar 18 19:42:22: Submitted from host &lt;icgen2host-XXX-YYY-ZZZ-37&gt;, CWD &lt;$HOME&gt;,                      Output File &lt;/mnt/data/3811.out&gt;, 4 Task(s);Mon Mar 18 19:45:11: Started 4 Task(s) on Host(s) &lt;icgen2host-XXX-YYY-ZZZ-44&gt; &lt;icg                     en2host-XXX-YYY-ZZZ-44&gt; &lt;icgen2host-XXX-YYY-ZZZ-44&gt; &lt;icgen2host-                     XXX-YYY-ZZZ-44&gt;, Allocated 4 Slot(s) on Host(s) &lt;icgen2host-X                     XX-YYY-ZZZ-44&gt; &lt;icgen2host-XXX-YYY-ZZZ-44&gt; &lt;icgen2host-XXX-YYY-ZZZ-                     44&gt; &lt;icgen2host-XXX-YYY-ZZZ-44&gt;, Execution Home &lt;/home/lsfadm                     in&gt;, Execution CWD &lt;/home/lsfadmin&gt;;Mon Mar 18 19:45:47: Resource usage collected.                     The CPU time used is 142 seconds.                     MEM: 3 Mbytes;  SWAP: 0 Mbytes;  NTHREAD: 8                     PGID: 2169;  PIDs: 2169 2170 2172 2173 2174 2175 2176  MEMORY USAGE: MAX MEM: 3 Mbytes;  AVG MEM: 3 Mbytes; MEM Efficiency: 0.00% CPU USAGE: CPU PEAK: 0.00 ;  CPU PEAK DURATION: 0 second(s) CPU AVERAGE EFFICIENCY: 0.00% ;  CPU PEAK EFFICIENCY: 0.00% SCHEDULING PARAMETERS:           r15s   r1m  r15m   ut      pg    io   ls    it    tmp    swp    mem loadSched   -     -     -     -       -     -    -     -     -      -      -   loadStop    -     -     -     -       -     -    -     -     -      -      -   RESOURCE REQUIREMENT DETAILS: Combined: select[type == local] order[r15s:pg] Effective: select[type == local] order[r15s:pg] $ bhist -l 3811Job &lt;3811&gt;, User &lt;lsfadmin&gt;, Project &lt;default&gt;, Command &lt;/usr/bin/stress --cpu                      4 --vm-bytes 8192MB --timeout 60s&gt;Mon Mar 18 19:42:22: Submitted from host &lt;icgen2host-XXX-YYY-ZZZ-37&gt;, to Queue &lt;no                     rmal&gt;, CWD &lt;$HOME&gt;, Output File &lt;/mnt/data/%J.out&gt;, 4 Task                     (s);Mon Mar 18 19:45:11: Dispatched 4 Task(s) on Host(s) &lt;icgen2host-XXX-YYY-ZZZ-44&gt; &lt;                     icgen2host-XXX-YYY-ZZZ-44&gt; &lt;icgen2host-XXX-YYY-ZZZ-44&gt; &lt;icgen2ho                     st-XXX-YYY-ZZZ-44&gt;, Allocated 4 Slot(s) on Host(s) &lt;icgen2hos                     t-XXX-YYY-ZZZ-44&gt; &lt;icgen2host-XXX-YYY-ZZZ-44&gt; &lt;icgen2host-XXX-YYY                     -ZZZ-44&gt; &lt;icgen2host-XXX-YYY-ZZZ-44&gt;, Effective RES_REQ &lt;select                     [type == local] order[r15s:pg] &gt;;Mon Mar 18 19:45:11: Starting (Pid 2169);Mon Mar 18 19:45:11: Running with execution home &lt;/home/lsfadmin&gt;, Execution CW                     D &lt;/home/lsfadmin&gt;, Execution Pid &lt;2169&gt;;Mon Mar 18 19:46:11: Done successfully. The CPU time used is 239.3 seconds;Mon Mar 18 19:46:12: Post job process done successfully;MEMORY USAGE:MAX MEM: 3 Mbytes;  AVG MEM: 2 Mbytes; MEM Efficiency: 0.00%CPU USAGE:CPU PEAK: 3.98 ;  CPU PEAK DURATION: 60 second(s)CPU AVERAGE EFFICIENCY: 99.58% ;  CPU PEAK EFFICIENCY: 99.58%Summary of time in seconds spent in various states by  Mon Mar 18 19:46:12  PEND     PSUSP    RUN      USUSP    SSUSP    UNKWN    TOTAL  169      0        60       0        0        0        229         Next, let’s submit the same job, but explicitly requesting the profile type mx2-16x128. To do this, we add the resource requirement requesting profile to be equal to mx2_16x128 as follows:$ bsub -q normal -R \"profile==mx2_16x128\" -n 4 -o /mnt/data/%J.out /usr/bin/stress --cpu 4 --vm-bytes 8192MB --timeout 60s Job &lt;3813&gt; is submitted to queue &lt;normal&gt;.After a few moments, a dynamic host with 16 CPUs and 128 GB RAM joins the cluster, which corresponds to instance type mx2-16x128.$ lshosts -wHOST_NAME                       type       model  cpuf ncpus maxmem maxswp server RESOURCESicgen2host-XXX-YYY-ZZZ-37        X86_64    Intel_E5  12.5     4  15.4G      -    Yes (mg docker ParaView)icgen2host-XXX-YYY-ZZZ-46        X86_64    Intel_E5  12.5    16 125.7G      -    Dyn (icgen2host docker)$ lsload -wHOST_NAME               status  r15s   r1m  r15m   ut    pg  ls    it   tmp   swp   memicgen2host-XXX-YYY-ZZZ-37      ok   0.0   0.1   0.1   3%   0.0   1     2   40G    0M 12.2Gicgen2host-XXX-YYY-ZZZ-46      ok   1.6   0.4   0.1  17%   0.0   0     0   88G    0M 123.8G$ bjobs -l -rJob &lt;3813&gt;, User &lt;lsfadmin&gt;, Project &lt;default&gt;, Status &lt;RUN&gt;, Queue &lt;normal&gt;, C                     ommand &lt;/usr/bin/stress --cpu 4 --vm-bytes 8192MB --timeou                     t 60s&gt;, Share group charged &lt;/lsfadmin&gt;Mon Mar 18 20:27:52: Submitted from host &lt;icgen2host-XXX-YYY-ZZZ-37&gt;, CWD &lt;$HOME&gt;,                      Output File &lt;/mnt/data/3813.out&gt;, 4 Task(s), Requested Re                     sources &lt;profile==mx2_16x128&gt;;Mon Mar 18 20:30:01: Started 4 Task(s) on Host(s) &lt;icgen2host-XXX-YYY-ZZZ-46&gt; &lt;icg                     en2host-XXX-YYY-ZZZ-46&gt; &lt;icgen2host-XXX-YYY-ZZZ-46&gt; &lt;icgen2host-                     XXX-YYY-ZZZ-46&gt;, Allocated 4 Slot(s) on Host(s) &lt;icgen2host                     -XXX-YYY-ZZZ-46&gt; &lt;icgen2host-XXX-YYY-ZZZ-46&gt; &lt;icgen2host-XXX-Y                     YY-ZZZ-46&gt; &lt;icgen2host-XXX-YYY-ZZZ-46&gt;, Execution Home &lt;/home/lsfadm                     in&gt;, Execution CWD &lt;/home/lsfadmin&gt;; MEMORY USAGE: MEM Efficiency: 0.00% CPU USAGE: CPU PEAK: 0.00 ;  CPU PEAK DURATION: 0 second(s) CPU AVERAGE EFFICIENCY: 0.00% ;  CPU PEAK EFFICIENCY: 0.00% SCHEDULING PARAMETERS:           r15s   r1m  r15m   ut      pg    io   ls    it    tmp    swp    mem loadSched   -     -     -     -       -     -    -     -     -      -      -   loadStop    -     -     -     -       -     -    -     -     -      -      -   RESOURCE REQUIREMENT DETAILS: Combined: select[(profile == mx2_16x128 ) &amp;&amp; (type == any)] order[r15s:pg] Effective: select[(profile == mx2_16x128 ) &amp;&amp; (type == any)] order[r15s:pg] Additionally, using the LSF lshosts command with the -s option, we can view information about the resources in the environment. We see in particular the profile resource is set to the value mx2_16x128 for the dynamic host icgen2host-XXX-YYY-ZZZ-46.$ lshosts -sRESOURCE                                VALUE       LOCATIONrc_account                            default       icgen2host-XXX-YYY-ZZZ-46 profile                            mx2_16x128       icgen2host-XXX-YYY-ZZZ-46 instanceID               0757_95e39240-22e7-4734-8fd5-9988ab247801                                                      icgen2host-XXX-YYY-ZZZ-46 We have shown the great flexibility that LSF provides for configuring the resource connector capability. Generally speaking, LSF provides many open interfaces which allow site specific configuration or customization to be realized. In the next blog in this series, we’ll take a closer look at running Docker jobs under LSF on dynamic cloud resources.",
            "content_html": "<p><strong>Overview</strong></p><p>Back in November 2023 I authored a blog titled <a href=\"https://www.gaborsamu.com/blog/lsf_rc_ibmcloud_part1/\">Advanced LSF resource connector configuration on IBM Cloud - part I</a>. As I signed off in that post, I mentioned that there would be a follow-on post to cover some more advanced configuration topics on LSF resource connector.  And that’s the topic of this article today.</p><p>To recap, the <a href=\"https://www.ibm.com/products/hpc-workload-management\">IBM LSF</a> resource connector functionality enables LSF clusters to dynamically spin-up cloud instances from supported resource providers in the cloud based on workload demand, and to destroy those instances when no longer required.</p><p>The LSF resource connector intelligently choses the most appropriate cloud instance type for a given job from the templates that have been defined by the administrator. This is done automatically and is transparent from the end user perspective. What if the job requires to run on a very specific instance type? In this post we’ll show you how this is feasible by defining an LSF string resource, along with the necessary configuration of LSF resource connector and a supporting script. This will allow users to submit jobs to LSF with a resource requirement string specifying the cloud instance type desired.</p><p>For the example below, we’ll be using an IBM LSF environment which has been deployed on IBM Cloud using the extensive deployment automation that is available via the IBM Cloud catalog for <a href=\"https://cloud.ibm.com/catalog/content/terraform-1623200063-71606cab-c6e1-4f95-a47a-2ce541dcbed8-global\">IBM LSF</a>. Using this automation, you can deploy an LSF cluster in about 10 minutes time including the creation of the virtual private cloud (VPC), networking, security, bastion node, NFS server node, LSF management nodes and optionally LSF Application Center.</p><p><strong>What is user_data.sh?</strong></p><p>We’ll start with a brief description of the LSF resource connector <em>user_data.sh</em> script. This script will play an important part in the configuration of the compute servers as we’ll see. The <em>user_data.sh</em> script is used to start-up the LSF daemons on the compute instances launched by LSF resource connector. It also crucially enables admins to configure settings, including LSF settings, which is what we’ll be using the in example below.</p><p><strong>Specifying the cloud instance type</strong></p><p>By default, the LSF resource connector intelligently chooses the cloud instance profile type based upon the job submission parameters. For example, it considers things like the number of processor requested, the memory requested just to name of few. And it will startup the compute instance or instances from the available configured templates which most closely matches the job requirement.</p><p>What if you need to request a very specific compute instance type for the work that you’ve submitted based upon other, site-specific needs?  Here we will show exactly how you can achieve this.</p><p><strong>Let the configuration begin!</strong></p><p>We begin with updating the LSF configuration to create a new string resource called profile. In the configuration file <em>$LSF_ENVDIR/lsf.shared</em>, define the new string resource profile in the <em>Resource</em> section.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">….….Begin ResourceRESOURCENAME\tTYPE\t    INTERVAL\tINCREASING\tDESCRIPTION        # Keywordsprofile \t  String   ()       ()             (IBM Cloud Gen2 profile type)End Resource….….</code></pre></div><p>To make the change take effect, reconfigure the LSF cluster with the LSF command <em>lsadmin reconfig</em>.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\"># lsadmin reconfig -vChecking configuration files ...EGO 3.4.0 build 1599999, Jan 04 2023Copyright International Business Machines Corp. 1992, 2016.US Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM Corp.  binary type: linux3.10-glibc2.17-x86_64Reading configuration from /opt/ibm/lsf/conf/lsf.confMar 13 16:57:25 2024 1478621 6 3.4.0 Lim starting...Mar 13 16:57:25 2024 1478621 6 3.4.0 LIM is running in advanced workload execution mode.Mar 13 16:57:25 2024 1478621 6 3.4.0 Master LIM is not running in EGO_DISABLE_UNRESOLVABLE_HOST mode.Mar 13 16:57:25 2024 1478621 5 3.4.0 /opt/ibm/lsf/10.1/linux3.10-glibc2.17-x86_64/etc/lim -CMar 13 16:57:26 2024 1478621 6 3.4.0 LIM is running as IBM Spectrum LSF Standard Edition.Mar 13 16:57:26 2024 1478621 6 3.4.0 reCheckClass: numhosts 1 so reset exchIntvl to 15.00Mar 13 16:57:26 2024 1478621 6 3.4.0 Checking Done.---------------------------------------------------------No errors found.Restart only the master candidate hosts? [y/n] nDo you really want to restart LIMs on all hosts? [y/n] yRestart LIM on &lt;icgen2host-AAA-BBB-CCC-DDD&gt; ...... done</code></pre></div><p>Now, check that the profile variable has been setup properly. This can be done using the LSF <em>lsinfo</em> command.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\"># lsinfo |grep profileprofile        String   N/A   IBM Cloud Gen2 profile type</code></pre></div><p>Now we’re ready to update the LSF resource connector templates to add the <em>profile</em> string variable. For this example, there are two templates defined for IBM Cloud profile types <em>bx2-4x16</em> and <em>mx2-16x128</em> in the configuration file <em>$LSF_ENVDIR/resource_connector/ibmcloudgen2/conf</em>. Within the template definition, the <em>profile</em> string variable is defined, and a value is set for each respective profile type. Note that the “-“ character cannot be used in the LSF string variables, and in place of that the “_” character is used. The specified profile string for each respective template is defined and used as the selection criteria by LSF resource connector. Then the <em>userData</em> field is used to ensure that this value gets passed and set in the compute instance that is started by the LSF resource connector when the <em>user_data.sh</em> script is run.</p><table><thead><tr><th style=\"text-align: left;\">Instance type</th><th>LSF <em>profile</em> variable string value</th></tr></thead><tbody><tr><td style=\"text-align: left;\">bx2-4x16</td><td>bx2_4x16</td></tr><tr><td style=\"text-align: left;\">mx2-16x128</td><td>mx2_16x128</td></tr></tbody></table><hr /><p><strong><em>ibmcloudgen2_templates.json</em>, with <em>profile</em> configured (obfuscated)</strong></p><div class=\"highlight\"><pre><code class=\"language-plaintext\">{    \"templates\": [        {            \"templateId\": \"Template-1\",            \"maxNumber\": 2,            \"attributes\": {                \"type\": [\"String\", \"X86_64\"],                \"ncores\": [\"Numeric\", \"2\"],                \"ncpus\": [\"Numeric\", \"4\"],                \"mem\": [\"Numeric\", \"16384\"],                \"icgen2host\": [\"Boolean\", \"1\"],                 \"profile\":[\"String\",\"bx2_4x16\"]            },            \"imageId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"subnetId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"vpcId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"vmType\": \"bx2-4x16\",            \"userData\":\"profile=bx2_4x16\",            \"securityGroupIds\": [\"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\"],            \"resourceGroupId\": \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\",            \"sshkey_id\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"priority\": \"10\",             \"region\": \"us-east\",            \"zone\": \"us-east-1\"         },       {            \"templateId\": \"Template-2\",            \"maxNumber\": 2,            \"attributes\": {                \"type\": [\"String\", \"X86_64\"],                \"ncores\": [\"Numeric\", \"8\"],                \"ncpus\": [\"Numeric\", \"16\"],                \"mem\": [\"Numeric\", \"131072\"],                \"icgen2host\": [\"Boolean\", \"1\"],\t\t       \"profile\":[\"String\",\"mx2_16x128\"]            },            \"imageId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"subnetId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"vpcId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"vmType\": \"mx2-16x128\",            \"userData\":\"profile=mx2_16x128\",            \"securityGroupIds\": [\"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\"],            \"resourceGroupId\": \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\",            \"sshkey_id\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"priority\": \"5\",            \"region\": \"us-east\",            \"zone\": \"us-east-1\"        }    ]}</code></pre></div><p>Now the <em>user_data.sh</em> script is required to be updated in order to set the value of the <em>profile</em> variable in the LSF resourcemap based upon what was requested by the user. This will be added to the LSF configuration during the bootup of the dynamic cloud instances. For more information about the LSF resourcemap read <a href=\"https://www.ibm.com/docs/en/spectrum-lsf/10.1.0?topic=resources-configure-lsfclustercluster-name-resourcemap-section\">here</a>.</p><p><strong>user_data.sh script portion</strong></p><div class=\"highlight\"><pre><code class=\"language-plaintext\">….….# Set value of profile variable in the LSF resourcemap. This is based# on the profile value requested at job submission time. if [ -n \"$profile\" ]; thensed -i \"s/\\(LSF_LOCAL_RESOURCES=.*\\)\\\"/\\1 [resourcemap $profile*profile]\\\"/\" $LSF_CONF_FILEecho \"update LSF_LOCAL_RESOURCES in $LSF_CONF_FILE successfully, add [resourcemap ${profile}*profile]\" &gt;&gt; $logfileelseecho \"profile doesn't exist in environment variable\" &gt;&gt; $logfilefi….….</code></pre></div><p>With all of the configuration in place, it’s now time to test things out. Initially, a stress job is submitted requesting 4 cores is submitted without requesting a specific compute profile. In this case, the LSF resource connector will chose the most appropriate instance type from the configured templates. In our configuration the templates for instance types <em>bx2-4x16</em> and <em>mx2-16x128</em> are configured. Given this, we expect the LSF resource connector to startup a <em>bx2-4x16</em> instance to satisfy the requirements for this example job.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ bsub -n 4 -q normal -o /mnt/data/%J.out /usr/bin/stress --cpu 4 --vm-bytes 8192MB --timeout 60s Job &lt;3811&gt; is submitted to queue &lt;normal&gt;.</code></pre></div><p>After a few moments, we see that a new host, <em>icgen2host-XXX-YYY-ZZZ-44</em> joins the LSF cluster and the job enters run state. We note that this is a host with the characteristics of 4 cores, and 16GB RAM, which matches the type <em>bx2-4x16</em>.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ lsload -wHOST_NAME               status  r15s   r1m  r15m   ut    pg  ls    it   tmp   swp   memicgen2host-XXX-YYY-ZZZ-37      ok   0.4   0.1   0.1   2%   0.0   1    16   40G    0icgen2host-XXX-YYY-ZZZ-44      ok   0.9   0.2   0.1  19%   0.0   0     0   88G    0M 14.9G$ lshosts -wHOST_NAME                       type       model  cpuf ncpus maxmem maxswp server RESOURCESicgen2host-XXX-YYY-ZZZ-37        X86_64    Intel_E5  12.5     4  15.4G      -    Yes (mg docker ParaView)icgen2host-XXX-YYY-ZZZ-44\t\t X86_64    Intel_E5  12.5     4  15.5G      -    Dyn (icgen2host docker)$ bjobs -l -rJob &lt;3811&gt;, User &lt;lsfadmin&gt;, Project &lt;default&gt;, Status &lt;RUN&gt;, Queue &lt;normal&gt;, C                     ommand &lt;/usr/bin/stress --cpu 4 --vm-bytes 8192MB --timeou                     t 60s&gt;, Share group charged &lt;/lsfadmin&gt;Mon Mar 18 19:42:22: Submitted from host &lt;icgen2host-XXX-YYY-ZZZ-37&gt;, CWD &lt;$HOME&gt;,                      Output File &lt;/mnt/data/3811.out&gt;, 4 Task(s);Mon Mar 18 19:45:11: Started 4 Task(s) on Host(s) &lt;icgen2host-XXX-YYY-ZZZ-44&gt; &lt;icg                     en2host-XXX-YYY-ZZZ-44&gt; &lt;icgen2host-XXX-YYY-ZZZ-44&gt; &lt;icgen2host-                     XXX-YYY-ZZZ-44&gt;, Allocated 4 Slot(s) on Host(s) &lt;icgen2host-X                     XX-YYY-ZZZ-44&gt; &lt;icgen2host-XXX-YYY-ZZZ-44&gt; &lt;icgen2host-XXX-YYY-ZZZ-                     44&gt; &lt;icgen2host-XXX-YYY-ZZZ-44&gt;, Execution Home &lt;/home/lsfadm                     in&gt;, Execution CWD &lt;/home/lsfadmin&gt;;Mon Mar 18 19:45:47: Resource usage collected.                     The CPU time used is 142 seconds.                     MEM: 3 Mbytes;  SWAP: 0 Mbytes;  NTHREAD: 8                     PGID: 2169;  PIDs: 2169 2170 2172 2173 2174 2175 2176  MEMORY USAGE: MAX MEM: 3 Mbytes;  AVG MEM: 3 Mbytes; MEM Efficiency: 0.00% CPU USAGE: CPU PEAK: 0.00 ;  CPU PEAK DURATION: 0 second(s) CPU AVERAGE EFFICIENCY: 0.00% ;  CPU PEAK EFFICIENCY: 0.00% SCHEDULING PARAMETERS:           r15s   r1m  r15m   ut      pg    io   ls    it    tmp    swp    mem loadSched   -     -     -     -       -     -    -     -     -      -      -   loadStop    -     -     -     -       -     -    -     -     -      -      -   RESOURCE REQUIREMENT DETAILS: Combined: select[type == local] order[r15s:pg] Effective: select[type == local] order[r15s:pg] $ bhist -l 3811Job &lt;3811&gt;, User &lt;lsfadmin&gt;, Project &lt;default&gt;, Command &lt;/usr/bin/stress --cpu                      4 --vm-bytes 8192MB --timeout 60s&gt;Mon Mar 18 19:42:22: Submitted from host &lt;icgen2host-XXX-YYY-ZZZ-37&gt;, to Queue &lt;no                     rmal&gt;, CWD &lt;$HOME&gt;, Output File &lt;/mnt/data/%J.out&gt;, 4 Task                     (s);Mon Mar 18 19:45:11: Dispatched 4 Task(s) on Host(s) &lt;icgen2host-XXX-YYY-ZZZ-44&gt; &lt;                     icgen2host-XXX-YYY-ZZZ-44&gt; &lt;icgen2host-XXX-YYY-ZZZ-44&gt; &lt;icgen2ho                     st-XXX-YYY-ZZZ-44&gt;, Allocated 4 Slot(s) on Host(s) &lt;icgen2hos                     t-XXX-YYY-ZZZ-44&gt; &lt;icgen2host-XXX-YYY-ZZZ-44&gt; &lt;icgen2host-XXX-YYY                     -ZZZ-44&gt; &lt;icgen2host-XXX-YYY-ZZZ-44&gt;, Effective RES_REQ &lt;select                     [type == local] order[r15s:pg] &gt;;Mon Mar 18 19:45:11: Starting (Pid 2169);Mon Mar 18 19:45:11: Running with execution home &lt;/home/lsfadmin&gt;, Execution CW                     D &lt;/home/lsfadmin&gt;, Execution Pid &lt;2169&gt;;Mon Mar 18 19:46:11: Done successfully. The CPU time used is 239.3 seconds;Mon Mar 18 19:46:12: Post job process done successfully;MEMORY USAGE:MAX MEM: 3 Mbytes;  AVG MEM: 2 Mbytes; MEM Efficiency: 0.00%CPU USAGE:CPU PEAK: 3.98 ;  CPU PEAK DURATION: 60 second(s)CPU AVERAGE EFFICIENCY: 99.58% ;  CPU PEAK EFFICIENCY: 99.58%Summary of time in seconds spent in various states by  Mon Mar 18 19:46:12  PEND     PSUSP    RUN      USUSP    SSUSP    UNKWN    TOTAL  169      0        60       0        0        0        229         </code></pre></div><p>Next, let’s submit the same job, but explicitly requesting the profile type <em>mx2-16x128</em>. To do this, we add the resource requirement requesting profile to be equal to <em>mx2_16x128</em> as follows:</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ bsub -q normal -R \"profile==mx2_16x128\" -n 4 -o /mnt/data/%J.out /usr/bin/stress --cpu 4 --vm-bytes 8192MB --timeout 60s Job &lt;3813&gt; is submitted to queue &lt;normal&gt;.</code></pre></div><p>After a few moments, a dynamic host with 16 CPUs and 128 GB RAM joins the cluster, which corresponds to instance type <em>mx2-16x128</em>.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ lshosts -wHOST_NAME                       type       model  cpuf ncpus maxmem maxswp server RESOURCESicgen2host-XXX-YYY-ZZZ-37        X86_64    Intel_E5  12.5     4  15.4G      -    Yes (mg docker ParaView)icgen2host-XXX-YYY-ZZZ-46        X86_64    Intel_E5  12.5    16 125.7G      -    Dyn (icgen2host docker)$ lsload -wHOST_NAME               status  r15s   r1m  r15m   ut    pg  ls    it   tmp   swp   memicgen2host-XXX-YYY-ZZZ-37      ok   0.0   0.1   0.1   3%   0.0   1     2   40G    0M 12.2Gicgen2host-XXX-YYY-ZZZ-46      ok   1.6   0.4   0.1  17%   0.0   0     0   88G    0M 123.8G$ bjobs -l -rJob &lt;3813&gt;, User &lt;lsfadmin&gt;, Project &lt;default&gt;, Status &lt;RUN&gt;, Queue &lt;normal&gt;, C                     ommand &lt;/usr/bin/stress --cpu 4 --vm-bytes 8192MB --timeou                     t 60s&gt;, Share group charged &lt;/lsfadmin&gt;Mon Mar 18 20:27:52: Submitted from host &lt;icgen2host-XXX-YYY-ZZZ-37&gt;, CWD &lt;$HOME&gt;,                      Output File &lt;/mnt/data/3813.out&gt;, 4 Task(s), Requested Re                     sources &lt;profile==mx2_16x128&gt;;Mon Mar 18 20:30:01: Started 4 Task(s) on Host(s) &lt;icgen2host-XXX-YYY-ZZZ-46&gt; &lt;icg                     en2host-XXX-YYY-ZZZ-46&gt; &lt;icgen2host-XXX-YYY-ZZZ-46&gt; &lt;icgen2host-                     XXX-YYY-ZZZ-46&gt;, Allocated 4 Slot(s) on Host(s) &lt;icgen2host                     -XXX-YYY-ZZZ-46&gt; &lt;icgen2host-XXX-YYY-ZZZ-46&gt; &lt;icgen2host-XXX-Y                     YY-ZZZ-46&gt; &lt;icgen2host-XXX-YYY-ZZZ-46&gt;, Execution Home &lt;/home/lsfadm                     in&gt;, Execution CWD &lt;/home/lsfadmin&gt;; MEMORY USAGE: MEM Efficiency: 0.00% CPU USAGE: CPU PEAK: 0.00 ;  CPU PEAK DURATION: 0 second(s) CPU AVERAGE EFFICIENCY: 0.00% ;  CPU PEAK EFFICIENCY: 0.00% SCHEDULING PARAMETERS:           r15s   r1m  r15m   ut      pg    io   ls    it    tmp    swp    mem loadSched   -     -     -     -       -     -    -     -     -      -      -   loadStop    -     -     -     -       -     -    -     -     -      -      -   RESOURCE REQUIREMENT DETAILS: Combined: select[(profile == mx2_16x128 ) &amp;&amp; (type == any)] order[r15s:pg] Effective: select[(profile == mx2_16x128 ) &amp;&amp; (type == any)] order[r15s:pg] </code></pre></div><p>Additionally, using the LSF lshosts command with the -s option, we can view information about the resources in the environment. We see in particular the profile resource is set to the value <em>mx2_16x128</em> for the dynamic host <em>icgen2host-XXX-YYY-ZZZ-46</em>.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">$ lshosts -sRESOURCE                                VALUE       LOCATIONrc_account                            default       icgen2host-XXX-YYY-ZZZ-46 profile                            mx2_16x128       icgen2host-XXX-YYY-ZZZ-46 instanceID               0757_95e39240-22e7-4734-8fd5-9988ab247801                                                      icgen2host-XXX-YYY-ZZZ-46 </code></pre></div><p>We have shown the great flexibility that LSF provides for configuring the resource connector capability. Generally speaking, LSF provides many open interfaces which allow site specific configuration or customization to be realized. In the next blog in this series, we’ll take a closer look at running Docker jobs under LSF on dynamic cloud resources.</p>",
            "url": "https://hpc.social/personal-blog/2024/advanced-lsf-resource-connector-configuration-on-ibm-cloud-part-ii/",
            
            
            
            
            
            "date_published": "2024-03-19T20:40:52-06:00",
            "date_modified": "2024-03-19T20:40:52-06:00",
            
                "author": "Ramblings of a supercomputing enthusiast."
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2024/pi-in-the-sky-a-compute-cluster-in-mini-itx-form-factor/",
            "title": "Pi in the sky? A compute cluster in mini ITX form factor",
            "summary": null,
            "content_text": "OverviewIt&rsquo;s taken me a while to get the wheels off the ground in 2024 in terms of blogging. This blog idea has been in the works actually for some time. Back in 2021, I wrote a blog titled Late to the party and a few bits short. This was a tongue in cheek title for a blog on the Novena Desktop System, which is based on a 32-bit processor, hence a few bits short. And late to the party referring to the fact that I was very late to purchase a second-hand Novena system.This blog is similar in that it&rsquo;s about the original Turing Pi V1 system which was released back in 2021 when the Turing Pi V2 launch was imminent. The Turing Pi V1 is a 7 node cluster in a mini-ITX form factor. It&rsquo;s based on the Raspberry Pi CM3(+) modules. This was really an impulse purchase the dark days of COVID. And as I found out, getting a hold of RPi CM3&rsquo;s was much harder than expected. As luck would have it, I even eventually found a source via an online marketplace here in Southern Ontario that was not charging and arm and a leg for them. I purchased a total of 7 CM3+ modules with no onboard storage and relied upon SD cards for storage. As (bad) luck would have it, I ended up having to purchase a CM3 with onboard storage because one of the SD card slots is defecting on the board; the spring mechanism doesn&rsquo;t work properly. And as we&rsquo;ll see later on, this also had an unusual side effect when running Linpack.I&rsquo;ve had the fully populated system for about 6 months now. And although the Turing Pi V1 is old news at this stage, I still wanted to write a bit about my experience with it. And of course, because it&rsquo;s a cluster, I definitely wanted to put it through it&rsquo;s paces running Linpack.The official Turing Pi V1 documentation was my goto for the system setup. The cluster was installed with the latest (at the time) Raspberry Pi OS (2023-02-21-raspios-bullseye-arm64-lite.img) based on Debian 11 (Bullseye).The following additional software packages were installed/compiled. Note that the head node of the cluster acts as an NFS server for the remaining cluster nodes (/opt).Arm Optimizing Compilers V22.0.2 (for Ubuntu-20.04)OpenMPI V4.1.5 (compiled with LSF support)IBM Spectrum LSF v10.1.0.13HPL V2.3 (compiled with Arm Optimizing Compilers)Here is the output of the LSF lshosts command. We see 6 CM3+ systems detected, and one CM3. Note that this required additional LSF configuration.lsfadmin@turingpi:/opt/HPC/hpl-2.3 $ lshosts -wHOST_NAME                       type       model  cpuf ncpus maxmem maxswp server RESOURCESturingpi                  LINUX_ARM64     CM3plus   6.0     4   910M   100M    Yes (mg)neumann                   LINUX_ARM64     CM3plus   6.0     4   910M   100M    Yes ()teller                    LINUX_ARM64     CM3plus   6.0     4   910M   100M    Yes ()szilard                   LINUX_ARM64     CM3plus   6.0     4   910M   100M    Yes ()wigner                    LINUX_ARM64     CM3plus   6.0     4   910M   100M    Yes ()kemeny                    LINUX_ARM64         CM3   5.0     4   910M   100M    Yes ()vonkarman                 LINUX_ARM64     CM3plus   6.0     4   910M   100M    Yes ()Those with a keen eye will note that the majority of the cluster nodes are named after Hungarian scientists:Neumann János (John von Neumann)Teller Ede (Edward Teller)Szilárd Leó (Leo Szilard)Wigner Jenő (Eugene Wigner)Kemény János (John Kemeny)Kármán Tódor (Theodore von Karman)The odd one out here is of course turingpi, which is the name of the head node of the cluster, and is of course named after Alan Turing. But I digress.For completeness, HPL V2.3 was compiled using the Arm Optimizing Compilers with the follwing flags:CCFLAGS      = $(HPL_DEFS) -Ofast -mcpu=native -fomit-frame-pointerLINKER       = armclang -armpl -lamath -lm -Ofast -mcpu=native -fomit-frame-pointerFor the first HPL run, we submit the job requesting a total of 24 cores. There are a total of 28 coresin the cluster, but we&rsquo;ve isolated the head node of the cluster as it&rsquo;s the NFS server for the environment. We see that the head node turingpi shows a closed status here, meaning that it won&rsquo;t accept any jobs from LSF.lsfadmin@turingpi:/opt/HPC/hpl-2.3/bin/cm3_3 $ bhostsHOST_NAME          STATUS       JL/U    MAX  NJOBS    RUN  SSUSP  USUSP    RSV kemeny             ok              -      4      0      0      0      0      0neumann            ok              -      4      0      0      0      0      0szilard            ok              -      4      0      0      0      0      0teller             ok              -      4      0      0      0      0      0turingpi           closed          -      4      0      0      0      0      0vonkarman          ok              -      4      0      0      0      0      0wigner             ok              -      4      0      0      0      0      0Turing up the heat - literallySubmit HPL using the LSF bsub command requesting 24 cores in the cluser with core affinity specified.lsfadmin@turingpi:/opt/HPC/hpl-2.3/bin/cm3_3 $ bsub -n 24 -R \"affinity[core(1)]\" -Is mpirun --mca btl_tcp_if_exclude lo,docker0 ./xhplJob &lt;41861&gt; is submitted to default queue &lt;interactive&gt;.&lt;&lt;Waiting for dispatch ...&gt;&gt;&lt;&lt;Starting on neumann&gt;&gt;================================================================================HPLinpack 2.3  --  High-Performance Linpack benchmark  --   December 2, 2018Written by A. Petitet and R. Clint Whaley,  Innovative Computing Laboratory, UTKModified by Piotr Luszczek, Innovative Computing Laboratory, UTKModified by Julien Langou, University of Colorado Denver================================================================================An explanation of the input/output parameters follows:T/V    : Wall time / encoded variant.N      : The order of the coefficient matrix A.NB     : The partitioning blocking factor.P      : The number of process rows.Q      : The number of process columns.Time   : Time in seconds to solve the linear system.Gflops : Rate of execution for solving the linear system.The following parameter values will be used:N      :   15968 NB     :      48       96      192 PMAP   : Row-major process mappingP      :       4        6 Q      :       6        4 PFACT  :   Right NBMIN  :       4 NDIV   :       2 RFACT  :   Crout BCAST  :  1ringM DEPTH  :       1 SWAP   : Mix (threshold = 64)L1     : transposed formU      : transposed formEQUIL  : yesALIGN  : 8 double precision words--------------------------------------------------------------------------------- The matrix A is randomly generated for each test.- The following scaled residual check will be computed:      ||Ax-b||_oo / ( eps * ( || x ||_oo * || A ||_oo + || b ||_oo ) * N )- The relative machine precision (eps) is taken to be               1.110223e-16- Computational tests pass if scaled residuals are less than                16.0--------------------------------------------------------------------------Primary job  terminated normally, but 1 process returneda non-zero exit code. Per user-direction, the job has been aborted.----------------------------------------------------------------------------------------------------------------------------------------------------An MPI communication peer process has unexpectedly disconnected.  Thisusually indicates a failure in the peer process (e.g., a crash orotherwise exiting without calling MPI_FINALIZE first).Although this local MPI process will likely now behave unpredictably(it may even hang or crash), the root cause of this problem is thefailure of the peer -- that is what you need to investigate.  Forexample, there may be a core file that you can examine.  Moregenerally: such peer hangups are frequently caused by application bugsor other external events.  Local host: teller  Local PID:  2253  Peer host:  kemeny----------------------------------------------------------------------------------------------------------------------------------------------------mpirun noticed that process rank 23 with PID 2448 on node kemeny exited on signal 4 (Illegal instruction).--------------------------------------------------------------------------We see above that the MPI rank(s) fail on host kemeny, which happens to be the CM3 module (not CM3+). Even though I compiled HPL natively on kemeny this issue persists. So ultimately, the HPL run was limited to the 5 remaining CM3+ nodes (i.e. 20 cores).Next, we submit HPL requesting 20 cores (all on CM3+ modules). Core affinity is specified, and we request specifically the model type &ldquo;CM3plus&rdquo;. The job was submitted interactively and the output follows:lsfadmin@turingpi:/opt/HPC/hpl-2.3/bin/cm3_3 $ bsub -n 20 -Is -R \"select[model==CM3plus] affinity[core(1)]\" mpirun --mca btl_tcp_if_exclude lo,docker0 ./xhplJob &lt;41865&gt; is submitted to default queue &lt;interactive&gt;.&lt;&lt;Waiting for dispatch ...&gt;&gt;&lt;&lt;Starting on vonkarman&gt;&gt;================================================================================HPLinpack 2.3  --  High-Performance Linpack benchmark  --   December 2, 2018Written by A. Petitet and R. Clint Whaley,  Innovative Computing Laboratory, UTKModified by Piotr Luszczek, Innovative Computing Laboratory, UTKModified by Julien Langou, University of Colorado Denver================================================================================An explanation of the input/output parameters follows:T/V    : Wall time / encoded variant.N      : The order of the coefficient matrix A.NB     : The partitioning blocking factor.P      : The number of process rows.Q      : The number of process columns.Time   : Time in seconds to solve the linear system.Gflops : Rate of execution for solving the linear system.The following parameter values will be used:N      :   15968 NB     :      48       96      192 PMAP   : Row-major process mappingP      :       4        5 Q      :       5        4 PFACT  :   Right NBMIN  :       4 NDIV   :       2 RFACT  :   Crout BCAST  :  1ringM DEPTH  :       1 SWAP   : Mix (threshold = 64)L1     : transposed formU      : transposed formEQUIL  : yesALIGN  : 8 double precision words--------------------------------------------------------------------------------- The matrix A is randomly generated for each test.- The following scaled residual check will be computed:      ||Ax-b||_oo / ( eps * ( || x ||_oo * || A ||_oo + || b ||_oo ) * N )- The relative machine precision (eps) is taken to be               1.110223e-16- Computational tests pass if scaled residuals are less than                16.0================================================================================T/V                N    NB     P     Q               Time                 Gflops--------------------------------------------------------------------------------WR11C2R4       15968    48     4     5             327.96             8.2776e+00HPL_pdgesv() start time Sun Mar  3 20:29:45 2024HPL_pdgesv() end time   Sun Mar  3 20:35:13 2024--------------------------------------------------------------------------------||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   2.74851526e-03 ...... PASSED================================================================================T/V                N    NB     P     Q               Time                 Gflops--------------------------------------------------------------------------------WR11C2R4       15968    96     4     5             315.71             8.5987e+00HPL_pdgesv() start time Sun Mar  3 20:35:18 2024HPL_pdgesv() end time   Sun Mar  3 20:40:34 2024--------------------------------------------------------------------------------||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   3.82600703e-03 ...... PASSED================================================================================T/V                N    NB     P     Q               Time                 Gflops--------------------------------------------------------------------------------WR11C2R4       15968   192     4     5             319.93             8.4854e+00HPL_pdgesv() start time Sun Mar  3 20:40:38 2024HPL_pdgesv() end time   Sun Mar  3 20:45:58 2024--------------------------------------------------------------------------------||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   2.56990081e-03 ...... PASSED================================================================================T/V                N    NB     P     Q               Time                 Gflops--------------------------------------------------------------------------------WR11C2R4       15968    48     5     4             342.36             7.9293e+00HPL_pdgesv() start time Sun Mar  3 20:46:03 2024HPL_pdgesv() end time   Sun Mar  3 20:51:45 2024--------------------------------------------------------------------------------||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   2.89956630e-03 ...... PASSED================================================================================T/V                N    NB     P     Q               Time                 Gflops--------------------------------------------------------------------------------WR11C2R4       15968    96     5     4             313.72             8.6531e+00HPL_pdgesv() start time Sun Mar  3 20:51:50 2024HPL_pdgesv() end time   Sun Mar  3 20:57:04 2024--------------------------------------------------------------------------------||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   3.04113830e-03 ...... PASSED================================================================================T/V                N    NB     P     Q               Time                 Gflops--------------------------------------------------------------------------------WR11C2R4       15968   192     5     4             312.48             8.6877e+00HPL_pdgesv() start time Sun Mar  3 20:57:08 2024HPL_pdgesv() end time   Sun Mar  3 21:02:21 2024--------------------------------------------------------------------------------||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   3.30812017e-03 ...... PASSED================================================================================Finished      6 tests with the following results:              6 tests completed and passed residual checks,              0 tests completed and failed residual checks,              0 tests skipped because of illegal input values.--------------------------------------------------------------------------------End of Tests.================================================================================We oberved during the HPL run that the CPU temperatures exceeded 80 degrees Celsius:root@turingpi:/home/lsfadmin# parallel-ssh -h /opt/workers -i \"/opt/tools/cputemp.sh\"[1] 20:47:30 [SUCCESS] kemenyCurrent CPU temperature is 61.22 degrees Celsius.[2] 20:47:30 [SUCCESS] tellerCurrent CPU temperature is 82.21 degrees Celsius.[3] 20:47:30 [SUCCESS] wignerCurrent CPU temperature is 82.74 degrees Celsius.[4] 20:47:31 [SUCCESS] szilardCurrent CPU temperature is 82.21 degrees Celsius.[5] 20:47:31 [SUCCESS] neumannCurrent CPU temperature is 82.74 degrees Celsius.[6] 20:47:31 [SUCCESS] vonkarmanCurrent CPU temperature is 83.28 degrees Celsius.root@turingpi:/home/lsfadmin# parallel-ssh -h /opt/workers -i \"/usr/bin/vcgencmd measure_clock arm\" [1] 20:47:42 [SUCCESS] kemenyfrequency(48)=1199998000[2] 20:47:43 [SUCCESS] szilardfrequency(48)=1034000000[3] 20:47:43 [SUCCESS] tellerfrequency(48)=980000000[4] 20:47:44 [SUCCESS] wignerfrequency(48)=926000000[5] 20:47:44 [SUCCESS] neumannfrequency(48)=818000000[6] 20:47:44 [SUCCESS] vonkarmanfrequency(48)=872000000And of course with high temperatures come CPU throttling. Clearly, with this thermal situation the runof HPL was not going to be optimal.Giant Tiger to the rescueEven for those in Canada this may see like a very strange reference. Giant Tiger is a discount store chain which sell everything from A through Z. Unfortunately the local &ldquo;GT Boutique&rdquo; as call it closed down this past January. I happened to purchase on a whim a USB powered desktop fan at the GT Boutique about a year ago. The idea was to help keep me cool at my keyboard during the hot summer days. But in thiscase, it was just what was needed to provide a bit of active cooling to the Turing Pi system.Repeating the run of HPL with the &ldquo;highly advanced active cooling&rdquo; measures in place, we were able to up the HPL results a tad while helping to preserve the life of the cluster nodes. And the results show going from 8.65 GFlops with passive cooling to 9.5 GFlops with the active cooling.lsfadmin@turingpi:/opt/HPC/hpl-2.3/bin/cm3_3 $ bsub -n 20 -Is -R \"select[model==CM3plus] affinity[core(1)]\" mpirun --mca btl_tcp_if_exclude lo,docker0 ./xhplJob &lt;41866&gt; is submitted to default queue &lt;interactive&gt;.&lt;&lt;Waiting for dispatch ...&gt;&gt;&lt;&lt;Starting on teller&gt;&gt;================================================================================HPLinpack 2.3  --  High-Performance Linpack benchmark  --   December 2, 2018Written by A. Petitet and R. Clint Whaley,  Innovative Computing Laboratory, UTKModified by Piotr Luszczek, Innovative Computing Laboratory, UTKModified by Julien Langou, University of Colorado Denver================================================================================An explanation of the input/output parameters follows:T/V    : Wall time / encoded variant.N      : The order of the coefficient matrix A.NB     : The partitioning blocking factor.P      : The number of process rows.Q      : The number of process columns.Time   : Time in seconds to solve the linear system.Gflops : Rate of execution for solving the linear system.The following parameter values will be used:N      :   15968 NB     :      48       96      192 PMAP   : Row-major process mappingP      :       4        5 Q      :       5        4 PFACT  :   Right NBMIN  :       4 NDIV   :       2 RFACT  :   Crout BCAST  :  1ringM DEPTH  :       1 SWAP   : Mix (threshold = 64)L1     : transposed formU      : transposed formEQUIL  : yesALIGN  : 8 double precision words--------------------------------------------------------------------------------- The matrix A is randomly generated for each test.- The following scaled residual check will be computed:      ||Ax-b||_oo / ( eps * ( || x ||_oo * || A ||_oo + || b ||_oo ) * N )- The relative machine precision (eps) is taken to be               1.110223e-16- Computational tests pass if scaled residuals are less than                16.0================================================================================T/V                N    NB     P     Q               Time                 Gflops--------------------------------------------------------------------------------WR11C2R4       15968    48     4     5             319.43             8.4985e+00HPL_pdgesv() start time Sun Mar  3 21:15:42 2024HPL_pdgesv() end time   Sun Mar  3 21:21:01 2024--------------------------------------------------------------------------------||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   2.74851526e-03 ...... PASSED================================================================================T/V                N    NB     P     Q               Time                 Gflops--------------------------------------------------------------------------------WR11C2R4       15968    96     4     5             296.94             9.1423e+00HPL_pdgesv() start time Sun Mar  3 21:21:05 2024HPL_pdgesv() end time   Sun Mar  3 21:26:02 2024--------------------------------------------------------------------------------||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   3.82600703e-03 ...... PASSED================================================================================T/V                N    NB     P     Q               Time                 Gflops--------------------------------------------------------------------------------WR11C2R4       15968   192     4     5             289.03             9.3926e+00HPL_pdgesv() start time Sun Mar  3 21:26:06 2024HPL_pdgesv() end time   Sun Mar  3 21:30:55 2024--------------------------------------------------------------------------------||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   2.56990081e-03 ...... PASSED================================================================================T/V                N    NB     P     Q               Time                 Gflops--------------------------------------------------------------------------------WR11C2R4       15968    48     5     4             316.20             8.5855e+00HPL_pdgesv() start time Sun Mar  3 21:30:59 2024HPL_pdgesv() end time   Sun Mar  3 21:36:15 2024--------------------------------------------------------------------------------||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   2.89956630e-03 ...... PASSED================================================================================T/V                N    NB     P     Q               Time                 Gflops--------------------------------------------------------------------------------WR11C2R4       15968    96     5     4             285.87             9.4961e+00HPL_pdgesv() start time Sun Mar  3 21:36:19 2024HPL_pdgesv() end time   Sun Mar  3 21:41:05 2024--------------------------------------------------------------------------------||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   3.04113830e-03 ...... PASSED================================================================================T/V                N    NB     P     Q               Time                 Gflops--------------------------------------------------------------------------------WR11C2R4       15968   192     5     4             284.69             9.5355e+00HPL_pdgesv() start time Sun Mar  3 21:41:09 2024HPL_pdgesv() end time   Sun Mar  3 21:45:53 2024--------------------------------------------------------------------------------||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   3.30812017e-03 ...... PASSED================================================================================Finished      6 tests with the following results:              6 tests completed and passed residual checks,              0 tests completed and failed residual checks,              0 tests skipped because of illegal input values.--------------------------------------------------------------------------------End of Tests.================================================================================And during the runtime, we see that no throttling occurrred and the CPU temperatures hovered in the high 50&rsquo;s to low 60 degree Celsius range.root@turingpi:/home/lsfadmin# parallel-ssh -h /opt/workers -i \"/opt/tools/cputemp.sh\"[1] 21:41:25 [SUCCESS] kemenyCurrent CPU temperature is 36.48 degrees Celsius.[2] 21:41:25 [SUCCESS] tellerCurrent CPU temperature is 58.53 degrees Celsius.[3] 21:41:25 [SUCCESS] vonkarmanCurrent CPU temperature is 58.00 degrees Celsius.[4] 21:41:25 [SUCCESS] neumannCurrent CPU temperature is 55.84 degrees Celsius.[5] 21:41:25 [SUCCESS] szilardCurrent CPU temperature is 61.76 degrees Celsius.[6] 21:41:25 [SUCCESS] wignerCurrent CPU temperature is 55.31 degrees Celsius.root@turingpi:/home/lsfadmin# parallel-ssh -h /opt/workers -i \"/usr/bin/vcgencmd measure_clock arm\" [1] 21:41:29 [SUCCESS] kemenyfrequency(48)=1200000000[2] 21:41:29 [SUCCESS] tellerfrequency(48)=1200000000[3] 21:41:29 [SUCCESS] vonkarmanfrequency(48)=1200000000[4] 21:41:29 [SUCCESS] wignerfrequency(48)=1200000000[5] 21:41:29 [SUCCESS] neumannfrequency(48)=1200000000[6] 21:41:29 [SUCCESS] szilardfrequency(48)=1200002000Wrap upI always liked the idea of a small cluster that you could easily take with you. That&rsquo;s why I&rsquo;m stronglyconsidering the Turing Pi V2.5, which can work with the much more powerful CM4 omdules, among other very capable modules. Budget allowing, I hope to purchase a Turing Pi V2.5 sometime in 2024. As always stay tuned for more exciting high performance computing tales. And at the end of the day, a compute cluster in a mini ITX format isn&rsquo;t a pie in the sky idea. For me, it&rsquo;s a great tool for learning!",
            "content_html": "<p><strong>Overview</strong></p><p>It&rsquo;s taken me a while to get the wheels off the ground in 2024 in terms of blogging. This blog idea has been in the works actually for some time. Back in 2021, I wrote a blog titled <a href=\"https://www.gaborsamu.com/blog/new_novena/\">Late to the party and a few bits short</a>. This was a tongue in cheek title for a blog on the Novena Desktop System, which is based on a 32-bit processor, hence a few bits short. And late to the party referring to the fact that I was very late to purchase a second-hand Novena system.</p><p>This blog is similar in that it&rsquo;s about the original Turing Pi V1 system which was released back in 2021 when the Turing Pi V2 launch was imminent. The Turing Pi V1 is a 7 node cluster in a mini-ITX form factor. It&rsquo;s based on the Raspberry Pi CM3(+) modules. This was really an impulse purchase the dark days of COVID. And as I found out, getting a hold of RPi CM3&rsquo;s was much harder than expected. As luck would have it, I even eventually found a source via an online marketplace here in Southern Ontario that was not charging and arm and a leg for them. I purchased a total of 7 CM3+ modules with no onboard storage and relied upon SD cards for storage. As (bad) luck would have it, I ended up having to purchase a CM3 with onboard storage because one of the SD card slots is defecting on the board; the spring mechanism doesn&rsquo;t work properly. And as we&rsquo;ll see later on, this also had an unusual side effect when running Linpack.</p><p>I&rsquo;ve had the fully populated system for about 6 months now. And although the Turing Pi V1 is old news at this stage, I still wanted to write a bit about my experience with it. And of course, because it&rsquo;s a cluster, I definitely wanted to put it through it&rsquo;s paces running Linpack.</p><p>The official Turing Pi V1 <a href=\"https://docs.turingpi.com/docs/turing-pi1-intro-specs\">documentation</a> was my goto for the system setup. The cluster was installed with the latest (at the time) Raspberry Pi OS (<em>2023-02-21-raspios-bullseye-arm64-lite.img</em>) based on Debian 11 (Bullseye).</p><p>The following additional software packages were installed/compiled. Note that the head node of the cluster acts as an NFS server for the remaining cluster nodes (<em>/opt</em>).</p><ul><li>Arm Optimizing Compilers V22.0.2 (for Ubuntu-20.04)</li><li>OpenMPI V4.1.5 (compiled with LSF support)</li><li>IBM Spectrum LSF v10.1.0.13</li><li>HPL V2.3 (compiled with Arm Optimizing Compilers)</li></ul><p>Here is the output of the LSF lshosts command. We see 6 CM3+ systems detected, and one CM3. Note that this required additional LSF configuration.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">lsfadmin@turingpi:/opt/HPC/hpl-2.3 $ lshosts -wHOST_NAME                       type       model  cpuf ncpus maxmem maxswp server RESOURCESturingpi                  LINUX_ARM64     CM3plus   6.0     4   910M   100M    Yes (mg)neumann                   LINUX_ARM64     CM3plus   6.0     4   910M   100M    Yes ()teller                    LINUX_ARM64     CM3plus   6.0     4   910M   100M    Yes ()szilard                   LINUX_ARM64     CM3plus   6.0     4   910M   100M    Yes ()wigner                    LINUX_ARM64     CM3plus   6.0     4   910M   100M    Yes ()kemeny                    LINUX_ARM64         CM3   5.0     4   910M   100M    Yes ()vonkarman                 LINUX_ARM64     CM3plus   6.0     4   910M   100M    Yes ()</code></pre></div><p>Those with a keen eye will note that the majority of the cluster nodes are named after Hungarian scientists:</p><ul><li><em>Neumann János</em> (John von Neumann)</li><li><em>Teller Ede</em> (Edward Teller)</li><li><em>Szilárd Leó</em> (Leo Szilard)</li><li><em>Wigner Jenő</em> (Eugene Wigner)</li><li><em>Kemény János</em> (John Kemeny)</li><li><em>Kármán Tódor</em> (Theodore von Karman)</li></ul><p>The odd one out here is of course turingpi, which is the name of the head node of the cluster, and is of course named after Alan Turing. But I digress.</p><p>For completeness, HPL V2.3 was compiled using the Arm Optimizing Compilers with the follwing flags:</p><ul><li>CCFLAGS      = $(HPL_DEFS) -Ofast -mcpu=native -fomit-frame-pointer</li><li>LINKER       = armclang -armpl -lamath -lm -Ofast -mcpu=native -fomit-frame-pointer</li></ul><p>For the first HPL run, we submit the job requesting a total of 24 cores. There are a total of 28 coresin the cluster, but we&rsquo;ve isolated the head node of the cluster as it&rsquo;s the NFS server for the environment. We see that the head node turingpi shows a closed status here, meaning that it won&rsquo;t accept any jobs from LSF.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">lsfadmin@turingpi:/opt/HPC/hpl-2.3/bin/cm3_3 $ bhostsHOST_NAME          STATUS       JL/U    MAX  NJOBS    RUN  SSUSP  USUSP    RSV kemeny             ok              -      4      0      0      0      0      0neumann            ok              -      4      0      0      0      0      0szilard            ok              -      4      0      0      0      0      0teller             ok              -      4      0      0      0      0      0turingpi           closed          -      4      0      0      0      0      0vonkarman          ok              -      4      0      0      0      0      0wigner             ok              -      4      0      0      0      0      0</code></pre></div><p><strong>Turing up the heat - literally</strong></p><p>Submit HPL using the LSF <em>bsub</em> command requesting 24 cores in the cluser with core affinity specified.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">lsfadmin@turingpi:/opt/HPC/hpl-2.3/bin/cm3_3 $ bsub -n 24 -R \"affinity[core(1)]\" -Is mpirun --mca btl_tcp_if_exclude lo,docker0 ./xhplJob &lt;41861&gt; is submitted to default queue &lt;interactive&gt;.&lt;&lt;Waiting for dispatch ...&gt;&gt;&lt;&lt;Starting on neumann&gt;&gt;================================================================================HPLinpack 2.3  --  High-Performance Linpack benchmark  --   December 2, 2018Written by A. Petitet and R. Clint Whaley,  Innovative Computing Laboratory, UTKModified by Piotr Luszczek, Innovative Computing Laboratory, UTKModified by Julien Langou, University of Colorado Denver================================================================================An explanation of the input/output parameters follows:T/V    : Wall time / encoded variant.N      : The order of the coefficient matrix A.NB     : The partitioning blocking factor.P      : The number of process rows.Q      : The number of process columns.Time   : Time in seconds to solve the linear system.Gflops : Rate of execution for solving the linear system.The following parameter values will be used:N      :   15968 NB     :      48       96      192 PMAP   : Row-major process mappingP      :       4        6 Q      :       6        4 PFACT  :   Right NBMIN  :       4 NDIV   :       2 RFACT  :   Crout BCAST  :  1ringM DEPTH  :       1 SWAP   : Mix (threshold = 64)L1     : transposed formU      : transposed formEQUIL  : yesALIGN  : 8 double precision words--------------------------------------------------------------------------------- The matrix A is randomly generated for each test.- The following scaled residual check will be computed:      ||Ax-b||_oo / ( eps * ( || x ||_oo * || A ||_oo + || b ||_oo ) * N )- The relative machine precision (eps) is taken to be               1.110223e-16- Computational tests pass if scaled residuals are less than                16.0--------------------------------------------------------------------------Primary job  terminated normally, but 1 process returneda non-zero exit code. Per user-direction, the job has been aborted.----------------------------------------------------------------------------------------------------------------------------------------------------An MPI communication peer process has unexpectedly disconnected.  Thisusually indicates a failure in the peer process (e.g., a crash orotherwise exiting without calling MPI_FINALIZE first).Although this local MPI process will likely now behave unpredictably(it may even hang or crash), the root cause of this problem is thefailure of the peer -- that is what you need to investigate.  Forexample, there may be a core file that you can examine.  Moregenerally: such peer hangups are frequently caused by application bugsor other external events.  Local host: teller  Local PID:  2253  Peer host:  kemeny----------------------------------------------------------------------------------------------------------------------------------------------------mpirun noticed that process rank 23 with PID 2448 on node kemeny exited on signal 4 (Illegal instruction).--------------------------------------------------------------------------</code></pre></div><p>We see above that the MPI rank(s) fail on host kemeny, which happens to be the CM3 module (not CM3+). Even though I compiled HPL natively on kemeny this issue persists. So ultimately, the HPL run was limited to the 5 remaining CM3+ nodes (i.e. 20 cores).</p><p>Next, we submit HPL requesting 20 cores (all on CM3+ modules). Core affinity is specified, and we request specifically the model type &ldquo;CM3plus&rdquo;. The job was submitted interactively and the output follows:</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">lsfadmin@turingpi:/opt/HPC/hpl-2.3/bin/cm3_3 $ bsub -n 20 -Is -R \"select[model==CM3plus] affinity[core(1)]\" mpirun --mca btl_tcp_if_exclude lo,docker0 ./xhplJob &lt;41865&gt; is submitted to default queue &lt;interactive&gt;.&lt;&lt;Waiting for dispatch ...&gt;&gt;&lt;&lt;Starting on vonkarman&gt;&gt;================================================================================HPLinpack 2.3  --  High-Performance Linpack benchmark  --   December 2, 2018Written by A. Petitet and R. Clint Whaley,  Innovative Computing Laboratory, UTKModified by Piotr Luszczek, Innovative Computing Laboratory, UTKModified by Julien Langou, University of Colorado Denver================================================================================An explanation of the input/output parameters follows:T/V    : Wall time / encoded variant.N      : The order of the coefficient matrix A.NB     : The partitioning blocking factor.P      : The number of process rows.Q      : The number of process columns.Time   : Time in seconds to solve the linear system.Gflops : Rate of execution for solving the linear system.The following parameter values will be used:N      :   15968 NB     :      48       96      192 PMAP   : Row-major process mappingP      :       4        5 Q      :       5        4 PFACT  :   Right NBMIN  :       4 NDIV   :       2 RFACT  :   Crout BCAST  :  1ringM DEPTH  :       1 SWAP   : Mix (threshold = 64)L1     : transposed formU      : transposed formEQUIL  : yesALIGN  : 8 double precision words--------------------------------------------------------------------------------- The matrix A is randomly generated for each test.- The following scaled residual check will be computed:      ||Ax-b||_oo / ( eps * ( || x ||_oo * || A ||_oo + || b ||_oo ) * N )- The relative machine precision (eps) is taken to be               1.110223e-16- Computational tests pass if scaled residuals are less than                16.0================================================================================T/V                N    NB     P     Q               Time                 Gflops--------------------------------------------------------------------------------WR11C2R4       15968    48     4     5             327.96             8.2776e+00HPL_pdgesv() start time Sun Mar  3 20:29:45 2024HPL_pdgesv() end time   Sun Mar  3 20:35:13 2024--------------------------------------------------------------------------------||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   2.74851526e-03 ...... PASSED================================================================================T/V                N    NB     P     Q               Time                 Gflops--------------------------------------------------------------------------------WR11C2R4       15968    96     4     5             315.71             8.5987e+00HPL_pdgesv() start time Sun Mar  3 20:35:18 2024HPL_pdgesv() end time   Sun Mar  3 20:40:34 2024--------------------------------------------------------------------------------||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   3.82600703e-03 ...... PASSED================================================================================T/V                N    NB     P     Q               Time                 Gflops--------------------------------------------------------------------------------WR11C2R4       15968   192     4     5             319.93             8.4854e+00HPL_pdgesv() start time Sun Mar  3 20:40:38 2024HPL_pdgesv() end time   Sun Mar  3 20:45:58 2024--------------------------------------------------------------------------------||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   2.56990081e-03 ...... PASSED================================================================================T/V                N    NB     P     Q               Time                 Gflops--------------------------------------------------------------------------------WR11C2R4       15968    48     5     4             342.36             7.9293e+00HPL_pdgesv() start time Sun Mar  3 20:46:03 2024HPL_pdgesv() end time   Sun Mar  3 20:51:45 2024--------------------------------------------------------------------------------||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   2.89956630e-03 ...... PASSED================================================================================T/V                N    NB     P     Q               Time                 Gflops--------------------------------------------------------------------------------WR11C2R4       15968    96     5     4             313.72             8.6531e+00HPL_pdgesv() start time Sun Mar  3 20:51:50 2024HPL_pdgesv() end time   Sun Mar  3 20:57:04 2024--------------------------------------------------------------------------------||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   3.04113830e-03 ...... PASSED================================================================================T/V                N    NB     P     Q               Time                 Gflops--------------------------------------------------------------------------------WR11C2R4       15968   192     5     4             312.48             8.6877e+00HPL_pdgesv() start time Sun Mar  3 20:57:08 2024HPL_pdgesv() end time   Sun Mar  3 21:02:21 2024--------------------------------------------------------------------------------||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   3.30812017e-03 ...... PASSED================================================================================Finished      6 tests with the following results:              6 tests completed and passed residual checks,              0 tests completed and failed residual checks,              0 tests skipped because of illegal input values.--------------------------------------------------------------------------------End of Tests.================================================================================</code></pre></div><p>We oberved during the HPL run that the CPU temperatures exceeded 80 degrees Celsius:</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">root@turingpi:/home/lsfadmin# parallel-ssh -h /opt/workers -i \"/opt/tools/cputemp.sh\"[1] 20:47:30 [SUCCESS] kemenyCurrent CPU temperature is 61.22 degrees Celsius.[2] 20:47:30 [SUCCESS] tellerCurrent CPU temperature is 82.21 degrees Celsius.[3] 20:47:30 [SUCCESS] wignerCurrent CPU temperature is 82.74 degrees Celsius.[4] 20:47:31 [SUCCESS] szilardCurrent CPU temperature is 82.21 degrees Celsius.[5] 20:47:31 [SUCCESS] neumannCurrent CPU temperature is 82.74 degrees Celsius.[6] 20:47:31 [SUCCESS] vonkarmanCurrent CPU temperature is 83.28 degrees Celsius.root@turingpi:/home/lsfadmin# parallel-ssh -h /opt/workers -i \"/usr/bin/vcgencmd measure_clock arm\" [1] 20:47:42 [SUCCESS] kemenyfrequency(48)=1199998000[2] 20:47:43 [SUCCESS] szilardfrequency(48)=1034000000[3] 20:47:43 [SUCCESS] tellerfrequency(48)=980000000[4] 20:47:44 [SUCCESS] wignerfrequency(48)=926000000[5] 20:47:44 [SUCCESS] neumannfrequency(48)=818000000[6] 20:47:44 [SUCCESS] vonkarmanfrequency(48)=872000000</code></pre></div><p>And of course with high temperatures come CPU throttling. Clearly, with this thermal situation the runof HPL was not going to be optimal.</p><p><strong>Giant Tiger to the rescue</strong></p><p>Even for those in Canada this may see like a very strange reference. <a href=\"https://www.gianttiger.com/\">Giant Tiger</a> is a discount store chain which sell everything from A through Z. Unfortunately the local &ldquo;GT Boutique&rdquo; as call it closed down this past January. I happened to purchase on a whim a USB powered desktop fan at the GT Boutique about a year ago. The idea was to help keep me cool at my keyboard during the hot summer days. But in thiscase, it was just what was needed to provide a bit of active cooling to the Turing Pi system.</p><p>Repeating the run of HPL with the &ldquo;highly advanced active cooling&rdquo; measures in place, we were able to up the HPL results a tad while helping to preserve the life of the cluster nodes. And the results show going from 8.65 GFlops with passive cooling to 9.5 GFlops with the active cooling.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">lsfadmin@turingpi:/opt/HPC/hpl-2.3/bin/cm3_3 $ bsub -n 20 -Is -R \"select[model==CM3plus] affinity[core(1)]\" mpirun --mca btl_tcp_if_exclude lo,docker0 ./xhplJob &lt;41866&gt; is submitted to default queue &lt;interactive&gt;.&lt;&lt;Waiting for dispatch ...&gt;&gt;&lt;&lt;Starting on teller&gt;&gt;================================================================================HPLinpack 2.3  --  High-Performance Linpack benchmark  --   December 2, 2018Written by A. Petitet and R. Clint Whaley,  Innovative Computing Laboratory, UTKModified by Piotr Luszczek, Innovative Computing Laboratory, UTKModified by Julien Langou, University of Colorado Denver================================================================================An explanation of the input/output parameters follows:T/V    : Wall time / encoded variant.N      : The order of the coefficient matrix A.NB     : The partitioning blocking factor.P      : The number of process rows.Q      : The number of process columns.Time   : Time in seconds to solve the linear system.Gflops : Rate of execution for solving the linear system.The following parameter values will be used:N      :   15968 NB     :      48       96      192 PMAP   : Row-major process mappingP      :       4        5 Q      :       5        4 PFACT  :   Right NBMIN  :       4 NDIV   :       2 RFACT  :   Crout BCAST  :  1ringM DEPTH  :       1 SWAP   : Mix (threshold = 64)L1     : transposed formU      : transposed formEQUIL  : yesALIGN  : 8 double precision words--------------------------------------------------------------------------------- The matrix A is randomly generated for each test.- The following scaled residual check will be computed:      ||Ax-b||_oo / ( eps * ( || x ||_oo * || A ||_oo + || b ||_oo ) * N )- The relative machine precision (eps) is taken to be               1.110223e-16- Computational tests pass if scaled residuals are less than                16.0================================================================================T/V                N    NB     P     Q               Time                 Gflops--------------------------------------------------------------------------------WR11C2R4       15968    48     4     5             319.43             8.4985e+00HPL_pdgesv() start time Sun Mar  3 21:15:42 2024HPL_pdgesv() end time   Sun Mar  3 21:21:01 2024--------------------------------------------------------------------------------||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   2.74851526e-03 ...... PASSED================================================================================T/V                N    NB     P     Q               Time                 Gflops--------------------------------------------------------------------------------WR11C2R4       15968    96     4     5             296.94             9.1423e+00HPL_pdgesv() start time Sun Mar  3 21:21:05 2024HPL_pdgesv() end time   Sun Mar  3 21:26:02 2024--------------------------------------------------------------------------------||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   3.82600703e-03 ...... PASSED================================================================================T/V                N    NB     P     Q               Time                 Gflops--------------------------------------------------------------------------------WR11C2R4       15968   192     4     5             289.03             9.3926e+00HPL_pdgesv() start time Sun Mar  3 21:26:06 2024HPL_pdgesv() end time   Sun Mar  3 21:30:55 2024--------------------------------------------------------------------------------||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   2.56990081e-03 ...... PASSED================================================================================T/V                N    NB     P     Q               Time                 Gflops--------------------------------------------------------------------------------WR11C2R4       15968    48     5     4             316.20             8.5855e+00HPL_pdgesv() start time Sun Mar  3 21:30:59 2024HPL_pdgesv() end time   Sun Mar  3 21:36:15 2024--------------------------------------------------------------------------------||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   2.89956630e-03 ...... PASSED================================================================================T/V                N    NB     P     Q               Time                 Gflops--------------------------------------------------------------------------------WR11C2R4       15968    96     5     4             285.87             9.4961e+00HPL_pdgesv() start time Sun Mar  3 21:36:19 2024HPL_pdgesv() end time   Sun Mar  3 21:41:05 2024--------------------------------------------------------------------------------||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   3.04113830e-03 ...... PASSED================================================================================T/V                N    NB     P     Q               Time                 Gflops--------------------------------------------------------------------------------WR11C2R4       15968   192     5     4             284.69             9.5355e+00HPL_pdgesv() start time Sun Mar  3 21:41:09 2024HPL_pdgesv() end time   Sun Mar  3 21:45:53 2024--------------------------------------------------------------------------------||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   3.30812017e-03 ...... PASSED================================================================================Finished      6 tests with the following results:              6 tests completed and passed residual checks,              0 tests completed and failed residual checks,              0 tests skipped because of illegal input values.--------------------------------------------------------------------------------End of Tests.================================================================================</code></pre></div><p>And during the runtime, we see that no throttling occurrred and the CPU temperatures hovered in the high 50&rsquo;s to low 60 degree Celsius range.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">root@turingpi:/home/lsfadmin# parallel-ssh -h /opt/workers -i \"/opt/tools/cputemp.sh\"[1] 21:41:25 [SUCCESS] kemenyCurrent CPU temperature is 36.48 degrees Celsius.[2] 21:41:25 [SUCCESS] tellerCurrent CPU temperature is 58.53 degrees Celsius.[3] 21:41:25 [SUCCESS] vonkarmanCurrent CPU temperature is 58.00 degrees Celsius.[4] 21:41:25 [SUCCESS] neumannCurrent CPU temperature is 55.84 degrees Celsius.[5] 21:41:25 [SUCCESS] szilardCurrent CPU temperature is 61.76 degrees Celsius.[6] 21:41:25 [SUCCESS] wignerCurrent CPU temperature is 55.31 degrees Celsius.root@turingpi:/home/lsfadmin# parallel-ssh -h /opt/workers -i \"/usr/bin/vcgencmd measure_clock arm\" [1] 21:41:29 [SUCCESS] kemenyfrequency(48)=1200000000[2] 21:41:29 [SUCCESS] tellerfrequency(48)=1200000000[3] 21:41:29 [SUCCESS] vonkarmanfrequency(48)=1200000000[4] 21:41:29 [SUCCESS] wignerfrequency(48)=1200000000[5] 21:41:29 [SUCCESS] neumannfrequency(48)=1200000000[6] 21:41:29 [SUCCESS] szilardfrequency(48)=1200002000</code></pre></div><p><strong>Wrap up</strong></p><p>I always liked the idea of a small cluster that you could easily take with you. That&rsquo;s why I&rsquo;m stronglyconsidering the Turing Pi V2.5, which can work with the much more powerful CM4 omdules, among other very capable modules. Budget allowing, I hope to purchase a Turing Pi V2.5 sometime in 2024. As always stay tuned for more exciting high performance computing tales. And at the end of the day, a compute cluster in a mini ITX format isn&rsquo;t a pie in the sky idea. For me, it&rsquo;s a great tool for learning!</p>",
            "url": "https://hpc.social/personal-blog/2024/pi-in-the-sky-a-compute-cluster-in-mini-itx-form-factor/",
            
            
            
            
            
            "date_published": "2024-03-04T17:07:46-07:00",
            "date_modified": "2024-03-04T17:07:46-07:00",
            
                "author": "Ramblings of a supercomputing enthusiast."
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2024/profiling-the-xrootd-monitoring-collector/",
            "title": "Profiling the XRootD Monitoring Collector",
            "summary": null,
            "content_text": "The XRootD Monitoring Collector (collector) receives file transfer accounting messages from XRootD servers.This transfer information is parsed by the collector and sent to the GRACC accounting database for visualization.Each transfer will generate multiple messages:  Connection message with client information  Token information  File open with file name  Transfer updates (potentially multiple)  File close with statistics about bytes read and written  DisconnectionWe can see 1000+ messages a second from XRootD servers across the OSG.  But, recently the collector has not been able to keep up.  Below is the traffic of messages to the collector from the OSG’s Message Bus:        Message bus traffic before optimization    The graph is from the message bus’s perspective, so publish is incoming to the message bus, and deliver is sending to consumers (the Collector).  We are receiving (Publish) ~1550 messages a second, while the collector is only able to process (Deliver) ~500 messages a second.  1550 messages a second is higher than our average, but we need to be able to process data as fast as it comes.  Messages that are not processed will wait on the queue.  If the queue gets too large (maximum is set to 1 Million messages) then the messages will be deleted, losing valuable transfer accounting data.  At a defecit 1000 messages a second, it would only take ~16 minutes to fill the queue.  It is clear that we missed data for a significant amount of time.ProfilingThe first step to optimizing the XRootD Monitoring Collector is to profile the current process.  Profiling is the process of measuring the performance of the collector to identify bottlenecks and areas for improvement.For profiling, I created a development environment on the National Research Platform (NRP) to host the collector.  I started a jupyter notebook on the NRP, and used VSCode to edit the collector code and a Jupyter notebook to process the data.  I used the cProfile package built into python to perform the profiling.I modified the collector to output a profile update every 10 seconds so I could see the progress of the collector.After profiling, I used snakeviz to visualize the profile.  Below is a visualization of the profile before any optimization.  The largest consumer of processing time was DNS resoluiton, highlighted in the below image in purple.        Snakeviz profile.  Purple is the DNS resolution function    The collector uses DNS to resolve the hostnames for all IPs it receives in order to provide a human friendly name for clients and servers.  Significant DNS resolution is expected as the collector is receiving messages from many different hosts.  However, the DNS resolution is taking up a significant amount of time and is a bottleneck for the collector.ImprovementAfter reviewing the profile, I added a cache to the DNS resolution so that the collecotr only needs to resolve the host once every 24 hours.  When I profiled after making the change, I saw a significant improvement in DNS resolution time.  Below is another visualization of the profile after the DNS caching, purple is the DNS resolution.        Snakeviz profile.  Purple is the DNS resolution function    Notice that the DNS resolution is a much smaller portion of the overall running time when compared to the previous profile.In the following graph, I show the time spent on DNS resolution over time for both before and after the optimization.  I would expect DNS resolution to increase for both, but as you can see, the increase after adding DNS caching is much slower.        Growth of DNS resolution time    ProductionWhen we applied the changes into production, we saw a significant improvement in the collector’s ability to process messages.  Below is the graph of the OSG’s Message Bus after the change:        RabbitMQ Message Parsing    The incoming messages decreased, but the collector is now able to process messages as fast as they are received.  This is a significant improvement over the previous state.  I suspect that the decrease in incoming messages is due to server load of sending more outgoing messages to the improved collector.  The message bus can slow down the incoming messages under heavier load.Conclusions and Future WorkSince we implemented the cache for DNS resolution, the collector has been able to keep up with the incoming messages.  This is a significant improvement over the previous state.  Over time, we expect the DNS cache to capture nearly all of the hosts, and the DNS resolution time to decrease even further.We continue to look for optimizations to the collector.  When looking at the output from the most recent profile, we noticed the collector is spending a significant amount of time in the logging functions.  By default, we have debug logging turned on.  We will look at turning off debug logging in the future.Additionally, the collector is spending a lot of time polling for messages.  In fact, the message bus is receiving ~1500 messages a second, which is increasing the load on the message bus.  After reading through optimizations for RabbitMQ, it appears that less but larger messages are better for the message bus.  We will look at batching messages in the future.",
            "content_html": "<p>The <a href=\"https://github.com/opensciencegrid/xrootd-monitoring-collector\">XRootD Monitoring Collector</a> (collector) receives file transfer accounting messages from <a href=\"https://xrootd.slac.stanford.edu/\">XRootD</a> servers.This transfer information is parsed by the collector and sent to the GRACC accounting database for visualization.Each transfer will generate multiple messages:</p><ol>  <li>Connection message with client information</li>  <li>Token information</li>  <li>File open with file name</li>  <li>Transfer updates (potentially multiple)</li>  <li>File close with statistics about bytes read and written</li>  <li>Disconnection</li></ol><p>We can see 1000+ messages a second from XRootD servers across the OSG.  But, recently the collector has not been able to keep up.  Below is the traffic of messages to the collector from the OSG’s Message Bus:</p><figure class=\"\">  <img alt=\"this is a placeholder image\" src=\"https://derekweitzel.com/images/posts/profiling-xrootd-collector/before-optimization-mq.png\" /><figcaption>      Message bus traffic before optimization    </figcaption></figure><p>The graph is from the message bus’s perspective, so publish is incoming to the message bus, and deliver is sending to consumers (the Collector).  We are receiving (Publish) ~1550 messages a second, while the collector is only able to process (Deliver) ~500 messages a second.  1550 messages a second is higher than our average, but we need to be able to process data as fast as it comes.  Messages that are not processed will wait on the queue.  If the queue gets too large (maximum is set to 1 Million messages) then the messages will be deleted, losing valuable transfer accounting data.  At a defecit 1000 messages a second, it would only take ~16 minutes to fill the queue.  It is clear that we missed data for a significant amount of time.</p><h2 id=\"profiling\">Profiling</h2><p>The first step to optimizing the XRootD Monitoring Collector is to profile the current process.  Profiling is the process of measuring the performance of the collector to identify bottlenecks and areas for improvement.</p><p>For profiling, I created a development environment on the <a href=\"https://nationalresearchplatform.org/\">National Research Platform (NRP)</a> to host the collector.  I started a <a href=\"https://docs.nationalresearchplatform.org/userdocs/jupyter/jupyterhub-service/\">jupyter notebook on the NRP</a>, and used VSCode to edit the collector code and a Jupyter notebook to process the data.  I used the <a href=\"https://docs.python.org/3/library/profile.html\">cProfile</a> package built into python to perform the profiling.I modified the collector to output a profile update every 10 seconds so I could see the progress of the collector.</p><p>After profiling, I used <a href=\"https://jiffyclub.github.io/snakeviz/\">snakeviz</a> to visualize the profile.  Below is a visualization of the profile before any optimization.  The largest consumer of processing time was DNS resoluiton, highlighted in the below image in purple.</p><figure class=\"\">  <img alt=\"this is a placeholder image\" src=\"https://derekweitzel.com/images/posts/profiling-xrootd-collector/before-optimization-profile.png\" /><figcaption>      Snakeviz profile.  Purple is the DNS resolution function    </figcaption></figure><p>The collector uses DNS to resolve the hostnames for all IPs it receives in order to provide a human friendly name for clients and servers.  Significant DNS resolution is expected as the collector is receiving messages from many different hosts.  However, the DNS resolution is taking up a significant amount of time and is a bottleneck for the collector.</p><h2 id=\"improvement\">Improvement</h2><p>After reviewing the profile, <a href=\"https://github.com/opensciencegrid/xrootd-monitoring-collector/pull/43\">I added a cache to the DNS resolution</a> so that the collecotr only needs to resolve the host once every 24 hours.  When I profiled after making the change, I saw a significant improvement in DNS resolution time.  Below is another visualization of the profile after the DNS caching, purple is the DNS resolution.</p><figure class=\"\">  <img alt=\"this is a placeholder image\" src=\"https://derekweitzel.com/images/posts/profiling-xrootd-collector/after-optimization-profile.png\" /><figcaption>      Snakeviz profile.  Purple is the DNS resolution function    </figcaption></figure><p>Notice that the DNS resolution is a much smaller portion of the overall running time when compared to the previous profile.</p><p>In the following graph, I show the time spent on DNS resolution over time for both before and after the optimization.  I would expect DNS resolution to increase for both, but as you can see, the increase after adding DNS caching is much slower.</p><figure class=\"\">  <img alt=\"this is a placeholder image\" src=\"https://derekweitzel.com/images/posts/profiling-xrootd-collector/dns-resolution.png\" /><figcaption>      Growth of DNS resolution time    </figcaption></figure><h2 id=\"production\">Production</h2><p>When we applied the changes into production, we saw a significant improvement in the collector’s ability to process messages.  Below is the graph of the OSG’s Message Bus after the change:</p><figure class=\"\">  <img alt=\"this is a placeholder image\" src=\"https://derekweitzel.com/images/posts/profiling-xrootd-collector/edited-production-mq.png\" /><figcaption>      RabbitMQ Message Parsing    </figcaption></figure><p>The incoming messages decreased, but the collector is now able to process messages as fast as they are received.  This is a significant improvement over the previous state.  I suspect that the decrease in incoming messages is due to server load of sending more outgoing messages to the improved collector.  The message bus can slow down the incoming messages under heavier load.</p><h2 id=\"conclusions-and-future-work\">Conclusions and Future Work</h2><p>Since we implemented the cache for DNS resolution, the collector has been able to keep up with the incoming messages.  This is a significant improvement over the previous state.  Over time, we expect the DNS cache to capture nearly all of the hosts, and the DNS resolution time to decrease even further.</p><p>We continue to look for optimizations to the collector.  When looking at the output from the most recent profile, we noticed the collector is spending a significant amount of time in the logging functions.  By default, we have debug logging turned on.  We will look at turning off debug logging in the future.</p><p>Additionally, the collector is spending a lot of time polling for messages.  In fact, the message bus is receiving ~1500 messages a second, which is increasing the load on the message bus.  After reading through optimizations for RabbitMQ, it appears that less but larger messages are better for the message bus.  We will look at batching messages in the future.</p>",
            "url": "https://hpc.social/personal-blog/2024/profiling-the-xrootd-monitoring-collector/",
            
            
            
            
            
            "date_published": "2024-01-31T05:00:00-07:00",
            "date_modified": "2024-01-31T05:00:00-07:00",
            
                "author": "Derek Weitzel's Blog"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2023/sc-23-recap/",
            "title": "SC'23 Recap",
            "summary": null,
            "content_text": "The largest high-performance computing industry conference of the year, SC23, was held in Denver last week. This year's conference attracted over 14,000 attendees and 438 exhibitors, finally breaking pre-pandemic records, and it solidly felt like the old days of the conference in terms of breadth of attendees, the technical program, and overall engagement and interaction across the community.This was the second time I've attended the conference as a vendor instead of a customer, and this meant I spent a fair amount of time running to and from meetings instead of walking the show floor or attending technical sessions. I'm sure I missed some major announcements and themes as a result, but I thought it still might be valuable to contribute my observations based on this narrow lens of an AI-minded storage product manager for a major cloud service provider. If you're interested in a more well-rounded perspective, check out the HPC Social Supercomputing 2023 Summary and contribute your own thoughts!I don't know the best way to organize the notes that I took, so I grouped them into a few broad categories:Big news on the Top500What's new in storage for HPC and AIThe emergence of pure-play GPU cloudsOther technological dribs and drabsPersonal thoughts and reflections on the conference and communityI must also disclose that I am employed by Microsoft and I attended SC23 in that capacity. However, everything in this post is my own personal viewpoint, and my employer had no say in what I did or didn't write here. Everything below is written from my perspective as an enthusiast, not an employee, although my day job probably colors my outlook on the HPC industry.With all that being said, let's dive into the big news of the week!Big news on the Top500Unveiling the new Top500 list is the tentpole event of SC every year regardless of how much people (including myself!) deride HPL, and unlike the lists over the past year, this newest listing had two big surprises. Many of us went into the SC23 season wondering if the Aurora system, whose hardware was delivered this past June, would be far enough in installation and shakeout to unseat Frontier as the second listed exascale system. At the same time, nobody had expected another &gt;500 PF supercomputer to appear on the list, much less one operated privately and for-profit. But both systems made big debuts in the top 5, carrying with them interesting implications.The new #2: Argonne's AuroraThe Aurora exascale system has a storied history going back to 2015; first conceived of as a 180 PF supercomputer to be delivered in 2018, it evolved into a GPU-based exascale supercomputer that was supposed to land in 2021. Now two years late and a few executives short, Intel and Argonne were stuck between a rock and a hard place in choosing whether to list their HPL results at SC23:If Aurora wasn't listed on SC23's Top500 list, it risked going up against El Capitan at ISC'24 and being completely overshadowed by the simultaneous launch of a newer, bigger exascale system.If Aurora was listed at SC23's Top500 list but in an incomplete form, it would fall short of its long-awaited debut as the #1 system and would require a careful narrative to avoid being seen as a failed system.Intel and Argonne ultimately chose option #2 and listed an HPL run that used only 5,439 of Aurora's 10,624 nodes (51.1% of the total machine), and as expected, people generally understood that this sub-exaflop score was not an indictment of the whole system underdelivering, but more a reflection that the system was still not stable at its full scale. Still, headlines in trade press were dour, and there was general confusion about how to extrapolate Aurora's HPL submission to the full system.  Does the half-system listing of 585.34 PF Rmax at 24.7 MW power mean that the full system will require 50 MW to achieve an Rmax that's still lower than Frontier? Why is the efficiency (Rmax/Rpeak = 55%) so low?Interestingly, about half the people I talked to thought that Argonne should've waited until ISC'24 to list the full system, and the other half agreed that listing half of Aurora at SC'23 was the better option. Clearly there was no clearly right answer here, and I don't think anyone can fault Argonne for doing the best they could given the Top500 submission deadline and the state of the supercomputer. In talking to a couple folks from ALCF, I got the impression that there's still plenty of room to improve the score since their HPL run was performed under a time crunch, and there were known issues affecting performance that couldn't have been repaired in time. With any luck, Aurora will be ready to go at full scale for ISC'24 and have its moment in the sun in Hamburg.The new #3: Microsoft's EagleThe other new Top500 entry near the top of the list was Eagle, Microsoft's surprise 561 PF supercomputer. Like Aurora, it is composed of GPU-heavy nodes, and like Aurora, the HPL run utilized only part (1,800 nodes) of the full system. Unlike Aurora though, the full size of Eagle is not publicly disclosed by Microsoft, and its GPU-heavy node architecture was designed for one specific workload: training large language models for generative AI.At the Top500 BOF, Prabhat Ram gave a brief talk about Eagle where he emphasized that the system wasn't a custom-built, one-off stunt machine. Rather, it was built from publicly available ND H100 v5 virtual machines on a single 400G NDR InfiniBand fat tree fabric, and Microsoft had one of the physical ND H100 v5 nodes at its booth.  Here's the back side of it:From top to bottom, you can see it has eight E1.S NVMe drives, 4x OSFP ports which support 2x 400G NDR InfiniBand each, a Microsoft SmartNIC, and a ton of power.  A view from the top shows the HGX baseboard and fans:&lt;p&gt;Logically, this node (and the ND H100 v5 VM that runs on it) looks a lot like the NVIDIA DGX reference architecture. Physically, it is an air-cooled, Microsoft-designed OCP server, and Eagle’s Top500 run used 1,800 of these servers.&lt;/p&gt;Big HPL number aside, the appearance of Eagle towards the top of Top500 has powerful implications on the supercomputing industry at large.  Consider the following.Microsoft is a for-profit, public enterprise whose success is ultimately determined by how much money it makes for its shareholders. Unlike government agencies who have historically dominated the top of the list to show their supremacy in advancing science, the Eagle submission shows that there is now a huge financial incentive to build giant supercomputers to train large language models. This is a major milestone in supercomputing; up to this point, the largest systems built by private industry have come from the oil &amp; gas industry, and they have typically deployed at scales below the top 10.Eagle is also built on the latest and greatest technology--NVIDIA's H100 and NDR InfiniBand--rather than previous-generation technology that's already been proven out by the national labs.  SC23 was the first time Hopper GPUs have appeared anywhere on the Top500 list, and Eagle is likely the single largest installation of both H100 and NDR InfiniBand on the planet. Not only does this signal that it's financially viable to stand up a leadership supercomputer for profit-generating R&amp;D, but industry is now willing to take on the high risk of deploying systems using untested technology if it can give them a first-mover advantage.Eagle also shows us that the potential upside of bringing a massive new AI model to market is worth both the buying all the infrastructure required to build a half-exaflop system and hiring the talent required to shake out what is literally a world-class supercomputer. And while the US government can always obtain a DPAS rating to ensure it gets dibs on GPUs before AI companies can, there is no DPAS rating for hiring skilled individuals to stand up gigantic systems. This all makes me wonder: if Aurora was a machine sitting in some cloud data center instead of Argonne, and its commissioning was blocking the development of the next GPT model, would it have been able to take the #1 spot from Frontier this year?The appearance of such a gigantic system on Top500, motivated by and paid for as part of the AI land grab, also raises some existential questions for the US government. What role should the government have in the supercomputing industry if private industry now has a strong financial driver to invest in the development of leadership supercomputing technologies? Historically, government has always incubated cutting-edge HPC technologies so that they could stabilize enough to be palatable to commercial buyers. Today's leadership supercomputers in the national labs have always wound up as tomorrow's midrange clusters that would be deployed for profit-generating activities like seismic imaging or computer-aided engineering. If the AI industry is now taking on that mantle of incubating and de-risking new HPC technologies, perhaps government now needs to focus on ensuring that the technologies developed and matured for AI can still be used to solve scientific problems.What's new in storage for HPC and AI?Since I spent much of my career working in HPC storage, and I now focus largely on AI, it should be no surprise that I heard a lot about the intersection of AI and storage.  AI remains high in the hype cycle, so it's natural that just about every storage vendor and discussion had some talk of AI forced into it regardless of it was really relevant or not. However, there were a few places where AI and storage topics intersect that I found noteworthy.The AI-storage echo chamber&lt;p&gt;I was asked a lot of questions about storage from journalists, VCs, and even trusted colleagues that followed a common theme: What storage technologies for AI excite me the most? What’s the future of storage for AI?&lt;/p&gt;I don't fault people for asking such a broad question because the HPC/AI storage industry is full of bombastic claims. For example, two prominent storage vendors emblazoned their booths with claims of what their products could do for AI:These photos illustrate the reality that, although there is general agreement that good storage is needed for GPUs and AI, what constitutes \"good storage\" is muddy and confusing. Assuming the above approach to marketing (10x faster! 20x faster!) is effective for someone out there, there appears to be a market opportunity in just capitalizing on this general confusion by (1) asserting what the I/O problem that's jamming up all AI workloads is, and (2) showing that your storage product does a great job at solving that specific problem.For example, the MLPerf Storage working group recently announced the first MLPerf Storage benchmark, and Huiho Zheng from Argonne (co-author of the underlying DLIO tool on which MLPerf Storage was built) described how the MLPerf Storage benchmark reproduces the I/O characteristics of model training at the Workshop on Software and Hardware Co-Design of Deep Learning Systems in Accelerators:When I saw this premise, I was scratching my head--my day job is to develop new storage products to meet the demands of large-scale AI model training and inferencing, and I have never had a customer come to me claiming that they need support for small and sparse I/O or random access. In fact, write-intensive checkpointing and fine-tuning, not read-intensive data loading, is the biggest challenge faced by those training large language models in my experience. It wasn't until a few slides later did I realize where these requirements may be coming from:Storage and accelerator vendors are both defining and solving the I/O problems of the AI community which seems counterproductive--shouldn't a benchmark be set by the practitioners and not the solution providers?What I learned from talking to attendees, visiting storage vendor booths, and viewing talks like Dr. Zheng's underscores a reality that I've faced on my own work with production AI workloads: AI doesn't actually have an I/O performance problem, so storage vendors are struggling to define ways in which they're relevant in the AI market.I outlined the ways in which LLM training uses storage in my HDF5 BOF talk, and their needs are easy to meet with some local storage and basic programming. So easy, in fact, that a reasonably sophisticated AI practitioner can duct tape their way around I/O problems very quickly and move on to harder problems. There's no reason for them to buy into a sophisticated Rube Goldberg storage system, because it still won't fundamentally get them away from having to resort to local disk to achieve the scalability needed to train massive LLMs.So yes, I've got no doubt that there are storage products that can deliver 10x or 20x higher performance for some specific AI workload. And MLPerf Storage is probably an excellent way to measure that 20x performance boost. But the reality I've experienced is that a half a day of coding will deliver 19x higher performance when compared to the most naive approach, and every AI practitioner knows and does this already. That's why there are a lot of storage vendors fishing in this AI storage pond, but none of them seem to be reeling in any whoppers.This isn't to say that there's nothing interesting going on in high-performance storage though. If the most common question I was asked was \"what's the future of storage for AI,\" the second most common question was \"what do you think about VAST and WEKA?\"VAST &amp; WEKABoth companies seem to be doing something right since they were top of mind for a lot of conference attendees, and it probably grinds their respective gears that the field still groups them together in the same bucket of \"interesting parallel storage systems that we should try out.\" Rather than throw my own opinion in the pot though (I work with and value both companies and their technologies!), I'll note the general sentiments I observed.WEKA came into the week riding high on their big win as U2's official technology partner in September. Their big booth attraction was a popular Guitar Hero game and leaderboard, and an oversized Bono, presumably rocking out to how much he loves WEKA, presided over one of their seating areas:Much of their marketing centered around accelerating AI and other GPU workloads, and the feedback I heard from the WEKA customers I bumped into during the week backed this up. One person shared that the WEKA client does a great job with otherwise difficult small-file workloads, particularly common in life sciences workloads, and this anecdote is supported by the appearance of a very fast WEKA cluster owned by MSK Cancer Center on the IO500 Production list. People also remarked about WEKA's need for dedicated CPU cores and local storage to deliver the highest performance; this, combined with its client scalability, lends itself well to smaller clusters of fat GPU nodes. I didn't run into anyone using WEKA in the cloud though, so I assume the feedback I gathered had a bias towards more conventional, on-prem styles of architecting storage for traditional HPC.Whereas WEKA leaned into its rock 'n' roll theme this year, VAST doubled down on handing out the irresistibly tacky light-up cowboy hats they introduced last year (which I'm sure their neighbors at the DDN booth absolutely loved). They were all-in on promoting their new identity as a \"data platform\" this year, and although I didn't hear anyone refer to VAST as anything but a file system, I couldn't throw a rock without hitting someone who either recently bought a VAST system or tried one out.Unlike last year though, customer sentiment around VAST wasn't all sunshine and rainbows, and I ran into a few customers who described their presales engagements as more formulaic than the white-glove treatment everyone seemed to be getting a year ago. This isn't surprising; there's no way to give all customers the same royal treatment as a business scales. But it does mean that the honeymoon period between VAST and the HPC industry is probably at an end, and they will have to spend the time between now and SC24 focusing on consistent execution to maintain the momentum they've gotten from the light-up cowboy hats.The good news for VAST is that they've landed some major deals this past year, and they came to SC with customers and partners in-hand. They co-hosted a standing-room-only party with CoreWeave early in the week and shared a stage with Lambda at a customer breakfast, but they also highlighted two traditional, on-prem HPC customers (TACC and NREL) at the latter event.VAST clearly isn't letting go of the on-prem HPC market as it also pursues partnerships with emerging GPU cloud service providers; this contrasted with WEKA's apparent focus on AI, GPUs, and the cloud. Time will tell which strategy (if either, or both) proves to be the better approach.DAOSThough commercial buyers were definitely most interested in VAST and WEKA, folks from the more sophisticated HPC shops around the world also tossed a few questions about DAOS my way this year.I usually make it a point to attend the annual DAOS User Group meeting since it is always attended by all the top minds in high-performance I/O research, but I had to miss it this year on account of it running at the same time as my I/O tutorial. Fortunately, DAOS was pervasive throughout the conference, and there was no shortage of opportunity to find out what the latest news in the DAOS was. For example, check out the lineup for PDSW 2023 this year:Three out of thirteen talks were about DAOS which is more than any other single storage product or project. DAOS also won big at this year's IO500, taking the top two spots in the production storage system list:&lt;div class=\"separator\" style=\"clear: both; text-align: center;\"&gt;&lt;/div&gt;In fact, DAOS underpinned every single new awardee this year, and DAOS is now the second most represented storage system on the list behind Lustre:Why is DAOS at the top of so many people's minds this year? Well, DAOS reached a few major milestones in the past few months which has thrust it into the public eye.  First, Aurora is finally online and running jobs, and while the compute system is only running at half its capability, the full DAOS system (all 220 petabytes of it, all of which is TLC NVMe) is up and running--a testament to the scalability of DAOS that many parallel storage systems--including VAST and WEKA--have not publicly demonstrated. Because DAOS is open-source software and Aurora is an open-science system, all of DAOS' at-scale warts are also on full display to the community in a way that no competitive storage system besides of Lustre is.Second, Google Cloud cast a bold vote of confidence in DAOS by launching Parallelstore, its high-performance parallel file service based on DAOS, in August. Whereas AWS and Azure have bet on Lustre to fill the high-performance file gap (via FSx Lustre and Azure Managed Lustre), GCP has planted a stake in the ground by betting that DAOS will be the better foundation for a high-performance file service for HPC and AI workloads.Parallelstore is still in private preview and details are scant, but GCP had DAOS and Parallelstore dignitaries at all the major storage sessions in the technical program to fill in the gaps. From what I gathered, Parallelstore is still in its early stages and is intended to be a fast scratch tier; it's using DRAM for metadata which means it relies on erasure coding across servers to avoid data loss on a single server reboot, and there's no way to recover data if the whole cluster goes down at once. This lack of durability makes it ineligible for the IO500 list right now, but the upcoming metadata-on-NVMe feature (which previews in upstream DAOS in 1H2024) will be the long-term solution to that limitation.Finally, the third major bit of DAOS news was about the formation of the DAOS Foundation. First announced earlier this month, this initiative lives under the umbrella of the Linux Foundation and is led by its five founding members:Argonne National Laboratory, who has a vested interest in seeing DAOS endure given its massive investment in it,Enakta Labs, a company spun out of Croit, a German storage services company that was contributing feature development to DAOS,Google Cloud, who has made a big bet on DAOS as the underpinnings for its Parallelstore service,HPE, who has a shared fate with the DAOS installation at Argonne and who has also been contributing feature development, andIntel, whose engineers largely developed DAOS as part of the Aurora program.I see this handoff of DAOS from Intel to this new foundation as a positive change that makes DAOS a more stable long-term bet; should Intel choose to divest itself of DAOS once its obligations to the Aurora program end, DAOS now can live on without the community having to fork it. The DAOS Foundation is somewhat analogous to OpenSFS (one of the nonprofits backing Lustre) in that it is a vendor-neutral organization around which the DAOS community can gather.But unlike OpenSFS, the DAOS Foundation will also assume the responsibility of releasing new versions of DAOS after Intel releases its final version (2.6) in March 2024. The DAOS Foundation will also steer feature prioritization, but seeing as how the DAOS Foundation doesn't fund developers directly, it's not clear that contributors like Intel or GCP are actually at the mercy of the foundation's decisions. It's more likely that the DAOS Foundation will just have authority to decide what features will roll up into the next formal DAOS release, and developers contributing code to DAOS will still prioritize whatever features their employers tell them to.So, DAOS was the talk of the town at SC23. Does this all mean that DAOS is ready for prime time?While Intel and Argonne may say yes, the community seems to have mixed feelings.  Consider this slide presented by László Szűcs from LRZ at the DAOS Storage Community BOF:DAOS is clearly crazy fast and scales to hundreds of petabytes in production--Aurora's IO500 listing proves that. However, that performance comes with a lot of complexity that is currently being foisted on application developers, end-users, and system administrators. The \"opportunities\" listed in László's slide are choices that people running at leadership HPC scale may be comfortable making, but the average HPC user is not equipped to make many of these decisions and make thoughtful choices about container types and library interfaces.The fact that DAOS was featured so prominently at PDSW--a research workshop--probably underscores this as well. This slide presented by Adrian Jackson's lighting talk sums up the complexity along two different dimensions:His results showed that your choice of DAOS object class and I/O library atop the DAOS POSIX interface can result in wildly different checkpoint bandwidth. It's hard enough to teach HPC users about getting optimal performance out of a parallel file system like Lustre; I can't imagine those same users will embrace the idea that they should be mindful of which object class they use as they generate data.The other DAOS-related research talk, presented by Greg Eisenhauer, was a full-length paper that caught me by surprise and exposed how much performance varies when using different APIs into DAOS. This slide is one of many that highlighted this:I naively thought that the choice of native userspace API (key-value or array) would have negligible effects on performance, but Eisenhauer's talk showed that this isn't true. The reality appears to be that, although DAOS is capable of handling unaligned writes better than Lustre, aligning arrays on large, power-of-two boundaries still has a significant performance benefit.Based on these sorts of technical talks about DAOS presented this year, the original question--is DAOS ready for prime time--can't be answered with a simple yes or no yet.  The performance it offers is truly best in class, but achieving that performance doesn't come easy right now. Teams who are already putting heroic effort into solving a high-value problems will probably leap at the opportunity to realize the I/O performance that DAOS can deliver. Such high value problems include things like training the next generation of foundational LLMs, and GCP's bet on DAOS probably adds differentiable value to their platform as a place to train such models as efficiently as possible. But the complexity of DAOS at present probably limits its appeal to the highest echelons of leadership HPC and AI, and I think it'll be a while before DAOS is in a place where a typical summer intern will be able to appreciate its full value.InfiniaIt would be unfair of me to give all this regard to WEKA, VAST, and DAOS without also mentioning DDN's brand new Infinia product, launched right before SC23. Those in the HPC storage industry have been awaiting its launch for years now, but despite the anticipation, it really didn't come up in any conversations in which I was involved. I did learn that the engineering team developing Infinia inside DDN is completely separate from the Whamcloud team who is developing Lustre, but this could be a double-edged sword. On the good side, it means that open-source Lustre development effort isn't competing with DDN's proprietary product in engineering priorities on a day-to-day basis. On the bad side though, I still struggle to see how Infinia and Lustre can avoid eventually competing for the same business.For the time being, Infinia does seem to prioritize more enterprisey features like multitenancy and hands-free operation while Lustre is squarely aimed at delivering maximum performance to a broadening range of workloads. Their paths may eventually cross, but that day is probably a long way off, and Lustre has the benefit of being deeply entrenched across the HPC industry.The emergence of pure-play GPU cloudsIn addition to chatting with people about what's new in storage, I also went into SC23 wanting to understand how other cloud service providers are structuring end-to-end solutions for large-scale AI workloads. What I didn't anticipate was how many smaller cloud service providers (CSPs) showed up to SC for the first time this year, all waving the banner of offering NVIDIA H100 GPUs. These are predominantly companies that either didn't exist a few years ago or have historically focused on commodity cloud services like virtual private servers and managed WordPress sites, so it was jarring to suddenly see them at an HPC conference. How did so many of these smaller CSPs suddenly become experts in deploying GPU-based supercomputers in the time between SC22 and SC23? I got to talking to a few folks at these smaller CSPs to figure out exactly what they were offering to customers, and their approach is quite different from how AWS, Azure, and GCP operate. Rather than defining a standard cluster architecture and deploying copies of it all over to be consumed by whoever is willing to pay, these smaller CSPs deploy clusters of whitebox GPU nodes to customer specification and sell them as dedicated resources for fixed terms. If a customer wants a bunch of HGX H100s interconnected with InfiniBand, that's what they get. If they want RoCE, the CSP will deploy that instead. And the same is true with storage: if a customer wants EXAScaler or Weka, they'll deploy that too.While this is much closer to a traditional on-prem cluster deployment than a typical elastic, pay-as-you-go infrastructure-as-a-service offering, this is different from being a fancy colo. The end customer still consumes those GPUs as a cloud resource and never has to worry about the infrastructure that has to be deployed behind the curtain, and when the customer's contract term is up, their cluster is still owned by the CSP. As a result, the CSP can either resell that same infrastructure via pay-as-you-go or repurpose it for another dedicated customer. By owning the GPUs and selling them as a service, these CSPs can also do weird stuff like take out giant loans to build more data centers using GPUs as collateral. Meanwhile, NVIDIA can sell GPUs wholesale to these CSPs, book the revenue en masse, and let the CSPs deal with making sure they're maintained in production and well utilized.It also seems like the services that customers of these smaller CSPs get is often more barebones than what they'd get from a Big 3 CSP (AWS, Azure, and GCP). They get big GPU nodes and an RDMA fabric, but managed services beyond that are hit and miss.For example, one of these smaller CSPs told me that most of their storage is built on hundreds of petabytes of open-source Ceph. Ceph fulfills the minimum required storage services that any cloud must provide (object, block, and file), but it's generally insufficient for large-scale model training. As a result, all the smaller CSPs with whom I spoke said they are also actively exploring VAST and Weka as options for their growing GPU-based workloads. Since both VAST and Weka offer solid S3 and file interfaces, either could conceivably act as the underpinnings of these GPU clouds' first-party storage services as well.As I said above though, it seems like the predominant model is for these CSPs to just ship whatever dedicated parallel storage the customer wants if something like Ceph isn't good enough. This, and the growing interest in storage from companies like VAST and Weka, suggest a few things:Some of these CSPs have been obtaining and deploying GPUs faster than they've had time to think about the end-to-end experience, and customers have so much pent-up demand for GPUs that they're willing to either work with whatever third-party storage vendor is brought to the table or take on the responsibility of choosing their preferred storage vendor themselves.Having giant piles of GPUs is necessary, but not sufficient, to have a competitive offering in the GPU cloud services landscape. A credible platform for AI training must also have an integrated high-performance storage service.It is looking like many pure-play GPU clouds are finding it more cost-effective to buy their way out of high-performance storage problems through partnerships than build and manage their own services atop open-source software like Lustre or DAOS.None of these observations are terribly surprising; at the price these smaller CSPs are offering GPUs compared to the Big 3 CSPs, their gross margin (and therefore their ability to invest in developing services on top of their IaaS offerings) has got to be pretty low. In the short term, it's cheaper and easier to deploy one-off high-performance storage systems alongside dedicated GPU clusters based on customer demand than develop and support a standard solution across all customers.Of course, building a low-cost GPU service opens the doors for other companies to develop their own AI services on top of inexpensive GPU IaaS that is cost-competitive with the Big 3's native AI platforms (AWS SageMaker, Azure Machine Learning, and Google AI Platform). For example, I chatted with some folks at together.ai, a startup whose booth caught my eye with its bold claim of being \"the fastest cloud for [generative] AI:\"Contrary to their banner, they aren't a cloud; rather, they provide AI services--think inferencing and fine-tuning--that are accessible through an API much like OpenAI's API. They've engineered their backend stack to be rapidly deployable on any cloud that provides basic IaaS like GPU-equipped VMs, and this allows them to actually run their computational backend on whatever cloud can offer the lowest-cost, no-frills GPU VMs. In a sense, companies like together.ai develop and sell the frills that these new GPU CSPs lack, establishing a symbiotic alternative to the vertically integrated AI platforms on bigger clouds.I did ask a few of these smaller CSPs what their overall pitch was. Why I would choose GPU cloud X over their direct competitor GPU cloud Y? The answers went in two directions:They offer lower cost per GPU hour than their competitionThey are faster to get GPUs off a truck and into production than their competitionThere's a big caveat here: I didn't talk to many representatives at these CSPs, so my sample size was small and not authoritative. However, taking these value propositions at face value struck me as being quite precarious since their value is really a byproduct of severe GPU shortages driven by the hyped-up AI industry. What happens to these CSPs (and the symbionts whose businesses depend on them) when AMD GPUs appear on the market in volume? What happens if NVIDIA changes course and, instead of peanut-buttering its GPUs across CSPs of all sizes, it focuses its attention on prioritizing deliveries to just a few blessed CSPs?There is no moat around generative AI, and I left SC23 feeling like there's a dearth of long-term value being generated by some of these smaller GPU CSPs. For those CSPs whose primary focus is buying and deploying as many GPUs in as short a time as possible, not everyone can survive. They'll either come out of this GPU shortage having lost a lot of money building data centers that will go unused, or they'll be sold for parts.More importantly to me though, I learned that I should give less credence to the splashy press events of hot AI-adjacent startups if their successes lie exclusively with smaller GPU CSPs. Some of these CSPs are paying to make their problems go away in an effort to keep their focus on racking and stacking GPUs in the short term, and I worry that there's a lack of long-term vision and strong opinions in some of these companies. Some of these smaller CSPs seem much more like coin-operated GPU cluster vending machines than platform providers, and that business model doesn't lend itself to making big bets and changing the industry.Put another way, my job--both previous and current--has always been to think beyond short-term band aids and make sure that my employer has a clear and opinionated view of the technical approach that will be needed to address the challenges of HPC ten years in the future. I know who my peers are at the other Big 3 CSPs and leadership computing facilities across the world, and I know they're thinking hard about the same problems that I am. What worries me is that I do not know who my peers are at these smaller CSPs, and given their speed of growth and smaller margins, I worry that they aren't as prepared for the future as they will need to be. The AI industry as a whole will be better off when GPUs are no longer in such short supply, but the ecosystem surrounding some of these smaller GPU CSPs is going to take some damage when that day comes.Other dribs and drabsI also had a lot of interesting conversations and noticed a few subtle themes last week that don't neatly fit into any other category, but I'd love to hear more from others if they noticed the same or have more informed opinions.APUs and superchips - are they really that useful?Because I spent my booth duty standing next to one of Eagle's 8-way HGX H100 nodes, a lot of people asked me if I thought the Grace Hopper superchip would be interesting. I'm not an expert in either GPUs or AI, but I did catch up with a few colleagues who are smarter than me in this space last week, and here's the story as I understand it:The Grace Hopper superchip (let's just call it GH100) is an evolution of the architecture developed for Summit, where V100 GPUs were cache-coherent with the CPUs through a special widget that converted NVLink to the on-chip coherence protocol for Power9. With GH100, the protocol used to maintain coherence across the CPU is directly compatible with the ARM AMBA coherence protocol, eliminating one bump in the path that Power9+V100 had. Grace also has a much more capable memory subsystem and NOC that makes accessing host memory from the GPU more beneficial.Now, do AI workloads really need 72 cores per H100 GPU? Probably not.What AI (and HPC) will need are some high-performance cores to handle all the parts of application execution that GPUs are bad at--divergent code paths, pointer chasing, and I/O. Putting capable CPU cores (Neoverse V2, not the N2 used in CPUs like new Microsoft's Cobalt 100) on a capable NOC that is connected to the GPU memory subsystem at 900 GB/s opens doors for using hierarchical memory to train LLMs in clever ways.For example, naively training an LLM whose weights and activations are evenly scattered across both host memory and GPU memory won't go well since that 900 GB/s of NVLink C2C would be on the critical path of many computations. However, techniques like activation checkpointing could become a lot more versatile when the cost of offloading certain tensors from GPU memory is so much lower. In essence, the presence of easily accessible host memory will likely allow GPU memory to be used more efficiently since the time required to transfer tensors into and out of HBM is easier to hide underneath other computational steps during training.Pairing an over-specified Grace CPU with a Hopper GPU also allows the rate of GPU development to proceed independently of CPU development. Even if workloads that saturate an H100 GPU might not also need all 72 cores of the Grace CPU, H200 or other future-generation GPUs can grow into the capabilities of Grace without having to rev the entire superchip.I didn't get a chance to talk to any of my colleagues at AMD to get their perspective on the MI300 APU, but I'd imagine their story is a bit simpler since their memory space is flatter than NVIDIA's superchip design. This will make training some models undoubtedly more straightforward but perhaps leave less room for sophisticated optimizations that can otherwise cram more of a model into a given capacity of HBM. I'm no expert though, and I'd be happy to reference any explanations that real experts can offer! What about quantum?Quantum computing has been a hot topic for many years of SC now, but it feels like a topic that is finally making its way out of pure CS research and into the minds of the everyday HPC facility leaders. I talked to several people last week who asked me for my opinion on quantum computing because they have come to the realization that they need to know more about it than they do, and I have to confess, I'm in the same boat as they are. I don't follow quantum computing advancements very closely, but I know an increasing number of people who do--and they're the sort who work in CTOs' offices and have to worry about risks and opportunities more than intellectual curiosities.It's hard to say there've been any seismic shifts in the state of the art in quantum computing at SC23; as best I can tell, there's still a rich ecosystem of venture capital-backed startups who keep cranking out more qubits. But this year felt like the first year where HPC facilities who haven't yet started thinking about their position on quantum computing are now behind. Not everyone needs a quantum computer, and not everyone even needs a quantum computing researcher on staff. But everyone should be prepared with a strong point of view if they are asked \"what will you be doing with quantum computing?\" by a funding agency or chief executive.NextSiliconOne of the least-stealthy stealth-mode startups in the HPC industry has been NextSilicon, a company who debuted from stealth mode at SC23, launched their new Maverick accelerator, and announced their first big win with Sandia National Lab's Vanguard II project. What's notable about NextSilicon is that, unlike just about every other accelerator startup out there, they are not trying to go head-to-head with NVIDIA in the AI acceleration market. Rather, they've created a dataflow accelerator that aims to accelerate challenging HPC workloads that GPUs are particularly bad at--things like irregular algorithms and sparse data structures. They've paired this hardware with a magical runtime that continually optimizes the way the computational kernel is mapped to the accelerator's reconfigurable units to progressively improve the throughput of the accelerator as the application is running.The concept of dataflow accelerators has always been intriguing since they're the only alternative to improving computational throughput besides making larger and larger vectors. The challenge has always been that these accelerators are more like FPGAs than general-purpose processors, and they require similar amounts of hardcore CS expertise to use well. NextSilicon claims to have cracked that nut with their runtime, and it seems like they're hiring the rights sorts of people--real HPC with respectable pedigrees--to make sure their accelerator can really deliver value to HPC workloads.I/O benchmarking developmentsAt the IO500 BOF, there was rich discussion about adding new benchmarking modes to IOR and IO500 to represent a wider range of patterns.More specifically, there's been an ongoing conversation about including a 4K random read test, and it sounds like the most outspoken critics against it have finally softened their stance. I've not been shy about why I think using IOPS as a measure of file system performance is dumb, but 4K random IOPS do establish a lower bound of performance for what a real application might experience. Seeing as how IO500 has always been problematic as any representation of how a file system will perform in real-world environments, adding the option to run a completely synthetic, worst-case workload will give IO500 the ability to define a complete bounding box around the lower and upper limits of I/O performance for a file system.Hendrik Nolte from GWDG also proposed a few new and appealing IOR modes that approach more realistic workload scenarios.  The first was a new locally random mode where data is randomized within IOR segments but segments are repeated:Compared to globally randomized reads (which is what IOR normally does), this is much closer representation of parallel workloads that are not bulk-synchronous; for example, NCBI BLAST uses thread pools and work sharing to walk through files, and the resulting I/O pattern is similar to this new mode.He also described a proposal to run concurrent, mixed workloads in a fashion similar to how fio currently works.  Instead of performing a bulk-synchronous parallel write followed by a bulk-synchronous parallel read, his proposal would allow IOR to perform reads and writes concurrently, more accurately reflecting the state of multitenant storage systems. I actually wrote a framework to do exactly this and quantify the effects of contention using IOR and elbencho, but I left the world of research before I could get it published. I'm glad to see others seeing value in pursuing this idea.The other noteworthy development in I/O benchmarking was presented by Sven Breuner at the Analyzing Parallel I/O BOF where he described a new netbench mode for his excellent elbencho benchmark tool. This netbench mode behaves similarly to iperf in that it is a network-level throughput test, but because it is part of elbencho, it can generate the high-bandwidth incasts and broadcasts that are typically encountered between clients and servers of parallel storage systems:This is an amazing development because it makes elbencho a one-stop shop for debugging the entire data path of a parallel storage system. For example, if you're trying to figure out why the end-to-end performance of a file system is below expectation, you can use elbencho to test the network layer, the object or file layer, the block layer, and the overall end-to-end path separately to find out which layer is underperforming. Some file systems have specialized included tools to perform the same network tests (e.g., nsdperf for IBM Spectrum Scale), but elbencho now has a nice generic way to generate these network patterns for any parallel storage system.Some personal thoughtsAs with last year, I couldn't attend most of the technical program due to a packed schedule of customer briefings and partner meetings, but the SC23 Digital Experience was excellently done, and I wound up watching a lot of the content I missed during the mornings and after the conference (at 2x speed!). In that sense, the hybrid nature of the conference is making it easier to attend as someone who has to juggle business interests with technical interests; while I can't jump into public arguments about the definition of storage \"QOS\", I can still tell that my old friends and colleagues are still fighting the good fight and challenging conventional thinking across the technical program.My Parallel I/O in Practice tutorialThis was the sixth year that I co-presented the Parallel I/O in Practice tutorial with my colleagues Rob Latham, Rob Ross, and Brent Welch. A conference photographer got this great photo of me in the act:Presenting this tutorial is always an incredibly gratifying experience; I've found that sharing what I know is one of the most fulfilling ways I can spend my time, and being able to start my week in such an energizing way is what sustains the sleep deprivation that always follows. Giving the tutorial is also an interesting window into what the next generation of I/O experts is worrying about; for example, we got a lot of questions and engagement around the low-level hardware content in our morning half, and the I/O benchmarking material in the late afternoon seemed particularly well received. The majority of attendees came from the systems side rather than the user/dev side as well, perhaps suggesting that the growth in demand for parallel storage systems (and experts to run them) is outstripping the demand for new ways to perform parallel I/O. Guessing wildly, perhaps this means new developers are coming into the field higher up the stack, using frameworks like fsspec that abstract away low-level I/O.Since I've jumped over to working in industry, it's been hard to find the business justification to keep putting work hours into the tutorial despite how much I enjoy it.  I have to confess that I didn't have time to update any of the slides I presented this year even though the world of parallel I/O has not remained the same, and I am going to have to figure out how to better balance these sorts of community contributions with the demands of a day job in the coming years.An aside on COVID safetyAt SC22, I fastidiously wore a KN95 mask while indoors and avoided all after-hours events and indoor dining to minimize my risk of catching COVID. At that time, neither my wife nor I had ever gotten COVID before, and I had no desire to bring it home to my family since my father died of COVID-related respiratory failure two years prior. Staying fully masked at SC22 turned out to be a great decision at the time since a significant number of other attendees, including many I spoke with, contracted COVID at SC22. By comparison, I maintained my COVID-free streak through 2022.This year I took a more risk-tolerant approach for two reasons:My wife and I both broke our streaks this past summer and contracted COVID while on vacation, so if I got sick, we knew what to expect, andI got my gazillionth COVID and flu shots in October in anticipation of attending SC.Part of my approach to managing risk was bringing my trusty Aranet4 CO2 sensor with me so that I could be aware of areas where there was air circulation and the risk of contracting an airborne illness would be higher. I only wore a KN95 at the airport gates and while on the airplane at SC23, and despite going in all-in on after-hours events, indoor dining, and copious meetings and tours of booth duty, I'm happy to report that I made it through the conference without getting sick.I have no doubt that being vaccinated helped, as I've had several people tell me they tested positive for COVID after we had dinner together in Denver. But it's also notable that the Denver Convention Center had much better ventilation than Kay Bailey Hutchison Convention Center in Dallas where SC22 was held last year. To show this quantitatively, let's compare air quality measurements from SC22 to SC23.My schedule for the day on which I give my tutorial is always the same: the tutorial runs from 8:30am to 5:00pm with breaks at 10:00, 12:00, and 3:00. Because of this consistent schedule, comparing the CO2 readings (which are a proxy for re-breathed air) for my tutorial day at SC22 versus SC23 shows how different the air quality was in the two conference centers. Here's what that comparison looks like:What the plot shows is that CO2 (re-breathed air) steadily increased at the start of the tutorial at both SC22 and SC23, but Denver's convention center kicked on fresh air ventilation after an hour while Dallas simply didn't. Air quality remained poor (over 1,000) throughout the day in Dallas, whereas Denver was pretty fresh (below 700) even during the breaks and the indoor luncheon. This relatively good air circulation inside the convention center at SC23 made me much more comfortable about going maskless throughout the week.This isn't to say that I felt there was no risk of getting sick this year; there was at least one busy, upscale restaurant/bar in which I dined where the air circulation was no better than in a car or airplane. For folks who just don't want to risk being sick over Thanksgiving, wearing a mask and avoiding crowded bars was probably still the best option this year. And fortunately, Denver's weather was gorgeous, so outdoor dining was completely viable during the week.AI's effects on the HPC communityAlthough AI has played a prominent role in previous SC conferences, this was the first year where I noticed that the AI industry is bleeding into the HPC community in weird ways.For example, I had a bunch of journalists and media types accost me and start asking rather pointed questions while I was on booth duty. Talking to journalists isn't entirely unusual since I've always been supportive of industry press, but the social contract between practitioners like me and journalists has always been pretty formal--scheduling a call in advance, being invited to speak at an event, and things like that have long been the norm. If I was being interviewed on the record, I knew it.This year though, it seemed like there was a new generation of younger journalists who approached me no differently than a casual booth visitor. Some did introduce themselves as members of the press after we got chatting (good), but others did not (not good) which led me to take away a learning: check names and affiliations before chatting with strangers, because the days where I could assume that all booth visitors would act in good faith are gone.Now, why the sudden change?  I can think of three possible reasons:I'm getting older, and there are now tech industry journalists who are younger than me and think I am worth talking to since I've always been around. Maybe the old-school HPC folks that predate me have always had to deal with this.The proliferation of platforms like Substack make it financially viable to be an independent journalist, and conversely, anyone can be a journalist without editorial oversight.The spotlight on the massive AI industry is also illuminating the HPC industry. HPC and AI are both built on the same foundational technologies (GPUs, RDMA fabrics, HBM, and the like) so AI journalists now have a reason to start showing up at HPC community events.It'd be fair to argue that #3 is a stretch and that this isn't an AI phenomenon if not for the fact that I was also accosted by a few venture capitalists for the first time this year. HPC has never been an industry that attracted the attention of venture capital in the way that AI does, so I have to assume being asked specific questions about the viability of some startup's technology is a direct result of the AI market opportunity.While it's nice to have a broader community of attendees and more media coverage, the increasing presence of AI-focused media and VC types in the SC community means I can't be as open and honest as I once was. Working for a corporation (with secrets of its own to protect) doesn't help there either, so maybe getting cagier when talking to strangers is just a part of growing up.SC23 as a milestone yearAttending SC23 this year coincided with two personal milestones for me as well.This is the tenth year I've been in the HPC business, and the first SC I ever attended was SC13.  I can't say that this is my eleventh SC because I didn't attend in 2014 (on account of working at a biotech startup), but I've been to SC13, SC15 through SC19, SC20 and SC21 virtually, and SC22 and SC23 in-person.  At SC13 ten years ago, the weather was a lot colder:But I still have the fondest memories of that conference because it that was the week where I felt like I had finally found my community after having spent a decade as an unhappy materials science student.SC23 is also a milestone year because it may be the last SC I attend as a storage and I/O guy. I recently signed on for a new position within Microsoft to help architect the next generation of supercomputers for AI, and I'll probably have to trade in the time I used to spend at workshops like PDSW for opportunities to follow the latest advancements in large-scale model training, RDMA fabrics, and accelerators. But I think I am OK with that.I never intended to become an I/O or storage expert when I first showed up at SC13; it wasn't until I joined NERSC that I found that I could learn and contribute the most by focusing on storage problems. The world has changed since then, and now that I'm at Microsoft, it seems like the problems faced at the cutting edge of large language models, generative AI, and the pursuit of AGI are where the greatest need lies. As I said earlier in this post, AI has bigger problems to deal with than storage and I/O, and those bigger problems are what I'll be chasing. With any luck, I'll be able to say I had a hand in designing the supercomputers that Microsoft builds after Eagle. And as has been true for my last ten years in this business, I'll keep sharing whatever I learn with whoever wants to know.",
            "content_html": "<p>The largest high-performance computing industry conference of the year, SC23, was held in Denver last week. This year's conference <a href=\"https://twitter.com/thedeadline/status/1724949606188847381?s=61&amp;t=5LlVTsVajaU1kTzzuGQL7Q\">attracted over 14,000 attendees</a> and <a href=\"https://hallerickson.ungerboeck.com/prod/app85.cshtml?aat=Z8CDp%2bb4HWU7dw3dA3PesG2LIb9lCzjs2VEXLZZxGP4%3d\">438 exhibitors</a>, finally breaking pre-pandemic records, and it solidly felt like the old days of the conference in terms of breadth of attendees, the technical program, and overall engagement and interaction across the community.</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>This was the second time I've attended the conference as a vendor instead of a customer, and this meant I spent a fair amount of time running to and from meetings instead of walking the show floor or attending technical sessions. I'm sure I missed some major announcements and themes as a result, but I thought it still might be valuable to contribute my observations based on this narrow lens of an AI-minded storage product manager for a major cloud service provider. If you're interested in a more well-rounded perspective, check out the <a href=\"https://hpc.social/news/2023/supercomputing-23-summary/\">HPC Social Supercomputing 2023 Summary</a> and contribute your own thoughts!</p><p><span></span></p><p></p><p>I don't know the best way to organize the notes that I took, so I grouped them into a few broad categories:</p><p></p><ol><li>Big news on the Top500</li><li>What's new in storage for HPC and AI</li><li>The emergence of pure-play GPU clouds</li><li>Other technological dribs and drabs</li><li>Personal thoughts and reflections on the conference and community</li></ol><p>I must also disclose that I am employed by Microsoft and I attended SC23 in that capacity. However, everything in this post is my own personal viewpoint, and my employer had no say in what I did or didn't write here. Everything below is written from my perspective as an enthusiast, not an employee, although my day job probably colors my outlook on the HPC industry.</p><p>With all that being said, let's dive into the big news of the week!</p><p></p><h2>Big news on the Top500</h2><div>Unveiling the new Top500 list is the tentpole event of SC every year regardless of how much people (including myself!) deride HPL, and unlike the lists over the past year, this newest listing had two big surprises. Many of us went into the SC23 season wondering if the Aurora system, whose <a href=\"https://www.intc.com/news-events/press-releases/detail/1631/aurora-supercomputer-blade-installation-complete\">hardware was delivered this past June</a>, would be far enough in installation and shakeout to unseat Frontier as the second listed exascale system. At the same time, nobody had expected another &gt;500 PF supercomputer to appear on the list, much less one operated privately and for-profit. But both systems made big debuts in the top 5, carrying with them interesting implications.</div><h3>The new #2: Argonne's Aurora</h3><p>The Aurora exascale system has a storied history going back to 2015; first conceived of as a 180 PF supercomputer to be delivered in 2018, it evolved into a GPU-based exascale supercomputer that was supposed to land in 2021. Now two years late and a few executives short, Intel and Argonne were stuck between a rock and a hard place in choosing whether to list their HPL results at SC23:</p><p></p><ol><li>If Aurora <u>wasn't</u> listed on SC23's Top500 list, it risked <a href=\"https://www.llnl.gov/article/46161/llnl-hpe-partner-amd-el-capitan-projected-worlds-fastest-supercomputer\">going up against El Capitan</a> at ISC'24 and being completely overshadowed by the simultaneous launch of a newer, bigger exascale system.</li><li>If Aurora <u>was</u> listed at SC23's Top500 list but in an incomplete form, it would fall short of its long-awaited debut as the #1 system and would require a careful narrative to avoid being seen as a failed system.</li></ol><p>Intel and Argonne ultimately chose option #2 and listed an HPL run that used only 5,439 of Aurora's 10,624 nodes (51.1% of the total machine), and as expected, people generally understood that this sub-exaflop score was not an indictment of the whole system underdelivering, but more a reflection that the system was still not stable at its full scale. Still, <a href=\"https://www.theregister.com/2023/11/13/aurora_top500_no2/\">headlines in trade press were dour</a>, and there was general confusion about how to extrapolate Aurora's HPL submission to the full system.  Does the half-system listing of 585.34 PF Rmax at 24.7 MW power mean that the full system will require 50 MW to achieve an Rmax that's still lower than Frontier? Why is the efficiency (Rmax/Rpeak = 55%) so low?</p><p>Interestingly, about half the people I talked to thought that Argonne should've waited until ISC'24 to list the full system, and the other half agreed that listing half of Aurora at SC'23 was the better option. Clearly there was no clearly right answer here, and I don't think anyone can fault Argonne for doing the best they could given the Top500 submission deadline and the state of the supercomputer. In talking to a couple folks from ALCF, I got the impression that there's still plenty of room to improve the score since their HPL run was performed under a time crunch, and there were known issues affecting performance that couldn't have been repaired in time. With any luck, Aurora will be ready to go at full scale for ISC'24 and have its moment in the sun in Hamburg.</p><p></p><h3>The new #3: Microsoft's Eagle</h3><p>The other new Top500 entry near the top of the list was Eagle, Microsoft's surprise 561 PF supercomputer. Like Aurora, it is composed of GPU-heavy nodes, and like Aurora, the HPL run utilized only part (1,800 nodes) of the full system. Unlike Aurora though, the full size of Eagle is not publicly disclosed by Microsoft, and its GPU-heavy node architecture was designed for one specific workload: training large language models for generative AI.</p><p>At the <a href=\"https://sc23.conference-program.com/presentation/?id=bof155&amp;sess=sess314\">Top500 BOF</a>, Prabhat Ram gave a brief talk about Eagle where he emphasized that the system wasn't a custom-built, one-off stunt machine. Rather, it was built from publicly available <a href=\"https://learn.microsoft.com/en-us/azure/virtual-machines/nd-h100-v5-series\">ND H100 v5 virtual machines</a> on a single 400G NDR InfiniBand fat tree fabric, and Microsoft had one of the physical ND H100 v5 nodes at its booth.  Here's the back side of it:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>From top to bottom, you can see it has eight E1.S NVMe drives, 4x OSFP ports which support 2x 400G NDR InfiniBand each, a Microsoft SmartNIC, and a ton of power.  A view from the top shows the HGX baseboard and fans:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p><br />&lt;p&gt;Logically, this node (and the ND H100 v5 VM that runs on it) looks a lot like the NVIDIA DGX reference architecture. Physically, it is an air-cooled, Microsoft-designed OCP server, and Eagle’s Top500 run used 1,800 of these servers.&lt;/p&gt;</p><p>Big HPL number aside, the appearance of Eagle towards the top of Top500 has powerful implications on the supercomputing industry at large.  Consider the following.</p><p>Microsoft is a for-profit, public enterprise whose success is ultimately determined by how much money it makes for its shareholders. Unlike government agencies who have historically dominated the top of the list to show their supremacy in advancing science, the Eagle submission shows that there is now a huge financial incentive to build giant supercomputers to train large language models. This is a major milestone in supercomputing; up to this point, the largest systems built by private industry have come from the oil &amp; gas industry, and they have typically deployed at scales below the top 10.</p><p>Eagle is also built on the latest and greatest technology--NVIDIA's H100 and NDR InfiniBand--rather than previous-generation technology that's already been proven out by the national labs.  SC23 was the first time Hopper GPUs have appeared anywhere on the Top500 list, and Eagle is likely the single largest installation of both H100 and NDR InfiniBand on the planet. Not only does this signal that it's financially viable to stand up a leadership supercomputer for profit-generating R&amp;D, but industry is now willing to take on the high risk of deploying systems using untested technology if it can give them a first-mover advantage.</p><p>Eagle also shows us that the potential upside of bringing a massive new AI model to market is worth both the buying all the infrastructure required to build a half-exaflop system <i>and</i> hiring the talent required to shake out what is literally a world-class supercomputer. And while the US government can always obtain a <a href=\"https://www.bis.doc.gov/index.php/other-areas/strategic-industries-and-economic-security-sies/defense-priorities-a-allocations-system-program-dpas\">DPAS rating</a> to ensure it gets dibs on GPUs before AI companies can, there is no DPAS rating for hiring skilled individuals to stand up gigantic systems. This all makes me wonder: if Aurora was a machine sitting in some cloud data center instead of Argonne, and its commissioning was blocking the development of the next GPT model, would it have been able to take the #1 spot from Frontier this year?</p><p>The appearance of such a gigantic system on Top500, motivated by and paid for as part of the AI land grab, also raises some existential questions for the US government. What role should the government have in the supercomputing industry if private industry now has a strong financial driver to invest in the development of leadership supercomputing technologies? Historically, government has always incubated cutting-edge HPC technologies so that they could stabilize enough to be palatable to commercial buyers. Today's leadership supercomputers in the national labs have always wound up as tomorrow's midrange clusters that would be deployed for profit-generating activities like seismic imaging or computer-aided engineering. If the AI industry is now taking on that mantle of incubating and de-risking new HPC technologies, perhaps government now needs to focus on ensuring that the technologies developed and matured for AI can still be used to solve scientific problems.</p><p></p><h2>What's new in storage for HPC and AI?</h2><div>Since I spent much of my career working in HPC storage, and I now focus largely on AI, it should be no surprise that I heard a lot about the intersection of AI and storage.  AI remains high in the hype cycle, so it's natural that just about every storage vendor and discussion had some talk of AI forced into it regardless of it was really relevant or not. However, there were a few places where AI and storage topics intersect that I found noteworthy.</div><h3>The AI-storage echo chamber</h3><p><span></span>&lt;p&gt;I was asked a lot of questions about storage from journalists, VCs, and even trusted colleagues that followed a common theme: What storage technologies for AI excite me the most? What’s the future of storage for AI?&lt;/p&gt;</p><p>I don't fault people for asking such a broad question because the HPC/AI storage industry is full of bombastic claims. For example, two prominent storage vendors emblazoned their booths with claims of what their products could do for AI:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>These photos illustrate the reality that, although there is general agreement that good storage is needed for GPUs and AI, what constitutes \"good storage\" is muddy and confusing. Assuming the above approach to marketing (10x faster! 20x faster!) is effective for someone out there, there appears to be a market opportunity in just capitalizing on this general confusion by (1) asserting what the I/O problem that's jamming up all AI workloads is, and (2) showing that your storage product does a great job at solving that specific problem.<br /></p><p>For example, the MLPerf Storage working group recently announced the first <a href=\"https://mlcommons.org/2023/06/introducing-the-mlperf-storage-benchmark-suite/\">MLPerf Storage benchmark</a>, and Huiho Zheng from Argonne (co-author of the underlying <a href=\"https://ieeexplore.ieee.org/document/9499416\">DLIO tool</a> on which MLPerf Storage was built) described how the MLPerf Storage benchmark reproduces the I/O characteristics of model training at the <a href=\"https://sc23.conference-program.com/presentation/?id=misc289&amp;sess=sess437\">Workshop on Software and Hardware Co-Design of Deep Learning Systems in Accelerators</a>:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>When I saw this premise, I was scratching my head--my day job is to develop new storage products to meet the demands of large-scale AI model training and inferencing, and I have never had a customer come to me claiming that they need support for small and sparse I/O or random access. In fact, write-intensive checkpointing and fine-tuning, not read-intensive data loading, is the biggest challenge faced by those training large language models in my experience. It wasn't until a few slides later did I realize where these requirements may be coming from:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>Storage and accelerator vendors are both defining and solving the I/O problems of the AI community which seems counterproductive--shouldn't a benchmark be set by the practitioners and not the solution providers?</p><p>What I learned from talking to attendees, visiting storage vendor booths, and viewing talks like Dr. Zheng's underscores a reality that I've faced on my own work with production AI workloads: <b>AI doesn't actually have an I/O performance problem, so storage vendors are struggling to define ways in which they're relevant in the AI market</b>.</p><p>I outlined the ways in which LLM training uses storage in <a href=\"https://sc23.conference-program.com/presentation/?id=bof212&amp;sess=sess354\">my HDF5 BOF talk</a>, and their needs are easy to meet with some local storage and basic programming. So easy, in fact, that a reasonably sophisticated AI practitioner can duct tape their way around I/O problems very quickly and move on to harder problems. There's no reason for them to buy into a sophisticated Rube Goldberg storage system, because it still won't fundamentally get them away from having to resort to local disk to achieve the scalability needed to train massive LLMs.</p><p>So yes, I've got no doubt that there are storage products that can deliver 10x or 20x higher performance for some specific AI workload. And MLPerf Storage is probably an excellent way to measure that 20x performance boost. But the reality I've experienced is that a half a day of coding will deliver 19x higher performance when compared to the most naive approach, and every AI practitioner knows and does this already. That's why there are a lot of storage vendors fishing in this AI storage pond, but none of them seem to be reeling in any whoppers.</p><p>This isn't to say that there's nothing interesting going on in high-performance storage though. If the most common question I was asked was \"what's the future of storage for AI,\" the second most common question was \"what do you think about VAST and WEKA?\"</p><h3>VAST &amp; WEKA</h3><p>Both companies seem to be doing something right since they were top of mind for a lot of conference attendees, and it probably grinds their respective gears that the field still groups them together in the same bucket of \"interesting parallel storage systems that we should try out.\" Rather than throw my own opinion in the pot though (I work with and value both companies and their technologies!), I'll note the general sentiments I observed.</p><p>WEKA came into the week riding high on their big win as <a href=\"https://www.weka.io/company/weka-newsroom/press-releases/weka-named-u2s-official-technology-partner-ahead-of-achtung-baby-shows/\">U2's official technology partner</a> in September. Their big booth attraction was a popular Guitar Hero game and leaderboard, and an oversized Bono, presumably rocking out to how much he loves WEKA, presided over one of their seating areas:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>Much of their marketing centered around accelerating AI and other GPU workloads, and the feedback I heard from the WEKA customers I bumped into during the week backed this up. One person shared that the WEKA client does a great job with otherwise difficult small-file workloads, particularly common in life sciences workloads, and this anecdote is supported by the appearance of a <a href=\"https://www.weka.io/blog/hpc/hot-take-real-customers-real-benchmarks/\">very fast WEKA cluster owned by MSK Cancer Center on the IO500 Production list</a>. People also remarked about WEKA's need for dedicated CPU cores and local storage to deliver the highest performance; this, combined with its client scalability, lends itself well to smaller clusters of fat GPU nodes. I didn't run into anyone using WEKA in the cloud though, so I assume the feedback I gathered had a bias towards more conventional, on-prem styles of architecting storage for traditional HPC.</p><p>Whereas WEKA leaned into its rock 'n' roll theme this year, VAST doubled down on handing out the irresistibly tacky light-up cowboy hats they introduced last year (which I'm sure their neighbors at the DDN booth absolutely loved). They were all-in on promoting their new identity as a \"data platform\" this year, and although I didn't hear anyone refer to VAST as anything but a file system, I couldn't throw a rock without hitting someone who either recently bought a VAST system or tried one out.</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>Unlike last year though, customer sentiment around VAST wasn't all sunshine and rainbows, and I ran into a few customers who described their presales engagements as more formulaic than the white-glove treatment everyone seemed to be getting a year ago. This isn't surprising; there's no way to give all customers the same royal treatment as a business scales. But it does mean that the honeymoon period between VAST and the HPC industry is probably at an end, and they will have to spend the time between now and SC24 focusing on consistent execution to maintain the momentum they've gotten from the light-up cowboy hats.</p><p>The good news for VAST is that they've landed some major deals this past year, and they came to SC with customers and partners in-hand. They co-hosted a standing-room-only party with CoreWeave early in the week and shared a stage with Lambda at a customer breakfast, but they also highlighted two traditional, on-prem HPC customers (TACC and NREL) at the latter event.</p><p>VAST clearly isn't letting go of the on-prem HPC market as it also pursues partnerships with emerging GPU cloud service providers; this contrasted with WEKA's apparent focus on AI, GPUs, and the cloud. Time will tell which strategy (if either, or both) proves to be the better approach.</p><h3>DAOS</h3><div>Though commercial buyers were definitely most interested in VAST and WEKA, folks from the more sophisticated HPC shops around the world also tossed a few questions about DAOS my way this year.</div><p>I usually make it a point to attend the annual DAOS User Group meeting since it is always attended by all the top minds in high-performance I/O research, but I had to miss it this year on account of it running at the same time as my I/O tutorial. Fortunately, DAOS was pervasive throughout the conference, and there was no shortage of opportunity to find out what the latest news in the DAOS was. For example, check out the lineup for <a href=\"https://www.pdsw.org/index.shtml\">PDSW 2023</a> this year:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>Three out of thirteen talks were about DAOS which is more than any other single storage product or project. DAOS also won big at this year's IO500, taking the top two spots in the production storage system list:</p><p><br />&lt;div class=\"separator\" style=\"clear: both; text-align: center;\"&gt;&lt;/div&gt;</p><p>In fact, DAOS underpinned every single new awardee this year, and DAOS is now the second most represented storage system on the list behind Lustre:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>Why is DAOS at the top of so many people's minds this year? Well, DAOS reached a few major milestones in the past few months which has thrust it into the public eye.  </p><p>First, Aurora is finally online and running jobs, and while the compute system is only running at half its capability, the full DAOS system (<a href=\"https://io500.org/submissions/configuration/688\">all 220 petabytes of it</a>, all of which is TLC NVMe) is up and running--a testament to the scalability of DAOS that many parallel storage systems--including VAST and WEKA--have not publicly demonstrated. Because DAOS is open-source software and Aurora is an open-science system, all of DAOS' at-scale warts are also on full display to the community in a way that no competitive storage system besides of Lustre is.</p><p>Second, Google Cloud cast a bold vote of confidence in DAOS by launching <a href=\"https://cloud.google.com/parallelstore\">Parallelstore, its high-performance parallel file service based on DAOS</a>, in August. Whereas AWS and Azure have bet on Lustre to fill the high-performance file gap (via FSx Lustre and Azure Managed Lustre), GCP has planted a stake in the ground by betting that DAOS will be the better foundation for a high-performance file service for HPC and AI workloads.</p><p>Parallelstore is still in private preview and details are scant, but GCP had DAOS and Parallelstore dignitaries at all the major storage sessions in the technical program to fill in the gaps. From what I gathered, Parallelstore is still in its early stages and is intended to be a fast scratch tier; it's using DRAM for metadata which means it relies on erasure coding across servers to avoid data loss on a single server reboot, and there's no way to recover data if the whole cluster goes down at once. This lack of durability makes it ineligible for the IO500 list right now, but the upcoming metadata-on-NVMe feature (which previews in upstream DAOS in 1H2024) will be the long-term solution to that limitation.</p><p>Finally, the third major bit of DAOS news was about the formation of the <a href=\"https://foundation.daos.io/\">DAOS Foundation</a>. First announced earlier this month, this initiative lives under the umbrella of the Linux Foundation and is led by its five founding members:</p><p></p><ul><li><b>Argonne National Laboratory</b>, who has a vested interest in seeing DAOS endure given its massive investment in it,</li><li><b>Enakta Labs</b>, a company spun out of <a href=\"https://croit.io/\">Croit</a>, a German storage services company that was contributing feature development to DAOS,</li><li><b>Google Cloud</b>, who has made a big bet on DAOS as the underpinnings for its Parallelstore service,</li><li><b>HPE</b>, who has a shared fate with the DAOS installation at Argonne and who has also been contributing feature development, and</li><li><b>Intel</b>, whose engineers largely developed DAOS as part of the Aurora program.</li></ul><p></p><p>I see this handoff of DAOS from Intel to this new foundation as a positive change that makes DAOS a more stable long-term bet; should Intel choose to divest itself of DAOS once its obligations to the Aurora program end, DAOS now can live on without the community having to fork it. The DAOS Foundation is somewhat analogous to OpenSFS (one of the nonprofits backing Lustre) in that it is a vendor-neutral organization around which the DAOS community can gather.</p><p>But unlike OpenSFS, the DAOS Foundation will also assume the responsibility of releasing new versions of DAOS after Intel releases its final version (2.6) in March 2024. The DAOS Foundation will also steer feature prioritization, but seeing as how the DAOS Foundation doesn't fund developers directly, it's not clear that contributors like Intel or GCP are actually at the mercy of the foundation's decisions. It's more likely that the DAOS Foundation will just have authority to decide what features will roll up into the next formal DAOS release, and developers contributing code to DAOS will still prioritize whatever features their employers tell them to.</p><div><p>So, DAOS was the talk of the town at SC23. Does this all mean that DAOS is ready for prime time?</p><p>While Intel and Argonne may say yes, the community seems to have mixed feelings.  Consider this slide presented by László Szűcs from LRZ at the <a href=\"https://sc23.conference-program.com/presentation/?id=bof131&amp;sess=sess392\">DAOS Storage Community BOF</a>:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>DAOS is clearly crazy fast and scales to hundreds of petabytes in production--Aurora's IO500 listing proves that. However, that performance comes with a lot of complexity that is currently being foisted on application developers, end-users, and system administrators. The \"opportunities\" listed in László's slide are choices that people running at leadership HPC scale may be comfortable making, but the average HPC user is not equipped to make many of these decisions and make thoughtful choices about container types and library interfaces.</p><p>The fact that DAOS was featured so prominently at PDSW--a research workshop--probably underscores this as well. This <a href=\"https://sc23.conference-program.com/presentation/?id=ws_pdswwip104&amp;sess=sess435\">slide presented by Adrian Jackson's lighting talk</a> sums up the complexity along two different dimensions:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>His results showed that your choice of DAOS object class and I/O library atop the DAOS POSIX interface can result in wildly different checkpoint bandwidth. It's hard enough to teach HPC users about getting optimal performance out of a parallel file system like Lustre; I can't imagine those same users will embrace the idea that they should be mindful of which object class they use as they generate data.</p><p>The other <a href=\"https://sc23.conference-program.com/presentation/?id=ws_pdsw111&amp;sess=sess435\">DAOS-related research talk, presented by Greg Eisenhauer</a>, was a full-length paper that caught me by surprise and exposed how much performance varies when using different APIs into DAOS. This slide is one of many that highlighted this:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>I naively thought that the choice of native userspace API (key-value or array) would have negligible effects on performance, but Eisenhauer's talk showed that this isn't true. The reality appears to be that, although DAOS is capable of handling unaligned writes better than Lustre, aligning arrays on large, power-of-two boundaries still has a significant performance benefit.</p><p>Based on these sorts of technical talks about DAOS presented this year, the original question--is DAOS ready for prime time--can't be answered with a simple yes or no yet.  The performance it offers is truly best in class, but achieving that performance doesn't come easy right now. Teams who are already putting heroic effort into solving a high-value problems will probably leap at the opportunity to realize the I/O performance that DAOS can deliver. Such high value problems include things like training the next generation of foundational LLMs, and GCP's bet on DAOS probably adds differentiable value to their platform as a place to train such models as efficiently as possible. But the complexity of DAOS at present probably limits its appeal to the highest echelons of leadership HPC and AI, and I think it'll be a while before DAOS is in a place where a typical summer intern will be able to appreciate its full value.</p><h3>Infinia</h3></div><p>It would be unfair of me to give all this regard to WEKA, VAST, and DAOS without also mentioning <a href=\"https://www.ddn.com/press-releases/ddn-launches-infinia-next-generation-software-defined-storage-for-enterprise-ai-and-cloud-needs-everywhere-and-all-at-once/\">DDN's brand new Infinia product, launched right before SC23</a>. Those in the HPC storage industry have been awaiting its launch for years now, but despite the anticipation, it really didn't come up in any conversations in which I was involved. I did learn that the engineering team developing Infinia inside DDN is completely separate from the Whamcloud team who is developing Lustre, but this could be a double-edged sword. On the good side, it means that open-source Lustre development effort isn't competing with DDN's proprietary product in engineering priorities on a day-to-day basis. On the bad side though, I still struggle to see how Infinia and Lustre can avoid eventually competing for the same business.</p><p>For the time being, Infinia does seem to prioritize more enterprisey features like multitenancy and hands-free operation while Lustre is squarely aimed at delivering maximum performance to a broadening range of workloads. Their paths may eventually cross, but that day is probably a long way off, and Lustre has the benefit of being deeply entrenched across the HPC industry.</p><h2>The emergence of pure-play GPU clouds</h2><p>In addition to chatting with people about what's new in storage, I also went into SC23 wanting to understand how other cloud service providers are structuring end-to-end solutions for large-scale AI workloads. What I didn't anticipate was how many smaller cloud service providers (CSPs) showed up to SC for the first time this year, all waving the banner of offering NVIDIA H100 GPUs. These are predominantly companies that either didn't exist a few years ago or have historically focused on commodity cloud services like virtual private servers and managed WordPress sites, so it was jarring to suddenly see them at an HPC conference. How did so many of these smaller CSPs suddenly become experts in deploying GPU-based supercomputers in the time between SC22 and SC23? </p><p>I got to talking to a few folks at these smaller CSPs to figure out exactly what they were offering to customers, and their approach is quite different from how AWS, Azure, and GCP operate. Rather than defining a standard cluster architecture and deploying copies of it all over to be consumed by whoever is willing to pay, these smaller CSPs deploy clusters of whitebox GPU nodes to customer specification and sell them as dedicated resources for fixed terms. If a customer wants a bunch of HGX H100s interconnected with InfiniBand, that's what they get. If they want RoCE, the CSP will deploy that instead. And the same is true with storage: if a customer wants EXAScaler or Weka, they'll deploy that too.</p><p>While this is much closer to a traditional on-prem cluster deployment than a typical elastic, pay-as-you-go infrastructure-as-a-service offering, this is different from being a fancy colo. The end customer still consumes those GPUs as a cloud resource and never has to worry about the infrastructure that has to be deployed behind the curtain, and when the customer's contract term is up, their cluster is still owned by the CSP. As a result, the CSP can either resell that same infrastructure via pay-as-you-go or repurpose it for another dedicated customer. By owning the GPUs and selling them as a service, these CSPs can also do weird stuff like take out giant loans to build more data centers using GPUs as collateral. Meanwhile, NVIDIA can sell GPUs wholesale to these CSPs, book the revenue en masse, and let the CSPs deal with making sure they're maintained in production and well utilized.</p><p>It also seems like the services that customers of these smaller CSPs get is often more barebones than what they'd get from a Big 3 CSP (AWS, Azure, and GCP). They get big GPU nodes and an RDMA fabric, but managed services beyond that are hit and miss.</p><p>For example, one of these smaller CSPs told me that most of their storage is built on hundreds of petabytes of open-source Ceph. Ceph fulfills the minimum required storage services that any cloud must provide (object, block, and file), but it's generally insufficient for large-scale model training. As a result, all the smaller CSPs with whom I spoke said they are also actively exploring VAST and Weka as options for their growing GPU-based workloads. Since both VAST and Weka offer solid S3 and file interfaces, either could conceivably act as the underpinnings of these GPU clouds' first-party storage services as well.</p><p>As I said above though, it seems like the predominant model is for these CSPs to just ship whatever dedicated parallel storage the customer wants if something like Ceph isn't good enough. This, and the growing interest in storage from companies like VAST and Weka, suggest a few things:</p><p></p><ul><li>Some of these CSPs have been obtaining and deploying GPUs faster than they've had time to think about the end-to-end experience, and customers have so much pent-up demand for GPUs that they're willing to either work with whatever third-party storage vendor is brought to the table or take on the responsibility of choosing their preferred storage vendor themselves.</li><li>Having giant piles of GPUs is necessary, but not sufficient, to have a competitive offering in the GPU cloud services landscape. A credible platform for AI training must also have an integrated high-performance storage service.</li><li>It is looking like many pure-play GPU clouds are finding it more cost-effective to buy their way out of high-performance storage problems through partnerships than build and manage their own services atop open-source software like Lustre or DAOS.</li></ul><p>None of these observations are terribly surprising; at the price these smaller CSPs are offering GPUs compared to the Big 3 CSPs, their gross margin (and therefore their ability to invest in developing services on top of their IaaS offerings) has got to be pretty low. In the short term, it's cheaper and easier to deploy one-off high-performance storage systems alongside dedicated GPU clusters based on customer demand than develop and support a standard solution across all customers.</p><p>Of course, building a low-cost GPU service opens the doors for other companies to develop their own AI services on top of inexpensive GPU IaaS that is cost-competitive with the Big 3's native AI platforms (AWS SageMaker, Azure Machine Learning, and Google AI Platform). For example, I chatted with some folks at <a href=\"https://www.together.ai/\">together.ai</a>, a startup whose booth caught my eye with its bold claim of being \"the fastest cloud for [generative] AI:\"</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>Contrary to their banner, they aren't a cloud; rather, they provide AI services--think inferencing and fine-tuning--that are accessible through an API much like <a href=\"https://platform.openai.com/docs/guides/fine-tuning\">OpenAI's API</a>. They've engineered their backend stack to be rapidly deployable on any cloud that provides basic IaaS like GPU-equipped VMs, and this allows them to actually run their computational backend on whatever cloud can offer the lowest-cost, no-frills GPU VMs. In a sense, companies like together.ai develop and sell the frills that these new GPU CSPs lack, establishing a symbiotic alternative to the vertically integrated AI platforms on bigger clouds.</p><p>I did ask a few of these smaller CSPs what their overall pitch was. Why I would choose GPU cloud X over their direct competitor GPU cloud Y? The answers went in two directions:</p><p></p><ol><li>They offer lower cost per GPU hour than their competition</li><li>They are faster to get GPUs off a truck and into production than their competition</li></ol><p>There's a big caveat here: I didn't talk to many representatives at these CSPs, so my sample size was small and not authoritative. However, taking these value propositions at face value struck me as being quite precarious since their value is really a byproduct of severe GPU shortages driven by the hyped-up AI industry. What happens to these CSPs (and the symbionts whose businesses depend on them) when AMD GPUs appear on the market in volume? What happens if NVIDIA changes course and, instead of peanut-buttering its GPUs across CSPs of all sizes, it focuses its attention on prioritizing deliveries to just a few blessed CSPs?</p><p>There is <a href=\"https://a16z.com/who-owns-the-generative-ai-platform/\">no moat around generative AI</a>, and I left SC23 feeling like there's a dearth of long-term value being generated by some of these smaller GPU CSPs. For those CSPs whose primary focus is buying and deploying as many GPUs in as short a time as possible, not everyone can survive. They'll either come out of this GPU shortage having lost a lot of money building data centers that will go unused, or they'll be sold for parts.</p><p>More importantly to me though, I learned that I should give less credence to the splashy press events of hot AI-adjacent startups if their successes lie exclusively with smaller GPU CSPs. Some of these CSPs are paying to make their problems go away in an effort to keep their focus on racking and stacking GPUs in the short term, and I worry that there's a lack of long-term vision and strong opinions in some of these companies. Some of these smaller CSPs seem much more like coin-operated GPU cluster vending machines than platform providers, and that business model doesn't lend itself to making big bets and changing the industry.</p><p>Put another way, my job--both previous and current--has always been to think beyond short-term band aids and make sure that my employer has a clear and opinionated view of the technical approach that will be needed to address the challenges of HPC ten years in the future. I know who my peers are at the other Big 3 CSPs and leadership computing facilities across the world, and I know they're thinking hard about the same problems that I am. What worries me is that I do <u>not</u> know who my peers are at these smaller CSPs, and given their speed of growth and smaller margins, I worry that they aren't as prepared for the future as they will need to be. The AI industry as a whole will be better off when GPUs are no longer in such short supply, but the ecosystem surrounding some of these smaller GPU CSPs is going to take some damage when that day comes.</p><p></p><p></p><h2>Other dribs and drabs</h2><div>I also had a lot of interesting conversations and noticed a few subtle themes last week that don't neatly fit into any other category, but I'd love to hear more from others if they noticed the same or have more informed opinions.</div><h3>APUs and superchips - are they really that useful?</h3><p>Because I spent my booth duty standing next to one of Eagle's 8-way HGX H100 nodes, a lot of people asked me if I thought the Grace Hopper superchip would be interesting. I'm not an expert in either GPUs or AI, but I did catch up with a few colleagues who are smarter than me in this space last week, and here's the story as I understand it:</p><p>The Grace Hopper superchip (let's just call it GH100) is an evolution of the architecture developed for Summit, where V100 GPUs were cache-coherent with the CPUs through a special widget that converted NVLink to the on-chip coherence protocol for Power9. With GH100, the protocol used to maintain coherence across the CPU is directly compatible with the ARM AMBA coherence protocol, eliminating one bump in the path that Power9+V100 had. Grace also has a much more capable memory subsystem and NOC that makes accessing host memory from the GPU more beneficial.</p><p>Now, do AI workloads really need 72 cores per H100 GPU? Probably not.</p><p>What AI (and HPC) will need are some high-performance cores to handle all the parts of application execution that GPUs are bad at--divergent code paths, pointer chasing, and I/O. Putting capable CPU cores (Neoverse V2, not the N2 used in CPUs like new <a href=\"https://www.tomshardware.com/news/microsoft-azure-maia-ai-accelerator-cobalt-cpu-custom\">Microsoft's Cobalt 100</a>) on a capable NOC that is connected to the GPU memory subsystem at 900 GB/s opens doors for using hierarchical memory to train LLMs in clever ways.</p><p>For example, naively training an LLM whose weights and activations are evenly scattered across both host memory and GPU memory won't go well since that 900 GB/s of NVLink C2C would be on the critical path of many computations. However, techniques like <a href=\"https://lightning.ai/docs/pytorch/1.4.4/advanced/advanced_gpu.html#fairscale-activation-checkpointing\">activation checkpointing</a> could become a lot more versatile when the cost of offloading certain tensors from GPU memory is so much lower. In essence, the presence of easily accessible host memory will likely allow GPU memory to be used more efficiently since the time required to transfer tensors into and out of HBM is easier to hide underneath other computational steps during training.</p><p>Pairing an over-specified Grace CPU with a Hopper GPU also allows the rate of GPU development to proceed independently of CPU development. Even if workloads that saturate an H100 GPU might not also need all 72 cores of the Grace CPU, H200 or other future-generation GPUs can grow into the capabilities of Grace without having to rev the entire superchip.</p><p>I didn't get a chance to talk to any of my colleagues at AMD to get their perspective on the MI300 APU, but I'd imagine their story is a bit simpler since their memory space is flatter than NVIDIA's superchip design. This will make training some models undoubtedly more straightforward but perhaps leave less room for sophisticated optimizations that can otherwise cram more of a model into a given capacity of HBM. I'm no expert though, and I'd be happy to reference any explanations that real experts can offer! </p><h3>What about quantum?</h3><p>Quantum computing has been a hot topic for many years of SC now, but it feels like a topic that is finally making its way out of pure CS research and into the minds of the everyday HPC facility leaders. I talked to several people last week who asked me for my opinion on quantum computing because they have come to the realization that they need to know more about it than they do, and I have to confess, I'm in the same boat as they are. I don't follow quantum computing advancements very closely, but I know an increasing number of people who do--and they're the sort who work in CTOs' offices and have to worry about risks and opportunities more than intellectual curiosities.</p><p>It's hard to say there've been any seismic shifts in the state of the art in quantum computing at SC23; as best I can tell, there's still a rich ecosystem of venture capital-backed startups who keep cranking out more qubits. But this year felt like the first year where HPC facilities who haven't yet started thinking about their position on quantum computing are now behind. Not everyone needs a quantum computer, and not everyone even needs a quantum computing researcher on staff. But everyone should be prepared with a strong point of view if they are asked \"what will you be doing with quantum computing?\" by a funding agency or chief executive.</p><h3>NextSilicon</h3><p>One of the least-stealthy stealth-mode startups in the HPC industry has been NextSilicon, a company who debuted from stealth mode at SC23, launched their new Maverick accelerator, and announced their first big win with <a href=\"https://www.sandia.gov/research/2023/11/09/sandia-partners-with-nextsilicon-and-penguin-solutions-to-deliver-first-of-its-kind-runtime-reconfigurable-accelerator-technology/\">Sandia National Lab's Vanguard II project</a>. </p><p>What's notable about NextSilicon is that, unlike just about every other accelerator startup out there, they are not trying to go head-to-head with NVIDIA in the AI acceleration market. Rather, they've created a dataflow accelerator that aims to accelerate challenging HPC workloads that GPUs are particularly bad at--things like irregular algorithms and sparse data structures. They've paired this hardware with a magical runtime that continually optimizes the way the computational kernel is mapped to the accelerator's reconfigurable units to progressively improve the throughput of the accelerator as the application is running.</p><p>The concept of dataflow accelerators has always been intriguing since they're the only alternative to improving computational throughput besides making larger and larger vectors. The challenge has always been that these accelerators are more like FPGAs than general-purpose processors, and they require similar amounts of hardcore CS expertise to use well. NextSilicon claims to have cracked that nut with their runtime, and it seems like they're hiring the rights sorts of people--real HPC with respectable pedigrees--to make sure their accelerator can really deliver value to HPC workloads.</p><h3>I/O benchmarking developments</h3><p>At the <a href=\"https://sc23.conference-program.com/presentation/?id=bof144&amp;sess=sess363\">IO500 BOF</a>, there was rich discussion about adding new benchmarking modes to IOR and IO500 to represent a wider range of patterns.</p><p>More specifically, there's been an ongoing conversation about including a 4K random read test, and it sounds like the most outspoken critics against it have finally softened their stance. I've not been shy about why I think using <a href=\"https://glennklockwood.blogspot.com/2021/10/iops-are-dumb.html\">IOPS as a measure of file system performance is dumb</a>, but 4K random IOPS do establish a lower bound of performance for what a real application might experience. Seeing as how <a href=\"https://www.glennklockwood.com/benchmarks/io500.html#interpreting-results\">IO500 has always been problematic as any representation of how a file system will perform in real-world environments</a>, adding the option to run a completely synthetic, worst-case workload will give IO500 the ability to define a complete bounding box around the lower and upper limits of I/O performance for a file system.</p><p>Hendrik Nolte from GWDG also proposed a few new and appealing IOR modes that approach more realistic workload scenarios.  The first was a new locally random mode where data is randomized within IOR segments but segments are repeated:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>Compared to globally randomized reads (which is what IOR normally does), this is much closer representation of parallel workloads that are not bulk-synchronous; for example, NCBI BLAST uses thread pools and work sharing to walk through files, and the resulting I/O pattern is similar to this new mode.</p><p>He also described a proposal to run concurrent, mixed workloads in a fashion similar to how fio currently works.  Instead of performing a bulk-synchronous parallel write followed by a bulk-synchronous parallel read, his proposal would allow IOR to perform reads and writes concurrently, more accurately reflecting the state of multitenant storage systems. I actually wrote <a href=\"https://github.com/glennklockwood/iopup\">a framework to do exactly this and quantify the effects of contention using IOR and elbencho</a>, but I left the world of research before I could get it published. I'm glad to see others seeing value in pursuing this idea.</p><p>The other noteworthy development in I/O benchmarking was presented by <a href=\"https://sc23.conference-program.com/presentation/?id=bof108&amp;sess=sess411\">Sven Breuner at the Analyzing Parallel I/O BOF</a> where he described a new netbench mode for his excellent <a href=\"https://github.com/breuner/elbencho\">elbencho benchmark tool</a>. This netbench mode behaves similarly to iperf in that it is a network-level throughput test, but because it is part of elbencho, it can generate the high-bandwidth incasts and broadcasts that are typically encountered between clients and servers of parallel storage systems:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>This is an amazing development because it makes elbencho a one-stop shop for debugging the entire data path of a parallel storage system. For example, if you're trying to figure out why the end-to-end performance of a file system is below expectation, you can use elbencho to test the network layer, the object or file layer, the block layer, and the overall end-to-end path separately to find out which layer is underperforming. Some file systems have specialized included tools to perform the same network tests (e.g., <a href=\"https://github.com/IBM/SpectrumScale_NETWORK_READINESS/blob/master/nsdperf.C\">nsdperf for IBM Spectrum Scale</a>), but elbencho now has a nice generic way to generate these network patterns for any parallel storage system.</p><h2>Some personal thoughts</h2><p>As with last year, I couldn't attend most of the technical program due to a packed schedule of customer briefings and partner meetings, but the <a href=\"https://sc23.supercomputing.org/attend/digital-experience/\">SC23 Digital Experience</a> was excellently done, and I wound up watching a lot of the content I missed during the mornings and after the conference (at 2x speed!). In that sense, the hybrid nature of the conference is making it easier to attend as someone who has to juggle business interests with technical interests; while I can't jump into <a href=\"https://sc23.conference-program.com/presentation/?id=bof131&amp;sess=sess392\">public arguments about the definition of storage \"QOS\"</a>, I can still tell that my old friends and colleagues are still fighting the good fight and challenging conventional thinking across the technical program.</p><h3>My Parallel I/O in Practice tutorial</h3><p>This was the sixth year that I co-presented the <a href=\"https://sc23.conference-program.com/presentation/?id=tut134&amp;sess=sess243\">Parallel I/O in Practice tutorial</a> with my colleagues Rob Latham, Rob Ross, and Brent Welch. A <a href=\"https://scphoto.passgallery.com/-sc23/gallery\">conference photographer got this great photo</a> of me in the act:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>Presenting this tutorial is always an incredibly gratifying experience; I've found that sharing what I know is one of the most fulfilling ways I can spend my time, and being able to start my week in such an energizing way is what sustains the sleep deprivation that always follows. Giving the tutorial is also an interesting window into what the next generation of I/O experts is worrying about; for example, we got a lot of questions and engagement around the low-level hardware content in our morning half, and the I/O benchmarking material in the late afternoon seemed particularly well received. The majority of attendees came from the systems side rather than the user/dev side as well, perhaps suggesting that the growth in demand for parallel storage systems (and experts to run them) is outstripping the demand for new ways to perform parallel I/O. Guessing wildly, perhaps this means new developers are coming into the field higher up the stack, using frameworks like <a href=\"https://pypi.org/project/fsspec/\">fsspec</a> that abstract away low-level I/O.</p><p>Since I've jumped over to working in industry, it's been hard to find the business justification to keep putting work hours into the tutorial despite how much I enjoy it.  I have to confess that I didn't have time to update any of the slides I presented this year even though the world of parallel I/O has not remained the same, and I am going to have to figure out how to better balance these sorts of community contributions with the demands of a day job in the coming years.</p><h3>An aside on COVID safety</h3><p>At SC22, I fastidiously wore a KN95 mask while indoors and avoided all after-hours events and indoor dining to minimize my risk of catching COVID. At that time, neither my wife nor I had ever gotten COVID before, and I had no desire to bring it home to my family since my father died of COVID-related respiratory failure two years prior. Staying fully masked at SC22 turned out to be a great decision at the time since a significant number of other attendees, including many I spoke with, contracted COVID at SC22. By comparison, I maintained my COVID-free streak through 2022.</p><p>This year I took a more risk-tolerant approach for two reasons:</p><p></p><ol><li>My wife and I both broke our streaks this past summer and contracted COVID while on vacation, so if I got sick, we knew what to expect, and</li><li>I got my gazillionth COVID and flu shots in October in anticipation of attending SC.</li></ol><p>Part of my approach to managing risk was bringing my trusty <a href=\"https://aranet.com/products/aranet4/\">Aranet4 CO2 sensor</a> with me so that I could be aware of areas where there was air circulation and the risk of contracting an airborne illness would be higher. I only wore a KN95 at the airport gates and while on the airplane at SC23, and despite going in all-in on after-hours events, indoor dining, and copious meetings and tours of booth duty, I'm happy to report that I made it through the conference without getting sick.</p><p>I have no doubt that being vaccinated helped, as I've had several people tell me they tested positive for COVID after we had dinner together in Denver. But it's also notable that the Denver Convention Center had <u>much</u> better ventilation than Kay Bailey Hutchison Convention Center in Dallas where SC22 was held last year. To show this quantitatively, let's compare air quality measurements from SC22 to SC23.</p><p>My schedule for the day on which I give my tutorial is always the same: the tutorial runs from 8:30am to 5:00pm with breaks at 10:00, 12:00, and 3:00. Because of this consistent schedule, comparing the CO2 readings (which are a proxy for re-breathed air) for my tutorial day at SC22 versus SC23 shows how different the air quality was in the two conference centers. Here's what that comparison looks like:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>What the plot shows is that CO2 (re-breathed air) steadily increased at the start of the tutorial at both SC22 and SC23, but Denver's convention center kicked on fresh air ventilation after an hour while Dallas simply didn't. Air quality remained poor (over 1,000) throughout the day in Dallas, whereas Denver was pretty fresh (below 700) even during the breaks and the indoor luncheon. This relatively good air circulation inside the convention center at SC23 made me much more comfortable about going maskless throughout the week.</p><p>This isn't to say that I felt there was no risk of getting sick this year; there was at least one busy, upscale restaurant/bar in which I dined where the air circulation was no better than in a car or airplane. For folks who just don't want to risk being sick over Thanksgiving, wearing a mask and avoiding crowded bars was probably still the best option this year. And fortunately, Denver's weather was gorgeous, so outdoor dining was completely viable during the week.</p><h3>AI's effects on the HPC community</h3><p>Although AI has played a prominent role in previous SC conferences, this was the first year where I noticed that the AI industry is bleeding into the HPC community in weird ways.</p><p>For example, I had a bunch of journalists and media types accost me and start asking rather pointed questions while I was on booth duty. Talking to journalists isn't entirely unusual since I've always been supportive of industry press, but the social contract between practitioners like me and journalists has always been pretty formal--scheduling a call in advance, being invited to speak at an event, and things like that have long been the norm. If I was being interviewed on the record, I knew it.</p><p>This year though, it seemed like there was a new generation of younger journalists who approached me no differently than a casual booth visitor. Some did introduce themselves as members of the press after we got chatting (good), but others did not (not good) which led me to take away a learning: check names and affiliations before chatting with strangers, because the days where I could assume that all booth visitors would act in good faith are gone.</p><p>Now, why the sudden change?  I can think of three possible reasons:</p><p></p><ol><li>I'm getting older, and there are now tech industry journalists who are younger than me and think I am worth talking to since I've always been around. Maybe the old-school HPC folks that predate me have always had to deal with this.</li><li>The proliferation of platforms like Substack make it financially viable to be an independent journalist, and conversely, anyone can be a journalist without editorial oversight.</li><li>The spotlight on the massive AI industry is also illuminating the HPC industry. HPC and AI are both built on the same foundational technologies (GPUs, RDMA fabrics, HBM, and the like) so AI journalists now have a reason to start showing up at HPC community events.</li></ol><p>It'd be fair to argue that #3 is a stretch and that this isn't an AI phenomenon if not for the fact that I was also accosted by a few venture capitalists for the first time this year. HPC has never been an industry that attracted the attention of venture capital in the way that AI does, so I have to assume being asked specific questions about the viability of some startup's technology is a direct result of the AI market opportunity.</p><p>While it's nice to have a broader community of attendees and more media coverage, the increasing presence of AI-focused media and VC types in the SC community means I can't be as open and honest as I once was. Working for a corporation (with secrets of its own to protect) doesn't help there either, so maybe getting cagier when talking to strangers is just a part of growing up.</p><p></p><h3>SC23 as a milestone year</h3><p>Attending SC23 this year coincided with two personal milestones for me as well.</p><p>This is the tenth year I've been in the HPC business, and the first SC I ever attended was SC13.  I can't say that this is my eleventh SC because I didn't attend in 2014 (on account of working at a biotech startup), but I've been to SC13, SC15 through SC19, SC20 and SC21 virtually, and SC22 and SC23 in-person.  At SC13 ten years ago, the weather was a lot colder:</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><p>But I still have the fondest memories of that conference because it that was the week where I felt like I had finally found my community after having spent a decade as an unhappy materials science student.</p><p>SC23 is also a milestone year because it may be the last SC I attend as a storage and I/O guy. I recently signed on for a new position within Microsoft to help architect the next generation of supercomputers for AI, and I'll probably have to trade in the time I used to spend at workshops like PDSW for opportunities to follow the latest advancements in large-scale model training, RDMA fabrics, and accelerators. But I think I am OK with that.</p><p>I never intended to become an I/O or storage expert when I first showed up at SC13; it wasn't until I joined NERSC that I found that I could learn and contribute the most by focusing on storage problems. The world has changed since then, and now that I'm at Microsoft, it seems like the problems faced at the cutting edge of large language models, generative AI, and the pursuit of AGI are where the greatest need lies. As I said earlier in this post, AI has bigger problems to deal with than storage and I/O, and those bigger problems are what I'll be chasing. With any luck, I'll be able to say I had a hand in designing the supercomputers that Microsoft builds after Eagle. And as has been true for my last ten years in this business, I'll keep sharing whatever I learn with whoever wants to know.</p>",
            "url": "https://hpc.social/personal-blog/2023/sc-23-recap/",
            
            
            
            
            
            "date_published": "2023-11-23T08:05:00-07:00",
            "date_modified": "2023-11-23T08:05:00-07:00",
            
                "author": "Glenn K. Lockwood's Blog"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2023/advanced-lsf-resource-connector-configuration-on-ibm-cloud-part-i/",
            "title": "Advanced LSF resource connector configuration on IBM Cloud - part I",
            "summary": null,
            "content_text": "OverviewThis is the first in a series of blogs that discusses some advancedconfiguration of the IBM LSF resource connector. LSF resource connector enablesLSF clusters to borrow resources from supported resource providers in thecloud. LSF includes resource connectors for the following resource providers:IBM CloudAWSGoogle Cloud PlatformMicrosoft AzureMicrosoft Azure CycleCloudRed Hat OpenShiftOpenStackThe resource connector plug-ins for LSF are available under an open sourcelicense (the Apache License 2.0) on the public IBM Spectrum Computing githubhere.LSF resource connector works in conjunction with the LSF multiclustercapability to create a flexible and dynamic hybrid HPC cloud. LSF multiclusterenables organizations to have multiple LSF clusters connect with one anotherand to define queues which can forward to remote clusters and receive jobsfrom remote clusters. Historically LSF multicluster was used by clients whohave multiple, geographically dispersed compute centres and it allowed them toconnect these environments and have work forwarded between them. Naturallythis can also be used to setup an LSF cluster in the cloud and tie it in toyour existing on-premises LSF cluster. According to a recent Hyperion Researchwhitepaper, the most widely adopted framework for leveraging HPC resources inthe cloud is in a hybrid environment where a user runs their HPC workloadsboth on-premises and in the cloud.1Cloud templatesAs part of the configuration of the LSF resource connector, it’s necessary todefine templates which are used to specify a specific cloud instance type. Inother words, the template is used to define a set of hosts with commonattributes including memory and number of cores and operating system image.These templates are used by LSF when requesting instances from a particularcloud to satisfy the workload demands.In this post, we’ll take a closer look at configuring multiple LSF resourceconnector templates for IBM Cloud as the resource provider. By default, whenmultiple templates are configured, LSF will sort the candidate templateservers alphabetically by template name. However, administrators may wish tosort the templates according to a specific priority. For example, anorganization may want to assign a high priority to a template corresponding toa less costly instance type. When priorities are specified for templates, LSFwill use high priority templates first.The environment used in this example was deployed using the IBM Cloudautomation for LSF. Using Terraform/IBM Cloud Schematics, it’s possible toautomatically deploy a fully functioning LSF cluster in about 10 minutes time.This includes a login node, NFS storage node, LSF manager node(s) andLSF Application Center. The automation also configures the LSF resourceconnector for the instance type specified at deployment time. More detailedinformation can be found about deploying LSF on IBM Cloud here.The following steps assume that LSF has been deployed on IBM Cloud. Note thatthis is a single LSF cluster and not a hybrid cloud that has been configured.Now, we’ll look at the following configuration examples:Specifying multiple templates for different cloud instance types with different prioritiesUpdate LSF resource connector user script to load required OS packages on compute nodesMultiple templatesThe LSF deployment automation on IBM Cloud configures a single template forthe compute VSI type specified at deployment time. In the example below, wesee a single template configured for VSI profile type bx2-4x16. A completelist of available IBM Cloud VPC VSI types can be found here.ibmcloudgen2_templates.json (obfuscated){    \"templates\": [        {            \"templateId\": \"Template-1\",            \"maxNumber\": 2,            \"attributes\": {                \"type\": [\"String\", \"X86_64\"],                \"ncores\": [\"Numeric\", \"2\"],                \"ncpus\": [\"Numeric\", \"4\"],                \"mem\": [\"Numeric\", \"16384\"],                \"icgen2host\": [\"Boolean\", \"1\"]            },            \"imageId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"subnetId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"vpcId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"vmType\": \"bx2-4x16\",            \"securityGroupIds\": [\"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\"],            \"resourceGroupId\": \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\",            \"sshkey_id\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"region\": \"us-east\",            \"zone\": \"us-east-1\"        }    ]}Defining prioritiesYou’ll note that there is no priority specified for the template. When nopriority is defined, LSF sorts the templates according to template name. Next,we’ll define a second template in the configuration for the VSI instance typemx2-16x128. At the same time, we’ll introduce the priority parameter andspecify a priority of 10 (higher) for bx2-4x16, and 5 (lower) formx2-16x128. More details regarding the parameters can be found here. With this configuration, LSF will favour and use the higher prioritytemplate first, which in this case will be for VSI instance type bx2-4x16.ibmcloudgen2_templates.json (obfuscated, with priorities configured){    \"templates\": [        {            \"templateId\": \"Template-1\",            \"maxNumber\": 2,            \"attributes\": {                \"type\": [\"String\", \"X86_64\"],                \"ncores\": [\"Numeric\", \"2\"],                \"ncpus\": [\"Numeric\", \"4\"],                \"mem\": [\"Numeric\", \"16384\"],                \"icgen2host\": [\"Boolean\", \"1\"]             },            \"imageId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"subnetId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"vpcId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"vmType\": \"bx2-4x16\",            \"securityGroupIds\": [\"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\"],            \"resourceGroupId\": \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\",            \"sshkey_id\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            **\"priority\": \"10\",**             \"region\": \"us-east\",            \"zone\": \"us-east-1\"         },       {            \"templateId\": \"Template-2\",            \"maxNumber\": 2,            \"attributes\": {                \"type\": [\"String\", \"X86_64\"],                \"ncores\": [\"Numeric\", \"8\"],                \"ncpus\": [\"Numeric\", \"16\"],                \"mem\": [\"Numeric\", \"131072\"],                \"icgen2host\": [\"Boolean\", \"1\"]            },            \"imageId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"subnetId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"vpcId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"vmType\": \"mx2-16x128\",            \"userData\":\"profile=mx2_16x128\",            \"\"securityGroupIds\": [\"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\"],            \"resourceGroupId\": \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\",            \"sshkey_id\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            **\"priority\": \"5\",**            \"region\": \"us-east\",            \"zone\": \"us-east-1\"        }    ]}Modifying compute server optionsNext, we’ll submit some example jobs to the LSF cluster and observe how theLSF resource connector template priority influences the startup of resourcesby the LSF resource connector. The example job we wish to run is the OSsupplied stress command. stress is not installed in the default computeimages and can be added as part of the startup of the compute servers via theLSF resource connector user_data.sh script. This script is used to setenvironment variables, and control the startup of LSF on the compute servers.It can also be used to perform customization of the environment, and pass LSFresources to the compute servers. The user_data.sh script is located in:/opt/ibm/lsf/conf/resource_connector/ibmcloudgen2.Modifying the user_data.sh script, I’ve inserted the line to install the OSstress package before the LSF daemons startup.user_data.sh......**# Install stress utility****dnf install stress -y**cat $LSF_CONF_FILE  &gt;&gt; $logfilesleep 5lsf_daemons start &amp;sleep 5lsf_daemons status &gt;&gt; $logfileecho END `date '+%Y-%m-%d %H:%M:%S'` &gt;&gt; $logfile# Allow login as lsfadminnfs_mount_dir=\"data\"mkdir -p /home/lsfadmin/.sshcp /mnt/data/ssh/authorized_keys /home/lsfadmin/.ssh/authorized_keyscat /mnt/data/ssh/id_rsa.pub &gt;&gt; /root/.ssh/authorized_keyschmod 600 /home/lsfadmin/.ssh/authorized_keyschmod 700 /home/lsfadmin/.sshchown -R lsfadmin:lsfadmin /home/lsfadmin/.sshecho \"MTU=9000\" &gt;&gt; \"/etc/sysconfig/network-scripts/ifcfg-eth0\"systemctl restart NetworkManager......With the updates made to the user_data.sh script, we’re now ready to submitjobs to LSF. We submit two stress jobs as follows:[lsfadmin@icgen2host-10-241-0-37 ~]$ bsub -q normal /usr/bin/stress --cpu 1 --vm-bytes 8192MB --timeout 60sJob &lt;2836&gt; is submitted to queue &lt;normal&gt;.[lsfadmin@icgen2host-10-241-0-37 ~]$ bsub -q normal /usr/bin/stress --cpu 1 --vm-bytes 8192MB --timeout 60sJob &lt;2837&gt; is submitted to queue &lt;normal&gt;.After a few moments, the LSF resource connector automatically starts up asingle compute server on the IBM Cloud with hostname icgen2host-10-241-0-42to satisfy the pending workload requirements. Note that the hostname prefixicgen2host stands for IBM Cloud Generation 2 VPC. The numeric portion of thehostname represents the IP address of the server that was automaticallystarted by the LSF resource connector. Therefore, this hostname may differin your environment. Compute server icgen2host-10-241-0-42 is equipped with4 cores and 16 GB RAM, matching the higher priority template bx2-4x16.[lsfadmin@icgen2host-10-241-0-37 ~]$ lshosts -wHOST_NAME                       type       model  cpuf ncpus maxmem maxswp server RESOURCESicgen2host-10-241-0-37        X86_64    Intel_E5  12.5     4  15.4G      -    Yes (mg)icgen2host-10-241-0-42        X86_64    Intel_E5  12.5     4  15.5G      -    Dyn (icgen2host)[lsfadmin@icgen2host-10-241-0-37 ~]$ bhosts -rc -wHOST_NAME          STATUS          JL/U    MAX  NJOBS    RUN  SSUSP  USUSP    RSV RC_STATUS             PROV_STATUS           UPDATED_AT             INSTANCE_ID               icgen2host-10-241-0-37 closed_Full     -      0      0      0      0      0      0           -                     -                     -                      -               icgen2host-10-241-0-42 ok              -      4      2      2      0      0      0 Allocated             running               2023-11-06T23:02:26UTC 0757_4f5295bd-a265-4fda-840c-6f89e326ca1f As there is no other work that has been submitted to the LSF cluster, once thestress jobs have completed, the LSF resource connector will automaticallyshut down the compute servers according to the LSB_RC_EXTERNAL_HOST_IDLE_TIMEparameter in lsf.conf. This defines the time interval after which the LSFresource connector will relinquish the cloud instances if no jobs are running.ConclusionWe’ve just scratched the surface in terms of the configuration possibilitieswith LSF resource connector. In the next article we’ll look at how LSFresources can be assigned to servers which are dynamically started by the LSFresource connector, as well as configuring Docker to point to a localrepository.The Evolution of HPC Includes Strong Use of Hybrid Cloud&#160;&#x21a9;&#xfe0e;",
            "content_html": "<p><strong>Overview</strong></p><p>This is the first in a series of blogs that discusses some advancedconfiguration of the IBM LSF resource connector. LSF resource connector enablesLSF clusters to borrow resources from supported resource providers in thecloud. LSF includes resource connectors for the following resource providers:</p><ul><li>IBM Cloud</li><li>AWS</li><li>Google Cloud Platform</li><li>Microsoft Azure</li><li>Microsoft Azure CycleCloud</li><li>Red Hat OpenShift</li><li>OpenStack</li></ul><p>The resource connector plug-ins for LSF are available under an open sourcelicense (the Apache License 2.0) on the public IBM Spectrum Computing github<a href=\"https://github.com/IBMSpectrumComputing/cloud-provider-plugins\">here</a>.</p><p>LSF resource connector works in conjunction with the LSF multiclustercapability to create a flexible and dynamic hybrid HPC cloud. LSF multiclusterenables organizations to have multiple LSF clusters connect with one anotherand to define queues which can forward to remote clusters and receive jobsfrom remote clusters. Historically LSF multicluster was used by clients whohave multiple, geographically dispersed compute centres and it allowed them toconnect these environments and have work forwarded between them. Naturallythis can also be used to setup an LSF cluster in the cloud and tie it in toyour existing on-premises LSF cluster. According to a recent Hyperion Researchwhitepaper, the most widely adopted framework for leveraging HPC resources inthe cloud is in a hybrid environment where a user runs their HPC workloadsboth on-premises and in the cloud.<sup id=\"fnref:1\"><a class=\"footnote-ref\" href=\"https://www.gaborsamu.com/blog/index.xml#fn:1\">1</a></sup></p><p><strong>Cloud templates</strong></p><p>As part of the configuration of the LSF resource connector, it’s necessary todefine templates which are used to specify a specific cloud instance type. Inother words, the template is used to define a set of hosts with commonattributes including memory and number of cores and operating system image.These templates are used by LSF when requesting instances from a particularcloud to satisfy the workload demands.</p><p>In this post, we’ll take a closer look at configuring multiple LSF resourceconnector templates for IBM Cloud as the resource provider. By default, whenmultiple templates are configured, LSF will sort the candidate templateservers alphabetically by template name. However, administrators may wish tosort the templates according to a specific priority. For example, anorganization may want to assign a high priority to a template corresponding toa less costly instance type. When priorities are specified for templates, LSFwill use high priority templates first.</p><p>The environment used in this example was deployed using the IBM Cloudautomation for LSF. Using Terraform/IBM Cloud Schematics, it’s possible toautomatically deploy a fully functioning LSF cluster in about 10 minutes time.This includes a login node, NFS storage node, LSF manager node(s) andLSF Application Center. The automation also configures the LSF resourceconnector for the instance type specified at deployment time. More detailedinformation can be found about deploying LSF on IBM Cloud <a href=\"https://cloud.ibm.com/docs/ibm-spectrum-lsf?topic=ibm-spectrum-lsf-getting-started-tutorial\">here</a>.</p><p>The following steps assume that LSF has been deployed on IBM Cloud. Note thatthis is a single LSF cluster and not a hybrid cloud that has been configured.Now, we’ll look at the following configuration examples:</p><ul><li>Specifying multiple templates for different cloud instance types with different priorities</li><li>Update LSF resource connector user script to load required OS packages on compute nodes</li></ul><p><strong>Multiple templates</strong></p><p>The LSF deployment automation on IBM Cloud configures a single template forthe compute VSI type specified at deployment time. In the example below, wesee a single template configured for VSI profile type <em>bx2-4x16</em>. A completelist of available IBM Cloud VPC VSI types can be found <a href=\"https://cloud.ibm.com/docs/vpc?topic=vpc-profiles&amp;interface=ui\">here</a>.</p><p><strong>ibmcloudgen2_templates.json (obfuscated)</strong><div class=\"highlight\"><pre><code class=\"language-plaintext\">{    \"templates\": [        {            \"templateId\": \"Template-1\",            \"maxNumber\": 2,            \"attributes\": {                \"type\": [\"String\", \"X86_64\"],                \"ncores\": [\"Numeric\", \"2\"],                \"ncpus\": [\"Numeric\", \"4\"],                \"mem\": [\"Numeric\", \"16384\"],                \"icgen2host\": [\"Boolean\", \"1\"]            },            \"imageId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"subnetId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"vpcId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"vmType\": \"bx2-4x16\",            \"securityGroupIds\": [\"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\"],            \"resourceGroupId\": \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\",            \"sshkey_id\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"region\": \"us-east\",            \"zone\": \"us-east-1\"        }    ]}</code></pre></div></p><p><strong>Defining priorities</strong></p><p>You’ll note that there is no priority specified for the template. When nopriority is defined, LSF sorts the templates according to template name. Next,we’ll define a second template in the configuration for the VSI instance type<em>mx2-16x128</em>. At the same time, we’ll introduce the priority parameter andspecify a priority of 10 (higher) for <em>bx2-4x16</em>, and 5 (lower) for<em>mx2-16x128</em>. More details regarding the parameters can be found <a href=\"https://www.ibm.com/docs/en/spectrum-lsf/10.1.0?topic=reference-ibmcloudgen2-templatesjson\">here</a>. With this configuration, LSF will favour and use the higher prioritytemplate first, which in this case will be for VSI instance type <em>bx2-4x16</em>.</p><p><strong>ibmcloudgen2_templates.json (obfuscated, with priorities configured)</strong></p><div class=\"highlight\"><pre><code class=\"language-plaintext\">{    \"templates\": [        {            \"templateId\": \"Template-1\",            \"maxNumber\": 2,            \"attributes\": {                \"type\": [\"String\", \"X86_64\"],                \"ncores\": [\"Numeric\", \"2\"],                \"ncpus\": [\"Numeric\", \"4\"],                \"mem\": [\"Numeric\", \"16384\"],                \"icgen2host\": [\"Boolean\", \"1\"]             },            \"imageId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"subnetId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"vpcId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"vmType\": \"bx2-4x16\",            \"securityGroupIds\": [\"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\"],            \"resourceGroupId\": \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\",            \"sshkey_id\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            **\"priority\": \"10\",**             \"region\": \"us-east\",            \"zone\": \"us-east-1\"         },       {            \"templateId\": \"Template-2\",            \"maxNumber\": 2,            \"attributes\": {                \"type\": [\"String\", \"X86_64\"],                \"ncores\": [\"Numeric\", \"8\"],                \"ncpus\": [\"Numeric\", \"16\"],                \"mem\": [\"Numeric\", \"131072\"],                \"icgen2host\": [\"Boolean\", \"1\"]            },            \"imageId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"subnetId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"vpcId\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            \"vmType\": \"mx2-16x128\",            \"userData\":\"profile=mx2_16x128\",            \"\"securityGroupIds\": [\"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\"],            \"resourceGroupId\": \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\",            \"sshkey_id\": \"aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff\",            **\"priority\": \"5\",**            \"region\": \"us-east\",            \"zone\": \"us-east-1\"        }    ]}</code></pre></div><p><strong>Modifying compute server options</strong></p><p>Next, we’ll submit some example jobs to the LSF cluster and observe how theLSF resource connector template priority influences the startup of resourcesby the LSF resource connector. The example job we wish to run is the OSsupplied <em>stress</em> command. <em>stress</em> is not installed in the default computeimages and can be added as part of the startup of the compute servers via theLSF resource connector user_data.sh script. This script is used to setenvironment variables, and control the startup of LSF on the compute servers.It can also be used to perform customization of the environment, and pass LSFresources to the compute servers. The user_data.sh script is located in:<em>/opt/ibm/lsf/conf/resource_connector/ibmcloudgen2</em>.</p><p>Modifying the <em>user_data.sh</em> script, I’ve inserted the line to install the OS<em>stress</em> package before the LSF daemons startup.</p><p><strong>user_data.sh</strong></p><div class=\"highlight\"><pre><code class=\"language-plaintext\">......**# Install stress utility****dnf install stress -y**cat $LSF_CONF_FILE  &gt;&gt; $logfilesleep 5lsf_daemons start &amp;sleep 5lsf_daemons status &gt;&gt; $logfileecho END `date '+%Y-%m-%d %H:%M:%S'` &gt;&gt; $logfile# Allow login as lsfadminnfs_mount_dir=\"data\"mkdir -p /home/lsfadmin/.sshcp /mnt/data/ssh/authorized_keys /home/lsfadmin/.ssh/authorized_keyscat /mnt/data/ssh/id_rsa.pub &gt;&gt; /root/.ssh/authorized_keyschmod 600 /home/lsfadmin/.ssh/authorized_keyschmod 700 /home/lsfadmin/.sshchown -R lsfadmin:lsfadmin /home/lsfadmin/.sshecho \"MTU=9000\" &gt;&gt; \"/etc/sysconfig/network-scripts/ifcfg-eth0\"systemctl restart NetworkManager......</code></pre></div><p>With the updates made to the user_data.sh script, we’re now ready to submitjobs to LSF. We submit two <em>stress</em> jobs as follows:</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">[lsfadmin@icgen2host-10-241-0-37 ~]$ bsub -q normal /usr/bin/stress --cpu 1 --vm-bytes 8192MB --timeout 60sJob &lt;2836&gt; is submitted to queue &lt;normal&gt;.[lsfadmin@icgen2host-10-241-0-37 ~]$ bsub -q normal /usr/bin/stress --cpu 1 --vm-bytes 8192MB --timeout 60sJob &lt;2837&gt; is submitted to queue &lt;normal&gt;.</code></pre></div><p>After a few moments, the LSF resource connector automatically starts up asingle compute server on the IBM Cloud with hostname <em>icgen2host-10-241-0-42</em>to satisfy the pending workload requirements. Note that the hostname prefixicgen2host stands for IBM Cloud Generation 2 VPC. The numeric portion of thehostname represents the IP address of the server that was automaticallystarted by the LSF resource connector. Therefore, this hostname may differin your environment. Compute server <em>icgen2host-10-241-0-42</em> is equipped with4 cores and 16 GB RAM, matching the higher priority template <em>bx2-4x16</em>.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\">[lsfadmin@icgen2host-10-241-0-37 ~]$ lshosts -wHOST_NAME                       type       model  cpuf ncpus maxmem maxswp server RESOURCESicgen2host-10-241-0-37        X86_64    Intel_E5  12.5     4  15.4G      -    Yes (mg)icgen2host-10-241-0-42        X86_64    Intel_E5  12.5     4  15.5G      -    Dyn (icgen2host)[lsfadmin@icgen2host-10-241-0-37 ~]$ bhosts -rc -wHOST_NAME          STATUS          JL/U    MAX  NJOBS    RUN  SSUSP  USUSP    RSV RC_STATUS             PROV_STATUS           UPDATED_AT             INSTANCE_ID               icgen2host-10-241-0-37 closed_Full     -      0      0      0      0      0      0           -                     -                     -                      -               icgen2host-10-241-0-42 ok              -      4      2      2      0      0      0 Allocated             running               2023-11-06T23:02:26UTC 0757_4f5295bd-a265-4fda-840c-6f89e326ca1f </code></pre></div><p>As there is no other work that has been submitted to the LSF cluster, once the<em>stress</em> jobs have completed, the LSF resource connector will automaticallyshut down the compute servers according to the <strong>LSB_RC_EXTERNAL_HOST_IDLE_TIME</strong>parameter in <em>lsf.conf</em>. This defines the time interval after which the LSFresource connector will relinquish the cloud instances if no jobs are running.</p><p><strong>Conclusion</strong></p><p>We’ve just scratched the surface in terms of the configuration possibilitieswith LSF resource connector. In the next article we’ll look at how LSFresources can be assigned to servers which are dynamically started by the LSFresource connector, as well as configuring Docker to point to a localrepository.</p><section class=\"footnotes\"><hr /><ol><li id=\"fn:1\"><p><a href=\"https://www.ibm.com/downloads/cas/RGKYOOKB\">The Evolution of HPC Includes Strong Use of Hybrid Cloud</a>&#160;<a class=\"footnote-backref\" href=\"https://www.gaborsamu.com/blog/index.xml#fnref:1\">&#x21a9;&#xfe0e;</a></p></li></ol></section>",
            "url": "https://hpc.social/personal-blog/2023/advanced-lsf-resource-connector-configuration-on-ibm-cloud-part-i/",
            
            
            
            
            
            "date_published": "2023-11-08T02:21:04-07:00",
            "date_modified": "2023-11-08T02:21:04-07:00",
            
                "author": "Ramblings of a supercomputing enthusiast."
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2023/gratis-offering-risk-and-our-post-zirp-environment/",
            "title": "Gratis Offering, Risk, and our Post-ZIRP environment",
            "summary": null,
            "content_text": "(Note: This post is adapted from #170 of the Research Computing Teams Newsletter)It’s fantastic today that there’s so many free (gratis) tiers of service and packages of software, open source or otherwise, that we can use as the foundations for the computing, software, or data services we offer to our research communities.It really is! I feel that viscerally, because when I was coming of age in this community, proprietary and only barely interoperable OSes, compilers, libraries, resource managers, data platforms… were the norm, not the exception.So I’ve never taken those free offerings for granted. It’s a fantastic development that I’m constantly conscious of.In some cases, it’s a vibrant distributed community that makes these offerings possible, with each contributor putting in largely volunteer effort off the sides of their desk, each person part of a massive international collaboration made possible by nearly ubiquitous internet access.There are real downsides to putting so much responsibility on the shoulders of unpaid volunteer labour — maintainer and contributor burnout being a big one — but there are communities and packages where it works well and empirically appears sustainable.In other cases, it’s a clear business decision, with a company freely offering a software package or service (often with some open source), with paid services or support built atop. The idea here is to fund the development of the open software or services (and then some) with paid offerings.Either way, these offerings are great boons for those of us supporting research with computing, software, or data services.But people’s time is (rightly) valuable and one way or another needs to be compensated. Similarly, equipment takes both money both for the equipment itself and for paying the people who operate it.These are facts that we’re exceedingly aware of within our own teams, but too often forget when it comes to external offerings.From the second tech boom (post the pets.com bubble) and then during years of low interest rates, money had been pretty easy to come by for tech companies. That’s meant that companies (or more properly their investors) could be pretty ok with high customer acquisition costs, like generous free tiers or open source software offerings; it meant that companies didn’t mind paying for some of their people to do open source development if it helped with recruiting; and it meant that other companies could be pretty ok paying for paid tiers of offerings even if they didn’t need it, “just in case”.But now things have changed.We’ve seen this with last year’s seemingly endless rounds of layoffs in tech companies, but it started before then. Docker no longer storing unused images indefinitely for free (#37) and then charging for Docker Desktop (#90); Heroku discontinuing their free tier; a number of CI/CD offerings chopping their free tier limits way back; GitLab said (and then partly walked back) that they were going to expire dormant repos; Google messed around with free Suite/Workspace plans; and of course we remember Binder’s struggles. Lots of small but key software packages have been archived, or are simply no longer being updated. This has been building for some time.And while I was on summer hiatus, the latest RedHat licensing brouhaha erupted.That Red Hat has put another bump in the path of generating free versions their distribution has a lot of people talking, because it directly impacts our systems and plans.(A lot of that talking went along the lines of “IBM/RedHat is shooting themselves in the foot in the RCD community with this move”. My siblings in Science — why would a software or services company spend five seconds thinking about a market whose primary defining characteristic is a passionate refusal to spend money on software or services?)There’s been very little discussion I’ve seen about anything closer to home, where we have much better knowledge and (more importantly) the very real ability to change things.Crucially, only once or twice have I heard (and exclusively in private), “We had that risk logged; we already had some plans in place”.In the RCT newsletter (as well as over at Manager, PhD) I talk about risk registers and risk management principally in the context of projects. But it matters for operations, too. We’re entrusted to provide foundational resources and expertise for researchers who need data, computing, and software development. It’s our responsibility to be aware of, and consistently revisit plans for, events that could risk our ability to deliver those resources or expertise.Given that since the late 90s, free (both libre and gratis) offerings have been so much the norm that its understandable that we’ve grown to take them for granted and not view them as risky (though maybe less understandable is surprise about this particular case, after all of the discussion when CentOS transitioned to CentOS stream so recently.)But it’s always been true that depending on some other group to provide something to you for free has some degree of risk associated with it, which must be managed.And yes, it is undoubtedly true that paid offerings sometimes also get cancelled without replacement unexpectedly. This has happened to me in both my professional and private life, and I have been Greatly Displeased each time.  But we are professionals, and colleagues. Let’s not pretend to each other that this happens at remotely the same rate or with the same consequences with which free offerings disappear or fade away.People who don’t want to think about this stuff will doubtlessly uncharitably summarize what I’m writing here as “He says we should all be paying for everything or we deserve what we get”. That is emphatically not what I’m saying.Given what we do and the constraints under which we do it, yes we should continue to take advantage of free offerings and free software! It would be ridiculous not to.The evolving environment does mean that we should be very conscious when we adopt such offerings in foundational ways, and make sure we have plans and alternatives should the offerings change. It also means we should revisit these plans and mitigations periodically and update when necessary.  Mitigations could mean having backup plans including ideas for transitions; it could mean contributing to the free offering in some way (bug reports, PRs, etc.) so as to be seen to be a valuable member of the community.Regardless of how we address it: as people with significant responsibility entrusted to us, responsibility that affects researchers’ ability to advance science and scholarship, it’s important for us to be aware of the shifting risk landscape around us.We should be alive to and monitoring the risks of crucial dependencies in the services and resources we offer. That’s true whether it’s crucial team members leaving, the terms of a free tier being drastically altered, funders changing their compliance requirements, or anything else.The fact those risks exist doesn’t mean we don’t hire great people, or never use free tiers of anything, nor take money from inconstant funders (which is all of them).It just means we don’t take them for granted.RCD shares academia’s greatest weakness — refusing to acknowledge that people’s time matters and has value, even our own.That can lead to taking contributions from within our community, and from external organizations, as a given and as something that we’re entitled to.The recent RedHat issues, when combined with other recent changes to our environment, will be well worth it if it reminds us that this isn’t the case, and that the responsibility entrusted to us requires a level of clear-headedness and professionalism around the value of things we rely on, dependencies, risks, and mitigations.",
            "content_html": "<p>(Note: This post is adapted from <a href=\"https://www.researchcomputingteams.org/newsletter_issues/0170\">#170</a> of the <a href=\"https://www.researchcomputingteams.org\">Research Computing Teams Newsletter</a>)</p><p>It’s fantastic today that there’s so many free (gratis) tiers of service and packages of software, open source or otherwise, that we can use as the foundations for the computing, software, or data services we offer to our research communities.</p><p>It really is! I feel that viscerally, because when I was coming of age in this community, proprietary and only barely interoperable OSes, compilers, libraries, resource managers, data platforms… were the norm, not the exception.</p><p>So I’ve never taken those free offerings for granted. It’s a fantastic development that I’m constantly conscious of.In some cases, it’s a vibrant distributed community that makes these offerings possible, with each contributor putting in largely volunteer effort off the sides of their desk, each person part of a massive international collaboration made possible by nearly ubiquitous internet access.</p><p>There are real downsides to putting so much responsibility on the shoulders of unpaid volunteer labour — maintainer and contributor burnout being a big one — but there are communities and packages where it works well and empirically appears sustainable.</p><p>In other cases, it’s a clear business decision, with a company freely offering a software package or service (often with some open source), with paid services or support built atop. The idea here is to fund the development of the open software or services (and then some) with paid offerings.</p><p>Either way, these offerings are great boons for those of us supporting research with computing, software, or data services.</p><p>But people’s time is (rightly) valuable and one way or another needs to be compensated. Similarly, equipment takes both money both for the equipment itself and for paying the people who operate it.</p><p>These are facts that we’re exceedingly aware of within our own teams, but too often forget when it comes to external offerings.</p><p>From the second tech boom (post the pets.com bubble) and then during years of low interest rates, money had been pretty easy to come by for tech companies. That’s meant that companies (or more properly their investors) could be pretty ok with high customer acquisition costs, like generous free tiers or open source software offerings; it meant that companies didn’t mind paying for some of their people to do open source development if it helped with recruiting; and it meant that other companies could be pretty ok paying for paid tiers of offerings even if they didn’t need it, “just in case”.</p><p>But now things have changed.</p><p>We’ve seen this with last year’s seemingly endless rounds of layoffs in tech companies, but it started before then. Docker no longer storing unused images indefinitely for free (<a href=\"https://www.researchcomputingteams.org/newsletter_issues/0037\">#37</a>) and then charging for Docker Desktop (<a href=\"https://www.researchcomputingteams.org/newsletter_issues/0090\">#90</a>); Heroku discontinuing their free tier; a number of CI/CD offerings chopping their free tier limits way back; GitLab said (and then partly walked back) that they were going to expire dormant repos; Google messed around with free Suite/Workspace plans; and of course we remember Binder’s struggles. Lots of small but key software packages have been archived, or are simply no longer being updated. This has been building for some time.</p><p>And while I was on summer hiatus, the latest RedHat licensing brouhaha erupted.</p><p>That Red Hat has put another bump in the path of generating free versions their distribution has a lot of people talking, because it directly impacts our systems and plans.</p><p>(A lot of that talking went along the lines of “IBM/RedHat is shooting themselves in the foot in the RCD community with this move”. My siblings in Science — why would a software or services company spend five seconds thinking about a market whose primary defining characteristic is a passionate refusal to spend money on software or services?)</p><p>There’s been very little discussion I’ve seen about anything closer to home, where we have much better knowledge and (more importantly) the very real ability to change things.</p><p>Crucially, only once or twice have I heard (and exclusively in private), “We had that risk logged; we already had some plans in place”.</p><p>In the RCT newsletter (as well as over at <a href=\"https://www.managerphd.com\">Manager, PhD</a>) I talk about risk registers and risk management principally in the context of projects. But it matters for operations, too. We’re entrusted to provide foundational resources and expertise for researchers who need data, computing, and software development. It’s our responsibility to be aware of, and consistently revisit plans for, events that could risk our ability to deliver those resources or expertise.</p><p>Given that since the late 90s, free (both <em>libre</em> and <em>gratis</em>) offerings have been so much the norm that its understandable that we’ve grown to take them for granted and not view them as risky (though maybe less understandable is surprise about this particular case, after all of the discussion when CentOS transitioned to CentOS stream so recently.)</p><p>But it’s always been true that depending on some other group to provide something to you for free has some degree of risk associated with it, which must be managed.</p><p>And yes, it is undoubtedly true that paid offerings sometimes also get cancelled without replacement unexpectedly. This has happened to me in both my professional and private life, and I have been Greatly Displeased each time.  But we are professionals, and colleagues. Let’s not pretend to each other that this happens at remotely the same rate or with the same consequences with which free offerings disappear or fade away.</p><p>People who don’t want to think about this stuff will doubtlessly uncharitably summarize what I’m writing here as “He says we should all be paying for everything or we deserve what we get”. That is emphatically <strong>not</strong> what I’m saying.</p><p>Given what we do and the constraints under which we do it, yes we should continue to take advantage of free offerings and free software! It would be ridiculous not to.</p><p>The evolving environment <em>does</em> mean that we should be very conscious when we adopt such offerings in foundational ways, and make sure we have plans and alternatives should the offerings change. It also means we should revisit these plans and mitigations periodically and update when necessary.  Mitigations could mean having backup plans including ideas for transitions; it could mean contributing to the free offering in some way (bug reports, PRs, etc.) so as to be seen to be a valuable member of the community.</p><p>Regardless of how we address it: as people with significant responsibility entrusted to us, responsibility that affects researchers’ ability to advance science and scholarship, it’s important for us to be aware of the shifting risk landscape around us.</p><p>We should be alive to and monitoring the risks of crucial dependencies in the services and resources we offer. That’s true whether it’s crucial team members leaving, the terms of a free tier being drastically altered, funders changing their compliance requirements, or anything else.</p><p>The fact those risks exist doesn’t mean we don’t hire great people, or never use free tiers of anything, nor take money from inconstant funders (which is all of them).</p><p>It just means we don’t take them for granted.</p><p>RCD shares academia’s greatest weakness — refusing to acknowledge that people’s time matters and has value, even our own.</p><p>That can lead to taking contributions from within our community, and from external organizations, as a given and as something that we’re entitled to.</p><p>The recent RedHat issues, when combined with other recent changes to our environment, will be well worth it if it reminds us that this isn’t the case, and that the responsibility entrusted to us requires a level of clear-headedness and professionalism around the value of things we rely on, dependencies, risks, and mitigations.</p>",
            "url": "https://hpc.social/personal-blog/2023/gratis-offering-risk-and-our-post-zirp-environment/",
            
            
            
            
            
            "date_published": "2023-09-24T00:00:00-06:00",
            "date_modified": "2023-09-24T00:00:00-06:00",
            
                "author": "Jonathan Dursi's Blog"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2023/lsf-client-on-macos-submitting-from-your-laptop/",
            "title": "LSF client on macOS - submitting from your laptop",
            "summary": null,
            "content_text": "In traditional HPC environments, login nodes are typically used as an access point for users to submitand manage jobs. Although login nodes are still used today, HPC environments areincreasingly being used by a broad class of users with domain expertise and not necessarily IT experts.In other words, such users may be more comfortable using their native desktopenvironment rather than the CLI. Given the factors, in the commercial HPC space, organizations are always lookingfor ways to lower the barto access and interact with HPC environments.Spectrum LSF provides many ways to submit and manage jobs in an HPC cluster. For power users, the richCLI functionality exists. There is also an available web-based interface for jobsubmission and management which provides customizable application templates to greatly simplify job submission, while hiding complexity of the underlying infrastructure. A RESTful APIis also available to users of IBM Spectrum LSF Application Center or IBM Spectrum LSF Suites, which enables organizations to access the HPC environment via web services.I&rsquo;ve written previously in detail about the the LSF web-based interface in the blogThe Easy HPC Button. Here, we&rsquo;ll take a closer look at theavailable LSF client for macOS that uses the RESTful API. First, a bit about LSF clients. LSF clientscan access resources on LSF server hosts without running the LSF daemons. LSF clients don&rsquo;t require a softwarelicense and from clients, users can run all of the familiar LSF commands. Additionally, LSF clients aresubmit only, and don&rsquo;t execute jobs.Note: The macOS LSF client uses the LSF RESTful API. This means that it will function in environmentsrunning LSF Standard Edition with LSF Application Center or LSF Suites.ConfigurationThe configuration used for the example below is as follows:HostnameOSDetailkilencCentOS Stream 8.4LSF Suite for HPC v10.2.0.13My-Macbook-AirmacOS Ventura 13.2.1 (Apple M1)LSF clientOn the Spectrum LSF Suite for HPC management host (kilenc), add the following variables to the Parametersection in the file lsf.cluster.name. The FLOAT_CLIENTS variable determines how many floating clients canjoin the LSF cluster, The FLOAT_CLIENTS_ADDR_RANGE specifies the allowable IP addresses. In this case, theclient system is on a 192.168.x.x network.Begin ParametersFLOAT_CLIENTS=2FLOAT_CLIENTS_ADDR_RANGE=192.*End ParametersTo make the changes take effect, issue the following commands as the LSF administrator:lsadmin reconfigbadmin reconfigObtain the tarball pacdesktop_client10.2.0.13_macos-x86_64.tar. For users with an LSF entitlement this package is available onIBM Fix Central. Note that this package will work on systems with Apple M1 silicon through emulation.Open a Terminal on the macOS client system, copy the tarball to the $HOME/Desktop directory of user lsfuser and uncompress the tarball.lsfuser@My-MacBook-Air Desktop % pwd/Users/lsfuser/Desktoplsfuser@My-MacBook-Air Desktop % ls -la pacdesktop_client10.2.0.13_macos-x86_64.tar-rw-r--r--@ 1 lsfuser  staff  18452480 27 Feb 17:12 pacdesktop_client10.2.0.13_macos-x86_64.tarlsfuser@My-MacBook-Air Desktop % tar -xvf pacdesktop_client10.2.0.13_macos-x86_64.tarx LSF_Desktop_Client/x LSF_Desktop_Client/bappx LSF_Desktop_Client/btopx LSF_Desktop_Client/bwaitx LSF_Desktop_Client/lseligiblex LSF_Desktop_Client/bslax LSF_Desktop_Client/blparamsx LSF_Desktop_Client/bhpartx LSF_Desktop_Client/bclustersx LSF_Desktop_Client/blstartupx LSF_Desktop_Client/lsacctx LSF_Desktop_Client/bsubx LSF_Desktop_Client/bugroupx LSF_Desktop_Client/bpeekx LSF_Desktop_Client/bacctx LSF_Desktop_Client/brequeuex LSF_Desktop_Client/bjgroupx LSF_Desktop_Client/bslotsx LSF_Desktop_Client/lsrunx LSF_Desktop_Client/bjobsx LSF_Desktop_Client/lshostsx LSF_Desktop_Client/lsloadx LSF_Desktop_Client/brlainfox LSF_Desktop_Client/bresourcesx LSF_Desktop_Client/bladminx LSF_Desktop_Client/bstatusx LSF_Desktop_Client/bmodx LSF_Desktop_Client/bpostx LSF_Desktop_Client/lsidx LSF_Desktop_Client/bentagsx LSF_Desktop_Client/chx LSF_Desktop_Client/bchkpntx LSF_Desktop_Client/bparamsx LSF_Desktop_Client/bjdepinfox LSF_Desktop_Client/bgmodx LSF_Desktop_Client/brestartx LSF_Desktop_Client/lsltasksx LSF_Desktop_Client/blusersx LSF_Desktop_Client/paclogonx LSF_Desktop_Client/regnotifyx LSF_Desktop_Client/cacert.pemx LSF_Desktop_Client/bresumex LSF_Desktop_Client/blstatx LSF_Desktop_Client/bhistx LSF_Desktop_Client/bqueuesx LSF_Desktop_Client/bltasksx LSF_Desktop_Client/bresizex LSF_Desktop_Client/blcollectx LSF_Desktop_Client/lsacctmrgx LSF_Desktop_Client/bgaddx LSF_Desktop_Client/bmigx LSF_Desktop_Client/bstopx LSF_Desktop_Client/bswitchx LSF_Desktop_Client/blhostsx LSF_Desktop_Client/blcstatx LSF_Desktop_Client/brsvsx LSF_Desktop_Client/brunx LSF_Desktop_Client/blinfox LSF_Desktop_Client/lsgrunx LSF_Desktop_Client/busersx LSF_Desktop_Client/lsloadadjx LSF_Desktop_Client/blkillx LSF_Desktop_Client/bbotx LSF_Desktop_Client/lsclustersx LSF_Desktop_Client/bconfx LSF_Desktop_Client/lsinfox LSF_Desktop_Client/lsmakex LSF_Desktop_Client/blimitsx LSF_Desktop_Client/bmgroupx LSF_Desktop_Client/breadx LSF_Desktop_Client/bkillx LSF_Desktop_Client/lstcshx LSF_Desktop_Client/lsrtasksx LSF_Desktop_Client/README.TXTx LSF_Desktop_Client/lsplacex LSF_Desktop_Client/bhostsx LSF_Desktop_Client/paclogoutx LSF_Desktop_Client/bgdelFollowing the directions in the file README.TXT, set the environment variable LSF_DESKTOP_CLIENT=yes, and set the PATH variable accordingly.lsfuser@My-MacBook-Air LSF_Desktop_Client % export LSF_DESKTOP_CLIENT=yeslsfuser@My-MacBook-Air LSF_Desktop_Client % export PATH=`pwd`:$PATHNext, it&rsquo;s necessary to run the paclogon command to connect to the LSF Application Center (or LSF Suite installation). Here we point to the LSF server kilenc on port 8080.lsfuser@My-MacBook-Air LSF_Desktop_Client % paclogonLog on to IBM Spectrum LSF Application CenterUser account: lsfuserEnter password: Specify the URL to connect to IBM Spectrum LSF Application Center. Format: http://host_name:port_number/platform or https://host_name:port_number/platformURL: http://kilenc:8080/platformYou have successfully logged on to IBM Spectrum LSF Application Center.After successfully logging in using the paclogon command, it should be possible to run LSF &ldquo;base&rdquo; commands from the macOS terminal including lsid, lsload, lshosts.lsfuser@My-MacBook-Air LSF_Desktop_Client % lsidIBM Spectrum LSF 10.1.0.13, Apr 15 2022Suite Edition: IBM Spectrum LSF Suite for HPC 10.2.0.13Copyright International Business Machines Corp. 1992, 2016.US Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM Corp.My cluster name is KlaszterMy master name is kilenclsfuser@My-MacBook-Air LSF_Desktop_Client % lshosts -wHOST_NAME                       type       model  cpuf ncpus maxmem maxswp server RESOURCESkilenc                    LINUXPPC64LE      POWER9  25.0    32  30.7G  15.8G    Yes (mg docker)lsfuser@My-MacBook-Air LSF_Desktop_Client % lsload -wHOST_NAME               status  r15s   r1m  r15m   ut    pg  ls    it   tmp   swp   memkilenc                      ok   0.8   2.1   2.4   7%   0.0   0  1156  551M 15.6G   10GNext, run the LSF batch commands bqueues and bhosts.lsfuser@My-MacBook-Air LSF_Desktop_Client % bqueuesQUEUE_NAME      PRIO STATUS          MAX JL/U JL/P JL/H NJOBS  PEND   RUN  SUSP admin            50  Open:Active       -    -    -    -     0     0     0     0owners           43  Open:Active       -    -    -    -     0     0     0     0priority         43  Open:Active       -    -    -    - 75835 75803    32     0night            40  Open:Inact        -    -    -    -     0     0     0     0short            35  Open:Active       -    -    -    -     0     0     0     0dataq            33  Open:Active       -    -    -    -     0     0     0     0normal           30  Open:Active       -    -    -    -     0     0     0     0interactive      30  Open:Active       -    -    -    -     0     0     0     0sendq            30  Open:Active       -    -    -    -     0     0     0     0idle             20  Open:Active       -    -    -    -     0     0     0     0lsfuser@My-MacBook-Air LSF_Desktop_Client % bhostsHOST_NAME          STATUS       JL/U    MAX  NJOBS    RUN  SSUSP  USUSP    RSV kilenc             ok              -     32     19     19      0      0      0Running the bjobs will result in a warning message appearing on macOS stating: &ldquo;bjobs&rdquo; cannot be opened because the developer cannot be verified.To remedy the issue observed in step 9, click cancel on the warning message and browse to System Settings -&gt; Privacy &amp; Security -&gt; Security Settings. In the Security Settings view,you&rsquo;ll see the message: &ldquo;bjobs&rdquo; was blocked from use because it is not from an identified developer. To allow the bjobs command to execute, click on the Allow Anyway button. You willthen be promped to authenticate to make the change take effect.Run the LSF bjobs command again. You will now receive a new warning error popup indicating: macOS cannot verify the developer of &ldquo;bjobs&rdquo;. Are you sure you want to open it?. Toproceed, click on the Open button.The bjobs command will then run to completion as expected.  Subsequent executions of bjobs will run without any system warnings. Finally, to submita job, run the bsub command. Here we try to submit a simple sleep job (i.e. bsub -q normal sleep 3600). As was the case with the bjobs command, the bsub command is also blocked. Here,repeat the steps 10, 11 as described above but for the bsub command. Once the steps have been completed, repeat the bsub job submission command.Finally, to submit a job, run the bsub command. Here we try to submit a simple sleep job (i.e. bsub -q normal sleep 3600). As was the case with the bjobs command, the bsubcommand is also blocked. Here, repeat the steps 10, 11 as described above but for the bsub command. Once the steps have been completed, repeat the bsub job submission command.lsfuser@My-MacBook-Air LSF_Desktop_Client % bsub -q normal sleep 3600Job &lt;617551&gt; is submitted to queue &lt;normal&gt;.",
            "content_html": "<p>In traditional HPC environments, login nodes are typically used as an access point for users to submitand manage jobs. Although login nodes are still used today, HPC environments areincreasingly being used by a broad class of users with domain expertise and not necessarily IT experts.In other words, such users may be more comfortable using their native desktopenvironment rather than the CLI. Given the factors, in the commercial HPC space, organizations are always lookingfor ways to lower the barto access and interact with HPC environments.</p><p>Spectrum LSF provides many ways to submit and manage jobs in an HPC cluster. For power users, the richCLI functionality exists. There is also an available web-based interface for jobsubmission and management which provides customizable application templates to greatly simplify job submission, while hiding complexity of the underlying infrastructure. A RESTful APIis also available to users of IBM Spectrum LSF Application Center or IBM Spectrum LSF Suites, which enables organizations to access the HPC environment via web services.</p><p>I&rsquo;ve written previously in detail about the the LSF web-based interface in the blog<a href=\"https://www.gaborsamu.com/blog/easy_hpc/\">The Easy HPC Button</a>. Here, we&rsquo;ll take a closer look at theavailable LSF client for macOS that uses the RESTful API. First, a bit about LSF clients. LSF clientscan access resources on LSF server hosts without running the LSF daemons. LSF clients don&rsquo;t require a softwarelicense and from clients, users can run all of the familiar LSF commands. Additionally, LSF clients aresubmit only, and don&rsquo;t execute jobs.</p><p><strong>Note:</strong> The macOS LSF client uses the LSF RESTful API. This means that it will function in environmentsrunning LSF Standard Edition with LSF Application Center or LSF Suites.</p><p><strong>Configuration</strong></p><p>The configuration used for the example below is as follows:</p><table><thead><tr><th style=\"text-align: left;\">Hostname</th><th>OS</th><th>Detail</th></tr></thead><tbody><tr><td style=\"text-align: left;\"><em>kilenc</em></td><td>CentOS Stream 8.4</td><td>LSF Suite for HPC v10.2.0.13</td></tr><tr><td style=\"text-align: left;\"><em>My-Macbook-Air</em></td><td>macOS Ventura 13.2.1 (Apple M1)</td><td>LSF client</td></tr></tbody></table><ol><li>On the Spectrum LSF Suite for HPC management host (<em>kilenc</em>), add the following variables to the Parametersection in the file lsf.cluster.<em>name</em>. The FLOAT_CLIENTS variable determines how many floating clients canjoin the LSF cluster, The FLOAT_CLIENTS_ADDR_RANGE specifies the allowable IP addresses. In this case, theclient system is on a 192.168.x.x network.</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\">Begin ParametersFLOAT_CLIENTS=2FLOAT_CLIENTS_ADDR_RANGE=192.*End Parameters</code></pre></div><ol start=\"2\"><li>To make the changes take effect, issue the following commands as the LSF administrator:</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\">lsadmin reconfigbadmin reconfig</code></pre></div><ol start=\"3\"><li><p>Obtain the tarball <em>pacdesktop_client10.2.0.13_macos-x86_64.tar</em>. For users with an LSF entitlement this package is available on<a href=\"https://www.ibm.com/support/fixcentral/\">IBM Fix Central</a>. Note that this package will work on systems with Apple M1 silicon through emulation.</p></li><li><p>Open a Terminal on the macOS client system, copy the tarball to the $HOME/Desktop directory of user lsfuser and uncompress the tarball.</p></li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\">lsfuser@My-MacBook-Air Desktop % pwd/Users/lsfuser/Desktoplsfuser@My-MacBook-Air Desktop % ls -la pacdesktop_client10.2.0.13_macos-x86_64.tar-rw-r--r--@ 1 lsfuser  staff  18452480 27 Feb 17:12 pacdesktop_client10.2.0.13_macos-x86_64.tarlsfuser@My-MacBook-Air Desktop % tar -xvf pacdesktop_client10.2.0.13_macos-x86_64.tarx LSF_Desktop_Client/x LSF_Desktop_Client/bappx LSF_Desktop_Client/btopx LSF_Desktop_Client/bwaitx LSF_Desktop_Client/lseligiblex LSF_Desktop_Client/bslax LSF_Desktop_Client/blparamsx LSF_Desktop_Client/bhpartx LSF_Desktop_Client/bclustersx LSF_Desktop_Client/blstartupx LSF_Desktop_Client/lsacctx LSF_Desktop_Client/bsubx LSF_Desktop_Client/bugroupx LSF_Desktop_Client/bpeekx LSF_Desktop_Client/bacctx LSF_Desktop_Client/brequeuex LSF_Desktop_Client/bjgroupx LSF_Desktop_Client/bslotsx LSF_Desktop_Client/lsrunx LSF_Desktop_Client/bjobsx LSF_Desktop_Client/lshostsx LSF_Desktop_Client/lsloadx LSF_Desktop_Client/brlainfox LSF_Desktop_Client/bresourcesx LSF_Desktop_Client/bladminx LSF_Desktop_Client/bstatusx LSF_Desktop_Client/bmodx LSF_Desktop_Client/bpostx LSF_Desktop_Client/lsidx LSF_Desktop_Client/bentagsx LSF_Desktop_Client/chx LSF_Desktop_Client/bchkpntx LSF_Desktop_Client/bparamsx LSF_Desktop_Client/bjdepinfox LSF_Desktop_Client/bgmodx LSF_Desktop_Client/brestartx LSF_Desktop_Client/lsltasksx LSF_Desktop_Client/blusersx LSF_Desktop_Client/paclogonx LSF_Desktop_Client/regnotifyx LSF_Desktop_Client/cacert.pemx LSF_Desktop_Client/bresumex LSF_Desktop_Client/blstatx LSF_Desktop_Client/bhistx LSF_Desktop_Client/bqueuesx LSF_Desktop_Client/bltasksx LSF_Desktop_Client/bresizex LSF_Desktop_Client/blcollectx LSF_Desktop_Client/lsacctmrgx LSF_Desktop_Client/bgaddx LSF_Desktop_Client/bmigx LSF_Desktop_Client/bstopx LSF_Desktop_Client/bswitchx LSF_Desktop_Client/blhostsx LSF_Desktop_Client/blcstatx LSF_Desktop_Client/brsvsx LSF_Desktop_Client/brunx LSF_Desktop_Client/blinfox LSF_Desktop_Client/lsgrunx LSF_Desktop_Client/busersx LSF_Desktop_Client/lsloadadjx LSF_Desktop_Client/blkillx LSF_Desktop_Client/bbotx LSF_Desktop_Client/lsclustersx LSF_Desktop_Client/bconfx LSF_Desktop_Client/lsinfox LSF_Desktop_Client/lsmakex LSF_Desktop_Client/blimitsx LSF_Desktop_Client/bmgroupx LSF_Desktop_Client/breadx LSF_Desktop_Client/bkillx LSF_Desktop_Client/lstcshx LSF_Desktop_Client/lsrtasksx LSF_Desktop_Client/README.TXTx LSF_Desktop_Client/lsplacex LSF_Desktop_Client/bhostsx LSF_Desktop_Client/paclogoutx LSF_Desktop_Client/bgdel</code></pre></div><ol start=\"5\"><li>Following the directions in the file README.TXT, set the environment variable LSF_DESKTOP_CLIENT=yes, and set the PATH variable accordingly.</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\">lsfuser@My-MacBook-Air LSF_Desktop_Client % export LSF_DESKTOP_CLIENT=yeslsfuser@My-MacBook-Air LSF_Desktop_Client % export PATH=`pwd`:$PATH</code></pre></div><ol start=\"6\"><li>Next, it&rsquo;s necessary to run the <em>paclogon</em> command to connect to the LSF Application Center (or LSF Suite installation). Here we point to the LSF server kilenc on port 8080.</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\">lsfuser@My-MacBook-Air LSF_Desktop_Client % paclogonLog on to IBM Spectrum LSF Application CenterUser account: lsfuserEnter password: Specify the URL to connect to IBM Spectrum LSF Application Center. Format: http://host_name:port_number/platform or https://host_name:port_number/platformURL: http://kilenc:8080/platformYou have successfully logged on to IBM Spectrum LSF Application Center.</code></pre></div><ol start=\"7\"><li>After successfully logging in using the paclogon command, it should be possible to run LSF &ldquo;base&rdquo; commands from the macOS terminal including <em>lsid</em>, <em>lsload</em>, <em>lshosts</em>.</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\">lsfuser@My-MacBook-Air LSF_Desktop_Client % lsidIBM Spectrum LSF 10.1.0.13, Apr 15 2022Suite Edition: IBM Spectrum LSF Suite for HPC 10.2.0.13Copyright International Business Machines Corp. 1992, 2016.US Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM Corp.My cluster name is KlaszterMy master name is kilenclsfuser@My-MacBook-Air LSF_Desktop_Client % lshosts -wHOST_NAME                       type       model  cpuf ncpus maxmem maxswp server RESOURCESkilenc                    LINUXPPC64LE      POWER9  25.0    32  30.7G  15.8G    Yes (mg docker)lsfuser@My-MacBook-Air LSF_Desktop_Client % lsload -wHOST_NAME               status  r15s   r1m  r15m   ut    pg  ls    it   tmp   swp   memkilenc                      ok   0.8   2.1   2.4   7%   0.0   0  1156  551M 15.6G   10G</code></pre></div><ol start=\"8\"><li>Next, run the LSF batch commands <em>bqueues</em> and <em>bhosts</em>.</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\">lsfuser@My-MacBook-Air LSF_Desktop_Client % bqueuesQUEUE_NAME      PRIO STATUS          MAX JL/U JL/P JL/H NJOBS  PEND   RUN  SUSP admin            50  Open:Active       -    -    -    -     0     0     0     0owners           43  Open:Active       -    -    -    -     0     0     0     0priority         43  Open:Active       -    -    -    - 75835 75803    32     0night            40  Open:Inact        -    -    -    -     0     0     0     0short            35  Open:Active       -    -    -    -     0     0     0     0dataq            33  Open:Active       -    -    -    -     0     0     0     0normal           30  Open:Active       -    -    -    -     0     0     0     0interactive      30  Open:Active       -    -    -    -     0     0     0     0sendq            30  Open:Active       -    -    -    -     0     0     0     0idle             20  Open:Active       -    -    -    -     0     0     0     0lsfuser@My-MacBook-Air LSF_Desktop_Client % bhostsHOST_NAME          STATUS       JL/U    MAX  NJOBS    RUN  SSUSP  USUSP    RSV kilenc             ok              -     32     19     19      0      0      0</code></pre></div><ol start=\"9\"><li>Running the bjobs will result in a warning message appearing on macOS stating: <em>&ldquo;bjobs&rdquo; cannot be opened because the developer cannot be verified.</em></li></ol><figure><img src=\"https://www.gaborsamu.com/images/bjobs_unverified.png\" /></figure><ol start=\"10\"><li>To remedy the issue observed in step 9, click cancel on the warning message and browse to <strong>System Settings -&gt; Privacy &amp; Security -&gt; Security Settings</strong>. In the Security Settings view,you&rsquo;ll see the message: <em>&ldquo;bjobs&rdquo; was blocked from use because it is not from an identified developer.</em> To allow the bjobs command to execute, click on the <strong>Allow Anyway</strong> button. You willthen be promped to authenticate to make the change take effect.</li></ol><p><figure><img src=\"https://www.gaborsamu.com/images/bjobs_allow.png\" /></figure><figure><img src=\"https://www.gaborsamu.com/images/bjobs_authenticate.png\" /></figure></p><ol start=\"11\"><li>Run the LSF <em>bjobs</em> command again. You will now receive a new warning error popup indicating: <em>macOS cannot verify the developer of &ldquo;bjobs&rdquo;. Are you sure you want to open it?</em>. Toproceed, click on the Open button.The bjobs command will then run to completion as expected.  Subsequent executions of bjobs will run without any system warnings. Finally, to submita job, run the bsub command. Here we try to submit a simple sleep job (i.e. bsub -q normal sleep 3600). As was the case with the bjobs command, the bsub command is also blocked. Here,repeat the steps 10, 11 as described above but for the bsub command. Once the steps have been completed, repeat the bsub job submission command.</li></ol><figure><img src=\"https://www.gaborsamu.com/images/bjobs_open.png\" /></figure><ol start=\"12\"><li>Finally, to submit a job, run the <em>bsub</em> command. Here we try to submit a simple sleep job (i.e. <em>bsub -q normal sleep 3600</em>). As was the case with the <em>bjobs</em> command, the <em>bsub</em>command is also blocked. Here, repeat the steps 10, 11 as described above but for the <em>bsub</em> command. Once the steps have been completed, repeat the <em>bsub</em> job submission command.</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\">lsfuser@My-MacBook-Air LSF_Desktop_Client % bsub -q normal sleep 3600Job &lt;617551&gt; is submitted to queue &lt;normal&gt;.</code></pre></div>",
            "url": "https://hpc.social/personal-blog/2023/lsf-client-on-macos-submitting-from-your-laptop/",
            
            
            
            
            
            "date_published": "2023-03-01T19:10:58-07:00",
            "date_modified": "2023-03-01T19:10:58-07:00",
            
                "author": "Ramblings of a supercomputing enthusiast."
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2023/monitoring-ibm-spectrum-lsf-with-the-tig-stack/",
            "title": "Monitoring .-.. ... ..-. (IBM Spectrum LSF) with the TIG stack",
            "summary": null,
            "content_text": "Much like dashboards in automobiles, dashboards in the context of HPC infrastructure are crucial to get an understanding of what&rsquo;s happening under the hood of your HPC cluster - ata glance. During my IT career, I&rsquo;ve used a myriad of monitoring solutions ranging from SNMP and Ganglia, to the ELK (Elasticsearch, Logstash, Kibana) stack. For example, I&rsquo;ve recentlywritten an overview on how it is possible to visualize IBM Spectrum LSF (LSF) data in Grafana. LSF is an HPC job scheduler which brings to the table three decades of experience inworkload and resource management.For this blog, I decided to take this to the next level by monitoring IBM Spectrum LSF with the well known TIG (Telegraf, InfluxDB, Grafana) stack. This article is not meant to be adebate on the advantages of one monitoring stack over another. Rather, the focus is to demonstrate what is feasible in terms of monitoring Spectrum LSF clusters with the TIG stack,given the many available ways to query LSF for key information using CLI commands.The JourneyThere already exists many write-ups on how to deploy the TIG stack to monitor systems. This isn&rsquo;t meant to be a guide on setting up the TIG stack. Rather, it&rsquo;s assumed that the readeralready has some familiarity with the TIG stack. If not, then [insert your favourite search engine] is your friend.On my home network, I decided to setup a VM running on my trusty Traverse Ten64 running Fedora where InfluxDB was installed. The idea was to run InfluxDB on a system that is guaranteedto be always on in my home environment and that is energy efficient. Installing telegraf on all of the LSF cluster servers (x3) proved to be straight forward. Note that in all cases, I used the OSsupplied versions of InfluxDB, Telegraf. Finally, I already had a Grafana server running on a server in my network.Out of the box, Telegraf has the ability to monitor numerous system metrics. Furthermore, there exists literally hundreds of plugins for Telegraf to monitor a wide variety of devices,services and software. A search however, didn&rsquo;t reveal the existence of any plugin to monitor LSF. So it was time to get creative.What to monitor?A bit of research revealed that InfluxDB supports what is known as &ldquo;line protocol&rdquo;. This is a well defined text-based format for writing data to InfluxDB. I used the followingreference on &ldquo;line protocol&rdquo; to guide me. Using line protocol it would be ultimately possible towrite a plugin for Telegraf to effecively scrape information from Spectrum LSF and output in line protocol format for writing to InfluxDB.Before I could begin writing the plugin, the key was to determine what information from Spectrum LSF would be useful to display in the dashboard, and how that information could beextracted. For this I followed the KISS principle to keep things as simple as possible. The key metrics I decided to report on were servers, queues and jobs (oh my!), as well as processinformation for the LSF scheduler daemons. Refer to the following table for details:Metric(s)CommandLSF scheduler performance metricsbadmin perfmon view -jsonLSF available servers, CPUs, cores, slotsbadmin showstatusLSF server by status (total number Ok, closed, unreachable, unavailable)badmin showstatusLSF job statistics (total number running, suspended, pending)badmin showstatusLSF queue statistics (per queue, total number of jobs running, suspended, pending)bqueues -json -o queue_name:12 njobs pend run susp rsv ususp ssuspLSF mbatchd process metrics(Telegraf - inputs.procstat)LSF mbschd process metrics(Telegraf - inputs.procstat)LSF management lim process metrics(Telegraf - inputs.procstat)Scrapin' funThese above metrics would give a good idea of the state of the Spectrum LSF cluster at a glance. With the list of metrics prepared, the next step was to create a plugin script which wouldscrape data from the noted commands. Both bqueues and badmin perfmon view support output in JSON format with the appropriate flags specified. However, badmin showstatus does not supportoutput in JSON format. This meant that for badmin showstatus it was necessary to scrape data assuming hard coded field positions in the output.A copy of the Telegraf plugin for Spectrum LSF is provided below. This is just an example and is provided &ldquo;as is&rdquo; for testing purposes. Your mileage may vary.  Example lsf_telegraf_agent.py script. Click to expand!  #!/usr/bin/python3.8# # v0.9 # Sample inputs.exec script for Telegraf which outputs metrics from an IBM Spectrum LSF management server# in InfluxDB Line Protocol input format.## NOTE: It is required to set the lsf_envfile variable to point to the LSF profile.lsf file# for the LSF installation. ## Gabor Samu# January 4, 2023# import osimport jsonimport timeimport subprocessimport sysfrom pathlib import Path## Variable declarations# **NOTE: lsf_envfile needs to be set to point to the profile.lsf file for the LSF installation. #lsf_envfile = \"/opt/ibm/lsfsuite/lsf/conf/profile.lsf\"## Source the Spectrum LSF profile.  # Check for existing of lsf_envfile (profile.lsf) and source the environment. # If the specified file does not exist, then exit.  #path = Path(lsf_envfile)if path.is_file():     lsf_env = (f'env -i sh -c \"source {lsf_envfile} &amp;&amp; env\"')    for line in subprocess.getoutput(lsf_env).split(\"\\n\"):        key, value = line.split(\"=\")        os.environ[key]= valueelse:    sys.exit(f'The file {lsf_envfile} does not exist.')    # # Get the time in nanoseconds since the epoch. # This is required as part of the InfluxDB line protocol reference. # Only supported on Python 3.7+#time_nanosec = time.time_ns()## Here we set the LSF environment variable LSB_NTRIES. This will be used to determine the # number of retries before failure of a LSF batch command. This is used to cover the case # when the LSF mbatchd is not running. #os.environ[\"LSB_NTRIES\"] = \"2\"## Check if LSF performance metric monitoring is enabled. This is done by running# 'badmin perfmon view'. If badmin is not found, then exit. ## Check the return status from 'badmin perfmon view' and take the appropriate action:#  - If return status is 7, it means that performance monitoring is not enabled. The script#    will enable LSF performance metric monitoring by running 'badmin perfmon start'.#    Note that a 70 second sleep is required before LSF metrics will be available.  #  - If return status is 65, it means that the badmin command reported that the#    LSF batch system is down. This is a fatal error which will cause the script#    to exit. #lsf_path = os.environ['LSF_BINDIR']badmin_path = lsf_path + \"/badmin\"bqueues_path = lsf_path + \"/bqueues\"path = Path(badmin_path)if path.is_file():    cmd = [badmin_path, 'perfmon', 'view']    p = subprocess.Popen(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)    while p.poll() is None:        time.sleep(0.1)    return_code = p.returncode    if return_code == 7:        cmd = [badmin_path, 'perfmon', 'start']        p = subprocess.Popen(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)        while p.poll() is None:            time.sleep(0.1)        return_code = p.returncode        time.sleep(70)    elif return_code == 65:        sys.exit(f'The LSF batch system is down.')else:    sys.exit(f'{badmin_path} does not exist.')## Run badmin with the \"perfmon view\" keywords and the -json option to product JSON output# We assume here that the LSF batch system is responsive (a check was done above); if# the mbatchd is very busy there is a possiblity that it may not be responsive here. This# case is not considered; LSB_NTRIES setting will determine how many tries are made before# badmin gives up the ghost.  # # Note: We previously checked for the existence of the 'badmin' binary. #cmd = [badmin_path, 'perfmon', 'view', '-json'] p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL, text=True) stdout, stderr = p.communicate()## Guard for the case that the performance monitor has just been enabled, but is not# producing any data as the first sample period has not elapsed. #if stdout == \"\":    sys.exit(f'Output from badmin perfmon view -json is empty.')else:     data = json.loads(stdout)# # Run badmin showstatus# Next, run the command 'badmin showstatus' and capture the output. Note that badmin showstatus# does not produce JSON output. So here we must do some scraping of the output. # The output from 'badmin showstatus' it placed into the array 'showstatus'. The hard coded# positions in the output of 'badmin showstatus' are assumed when building the output # strings below. Should the format of the output of 'badmin showstatus' change, this will# need to be updated. cmd = [badmin_path, 'showstatus']p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL, text=True)stdout, stderr = p.communicate()# Convert badmin showstatus output into an arrayshowstatus = stdout.split()## Run bqueues#cmd = [bqueues_path, '-json', '-o', 'queue_name:12 njobs pend run susp rsv ususp ssusp']p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL, text=True)stdout, stderr = p.communicate()data_queues = json.loads(stdout)## At this stage, we've captured the output from 'badmin perfmon view -json' and # 'badmin showstatus'. We're now ready to print to standard output the metric# strings in InfluxDB line procotol format. ## Details about the line protocol format can be found here:# https://docs.influxdata.com/influxdb/v2.6/reference/syntax/line-protocol/# # ## LSF server status#print(\"lsf_servers,\",\"status=total\",\" value=\",showstatus[21],\"i \",time_nanosec,sep='')print(\"lsf_servers,\",\"status=ok\",\" value=\",showstatus[23],\"i \",time_nanosec,sep='')print(\"lsf_servers,\",\"status=closed\",\" value=\",showstatus[25],\"i \",time_nanosec,sep='')print(\"lsf_servers,\",\"status=unreachable\",\" value=\",showstatus[27],\"i \",time_nanosec,sep='')print(\"lsf_servers,\",\"status=unavailable\",\" value=\",showstatus[29],\"i \",time_nanosec,sep='')## LSF job status#print(\"lsf_jobs,\",\"state=total\",\" value=\",showstatus[33],\"i \",time_nanosec,sep='')print(\"lsf_jobs,\",\"state=running\",\" value=\",showstatus[35],\"i \",time_nanosec,sep='')print(\"lsf_jobs,\",\"state=suspended\",\" value=\",showstatus[37],\"i \",time_nanosec,sep='')print(\"lsf_jobs,\",\"state=pending\",\" value=\",showstatus[39],\"i \",time_nanosec,sep='')print(\"lsf_jobs,\",\"state=finished\",\" value=\",showstatus[41],\"i \",time_nanosec,sep='')## LSF user stats#print(\"lsf_users,\",\"state=numusers\",\" value=\",showstatus[45],\"i \",time_nanosec,sep='')print(\"lsf_users,\",\"state=numgroups\",\" value=\",showstatus[50],\"i \",time_nanosec,sep='')print(\"lsf_users,\",\"state=numactive\",\" value=\",showstatus[55],\"i \",time_nanosec,sep='')## LSF hosts stats# First we split out the current and peak values for clients, servers, cpus, cores, and slots.# The current and peak values are separated by the \"/\" delimiter.# clientssplit = showstatus[9].split(\"/\")serverssplit = showstatus[11].split(\"/\")cpussplit = showstatus[13].split(\"/\")coressplit = showstatus[15].split(\"/\")slotssplit = showstatus[17].split(\"/\")print(\"lsf_hosts,\",\"state=clients\",\" current=\",clientssplit[0],\"i,\",\"peak=\",clientssplit[1],\"i \",time_nanosec,sep='')print(\"lsf_hosts,\",\"state=servers\",\" current=\",serverssplit[0],\"i,\",\"peak=\",serverssplit[1],\"i \",time_nanosec,sep='')print(\"lsf_hosts,\",\"state=cpus\",\" current=\",cpussplit[0],\"i,\",\"peak=\",cpussplit[1],\"i \",time_nanosec,sep='')print(\"lsf_hosts,\",\"state=cores\",\" current=\",coressplit[0],\"i,\",\"peak=\",coressplit[1],\"i \",time_nanosec,sep='')print(\"lsf_hosts,\",\"state=slots\",\" current=\",slotssplit[0],\"i,\",\"peak=\",slotssplit[1],\"i \",time_nanosec,sep='')## Print mbatchd query metrics#print(\"lsf_mbatchd,\",\"query=job\",\" value=\",data['record'][1]['current'],\"i \",time_nanosec,sep='')print(\"lsf_mbatchd,\",\"query=host\",\" value=\",data['record'][2]['current'],\"i \",time_nanosec,sep='')print(\"lsf_mbatchd,\",\"query=queue\",\" value=\",data['record'][3]['current'],\"i \",time_nanosec,sep='')## Print mbatchd job metrics#print(\"lsf_mbatchd,\",\"jobs=submitreqs\",\" value=\",data['record'][4]['current'],\"i \",time_nanosec,sep='')print(\"lsf_mbatchd,\",\"jobs=submitted\",\" value=\",data['record'][5]['current'],\"i \",time_nanosec,sep='')print(\"lsf_mbatchd,\",\"jobs=dispatched\",\" value=\",data['record'][6]['current'],\"i \",time_nanosec,sep='')print(\"lsf_mbatchd,\",\"jobs=completed\",\" value=\",data['record'][7]['current'],\"i \",time_nanosec,sep='')print(\"lsf_mbatchd,\",\"jobs=sentremote\",\" value=\",data['record'][8]['current'],\"i \",time_nanosec,sep='')print(\"lsf_mbatchd,\",\"jobs=acceptremote\",\" value=\",data['record'][9]['current'],\"i \",time_nanosec,sep='')print(\"lsf_mbatchd,\",\"sched=interval\",\" value=\",data['record'][10]['current'],\"i \",time_nanosec,sep='')print(\"lsf_mbatchd,\",\"sched=matchhost\",\" value=\",data['record'][11]['current'],\"i \",time_nanosec,sep='')print(\"lsf_mbatchd,\",\"sched=buckets\",\" value=\",data['record'][12]['current'],\"i \",time_nanosec,sep='')print(\"lsf_mbatchd,\",\"sched=reordered\",\" value=\",data['record'][13]['current'],\"i \",time_nanosec,sep='')## Print mbatchd efficiency metrics. Here check if the efficiency metric indicated is \"-\". If so, # then assume a zero value. The trailing \"%\" sign on the metrics (percentages) is also stripped here. #slots = (data['record'][14]['current'])slots_percent = slotsif slots_percent == \"-\":    slots_percent = \"0\"elif slots_percent != \"0\":    # Strip % sign and decimal. This is to work around issue inserting float to InfluxDB    # \"type float, already exists as type integer dropped ...\"    slots_percent = slots[:-4]memory = (data['record'][15]['current'])memory_percent = memoryif memory_percent == \"-\":    memory_percent = \"0\"elif memory_percent != \"0\":    # Strip % sign and decimal. This is to work around issue inserting float to InfluxDB    # \"type float, already exists as type integer dropped ...\"    memory_percent = memory[:-4]print(\"lsf_mbatchd,\",\"utilization=slots\",\" value=\",slots_percent,\"i \",time_nanosec,sep='')print(\"lsf_mbatchd,\",\"utilization=memory\",\" value=\",memory_percent,\"i \",time_nanosec,sep='')## Print mbatchd file descriptor usage#print(\"lsf_mbatchd,\",\"fd=free\",\" value=\",data['fd']['free'],\"i \",time_nanosec,sep='')print(\"lsf_mbatchd,\",\"fd=used\",\" value=\",data['fd']['used'],\"i \",time_nanosec,sep='')print(\"lsf_mbatchd,\",\"fd=total\",\" value=\",data['fd']['total'],\"i \",time_nanosec,sep='')## Print LSF queue status (njobs)#iterations = data_queues[\"QUEUES\"]for n in range(iterations):    print(\"lsf_queues,\",\"name=\", data_queues['RECORDS'][n]['QUEUE_NAME'], \" njobs=\", data_queues['RECORDS'][n]['NJOBS'],\"i,\",          \"pend=\", data_queues['RECORDS'][n]['PEND'],\"i,\",          \"run=\", data_queues['RECORDS'][n]['RUN'],\"i,\",          \"susp=\", data_queues['RECORDS'][n]['SUSP'],\"i,\",          \"rsv=\", data_queues['RECORDS'][n]['RSV'],\"i,\",          \"ususp=\", data_queues['RECORDS'][n]['USUSP'],\"i,\",          \"ssusp=\", data_queues['RECORDS'][n]['SSUSP'],\"i \",          time_nanosec, sep='')exit()    Bringing it all togetherFor completeness, below is the detail regarding the configuration of the environment. It should be noted that the simple test environment consists of a single server running IBMSpectrum LSF Suite for HPC and a separate server which runs the InfluxDB instance.HostnameComponentVersionkilencOS (LSF mgmt server)CentOS Stream release 8 (ppc64le)kilencSpectrum LSF Suite for HPCv10.2.0.13adatbazisOS (InfluxDB server)Fedora release 36 (aarch64)adatbazisInfluxDBv1.8.10kilencTelegrafv1.24.3kilencGrafanav9.1.6The follwing steps assume that IBM Spectrum LSF Suite for HPC, InfluxDB and Telegraf have been installed.Start InfluxDB on the host adatbazisOn the LSF management server kilenc, configure telegraf to connect to the influxDB instance on host adatbazis. Edit the configuration /etc/telegraf/telegraf.conf and specifythe correct URL in the outputs.influxdb section as follows:# # Configuration for sending metrics to InfluxDB[[outputs.influxdb]]#   ## The full HTTP or UDP URL for your InfluxDB instance.#   ###   ## Multiple URLs can be specified for a single cluster, only ONE of the#   ## urls will be written to each interval.#   # urls = [\"unix:///var/run/influxdb.sock\"]#   # urls = [\"udp://127.0.0.1:8089\"]#   # urls = [\"http://127.0.0.1:8086\"]# Added gsamu Jan 04 2023urls = [\"http://adatbazis:8086\"]On the LSF management server kilenc, configure telegraf with the custom plugin script lsf_telegraf_agent_0.9.py to collect and log metrics from IBM Spectrum LSF Suite for HPC.Edit the configuration /etc/telegraf/telegraf.conf and specify the correct command path in the section inputs.exec. Additionally, set data_format equal to influx.Note that thescript lsf_telegraf_agent_0.9.py was copied to the directory /etc/telegraf/telegraf.d/scripts with permissions octal 755 and owner set to user telegraf.Note: User telegraf was automatically created during the installation of telegraf. # ## Gather LSF metrics[[inputs.exec]]  ## Commands array   commands = [  \"/etc/telegraf/telegraf.d/scripts/lsf_telegraf_agent_0.9.py\" ]   timeout = \"30s\"   interval = \"30s\"   data_format = \"influx\" # ## End LSF metricsTelegraf provides the ability to collect metrics on processes. Here we&rsquo;ll use the telegraf procstat facility to monitor the LSF mbatchd and mbschd processes. These are the keydaemons involved in handling query requests and making scheduling decisions for jobs in the environment. Edit the configuration /etc/telegraf/telegraf.conf and configure the twofollowing inputs.procstat sections.# ## Monitor CPU and memory utilization for LSF processes# ## mbatchd, mbschd, lim (manager)[[inputs.procstat]]exe = \"lim\"pattern = \"lim\"pid_finder = \"pgrep\"[[inputs.procstat]]exe = \"mbschd\"pattern = \"mbschd\"pid_finder = \"pgrep\"[[inputs.procstat]]exe = \"mbatchd\"pattern = \"mbatchd\"pid_finder = \"pgrep\"With the configuration to telegraf complete, it&rsquo;s now time to test if the configuration and custom LSF agent is functioning as expected. Note that the following operation is performedon the LSF management candidate host kilenc and assumes that the LSF daemons are up and running. This is achieve by running the command:telegraf &ndash;config /etc/telegraf/telegraf.conf &ndash;test. Note: Any errors in the configuration file /etc/telegraf/telegraf.conf will result in errors in the output.  Output of telegraf &ndash;config /etc/telegraf/telegraf.conf &ndash;test. Click to expand!  [root@kilenc telegraf]# pwd/etc/telegraf[root@kilenc telegraf]# telegraf --config /etc/telegraf/telegraf.conf --test&gt; mem,host=kilenc active=1938817024i,available=6820003840i,available_percent=20.653390597462806,buffered=4849664i,cached=6317735936i,commit_limit=33560395776i,committed_as=18635292672i,dirty=4128768i,free=2623799296i,high_free=0i,high_total=0i,huge_page_size=2097152i,huge_pages_free=0i,huge_pages_total=0i,inactive=13852016640i,low_free=0i,low_total=0i,mapped=1007353856i,page_tables=22478848i,shared=259063808i,slab=4946919424i,sreclaimable=902234112i,sunreclaim=4044685312i,swap_cached=3866624i,swap_free=16994729984i,swap_total=17049780224i,total=33021231104i,used=24074846208i,used_percent=72.90717336424115,vmalloc_chunk=0i,vmalloc_total=562949953421312i,vmalloc_used=0i,write_back=0i,write_back_tmp=0i 1674246976000000000&gt; kernel,host=kilenc boot_time=1673790850i,context_switches=1943864437i,entropy_avail=4037i,interrupts=1294179599i,processes_forked=4255316i 1674246976000000000&gt; swap,host=kilenc free=16994729984i,total=17049780224i,used=55050240i,used_percent=0.3228794698626609 1674246976000000000&gt; swap,host=kilenc in=172032i,out=851968i 1674246976000000000&gt; net,host=kilenc,interface=lo bytes_recv=90039931116i,bytes_sent=90039931116i,drop_in=0i,drop_out=0i,err_in=0i,err_out=0i,packets_recv=17245997i,packets_sent=17245997i 1674246976000000000&gt; net,host=kilenc,interface=enP4p1s0f0 bytes_recv=0i,bytes_sent=0i,drop_in=0i,drop_out=0i,err_in=0i,err_out=0i,packets_recv=0i,packets_sent=0i 1674246976000000000&gt; net,host=kilenc,interface=enP4p1s0f1 bytes_recv=11791041280i,bytes_sent=1701152001i,drop_in=0i,drop_out=0i,err_in=0i,err_out=0i,packets_recv=10322276i,packets_sent=4594948i 1674246976000000000&gt; net,host=kilenc,interface=all icmp_inaddrmaskreps=0i,icmp_inaddrmasks=0i,icmp_incsumerrors=0i,icmp_indestunreachs=8609i,icmp_inechoreps=20i,icmp_inechos=11i,icmp_inerrors=1084i,icmp_inmsgs=8640i,icmp_inparmprobs=0i,icmp_inredirects=0i,icmp_insrcquenchs=0i,icmp_intimeexcds=0i,icmp_intimestampreps=0i,icmp_intimestamps=0i,icmp_outaddrmaskreps=0i,icmp_outaddrmasks=0i,icmp_outdestunreachs=4805i,icmp_outechoreps=11i,icmp_outechos=94i,icmp_outerrors=0i,icmp_outmsgs=4910i,icmp_outparmprobs=0i,icmp_outredirects=0i,icmp_outsrcquenchs=0i,icmp_outtimeexcds=0i,icmp_outtimestampreps=0i,icmp_outtimestamps=0i,icmpmsg_intype0=20i,icmpmsg_intype3=8609i,icmpmsg_intype8=11i,icmpmsg_outtype0=11i,icmpmsg_outtype3=4805i,icmpmsg_outtype8=94i,ip_defaultttl=64i,ip_forwarding=1i,ip_forwdatagrams=0i,ip_fragcreates=62958i,ip_fragfails=0i,ip_fragoks=12611i,ip_inaddrerrors=1i,ip_indelivers=21324370i,ip_indiscards=0i,ip_inhdrerrors=0i,ip_inreceives=21324371i,ip_inunknownprotos=0i,ip_outdiscards=0i,ip_outnoroutes=30i,ip_outrequests=21248264i,ip_reasmfails=0i,ip_reasmoks=0i,ip_reasmreqds=0i,ip_reasmtimeout=0i,tcp_activeopens=763497i,tcp_attemptfails=96617i,tcp_currestab=118i,tcp_estabresets=1917i,tcp_incsumerrors=0i,tcp_inerrs=0i,tcp_insegs=19488475i,tcp_maxconn=-1i,tcp_outrsts=137188i,tcp_outsegs=20220038i,tcp_passiveopens=675805i,tcp_retranssegs=9827i,tcp_rtoalgorithm=1i,tcp_rtomax=120000i,tcp_rtomin=200i,udp_ignoredmulti=10509i,udp_incsumerrors=0i,udp_indatagrams=1816997i,udp_inerrors=0i,udp_memerrors=0i,udp_noports=264i,udp_outdatagrams=1506724i,udp_rcvbuferrors=0i,udp_sndbuferrors=0i,udplite_ignoredmulti=0i,udplite_incsumerrors=0i,udplite_indatagrams=0i,udplite_inerrors=0i,udplite_memerrors=0i,udplite_noports=0i,udplite_outdatagrams=0i,udplite_rcvbuferrors=0i,udplite_sndbuferrors=0i 1674246976000000000&gt; diskio,host=kilenc,name=dm-2 io_time=9739370i,iops_in_progress=0i,merged_reads=0i,merged_writes=0i,read_bytes=4015612416i,read_time=604060i,reads=40592i,weighted_io_time=60563370i,write_bytes=47025459712i,write_time=59959310i,writes=1079691i 1674246976000000000&gt; diskio,host=kilenc,name=sda1 io_time=1460i,iops_in_progress=0i,merged_reads=0i,merged_writes=0i,read_bytes=4849664i,read_time=1304i,reads=67i,weighted_io_time=1304i,write_bytes=0i,write_time=0i,writes=0i 1674246976000000000&gt; diskio,host=kilenc,name=sda3 io_time=45872430i,iops_in_progress=0i,merged_reads=623i,merged_writes=1061314i,read_bytes=16398521856i,read_time=3371612i,reads=139298i,weighted_io_time=311521720i,write_bytes=133715422208i,write_time=308150107i,writes=7031512i 1674246976000000000&gt; diskio,host=kilenc,name=dm-1 io_time=5780i,iops_in_progress=0i,merged_reads=0i,merged_writes=0i,read_bytes=5636096i,read_time=3030i,reads=81i,weighted_io_time=26500i,write_bytes=13631488i,write_time=23470i,writes=208i 1674246976000000000&gt; disk,device=dm-0,fstype=xfs,host=kilenc,mode=rw,path=/ free=9315028992i,inodes_free=18214222i,inodes_total=19822888i,inodes_used=1608666i,total=53660876800i,used=44345847808i,used_percent=82.64093032486566 1674246976000000000&gt; disk,device=sda2,fstype=ext4,host=kilenc,mode=rw,path=/boot free=309653504i,inodes_free=65264i,inodes_total=65536i,inodes_used=272i,total=1020702720i,used=640585728i,used_percent=67.41310045173972 1674246976000000000&gt; disk,device=dm-2,fstype=xfs,host=kilenc,mode=rw,path=/home free=856442515456i,inodes_free=452529686i,inodes_total=453312512i,inodes_used=782826i,total=927930712064i,used=71488196608i,used_percent=7.704044674735306 1674246976000000000&gt; disk,device=dm-2,fstype=xfs,host=kilenc,mode=rw,path=/home/opt/at13.0/lib free=856442515456i,inodes_free=452529686i,inodes_total=453312512i,inodes_used=782826i,total=927930712064i,used=71488196608i,used_percent=7.704044674735306 1674246976000000000&gt; disk,device=dm-2,fstype=xfs,host=kilenc,mode=rw,path=/home/opt/at13.0/lib64 free=856442515456i,inodes_free=452529686i,inodes_total=453312512i,inodes_used=782826i,total=927930712064i,used=71488196608i,used_percent=7.704044674735306 1674246976000000000&gt; disk,device=ST31000524AS/raktar,fstype=zfs,host=kilenc,mode=rw,path=/mnt/ST31000524AS free=210837438464i,inodes_free=411792117i,inodes_total=412304487i,inodes_used=512370i,total=965496143872i,used=754658705408i,used_percent=78.16278813725106 1674246976000000000&gt; diskio,host=kilenc,name=sda io_time=45899860i,iops_in_progress=0i,merged_reads=650i,merged_writes=1061332i,read_bytes=16495536128i,read_time=3440899i,reads=141325i,weighted_io_time=311596362i,write_bytes=133715696640i,write_time=308155462i,writes=7031531i 1674246976000000000&gt; disk,device=ST31000524AS,fstype=zfs,host=kilenc,mode=rw,path=/ST31000524AS free=210837438464i,inodes_free=411792117i,inodes_total=411792123i,inodes_used=6i,total=210837569536i,used=131072i,used_percent=0.00006216728844316324 1674246976000000000&gt; diskio,host=kilenc,name=sda2 io_time=18060i,iops_in_progress=0i,merged_reads=27i,merged_writes=18i,read_bytes=88372224i,read_time=31224i,reads=436i,weighted_io_time=36579i,write_bytes=274432i,write_time=5355i,writes=19i 1674246976000000000&gt; diskio,host=kilenc,name=dm-0 io_time=38788720i,iops_in_progress=0i,merged_reads=0i,merged_writes=0i,read_bytes=12341294080i,read_time=1143210i,reads=51814i,weighted_io_time=303329620i,write_bytes=86676331008i,write_time=302186410i,writes=6798400i 1674246976000000000&gt; diskio,host=kilenc,name=sdb io_time=668810i,iops_in_progress=0i,merged_reads=9i,merged_writes=58i,read_bytes=104550912i,read_time=746540i,reads=31054i,weighted_io_time=1445858i,write_bytes=10845920256i,write_time=699318i,writes=124780i 1674246976000000000&gt; diskio,host=kilenc,name=sdb1 io_time=341330i,iops_in_progress=0i,merged_reads=0i,merged_writes=58i,read_bytes=95562240i,read_time=383066i,reads=25026i,weighted_io_time=1082385i,write_bytes=10845920256i,write_time=699318i,writes=124780i 1674246976000000000&gt; diskio,host=kilenc,name=sdb9 io_time=190i,iops_in_progress=0i,merged_reads=0i,merged_writes=0i,read_bytes=4980736i,read_time=37i,reads=69i,weighted_io_time=37i,write_bytes=0i,write_time=0i,writes=0i 1674246976000000000&gt; system,host=kilenc load1=2.06,load15=2.12,load5=2.12,n_cpus=32i,n_users=0i 1674246976000000000&gt; system,host=kilenc uptime=456127i 1674246976000000000&gt; system,host=kilenc uptime_format=\"5 days,  6:42\" 1674246976000000000&gt; processes,host=kilenc blocked=1i,dead=0i,idle=569i,paging=0i,parked=1i,running=0i,sleeping=412i,stopped=0i,total=1366i,total_threads=2683i,unknown=0i,zombies=0i 1674246976000000000&gt; lsf_servers,host=kilenc,status=total value=1i 1674246976000000000&gt; lsf_servers,host=kilenc,status=ok value=1i 1674246976000000000&gt; lsf_servers,host=kilenc,status=closed value=0i 1674246976000000000&gt; lsf_servers,host=kilenc,status=unreachable value=0i 1674246976000000000&gt; lsf_servers,host=kilenc,status=unavailable value=0i 1674246976000000000&gt; lsf_jobs,host=kilenc,state=total value=121776i 1674246976000000000&gt; lsf_jobs,host=kilenc,state=running value=32i 1674246976000000000&gt; lsf_jobs,host=kilenc,state=suspended value=0i 1674246976000000000&gt; lsf_jobs,host=kilenc,state=pending value=120771i 1674246976000000000&gt; lsf_jobs,host=kilenc,state=finished value=973i 1674246976000000000&gt; lsf_users,host=kilenc,state=numusers value=4i 1674246976000000000&gt; lsf_users,host=kilenc,state=numgroups value=1i 1674246976000000000&gt; lsf_users,host=kilenc,state=numactive value=1i 1674246976000000000&gt; lsf_hosts,host=kilenc,state=clients current=0i,peak=0i 1674246976000000000&gt; lsf_hosts,host=kilenc,state=servers current=1i,peak=1i 1674246976000000000&gt; lsf_hosts,host=kilenc,state=cpus current=2i,peak=2i 1674246976000000000&gt; lsf_hosts,host=kilenc,state=cores current=32i,peak=32i 1674246976000000000&gt; lsf_hosts,host=kilenc,state=slots current=32i,peak=32i 1674246976000000000&gt; lsf_mbatchd,host=kilenc,query=job value=0i 1674246976000000000&gt; lsf_mbatchd,host=kilenc,query=host value=0i 1674246976000000000&gt; lsf_mbatchd,host=kilenc,query=queue value=2i 1674246976000000000&gt; lsf_mbatchd,host=kilenc,jobs=submitreqs value=0i 1674246976000000000&gt; lsf_mbatchd,host=kilenc,jobs=submitted value=0i 1674246976000000000&gt; lsf_mbatchd,host=kilenc,jobs=dispatched value=19i 1674246976000000000&gt; lsf_mbatchd,host=kilenc,jobs=completed value=12i 1674246976000000000&gt; lsf_mbatchd,host=kilenc,jobs=sentremote value=0i 1674246976000000000&gt; lsf_mbatchd,host=kilenc,jobs=acceptremote value=0i 1674246976000000000&gt; lsf_mbatchd,host=kilenc,sched=interval value=1i 1674246976000000000&gt; lsf_mbatchd,host=kilenc,sched=matchhost value=5i 1674246976000000000&gt; lsf_mbatchd,host=kilenc,sched=buckets value=5i 1674246976000000000&gt; lsf_mbatchd,host=kilenc,sched=reordered value=7i 1674246976000000000&gt; lsf_mbatchd,host=kilenc,utilization=slots value=100i 1674246976000000000&gt; lsf_mbatchd,host=kilenc,utilization=memory value=0i 1674246976000000000&gt; lsf_mbatchd,fd=free,host=kilenc value=65509i 1674246976000000000&gt; lsf_mbatchd,fd=used,host=kilenc value=26i 1674246976000000000&gt; lsf_mbatchd,fd=total,host=kilenc value=65535i 1674246976000000000&gt; lsf_queues,host=kilenc,name=admin njobs=0i,pend=0i,rsv=0i,run=0i,ssusp=0i,susp=0i,ususp=0i 1674246976000000000&gt; lsf_queues,host=kilenc,name=owners njobs=0i,pend=0i,rsv=0i,run=0i,ssusp=0i,susp=0i,ususp=0i 1674246976000000000&gt; lsf_queues,host=kilenc,name=priority njobs=93951i,pend=93923i,rsv=0i,run=28i,ssusp=0i,susp=0i,ususp=0i 1674246976000000000&gt; lsf_queues,host=kilenc,name=night njobs=0i,pend=0i,rsv=0i,run=0i,ssusp=0i,susp=0i,ususp=0i 1674246976000000000&gt; lsf_queues,host=kilenc,name=short njobs=2504i,pend=2504i,rsv=0i,run=0i,ssusp=0i,susp=0i,ususp=0i 1674246976000000000&gt; lsf_queues,host=kilenc,name=dataq njobs=0i,pend=0i,rsv=0i,run=0i,ssusp=0i,susp=0i,ususp=0i 1674246976000000000&gt; lsf_queues,host=kilenc,name=normal njobs=1750i,pend=1750i,rsv=0i,run=0i,ssusp=0i,susp=0i,ususp=0i 1674246976000000000&gt; lsf_queues,host=kilenc,name=interactive njobs=0i,pend=0i,rsv=0i,run=0i,ssusp=0i,susp=0i,ususp=0i 1674246976000000000&gt; lsf_queues,host=kilenc,name=sendq njobs=22598i,pend=22594i,rsv=0i,run=4i,ssusp=0i,susp=0i,ususp=0i 1674246976000000000&gt; lsf_queues,host=kilenc,name=idle njobs=0i,pend=0i,rsv=0i,run=0i,ssusp=0i,susp=0i,ususp=0i 1674246976000000000&gt; cpu,cpu=cpu0,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=100,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu4,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=100,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu8,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=100,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu12,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=100,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu16,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=98.03921568448419,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=1.9607843137324836 1674246977000000000&gt; cpu,cpu=cpu20,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=100,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu24,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=100,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu28,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=100,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu32,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=100,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu36,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=100,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu40,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=98.03921568448419,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=1.9607843136879006,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu44,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=100,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu48,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=100,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu52,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=0,usage_iowait=100,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu56,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=100,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu60,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=100,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu64,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=87.99999999906868,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=10.000000001155058,usage_user=2.0000000002764864 1674246977000000000&gt; cpu,cpu=cpu68,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=100,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu72,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=86.27450980280263,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=11.764705882127403,usage_user=1.9607843137324836 1674246977000000000&gt; cpu,cpu=cpu76,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=100,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu80,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=92.30769231113655,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=3.8461538464431086,usage_user=3.84615384653056 1674246977000000000&gt; cpu,cpu=cpu84,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=94.11764706486585,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=5.882352941197451 1674246977000000000&gt; cpu,cpu=cpu88,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=100,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu92,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=70.58823529344627,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=29.411764701983955,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu96,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=96.15384615040192,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=3.8461538460125784,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu100,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=97.99999999813735,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=1.999999999998181,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu104,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=96.07843137993407,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=3.92156862782338,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu108,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=96.07843136896838,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=1.9607843136879006,usage_user=1.9607843137324836 1674246977000000000&gt; cpu,cpu=cpu112,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=100,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu116,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=95.91836734305988,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=4.08163265313509,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu120,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=84.61538461280144,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=3.8461538460344413,usage_user=11.53846153830009 1674246977000000000&gt; cpu,cpu=cpu124,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=100,usage_iowait=0,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=0,usage_user=0 1674246977000000000&gt; cpu,cpu=cpu-total,host=kilenc usage_guest=0,usage_guest_nice=0,usage_idle=93.47826086554115,usage_iowait=3.1055900618243673,usage_irq=0,usage_nice=0,usage_softirq=0,usage_steal=0,usage_system=2.484472049468532,usage_user=0.9316770186919254 1674246977000000000&gt; procstat,exe=mbatchd,host=kilenc,process_name=mbatchd,user=root child_major_faults=0i,child_minor_faults=0i,cpu_time=0i,cpu_time_guest=0,cpu_time_guest_nice=0,cpu_time_idle=0,cpu_time_iowait=0,cpu_time_irq=0,cpu_time_nice=0,cpu_time_soft_irq=0,cpu_time_steal=0,cpu_time_system=0.03,cpu_time_user=0.05,cpu_usage=0,created_at=1674246974000000000i,involuntary_context_switches=1i,major_faults=0i,memory_data=834994176i,memory_locked=0i,memory_rss=815595520i,memory_stack=327680i,memory_swap=0i,memory_usage=2.469912528991699,memory_vms=1091108864i,minor_faults=726i,nice_priority=20i,num_fds=10i,num_threads=2i,pid=62056i,ppid=4103699i,read_bytes=0i,read_count=27i,realtime_priority=0i,rlimit_cpu_time_hard=9223372036854775807i,rlimit_cpu_time_soft=9223372036854775807i,rlimit_file_locks_hard=9223372036854775807i,rlimit_file_locks_soft=9223372036854775807i,rlimit_memory_data_hard=9223372036854775807i,rlimit_memory_data_soft=9223372036854775807i,rlimit_memory_locked_hard=67108864i,rlimit_memory_locked_soft=67108864i,rlimit_memory_rss_hard=9223372036854775807i,rlimit_memory_rss_soft=9223372036854775807i,rlimit_memory_stack_hard=9223372036854775807i,rlimit_memory_stack_soft=8388608i,rlimit_memory_vms_hard=9223372036854775807i,rlimit_memory_vms_soft=9223372036854775807i,rlimit_nice_priority_hard=0i,rlimit_nice_priority_soft=0i,rlimit_num_fds_hard=262144i,rlimit_num_fds_soft=65535i,rlimit_realtime_priority_hard=0i,rlimit_realtime_priority_soft=0i,rlimit_signals_pending_hard=118856i,rlimit_signals_pending_soft=118856i,signals_pending=0i,voluntary_context_switches=5i,write_bytes=0i,write_count=16i 1674246977000000000&gt; procstat,exe=mbschd,host=kilenc,process_name=mbschd,user=lsfadmin child_major_faults=0i,child_minor_faults=2457641i,cpu_time=320i,cpu_time_guest=0,cpu_time_guest_nice=0,cpu_time_idle=0,cpu_time_iowait=0.02,cpu_time_irq=0,cpu_time_nice=0,cpu_time_soft_irq=0,cpu_time_steal=0,cpu_time_system=8.4,cpu_time_user=312.14,cpu_usage=1.836645120693344,created_at=1674227581000000000i,involuntary_context_switches=3553i,major_faults=1i,memory_data=228851712i,memory_locked=0i,memory_rss=236847104i,memory_stack=196608i,memory_swap=0i,memory_usage=0.717257022857666,memory_vms=246808576i,minor_faults=2137969i,nice_priority=20i,num_fds=3i,num_threads=1i,pid=4103740i,ppid=4103699i,read_bytes=1552384i,read_count=936861i,realtime_priority=0i,rlimit_cpu_time_hard=9223372036854775807i,rlimit_cpu_time_soft=9223372036854775807i,rlimit_file_locks_hard=9223372036854775807i,rlimit_file_locks_soft=9223372036854775807i,rlimit_memory_data_hard=9223372036854775807i,rlimit_memory_data_soft=9223372036854775807i,rlimit_memory_locked_hard=67108864i,rlimit_memory_locked_soft=67108864i,rlimit_memory_rss_hard=9223372036854775807i,rlimit_memory_rss_soft=9223372036854775807i,rlimit_memory_stack_hard=9223372036854775807i,rlimit_memory_stack_soft=8388608i,rlimit_memory_vms_hard=9223372036854775807i,rlimit_memory_vms_soft=9223372036854775807i,rlimit_nice_priority_hard=0i,rlimit_nice_priority_soft=0i,rlimit_num_fds_hard=262144i,rlimit_num_fds_soft=65535i,rlimit_realtime_priority_hard=0i,rlimit_realtime_priority_soft=0i,rlimit_signals_pending_hard=118856i,rlimit_signals_pending_soft=118856i,signals_pending=0i,voluntary_context_switches=43952i,write_bytes=0i,write_count=42311i 1674246977000000000&gt; procstat_lookup,exe=mbschd,host=kilenc,pid_finder=pgrep,result=success pid_count=1i,result_code=0i,running=1i 1674246977000000000&gt; procstat,exe=mbatchd,host=kilenc,process_name=mbatchd,user=root child_major_faults=2i,child_minor_faults=4476280i,cpu_time=177i,cpu_time_guest=0,cpu_time_guest_nice=0,cpu_time_idle=0,cpu_time_iowait=6.68,cpu_time_irq=0,cpu_time_nice=0,cpu_time_soft_irq=0,cpu_time_steal=0,cpu_time_system=51.01,cpu_time_user=126.42,cpu_usage=0,created_at=1674227573000000000i,involuntary_context_switches=4993i,major_faults=3i,memory_data=834994176i,memory_locked=0i,memory_rss=827785216i,memory_stack=327680i,memory_swap=0i,memory_usage=2.5068273544311523,memory_vms=1091108864i,minor_faults=2406945i,nice_priority=20i,num_fds=26i,num_threads=3i,pid=4103699i,ppid=4103684i,read_bytes=21008384i,read_count=364726i,realtime_priority=0i,rlimit_cpu_time_hard=9223372036854775807i,rlimit_cpu_time_soft=9223372036854775807i,rlimit_file_locks_hard=9223372036854775807i,rlimit_file_locks_soft=9223372036854775807i,rlimit_memory_data_hard=9223372036854775807i,rlimit_memory_data_soft=9223372036854775807i,rlimit_memory_locked_hard=67108864i,rlimit_memory_locked_soft=67108864i,rlimit_memory_rss_hard=9223372036854775807i,rlimit_memory_rss_soft=9223372036854775807i,rlimit_memory_stack_hard=9223372036854775807i,rlimit_memory_stack_soft=8388608i,rlimit_memory_vms_hard=9223372036854775807i,rlimit_memory_vms_soft=9223372036854775807i,rlimit_nice_priority_hard=0i,rlimit_nice_priority_soft=0i,rlimit_num_fds_hard=262144i,rlimit_num_fds_soft=65535i,rlimit_realtime_priority_hard=0i,rlimit_realtime_priority_soft=0i,rlimit_signals_pending_hard=118856i,rlimit_signals_pending_soft=118856i,signals_pending=0i,voluntary_context_switches=172583i,write_bytes=1562181632i,write_count=12164760i 1674246977000000000&gt; procstat_lookup,exe=mbatchd,host=kilenc,pid_finder=pgrep,result=success pid_count=2i,result_code=0i,running=2i 1674246977000000000Assuming there were no errors in the previous step with telegraf, proceed to start the telegraf process via systemd.[root@kilenc telegraf]# systemctl start telegraf[root@kilenc telegraf]# systemctl status telegraf● telegraf.service - Telegraf   Loaded: loaded (/usr/lib/systemd/system/telegraf.service; enabled; vendor preset: disabled)   Active: active (running) since Thu 2023-01-19 14:13:51 EST; 1 day 1h ago     Docs: https://github.com/influxdata/telegraf Main PID: 3225959 (telegraf)    Tasks: 35 (limit: 190169)   Memory: 192.6M   CGroup: /system.slice/telegraf.service           └─3225959 /usr/bin/telegraf -config /etc/telegraf/telegraf.conf -config-directory /etc/tele&gt;Jan 19 14:13:51 kilenc systemd[1]: Starting Telegraf...Jan 19 14:13:51 kilenc systemd[1]: Started Telegraf.On the host running the database instance, adatbazis, perform queries to check whether the database telegraf exists, as well as checking if LSF related data is being logged.This is confirmed in the output below.  Output from InfluxDB queries. Click to expand!  [root@adatbazis fedora]# influxConnected to https://localhost:8086 version 1.8.10InfluxDB shell version: 1.8.10&gt; authusername: influxpassword: &gt; show databasesname: databasesname----_internaltelegraf&gt; use telegrafUsing database telegraf&gt; show field keysname: cpufieldKey         fieldType--------         ---------usage_guest      floatusage_guest_nice floatusage_idle       floatusage_iowait     floatusage_irq        floatusage_nice       floatusage_softirq    floatusage_steal      floatusage_system     floatusage_user       floatname: diskfieldKey     fieldType--------     ---------free         integerinodes_free  integerinodes_total integerinodes_used  integertotal        integerused         integerused_percent floatname: diskiofieldKey         fieldType--------         ---------io_time          integeriops_in_progress integermerged_reads     integermerged_writes    integerread_bytes       integerread_time        integerreads            integerweighted_io_time integerwrite_bytes      integerwrite_time       integerwrites           integername: kernelfieldKey         fieldType--------         ---------boot_time        integercontext_switches integerentropy_avail    integerinterrupts       integerprocesses_forked integername: lsf_hostsfieldKey fieldType-------- ---------current  integerpeak     integername: lsf_jobsfieldKey fieldType-------- ---------value    integername: lsf_mbatchdfieldKey fieldType-------- ---------value    integername: lsf_queuesfieldKey fieldType-------- ---------njobs    integerpend     integerrsv      integerrun      integerssusp    integersusp     integerususp    integername: lsf_serversfieldKey fieldType-------- ---------value    integername: lsf_usersfieldKey fieldType-------- ---------value    integername: memfieldKey          fieldType--------          ---------active            integeravailable         integeravailable_percent floatbuffered          integercached            integercommit_limit      integercommitted_as      integerdirty             integerfree              integerhigh_free         integerhigh_total        integerhuge_page_size    integerhuge_pages_free   integerhuge_pages_total  integerinactive          integerlow_free          integerlow_total         integermapped            integerpage_tables       integershared            integerslab              integersreclaimable      integersunreclaim        integerswap_cached       integerswap_free         integerswap_total        integertotal             integerused              integerused_percent      floatvmalloc_chunk     integervmalloc_total     integervmalloc_used      integerwrite_back        integerwrite_back_tmp    integername: netfieldKey              fieldType--------              ---------bytes_recv            integerbytes_sent            integerdrop_in               integerdrop_out              integererr_in                integererr_out               integericmp_inaddrmaskreps   integericmp_inaddrmasks      integericmp_incsumerrors     integericmp_indestunreachs   integericmp_inechoreps       integericmp_inechos          integericmp_inerrors         integericmp_inmsgs           integericmp_inparmprobs      integericmp_inredirects      integericmp_insrcquenchs     integericmp_intimeexcds      integericmp_intimestampreps  integericmp_intimestamps     integericmp_outaddrmaskreps  integericmp_outaddrmasks     integericmp_outdestunreachs  integericmp_outechoreps      integericmp_outechos         integericmp_outerrors        integericmp_outmsgs          integericmp_outparmprobs     integericmp_outredirects     integericmp_outsrcquenchs    integericmp_outtimeexcds     integericmp_outtimestampreps integericmp_outtimestamps    integericmpmsg_intype0       integericmpmsg_intype3       integericmpmsg_intype8       integericmpmsg_outtype0      integericmpmsg_outtype3      integericmpmsg_outtype8      integerip_defaultttl         integerip_forwarding         integerip_forwdatagrams      integerip_fragcreates        integerip_fragfails          integerip_fragoks            integerip_inaddrerrors       integerip_indelivers         integerip_indiscards         integerip_inhdrerrors        integerip_inreceives         integerip_inunknownprotos    integerip_outdiscards        integerip_outnoroutes        integerip_outrequests        integerip_reasmfails         integerip_reasmoks           integerip_reasmreqds         integerip_reasmtimeout       integerpackets_recv          integerpackets_sent          integertcp_activeopens       integertcp_attemptfails      integertcp_currestab         integertcp_estabresets       integertcp_incsumerrors      integertcp_inerrs            integertcp_insegs            integertcp_maxconn           integertcp_outrsts           integertcp_outsegs           integertcp_passiveopens      integertcp_retranssegs       integertcp_rtoalgorithm      integertcp_rtomax            integertcp_rtomin            integerudp_ignoredmulti      integerudp_incsumerrors      integerudp_indatagrams       integerudp_inerrors          integerudp_memerrors         integerudp_noports           integerudp_outdatagrams      integerudp_rcvbuferrors      integerudp_sndbuferrors      integerudplite_ignoredmulti  integerudplite_incsumerrors  integerudplite_indatagrams   integerudplite_inerrors      integerudplite_memerrors     integerudplite_noports       integerudplite_outdatagrams  integerudplite_rcvbuferrors  integerudplite_sndbuferrors  integername: processesfieldKey      fieldType--------      ---------blocked       integerdead          integeridle          integerpaging        integerparked        integerrunning       integersleeping      integerstopped       integertotal         integertotal_threads integerunknown       integerzombies       integername: procstatfieldKey                     fieldType--------                     ---------child_major_faults           integerchild_minor_faults           integercpu_time_guest               floatcpu_time_guest_nice          floatcpu_time_idle                floatcpu_time_iowait              floatcpu_time_irq                 floatcpu_time_nice                floatcpu_time_soft_irq            floatcpu_time_steal               floatcpu_time_system              floatcpu_time_user                floatcpu_usage                    floatcreated_at                   integerinvoluntary_context_switches integermajor_faults                 integermemory_data                  integermemory_locked                integermemory_rss                   integermemory_stack                 integermemory_swap                  integermemory_usage                 floatmemory_vms                   integerminor_faults                 integernum_threads                  integerpid                          integerppid                         integervoluntary_context_switches   integername: procstat_lookupfieldKey    fieldType--------    ---------pid_count   integerresult_code integerrunning     integername: swapfieldKey     fieldType--------     ---------free         integerin           integerout          integertotal        integerused         integerused_percent floatname: systemfieldKey       fieldType--------       ---------load1          floatload15         floatload5          floatn_cpus         integern_unique_users integern_users        integeruptime         integeruptime_format  string&gt; select * from metrics&gt; SELECT * FROM \"lsf_hosts\";name: lsf_hoststime                current host   peak state----                ------- ----   ---- -----1674493170000000000 0       kilenc 0    clients1674493170000000000 32      kilenc 32   slots1674493170000000000 32      kilenc 32   cores1674493170000000000 1       kilenc 1    servers1674493170000000000 2       kilenc 2    cpus1674493200000000000 1       kilenc 1    servers1674493200000000000 2       kilenc 2    cpus1674493200000000000 32      kilenc 32   slots1674493200000000000 0       kilenc 0    clients1674493200000000000 32      kilenc 32   cores1674493230000000000 0       kilenc 0    clients1674493230000000000 32      kilenc 32   cores1674493230000000000 2       kilenc 2    cpus1674493230000000000 1       kilenc 1    servers1674493230000000000 32      kilenc 32   slots1674493260000000000 1       kilenc 1    servers1674493260000000000 32      kilenc 32   slots1674493260000000000 0       kilenc 0    clients1674493260000000000 2       kilenc 2    cpus1674493260000000000 32      kilenc 32   cores&gt; quitWith telegraf successfully logging data to the InfluxDB instance, it will now be possible to create a data source in Grafana in order to create a dashboard containing LSF metrics.As noted at the outset, this article is not meant to be an extensive guide to the creation of dashoards in Grafana. In the Grafana navigation select Configuration &gt; Data sources.Select the Add data source button, followed by InfluxDB, which is listed under Time series databases. On the settings page specify following values:VariableValueURLhttp://adatbazis:8086DatabasetelegrafBasic auth(enable)User&lt;influxdb_username&gt;Password&lt;influxdb_passwordNext, click on Save &amp; test. If all variables and settings were properly specified, the message datasource is working. 17 measurements found.With the datasource configured in Grafana, the final step is to create a dashboard. Creating a dashboard requires creating panels which display data pulled from the confiugred datasource using targeted queries. With a bit of effort, I was able to piece together the following dashboard which includes both metrics from LSF, as well as metrics from Telegrafinput.procstat for the LSF processes mbatchd, mbschd and the management lim.  Example dashboard definition (JSON). Click to expand!  {  \"annotations\": {    \"list\": [      {        \"builtIn\": 1,        \"datasource\": {          \"type\": \"datasource\",          \"uid\": \"grafana\"        },        \"enable\": true,        \"hide\": true,        \"iconColor\": \"rgba(0, 211, 255, 1)\",        \"name\": \"Annotations &amp; Alerts\",        \"target\": {          \"limit\": 100,          \"matchAny\": false,          \"tags\": [],          \"type\": \"dashboard\"        },        \"type\": \"dashboard\"      }    ]  },  \"editable\": true,  \"fiscalYearStartMonth\": 0,  \"graphTooltip\": 0,  \"id\": 17,  \"links\": [],  \"liveNow\": false,  \"panels\": [    {      \"collapsed\": false,      \"gridPos\": {        \"h\": 1,        \"w\": 24,        \"x\": 0,        \"y\": 0      },      \"id\": 35,      \"panels\": [],      \"title\": \"Cluster aggregate current statistics\",      \"type\": \"row\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"A view of the current status of the LSF servers in the cluster. Servers can be in one of four states: Ok, Unavailable, Closed and Unreachable. \",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"palette-classic\"          },          \"custom\": {            \"hideFrom\": {              \"legend\": false,              \"tooltip\": false,              \"viz\": false            }          },          \"decimals\": 2,          \"mappings\": []        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 8,        \"w\": 9,        \"x\": 0,        \"y\": 1      },      \"id\": 32,      \"options\": {        \"displayLabels\": [          \"name\",          \"value\"        ],        \"legend\": {          \"displayMode\": \"table\",          \"placement\": \"right\",          \"showLegend\": true,          \"sortBy\": \"Value\",          \"sortDesc\": true,          \"values\": [            \"value\",            \"percent\"          ]        },        \"pieType\": \"donut\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"\",          \"values\": false        },        \"tooltip\": {          \"mode\": \"multi\",          \"sort\": \"none\"        }      },      \"targets\": [        {          \"alias\": \"Ok\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_servers\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ]          ],          \"tags\": [            {              \"key\": \"status\",              \"operator\": \"=\",              \"value\": \"ok\"            }          ]        },        {          \"alias\": \"Closed\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_servers\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"B\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ]          ],          \"tags\": [            {              \"key\": \"status\",              \"operator\": \"=\",              \"value\": \"closed\"            }          ]        },        {          \"alias\": \"Unreachable\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_servers\",          \"orderByTime\": \"ASC\",          \"policy\": \"default\",          \"refId\": \"C\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ]          ],          \"tags\": [            {              \"key\": \"status\",              \"operator\": \"=\",              \"value\": \"unreachable\"            }          ]        },        {          \"alias\": \"Unavailable\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_servers\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"D\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"status\",              \"operator\": \"=\",              \"value\": \"unavailable\"            }          ]        }      ],      \"title\": \"Current aggregate LSF server statistics\",      \"type\": \"piechart\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"green\",                \"value\": null              }            ]          }        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 9,        \"y\": 1      },      \"id\": 43,      \"options\": {        \"colorMode\": \"value\",        \"graphMode\": \"none\",        \"justifyMode\": \"auto\",        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"\",          \"values\": false        },        \"text\": {},        \"textMode\": \"auto\"      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_jobs\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"distinct\"              }            ]          ],          \"tags\": [            {              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"running\"            }          ]        }      ],      \"title\": \"Currently running\",      \"type\": \"stat\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"light-red\",                \"value\": null              }            ]          }        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 12,        \"y\": 1      },      \"id\": 45,      \"options\": {        \"colorMode\": \"value\",        \"graphMode\": \"none\",        \"justifyMode\": \"auto\",        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"\",          \"values\": false        },        \"text\": {},        \"textMode\": \"auto\"      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"measurement\": \"lsf_jobs\",          \"orderByTime\": \"ASC\",          \"policy\": \"default\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"suspended\"            }          ]        }      ],      \"title\": \"Currently suspended\",      \"type\": \"stat\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"palette-classic\"          },          \"custom\": {            \"hideFrom\": {              \"legend\": false,              \"tooltip\": false,              \"viz\": false            }          },          \"decimals\": 2,          \"mappings\": []        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 8,        \"w\": 9,        \"x\": 15,        \"y\": 1      },      \"id\": 33,      \"options\": {        \"displayLabels\": [          \"name\",          \"value\"        ],        \"legend\": {          \"displayMode\": \"table\",          \"placement\": \"right\",          \"showLegend\": true,          \"sortBy\": \"Value\",          \"sortDesc\": true,          \"values\": [            \"value\",            \"percent\"          ]        },        \"pieType\": \"donut\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"\",          \"values\": false        },        \"tooltip\": {          \"mode\": \"multi\",          \"sort\": \"none\"        }      },      \"targets\": [        {          \"alias\": \"Running\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_jobs\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ]          ],          \"tags\": [            {              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"running\"            }          ]        },        {          \"alias\": \"Pending\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_jobs\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"B\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ]          ],          \"tags\": [            {              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"pending\"            }          ]        },        {          \"alias\": \"Suspended\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_jobs\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"C\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ]          ],          \"tags\": [            {              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"suspended\"            }          ]        }      ],      \"title\": \"Current aggregate LSF job statistics\",      \"type\": \"piechart\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"yellow\",                \"value\": null              }            ]          }        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 9,        \"y\": 5      },      \"id\": 44,      \"options\": {        \"colorMode\": \"value\",        \"graphMode\": \"none\",        \"justifyMode\": \"auto\",        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"\",          \"values\": false        },        \"text\": {},        \"textMode\": \"auto\"      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"measurement\": \"lsf_jobs\",          \"orderByTime\": \"ASC\",          \"policy\": \"default\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"pending\"            }          ]        }      ],      \"title\": \"Currently pending \",      \"type\": \"stat\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"blue\",                \"value\": null              }            ]          }        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 12,        \"y\": 5      },      \"id\": 46,      \"options\": {        \"colorMode\": \"value\",        \"graphMode\": \"none\",        \"justifyMode\": \"auto\",        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"\",          \"values\": false        },        \"text\": {},        \"textMode\": \"auto\"      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"measurement\": \"lsf_jobs\",          \"orderByTime\": \"ASC\",          \"policy\": \"default\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"finished\"            }          ]        }      ],      \"title\": \"Finished (past hour)\",      \"type\": \"stat\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"Spectrum LSF queue statistics. Here we show jobs in running, pending and suspended jobs. \",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"palette-classic\"          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"green\",                \"value\": null              },              {                \"color\": \"red\",                \"value\": 80              }            ]          }        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 8,        \"w\": 9,        \"x\": 0,        \"y\": 9      },      \"id\": 41,      \"options\": {        \"displayMode\": \"lcd\",        \"minVizHeight\": 10,        \"minVizWidth\": 0,        \"orientation\": \"horizontal\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"\",          \"values\": false        },        \"showUnfilled\": true      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"alias\": \"Running\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"measurement\": \"lsf_queues\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"run\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ]          ],          \"tags\": [            {              \"key\": \"name\",              \"operator\": \"=~\",              \"value\": \"/^$Queue$/\"            }          ]        },        {          \"alias\": \"Pending\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_queues\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"B\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"pend\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ]          ],          \"tags\": [            {              \"key\": \"name\",              \"operator\": \"=~\",              \"value\": \"/^$Queue$/\"            }          ]        },        {          \"alias\": \"Suspended\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_queues\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"C\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"susp\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ]          ],          \"tags\": [            {              \"key\": \"name\",              \"operator\": \"=~\",              \"value\": \"/^$Queue$/\"            }          ]        }      ],      \"title\": \"Current queue statistics ($Queue)\",      \"type\": \"bargauge\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"min\": 0,          \"thresholds\": {            \"mode\": \"percentage\",            \"steps\": [              {                \"color\": \"green\",                \"value\": null              }            ]          },          \"unit\": \"none\"        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 9,        \"y\": 9      },      \"id\": 53,      \"options\": {        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"/^lsf_hosts\\\\.last$/\",          \"values\": false        },        \"showThresholdLabels\": false,        \"showThresholdMarkers\": true      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_hosts\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"current\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ],            [              {                \"params\": [                  \"peak\"                ],                \"type\": \"field\"              }            ]          ],          \"tags\": [            {              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            },            {              \"condition\": \"AND\",              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"servers\"            }          ]        }      ],      \"title\": \"Servers\",      \"type\": \"gauge\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"min\": 0,          \"thresholds\": {            \"mode\": \"percentage\",            \"steps\": [              {                \"color\": \"yellow\",                \"value\": null              }            ]          },          \"unit\": \"none\"        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 12,        \"y\": 9      },      \"id\": 54,      \"options\": {        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"/^lsf_hosts\\\\.last$/\",          \"values\": false        },        \"showThresholdLabels\": false,        \"showThresholdMarkers\": true      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_hosts\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"current\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ],            [              {                \"params\": [                  \"peak\"                ],                \"type\": \"field\"              }            ]          ],          \"tags\": [            {              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            },            {              \"condition\": \"AND\",              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"cpus\"            }          ]        }      ],      \"title\": \"CPUs\",      \"type\": \"gauge\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"palette-classic\"          },          \"custom\": {            \"axisCenteredZero\": false,            \"axisColorMode\": \"text\",            \"axisLabel\": \"\",            \"axisPlacement\": \"auto\",            \"barAlignment\": 0,            \"drawStyle\": \"line\",            \"fillOpacity\": 0,            \"gradientMode\": \"none\",            \"hideFrom\": {              \"legend\": false,              \"tooltip\": false,              \"viz\": false            },            \"lineInterpolation\": \"stepBefore\",            \"lineWidth\": 1,            \"pointSize\": 5,            \"scaleDistribution\": {              \"log\": 2,              \"type\": \"log\"            },            \"showPoints\": \"auto\",            \"spanNulls\": true,            \"stacking\": {              \"group\": \"A\",              \"mode\": \"none\"            },            \"thresholdsStyle\": {              \"mode\": \"off\"            }          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"green\",                \"value\": null              },              {                \"color\": \"red\",                \"value\": 80              }            ]          }        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 8,        \"w\": 9,        \"x\": 15,        \"y\": 9      },      \"id\": 42,      \"options\": {        \"legend\": {          \"calcs\": [],          \"displayMode\": \"list\",          \"placement\": \"bottom\",          \"showLegend\": true        },        \"tooltip\": {          \"mode\": \"single\",          \"sort\": \"none\"        }      },      \"targets\": [        {          \"alias\": \"Running\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_jobs\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ]          ],          \"tags\": [            {              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"running\"            }          ]        },        {          \"alias\": \"Pending\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_jobs\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"B\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ]          ],          \"tags\": [            {              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"pending\"            }          ]        },        {          \"alias\": \"Suspended\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_jobs\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"C\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ]          ],          \"tags\": [            {              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"suspended\"            }          ]        }      ],      \"title\": \"Aggregate LSF job statistics\",      \"type\": \"timeseries\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"min\": 0,          \"thresholds\": {            \"mode\": \"percentage\",            \"steps\": [              {                \"color\": \"light-red\",                \"value\": null              }            ]          },          \"unit\": \"none\"        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 9,        \"y\": 13      },      \"id\": 55,      \"options\": {        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"/^lsf_hosts\\\\.last$/\",          \"values\": false        },        \"showThresholdLabels\": false,        \"showThresholdMarkers\": true      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_hosts\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"current\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ],            [              {                \"params\": [                  \"peak\"                ],                \"type\": \"field\"              }            ]          ],          \"tags\": [            {              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            },            {              \"condition\": \"AND\",              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"cores\"            }          ]        }      ],      \"title\": \"Cores\",      \"type\": \"gauge\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"min\": 0,          \"thresholds\": {            \"mode\": \"percentage\",            \"steps\": [              {                \"color\": \"blue\",                \"value\": null              }            ]          },          \"unit\": \"none\"        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 12,        \"y\": 13      },      \"id\": 56,      \"options\": {        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"/^lsf_hosts\\\\.last$/\",          \"values\": false        },        \"showThresholdLabels\": false,        \"showThresholdMarkers\": true      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_hosts\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"current\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ],            [              {                \"params\": [                  \"peak\"                ],                \"type\": \"field\"              }            ]          ],          \"tags\": [            {              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            },            {              \"condition\": \"AND\",              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"slots\"            }          ]        }      ],      \"title\": \"Slots\",      \"type\": \"gauge\"    },    {      \"collapsed\": false,      \"gridPos\": {        \"h\": 1,        \"w\": 24,        \"x\": 0,        \"y\": 17      },      \"id\": 37,      \"panels\": [],      \"title\": \"LSF scheduler statistics\",      \"type\": \"row\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"palette-classic\"          },          \"custom\": {            \"axisCenteredZero\": false,            \"axisColorMode\": \"text\",            \"axisLabel\": \"\",            \"axisPlacement\": \"auto\",            \"barAlignment\": 0,            \"drawStyle\": \"line\",            \"fillOpacity\": 10,            \"gradientMode\": \"none\",            \"hideFrom\": {              \"graph\": false,              \"legend\": false,              \"tooltip\": false,              \"viz\": false            },            \"lineInterpolation\": \"linear\",            \"lineWidth\": 1,            \"pointSize\": 5,            \"scaleDistribution\": {              \"type\": \"linear\"            },            \"showPoints\": \"never\",            \"spanNulls\": true,            \"stacking\": {              \"group\": \"A\",              \"mode\": \"none\"            },            \"thresholdsStyle\": {              \"mode\": \"off\"            }          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"green\",                \"value\": null              },              {                \"color\": \"red\",                \"value\": 80              }            ]          },          \"unit\": \"short\"        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 8,        \"w\": 12,        \"x\": 0,        \"y\": 18      },      \"id\": 20,      \"options\": {        \"graph\": {},        \"legend\": {          \"calcs\": [],          \"displayMode\": \"list\",          \"placement\": \"right\",          \"showLegend\": true        },        \"tooltip\": {          \"mode\": \"single\",          \"sort\": \"none\"        }      },      \"pluginVersion\": \"7.5.15\",      \"targets\": [        {          \"alias\": \"CPU utilization (%)\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"measurement\": \"procstat\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"cpu_usage\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"exe\",              \"operator\": \"=\",              \"value\": \"mbatchd\"            },            {              \"condition\": \"AND\",              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            }          ]        },        {          \"alias\": \"Memory utilization (%)\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"procstat\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"B\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"memory_usage\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"exe\",              \"operator\": \"=\",              \"value\": \"mbatchd\"            },            {              \"condition\": \"AND\",              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            }          ]        },        {          \"alias\": \"Number of threads\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"procstat\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"C\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"num_threads\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"exe\",              \"operator\": \"=\",              \"value\": \"mbatchd\"            },            {              \"condition\": \"AND\",              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            }          ]        },        {          \"alias\": \"File descriptors\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_mbatchd\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"D\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"fd\",              \"operator\": \"=\",              \"value\": \"used\"            },            {              \"condition\": \"AND\",              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            }          ]        }      ],      \"title\": \"LSF mbatchd process metrics\",      \"type\": \"timeseries\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"palette-classic\"          },          \"custom\": {            \"axisCenteredZero\": false,            \"axisColorMode\": \"text\",            \"axisLabel\": \"\",            \"axisPlacement\": \"auto\",            \"barAlignment\": 0,            \"drawStyle\": \"line\",            \"fillOpacity\": 10,            \"gradientMode\": \"none\",            \"hideFrom\": {              \"graph\": false,              \"legend\": false,              \"tooltip\": false,              \"viz\": false            },            \"lineInterpolation\": \"linear\",            \"lineWidth\": 1,            \"pointSize\": 5,            \"scaleDistribution\": {              \"type\": \"linear\"            },            \"showPoints\": \"never\",            \"spanNulls\": true,            \"stacking\": {              \"group\": \"A\",              \"mode\": \"none\"            },            \"thresholdsStyle\": {              \"mode\": \"off\"            }          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"green\",                \"value\": null              },              {                \"color\": \"red\",                \"value\": 80              }            ]          },          \"unit\": \"short\"        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 8,        \"w\": 12,        \"x\": 12,        \"y\": 18      },      \"id\": 57,      \"options\": {        \"graph\": {},        \"legend\": {          \"calcs\": [],          \"displayMode\": \"list\",          \"placement\": \"right\",          \"showLegend\": true        },        \"tooltip\": {          \"mode\": \"single\",          \"sort\": \"none\"        }      },      \"pluginVersion\": \"7.5.15\",      \"targets\": [        {          \"alias\": \"CPU utilization (%)\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"measurement\": \"procstat\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"cpu_usage\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"exe\",              \"operator\": \"=\",              \"value\": \"lim\"            },            {              \"condition\": \"AND\",              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            }          ]        },        {          \"alias\": \"Memory utilization (%)\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"procstat\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"B\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"memory_usage\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"exe\",              \"operator\": \"=\",              \"value\": \"lim\"            },            {              \"condition\": \"AND\",              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            }          ]        },        {          \"alias\": \"Number of threads\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"procstat\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"C\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"num_threads\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"exe\",              \"operator\": \"=\",              \"value\": \"lim\"            },            {              \"condition\": \"AND\",              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            }          ]        }      ],      \"title\": \"LSF management lim process metrics\",      \"type\": \"timeseries\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"palette-classic\"          },          \"custom\": {            \"axisCenteredZero\": false,            \"axisColorMode\": \"text\",            \"axisLabel\": \"\",            \"axisPlacement\": \"auto\",            \"barAlignment\": 0,            \"drawStyle\": \"line\",            \"fillOpacity\": 10,            \"gradientMode\": \"none\",            \"hideFrom\": {              \"graph\": false,              \"legend\": false,              \"tooltip\": false,              \"viz\": false            },            \"lineInterpolation\": \"linear\",            \"lineWidth\": 1,            \"pointSize\": 5,            \"scaleDistribution\": {              \"type\": \"linear\"            },            \"showPoints\": \"never\",            \"spanNulls\": true,            \"stacking\": {              \"group\": \"A\",              \"mode\": \"none\"            },            \"thresholdsStyle\": {              \"mode\": \"off\"            }          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"green\",                \"value\": null              },              {                \"color\": \"red\",                \"value\": 80              }            ]          },          \"unit\": \"short\"        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 8,        \"w\": 12,        \"x\": 0,        \"y\": 26      },      \"id\": 27,      \"options\": {        \"graph\": {},        \"legend\": {          \"calcs\": [],          \"displayMode\": \"list\",          \"placement\": \"right\",          \"showLegend\": true        },        \"tooltip\": {          \"mode\": \"single\",          \"sort\": \"none\"        }      },      \"pluginVersion\": \"7.5.15\",      \"targets\": [        {          \"alias\": \"Job buckets\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"measurement\": \"lsf_mbatchd\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"sched\",              \"operator\": \"=\",              \"value\": \"buckets\"            },            {              \"condition\": \"AND\",              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            }          ]        },        {          \"alias\": \"Matching host criteria\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_mbatchd\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"B\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"sched\",              \"operator\": \"=\",              \"value\": \"matchhost\"            },            {              \"condition\": \"AND\",              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            }          ]        },        {          \"alias\": \"Scheduling interval (seconds)\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_mbatchd\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"C\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"sched\",              \"operator\": \"=\",              \"value\": \"interval\"            },            {              \"condition\": \"AND\",              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            }          ]        }      ],      \"title\": \"LSF scheduler metrics\",      \"type\": \"timeseries\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"palette-classic\"          },          \"custom\": {            \"axisCenteredZero\": false,            \"axisColorMode\": \"text\",            \"axisLabel\": \"\",            \"axisPlacement\": \"auto\",            \"barAlignment\": 0,            \"drawStyle\": \"line\",            \"fillOpacity\": 10,            \"gradientMode\": \"none\",            \"hideFrom\": {              \"graph\": false,              \"legend\": false,              \"tooltip\": false,              \"viz\": false            },            \"lineInterpolation\": \"linear\",            \"lineWidth\": 1,            \"pointSize\": 5,            \"scaleDistribution\": {              \"type\": \"linear\"            },            \"showPoints\": \"never\",            \"spanNulls\": true,            \"stacking\": {              \"group\": \"A\",              \"mode\": \"none\"            },            \"thresholdsStyle\": {              \"mode\": \"off\"            }          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"green\",                \"value\": null              },              {                \"color\": \"red\",                \"value\": 80              }            ]          },          \"unit\": \"short\"        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 8,        \"w\": 12,        \"x\": 12,        \"y\": 26      },      \"id\": 58,      \"options\": {        \"graph\": {},        \"legend\": {          \"calcs\": [],          \"displayMode\": \"list\",          \"placement\": \"right\",          \"showLegend\": true        },        \"tooltip\": {          \"mode\": \"single\",          \"sort\": \"none\"        }      },      \"pluginVersion\": \"7.5.15\",      \"targets\": [        {          \"alias\": \"CPU utilization (%)\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"measurement\": \"procstat\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"cpu_usage\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"exe\",              \"operator\": \"=\",              \"value\": \"mbschd\"            },            {              \"condition\": \"AND\",              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            }          ]        },        {          \"alias\": \"Memory utilization (%)\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"procstat\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"B\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"memory_usage\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"exe\",              \"operator\": \"=\",              \"value\": \"mbatchd\"            },            {              \"condition\": \"AND\",              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            }          ]        },        {          \"alias\": \"Number of threads\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"procstat\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"C\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"num_threads\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"exe\",              \"operator\": \"=\",              \"value\": \"mbatchd\"            },            {              \"condition\": \"AND\",              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            }          ]        }      ],      \"title\": \"LSF mbschd process metrics\",      \"type\": \"timeseries\"    },    {      \"collapsed\": false,      \"gridPos\": {        \"h\": 1,        \"w\": 24,        \"x\": 0,        \"y\": 34      },      \"id\": 39,      \"panels\": [],      \"title\": \"Additional metrics (scratch)\",      \"type\": \"row\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"green\",                \"value\": null              }            ]          }        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 0,        \"y\": 35      },      \"id\": 2,      \"options\": {        \"colorMode\": \"value\",        \"graphMode\": \"none\",        \"justifyMode\": \"auto\",        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"\",          \"values\": false        },        \"text\": {},        \"textMode\": \"auto\"      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_jobs\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"distinct\"              }            ]          ],          \"tags\": [            {              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"running\"            }          ]        }      ],      \"title\": \"Running\",      \"type\": \"stat\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"yellow\",                \"value\": null              }            ]          }        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 3,        \"y\": 35      },      \"id\": 5,      \"options\": {        \"colorMode\": \"value\",        \"graphMode\": \"none\",        \"justifyMode\": \"auto\",        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"\",          \"values\": false        },        \"text\": {},        \"textMode\": \"auto\"      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"measurement\": \"lsf_jobs\",          \"orderByTime\": \"ASC\",          \"policy\": \"default\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"pending\"            }          ]        }      ],      \"title\": \"Pending\",      \"type\": \"stat\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"red\",                \"value\": null              }            ]          }        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 6,        \"y\": 35      },      \"id\": 6,      \"options\": {        \"colorMode\": \"value\",        \"graphMode\": \"none\",        \"justifyMode\": \"auto\",        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"\",          \"values\": false        },        \"text\": {},        \"textMode\": \"auto\"      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"measurement\": \"lsf_jobs\",          \"orderByTime\": \"ASC\",          \"policy\": \"default\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"suspended\"            }          ]        }      ],      \"title\": \"Suspended\",      \"type\": \"stat\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"blue\",                \"value\": null              }            ]          }        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 9,        \"y\": 35      },      \"id\": 7,      \"options\": {        \"colorMode\": \"value\",        \"graphMode\": \"none\",        \"justifyMode\": \"auto\",        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"\",          \"values\": false        },        \"text\": {},        \"textMode\": \"auto\"      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"measurement\": \"lsf_jobs\",          \"orderByTime\": \"ASC\",          \"policy\": \"default\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"finished\"            }          ]        }      ],      \"title\": \"Finished\",      \"type\": \"stat\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"green\",                \"value\": null              }            ]          }        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 12,        \"y\": 35      },      \"id\": 15,      \"options\": {        \"colorMode\": \"value\",        \"graphMode\": \"none\",        \"justifyMode\": \"auto\",        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"\",          \"values\": false        },        \"text\": {},        \"textMode\": \"auto\"      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"alias\": \"Ok\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_servers\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"status\",              \"operator\": \"=\",              \"value\": \"ok\"            }          ]        }      ],      \"title\": \"Ok\",      \"type\": \"stat\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"blue\",                \"value\": null              }            ]          }        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 15,        \"y\": 35      },      \"id\": 16,      \"options\": {        \"colorMode\": \"value\",        \"graphMode\": \"none\",        \"justifyMode\": \"auto\",        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"\",          \"values\": false        },        \"text\": {},        \"textMode\": \"auto\"      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"alias\": \"Closed\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_servers\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"status\",              \"operator\": \"=\",              \"value\": \"closed\"            }          ]        }      ],      \"title\": \"Closed\",      \"type\": \"stat\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"yellow\",                \"value\": null              }            ]          }        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 18,        \"y\": 35      },      \"id\": 17,      \"options\": {        \"colorMode\": \"value\",        \"graphMode\": \"none\",        \"justifyMode\": \"auto\",        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"\",          \"values\": false        },        \"text\": {},        \"textMode\": \"auto\"      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"alias\": \"Unreachable\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_servers\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"status\",              \"operator\": \"=\",              \"value\": \"unreachable\"            }          ]        }      ],      \"title\": \"Unreachable\",      \"type\": \"stat\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"red\",                \"value\": null              }            ]          }        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 21,        \"y\": 35      },      \"id\": 18,      \"options\": {        \"colorMode\": \"value\",        \"graphMode\": \"none\",        \"justifyMode\": \"auto\",        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"\",          \"values\": false        },        \"text\": {},        \"textMode\": \"auto\"      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"alias\": \"Unavailable\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_servers\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"value\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"mean\"              }            ]          ],          \"tags\": [            {              \"key\": \"status\",              \"operator\": \"=\",              \"value\": \"unavailable\"            }          ]        }      ],      \"title\": \"Unavailable\",      \"type\": \"stat\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"green\",                \"value\": null              }            ]          }        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 0,        \"y\": 39      },      \"id\": 21,      \"options\": {        \"colorMode\": \"value\",        \"graphMode\": \"none\",        \"justifyMode\": \"auto\",        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"\",          \"values\": false        },        \"text\": {},        \"textMode\": \"auto\"      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"alias\": \"Clients\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_hosts\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"current\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ]          ],          \"tags\": [            {              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            },            {              \"condition\": \"AND\",              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"clients\"            }          ]        }      ],      \"title\": \"Clients\",      \"type\": \"stat\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"green\",                \"value\": null              }            ]          }        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 3,        \"y\": 39      },      \"id\": 22,      \"options\": {        \"colorMode\": \"value\",        \"graphMode\": \"none\",        \"justifyMode\": \"auto\",        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"\",          \"values\": false        },        \"text\": {},        \"textMode\": \"auto\"      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"alias\": \"Servers\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_hosts\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"current\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ]          ],          \"tags\": [            {              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            },            {              \"condition\": \"AND\",              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"servers\"            }          ]        }      ],      \"title\": \"Servers\",      \"type\": \"stat\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"green\",                \"value\": null              }            ]          }        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 6,        \"y\": 39      },      \"id\": 23,      \"options\": {        \"colorMode\": \"value\",        \"graphMode\": \"none\",        \"justifyMode\": \"auto\",        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"\",          \"values\": false        },        \"text\": {},        \"textMode\": \"auto\"      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"alias\": \"Servers\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_hosts\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"current\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ]          ],          \"tags\": [            {              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            },            {              \"condition\": \"AND\",              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"cpus\"            }          ]        }      ],      \"title\": \"CPUs\",      \"type\": \"stat\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"green\",                \"value\": null              }            ]          }        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 9,        \"y\": 39      },      \"id\": 24,      \"options\": {        \"colorMode\": \"value\",        \"graphMode\": \"none\",        \"justifyMode\": \"auto\",        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"\",          \"values\": false        },        \"text\": {},        \"textMode\": \"auto\"      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"alias\": \"Cores\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_hosts\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"current\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ]          ],          \"tags\": [            {              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            },            {              \"condition\": \"AND\",              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"cores\"            }          ]        }      ],      \"title\": \"Cores\",      \"type\": \"stat\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"thresholds\": {            \"mode\": \"absolute\",            \"steps\": [              {                \"color\": \"green\",                \"value\": null              }            ]          }        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 12,        \"y\": 39      },      \"id\": 25,      \"options\": {        \"colorMode\": \"value\",        \"graphMode\": \"none\",        \"justifyMode\": \"auto\",        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"\",          \"values\": false        },        \"text\": {},        \"textMode\": \"auto\"      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"alias\": \"Slots\",          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_hosts\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"current\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ]          ],          \"tags\": [            {              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            },            {              \"condition\": \"AND\",              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"slots\"            }          ]        }      ],      \"title\": \"Slots\",      \"type\": \"stat\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"min\": 0,          \"thresholds\": {            \"mode\": \"percentage\",            \"steps\": [              {                \"color\": \"green\",                \"value\": null              }            ]          },          \"unit\": \"none\"        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 3,        \"y\": 43      },      \"id\": 52,      \"options\": {        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"/^lsf_hosts\\\\.last$/\",          \"values\": false        },        \"showThresholdLabels\": false,        \"showThresholdMarkers\": true      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_hosts\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"current\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ],            [              {                \"params\": [                  \"peak\"                ],                \"type\": \"field\"              }            ]          ],          \"tags\": [            {              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            },            {              \"condition\": \"AND\",              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"servers\"            }          ]        }      ],      \"title\": \"Servers\",      \"type\": \"gauge\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"min\": 0,          \"thresholds\": {            \"mode\": \"percentage\",            \"steps\": [              {                \"color\": \"yellow\",                \"value\": null              }            ]          },          \"unit\": \"none\"        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 6,        \"y\": 43      },      \"id\": 51,      \"options\": {        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"/^lsf_hosts\\\\.last$/\",          \"values\": false        },        \"showThresholdLabels\": false,        \"showThresholdMarkers\": true      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_hosts\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"current\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ],            [              {                \"params\": [                  \"peak\"                ],                \"type\": \"field\"              }            ]          ],          \"tags\": [            {              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            },            {              \"condition\": \"AND\",              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"cpus\"            }          ]        }      ],      \"title\": \"CPUs\",      \"type\": \"gauge\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"min\": 0,          \"thresholds\": {            \"mode\": \"percentage\",            \"steps\": [              {                \"color\": \"light-red\",                \"value\": null              }            ]          },          \"unit\": \"none\"        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 9,        \"y\": 43      },      \"id\": 50,      \"options\": {        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"/^lsf_hosts\\\\.last$/\",          \"values\": false        },        \"showThresholdLabels\": false,        \"showThresholdMarkers\": true      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_hosts\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"current\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ],            [              {                \"params\": [                  \"peak\"                ],                \"type\": \"field\"              }            ]          ],          \"tags\": [            {              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            },            {              \"condition\": \"AND\",              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"cores\"            }          ]        }      ],      \"title\": \"Cores\",      \"type\": \"gauge\"    },    {      \"datasource\": {        \"type\": \"influxdb\",        \"uid\": \"eNfWCy5Vk\"      },      \"description\": \"\",      \"fieldConfig\": {        \"defaults\": {          \"color\": {            \"mode\": \"thresholds\"          },          \"mappings\": [],          \"min\": 0,          \"thresholds\": {            \"mode\": \"percentage\",            \"steps\": [              {                \"color\": \"blue\",                \"value\": null              }            ]          },          \"unit\": \"none\"        },        \"overrides\": []      },      \"gridPos\": {        \"h\": 4,        \"w\": 3,        \"x\": 12,        \"y\": 43      },      \"id\": 49,      \"options\": {        \"orientation\": \"auto\",        \"reduceOptions\": {          \"calcs\": [            \"lastNotNull\"          ],          \"fields\": \"/^lsf_hosts\\\\.last$/\",          \"values\": false        },        \"showThresholdLabels\": false,        \"showThresholdMarkers\": true      },      \"pluginVersion\": \"9.1.6\",      \"targets\": [        {          \"datasource\": {            \"type\": \"influxdb\",            \"uid\": \"eNfWCy5Vk\"          },          \"groupBy\": [            {              \"params\": [                \"$__interval\"              ],              \"type\": \"time\"            },            {              \"params\": [                \"null\"              ],              \"type\": \"fill\"            }          ],          \"hide\": false,          \"measurement\": \"lsf_hosts\",          \"orderByTime\": \"ASC\",          \"policy\": \"autogen\",          \"refId\": \"A\",          \"resultFormat\": \"time_series\",          \"select\": [            [              {                \"params\": [                  \"current\"                ],                \"type\": \"field\"              },              {                \"params\": [],                \"type\": \"last\"              }            ],            [              {                \"params\": [                  \"peak\"                ],                \"type\": \"field\"              }            ]          ],          \"tags\": [            {              \"key\": \"host\",              \"operator\": \"=\",              \"value\": \"kilenc\"            },            {              \"condition\": \"AND\",              \"key\": \"state\",              \"operator\": \"=\",              \"value\": \"slots\"            }          ]        }      ],      \"title\": \"Slots\",      \"type\": \"gauge\"    }  ],  \"refresh\": \"30s\",  \"schemaVersion\": 37,  \"style\": \"dark\",  \"tags\": [],  \"templating\": {    \"list\": [      {        \"current\": {          \"selected\": true,          \"text\": [            \"priority\"          ],          \"value\": [            \"priority\"          ]        },        \"datasource\": {          \"type\": \"influxdb\",          \"uid\": \"oSnSlVc4k\"        },        \"definition\": \"show tag values from \\\"lsf_queues\\\" with key=\\\"name\\\"\",        \"hide\": 0,        \"includeAll\": false,        \"multi\": false,        \"name\": \"Queue\",        \"options\": [],        \"query\": \"show tag values from \\\"lsf_queues\\\" with key=\\\"name\\\"\",        \"refresh\": 1,        \"regex\": \"\",        \"skipUrlSync\": false,        \"sort\": 0,        \"tagValuesQuery\": \"\",        \"tagsQuery\": \"\",        \"type\": \"query\",        \"useTags\": false      }    ]  },  \"time\": {    \"from\": \"now-1h\",    \"to\": \"now\"  },  \"timepicker\": {},  \"timezone\": \"\",  \"title\": \"LSF cluster status\",  \"uid\": \"ORojp8cVz\",  \"version\": 160,  \"weekStart\": \"\"}As you can see, with a short plugin script to collect information from LSF, it&rsquo;s possible to monitor your LSF cluster using the TIG stack. It&rsquo;s important to note that there are powerfulmonitoring and reporting tools available from IBM as add-ons to LSF; IBM Spectrum LSF RTM and IBM Spectrum LSF Explorer. You can find more details about the add-on capabilities for LSFhere.",
            "content_html": "<p>Much like dashboards in automobiles, dashboards in the context of HPC infrastructure are crucial to get an understanding of what&rsquo;s happening under the hood of your HPC cluster - ata glance. During my IT career, I&rsquo;ve used a myriad of monitoring solutions ranging from SNMP and Ganglia, to the ELK (Elasticsearch, Logstash, Kibana) stack. For example, I&rsquo;ve recentlywritten an overview on how it is possible to visualize <a href=\"https://www.ibm.com/products/hpc-workload-management\">IBM Spectrum LSF</a> (LSF) data in Grafana. LSF is an HPC job scheduler which brings to the table three decades of experience inworkload and resource management.</p><p>For this blog, I decided to take this to the next level by monitoring IBM Spectrum LSF with the well known TIG (Telegraf, InfluxDB, Grafana) stack. This article is not meant to be adebate on the advantages of one monitoring stack over another. Rather, the focus is to demonstrate what is feasible in terms of monitoring Spectrum LSF clusters with the TIG stack,given the many available ways to query LSF for key information using CLI commands.</p><hr /><p><strong>The Journey</strong></p><p>There already exists many write-ups on how to deploy the TIG stack to monitor systems. This isn&rsquo;t meant to be a guide on setting up the TIG stack. Rather, it&rsquo;s assumed that the readeralready has some familiarity with the TIG stack. If not, then [<em>insert your favourite search engine</em>] is your friend.</p><p>On my home network, I decided to setup a VM running on my trusty <a href=\"https://traverse.com.au/products/ten64-networking-platform/\">Traverse Ten64</a> running Fedora where InfluxDB was installed. The idea was to run InfluxDB on a system that is guaranteedto be always on in my home environment and that is energy efficient. Installing telegraf on all of the LSF cluster servers (x3) proved to be straight forward. Note that in all cases, I used the OSsupplied versions of InfluxDB, Telegraf. Finally, I already had a Grafana server running on a server in my network.</p><p>Out of the box, Telegraf has the ability to monitor numerous system metrics. Furthermore, there exists literally hundreds of plugins for Telegraf to monitor a wide variety of devices,services and software. A search however, didn&rsquo;t reveal the existence of any plugin to monitor LSF. So it was time to get creative.</p><hr /><p><strong>What to monitor?</strong></p><p>A bit of research revealed that InfluxDB supports what is known as &ldquo;line protocol&rdquo;. This is a well defined text-based format for writing data to InfluxDB. I used the following<a href=\"https://docs.influxdata.com/influxdb/v1.8/write_protocols/line_protocol_tutorial/\">reference</a> on &ldquo;line protocol&rdquo; to guide me. Using line protocol it would be ultimately possible towrite a plugin for Telegraf to effecively scrape information from Spectrum LSF and output in line protocol format for writing to InfluxDB.</p><p>Before I could begin writing the plugin, the key was to determine what information from Spectrum LSF would be useful to display in the dashboard, and how that information could beextracted. For this I followed the KISS principle to keep things as simple as possible. The key metrics I decided to report on were servers, queues and jobs (oh my!), as well as processinformation for the LSF scheduler daemons. Refer to the following table for details:</p><hr /><table><thead><tr><th>Metric(s)</th><th>Command</th></tr></thead><tbody><tr><td>LSF scheduler performance metrics</td><td><em>badmin perfmon view -json</em></td></tr><tr><td>LSF available servers, CPUs, cores, slots</td><td><em>badmin showstatus</em></td></tr><tr><td>LSF server by status (total number Ok, closed, unreachable, unavailable)</td><td><em>badmin showstatus</em></td></tr><tr><td>LSF job statistics (total number running, suspended, pending)</td><td><em>badmin showstatus</em></td></tr><tr><td>LSF queue statistics (per queue, total number of jobs running, suspended, pending)</td><td><em>bqueues -json -o queue_name:12 njobs pend run susp rsv ususp ssusp</em></td></tr><tr><td>LSF mbatchd process metrics</td><td>(Telegraf - inputs.procstat)</td></tr><tr><td>LSF mbschd process metrics</td><td>(Telegraf - inputs.procstat)</td></tr><tr><td>LSF management lim process metrics</td><td>(Telegraf - inputs.procstat)</td></tr></tbody></table><hr /><p><strong>Scrapin' fun</strong></p><p>These above metrics would give a good idea of the state of the Spectrum LSF cluster at a glance. With the list of metrics prepared, the next step was to create a plugin script which wouldscrape data from the noted commands. Both <em>bqueues</em> and <em>badmin perfmon view</em> support output in JSON format with the appropriate flags specified. However, <em>badmin showstatus</em> does not supportoutput in JSON format. This meant that for <em>badmin showstatus</em> it was necessary to scrape data assuming hard coded field positions in the output.</p><p>A copy of the Telegraf plugin for Spectrum LSF is provided below. This is just an example and is provided &ldquo;as is&rdquo; for testing purposes. Your mileage may vary.</p><hr /><details>  <strong>Example lsf_telegraf_agent.py script. Click to expand!</strong>  <div class=\"highlight\"><pre><code class=\"language-python\"><span style=\"color: #75715e;\">#!/usr/bin/python3.8</span><span style=\"color: #75715e;\"># </span><span style=\"color: #75715e;\"># v0.9 </span><span style=\"color: #75715e;\"># Sample inputs.exec script for Telegraf which outputs metrics from an IBM Spectrum LSF management server</span><span style=\"color: #75715e;\"># in InfluxDB Line Protocol input format.</span><span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># NOTE: It is required to set the lsf_envfile variable to point to the LSF profile.lsf file</span><span style=\"color: #75715e;\"># for the LSF installation. </span><span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># Gabor Samu</span><span style=\"color: #75715e;\"># January 4, 2023</span><span style=\"color: #75715e;\">#</span> <span style=\"color: #f92672;\">import</span> os<span style=\"color: #f92672;\">import</span> json<span style=\"color: #f92672;\">import</span> time<span style=\"color: #f92672;\">import</span> subprocess<span style=\"color: #f92672;\">import</span> sys<span style=\"color: #f92672;\">from</span> pathlib <span style=\"color: #f92672;\">import</span> Path<span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># Variable declarations</span><span style=\"color: #75715e;\"># **NOTE: lsf_envfile needs to be set to point to the profile.lsf file for the LSF installation. </span><span style=\"color: #75715e;\">#</span>lsf_envfile <span style=\"color: #f92672;\">=</span> <span style=\"color: #e6db74;\">\"/opt/ibm/lsfsuite/lsf/conf/profile.lsf\"</span><span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># Source the Spectrum LSF profile.  </span><span style=\"color: #75715e;\"># Check for existing of lsf_envfile (profile.lsf) and source the environment. </span><span style=\"color: #75715e;\"># If the specified file does not exist, then exit.  </span><span style=\"color: #75715e;\">#</span>path <span style=\"color: #f92672;\">=</span> Path(lsf_envfile)<span style=\"color: #66d9ef;\">if</span> path<span style=\"color: #f92672;\">.</span>is_file():     lsf_env <span style=\"color: #f92672;\">=</span> (<span style=\"color: #e6db74;\">f</span><span style=\"color: #e6db74;\">'env -i sh -c \"source </span><span style=\"color: #e6db74;\">{</span>lsf_envfile<span style=\"color: #e6db74;\">}</span><span style=\"color: #e6db74;\"> &amp;&amp; env\"'</span>)    <span style=\"color: #66d9ef;\">for</span> line <span style=\"color: #f92672;\">in</span> subprocess<span style=\"color: #f92672;\">.</span>getoutput(lsf_env)<span style=\"color: #f92672;\">.</span>split(<span style=\"color: #e6db74;\">\"</span><span style=\"color: #ae81ff;\">\\n</span><span style=\"color: #e6db74;\">\"</span>):        key, value <span style=\"color: #f92672;\">=</span> line<span style=\"color: #f92672;\">.</span>split(<span style=\"color: #e6db74;\">\"=\"</span>)        os<span style=\"color: #f92672;\">.</span>environ[key]<span style=\"color: #f92672;\">=</span> value<span style=\"color: #66d9ef;\">else</span>:    sys<span style=\"color: #f92672;\">.</span>exit(<span style=\"color: #e6db74;\">f</span><span style=\"color: #e6db74;\">'The file </span><span style=\"color: #e6db74;\">{</span>lsf_envfile<span style=\"color: #e6db74;\">}</span><span style=\"color: #e6db74;\"> does not exist.'</span>)    <span style=\"color: #75715e;\"># </span><span style=\"color: #75715e;\"># Get the time in nanoseconds since the epoch. </span><span style=\"color: #75715e;\"># This is required as part of the InfluxDB line protocol reference. </span><span style=\"color: #75715e;\"># Only supported on Python 3.7+</span><span style=\"color: #75715e;\">#</span>time_nanosec <span style=\"color: #f92672;\">=</span> time<span style=\"color: #f92672;\">.</span>time_ns()<span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># Here we set the LSF environment variable LSB_NTRIES. This will be used to determine the </span><span style=\"color: #75715e;\"># number of retries before failure of a LSF batch command. This is used to cover the case </span><span style=\"color: #75715e;\"># when the LSF mbatchd is not running. </span><span style=\"color: #75715e;\">#</span>os<span style=\"color: #f92672;\">.</span>environ[<span style=\"color: #e6db74;\">\"LSB_NTRIES\"</span>] <span style=\"color: #f92672;\">=</span> <span style=\"color: #e6db74;\">\"2\"</span><span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># Check if LSF performance metric monitoring is enabled. This is done by running</span><span style=\"color: #75715e;\"># 'badmin perfmon view'. If badmin is not found, then exit. </span><span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># Check the return status from 'badmin perfmon view' and take the appropriate action:</span><span style=\"color: #75715e;\">#  - If return status is 7, it means that performance monitoring is not enabled. The script</span><span style=\"color: #75715e;\">#    will enable LSF performance metric monitoring by running 'badmin perfmon start'.</span><span style=\"color: #75715e;\">#    Note that a 70 second sleep is required before LSF metrics will be available.  </span><span style=\"color: #75715e;\">#  - If return status is 65, it means that the badmin command reported that the</span><span style=\"color: #75715e;\">#    LSF batch system is down. This is a fatal error which will cause the script</span><span style=\"color: #75715e;\">#    to exit. </span><span style=\"color: #75715e;\">#</span>lsf_path <span style=\"color: #f92672;\">=</span> os<span style=\"color: #f92672;\">.</span>environ[<span style=\"color: #e6db74;\">'LSF_BINDIR'</span>]badmin_path <span style=\"color: #f92672;\">=</span> lsf_path <span style=\"color: #f92672;\">+</span> <span style=\"color: #e6db74;\">\"/badmin\"</span>bqueues_path <span style=\"color: #f92672;\">=</span> lsf_path <span style=\"color: #f92672;\">+</span> <span style=\"color: #e6db74;\">\"/bqueues\"</span>path <span style=\"color: #f92672;\">=</span> Path(badmin_path)<span style=\"color: #66d9ef;\">if</span> path<span style=\"color: #f92672;\">.</span>is_file():    cmd <span style=\"color: #f92672;\">=</span> [badmin_path, <span style=\"color: #e6db74;\">'perfmon'</span>, <span style=\"color: #e6db74;\">'view'</span>]    p <span style=\"color: #f92672;\">=</span> subprocess<span style=\"color: #f92672;\">.</span>Popen(cmd, stdout<span style=\"color: #f92672;\">=</span>subprocess<span style=\"color: #f92672;\">.</span>DEVNULL, stderr<span style=\"color: #f92672;\">=</span>subprocess<span style=\"color: #f92672;\">.</span>DEVNULL)    <span style=\"color: #66d9ef;\">while</span> p<span style=\"color: #f92672;\">.</span>poll() <span style=\"color: #f92672;\">is</span> <span style=\"color: #66d9ef;\">None</span>:        time<span style=\"color: #f92672;\">.</span>sleep(<span style=\"color: #ae81ff;\">0.1</span>)    return_code <span style=\"color: #f92672;\">=</span> p<span style=\"color: #f92672;\">.</span>returncode    <span style=\"color: #66d9ef;\">if</span> return_code <span style=\"color: #f92672;\">==</span> <span style=\"color: #ae81ff;\">7</span>:        cmd <span style=\"color: #f92672;\">=</span> [badmin_path, <span style=\"color: #e6db74;\">'perfmon'</span>, <span style=\"color: #e6db74;\">'start'</span>]        p <span style=\"color: #f92672;\">=</span> subprocess<span style=\"color: #f92672;\">.</span>Popen(cmd, stdout<span style=\"color: #f92672;\">=</span>subprocess<span style=\"color: #f92672;\">.</span>DEVNULL, stderr<span style=\"color: #f92672;\">=</span>subprocess<span style=\"color: #f92672;\">.</span>DEVNULL)        <span style=\"color: #66d9ef;\">while</span> p<span style=\"color: #f92672;\">.</span>poll() <span style=\"color: #f92672;\">is</span> <span style=\"color: #66d9ef;\">None</span>:            time<span style=\"color: #f92672;\">.</span>sleep(<span style=\"color: #ae81ff;\">0.1</span>)        return_code <span style=\"color: #f92672;\">=</span> p<span style=\"color: #f92672;\">.</span>returncode        time<span style=\"color: #f92672;\">.</span>sleep(<span style=\"color: #ae81ff;\">70</span>)    <span style=\"color: #66d9ef;\">elif</span> return_code <span style=\"color: #f92672;\">==</span> <span style=\"color: #ae81ff;\">65</span>:        sys<span style=\"color: #f92672;\">.</span>exit(<span style=\"color: #e6db74;\">f</span><span style=\"color: #e6db74;\">'The LSF batch system is down.'</span>)<span style=\"color: #66d9ef;\">else</span>:    sys<span style=\"color: #f92672;\">.</span>exit(<span style=\"color: #e6db74;\">f</span><span style=\"color: #e6db74;\">'</span><span style=\"color: #e6db74;\">{</span>badmin_path<span style=\"color: #e6db74;\">}</span><span style=\"color: #e6db74;\"> does not exist.'</span>)<span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># Run badmin with the \"perfmon view\" keywords and the -json option to product JSON output</span><span style=\"color: #75715e;\"># We assume here that the LSF batch system is responsive (a check was done above); if</span><span style=\"color: #75715e;\"># the mbatchd is very busy there is a possiblity that it may not be responsive here. This</span><span style=\"color: #75715e;\"># case is not considered; LSB_NTRIES setting will determine how many tries are made before</span><span style=\"color: #75715e;\"># badmin gives up the ghost.  </span><span style=\"color: #75715e;\"># </span><span style=\"color: #75715e;\"># Note: We previously checked for the existence of the 'badmin' binary. </span><span style=\"color: #75715e;\">#</span>cmd <span style=\"color: #f92672;\">=</span> [badmin_path, <span style=\"color: #e6db74;\">'perfmon'</span>, <span style=\"color: #e6db74;\">'view'</span>, <span style=\"color: #e6db74;\">'-json'</span>] p <span style=\"color: #f92672;\">=</span> subprocess<span style=\"color: #f92672;\">.</span>Popen(cmd, stdout<span style=\"color: #f92672;\">=</span>subprocess<span style=\"color: #f92672;\">.</span>PIPE, stderr<span style=\"color: #f92672;\">=</span>subprocess<span style=\"color: #f92672;\">.</span>DEVNULL, text<span style=\"color: #f92672;\">=</span><span style=\"color: #66d9ef;\">True</span>) stdout, stderr <span style=\"color: #f92672;\">=</span> p<span style=\"color: #f92672;\">.</span>communicate()<span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># Guard for the case that the performance monitor has just been enabled, but is not</span><span style=\"color: #75715e;\"># producing any data as the first sample period has not elapsed. </span><span style=\"color: #75715e;\">#</span><span style=\"color: #66d9ef;\">if</span> stdout <span style=\"color: #f92672;\">==</span> <span style=\"color: #e6db74;\">\"\"</span>:    sys<span style=\"color: #f92672;\">.</span>exit(<span style=\"color: #e6db74;\">f</span><span style=\"color: #e6db74;\">'Output from badmin perfmon view -json is empty.'</span>)<span style=\"color: #66d9ef;\">else</span>:     data <span style=\"color: #f92672;\">=</span> json<span style=\"color: #f92672;\">.</span>loads(stdout)<span style=\"color: #75715e;\"># </span><span style=\"color: #75715e;\"># Run badmin showstatus</span><span style=\"color: #75715e;\"># Next, run the command 'badmin showstatus' and capture the output. Note that badmin showstatus</span><span style=\"color: #75715e;\"># does not produce JSON output. So here we must do some scraping of the output. </span><span style=\"color: #75715e;\"># The output from 'badmin showstatus' it placed into the array 'showstatus'. The hard coded</span><span style=\"color: #75715e;\"># positions in the output of 'badmin showstatus' are assumed when building the output </span><span style=\"color: #75715e;\"># strings below. Should the format of the output of 'badmin showstatus' change, this will</span><span style=\"color: #75715e;\"># need to be updated. </span>cmd <span style=\"color: #f92672;\">=</span> [badmin_path, <span style=\"color: #e6db74;\">'showstatus'</span>]p <span style=\"color: #f92672;\">=</span> subprocess<span style=\"color: #f92672;\">.</span>Popen(cmd, stdout<span style=\"color: #f92672;\">=</span>subprocess<span style=\"color: #f92672;\">.</span>PIPE, stderr<span style=\"color: #f92672;\">=</span>subprocess<span style=\"color: #f92672;\">.</span>DEVNULL, text<span style=\"color: #f92672;\">=</span><span style=\"color: #66d9ef;\">True</span>)stdout, stderr <span style=\"color: #f92672;\">=</span> p<span style=\"color: #f92672;\">.</span>communicate()<span style=\"color: #75715e;\"># Convert badmin showstatus output into an array</span>showstatus <span style=\"color: #f92672;\">=</span> stdout<span style=\"color: #f92672;\">.</span>split()<span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># Run bqueues</span><span style=\"color: #75715e;\">#</span>cmd <span style=\"color: #f92672;\">=</span> [bqueues_path, <span style=\"color: #e6db74;\">'-json'</span>, <span style=\"color: #e6db74;\">'-o'</span>, <span style=\"color: #e6db74;\">'queue_name:12 njobs pend run susp rsv ususp ssusp'</span>]p <span style=\"color: #f92672;\">=</span> subprocess<span style=\"color: #f92672;\">.</span>Popen(cmd, stdout<span style=\"color: #f92672;\">=</span>subprocess<span style=\"color: #f92672;\">.</span>PIPE, stderr<span style=\"color: #f92672;\">=</span>subprocess<span style=\"color: #f92672;\">.</span>DEVNULL, text<span style=\"color: #f92672;\">=</span><span style=\"color: #66d9ef;\">True</span>)stdout, stderr <span style=\"color: #f92672;\">=</span> p<span style=\"color: #f92672;\">.</span>communicate()data_queues <span style=\"color: #f92672;\">=</span> json<span style=\"color: #f92672;\">.</span>loads(stdout)<span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># At this stage, we've captured the output from 'badmin perfmon view -json' and </span><span style=\"color: #75715e;\"># 'badmin showstatus'. We're now ready to print to standard output the metric</span><span style=\"color: #75715e;\"># strings in InfluxDB line procotol format. </span><span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># Details about the line protocol format can be found here:</span><span style=\"color: #75715e;\"># https://docs.influxdata.com/influxdb/v2.6/reference/syntax/line-protocol/</span><span style=\"color: #75715e;\"># </span><span style=\"color: #75715e;\"># </span><span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># LSF server status</span><span style=\"color: #75715e;\">#</span>print(<span style=\"color: #e6db74;\">\"lsf_servers,\"</span>,<span style=\"color: #e6db74;\">\"status=total\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,showstatus[<span style=\"color: #ae81ff;\">21</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_servers,\"</span>,<span style=\"color: #e6db74;\">\"status=ok\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,showstatus[<span style=\"color: #ae81ff;\">23</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_servers,\"</span>,<span style=\"color: #e6db74;\">\"status=closed\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,showstatus[<span style=\"color: #ae81ff;\">25</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_servers,\"</span>,<span style=\"color: #e6db74;\">\"status=unreachable\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,showstatus[<span style=\"color: #ae81ff;\">27</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_servers,\"</span>,<span style=\"color: #e6db74;\">\"status=unavailable\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,showstatus[<span style=\"color: #ae81ff;\">29</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)<span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># LSF job status</span><span style=\"color: #75715e;\">#</span>print(<span style=\"color: #e6db74;\">\"lsf_jobs,\"</span>,<span style=\"color: #e6db74;\">\"state=total\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,showstatus[<span style=\"color: #ae81ff;\">33</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_jobs,\"</span>,<span style=\"color: #e6db74;\">\"state=running\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,showstatus[<span style=\"color: #ae81ff;\">35</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_jobs,\"</span>,<span style=\"color: #e6db74;\">\"state=suspended\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,showstatus[<span style=\"color: #ae81ff;\">37</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_jobs,\"</span>,<span style=\"color: #e6db74;\">\"state=pending\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,showstatus[<span style=\"color: #ae81ff;\">39</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_jobs,\"</span>,<span style=\"color: #e6db74;\">\"state=finished\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,showstatus[<span style=\"color: #ae81ff;\">41</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)<span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># LSF user stats</span><span style=\"color: #75715e;\">#</span>print(<span style=\"color: #e6db74;\">\"lsf_users,\"</span>,<span style=\"color: #e6db74;\">\"state=numusers\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,showstatus[<span style=\"color: #ae81ff;\">45</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_users,\"</span>,<span style=\"color: #e6db74;\">\"state=numgroups\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,showstatus[<span style=\"color: #ae81ff;\">50</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_users,\"</span>,<span style=\"color: #e6db74;\">\"state=numactive\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,showstatus[<span style=\"color: #ae81ff;\">55</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)<span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># LSF hosts stats</span><span style=\"color: #75715e;\"># First we split out the current and peak values for clients, servers, cpus, cores, and slots.</span><span style=\"color: #75715e;\"># The current and peak values are separated by the \"/\" delimiter.</span><span style=\"color: #75715e;\"># </span>clientssplit <span style=\"color: #f92672;\">=</span> showstatus[<span style=\"color: #ae81ff;\">9</span>]<span style=\"color: #f92672;\">.</span>split(<span style=\"color: #e6db74;\">\"/\"</span>)serverssplit <span style=\"color: #f92672;\">=</span> showstatus[<span style=\"color: #ae81ff;\">11</span>]<span style=\"color: #f92672;\">.</span>split(<span style=\"color: #e6db74;\">\"/\"</span>)cpussplit <span style=\"color: #f92672;\">=</span> showstatus[<span style=\"color: #ae81ff;\">13</span>]<span style=\"color: #f92672;\">.</span>split(<span style=\"color: #e6db74;\">\"/\"</span>)coressplit <span style=\"color: #f92672;\">=</span> showstatus[<span style=\"color: #ae81ff;\">15</span>]<span style=\"color: #f92672;\">.</span>split(<span style=\"color: #e6db74;\">\"/\"</span>)slotssplit <span style=\"color: #f92672;\">=</span> showstatus[<span style=\"color: #ae81ff;\">17</span>]<span style=\"color: #f92672;\">.</span>split(<span style=\"color: #e6db74;\">\"/\"</span>)print(<span style=\"color: #e6db74;\">\"lsf_hosts,\"</span>,<span style=\"color: #e6db74;\">\"state=clients\"</span>,<span style=\"color: #e6db74;\">\" current=\"</span>,clientssplit[<span style=\"color: #ae81ff;\">0</span>],<span style=\"color: #e6db74;\">\"i,\"</span>,<span style=\"color: #e6db74;\">\"peak=\"</span>,clientssplit[<span style=\"color: #ae81ff;\">1</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_hosts,\"</span>,<span style=\"color: #e6db74;\">\"state=servers\"</span>,<span style=\"color: #e6db74;\">\" current=\"</span>,serverssplit[<span style=\"color: #ae81ff;\">0</span>],<span style=\"color: #e6db74;\">\"i,\"</span>,<span style=\"color: #e6db74;\">\"peak=\"</span>,serverssplit[<span style=\"color: #ae81ff;\">1</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_hosts,\"</span>,<span style=\"color: #e6db74;\">\"state=cpus\"</span>,<span style=\"color: #e6db74;\">\" current=\"</span>,cpussplit[<span style=\"color: #ae81ff;\">0</span>],<span style=\"color: #e6db74;\">\"i,\"</span>,<span style=\"color: #e6db74;\">\"peak=\"</span>,cpussplit[<span style=\"color: #ae81ff;\">1</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_hosts,\"</span>,<span style=\"color: #e6db74;\">\"state=cores\"</span>,<span style=\"color: #e6db74;\">\" current=\"</span>,coressplit[<span style=\"color: #ae81ff;\">0</span>],<span style=\"color: #e6db74;\">\"i,\"</span>,<span style=\"color: #e6db74;\">\"peak=\"</span>,coressplit[<span style=\"color: #ae81ff;\">1</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_hosts,\"</span>,<span style=\"color: #e6db74;\">\"state=slots\"</span>,<span style=\"color: #e6db74;\">\" current=\"</span>,slotssplit[<span style=\"color: #ae81ff;\">0</span>],<span style=\"color: #e6db74;\">\"i,\"</span>,<span style=\"color: #e6db74;\">\"peak=\"</span>,slotssplit[<span style=\"color: #ae81ff;\">1</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)<span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># Print mbatchd query metrics</span><span style=\"color: #75715e;\">#</span>print(<span style=\"color: #e6db74;\">\"lsf_mbatchd,\"</span>,<span style=\"color: #e6db74;\">\"query=job\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,data[<span style=\"color: #e6db74;\">'record'</span>][<span style=\"color: #ae81ff;\">1</span>][<span style=\"color: #e6db74;\">'current'</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_mbatchd,\"</span>,<span style=\"color: #e6db74;\">\"query=host\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,data[<span style=\"color: #e6db74;\">'record'</span>][<span style=\"color: #ae81ff;\">2</span>][<span style=\"color: #e6db74;\">'current'</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_mbatchd,\"</span>,<span style=\"color: #e6db74;\">\"query=queue\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,data[<span style=\"color: #e6db74;\">'record'</span>][<span style=\"color: #ae81ff;\">3</span>][<span style=\"color: #e6db74;\">'current'</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)<span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># Print mbatchd job metrics</span><span style=\"color: #75715e;\">#</span>print(<span style=\"color: #e6db74;\">\"lsf_mbatchd,\"</span>,<span style=\"color: #e6db74;\">\"jobs=submitreqs\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,data[<span style=\"color: #e6db74;\">'record'</span>][<span style=\"color: #ae81ff;\">4</span>][<span style=\"color: #e6db74;\">'current'</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_mbatchd,\"</span>,<span style=\"color: #e6db74;\">\"jobs=submitted\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,data[<span style=\"color: #e6db74;\">'record'</span>][<span style=\"color: #ae81ff;\">5</span>][<span style=\"color: #e6db74;\">'current'</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_mbatchd,\"</span>,<span style=\"color: #e6db74;\">\"jobs=dispatched\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,data[<span style=\"color: #e6db74;\">'record'</span>][<span style=\"color: #ae81ff;\">6</span>][<span style=\"color: #e6db74;\">'current'</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_mbatchd,\"</span>,<span style=\"color: #e6db74;\">\"jobs=completed\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,data[<span style=\"color: #e6db74;\">'record'</span>][<span style=\"color: #ae81ff;\">7</span>][<span style=\"color: #e6db74;\">'current'</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_mbatchd,\"</span>,<span style=\"color: #e6db74;\">\"jobs=sentremote\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,data[<span style=\"color: #e6db74;\">'record'</span>][<span style=\"color: #ae81ff;\">8</span>][<span style=\"color: #e6db74;\">'current'</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_mbatchd,\"</span>,<span style=\"color: #e6db74;\">\"jobs=acceptremote\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,data[<span style=\"color: #e6db74;\">'record'</span>][<span style=\"color: #ae81ff;\">9</span>][<span style=\"color: #e6db74;\">'current'</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">'</span><span style=\"color: #e6db74;\">')</span>print(<span style=\"color: #e6db74;\">\"lsf_mbatchd,\"</span>,<span style=\"color: #e6db74;\">\"sched=interval\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,data[<span style=\"color: #e6db74;\">'record'</span>][<span style=\"color: #ae81ff;\">10</span>][<span style=\"color: #e6db74;\">'current'</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_mbatchd,\"</span>,<span style=\"color: #e6db74;\">\"sched=matchhost\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,data[<span style=\"color: #e6db74;\">'record'</span>][<span style=\"color: #ae81ff;\">11</span>][<span style=\"color: #e6db74;\">'current'</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_mbatchd,\"</span>,<span style=\"color: #e6db74;\">\"sched=buckets\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,data[<span style=\"color: #e6db74;\">'record'</span>][<span style=\"color: #ae81ff;\">12</span>][<span style=\"color: #e6db74;\">'current'</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_mbatchd,\"</span>,<span style=\"color: #e6db74;\">\"sched=reordered\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,data[<span style=\"color: #e6db74;\">'record'</span>][<span style=\"color: #ae81ff;\">13</span>][<span style=\"color: #e6db74;\">'current'</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)<span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># Print mbatchd efficiency metrics. Here check if the efficiency metric indicated is \"-\". If so, </span><span style=\"color: #75715e;\"># then assume a zero value. The trailing \"%\" sign on the metrics (percentages) is also stripped here. </span><span style=\"color: #75715e;\">#</span>slots <span style=\"color: #f92672;\">=</span> (data[<span style=\"color: #e6db74;\">'record'</span>][<span style=\"color: #ae81ff;\">14</span>][<span style=\"color: #e6db74;\">'current'</span>])slots_percent <span style=\"color: #f92672;\">=</span> slots<span style=\"color: #66d9ef;\">if</span> slots_percent <span style=\"color: #f92672;\">==</span> <span style=\"color: #e6db74;\">\"-\"</span>:    slots_percent <span style=\"color: #f92672;\">=</span> <span style=\"color: #e6db74;\">\"0\"</span><span style=\"color: #66d9ef;\">elif</span> slots_percent <span style=\"color: #f92672;\">!=</span> <span style=\"color: #e6db74;\">\"0\"</span>:    <span style=\"color: #75715e;\"># Strip % sign and decimal. This is to work around issue inserting float to InfluxDB</span>    <span style=\"color: #75715e;\"># \"type float, already exists as type integer dropped ...\"</span>    slots_percent <span style=\"color: #f92672;\">=</span> slots[:<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">4</span>]memory <span style=\"color: #f92672;\">=</span> (data[<span style=\"color: #e6db74;\">'record'</span>][<span style=\"color: #ae81ff;\">15</span>][<span style=\"color: #e6db74;\">'current'</span>])memory_percent <span style=\"color: #f92672;\">=</span> memory<span style=\"color: #66d9ef;\">if</span> memory_percent <span style=\"color: #f92672;\">==</span> <span style=\"color: #e6db74;\">\"-\"</span>:    memory_percent <span style=\"color: #f92672;\">=</span> <span style=\"color: #e6db74;\">\"0\"</span><span style=\"color: #66d9ef;\">elif</span> memory_percent <span style=\"color: #f92672;\">!=</span> <span style=\"color: #e6db74;\">\"0\"</span>:    <span style=\"color: #75715e;\"># Strip % sign and decimal. This is to work around issue inserting float to InfluxDB</span>    <span style=\"color: #75715e;\"># \"type float, already exists as type integer dropped ...\"</span>    memory_percent <span style=\"color: #f92672;\">=</span> memory[:<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">4</span>]print(<span style=\"color: #e6db74;\">\"lsf_mbatchd,\"</span>,<span style=\"color: #e6db74;\">\"utilization=slots\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,slots_percent,<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_mbatchd,\"</span>,<span style=\"color: #e6db74;\">\"utilization=memory\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,memory_percent,<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)<span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># Print mbatchd file descriptor usage</span><span style=\"color: #75715e;\">#</span>print(<span style=\"color: #e6db74;\">\"lsf_mbatchd,\"</span>,<span style=\"color: #e6db74;\">\"fd=free\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,data[<span style=\"color: #e6db74;\">'fd'</span>][<span style=\"color: #e6db74;\">'free'</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_mbatchd,\"</span>,<span style=\"color: #e6db74;\">\"fd=used\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,data[<span style=\"color: #e6db74;\">'fd'</span>][<span style=\"color: #e6db74;\">'used'</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)print(<span style=\"color: #e6db74;\">\"lsf_mbatchd,\"</span>,<span style=\"color: #e6db74;\">\"fd=total\"</span>,<span style=\"color: #e6db74;\">\" value=\"</span>,data[<span style=\"color: #e6db74;\">'fd'</span>][<span style=\"color: #e6db74;\">'total'</span>],<span style=\"color: #e6db74;\">\"i \"</span>,time_nanosec,sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)<span style=\"color: #75715e;\">#</span><span style=\"color: #75715e;\"># Print LSF queue status (njobs)</span><span style=\"color: #75715e;\">#</span>iterations <span style=\"color: #f92672;\">=</span> data_queues[<span style=\"color: #e6db74;\">\"QUEUES\"</span>]<span style=\"color: #66d9ef;\">for</span> n <span style=\"color: #f92672;\">in</span> range(iterations):    print(<span style=\"color: #e6db74;\">\"lsf_queues,\"</span>,<span style=\"color: #e6db74;\">\"name=\"</span>, data_queues[<span style=\"color: #e6db74;\">'RECORDS'</span>][n][<span style=\"color: #e6db74;\">'QUEUE_NAME'</span>], <span style=\"color: #e6db74;\">\" njobs=\"</span>, data_queues[<span style=\"color: #e6db74;\">'RECOR</span>DS<span style=\"color: #e6db74;\">'][n]['</span>NJOBS<span style=\"color: #e6db74;\">'],\"i,\",</span>          <span style=\"color: #e6db74;\">\"pend=\"</span>, data_queues[<span style=\"color: #e6db74;\">'RECORDS'</span>][n][<span style=\"color: #e6db74;\">'PEND'</span>],<span style=\"color: #e6db74;\">\"i,\"</span>,          <span style=\"color: #e6db74;\">\"run=\"</span>, data_queues[<span style=\"color: #e6db74;\">'RECORDS'</span>][n][<span style=\"color: #e6db74;\">'RUN'</span>],<span style=\"color: #e6db74;\">\"i,\"</span>,          <span style=\"color: #e6db74;\">\"susp=\"</span>, data_queues[<span style=\"color: #e6db74;\">'RECORDS'</span>][n][<span style=\"color: #e6db74;\">'SUSP'</span>],<span style=\"color: #e6db74;\">\"i,\"</span>,          <span style=\"color: #e6db74;\">\"rsv=\"</span>, data_queues[<span style=\"color: #e6db74;\">'RECORDS'</span>][n][<span style=\"color: #e6db74;\">'RSV'</span>],<span style=\"color: #e6db74;\">\"i,\"</span>,          <span style=\"color: #e6db74;\">\"ususp=\"</span>, data_queues[<span style=\"color: #e6db74;\">'RECORDS'</span>][n][<span style=\"color: #e6db74;\">'USUSP'</span>],<span style=\"color: #e6db74;\">\"i,\"</span>,          <span style=\"color: #e6db74;\">\"ssusp=\"</span>, data_queues[<span style=\"color: #e6db74;\">'RECORDS'</span>][n][<span style=\"color: #e6db74;\">'SSUSP'</span>],<span style=\"color: #e6db74;\">\"i \"</span>,          time_nanosec, sep<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">''</span>)exit()    </code></pre></div></details><hr /><p><strong>Bringing it all together</strong></p><p>For completeness, below is the detail regarding the configuration of the environment. It should be noted that the simple test environment consists of a single server running IBMSpectrum LSF Suite for HPC and a separate server which runs the InfluxDB instance.</p><hr /><table><thead><tr><th style=\"text-align: left;\">Hostname</th><th>Component</th><th>Version</th></tr></thead><tbody><tr><td style=\"text-align: left;\"><em>kilenc</em></td><td>OS (LSF mgmt server)</td><td><em>CentOS Stream release 8 (ppc64le)</em></td></tr><tr><td style=\"text-align: left;\"><em>kilenc</em></td><td>Spectrum LSF Suite for HPC</td><td><em>v10.2.0.13</em></td></tr><tr><td style=\"text-align: left;\"><em>adatbazis</em></td><td>OS (InfluxDB server)</td><td><em>Fedora release 36 (aarch64)</em></td></tr><tr><td style=\"text-align: left;\"><em>adatbazis</em></td><td>InfluxDB</td><td><em>v1.8.10</em></td></tr><tr><td style=\"text-align: left;\"><em>kilenc</em></td><td>Telegraf</td><td><em>v1.24.3</em></td></tr><tr><td style=\"text-align: left;\"><em>kilenc</em></td><td>Grafana</td><td><em>v9.1.6</em></td></tr></tbody></table><hr /><p>The follwing steps assume that IBM Spectrum LSF Suite for HPC, InfluxDB and Telegraf have been installed.</p><ol><li><p>Start InfluxDB on the host <em>adatbazis</em></p></li><li><p>On the LSF management server <em>kilenc</em>, configure telegraf to connect to the influxDB instance on host <em>adatbazis</em>. Edit the configuration <em>/etc/telegraf/telegraf.conf</em> and specifythe correct URL in the <em>outputs.influxdb</em> section as follows:</p></li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\"># # Configuration for sending metrics to InfluxDB[[outputs.influxdb]]#   ## The full HTTP or UDP URL for your InfluxDB instance.#   ###   ## Multiple URLs can be specified for a single cluster, only ONE of the#   ## urls will be written to each interval.#   # urls = [\"unix:///var/run/influxdb.sock\"]#   # urls = [\"udp://127.0.0.1:8089\"]#   # urls = [\"http://127.0.0.1:8086\"]# Added gsamu Jan 04 2023urls = [\"http://adatbazis:8086\"]</code></pre></div><ol start=\"3\"><li>On the LSF management server <em>kilenc</em>, configure telegraf with the custom plugin script <em>lsf_telegraf_agent_0.9.py</em> to collect and log metrics from IBM Spectrum LSF Suite for HPC.Edit the configuration <em>/etc/telegraf/telegraf.conf</em> and specify the correct command path in the section <em>inputs.exec</em>. Additionally, set <em>data_format</em> equal to <em>influx</em>.Note that thescript <em>lsf_telegraf_agent_0.9.py</em> was copied to the directory <em>/etc/telegraf/telegraf.d/scripts</em> with permissions octal 755 and owner set to user <em>telegraf</em>.<strong>Note:</strong> User <em>telegraf</em> was automatically created during the installation of telegraf.</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\"> # ## Gather LSF metrics[[inputs.exec]]  ## Commands array   commands = [  \"/etc/telegraf/telegraf.d/scripts/lsf_telegraf_agent_0.9.py\" ]   timeout = \"30s\"   interval = \"30s\"   data_format = \"influx\" # ## End LSF metrics</code></pre></div><ol start=\"4\"><li>Telegraf provides the ability to collect metrics on processes. Here we&rsquo;ll use the telegraf <em>procstat</em> facility to monitor the LSF mbatchd and mbschd processes. These are the keydaemons involved in handling query requests and making scheduling decisions for jobs in the environment. Edit the configuration <em>/etc/telegraf/telegraf.conf</em> and configure the twofollowing <em>inputs.procstat</em> sections.</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\"># ## Monitor CPU and memory utilization for LSF processes# ## mbatchd, mbschd, lim (manager)[[inputs.procstat]]exe = \"lim\"pattern = \"lim\"pid_finder = \"pgrep\"[[inputs.procstat]]exe = \"mbschd\"pattern = \"mbschd\"pid_finder = \"pgrep\"[[inputs.procstat]]exe = \"mbatchd\"pattern = \"mbatchd\"pid_finder = \"pgrep\"</code></pre></div><ol start=\"5\"><li>With the configuration to telegraf complete, it&rsquo;s now time to test if the configuration and custom LSF agent is functioning as expected. Note that the following operation is performedon the LSF management candidate host <em>kilenc</em> and assumes that the LSF daemons are up and running. This is achieve by running the command:<em>telegraf &ndash;config /etc/telegraf/telegraf.conf &ndash;test</em>. <strong>Note:</strong> Any errors in the configuration file <em>/etc/telegraf/telegraf.conf</em> will result in errors in the output.</li></ol><hr /><details>  <strong>Output of <em>telegraf &ndash;config /etc/telegraf/telegraf.conf &ndash;test</em>. Click to expand!</strong>  <div class=\"highlight\"><pre><code class=\"language-python\">[root<span style=\"color: #a6e22e;\">@kilenc</span> telegraf]<span style=\"color: #75715e;\"># pwd</span><span style=\"color: #f92672;\">/</span>etc<span style=\"color: #f92672;\">/</span>telegraf[root<span style=\"color: #a6e22e;\">@kilenc</span> telegraf]<span style=\"color: #75715e;\"># telegraf --config /etc/telegraf/telegraf.conf --test</span><span style=\"color: #f92672;\">&gt;</span> mem,host<span style=\"color: #f92672;\">=</span>kilenc active<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1938817024</span>i,available<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">6820003840</span>i,available_percent<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">20.653390597462806</span>,buffered<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4849664</span>i,cached<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">6317735936</span>i,commit_limit<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">33560395776</span>i,committed_as<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">18635292672</span>i,dirty<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4128768</span>i,free<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2623799296</span>i,high_free<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,high_total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,huge_page_size<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2097152</span>i,huge_pages_free<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,huge_pages_total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,inactive<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">13852016640</span>i,low_free<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,low_total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,mapped<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1007353856</span>i,page_tables<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">22478848</span>i,shared<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">259063808</span>i,slab<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4946919424</span>i,sreclaimable<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">902234112</span>i,sunreclaim<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4044685312</span>i,swap_cached<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">3866624</span>i,swap_free<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">16994729984</span>i,swap_total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">17049780224</span>i,total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">33021231104</span>i,used<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">24074846208</span>i,used_percent<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">72.90717336424115</span>,vmalloc_chunk<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,vmalloc_total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">562949953421312</span>i,vmalloc_used<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,write_back<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,write_back_tmp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> kernel,host<span style=\"color: #f92672;\">=</span>kilenc boot_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1673790850</span>i,context_switches<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1943864437</span>i,entropy_avail<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4037</span>i,interrupts<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1294179599</span>i,processes_forked<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4255316</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> swap,host<span style=\"color: #f92672;\">=</span>kilenc free<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">16994729984</span>i,total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">17049780224</span>i,used<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">55050240</span>i,used_percent<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0.3228794698626609</span> <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> swap,host<span style=\"color: #f92672;\">=</span>kilenc <span style=\"color: #f92672;\">in</span><span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">172032</span>i,out<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">851968</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> net,host<span style=\"color: #f92672;\">=</span>kilenc,interface<span style=\"color: #f92672;\">=</span>lo bytes_recv<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">90039931116</span>i,bytes_sent<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">90039931116</span>i,drop_in<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,drop_out<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,err_in<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,err_out<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,packets_recv<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">17245997</span>i,packets_sent<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">17245997</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> net,host<span style=\"color: #f92672;\">=</span>kilenc,interface<span style=\"color: #f92672;\">=</span>enP4p1s0f0 bytes_recv<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,bytes_sent<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,drop_in<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,drop_out<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,err_in<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,err_out<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,packets_recv<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,packets_sent<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> net,host<span style=\"color: #f92672;\">=</span>kilenc,interface<span style=\"color: #f92672;\">=</span>enP4p1s0f1 bytes_recv<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">11791041280</span>i,bytes_sent<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1701152001</span>i,drop_in<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,drop_out<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,err_in<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,err_out<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,packets_recv<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">10322276</span>i,packets_sent<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4594948</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> net,host<span style=\"color: #f92672;\">=</span>kilenc,interface<span style=\"color: #f92672;\">=</span>all icmp_inaddrmaskreps<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,icmp_inaddrmasks<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,icmp_incsumerrors<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,icmp_indestunreachs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">8609</span>i,icmp_inechoreps<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">20</span>i,icmp_inechos<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">11</span>i,icmp_inerrors<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1084</span>i,icmp_inmsgs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">8640</span>i,icmp_inparmprobs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,icmp_inredirects<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,icmp_insrcquenchs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,icmp_intimeexcds<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,icmp_intimestampreps<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,icmp_intimestamps<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,icmp_outaddrmaskreps<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,icmp_outaddrmasks<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,icmp_outdestunreachs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4805</span>i,icmp_outechoreps<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">11</span>i,icmp_outechos<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">94</span>i,icmp_outerrors<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,icmp_outmsgs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4910</span>i,icmp_outparmprobs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,icmp_outredirects<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,icmp_outsrcquenchs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,icmp_outtimeexcds<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,icmp_outtimestampreps<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,icmp_outtimestamps<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,icmpmsg_intype0<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">20</span>i,icmpmsg_intype3<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">8609</span>i,icmpmsg_intype8<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">11</span>i,icmpmsg_outtype0<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">11</span>i,icmpmsg_outtype3<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4805</span>i,icmpmsg_outtype8<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">94</span>i,ip_defaultttl<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">64</span>i,ip_forwarding<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1</span>i,ip_forwdatagrams<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ip_fragcreates<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">62958</span>i,ip_fragfails<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ip_fragoks<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">12611</span>i,ip_inaddrerrors<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1</span>i,ip_indelivers<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">21324370</span>i,ip_indiscards<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ip_inhdrerrors<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ip_inreceives<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">21324371</span>i,ip_inunknownprotos<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ip_outdiscards<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ip_outnoroutes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">30</span>i,ip_outrequests<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">21248264</span>i,ip_reasmfails<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ip_reasmoks<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ip_reasmreqds<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ip_reasmtimeout<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,tcp_activeopens<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">763497</span>i,tcp_attemptfails<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">96617</span>i,tcp_currestab<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">118</span>i,tcp_estabresets<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1917</span>i,tcp_incsumerrors<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,tcp_inerrs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,tcp_insegs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">19488475</span>i,tcp_maxconn<span style=\"color: #f92672;\">=-</span><span style=\"color: #ae81ff;\">1</span>i,tcp_outrsts<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">137188</span>i,tcp_outsegs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">20220038</span>i,tcp_passiveopens<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">675805</span>i,tcp_retranssegs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9827</span>i,tcp_rtoalgorithm<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1</span>i,tcp_rtomax<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">120000</span>i,tcp_rtomin<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">200</span>i,udp_ignoredmulti<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">10509</span>i,udp_incsumerrors<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,udp_indatagrams<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1816997</span>i,udp_inerrors<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,udp_memerrors<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,udp_noports<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">264</span>i,udp_outdatagrams<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1506724</span>i,udp_rcvbuferrors<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,udp_sndbuferrors<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,udplite_ignoredmulti<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,udplite_incsumerrors<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,udplite_indatagrams<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,udplite_inerrors<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,udplite_memerrors<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,udplite_noports<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,udplite_outdatagrams<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,udplite_rcvbuferrors<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,udplite_sndbuferrors<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> diskio,host<span style=\"color: #f92672;\">=</span>kilenc,name<span style=\"color: #f92672;\">=</span>dm<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span> io_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9739370</span>i,iops_in_progress<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,merged_reads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,merged_writes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,read_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4015612416</span>i,read_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">604060</span>i,reads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">40592</span>i,weighted_io_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">60563370</span>i,write_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">47025459712</span>i,write_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">59959310</span>i,writes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1079691</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> diskio,host<span style=\"color: #f92672;\">=</span>kilenc,name<span style=\"color: #f92672;\">=</span>sda1 io_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1460</span>i,iops_in_progress<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,merged_reads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,merged_writes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,read_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4849664</span>i,read_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1304</span>i,reads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">67</span>i,weighted_io_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1304</span>i,write_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,write_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,writes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> diskio,host<span style=\"color: #f92672;\">=</span>kilenc,name<span style=\"color: #f92672;\">=</span>sda3 io_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">45872430</span>i,iops_in_progress<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,merged_reads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">623</span>i,merged_writes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1061314</span>i,read_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">16398521856</span>i,read_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">3371612</span>i,reads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">139298</span>i,weighted_io_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">311521720</span>i,write_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">133715422208</span>i,write_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">308150107</span>i,writes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">7031512</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> diskio,host<span style=\"color: #f92672;\">=</span>kilenc,name<span style=\"color: #f92672;\">=</span>dm<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">1</span> io_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">5780</span>i,iops_in_progress<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,merged_reads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,merged_writes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,read_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">5636096</span>i,read_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">3030</span>i,reads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">81</span>i,weighted_io_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">26500</span>i,write_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">13631488</span>i,write_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">23470</span>i,writes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">208</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> disk,device<span style=\"color: #f92672;\">=</span>dm<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">0</span>,fstype<span style=\"color: #f92672;\">=</span>xfs,host<span style=\"color: #f92672;\">=</span>kilenc,mode<span style=\"color: #f92672;\">=</span>rw,path<span style=\"color: #f92672;\">=/</span> free<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9315028992</span>i,inodes_free<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">18214222</span>i,inodes_total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">19822888</span>i,inodes_used<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1608666</span>i,total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">53660876800</span>i,used<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">44345847808</span>i,used_percent<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">82.64093032486566</span> <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> disk,device<span style=\"color: #f92672;\">=</span>sda2,fstype<span style=\"color: #f92672;\">=</span>ext4,host<span style=\"color: #f92672;\">=</span>kilenc,mode<span style=\"color: #f92672;\">=</span>rw,path<span style=\"color: #f92672;\">=/</span>boot free<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">309653504</span>i,inodes_free<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">65264</span>i,inodes_total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">65536</span>i,inodes_used<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">272</span>i,total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1020702720</span>i,used<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">640585728</span>i,used_percent<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">67.41310045173972</span> <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> disk,device<span style=\"color: #f92672;\">=</span>dm<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span>,fstype<span style=\"color: #f92672;\">=</span>xfs,host<span style=\"color: #f92672;\">=</span>kilenc,mode<span style=\"color: #f92672;\">=</span>rw,path<span style=\"color: #f92672;\">=/</span>home free<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">856442515456</span>i,inodes_free<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">452529686</span>i,inodes_total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">453312512</span>i,inodes_used<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">782826</span>i,total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">927930712064</span>i,used<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">71488196608</span>i,used_percent<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">7.704044674735306</span> <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> disk,device<span style=\"color: #f92672;\">=</span>dm<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span>,fstype<span style=\"color: #f92672;\">=</span>xfs,host<span style=\"color: #f92672;\">=</span>kilenc,mode<span style=\"color: #f92672;\">=</span>rw,path<span style=\"color: #f92672;\">=/</span>home<span style=\"color: #f92672;\">/</span>opt<span style=\"color: #f92672;\">/</span>at13<span style=\"color: #ae81ff;\">.0</span><span style=\"color: #f92672;\">/</span>lib free<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">856442515456</span>i,inodes_free<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">452529686</span>i,inodes_total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">453312512</span>i,inodes_used<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">782826</span>i,total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">927930712064</span>i,used<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">71488196608</span>i,used_percent<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">7.704044674735306</span> <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> disk,device<span style=\"color: #f92672;\">=</span>dm<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">2</span>,fstype<span style=\"color: #f92672;\">=</span>xfs,host<span style=\"color: #f92672;\">=</span>kilenc,mode<span style=\"color: #f92672;\">=</span>rw,path<span style=\"color: #f92672;\">=/</span>home<span style=\"color: #f92672;\">/</span>opt<span style=\"color: #f92672;\">/</span>at13<span style=\"color: #ae81ff;\">.0</span><span style=\"color: #f92672;\">/</span>lib64 free<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">856442515456</span>i,inodes_free<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">452529686</span>i,inodes_total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">453312512</span>i,inodes_used<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">782826</span>i,total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">927930712064</span>i,used<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">71488196608</span>i,used_percent<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">7.704044674735306</span> <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> disk,device<span style=\"color: #f92672;\">=</span>ST31000524AS<span style=\"color: #f92672;\">/</span>raktar,fstype<span style=\"color: #f92672;\">=</span>zfs,host<span style=\"color: #f92672;\">=</span>kilenc,mode<span style=\"color: #f92672;\">=</span>rw,path<span style=\"color: #f92672;\">=/</span>mnt<span style=\"color: #f92672;\">/</span>ST31000524AS free<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">210837438464</span>i,inodes_free<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">411792117</span>i,inodes_total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">412304487</span>i,inodes_used<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">512370</span>i,total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">965496143872</span>i,used<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">754658705408</span>i,used_percent<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">78.16278813725106</span> <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> diskio,host<span style=\"color: #f92672;\">=</span>kilenc,name<span style=\"color: #f92672;\">=</span>sda io_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">45899860</span>i,iops_in_progress<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,merged_reads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">650</span>i,merged_writes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1061332</span>i,read_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">16495536128</span>i,read_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">3440899</span>i,reads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">141325</span>i,weighted_io_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">311596362</span>i,write_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">133715696640</span>i,write_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">308155462</span>i,writes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">7031531</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> disk,device<span style=\"color: #f92672;\">=</span>ST31000524AS,fstype<span style=\"color: #f92672;\">=</span>zfs,host<span style=\"color: #f92672;\">=</span>kilenc,mode<span style=\"color: #f92672;\">=</span>rw,path<span style=\"color: #f92672;\">=/</span>ST31000524AS free<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">210837438464</span>i,inodes_free<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">411792117</span>i,inodes_total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">411792123</span>i,inodes_used<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">6</span>i,total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">210837569536</span>i,used<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">131072</span>i,used_percent<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0.00006216728844316324</span> <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> diskio,host<span style=\"color: #f92672;\">=</span>kilenc,name<span style=\"color: #f92672;\">=</span>sda2 io_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">18060</span>i,iops_in_progress<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,merged_reads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">27</span>i,merged_writes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">18</span>i,read_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">88372224</span>i,read_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">31224</span>i,reads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">436</span>i,weighted_io_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">36579</span>i,write_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">274432</span>i,write_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">5355</span>i,writes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">19</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> diskio,host<span style=\"color: #f92672;\">=</span>kilenc,name<span style=\"color: #f92672;\">=</span>dm<span style=\"color: #f92672;\">-</span><span style=\"color: #ae81ff;\">0</span> io_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">38788720</span>i,iops_in_progress<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,merged_reads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,merged_writes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,read_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">12341294080</span>i,read_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1143210</span>i,reads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">51814</span>i,weighted_io_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">303329620</span>i,write_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">86676331008</span>i,write_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">302186410</span>i,writes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">6798400</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> diskio,host<span style=\"color: #f92672;\">=</span>kilenc,name<span style=\"color: #f92672;\">=</span>sdb io_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">668810</span>i,iops_in_progress<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,merged_reads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9</span>i,merged_writes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">58</span>i,read_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">104550912</span>i,read_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">746540</span>i,reads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">31054</span>i,weighted_io_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1445858</span>i,write_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">10845920256</span>i,write_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">699318</span>i,writes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">124780</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> diskio,host<span style=\"color: #f92672;\">=</span>kilenc,name<span style=\"color: #f92672;\">=</span>sdb1 io_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">341330</span>i,iops_in_progress<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,merged_reads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,merged_writes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">58</span>i,read_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">95562240</span>i,read_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">383066</span>i,reads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">25026</span>i,weighted_io_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1082385</span>i,write_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">10845920256</span>i,write_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">699318</span>i,writes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">124780</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> diskio,host<span style=\"color: #f92672;\">=</span>kilenc,name<span style=\"color: #f92672;\">=</span>sdb9 io_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">190</span>i,iops_in_progress<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,merged_reads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,merged_writes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,read_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4980736</span>i,read_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">37</span>i,reads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">69</span>i,weighted_io_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">37</span>i,write_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,write_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,writes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> system,host<span style=\"color: #f92672;\">=</span>kilenc load1<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2.06</span>,load15<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2.12</span>,load5<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2.12</span>,n_cpus<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">32</span>i,n_users<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> system,host<span style=\"color: #f92672;\">=</span>kilenc uptime<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">456127</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> system,host<span style=\"color: #f92672;\">=</span>kilenc uptime_format<span style=\"color: #f92672;\">=</span><span style=\"color: #e6db74;\">\"5 days,  6:42\"</span> <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> processes,host<span style=\"color: #f92672;\">=</span>kilenc blocked<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1</span>i,dead<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">569</span>i,paging<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,parked<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1</span>i,running<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,sleeping<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">412</span>i,stopped<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,total<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1366</span>i,total_threads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2683</span>i,unknown<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,zombies<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_servers,host<span style=\"color: #f92672;\">=</span>kilenc,status<span style=\"color: #f92672;\">=</span>total value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_servers,host<span style=\"color: #f92672;\">=</span>kilenc,status<span style=\"color: #f92672;\">=</span>ok value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_servers,host<span style=\"color: #f92672;\">=</span>kilenc,status<span style=\"color: #f92672;\">=</span>closed value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_servers,host<span style=\"color: #f92672;\">=</span>kilenc,status<span style=\"color: #f92672;\">=</span>unreachable value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_servers,host<span style=\"color: #f92672;\">=</span>kilenc,status<span style=\"color: #f92672;\">=</span>unavailable value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_jobs,host<span style=\"color: #f92672;\">=</span>kilenc,state<span style=\"color: #f92672;\">=</span>total value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">121776</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_jobs,host<span style=\"color: #f92672;\">=</span>kilenc,state<span style=\"color: #f92672;\">=</span>running value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">32</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_jobs,host<span style=\"color: #f92672;\">=</span>kilenc,state<span style=\"color: #f92672;\">=</span>suspended value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_jobs,host<span style=\"color: #f92672;\">=</span>kilenc,state<span style=\"color: #f92672;\">=</span>pending value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">120771</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_jobs,host<span style=\"color: #f92672;\">=</span>kilenc,state<span style=\"color: #f92672;\">=</span>finished value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">973</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_users,host<span style=\"color: #f92672;\">=</span>kilenc,state<span style=\"color: #f92672;\">=</span>numusers value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_users,host<span style=\"color: #f92672;\">=</span>kilenc,state<span style=\"color: #f92672;\">=</span>numgroups value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_users,host<span style=\"color: #f92672;\">=</span>kilenc,state<span style=\"color: #f92672;\">=</span>numactive value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_hosts,host<span style=\"color: #f92672;\">=</span>kilenc,state<span style=\"color: #f92672;\">=</span>clients current<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,peak<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_hosts,host<span style=\"color: #f92672;\">=</span>kilenc,state<span style=\"color: #f92672;\">=</span>servers current<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1</span>i,peak<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_hosts,host<span style=\"color: #f92672;\">=</span>kilenc,state<span style=\"color: #f92672;\">=</span>cpus current<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2</span>i,peak<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_hosts,host<span style=\"color: #f92672;\">=</span>kilenc,state<span style=\"color: #f92672;\">=</span>cores current<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">32</span>i,peak<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">32</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_hosts,host<span style=\"color: #f92672;\">=</span>kilenc,state<span style=\"color: #f92672;\">=</span>slots current<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">32</span>i,peak<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">32</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_mbatchd,host<span style=\"color: #f92672;\">=</span>kilenc,query<span style=\"color: #f92672;\">=</span>job value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_mbatchd,host<span style=\"color: #f92672;\">=</span>kilenc,query<span style=\"color: #f92672;\">=</span>host value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_mbatchd,host<span style=\"color: #f92672;\">=</span>kilenc,query<span style=\"color: #f92672;\">=</span>queue value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_mbatchd,host<span style=\"color: #f92672;\">=</span>kilenc,jobs<span style=\"color: #f92672;\">=</span>submitreqs value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_mbatchd,host<span style=\"color: #f92672;\">=</span>kilenc,jobs<span style=\"color: #f92672;\">=</span>submitted value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_mbatchd,host<span style=\"color: #f92672;\">=</span>kilenc,jobs<span style=\"color: #f92672;\">=</span>dispatched value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">19</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_mbatchd,host<span style=\"color: #f92672;\">=</span>kilenc,jobs<span style=\"color: #f92672;\">=</span>completed value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">12</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_mbatchd,host<span style=\"color: #f92672;\">=</span>kilenc,jobs<span style=\"color: #f92672;\">=</span>sentremote value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_mbatchd,host<span style=\"color: #f92672;\">=</span>kilenc,jobs<span style=\"color: #f92672;\">=</span>acceptremote value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_mbatchd,host<span style=\"color: #f92672;\">=</span>kilenc,sched<span style=\"color: #f92672;\">=</span>interval value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_mbatchd,host<span style=\"color: #f92672;\">=</span>kilenc,sched<span style=\"color: #f92672;\">=</span>matchhost value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">5</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_mbatchd,host<span style=\"color: #f92672;\">=</span>kilenc,sched<span style=\"color: #f92672;\">=</span>buckets value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">5</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_mbatchd,host<span style=\"color: #f92672;\">=</span>kilenc,sched<span style=\"color: #f92672;\">=</span>reordered value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">7</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_mbatchd,host<span style=\"color: #f92672;\">=</span>kilenc,utilization<span style=\"color: #f92672;\">=</span>slots value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">100</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_mbatchd,host<span style=\"color: #f92672;\">=</span>kilenc,utilization<span style=\"color: #f92672;\">=</span>memory value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_mbatchd,fd<span style=\"color: #f92672;\">=</span>free,host<span style=\"color: #f92672;\">=</span>kilenc value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">65509</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_mbatchd,fd<span style=\"color: #f92672;\">=</span>used,host<span style=\"color: #f92672;\">=</span>kilenc value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">26</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_mbatchd,fd<span style=\"color: #f92672;\">=</span>total,host<span style=\"color: #f92672;\">=</span>kilenc value<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">65535</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_queues,host<span style=\"color: #f92672;\">=</span>kilenc,name<span style=\"color: #f92672;\">=</span>admin njobs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,pend<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rsv<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,run<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ssusp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,susp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ususp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_queues,host<span style=\"color: #f92672;\">=</span>kilenc,name<span style=\"color: #f92672;\">=</span>owners njobs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,pend<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rsv<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,run<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ssusp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,susp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ususp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_queues,host<span style=\"color: #f92672;\">=</span>kilenc,name<span style=\"color: #f92672;\">=</span>priority njobs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">93951</span>i,pend<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">93923</span>i,rsv<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,run<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">28</span>i,ssusp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,susp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ususp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_queues,host<span style=\"color: #f92672;\">=</span>kilenc,name<span style=\"color: #f92672;\">=</span>night njobs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,pend<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rsv<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,run<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ssusp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,susp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ususp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_queues,host<span style=\"color: #f92672;\">=</span>kilenc,name<span style=\"color: #f92672;\">=</span>short njobs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2504</span>i,pend<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2504</span>i,rsv<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,run<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ssusp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,susp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ususp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_queues,host<span style=\"color: #f92672;\">=</span>kilenc,name<span style=\"color: #f92672;\">=</span>dataq njobs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,pend<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rsv<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,run<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ssusp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,susp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ususp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_queues,host<span style=\"color: #f92672;\">=</span>kilenc,name<span style=\"color: #f92672;\">=</span>normal njobs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1750</span>i,pend<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1750</span>i,rsv<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,run<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ssusp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,susp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ususp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_queues,host<span style=\"color: #f92672;\">=</span>kilenc,name<span style=\"color: #f92672;\">=</span>interactive njobs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,pend<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rsv<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,run<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ssusp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,susp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ususp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_queues,host<span style=\"color: #f92672;\">=</span>kilenc,name<span style=\"color: #f92672;\">=</span>sendq njobs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">22598</span>i,pend<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">22594</span>i,rsv<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,run<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4</span>i,ssusp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,susp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ususp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> lsf_queues,host<span style=\"color: #f92672;\">=</span>kilenc,name<span style=\"color: #f92672;\">=</span>idle njobs<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,pend<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rsv<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,run<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ssusp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,susp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,ususp<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i <span style=\"color: #ae81ff;\">1674246976000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu0,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">100</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu4,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">100</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu8,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">100</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu12,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">100</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu16,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">98.03921568448419</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1.9607843137324836</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu20,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">100</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu24,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">100</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu28,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">100</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu32,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">100</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu36,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">100</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu40,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">98.03921568448419</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1.9607843136879006</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu44,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">100</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu48,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">100</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu52,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">100</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu56,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">100</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu60,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">100</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu64,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">87.99999999906868</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">10.000000001155058</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2.0000000002764864</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu68,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">100</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu72,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">86.27450980280263</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">11.764705882127403</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1.9607843137324836</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu76,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">100</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu80,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">92.30769231113655</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">3.8461538464431086</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">3.84615384653056</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu84,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">94.11764706486585</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">5.882352941197451</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu88,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">100</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu92,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">70.58823529344627</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">29.411764701983955</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu96,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">96.15384615040192</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">3.8461538460125784</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu100,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">97.99999999813735</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1.999999999998181</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu104,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">96.07843137993407</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">3.92156862782338</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu108,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">96.07843136896838</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1.9607843136879006</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1.9607843137324836</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu112,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">100</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu116,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">95.91836734305988</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4.08163265313509</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu120,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">84.61538461280144</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">3.8461538460344413</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">11.53846153830009</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu124,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">100</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> cpu,cpu<span style=\"color: #f92672;\">=</span>cpu<span style=\"color: #f92672;\">-</span>total,host<span style=\"color: #f92672;\">=</span>kilenc usage_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">93.47826086554115</span>,usage_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">3.1055900618243673</span>,usage_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_softirq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,usage_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2.484472049468532</span>,usage_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0.9316770186919254</span> <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> procstat,exe<span style=\"color: #f92672;\">=</span>mbatchd,host<span style=\"color: #f92672;\">=</span>kilenc,process_name<span style=\"color: #f92672;\">=</span>mbatchd,user<span style=\"color: #f92672;\">=</span>root child_major_faults<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,child_minor_faults<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,cpu_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,cpu_time_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_soft_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0.03</span>,cpu_time_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0.05</span>,cpu_usage<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,created_at<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1674246974000000000</span>i,involuntary_context_switches<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1</span>i,major_faults<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,memory_data<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">834994176</span>i,memory_locked<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,memory_rss<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">815595520</span>i,memory_stack<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">327680</span>i,memory_swap<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,memory_usage<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2.469912528991699</span>,memory_vms<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1091108864</span>i,minor_faults<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">726</span>i,nice_priority<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">20</span>i,num_fds<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">10</span>i,num_threads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2</span>i,pid<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">62056</span>i,ppid<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4103699</span>i,read_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,read_count<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">27</span>i,realtime_priority<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rlimit_cpu_time_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_cpu_time_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_file_locks_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_file_locks_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_data_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_data_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_locked_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">67108864</span>i,rlimit_memory_locked_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">67108864</span>i,rlimit_memory_rss_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_rss_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_stack_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_stack_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">8388608</span>i,rlimit_memory_vms_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_vms_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_nice_priority_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rlimit_nice_priority_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rlimit_num_fds_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">262144</span>i,rlimit_num_fds_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">65535</span>i,rlimit_realtime_priority_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rlimit_realtime_priority_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rlimit_signals_pending_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">118856</span>i,rlimit_signals_pending_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">118856</span>i,signals_pending<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,voluntary_context_switches<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">5</span>i,write_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,write_count<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">16</span>i <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> procstat,exe<span style=\"color: #f92672;\">=</span>mbschd,host<span style=\"color: #f92672;\">=</span>kilenc,process_name<span style=\"color: #f92672;\">=</span>mbschd,user<span style=\"color: #f92672;\">=</span>lsfadmin child_major_faults<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,child_minor_faults<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2457641</span>i,cpu_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">320</span>i,cpu_time_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0.02</span>,cpu_time_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_soft_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">8.4</span>,cpu_time_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">312.14</span>,cpu_usage<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1.836645120693344</span>,created_at<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1674227581000000000</span>i,involuntary_context_switches<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">3553</span>i,major_faults<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1</span>i,memory_data<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">228851712</span>i,memory_locked<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,memory_rss<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">236847104</span>i,memory_stack<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">196608</span>i,memory_swap<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,memory_usage<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0.717257022857666</span>,memory_vms<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">246808576</span>i,minor_faults<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2137969</span>i,nice_priority<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">20</span>i,num_fds<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">3</span>i,num_threads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1</span>i,pid<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4103740</span>i,ppid<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4103699</span>i,read_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1552384</span>i,read_count<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">936861</span>i,realtime_priority<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rlimit_cpu_time_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_cpu_time_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_file_locks_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_file_locks_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_data_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_data_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_locked_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">67108864</span>i,rlimit_memory_locked_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">67108864</span>i,rlimit_memory_rss_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_rss_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_stack_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_stack_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">8388608</span>i,rlimit_memory_vms_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_vms_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_nice_priority_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rlimit_nice_priority_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rlimit_num_fds_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">262144</span>i,rlimit_num_fds_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">65535</span>i,rlimit_realtime_priority_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rlimit_realtime_priority_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rlimit_signals_pending_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">118856</span>i,rlimit_signals_pending_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">118856</span>i,signals_pending<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,voluntary_context_switches<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">43952</span>i,write_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,write_count<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">42311</span>i <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> procstat_lookup,exe<span style=\"color: #f92672;\">=</span>mbschd,host<span style=\"color: #f92672;\">=</span>kilenc,pid_finder<span style=\"color: #f92672;\">=</span>pgrep,result<span style=\"color: #f92672;\">=</span>success pid_count<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1</span>i,result_code<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,running<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1</span>i <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> procstat,exe<span style=\"color: #f92672;\">=</span>mbatchd,host<span style=\"color: #f92672;\">=</span>kilenc,process_name<span style=\"color: #f92672;\">=</span>mbatchd,user<span style=\"color: #f92672;\">=</span>root child_major_faults<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2</span>i,child_minor_faults<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4476280</span>i,cpu_time<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">177</span>i,cpu_time_guest<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_guest_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_idle<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_iowait<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">6.68</span>,cpu_time_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_nice<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_soft_irq<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_steal<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,cpu_time_system<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">51.01</span>,cpu_time_user<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">126.42</span>,cpu_usage<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>,created_at<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1674227573000000000</span>i,involuntary_context_switches<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4993</span>i,major_faults<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">3</span>i,memory_data<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">834994176</span>i,memory_locked<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,memory_rss<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">827785216</span>i,memory_stack<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">327680</span>i,memory_swap<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,memory_usage<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2.5068273544311523</span>,memory_vms<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1091108864</span>i,minor_faults<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2406945</span>i,nice_priority<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">20</span>i,num_fds<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">26</span>i,num_threads<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">3</span>i,pid<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4103699</span>i,ppid<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">4103684</span>i,read_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">21008384</span>i,read_count<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">364726</span>i,realtime_priority<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rlimit_cpu_time_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_cpu_time_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_file_locks_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_file_locks_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_data_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_data_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_locked_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">67108864</span>i,rlimit_memory_locked_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">67108864</span>i,rlimit_memory_rss_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_rss_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_stack_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_stack_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">8388608</span>i,rlimit_memory_vms_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_memory_vms_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">9223372036854775807</span>i,rlimit_nice_priority_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rlimit_nice_priority_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rlimit_num_fds_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">262144</span>i,rlimit_num_fds_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">65535</span>i,rlimit_realtime_priority_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rlimit_realtime_priority_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,rlimit_signals_pending_hard<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">118856</span>i,rlimit_signals_pending_soft<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">118856</span>i,signals_pending<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,voluntary_context_switches<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">172583</span>i,write_bytes<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">1562181632</span>i,write_count<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">12164760</span>i <span style=\"color: #ae81ff;\">1674246977000000000</span><span style=\"color: #f92672;\">&gt;</span> procstat_lookup,exe<span style=\"color: #f92672;\">=</span>mbatchd,host<span style=\"color: #f92672;\">=</span>kilenc,pid_finder<span style=\"color: #f92672;\">=</span>pgrep,result<span style=\"color: #f92672;\">=</span>success pid_count<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2</span>i,result_code<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">0</span>i,running<span style=\"color: #f92672;\">=</span><span style=\"color: #ae81ff;\">2</span>i <span style=\"color: #ae81ff;\">1674246977000000000</span></code></pre></div></details><hr /><ol start=\"6\"><li>Assuming there were no errors in the previous step with telegraf, proceed to start the telegraf process via systemd.</li></ol><div class=\"highlight\"><pre><code class=\"language-plaintext\">[root@kilenc telegraf]# systemctl start telegraf[root@kilenc telegraf]# systemctl status telegraf● telegraf.service - Telegraf   Loaded: loaded (/usr/lib/systemd/system/telegraf.service; enabled; vendor preset: disabled)   Active: active (running) since Thu 2023-01-19 14:13:51 EST; 1 day 1h ago     Docs: https://github.com/influxdata/telegraf Main PID: 3225959 (telegraf)    Tasks: 35 (limit: 190169)   Memory: 192.6M   CGroup: /system.slice/telegraf.service           └─3225959 /usr/bin/telegraf -config /etc/telegraf/telegraf.conf -config-directory /etc/tele&gt;Jan 19 14:13:51 kilenc systemd[1]: Starting Telegraf...Jan 19 14:13:51 kilenc systemd[1]: Started Telegraf.</code></pre></div><ol start=\"7\"><li>On the host running the database instance, <em>adatbazis</em>, perform queries to check whether the database <em>telegraf</em> exists, as well as checking if LSF related data is being logged.This is confirmed in the output below.</li></ol><hr /><details>  <strong>Output from InfluxDB queries. Click to expand!</strong>  <div class=\"highlight\"><pre><code class=\"language-js\">[<span style=\"color: #a6e22e;\">root</span><span style=\"color: #960050; background-color: #1e0010;\">@</span><span style=\"color: #a6e22e;\">adatbazis</span> <span style=\"color: #a6e22e;\">fedora</span>]<span style=\"color: #960050; background-color: #1e0010;\">#</span> <span style=\"color: #a6e22e;\">influx</span><span style=\"color: #a6e22e;\">Connected</span> <span style=\"color: #a6e22e;\">to</span> <span style=\"color: #a6e22e;\">https</span><span style=\"color: #f92672;\">:</span><span style=\"color: #75715e;\">//localhost:8086 version 1.8.10</span><span style=\"color: #75715e;\"></span><span style=\"color: #a6e22e;\">InfluxDB</span> <span style=\"color: #a6e22e;\">shell</span> <span style=\"color: #a6e22e;\">version</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #ae81ff;\">1.8</span>.<span style=\"color: #ae81ff;\">10</span><span style=\"color: #f92672;\">&gt;</span> <span style=\"color: #a6e22e;\">auth</span><span style=\"color: #a6e22e;\">username</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #a6e22e;\">influx</span><span style=\"color: #a6e22e;\">password</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #f92672;\">&gt;</span> <span style=\"color: #a6e22e;\">show</span> <span style=\"color: #a6e22e;\">databases</span><span style=\"color: #a6e22e;\">name</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #a6e22e;\">databases</span><span style=\"color: #a6e22e;\">name</span><span style=\"color: #f92672;\">----</span><span style=\"color: #ae81ff;\">_</span><span style=\"color: #a6e22e;\">internal</span><span style=\"color: #a6e22e;\">telegraf</span><span style=\"color: #f92672;\">&gt;</span> <span style=\"color: #a6e22e;\">use</span> <span style=\"color: #a6e22e;\">telegraf</span><span style=\"color: #a6e22e;\">Using</span> <span style=\"color: #a6e22e;\">database</span> <span style=\"color: #a6e22e;\">telegraf</span><span style=\"color: #f92672;\">&gt;</span> <span style=\"color: #a6e22e;\">show</span> <span style=\"color: #a6e22e;\">field</span> <span style=\"color: #a6e22e;\">keys</span><span style=\"color: #a6e22e;\">name</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #a6e22e;\">cpu</span><span style=\"color: #a6e22e;\">fieldKey</span>         <span style=\"color: #a6e22e;\">fieldType</span><span style=\"color: #f92672;\">--------</span>         <span style=\"color: #f92672;\">---------</span><span style=\"color: #a6e22e;\">usage_guest</span>      <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">usage_guest_nice</span> <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">usage_idle</span>       <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">usage_iowait</span>     <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">usage_irq</span>        <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">usage_nice</span>       <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">usage_softirq</span>    <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">usage_steal</span>      <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">usage_system</span>     <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">usage_user</span>       <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">name</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #a6e22e;\">disk</span><span style=\"color: #a6e22e;\">fieldKey</span>     <span style=\"color: #a6e22e;\">fieldType</span><span style=\"color: #f92672;\">--------</span>     <span style=\"color: #f92672;\">---------</span><span style=\"color: #a6e22e;\">free</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">inodes_free</span>  <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">inodes_total</span> <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">inodes_used</span>  <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">total</span>        <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">used</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">used_percent</span> <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">name</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #a6e22e;\">diskio</span><span style=\"color: #a6e22e;\">fieldKey</span>         <span style=\"color: #a6e22e;\">fieldType</span><span style=\"color: #f92672;\">--------</span>         <span style=\"color: #f92672;\">---------</span><span style=\"color: #a6e22e;\">io_time</span>          <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">iops_in_progress</span> <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">merged_reads</span>     <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">merged_writes</span>    <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">read_bytes</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">read_time</span>        <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">reads</span>            <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">weighted_io_time</span> <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">write_bytes</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">write_time</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">writes</span>           <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">name</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #a6e22e;\">kernel</span><span style=\"color: #a6e22e;\">fieldKey</span>         <span style=\"color: #a6e22e;\">fieldType</span><span style=\"color: #f92672;\">--------</span>         <span style=\"color: #f92672;\">---------</span><span style=\"color: #a6e22e;\">boot_time</span>        <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">context_switches</span> <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">entropy_avail</span>    <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">interrupts</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">processes_forked</span> <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">name</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #a6e22e;\">lsf_hosts</span><span style=\"color: #a6e22e;\">fieldKey</span> <span style=\"color: #a6e22e;\">fieldType</span><span style=\"color: #f92672;\">--------</span> <span style=\"color: #f92672;\">---------</span><span style=\"color: #a6e22e;\">current</span>  <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">peak</span>     <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">name</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #a6e22e;\">lsf_jobs</span><span style=\"color: #a6e22e;\">fieldKey</span> <span style=\"color: #a6e22e;\">fieldType</span><span style=\"color: #f92672;\">--------</span> <span style=\"color: #f92672;\">---------</span><span style=\"color: #a6e22e;\">value</span>    <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">name</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #a6e22e;\">lsf_mbatchd</span><span style=\"color: #a6e22e;\">fieldKey</span> <span style=\"color: #a6e22e;\">fieldType</span><span style=\"color: #f92672;\">--------</span> <span style=\"color: #f92672;\">---------</span><span style=\"color: #a6e22e;\">value</span>    <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">name</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #a6e22e;\">lsf_queues</span><span style=\"color: #a6e22e;\">fieldKey</span> <span style=\"color: #a6e22e;\">fieldType</span><span style=\"color: #f92672;\">--------</span> <span style=\"color: #f92672;\">---------</span><span style=\"color: #a6e22e;\">njobs</span>    <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">pend</span>     <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">rsv</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">run</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ssusp</span>    <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">susp</span>     <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ususp</span>    <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">name</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #a6e22e;\">lsf_servers</span><span style=\"color: #a6e22e;\">fieldKey</span> <span style=\"color: #a6e22e;\">fieldType</span><span style=\"color: #f92672;\">--------</span> <span style=\"color: #f92672;\">---------</span><span style=\"color: #a6e22e;\">value</span>    <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">name</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #a6e22e;\">lsf_users</span><span style=\"color: #a6e22e;\">fieldKey</span> <span style=\"color: #a6e22e;\">fieldType</span><span style=\"color: #f92672;\">--------</span> <span style=\"color: #f92672;\">---------</span><span style=\"color: #a6e22e;\">value</span>    <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">name</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #a6e22e;\">mem</span><span style=\"color: #a6e22e;\">fieldKey</span>          <span style=\"color: #a6e22e;\">fieldType</span><span style=\"color: #f92672;\">--------</span>          <span style=\"color: #f92672;\">---------</span><span style=\"color: #a6e22e;\">active</span>            <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">available</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">available_percent</span> <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">buffered</span>          <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">cached</span>            <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">commit_limit</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">committed_as</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">dirty</span>             <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">free</span>              <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">high_free</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">high_total</span>        <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">huge_page_size</span>    <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">huge_pages_free</span>   <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">huge_pages_total</span>  <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">inactive</span>          <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">low_free</span>          <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">low_total</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">mapped</span>            <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">page_tables</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">shared</span>            <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">slab</span>              <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">sreclaimable</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">sunreclaim</span>        <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">swap_cached</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">swap_free</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">swap_total</span>        <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">total</span>             <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">used</span>              <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">used_percent</span>      <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">vmalloc_chunk</span>     <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">vmalloc_total</span>     <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">vmalloc_used</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">write_back</span>        <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">write_back_tmp</span>    <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">name</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #a6e22e;\">net</span><span style=\"color: #a6e22e;\">fieldKey</span>              <span style=\"color: #a6e22e;\">fieldType</span><span style=\"color: #f92672;\">--------</span>              <span style=\"color: #f92672;\">---------</span><span style=\"color: #a6e22e;\">bytes_recv</span>            <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">bytes_sent</span>            <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">drop_in</span>               <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">drop_out</span>              <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">err_in</span>                <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">err_out</span>               <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_inaddrmaskreps</span>   <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_inaddrmasks</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_incsumerrors</span>     <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_indestunreachs</span>   <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_inechoreps</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_inechos</span>          <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_inerrors</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_inmsgs</span>           <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_inparmprobs</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_inredirects</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_insrcquenchs</span>     <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_intimeexcds</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_intimestampreps</span>  <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_intimestamps</span>     <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_outaddrmaskreps</span>  <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_outaddrmasks</span>     <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_outdestunreachs</span>  <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_outechoreps</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_outechos</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_outerrors</span>        <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_outmsgs</span>          <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_outparmprobs</span>     <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_outredirects</span>     <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_outsrcquenchs</span>    <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_outtimeexcds</span>     <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_outtimestampreps</span> <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmp_outtimestamps</span>    <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmpmsg_intype0</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmpmsg_intype3</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmpmsg_intype8</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmpmsg_outtype0</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmpmsg_outtype3</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">icmpmsg_outtype8</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ip_defaultttl</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ip_forwarding</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ip_forwdatagrams</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ip_fragcreates</span>        <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ip_fragfails</span>          <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ip_fragoks</span>            <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ip_inaddrerrors</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ip_indelivers</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ip_indiscards</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ip_inhdrerrors</span>        <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ip_inreceives</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ip_inunknownprotos</span>    <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ip_outdiscards</span>        <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ip_outnoroutes</span>        <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ip_outrequests</span>        <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ip_reasmfails</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ip_reasmoks</span>           <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ip_reasmreqds</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ip_reasmtimeout</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">packets_recv</span>          <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">packets_sent</span>          <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">tcp_activeopens</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">tcp_attemptfails</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">tcp_currestab</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">tcp_estabresets</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">tcp_incsumerrors</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">tcp_inerrs</span>            <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">tcp_insegs</span>            <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">tcp_maxconn</span>           <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">tcp_outrsts</span>           <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">tcp_outsegs</span>           <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">tcp_passiveopens</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">tcp_retranssegs</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">tcp_rtoalgorithm</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">tcp_rtomax</span>            <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">tcp_rtomin</span>            <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">udp_ignoredmulti</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">udp_incsumerrors</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">udp_indatagrams</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">udp_inerrors</span>          <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">udp_memerrors</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">udp_noports</span>           <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">udp_outdatagrams</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">udp_rcvbuferrors</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">udp_sndbuferrors</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">udplite_ignoredmulti</span>  <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">udplite_incsumerrors</span>  <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">udplite_indatagrams</span>   <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">udplite_inerrors</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">udplite_memerrors</span>     <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">udplite_noports</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">udplite_outdatagrams</span>  <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">udplite_rcvbuferrors</span>  <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">udplite_sndbuferrors</span>  <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">name</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #a6e22e;\">processes</span><span style=\"color: #a6e22e;\">fieldKey</span>      <span style=\"color: #a6e22e;\">fieldType</span><span style=\"color: #f92672;\">--------</span>      <span style=\"color: #f92672;\">---------</span><span style=\"color: #a6e22e;\">blocked</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">dead</span>          <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">idle</span>          <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">paging</span>        <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">parked</span>        <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">running</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">sleeping</span>      <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">stopped</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">total</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">total_threads</span> <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">unknown</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">zombies</span>       <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">name</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #a6e22e;\">procstat</span><span style=\"color: #a6e22e;\">fieldKey</span>                     <span style=\"color: #a6e22e;\">fieldType</span><span style=\"color: #f92672;\">--------</span>                     <span style=\"color: #f92672;\">---------</span><span style=\"color: #a6e22e;\">child_major_faults</span>           <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">child_minor_faults</span>           <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">cpu_time_guest</span>               <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">cpu_time_guest_nice</span>          <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">cpu_time_idle</span>                <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">cpu_time_iowait</span>              <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">cpu_time_irq</span>                 <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">cpu_time_nice</span>                <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">cpu_time_soft_irq</span>            <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">cpu_time_steal</span>               <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">cpu_time_system</span>              <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">cpu_time_user</span>                <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">cpu_usage</span>                    <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">created_at</span>                   <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">involuntary_context_switches</span> <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">major_faults</span>                 <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">memory_data</span>                  <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">memory_locked</span>                <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">memory_rss</span>                   <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">memory_stack</span>                 <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">memory_swap</span>                  <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">memory_usage</span>                 <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">memory_vms</span>                   <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">minor_faults</span>                 <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">num_threads</span>                  <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">pid</span>                          <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">ppid</span>                         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">voluntary_context_switches</span>   <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">name</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #a6e22e;\">procstat_lookup</span><span style=\"color: #a6e22e;\">fieldKey</span>    <span style=\"color: #a6e22e;\">fieldType</span><span style=\"color: #f92672;\">--------</span>    <span style=\"color: #f92672;\">---------</span><span style=\"color: #a6e22e;\">pid_count</span>   <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">result_code</span> <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">running</span>     <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">name</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #a6e22e;\">swap</span><span style=\"color: #a6e22e;\">fieldKey</span>     <span style=\"color: #a6e22e;\">fieldType</span><span style=\"color: #f92672;\">--------</span>     <span style=\"color: #f92672;\">---------</span><span style=\"color: #a6e22e;\">free</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #66d9ef;\">in</span>           <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">out</span>          <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">total</span>        <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">used</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">used_percent</span> <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">name</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #a6e22e;\">system</span><span style=\"color: #a6e22e;\">fieldKey</span>       <span style=\"color: #a6e22e;\">fieldType</span><span style=\"color: #f92672;\">--------</span>       <span style=\"color: #f92672;\">---------</span><span style=\"color: #a6e22e;\">load1</span>          <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">load15</span>         <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">load5</span>          <span style=\"color: #66d9ef;\">float</span><span style=\"color: #a6e22e;\">n_cpus</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">n_unique_users</span> <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">n_users</span>        <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">uptime</span>         <span style=\"color: #a6e22e;\">integer</span><span style=\"color: #a6e22e;\">uptime_format</span>  <span style=\"color: #a6e22e;\">string</span><span style=\"color: #f92672;\">&gt;</span> <span style=\"color: #a6e22e;\">select</span> <span style=\"color: #f92672;\">*</span> <span style=\"color: #a6e22e;\">from</span> <span style=\"color: #a6e22e;\">metrics</span><span style=\"color: #f92672;\">&gt;</span> <span style=\"color: #a6e22e;\">SELECT</span> <span style=\"color: #f92672;\">*</span> <span style=\"color: #a6e22e;\">FROM</span> <span style=\"color: #e6db74;\">\"lsf_hosts\"</span>;<span style=\"color: #a6e22e;\">name</span><span style=\"color: #f92672;\">:</span> <span style=\"color: #a6e22e;\">lsf_hosts</span><span style=\"color: #a6e22e;\">time</span>                <span style=\"color: #a6e22e;\">current</span> <span style=\"color: #a6e22e;\">host</span>   <span style=\"color: #a6e22e;\">peak</span> <span style=\"color: #a6e22e;\">state</span><span style=\"color: #f92672;\">----</span>                <span style=\"color: #f92672;\">-------</span> <span style=\"color: #f92672;\">----</span>   <span style=\"color: #f92672;\">----</span> <span style=\"color: #f92672;\">-----</span><span style=\"color: #ae81ff;\">1674493170000000000</span> <span style=\"color: #ae81ff;\">0</span>       <span style=\"color: #a6e22e;\">kilenc</span> <span style=\"color: #ae81ff;\">0</span>    <span style=\"color: #a6e22e;\">clients</span><span style=\"color: #ae81ff;\">1674493170000000000</span> <span style=\"color: #ae81ff;\">32</span>      <span style=\"color: #a6e22e;\">kilenc</span> <span style=\"color: #ae81ff;\">32</span>   <span style=\"color: #a6e22e;\">slots</span><span style=\"color: #ae81ff;\">1674493170000000000</span> <span style=\"color: #ae81ff;\">32</span>      <span style=\"color: #a6e22e;\">kilenc</span> <span style=\"color: #ae81ff;\">32</span>   <span style=\"color: #a6e22e;\">cores</span><span style=\"color: #ae81ff;\">1674493170000000000</span> <span style=\"color: #ae81ff;\">1</span>       <span style=\"color: #a6e22e;\">kilenc</span> <span style=\"color: #ae81ff;\">1</span>    <span style=\"color: #a6e22e;\">servers</span><span style=\"color: #ae81ff;\">1674493170000000000</span> <span style=\"color: #ae81ff;\">2</span>       <span style=\"color: #a6e22e;\">kilenc</span> <span style=\"color: #ae81ff;\">2</span>    <span style=\"color: #a6e22e;\">cpus</span><span style=\"color: #ae81ff;\">1674493200000000000</span> <span style=\"color: #ae81ff;\">1</span>       <span style=\"color: #a6e22e;\">kilenc</span> <span style=\"color: #ae81ff;\">1</span>    <span style=\"color: #a6e22e;\">servers</span><span style=\"color: #ae81ff;\">1674493200000000000</span> <span style=\"color: #ae81ff;\">2</span>       <span style=\"color: #a6e22e;\">kilenc</span> <span style=\"color: #ae81ff;\">2</span>    <span style=\"color: #a6e22e;\">cpus</span><span style=\"color: #ae81ff;\">1674493200000000000</span> <span style=\"color: #ae81ff;\">32</span>      <span style=\"color: #a6e22e;\">kilenc</span> <span style=\"color: #ae81ff;\">32</span>   <span style=\"color: #a6e22e;\">slots</span><span style=\"color: #ae81ff;\">1674493200000000000</span> <span style=\"color: #ae81ff;\">0</span>       <span style=\"color: #a6e22e;\">kilenc</span> <span style=\"color: #ae81ff;\">0</span>    <span style=\"color: #a6e22e;\">clients</span><span style=\"color: #ae81ff;\">1674493200000000000</span> <span style=\"color: #ae81ff;\">32</span>      <span style=\"color: #a6e22e;\">kilenc</span> <span style=\"color: #ae81ff;\">32</span>   <span style=\"color: #a6e22e;\">cores</span><span style=\"color: #ae81ff;\">1674493230000000000</span> <span style=\"color: #ae81ff;\">0</span>       <span style=\"color: #a6e22e;\">kilenc</span> <span style=\"color: #ae81ff;\">0</span>    <span style=\"color: #a6e22e;\">clients</span><span style=\"color: #ae81ff;\">1674493230000000000</span> <span style=\"color: #ae81ff;\">32</span>      <span style=\"color: #a6e22e;\">kilenc</span> <span style=\"color: #ae81ff;\">32</span>   <span style=\"color: #a6e22e;\">cores</span><span style=\"color: #ae81ff;\">1674493230000000000</span> <span style=\"color: #ae81ff;\">2</span>       <span style=\"color: #a6e22e;\">kilenc</span> <span style=\"color: #ae81ff;\">2</span>    <span style=\"color: #a6e22e;\">cpus</span><span style=\"color: #ae81ff;\">1674493230000000000</span> <span style=\"color: #ae81ff;\">1</span>       <span style=\"color: #a6e22e;\">kilenc</span> <span style=\"color: #ae81ff;\">1</span>    <span style=\"color: #a6e22e;\">servers</span><span style=\"color: #ae81ff;\">1674493230000000000</span> <span style=\"color: #ae81ff;\">32</span>      <span style=\"color: #a6e22e;\">kilenc</span> <span style=\"color: #ae81ff;\">32</span>   <span style=\"color: #a6e22e;\">slots</span><span style=\"color: #ae81ff;\">1674493260000000000</span> <span style=\"color: #ae81ff;\">1</span>       <span style=\"color: #a6e22e;\">kilenc</span> <span style=\"color: #ae81ff;\">1</span>    <span style=\"color: #a6e22e;\">servers</span><span style=\"color: #ae81ff;\">1674493260000000000</span> <span style=\"color: #ae81ff;\">32</span>      <span style=\"color: #a6e22e;\">kilenc</span> <span style=\"color: #ae81ff;\">32</span>   <span style=\"color: #a6e22e;\">slots</span><span style=\"color: #ae81ff;\">1674493260000000000</span> <span style=\"color: #ae81ff;\">0</span>       <span style=\"color: #a6e22e;\">kilenc</span> <span style=\"color: #ae81ff;\">0</span>    <span style=\"color: #a6e22e;\">clients</span><span style=\"color: #ae81ff;\">1674493260000000000</span> <span style=\"color: #ae81ff;\">2</span>       <span style=\"color: #a6e22e;\">kilenc</span> <span style=\"color: #ae81ff;\">2</span>    <span style=\"color: #a6e22e;\">cpus</span><span style=\"color: #ae81ff;\">1674493260000000000</span> <span style=\"color: #ae81ff;\">32</span>      <span style=\"color: #a6e22e;\">kilenc</span> <span style=\"color: #ae81ff;\">32</span>   <span style=\"color: #a6e22e;\">cores</span><span style=\"color: #f92672;\">&gt;</span> <span style=\"color: #a6e22e;\">quit</span></code></pre></div></details><hr /><ol start=\"8\"><li>With telegraf successfully logging data to the InfluxDB instance, it will now be possible to create a data source in Grafana in order to create a dashboard containing LSF metrics.As noted at the outset, this article is not meant to be an extensive guide to the creation of dashoards in Grafana. In the Grafana navigation select <em>Configuration</em> &gt; <em>Data sources</em>.</li></ol><figure><img src=\"https://www.gaborsamu.com/images/configure_datasource.png\" /></figure><ol start=\"9\"><li>Select the <em>Add data source</em> button, followed by InfluxDB, which is listed under <em>Time series databases</em>. On the settings page specify following values:</li></ol><hr /><table><thead><tr><th>Variable</th><th>Value</th></tr></thead><tbody><tr><td>URL</td><td><em>http://adatbazis:8086</em></td></tr><tr><td>Database</td><td><em>telegraf</em></td></tr><tr><td>Basic auth</td><td>(enable)</td></tr><tr><td>User</td><td>&lt;influxdb_username&gt;</td></tr><tr><td>Password</td><td>&lt;influxdb_password</td></tr></tbody></table><hr /><p>Next, click on <em>Save &amp; test</em>. If all variables and settings were properly specified, the message <em>datasource is working. 17 measurements found</em>.</p><figure><img src=\"https://www.gaborsamu.com/images/test_datasource.png\" /></figure><ol start=\"10\"><li>With the datasource configured in Grafana, the final step is to create a dashboard. Creating a dashboard requires creating panels which display data pulled from the confiugred datasource using targeted queries. With a bit of effort, I was able to piece together the following dashboard which includes both metrics from LSF, as well as metrics from Telegraf<em>input.procstat</em> for the LSF processes <em>mbatchd</em>, <em>mbschd</em> and the management <em>lim</em>.</li></ol><figure><img src=\"https://www.gaborsamu.com/images/lsf_dashboard3.jpg\" /></figure><hr /><details>  <strong>Example dashboard definition (JSON). Click to expand!</strong>  <div class=\"highlight\"><pre><code class=\"language-json\">{  <span style=\"color: #f92672;\">\"annotations\"</span>: {    <span style=\"color: #f92672;\">\"list\"</span>: [      {        <span style=\"color: #f92672;\">\"builtIn\"</span>: <span style=\"color: #ae81ff;\">1</span>,        <span style=\"color: #f92672;\">\"datasource\"</span>: {          <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"datasource\"</span>,          <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"grafana\"</span>        },        <span style=\"color: #f92672;\">\"enable\"</span>: <span style=\"color: #66d9ef;\">true</span>,        <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">true</span>,        <span style=\"color: #f92672;\">\"iconColor\"</span>: <span style=\"color: #e6db74;\">\"rgba(0, 211, 255, 1)\"</span>,        <span style=\"color: #f92672;\">\"name\"</span>: <span style=\"color: #e6db74;\">\"Annotations &amp; Alerts\"</span>,        <span style=\"color: #f92672;\">\"target\"</span>: {          <span style=\"color: #f92672;\">\"limit\"</span>: <span style=\"color: #ae81ff;\">100</span>,          <span style=\"color: #f92672;\">\"matchAny\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"tags\"</span>: [],          <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"dashboard\"</span>        },        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"dashboard\"</span>      }    ]  },  <span style=\"color: #f92672;\">\"editable\"</span>: <span style=\"color: #66d9ef;\">true</span>,  <span style=\"color: #f92672;\">\"fiscalYearStartMonth\"</span>: <span style=\"color: #ae81ff;\">0</span>,  <span style=\"color: #f92672;\">\"graphTooltip\"</span>: <span style=\"color: #ae81ff;\">0</span>,  <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">17</span>,  <span style=\"color: #f92672;\">\"links\"</span>: [],  <span style=\"color: #f92672;\">\"liveNow\"</span>: <span style=\"color: #66d9ef;\">false</span>,  <span style=\"color: #f92672;\">\"panels\"</span>: [    {      <span style=\"color: #f92672;\">\"collapsed\"</span>: <span style=\"color: #66d9ef;\">false</span>,      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">1</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">24</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">0</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">0</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">35</span>,      <span style=\"color: #f92672;\">\"panels\"</span>: [],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Cluster aggregate current statistics\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"row\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"A view of the current status of the LSF servers in the cluster. Servers can be in one of four states: Ok, Unavailable, Closed and Unreachable. \"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"palette-classic\"</span>          },          <span style=\"color: #f92672;\">\"custom\"</span>: {            <span style=\"color: #f92672;\">\"hideFrom\"</span>: {              <span style=\"color: #f92672;\">\"legend\"</span>: <span style=\"color: #66d9ef;\">false</span>,              <span style=\"color: #f92672;\">\"tooltip\"</span>: <span style=\"color: #66d9ef;\">false</span>,              <span style=\"color: #f92672;\">\"viz\"</span>: <span style=\"color: #66d9ef;\">false</span>            }          },          <span style=\"color: #f92672;\">\"decimals\"</span>: <span style=\"color: #ae81ff;\">2</span>,          <span style=\"color: #f92672;\">\"mappings\"</span>: []        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">8</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">9</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">0</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">1</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">32</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"displayLabels\"</span>: [          <span style=\"color: #e6db74;\">\"name\"</span>,          <span style=\"color: #e6db74;\">\"value\"</span>        ],        <span style=\"color: #f92672;\">\"legend\"</span>: {          <span style=\"color: #f92672;\">\"displayMode\"</span>: <span style=\"color: #e6db74;\">\"table\"</span>,          <span style=\"color: #f92672;\">\"placement\"</span>: <span style=\"color: #e6db74;\">\"right\"</span>,          <span style=\"color: #f92672;\">\"showLegend\"</span>: <span style=\"color: #66d9ef;\">true</span>,          <span style=\"color: #f92672;\">\"sortBy\"</span>: <span style=\"color: #e6db74;\">\"Value\"</span>,          <span style=\"color: #f92672;\">\"sortDesc\"</span>: <span style=\"color: #66d9ef;\">true</span>,          <span style=\"color: #f92672;\">\"values\"</span>: [            <span style=\"color: #e6db74;\">\"value\"</span>,            <span style=\"color: #e6db74;\">\"percent\"</span>          ]        },        <span style=\"color: #f92672;\">\"pieType\"</span>: <span style=\"color: #e6db74;\">\"donut\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"tooltip\"</span>: {          <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"multi\"</span>,          <span style=\"color: #f92672;\">\"sort\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>        }      },      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Ok\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_servers\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"status\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"ok\"</span>            }          ]        },        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Closed\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_servers\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"B\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"status\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"closed\"</span>            }          ]        },        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Unreachable\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_servers\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"default\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"C\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"status\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"unreachable\"</span>            }          ]        },        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Unavailable\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_servers\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"D\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"status\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"unavailable\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Current aggregate LSF server statistics\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"piechart\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"green\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          }        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">9</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">1</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">43</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"colorMode\"</span>: <span style=\"color: #e6db74;\">\"value\"</span>,        <span style=\"color: #f92672;\">\"graphMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,        <span style=\"color: #f92672;\">\"justifyMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"text\"</span>: {},        <span style=\"color: #f92672;\">\"textMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_jobs\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"distinct\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"running\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Currently running\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"stat\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"light-red\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          }        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">12</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">1</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">45</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"colorMode\"</span>: <span style=\"color: #e6db74;\">\"value\"</span>,        <span style=\"color: #f92672;\">\"graphMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,        <span style=\"color: #f92672;\">\"justifyMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"text\"</span>: {},        <span style=\"color: #f92672;\">\"textMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_jobs\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"default\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"suspended\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Currently suspended\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"stat\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"palette-classic\"</span>          },          <span style=\"color: #f92672;\">\"custom\"</span>: {            <span style=\"color: #f92672;\">\"hideFrom\"</span>: {              <span style=\"color: #f92672;\">\"legend\"</span>: <span style=\"color: #66d9ef;\">false</span>,              <span style=\"color: #f92672;\">\"tooltip\"</span>: <span style=\"color: #66d9ef;\">false</span>,              <span style=\"color: #f92672;\">\"viz\"</span>: <span style=\"color: #66d9ef;\">false</span>            }          },          <span style=\"color: #f92672;\">\"decimals\"</span>: <span style=\"color: #ae81ff;\">2</span>,          <span style=\"color: #f92672;\">\"mappings\"</span>: []        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">8</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">9</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">15</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">1</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">33</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"displayLabels\"</span>: [          <span style=\"color: #e6db74;\">\"name\"</span>,          <span style=\"color: #e6db74;\">\"value\"</span>        ],        <span style=\"color: #f92672;\">\"legend\"</span>: {          <span style=\"color: #f92672;\">\"displayMode\"</span>: <span style=\"color: #e6db74;\">\"table\"</span>,          <span style=\"color: #f92672;\">\"placement\"</span>: <span style=\"color: #e6db74;\">\"right\"</span>,          <span style=\"color: #f92672;\">\"showLegend\"</span>: <span style=\"color: #66d9ef;\">true</span>,          <span style=\"color: #f92672;\">\"sortBy\"</span>: <span style=\"color: #e6db74;\">\"Value\"</span>,          <span style=\"color: #f92672;\">\"sortDesc\"</span>: <span style=\"color: #66d9ef;\">true</span>,          <span style=\"color: #f92672;\">\"values\"</span>: [            <span style=\"color: #e6db74;\">\"value\"</span>,            <span style=\"color: #e6db74;\">\"percent\"</span>          ]        },        <span style=\"color: #f92672;\">\"pieType\"</span>: <span style=\"color: #e6db74;\">\"donut\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"tooltip\"</span>: {          <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"multi\"</span>,          <span style=\"color: #f92672;\">\"sort\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>        }      },      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Running\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_jobs\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"running\"</span>            }          ]        },        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Pending\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_jobs\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"B\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"pending\"</span>            }          ]        },        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Suspended\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_jobs\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"C\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"suspended\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Current aggregate LSF job statistics\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"piechart\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"yellow\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          }        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">9</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">5</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">44</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"colorMode\"</span>: <span style=\"color: #e6db74;\">\"value\"</span>,        <span style=\"color: #f92672;\">\"graphMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,        <span style=\"color: #f92672;\">\"justifyMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"text\"</span>: {},        <span style=\"color: #f92672;\">\"textMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_jobs\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"default\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"pending\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Currently pending \"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"stat\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"blue\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          }        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">12</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">5</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">46</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"colorMode\"</span>: <span style=\"color: #e6db74;\">\"value\"</span>,        <span style=\"color: #f92672;\">\"graphMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,        <span style=\"color: #f92672;\">\"justifyMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"text\"</span>: {},        <span style=\"color: #f92672;\">\"textMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_jobs\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"default\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"finished\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Finished (past hour)\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"stat\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"Spectrum LSF queue statistics. Here we show jobs in running, pending and suspended jobs. \"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"palette-classic\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"green\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              },              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"red\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #ae81ff;\">80</span>              }            ]          }        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">8</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">9</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">0</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">9</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">41</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"displayMode\"</span>: <span style=\"color: #e6db74;\">\"lcd\"</span>,        <span style=\"color: #f92672;\">\"minVizHeight\"</span>: <span style=\"color: #ae81ff;\">10</span>,        <span style=\"color: #f92672;\">\"minVizWidth\"</span>: <span style=\"color: #ae81ff;\">0</span>,        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"horizontal\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"showUnfilled\"</span>: <span style=\"color: #66d9ef;\">true</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Running\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_queues\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"run\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"name\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=~\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"/^$Queue$/\"</span>            }          ]        },        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Pending\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_queues\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"B\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"pend\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"name\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=~\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"/^$Queue$/\"</span>            }          ]        },        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Suspended\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_queues\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"C\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"susp\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"name\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=~\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"/^$Queue$/\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Current queue statistics ($Queue)\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"bargauge\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"min\"</span>: <span style=\"color: #ae81ff;\">0</span>,          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"percentage\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"green\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          },          <span style=\"color: #f92672;\">\"unit\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">9</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">9</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">53</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"/^lsf_hosts\\\\.last$/\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"showThresholdLabels\"</span>: <span style=\"color: #66d9ef;\">false</span>,        <span style=\"color: #f92672;\">\"showThresholdMarkers\"</span>: <span style=\"color: #66d9ef;\">true</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_hosts\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"current\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ],            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"peak\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"servers\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Servers\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"gauge\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"min\"</span>: <span style=\"color: #ae81ff;\">0</span>,          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"percentage\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"yellow\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          },          <span style=\"color: #f92672;\">\"unit\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">12</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">9</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">54</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"/^lsf_hosts\\\\.last$/\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"showThresholdLabels\"</span>: <span style=\"color: #66d9ef;\">false</span>,        <span style=\"color: #f92672;\">\"showThresholdMarkers\"</span>: <span style=\"color: #66d9ef;\">true</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_hosts\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"current\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ],            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"peak\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"cpus\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"CPUs\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"gauge\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"palette-classic\"</span>          },          <span style=\"color: #f92672;\">\"custom\"</span>: {            <span style=\"color: #f92672;\">\"axisCenteredZero\"</span>: <span style=\"color: #66d9ef;\">false</span>,            <span style=\"color: #f92672;\">\"axisColorMode\"</span>: <span style=\"color: #e6db74;\">\"text\"</span>,            <span style=\"color: #f92672;\">\"axisLabel\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,            <span style=\"color: #f92672;\">\"axisPlacement\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,            <span style=\"color: #f92672;\">\"barAlignment\"</span>: <span style=\"color: #ae81ff;\">0</span>,            <span style=\"color: #f92672;\">\"drawStyle\"</span>: <span style=\"color: #e6db74;\">\"line\"</span>,            <span style=\"color: #f92672;\">\"fillOpacity\"</span>: <span style=\"color: #ae81ff;\">0</span>,            <span style=\"color: #f92672;\">\"gradientMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,            <span style=\"color: #f92672;\">\"hideFrom\"</span>: {              <span style=\"color: #f92672;\">\"legend\"</span>: <span style=\"color: #66d9ef;\">false</span>,              <span style=\"color: #f92672;\">\"tooltip\"</span>: <span style=\"color: #66d9ef;\">false</span>,              <span style=\"color: #f92672;\">\"viz\"</span>: <span style=\"color: #66d9ef;\">false</span>            },            <span style=\"color: #f92672;\">\"lineInterpolation\"</span>: <span style=\"color: #e6db74;\">\"stepBefore\"</span>,            <span style=\"color: #f92672;\">\"lineWidth\"</span>: <span style=\"color: #ae81ff;\">1</span>,            <span style=\"color: #f92672;\">\"pointSize\"</span>: <span style=\"color: #ae81ff;\">5</span>,            <span style=\"color: #f92672;\">\"scaleDistribution\"</span>: {              <span style=\"color: #f92672;\">\"log\"</span>: <span style=\"color: #ae81ff;\">2</span>,              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"log\"</span>            },            <span style=\"color: #f92672;\">\"showPoints\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,            <span style=\"color: #f92672;\">\"spanNulls\"</span>: <span style=\"color: #66d9ef;\">true</span>,            <span style=\"color: #f92672;\">\"stacking\"</span>: {              <span style=\"color: #f92672;\">\"group\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,              <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>            },            <span style=\"color: #f92672;\">\"thresholdsStyle\"</span>: {              <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"off\"</span>            }          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"green\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              },              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"red\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #ae81ff;\">80</span>              }            ]          }        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">8</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">9</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">15</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">9</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">42</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"legend\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [],          <span style=\"color: #f92672;\">\"displayMode\"</span>: <span style=\"color: #e6db74;\">\"list\"</span>,          <span style=\"color: #f92672;\">\"placement\"</span>: <span style=\"color: #e6db74;\">\"bottom\"</span>,          <span style=\"color: #f92672;\">\"showLegend\"</span>: <span style=\"color: #66d9ef;\">true</span>        },        <span style=\"color: #f92672;\">\"tooltip\"</span>: {          <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"single\"</span>,          <span style=\"color: #f92672;\">\"sort\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>        }      },      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Running\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_jobs\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"running\"</span>            }          ]        },        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Pending\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_jobs\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"B\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"pending\"</span>            }          ]        },        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Suspended\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_jobs\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"C\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"suspended\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Aggregate LSF job statistics\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"timeseries\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"min\"</span>: <span style=\"color: #ae81ff;\">0</span>,          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"percentage\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"light-red\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          },          <span style=\"color: #f92672;\">\"unit\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">9</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">13</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">55</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"/^lsf_hosts\\\\.last$/\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"showThresholdLabels\"</span>: <span style=\"color: #66d9ef;\">false</span>,        <span style=\"color: #f92672;\">\"showThresholdMarkers\"</span>: <span style=\"color: #66d9ef;\">true</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_hosts\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"current\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ],            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"peak\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"cores\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Cores\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"gauge\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"min\"</span>: <span style=\"color: #ae81ff;\">0</span>,          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"percentage\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"blue\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          },          <span style=\"color: #f92672;\">\"unit\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">12</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">13</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">56</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"/^lsf_hosts\\\\.last$/\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"showThresholdLabels\"</span>: <span style=\"color: #66d9ef;\">false</span>,        <span style=\"color: #f92672;\">\"showThresholdMarkers\"</span>: <span style=\"color: #66d9ef;\">true</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_hosts\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"current\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ],            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"peak\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"slots\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Slots\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"gauge\"</span>    },    {      <span style=\"color: #f92672;\">\"collapsed\"</span>: <span style=\"color: #66d9ef;\">false</span>,      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">1</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">24</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">0</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">17</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">37</span>,      <span style=\"color: #f92672;\">\"panels\"</span>: [],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"LSF scheduler statistics\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"row\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"palette-classic\"</span>          },          <span style=\"color: #f92672;\">\"custom\"</span>: {            <span style=\"color: #f92672;\">\"axisCenteredZero\"</span>: <span style=\"color: #66d9ef;\">false</span>,            <span style=\"color: #f92672;\">\"axisColorMode\"</span>: <span style=\"color: #e6db74;\">\"text\"</span>,            <span style=\"color: #f92672;\">\"axisLabel\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,            <span style=\"color: #f92672;\">\"axisPlacement\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,            <span style=\"color: #f92672;\">\"barAlignment\"</span>: <span style=\"color: #ae81ff;\">0</span>,            <span style=\"color: #f92672;\">\"drawStyle\"</span>: <span style=\"color: #e6db74;\">\"line\"</span>,            <span style=\"color: #f92672;\">\"fillOpacity\"</span>: <span style=\"color: #ae81ff;\">10</span>,            <span style=\"color: #f92672;\">\"gradientMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,            <span style=\"color: #f92672;\">\"hideFrom\"</span>: {              <span style=\"color: #f92672;\">\"graph\"</span>: <span style=\"color: #66d9ef;\">false</span>,              <span style=\"color: #f92672;\">\"legend\"</span>: <span style=\"color: #66d9ef;\">false</span>,              <span style=\"color: #f92672;\">\"tooltip\"</span>: <span style=\"color: #66d9ef;\">false</span>,              <span style=\"color: #f92672;\">\"viz\"</span>: <span style=\"color: #66d9ef;\">false</span>            },            <span style=\"color: #f92672;\">\"lineInterpolation\"</span>: <span style=\"color: #e6db74;\">\"linear\"</span>,            <span style=\"color: #f92672;\">\"lineWidth\"</span>: <span style=\"color: #ae81ff;\">1</span>,            <span style=\"color: #f92672;\">\"pointSize\"</span>: <span style=\"color: #ae81ff;\">5</span>,            <span style=\"color: #f92672;\">\"scaleDistribution\"</span>: {              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"linear\"</span>            },            <span style=\"color: #f92672;\">\"showPoints\"</span>: <span style=\"color: #e6db74;\">\"never\"</span>,            <span style=\"color: #f92672;\">\"spanNulls\"</span>: <span style=\"color: #66d9ef;\">true</span>,            <span style=\"color: #f92672;\">\"stacking\"</span>: {              <span style=\"color: #f92672;\">\"group\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,              <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>            },            <span style=\"color: #f92672;\">\"thresholdsStyle\"</span>: {              <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"off\"</span>            }          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"green\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              },              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"red\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #ae81ff;\">80</span>              }            ]          },          <span style=\"color: #f92672;\">\"unit\"</span>: <span style=\"color: #e6db74;\">\"short\"</span>        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">8</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">12</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">0</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">18</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">20</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"graph\"</span>: {},        <span style=\"color: #f92672;\">\"legend\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [],          <span style=\"color: #f92672;\">\"displayMode\"</span>: <span style=\"color: #e6db74;\">\"list\"</span>,          <span style=\"color: #f92672;\">\"placement\"</span>: <span style=\"color: #e6db74;\">\"right\"</span>,          <span style=\"color: #f92672;\">\"showLegend\"</span>: <span style=\"color: #66d9ef;\">true</span>        },        <span style=\"color: #f92672;\">\"tooltip\"</span>: {          <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"single\"</span>,          <span style=\"color: #f92672;\">\"sort\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>        }      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"7.5.15\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"CPU utilization (%)\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"procstat\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"cpu_usage\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"exe\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"mbatchd\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            }          ]        },        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Memory utilization (%)\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"procstat\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"B\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"memory_usage\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"exe\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"mbatchd\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            }          ]        },        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Number of threads\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"procstat\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"C\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"num_threads\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"exe\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"mbatchd\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            }          ]        },        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"File descriptors\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_mbatchd\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"D\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"fd\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"used\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"LSF mbatchd process metrics\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"timeseries\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"palette-classic\"</span>          },          <span style=\"color: #f92672;\">\"custom\"</span>: {            <span style=\"color: #f92672;\">\"axisCenteredZero\"</span>: <span style=\"color: #66d9ef;\">false</span>,            <span style=\"color: #f92672;\">\"axisColorMode\"</span>: <span style=\"color: #e6db74;\">\"text\"</span>,            <span style=\"color: #f92672;\">\"axisLabel\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,            <span style=\"color: #f92672;\">\"axisPlacement\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,            <span style=\"color: #f92672;\">\"barAlignment\"</span>: <span style=\"color: #ae81ff;\">0</span>,            <span style=\"color: #f92672;\">\"drawStyle\"</span>: <span style=\"color: #e6db74;\">\"line\"</span>,            <span style=\"color: #f92672;\">\"fillOpacity\"</span>: <span style=\"color: #ae81ff;\">10</span>,            <span style=\"color: #f92672;\">\"gradientMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,            <span style=\"color: #f92672;\">\"hideFrom\"</span>: {              <span style=\"color: #f92672;\">\"graph\"</span>: <span style=\"color: #66d9ef;\">false</span>,              <span style=\"color: #f92672;\">\"legend\"</span>: <span style=\"color: #66d9ef;\">false</span>,              <span style=\"color: #f92672;\">\"tooltip\"</span>: <span style=\"color: #66d9ef;\">false</span>,              <span style=\"color: #f92672;\">\"viz\"</span>: <span style=\"color: #66d9ef;\">false</span>            },            <span style=\"color: #f92672;\">\"lineInterpolation\"</span>: <span style=\"color: #e6db74;\">\"linear\"</span>,            <span style=\"color: #f92672;\">\"lineWidth\"</span>: <span style=\"color: #ae81ff;\">1</span>,            <span style=\"color: #f92672;\">\"pointSize\"</span>: <span style=\"color: #ae81ff;\">5</span>,            <span style=\"color: #f92672;\">\"scaleDistribution\"</span>: {              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"linear\"</span>            },            <span style=\"color: #f92672;\">\"showPoints\"</span>: <span style=\"color: #e6db74;\">\"never\"</span>,            <span style=\"color: #f92672;\">\"spanNulls\"</span>: <span style=\"color: #66d9ef;\">true</span>,            <span style=\"color: #f92672;\">\"stacking\"</span>: {              <span style=\"color: #f92672;\">\"group\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,              <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>            },            <span style=\"color: #f92672;\">\"thresholdsStyle\"</span>: {              <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"off\"</span>            }          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"green\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              },              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"red\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #ae81ff;\">80</span>              }            ]          },          <span style=\"color: #f92672;\">\"unit\"</span>: <span style=\"color: #e6db74;\">\"short\"</span>        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">8</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">12</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">12</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">18</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">57</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"graph\"</span>: {},        <span style=\"color: #f92672;\">\"legend\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [],          <span style=\"color: #f92672;\">\"displayMode\"</span>: <span style=\"color: #e6db74;\">\"list\"</span>,          <span style=\"color: #f92672;\">\"placement\"</span>: <span style=\"color: #e6db74;\">\"right\"</span>,          <span style=\"color: #f92672;\">\"showLegend\"</span>: <span style=\"color: #66d9ef;\">true</span>        },        <span style=\"color: #f92672;\">\"tooltip\"</span>: {          <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"single\"</span>,          <span style=\"color: #f92672;\">\"sort\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>        }      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"7.5.15\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"CPU utilization (%)\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"procstat\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"cpu_usage\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"exe\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"lim\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            }          ]        },        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Memory utilization (%)\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"procstat\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"B\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"memory_usage\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"exe\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"lim\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            }          ]        },        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Number of threads\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"procstat\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"C\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"num_threads\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"exe\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"lim\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"LSF management lim process metrics\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"timeseries\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"palette-classic\"</span>          },          <span style=\"color: #f92672;\">\"custom\"</span>: {            <span style=\"color: #f92672;\">\"axisCenteredZero\"</span>: <span style=\"color: #66d9ef;\">false</span>,            <span style=\"color: #f92672;\">\"axisColorMode\"</span>: <span style=\"color: #e6db74;\">\"text\"</span>,            <span style=\"color: #f92672;\">\"axisLabel\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,            <span style=\"color: #f92672;\">\"axisPlacement\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,            <span style=\"color: #f92672;\">\"barAlignment\"</span>: <span style=\"color: #ae81ff;\">0</span>,            <span style=\"color: #f92672;\">\"drawStyle\"</span>: <span style=\"color: #e6db74;\">\"line\"</span>,            <span style=\"color: #f92672;\">\"fillOpacity\"</span>: <span style=\"color: #ae81ff;\">10</span>,            <span style=\"color: #f92672;\">\"gradientMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,            <span style=\"color: #f92672;\">\"hideFrom\"</span>: {              <span style=\"color: #f92672;\">\"graph\"</span>: <span style=\"color: #66d9ef;\">false</span>,              <span style=\"color: #f92672;\">\"legend\"</span>: <span style=\"color: #66d9ef;\">false</span>,              <span style=\"color: #f92672;\">\"tooltip\"</span>: <span style=\"color: #66d9ef;\">false</span>,              <span style=\"color: #f92672;\">\"viz\"</span>: <span style=\"color: #66d9ef;\">false</span>            },            <span style=\"color: #f92672;\">\"lineInterpolation\"</span>: <span style=\"color: #e6db74;\">\"linear\"</span>,            <span style=\"color: #f92672;\">\"lineWidth\"</span>: <span style=\"color: #ae81ff;\">1</span>,            <span style=\"color: #f92672;\">\"pointSize\"</span>: <span style=\"color: #ae81ff;\">5</span>,            <span style=\"color: #f92672;\">\"scaleDistribution\"</span>: {              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"linear\"</span>            },            <span style=\"color: #f92672;\">\"showPoints\"</span>: <span style=\"color: #e6db74;\">\"never\"</span>,            <span style=\"color: #f92672;\">\"spanNulls\"</span>: <span style=\"color: #66d9ef;\">true</span>,            <span style=\"color: #f92672;\">\"stacking\"</span>: {              <span style=\"color: #f92672;\">\"group\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,              <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>            },            <span style=\"color: #f92672;\">\"thresholdsStyle\"</span>: {              <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"off\"</span>            }          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"green\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              },              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"red\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #ae81ff;\">80</span>              }            ]          },          <span style=\"color: #f92672;\">\"unit\"</span>: <span style=\"color: #e6db74;\">\"short\"</span>        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">8</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">12</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">0</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">26</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">27</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"graph\"</span>: {},        <span style=\"color: #f92672;\">\"legend\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [],          <span style=\"color: #f92672;\">\"displayMode\"</span>: <span style=\"color: #e6db74;\">\"list\"</span>,          <span style=\"color: #f92672;\">\"placement\"</span>: <span style=\"color: #e6db74;\">\"right\"</span>,          <span style=\"color: #f92672;\">\"showLegend\"</span>: <span style=\"color: #66d9ef;\">true</span>        },        <span style=\"color: #f92672;\">\"tooltip\"</span>: {          <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"single\"</span>,          <span style=\"color: #f92672;\">\"sort\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>        }      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"7.5.15\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Job buckets\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_mbatchd\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"sched\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"buckets\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            }          ]        },        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Matching host criteria\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_mbatchd\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"B\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"sched\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"matchhost\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            }          ]        },        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Scheduling interval (seconds)\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_mbatchd\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"C\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"sched\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"interval\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"LSF scheduler metrics\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"timeseries\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"palette-classic\"</span>          },          <span style=\"color: #f92672;\">\"custom\"</span>: {            <span style=\"color: #f92672;\">\"axisCenteredZero\"</span>: <span style=\"color: #66d9ef;\">false</span>,            <span style=\"color: #f92672;\">\"axisColorMode\"</span>: <span style=\"color: #e6db74;\">\"text\"</span>,            <span style=\"color: #f92672;\">\"axisLabel\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,            <span style=\"color: #f92672;\">\"axisPlacement\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,            <span style=\"color: #f92672;\">\"barAlignment\"</span>: <span style=\"color: #ae81ff;\">0</span>,            <span style=\"color: #f92672;\">\"drawStyle\"</span>: <span style=\"color: #e6db74;\">\"line\"</span>,            <span style=\"color: #f92672;\">\"fillOpacity\"</span>: <span style=\"color: #ae81ff;\">10</span>,            <span style=\"color: #f92672;\">\"gradientMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,            <span style=\"color: #f92672;\">\"hideFrom\"</span>: {              <span style=\"color: #f92672;\">\"graph\"</span>: <span style=\"color: #66d9ef;\">false</span>,              <span style=\"color: #f92672;\">\"legend\"</span>: <span style=\"color: #66d9ef;\">false</span>,              <span style=\"color: #f92672;\">\"tooltip\"</span>: <span style=\"color: #66d9ef;\">false</span>,              <span style=\"color: #f92672;\">\"viz\"</span>: <span style=\"color: #66d9ef;\">false</span>            },            <span style=\"color: #f92672;\">\"lineInterpolation\"</span>: <span style=\"color: #e6db74;\">\"linear\"</span>,            <span style=\"color: #f92672;\">\"lineWidth\"</span>: <span style=\"color: #ae81ff;\">1</span>,            <span style=\"color: #f92672;\">\"pointSize\"</span>: <span style=\"color: #ae81ff;\">5</span>,            <span style=\"color: #f92672;\">\"scaleDistribution\"</span>: {              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"linear\"</span>            },            <span style=\"color: #f92672;\">\"showPoints\"</span>: <span style=\"color: #e6db74;\">\"never\"</span>,            <span style=\"color: #f92672;\">\"spanNulls\"</span>: <span style=\"color: #66d9ef;\">true</span>,            <span style=\"color: #f92672;\">\"stacking\"</span>: {              <span style=\"color: #f92672;\">\"group\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,              <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>            },            <span style=\"color: #f92672;\">\"thresholdsStyle\"</span>: {              <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"off\"</span>            }          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"green\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              },              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"red\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #ae81ff;\">80</span>              }            ]          },          <span style=\"color: #f92672;\">\"unit\"</span>: <span style=\"color: #e6db74;\">\"short\"</span>        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">8</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">12</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">12</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">26</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">58</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"graph\"</span>: {},        <span style=\"color: #f92672;\">\"legend\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [],          <span style=\"color: #f92672;\">\"displayMode\"</span>: <span style=\"color: #e6db74;\">\"list\"</span>,          <span style=\"color: #f92672;\">\"placement\"</span>: <span style=\"color: #e6db74;\">\"right\"</span>,          <span style=\"color: #f92672;\">\"showLegend\"</span>: <span style=\"color: #66d9ef;\">true</span>        },        <span style=\"color: #f92672;\">\"tooltip\"</span>: {          <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"single\"</span>,          <span style=\"color: #f92672;\">\"sort\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>        }      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"7.5.15\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"CPU utilization (%)\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"procstat\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"cpu_usage\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"exe\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"mbschd\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            }          ]        },        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Memory utilization (%)\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"procstat\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"B\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"memory_usage\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"exe\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"mbatchd\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            }          ]        },        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Number of threads\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"procstat\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"C\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"num_threads\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"exe\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"mbatchd\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"LSF mbschd process metrics\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"timeseries\"</span>    },    {      <span style=\"color: #f92672;\">\"collapsed\"</span>: <span style=\"color: #66d9ef;\">false</span>,      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">1</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">24</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">0</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">34</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">39</span>,      <span style=\"color: #f92672;\">\"panels\"</span>: [],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Additional metrics (scratch)\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"row\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"green\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          }        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">0</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">35</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">2</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"colorMode\"</span>: <span style=\"color: #e6db74;\">\"value\"</span>,        <span style=\"color: #f92672;\">\"graphMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,        <span style=\"color: #f92672;\">\"justifyMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"text\"</span>: {},        <span style=\"color: #f92672;\">\"textMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_jobs\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"distinct\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"running\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Running\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"stat\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"yellow\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          }        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">35</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">5</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"colorMode\"</span>: <span style=\"color: #e6db74;\">\"value\"</span>,        <span style=\"color: #f92672;\">\"graphMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,        <span style=\"color: #f92672;\">\"justifyMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"text\"</span>: {},        <span style=\"color: #f92672;\">\"textMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_jobs\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"default\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"pending\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Pending\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"stat\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"red\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          }        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">6</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">35</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">6</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"colorMode\"</span>: <span style=\"color: #e6db74;\">\"value\"</span>,        <span style=\"color: #f92672;\">\"graphMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,        <span style=\"color: #f92672;\">\"justifyMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"text\"</span>: {},        <span style=\"color: #f92672;\">\"textMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_jobs\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"default\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"suspended\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Suspended\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"stat\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"blue\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          }        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">9</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">35</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">7</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"colorMode\"</span>: <span style=\"color: #e6db74;\">\"value\"</span>,        <span style=\"color: #f92672;\">\"graphMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,        <span style=\"color: #f92672;\">\"justifyMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"text\"</span>: {},        <span style=\"color: #f92672;\">\"textMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_jobs\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"default\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"finished\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Finished\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"stat\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"green\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          }        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">12</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">35</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">15</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"colorMode\"</span>: <span style=\"color: #e6db74;\">\"value\"</span>,        <span style=\"color: #f92672;\">\"graphMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,        <span style=\"color: #f92672;\">\"justifyMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"text\"</span>: {},        <span style=\"color: #f92672;\">\"textMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Ok\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_servers\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"status\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"ok\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Ok\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"stat\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"blue\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          }        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">15</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">35</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">16</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"colorMode\"</span>: <span style=\"color: #e6db74;\">\"value\"</span>,        <span style=\"color: #f92672;\">\"graphMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,        <span style=\"color: #f92672;\">\"justifyMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"text\"</span>: {},        <span style=\"color: #f92672;\">\"textMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Closed\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_servers\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"status\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"closed\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Closed\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"stat\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"yellow\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          }        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">18</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">35</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">17</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"colorMode\"</span>: <span style=\"color: #e6db74;\">\"value\"</span>,        <span style=\"color: #f92672;\">\"graphMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,        <span style=\"color: #f92672;\">\"justifyMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"text\"</span>: {},        <span style=\"color: #f92672;\">\"textMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Unreachable\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_servers\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"status\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"unreachable\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Unreachable\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"stat\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"red\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          }        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">21</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">35</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">18</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"colorMode\"</span>: <span style=\"color: #e6db74;\">\"value\"</span>,        <span style=\"color: #f92672;\">\"graphMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,        <span style=\"color: #f92672;\">\"justifyMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"text\"</span>: {},        <span style=\"color: #f92672;\">\"textMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Unavailable\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_servers\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"value\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"mean\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"status\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"unavailable\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Unavailable\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"stat\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"green\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          }        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">0</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">39</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">21</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"colorMode\"</span>: <span style=\"color: #e6db74;\">\"value\"</span>,        <span style=\"color: #f92672;\">\"graphMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,        <span style=\"color: #f92672;\">\"justifyMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"text\"</span>: {},        <span style=\"color: #f92672;\">\"textMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Clients\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_hosts\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"current\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"clients\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Clients\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"stat\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"green\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          }        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">39</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">22</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"colorMode\"</span>: <span style=\"color: #e6db74;\">\"value\"</span>,        <span style=\"color: #f92672;\">\"graphMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,        <span style=\"color: #f92672;\">\"justifyMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"text\"</span>: {},        <span style=\"color: #f92672;\">\"textMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Servers\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_hosts\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"current\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"servers\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Servers\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"stat\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"green\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          }        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">6</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">39</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">23</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"colorMode\"</span>: <span style=\"color: #e6db74;\">\"value\"</span>,        <span style=\"color: #f92672;\">\"graphMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,        <span style=\"color: #f92672;\">\"justifyMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"text\"</span>: {},        <span style=\"color: #f92672;\">\"textMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Servers\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_hosts\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"current\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"cpus\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"CPUs\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"stat\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"green\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          }        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">9</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">39</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">24</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"colorMode\"</span>: <span style=\"color: #e6db74;\">\"value\"</span>,        <span style=\"color: #f92672;\">\"graphMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,        <span style=\"color: #f92672;\">\"justifyMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"text\"</span>: {},        <span style=\"color: #f92672;\">\"textMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Cores\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_hosts\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"current\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"cores\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Cores\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"stat\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"absolute\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"green\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          }        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">12</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">39</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">25</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"colorMode\"</span>: <span style=\"color: #e6db74;\">\"value\"</span>,        <span style=\"color: #f92672;\">\"graphMode\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>,        <span style=\"color: #f92672;\">\"justifyMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"text\"</span>: {},        <span style=\"color: #f92672;\">\"textMode\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"alias\"</span>: <span style=\"color: #e6db74;\">\"Slots\"</span>,          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_hosts\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"current\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"slots\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Slots\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"stat\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"min\"</span>: <span style=\"color: #ae81ff;\">0</span>,          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"percentage\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"green\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          },          <span style=\"color: #f92672;\">\"unit\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">43</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">52</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"/^lsf_hosts\\\\.last$/\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"showThresholdLabels\"</span>: <span style=\"color: #66d9ef;\">false</span>,        <span style=\"color: #f92672;\">\"showThresholdMarkers\"</span>: <span style=\"color: #66d9ef;\">true</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_hosts\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"current\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ],            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"peak\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"servers\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Servers\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"gauge\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"min\"</span>: <span style=\"color: #ae81ff;\">0</span>,          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"percentage\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"yellow\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          },          <span style=\"color: #f92672;\">\"unit\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">6</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">43</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">51</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"/^lsf_hosts\\\\.last$/\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"showThresholdLabels\"</span>: <span style=\"color: #66d9ef;\">false</span>,        <span style=\"color: #f92672;\">\"showThresholdMarkers\"</span>: <span style=\"color: #66d9ef;\">true</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_hosts\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"current\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ],            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"peak\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"cpus\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"CPUs\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"gauge\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"min\"</span>: <span style=\"color: #ae81ff;\">0</span>,          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"percentage\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"light-red\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          },          <span style=\"color: #f92672;\">\"unit\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">9</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">43</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">50</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"/^lsf_hosts\\\\.last$/\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"showThresholdLabels\"</span>: <span style=\"color: #66d9ef;\">false</span>,        <span style=\"color: #f92672;\">\"showThresholdMarkers\"</span>: <span style=\"color: #66d9ef;\">true</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_hosts\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"current\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ],            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"peak\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"cores\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Cores\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"gauge\"</span>    },    {      <span style=\"color: #f92672;\">\"datasource\"</span>: {        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,        <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>      },      <span style=\"color: #f92672;\">\"description\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,      <span style=\"color: #f92672;\">\"fieldConfig\"</span>: {        <span style=\"color: #f92672;\">\"defaults\"</span>: {          <span style=\"color: #f92672;\">\"color\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"thresholds\"</span>          },          <span style=\"color: #f92672;\">\"mappings\"</span>: [],          <span style=\"color: #f92672;\">\"min\"</span>: <span style=\"color: #ae81ff;\">0</span>,          <span style=\"color: #f92672;\">\"thresholds\"</span>: {            <span style=\"color: #f92672;\">\"mode\"</span>: <span style=\"color: #e6db74;\">\"percentage\"</span>,            <span style=\"color: #f92672;\">\"steps\"</span>: [              {                <span style=\"color: #f92672;\">\"color\"</span>: <span style=\"color: #e6db74;\">\"blue\"</span>,                <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #66d9ef;\">null</span>              }            ]          },          <span style=\"color: #f92672;\">\"unit\"</span>: <span style=\"color: #e6db74;\">\"none\"</span>        },        <span style=\"color: #f92672;\">\"overrides\"</span>: []      },      <span style=\"color: #f92672;\">\"gridPos\"</span>: {        <span style=\"color: #f92672;\">\"h\"</span>: <span style=\"color: #ae81ff;\">4</span>,        <span style=\"color: #f92672;\">\"w\"</span>: <span style=\"color: #ae81ff;\">3</span>,        <span style=\"color: #f92672;\">\"x\"</span>: <span style=\"color: #ae81ff;\">12</span>,        <span style=\"color: #f92672;\">\"y\"</span>: <span style=\"color: #ae81ff;\">43</span>      },      <span style=\"color: #f92672;\">\"id\"</span>: <span style=\"color: #ae81ff;\">49</span>,      <span style=\"color: #f92672;\">\"options\"</span>: {        <span style=\"color: #f92672;\">\"orientation\"</span>: <span style=\"color: #e6db74;\">\"auto\"</span>,        <span style=\"color: #f92672;\">\"reduceOptions\"</span>: {          <span style=\"color: #f92672;\">\"calcs\"</span>: [            <span style=\"color: #e6db74;\">\"lastNotNull\"</span>          ],          <span style=\"color: #f92672;\">\"fields\"</span>: <span style=\"color: #e6db74;\">\"/^lsf_hosts\\\\.last$/\"</span>,          <span style=\"color: #f92672;\">\"values\"</span>: <span style=\"color: #66d9ef;\">false</span>        },        <span style=\"color: #f92672;\">\"showThresholdLabels\"</span>: <span style=\"color: #66d9ef;\">false</span>,        <span style=\"color: #f92672;\">\"showThresholdMarkers\"</span>: <span style=\"color: #66d9ef;\">true</span>      },      <span style=\"color: #f92672;\">\"pluginVersion\"</span>: <span style=\"color: #e6db74;\">\"9.1.6\"</span>,      <span style=\"color: #f92672;\">\"targets\"</span>: [        {          <span style=\"color: #f92672;\">\"datasource\"</span>: {            <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,            <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"eNfWCy5Vk\"</span>          },          <span style=\"color: #f92672;\">\"groupBy\"</span>: [            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"$__interval\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"time\"</span>            },            {              <span style=\"color: #f92672;\">\"params\"</span>: [                <span style=\"color: #e6db74;\">\"null\"</span>              ],              <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"fill\"</span>            }          ],          <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #66d9ef;\">false</span>,          <span style=\"color: #f92672;\">\"measurement\"</span>: <span style=\"color: #e6db74;\">\"lsf_hosts\"</span>,          <span style=\"color: #f92672;\">\"orderByTime\"</span>: <span style=\"color: #e6db74;\">\"ASC\"</span>,          <span style=\"color: #f92672;\">\"policy\"</span>: <span style=\"color: #e6db74;\">\"autogen\"</span>,          <span style=\"color: #f92672;\">\"refId\"</span>: <span style=\"color: #e6db74;\">\"A\"</span>,          <span style=\"color: #f92672;\">\"resultFormat\"</span>: <span style=\"color: #e6db74;\">\"time_series\"</span>,          <span style=\"color: #f92672;\">\"select\"</span>: [            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"current\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              },              {                <span style=\"color: #f92672;\">\"params\"</span>: [],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"last\"</span>              }            ],            [              {                <span style=\"color: #f92672;\">\"params\"</span>: [                  <span style=\"color: #e6db74;\">\"peak\"</span>                ],                <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"field\"</span>              }            ]          ],          <span style=\"color: #f92672;\">\"tags\"</span>: [            {              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"host\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"kilenc\"</span>            },            {              <span style=\"color: #f92672;\">\"condition\"</span>: <span style=\"color: #e6db74;\">\"AND\"</span>,              <span style=\"color: #f92672;\">\"key\"</span>: <span style=\"color: #e6db74;\">\"state\"</span>,              <span style=\"color: #f92672;\">\"operator\"</span>: <span style=\"color: #e6db74;\">\"=\"</span>,              <span style=\"color: #f92672;\">\"value\"</span>: <span style=\"color: #e6db74;\">\"slots\"</span>            }          ]        }      ],      <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"Slots\"</span>,      <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"gauge\"</span>    }  ],  <span style=\"color: #f92672;\">\"refresh\"</span>: <span style=\"color: #e6db74;\">\"30s\"</span>,  <span style=\"color: #f92672;\">\"schemaVersion\"</span>: <span style=\"color: #ae81ff;\">37</span>,  <span style=\"color: #f92672;\">\"style\"</span>: <span style=\"color: #e6db74;\">\"dark\"</span>,  <span style=\"color: #f92672;\">\"tags\"</span>: [],  <span style=\"color: #f92672;\">\"templating\"</span>: {    <span style=\"color: #f92672;\">\"list\"</span>: [      {        <span style=\"color: #f92672;\">\"current\"</span>: {          <span style=\"color: #f92672;\">\"selected\"</span>: <span style=\"color: #66d9ef;\">true</span>,          <span style=\"color: #f92672;\">\"text\"</span>: [            <span style=\"color: #e6db74;\">\"priority\"</span>          ],          <span style=\"color: #f92672;\">\"value\"</span>: [            <span style=\"color: #e6db74;\">\"priority\"</span>          ]        },        <span style=\"color: #f92672;\">\"datasource\"</span>: {          <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"influxdb\"</span>,          <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"oSnSlVc4k\"</span>        },        <span style=\"color: #f92672;\">\"definition\"</span>: <span style=\"color: #e6db74;\">\"show tag values from \\\"lsf_queues\\\" with key=\\\"name\\\"\"</span>,        <span style=\"color: #f92672;\">\"hide\"</span>: <span style=\"color: #ae81ff;\">0</span>,        <span style=\"color: #f92672;\">\"includeAll\"</span>: <span style=\"color: #66d9ef;\">false</span>,        <span style=\"color: #f92672;\">\"multi\"</span>: <span style=\"color: #66d9ef;\">false</span>,        <span style=\"color: #f92672;\">\"name\"</span>: <span style=\"color: #e6db74;\">\"Queue\"</span>,        <span style=\"color: #f92672;\">\"options\"</span>: [],        <span style=\"color: #f92672;\">\"query\"</span>: <span style=\"color: #e6db74;\">\"show tag values from \\\"lsf_queues\\\" with key=\\\"name\\\"\"</span>,        <span style=\"color: #f92672;\">\"refresh\"</span>: <span style=\"color: #ae81ff;\">1</span>,        <span style=\"color: #f92672;\">\"regex\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,        <span style=\"color: #f92672;\">\"skipUrlSync\"</span>: <span style=\"color: #66d9ef;\">false</span>,        <span style=\"color: #f92672;\">\"sort\"</span>: <span style=\"color: #ae81ff;\">0</span>,        <span style=\"color: #f92672;\">\"tagValuesQuery\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,        <span style=\"color: #f92672;\">\"tagsQuery\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,        <span style=\"color: #f92672;\">\"type\"</span>: <span style=\"color: #e6db74;\">\"query\"</span>,        <span style=\"color: #f92672;\">\"useTags\"</span>: <span style=\"color: #66d9ef;\">false</span>      }    ]  },  <span style=\"color: #f92672;\">\"time\"</span>: {    <span style=\"color: #f92672;\">\"from\"</span>: <span style=\"color: #e6db74;\">\"now-1h\"</span>,    <span style=\"color: #f92672;\">\"to\"</span>: <span style=\"color: #e6db74;\">\"now\"</span>  },  <span style=\"color: #f92672;\">\"timepicker\"</span>: {},  <span style=\"color: #f92672;\">\"timezone\"</span>: <span style=\"color: #e6db74;\">\"\"</span>,  <span style=\"color: #f92672;\">\"title\"</span>: <span style=\"color: #e6db74;\">\"LSF cluster status\"</span>,  <span style=\"color: #f92672;\">\"uid\"</span>: <span style=\"color: #e6db74;\">\"ORojp8cVz\"</span>,  <span style=\"color: #f92672;\">\"version\"</span>: <span style=\"color: #ae81ff;\">160</span>,  <span style=\"color: #f92672;\">\"weekStart\"</span>: <span style=\"color: #e6db74;\">\"\"</span>}</code></pre></div></details><hr /><p>As you can see, with a short plugin script to collect information from LSF, it&rsquo;s possible to monitor your LSF cluster using the TIG stack. It&rsquo;s important to note that there are powerfulmonitoring and reporting tools available from IBM as add-ons to LSF; IBM Spectrum LSF RTM and IBM Spectrum LSF Explorer. You can find more details about the add-on capabilities for LSF<a href=\"https://www.ibm.com/products/hpc-workload-management/resources\">here</a>.</p>",
            "url": "https://hpc.social/personal-blog/2023/monitoring-ibm-spectrum-lsf-with-the-tig-stack/",
            
            
            
            
            
            "date_published": "2023-01-24T19:48:44-07:00",
            "date_modified": "2023-01-24T19:48:44-07:00",
            
                "author": "Ramblings of a supercomputing enthusiast."
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/adam-s-weekly-ish-update-2022-12-20/",
            "title": "Adam’s weekly (-ish) update, 2022-12-20",
            "summary": null,
            "content_text": "What&#8217;s newThe past few weeks have been on the intense side at work, so I completely lost track of the blog and haven&#8217;t had a chance to write much in that time. However, I&#8217;m now on a holiday break, and finally have time to sit down at a keyboard to write more than code and Slack messages.One of the highlights of the past few weeks was a trip to San Jose, and the NVIDIA headquarters. I changed teams at work back in July, transferring from a group that was closely integrated with product management, to a more straightforward engineering team which designs and builds new high-performance computing systems. This was the first chance I&#8217;ve had to meet up with other members of my new team in person, and it was a really wonderful experience to be in the same physical space as folks who were previously just images on my screen. I love working remotely, but it&#8217;s also great to be able to stand in front of a white board with someone and brainstorm, or get coffee and just have a chat with a coworker outside of a video call with an agenda.(Plus, we were all careful and managed to avoid catching COVID from each other! Which was a win on its own.)Now, for the next two weeks I&#8217;m off work, and planning to take some time to relax and spend time on projects that are harder to focus on during busy work weeks. Expect (maybe) less about computers in my blog and social feeds, and more about D&amp;D, baking, and tasty cocktails.What I&#8217;m reading, watching, and listening toI&#8217;ve been a bit too scattered to focus on actual books the past few weeks, but I did find time for a few interesting articles and podcasts. In particular,&#8220;Why Roman Egypt was such a strange province&#8221;, from Bret Devereaux: As usual from Devereaux, an accessible but extremely detailed discussion of why so much of what we know about the Roman empire is from Egyptian records, but why that also might not be representative of the broader empire.&#8220;Emoji as incident resolution tools&#8221;, from Will Gallego: A fun discussion of how using emoji as part of a team&#8217;s communication can add nuance and shared understanding during incident management, along with a discussion of the disadvantages and costs associated with the practice.&#8220;What does modern software architecture look like in 2022?&#8221;, from Bartosz Mikulski: A nice  article which discusses how service-oriented software architecture can often include an explicit expectation of change. For example, the architecture might include notes on an ongoing deprecation of a library, or might signpost the need to factor a new microservice out when overall system load gets high enough.The Brady Heywood podcast: Found via the Oxide and Friends podcast, the Brady Heywood podcast is a series on engineering disasters and their consequences from a forensic engineering firm. It&#8217;s mostly not being updated any more (with the podcasters moving on to a separate series on complexity science), but it has a deep back catalog of good episodes, and includes thoughtful discussions of human factors, safety engineering, and how organizational pressures become manifest in engineering artifacts.Recent recipesSmitten Kitchen&#8217;s Homemade Irish Cream: This is a recipe I make every year, and I often give away small bottles of it as holiday gifts. It&#8217;s really ridiculously tasty, much better than Baileys  or similar, and good either on its own or in hot chocolate.Smitten Kitchen&#8217;s Fairytale of New York: This is a really tasty whiskey cocktail, and the star of the show is a &#8220;winter warmth syrup&#8221; that substitutes in for simple syrup. The syrup is simply very tasty, and turns what&#8217;s effectively an OId Fashioned variant into a lovely holiday cocktail.Sparkling gingerbread from Yossy Arefi&#8217;s Snaking Cakes: This recipe takes a little more prep than most of Arefi&#8217;s &#8220;snacking cakes&#8221;, as it includes ginger three ways (ground, fresh, and crystallized), but it&#8217;s worth the few minutes of extra work.Pet photosI&#8217;m pretty sure these two want me to turn the fireplace on.Just Percy bullying the dog by stealing his bed.",
            "content_html": "<h2>What&#8217;s new</h2><p>The past few weeks have been on the intense side at work, so I completely lost track of the blog and haven&#8217;t had a chance to write much in that time. However, I&#8217;m now on a holiday break, and finally have time to sit down at a keyboard to write more than code and Slack messages.</p><p><span id=\"more-289\"></span></p><p>One of the highlights of the past few weeks was a trip to San Jose, and the NVIDIA headquarters. I changed teams at work back in July, transferring from a group that was closely integrated with product management, to a more straightforward engineering team which <a href=\"https://blogs.nvidia.com/blog/2020/08/14/making-selene-pandemic-ai/\">designs and builds new high-performance computing systems</a>. </p><p>This was the first chance I&#8217;ve had to meet up with other members of my new team in person, and it was a really wonderful experience to be in the same physical space as folks who were previously just images on my screen. I love working remotely, but it&#8217;s also great to be able to stand in front of a white board with someone and brainstorm, or get coffee and just have a chat with a coworker outside of a video call with an agenda.</p><p>(Plus, we were all careful and managed to avoid catching COVID from each other! Which was a win on its own.)</p><p>Now, for the next two weeks I&#8217;m off work, and planning to take some time to relax and spend time on projects that are harder to focus on during busy work weeks. Expect (maybe) less about computers in my blog and social feeds, and more about D&amp;D, baking, and tasty cocktails.</p><h2>What I&#8217;m reading, watching, and listening to</h2><p>I&#8217;ve been a bit too scattered to focus on actual books the past few weeks, but I did find time for a few interesting articles and podcasts. In particular,</p><ul><li><a href=\"https://acoup.blog/2022/12/02/collections-why-roman-egypt-was-such-a-strange-province/\">&#8220;Why Roman Egypt was such a strange province&#8221;</a>, from Bret Devereaux: As usual from Devereaux, an accessible but extremely detailed discussion of why so much of what we know about the Roman empire is from Egyptian records, but why that also might not be representative of the broader empire.</li><li><a href=\"https://willgallego.com/2022/12/18/emoji-as-incident-resolution-tools/\">&#8220;Emoji as incident resolution tools&#8221;</a>, from Will Gallego: A fun discussion of how using emoji as part of a team&#8217;s communication can add nuance and shared understanding during incident management, along with a discussion of the disadvantages and costs associated with the practice.</li><li><a href=\"https://www.mikulskibartosz.name/modern-software-architecture-in-2022/\">&#8220;What does modern software architecture look like in 2022?&#8221;</a>, from Bartosz Mikulski: A nice  article which discusses how service-oriented software architecture can often include an explicit expectation of change. For example, the architecture might include notes on an ongoing deprecation of a library, or might signpost the need to factor a new microservice out when overall system load gets high enough.</li><li><a href=\"https://www.bradyheywood.com.au/podcasts/\">The Brady Heywood podcast</a>: Found via the <a href=\"https://oxide.computer/podcasts/oxide-and-friends/1137359\">Oxide and Friends podcast</a>, the Brady Heywood podcast is a series on engineering disasters and their consequences from a forensic engineering firm. It&#8217;s mostly not being updated any more (with the podcasters moving on to a separate series on complexity science), but it has a deep back catalog of good episodes, and includes thoughtful discussions of human factors, safety engineering, and how organizational pressures become manifest in engineering artifacts.</li></ul><h2>Recent recipes</h2><ul><li><a href=\"https://smittenkitchen.com/2016/12/homemade-irish-cream/\">Smitten Kitchen&#8217;s Homemade Irish Cream</a>: This is a recipe I make every year, and I often give away small bottles of it as holiday gifts. It&#8217;s really ridiculously tasty, much better than Baileys  or similar, and good either on its own or in hot chocolate.</li><li><a href=\"https://smittenkitchen.com/2014/12/fairytale-of-new-york/\">Smitten Kitchen&#8217;s Fairytale of New York</a>: This is a really tasty whiskey cocktail, and the star of the show is a &#8220;winter warmth syrup&#8221; that substitutes in for simple syrup. The syrup is simply very tasty, and turns what&#8217;s effectively an OId Fashioned variant into a lovely holiday cocktail.</li><li>Sparkling gingerbread from <a href=\"http://www.apt2bbakingco.com/snacking-cakes\">Yossy Arefi&#8217;s Snaking Cakes</a>: This recipe takes a little more prep than most of Arefi&#8217;s &#8220;snacking cakes&#8221;, as it includes ginger three ways (ground, fresh, and crystallized), but it&#8217;s worth the few minutes of extra work.</li></ul><h2>Pet photos</h2><figure class=\"wp-block-image size-large is-resized\"><img alt=\"A white calico cat and a gray tabby cat lounging on a large brown pet bed in front of a gas fireplace.\" class=\"wp-image-295\" height=\"512\" src=\"https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_7207-768x1024.jpeg\" width=\"384\" /><figcaption class=\"wp-element-caption\">I&#8217;m pretty sure these two want me to turn the fireplace on.</figcaption></figure><figure class=\"wp-block-image size-large is-resized\"><img alt=\"A gray tabby cat lounges on a dog bed, while a golden doodle lays on the floor nearby and looks forlornly at the bed.\" class=\"wp-image-294\" height=\"512\" src=\"https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_1725-1024x1024.jpeg\" width=\"512\" /><figcaption class=\"wp-element-caption\">Just Percy bullying the dog by stealing his bed.</figcaption></figure>",
            "url": "https://hpc.social/personal-blog/2022/adam-s-weekly-ish-update-2022-12-20/",
            
            
            
            
            
            "date_published": "2022-12-20T18:14:52-07:00",
            "date_modified": "2022-12-20T18:14:52-07:00",
            
                "author": "Thinking Out Loud"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/visualizing-spectrum-lsf-data-with-grafana/",
            "title": "Visualizing Spectrum LSF data with Grafana",
            "summary": null,
            "content_text": "OverviewSystem monitoring is a fundamental part of IT best practices. High performance computing (HPC) environments are no exception to this. At the high-end, HPC clusters can consist ofthousands of servers, processing millions of jobs per day. HPC admins need ways to monitor the overall cluster to determine system status and availability through to the efficiencyof workloads. Servers today produce a wide array of metrics which can be monitored for example to check for various conditions. Additionally, workload schedulers also produce a wealthof data about jobs. Having a single dashboard to show this type of detail can be of great benefit.IBM Spectrum LSF Suites provide a complete solution for HPC workload management. This includes reporting capabilities out of the box. Spectrum LSF Suite features an integrated webinterface for job management and reporting. The reporting capabilities include a number of reports out of the box, with the ability to customize and add new reports. The reportingcapability in Spectrum LSF Suite and IBM Spectrum LSF Explorer is underpinned by Elasticsearch, which is used to store, index and query data. With LSF data in Elasticsearch, it’salso possible to configure LSF command-line interface (CLI) tools to query information from Elasticsearch rather than flat files – for greater performance. This is controlled viathe LSF_QUERY_ES_FUNCTIONS parameter of Spectrum LSF. More details about the LSF_QUERY_ES_FUNCTIONS can be found in the LSF documentation here.(1) Here is a look at the indices that are created by LSF in Elasticsearch. Note that the status shows as yellow because I only have a single Elasticsearch node.# curl -XGET localhost:9200/_cat/indicesyellow open lsf_events-202205             tejh7jsMSwSeQUJzYM7cww 5 1    1137     0 808.1kb 808.1kbyellow open lsf_jobs_pendingreason-202204 4wi7Ta8uQPSXlFBqPh4kOQ 5 1   90531     0   8.6mb   8.6mbyellow open lsf_events-202204             tWYvW_w8TVyU1deRFOEoZg 5 1  116957 32691  59.1mb  59.1mbyellow open lsf_jobs_active-202212        Q0pStQxvTgaeL7R-f02XWA 5 1  210052     0  50.6mb  50.6mbyellow open lsf_jobs_pendingreason-202206 ENWIwfGrSqCHvi53aUQXJQ 5 1   44991     0   4.5mb   4.5mbyellow open host_booleanres_latest        RE8thZCgTGeMBGodeMfXEQ 5 1       5     0  23.3kb  23.3kbyellow open lsf_jobs_pendingreason-202205 yo0iZH_4TvOqq6kQgBluvA 5 1     111     0 181.4kb 181.4kbyellow open lsf_jobs_pend-202212          9ViIS3nDRFewrqtILEbKTQ 5 1     707     0 446.9kb 446.9kbyellow open lsf_hostconf_latest           9N1Y8ML4TiyaamCPEDRQog 5 1       2     0  10.6kb  10.6kbyellow open lsf_events-202209             rtKQ8F4bSleHl8EbAQez8A 5 1    8200   955   4.4mb   4.4mbyellow open lsf_events-202206             UUKPWfN7SZ-dzVs5NAkjUg 5 1   79503 23452  36.8mb  36.8mbyellow open lsf_hostmetrics-202209        7FUNFCWPQtuGyx5jTJLb1A 5 1    4701     0   2.2mb   2.2mbyellow open lsf_hostmetrics-202208        52xef_3hQWK-jVuJqyUpHA 5 1    3823     0   1.9mb   1.9mbyellow open lsf_hostmetrics-202207        IqZYhU0RQNGIFWSRH-Ym8Q 5 1    6316     0   2.9mb   2.9mbyellow open lsf_job_acct-202209           h1ZgCSB8RwCBxwIUUzDHEQ 5 1    2050   438   1.9mb   1.9mbyellow open lsf_jobs_active-202209        iBfnf07CTcS7Gb6TxwomRA 5 1    2658     0     1mb     1mbyellow open lsf_hostmetrics-202206        0PXSYBOgTA2Qa_zzaafUPg 5 1    4301     0   2.1mb   2.1mbyellow open model                         xSqB_T_VSByOzYavEcEVyQ 1 1      55     0   257kb   257kbyellow open lsf_job_acct-202206           C639GnzBSjCEVczfh5u23g 5 1   16719   353   8.9mb   8.9mbyellow open lsf_jobs_active-202204        8gN_ENkQRTSfnmxrtMcOlA 5 1   33286     0   9.8mb   9.8mbyellow open lsf_job_acct-202205           LOxmhm_8RxaCuTd7YWYbLw 5 1     274     0 439.4kb 439.4kbyellow open lsf_jobs_active-202205        61u2RlXgR_SXagmZfrmttQ 5 1    1880     0   1.1mb   1.1mbyellow open lsf_jobs_pend-202209          eTgqPp9nQOScNiwyUWXmHA 5 1       9     0 106.2kb 106.2kbyellow open lsf_job_acct-202204           dDDegS6RQSWtWN99eklexg 5 1   28902  2177  17.4mb  17.4mbyellow open lsf_jobs_active-202206        8ivkjWSNR1Sh_BxWACP0ZA 5 1   16921     0   4.6mb   4.6mbyellow open lsf_current_status            92KE3V4YSJ-RtRp_kepxYg 5 1  115450     0     9mb     9mbyellow open lsf_hostmetrics-202210        vbuK2wW3RRmXuY07tDPUNQ 5 1     785     0 942.1kb 942.1kbyellow open lsf_jobs_pend-202206          OhSwn-b0SiSj8mCW5tcNIA 5 1      22     0 244.6kb 244.6kbyellow open lsf_jobs_pend-202205          OfBtWklETYK9cRx000aNPw 5 1       1     0  12.7kb  12.7kbyellow open lsf_events-202212             WUC5KJWmS-2WIN8XCQpSuw 5 1  712399 74728   337mb   337mbyellow open lsf_jobs_pend-202204          OhUsXqohSciZTPZlTryMyA 5 1      50     0 275.3kb 275.3kbyellow open resource_attributes_latest    R9bk_WIPTU62dVg3O1LDBA 5 1       5     0  24.4kb  24.4kbyellow open lsf_jobs_pendingreason-202212 55iwDC5mRI-eRbzQLwWP6Q 5 1 3314828     0 288.7mb 288.7mbyellow open pa-lite-log                   o8-jaNoGTsSVcjJW5Ufs0w 5 1    1549     0 547.2kb 547.2kbyellow open lsf_job_acct-202212           4HXvAD02Sxq0tgp2fS2cfQ 5 1  161502     0  73.6mb  73.6mbyellow open lsf_hostmetrics-202212        Tki6OJ41R363u9Tx02N4zw 5 1    2548     0   1.7mb   1.7mbyellow open lsf_jobs_pendingreason-202209 D3TOZY2ORiK9PppGVt10Fg 5 1    2511     0 381.4kb 381.4kb(2) With the LSF data stored in Elasticsearch, the next step is to connect to the Grafana server. Here we point our browser to the Grafana server on the default port: http://lsf_manager:3000 and login to Grafana. This step assumes an account has already been setup on Grafana. Here we are using the default admin account.(3) In Grafana, navigate to Configuration -&gt; Data sources. It’s here that it will be possible to add an Elasticsearch data source(4) Next, click the Add data source button.(5 In the list of data sources, filter by name for Elasticsearch and click the Select button on the Elasticsearch entry.(6) When configuring the data source, it’s necessary to specify an index name. This is where the list of indices in Elasticsearch that we generated earlier will come in handy. For this example, we wish to display the total number of pending jobs in the Spectrum LSF cluster over time. This data is stored in the lsf_jobs_pend* indices in Elasticsearch. To configure the data source appropriately, we specify the following values:Name:\t“LSF pending jobs”URL: http://localhost:9200Index name: “lsf_jobs_pend*”Time field name: “time_stamp”Version: 7.0+Note that the URL needs to point to the Elasticsearch server. In this case, both the Elasticsearch server and Grafana server are running on the same host.Next click on the Save &amp; Test button. It should return the message Index OK. Time field name OK..Assuming that no errors were found, click on the Back button.(7) Now you should see LSF pending jobs listed as a Data Source.(8) With the data source configured, we’re now ready to configure a dashboard to display the LSF pending job information. Navigate to Create -&gt; Dashboard.(9) Click on Add an empty panel. This is used to create a new panel where the LSF pending job information will be plotted.(10) In the panel editor, specify the following options:Panel title: “LSF pending jobs”Specify the data source “LSF pending jobs” which was created previouslySpecify a suitable time range (2 days)Line width (5 points)You should immediately see in the panel editor the plot of the hourly pending jobs.  Click on the Apply button to save the changes.(11) After clicking Apply, you will be returned to the Dashboard screen. The Dashboard should now display the new LSF pending jobs panel that was created above. This Dashboard could also include panels for system metrics collected by Prometheus for example.(12) Next, click on the diskette icon in the upper right to save the Dashboard with the LSF pending jobs panel.  We’ll name it Spectrum LSF cluster status.Additional panels can be added to the Spectrum LSF cluster status based on the data logged by Spectrum LSF to Elasticsearch.That concludes the simple example of plotting Spectrum LSF cluster data from Elasticsearch in Grafana.  As mentioned, the IBM Spectrum LSF Suites integrated web interface also provides reporting capabilities, with several built-in reports provided out of the box. Below, we’ve included a screenshot of the pending job analysis report included with Spectrum LSF Suites.SummarySpectrum LSF provides many hooks and integration points enabling administrators to change things ranging from scheduling behavior and the output of query commands through to job information being logged to Elasticsearch. Spectrum LSF is highly customizable by organizations to suit specific needs and requirements. We’ve demonstrated this using Grafana to visualize data from the LSF scheduler in a simple example. Following the above example, administrators can combine existing HPC cluster system level reporting in Grafana with job information from Spectrum LSF for a better overall view and understanding of the infrastructure.",
            "content_html": "<p><strong>Overview</strong></p><p>System monitoring is a fundamental part of IT best practices. High performance computing (HPC) environments are no exception to this. At the high-end, HPC clusters can consist ofthousands of servers, processing millions of jobs per day. HPC admins need ways to monitor the overall cluster to determine system status and availability through to the efficiencyof workloads. Servers today produce a wide array of metrics which can be monitored for example to check for various conditions. Additionally, workload schedulers also produce a wealthof data about jobs. Having a single dashboard to show this type of detail can be of great benefit.</p><p><a href=\"https://www.ibm.com/products/hpc-workload-management\">IBM Spectrum LSF Suites</a> provide a complete solution for HPC workload management. This includes reporting capabilities out of the box. Spectrum LSF Suite features an integrated webinterface for job management and reporting. The reporting capabilities include a number of reports out of the box, with the ability to customize and add new reports. The reportingcapability in Spectrum LSF Suite and IBM Spectrum LSF Explorer is underpinned by Elasticsearch, which is used to store, index and query data. With LSF data in Elasticsearch, it’salso possible to configure LSF command-line interface (CLI) tools to query information from Elasticsearch rather than flat files – for greater performance. This is controlled viathe <strong>LSF_QUERY_ES_FUNCTIONS</strong> parameter of Spectrum LSF. More details about the <strong>LSF_QUERY_ES_FUNCTIONS</strong> can be found in the LSF documentation <a href=\"https://www.ibm.com/docs/en/spectrum-lsf/10.1.0?topic=lsfconf-lsf-query-es-functions\">here</a>.</p><p>(1) Here is a look at the indices that are created by LSF in Elasticsearch. Note that the status shows as yellow because I only have a single Elasticsearch node.</p><div class=\"highlight\"><pre><code class=\"language-plaintext\"># curl -XGET localhost:9200/_cat/indicesyellow open lsf_events-202205             tejh7jsMSwSeQUJzYM7cww 5 1    1137     0 808.1kb 808.1kbyellow open lsf_jobs_pendingreason-202204 4wi7Ta8uQPSXlFBqPh4kOQ 5 1   90531     0   8.6mb   8.6mbyellow open lsf_events-202204             tWYvW_w8TVyU1deRFOEoZg 5 1  116957 32691  59.1mb  59.1mbyellow open lsf_jobs_active-202212        Q0pStQxvTgaeL7R-f02XWA 5 1  210052     0  50.6mb  50.6mbyellow open lsf_jobs_pendingreason-202206 ENWIwfGrSqCHvi53aUQXJQ 5 1   44991     0   4.5mb   4.5mbyellow open host_booleanres_latest        RE8thZCgTGeMBGodeMfXEQ 5 1       5     0  23.3kb  23.3kbyellow open lsf_jobs_pendingreason-202205 yo0iZH_4TvOqq6kQgBluvA 5 1     111     0 181.4kb 181.4kbyellow open lsf_jobs_pend-202212          9ViIS3nDRFewrqtILEbKTQ 5 1     707     0 446.9kb 446.9kbyellow open lsf_hostconf_latest           9N1Y8ML4TiyaamCPEDRQog 5 1       2     0  10.6kb  10.6kbyellow open lsf_events-202209             rtKQ8F4bSleHl8EbAQez8A 5 1    8200   955   4.4mb   4.4mbyellow open lsf_events-202206             UUKPWfN7SZ-dzVs5NAkjUg 5 1   79503 23452  36.8mb  36.8mbyellow open lsf_hostmetrics-202209        7FUNFCWPQtuGyx5jTJLb1A 5 1    4701     0   2.2mb   2.2mbyellow open lsf_hostmetrics-202208        52xef_3hQWK-jVuJqyUpHA 5 1    3823     0   1.9mb   1.9mbyellow open lsf_hostmetrics-202207        IqZYhU0RQNGIFWSRH-Ym8Q 5 1    6316     0   2.9mb   2.9mbyellow open lsf_job_acct-202209           h1ZgCSB8RwCBxwIUUzDHEQ 5 1    2050   438   1.9mb   1.9mbyellow open lsf_jobs_active-202209        iBfnf07CTcS7Gb6TxwomRA 5 1    2658     0     1mb     1mbyellow open lsf_hostmetrics-202206        0PXSYBOgTA2Qa_zzaafUPg 5 1    4301     0   2.1mb   2.1mbyellow open model                         xSqB_T_VSByOzYavEcEVyQ 1 1      55     0   257kb   257kbyellow open lsf_job_acct-202206           C639GnzBSjCEVczfh5u23g 5 1   16719   353   8.9mb   8.9mbyellow open lsf_jobs_active-202204        8gN_ENkQRTSfnmxrtMcOlA 5 1   33286     0   9.8mb   9.8mbyellow open lsf_job_acct-202205           LOxmhm_8RxaCuTd7YWYbLw 5 1     274     0 439.4kb 439.4kbyellow open lsf_jobs_active-202205        61u2RlXgR_SXagmZfrmttQ 5 1    1880     0   1.1mb   1.1mbyellow open lsf_jobs_pend-202209          eTgqPp9nQOScNiwyUWXmHA 5 1       9     0 106.2kb 106.2kbyellow open lsf_job_acct-202204           dDDegS6RQSWtWN99eklexg 5 1   28902  2177  17.4mb  17.4mbyellow open lsf_jobs_active-202206        8ivkjWSNR1Sh_BxWACP0ZA 5 1   16921     0   4.6mb   4.6mbyellow open lsf_current_status            92KE3V4YSJ-RtRp_kepxYg 5 1  115450     0     9mb     9mbyellow open lsf_hostmetrics-202210        vbuK2wW3RRmXuY07tDPUNQ 5 1     785     0 942.1kb 942.1kbyellow open lsf_jobs_pend-202206          OhSwn-b0SiSj8mCW5tcNIA 5 1      22     0 244.6kb 244.6kbyellow open lsf_jobs_pend-202205          OfBtWklETYK9cRx000aNPw 5 1       1     0  12.7kb  12.7kbyellow open lsf_events-202212             WUC5KJWmS-2WIN8XCQpSuw 5 1  712399 74728   337mb   337mbyellow open lsf_jobs_pend-202204          OhUsXqohSciZTPZlTryMyA 5 1      50     0 275.3kb 275.3kbyellow open resource_attributes_latest    R9bk_WIPTU62dVg3O1LDBA 5 1       5     0  24.4kb  24.4kbyellow open lsf_jobs_pendingreason-202212 55iwDC5mRI-eRbzQLwWP6Q 5 1 3314828     0 288.7mb 288.7mbyellow open pa-lite-log                   o8-jaNoGTsSVcjJW5Ufs0w 5 1    1549     0 547.2kb 547.2kbyellow open lsf_job_acct-202212           4HXvAD02Sxq0tgp2fS2cfQ 5 1  161502     0  73.6mb  73.6mbyellow open lsf_hostmetrics-202212        Tki6OJ41R363u9Tx02N4zw 5 1    2548     0   1.7mb   1.7mbyellow open lsf_jobs_pendingreason-202209 D3TOZY2ORiK9PppGVt10Fg 5 1    2511     0 381.4kb 381.4kb</code></pre></div><p>(2) With the LSF data stored in Elasticsearch, the next step is to connect to the Grafana server. Here we point our browser to the Grafana server on the default port: <em>http://lsf_manager:3000</em> and login to Grafana. This step assumes an account has already been setup on Grafana. Here we are using the default admin account.</p><p>(3) In Grafana, navigate to <strong>Configuration</strong> -&gt; <strong>Data sources</strong>. It’s here that it will be possible to add an Elasticsearch data source</p><figure><img src=\"https://www.gaborsamu.com/images/grafana_3.png\" /></figure><p>(4) Next, click the <strong>Add data source</strong> button.</p><figure><img src=\"https://www.gaborsamu.com/images/grafana_4.png\" /></figure><p>(5 In the list of data sources, filter by name for <em>Elasticsearch</em> and click the Select button on the Elasticsearch entry.</p><figure><img src=\"https://www.gaborsamu.com/images/grafana_5.png\" /></figure><p>(6) When configuring the data source, it’s necessary to specify an index name. This is where the list of indices in Elasticsearch that we generated earlier will come in handy. For this example, we wish to display the total number of pending jobs in the Spectrum LSF cluster over time. This data is stored in the <em>lsf_jobs_pend*</em> indices in Elasticsearch. To configure the data source appropriately, we specify the following values:</p><ul><li>Name:\t“LSF pending jobs”</li><li>URL: http://localhost:9200</li><li>Index name: “lsf_jobs_pend*”</li><li>Time field name: “time_stamp”</li><li>Version: 7.0+Note that the URL needs to point to the Elasticsearch server. In this case, both the Elasticsearch server and Grafana server are running on the same host.</li></ul><p>Next click on the <strong>Save &amp; Test button</strong>. It should return the message <em>Index OK. Time field name OK.</em>.</p><p>Assuming that no errors were found, click on the <strong>Back</strong> button.</p><figure><img src=\"https://www.gaborsamu.com/images/grafana_6.png\" /></figure><p>(7) Now you should see <em>LSF pending jobs</em> listed as a Data Source.</p><figure><img src=\"https://www.gaborsamu.com/images/grafana_7.png\" /></figure><p>(8) With the data source configured, we’re now ready to configure a dashboard to display the LSF pending job information. Navigate to <strong>Create</strong> -&gt; <strong>Dashboard</strong>.</p><figure><img src=\"https://www.gaborsamu.com/images/grafana_8.png\" /></figure><p>(9) Click on <strong>Add an empty panel</strong>. This is used to create a new panel where the LSF pending job information will be plotted.</p><figure><img src=\"https://www.gaborsamu.com/images/grafana_9.png\" /></figure><p>(10) In the panel editor, specify the following options:</p><ul><li>Panel title: “LSF pending jobs”</li><li>Specify the data source “LSF pending jobs” which was created previously</li><li>Specify a suitable time range (2 days)</li><li>Line width (5 points)</li></ul><p>You should immediately see in the panel editor the plot of the hourly pending jobs.  Click on the <strong>Apply</strong> button to save the changes.</p><figure><img src=\"https://www.gaborsamu.com/images/grafana_10.png\" /></figure><p>(11) After clicking Apply, you will be returned to the Dashboard screen. The Dashboard should now display the new LSF pending jobs panel that was created above. This Dashboard could also include panels for system metrics collected by Prometheus for example.</p><figure><img src=\"https://www.gaborsamu.com/images/grafana_11.png\" /></figure><p>(12) Next, click on the diskette icon in the upper right to save the Dashboard with the LSF pending jobs panel.  We’ll name it <em>Spectrum LSF cluster status</em>.</p><figure><img src=\"https://www.gaborsamu.com/images/grafana_12.png\" /></figure><p>Additional panels can be added to the <em>Spectrum LSF cluster status</em> based on the data logged by Spectrum LSF to Elasticsearch.</p><p>That concludes the simple example of plotting Spectrum LSF cluster data from Elasticsearch in Grafana.  As mentioned, the IBM Spectrum LSF Suites integrated web interface also provides reporting capabilities, with several built-in reports provided out of the box. Below, we’ve included a screenshot of the <em>pending job analysis</em> report included with Spectrum LSF Suites.</p><figure><img src=\"https://www.gaborsamu.com/images/lsf_pending.png\" /></figure><p><strong>Summary</strong></p><p>Spectrum LSF provides many hooks and integration points enabling administrators to change things ranging from scheduling behavior and the output of query commands through to job information being logged to Elasticsearch. Spectrum LSF is highly customizable by organizations to suit specific needs and requirements. We’ve demonstrated this using Grafana to visualize data from the LSF scheduler in a simple example. Following the above example, administrators can combine existing HPC cluster system level reporting in Grafana with job information from Spectrum LSF for a better overall view and understanding of the infrastructure.</p>",
            "url": "https://hpc.social/personal-blog/2022/visualizing-spectrum-lsf-data-with-grafana/",
            
            
            
            
            
            "date_published": "2022-12-13T00:06:51-07:00",
            "date_modified": "2022-12-13T00:06:51-07:00",
            
                "author": "Ramblings of a supercomputing enthusiast."
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/adam-s-weekly-update-2022-12-04/",
            "title": "Adam’s weekly update, 2022-12-04",
            "summary": null,
            "content_text": "What&#8217;s newThis week was really intense from a work perspective. Not &#8220;bad intense&#8221;, but the kind of week where every day was spent with such a level of focus, that at 5 PM or so I found myself staring off into space and forgetting words. I think I got some good things accomplished, but my brain also felt like mush by the time the weekend came.This week I&#8217;m traveling to San Jose for work (I just checked into my hotel a little while ago!), so I fully expect this week to also be eaten by work. So I don&#8217;t promise anything terribly interesting for next week&#8217;s post&#8230;However, I did take advantage of a Sunday in San Jose to visit the Computer History Museum in Mountain View! I try to visit the museum every few years, and while a lot of the exhibits are the same, enough things change that I always get something new from the visit. Also, I&#8217;ve been doing a lot of reading about hardware development and the history thereof lately, so it was interesting to examine the museum through that new lens.I may write more about my visit later this week &#8212; it definitely sparked some thoughts &#8212; but in the mean time, here are a few photos I took while wandering around the museum.The Babbage Difference Engine, and other mechanical computers, have always fascinated me.Can&#8217;t visit the museum without visiting the Cray-1.I would have loved to have seen a CM-1 in operation, with its red LEDs showing the operation of its many single-bit CPUs.Having recently read Charles Petzold&#8217;s &#8220;Code&#8221;, I was struck by how closely the front panel of the Altair 8800 resembles the fictional front panel of the computer that Petzold constructs from logic gates up.The CHM Learning Lab now includes a back room with a couple of Dell PowerEdge R710 servers, complete with instructions for how to disassemble and reassemble them. Anyone who wants can wander in and take them apart. It was great fun watching a 5-year-old kid pulling components out of one of these&#8230; As well as feeling a little weird, as I think I&#8217;ve run these in production!What I&#8217;m readingI don&#8217;t have a ton to share this week &#8212; honestly, the whole week feels like a blur &#8212; but here are two books that I recommend.The Red Scholar&#8217;s Wake, by Aliette de Bodard: As the blurb says, &#8220;Lesbian space pirates!&#8221; Also, a really wonderful novella about building a new relationship amidst grief, power differentials, politics, and space battles. I think I basically recommend everything that de Bodard writes, but especially this. And it basically stands alone! So you can read this first, without going back to the other stories in the same world.Dealers of Lightning: XEROX PARC and the Dawn of the Computer Age, by Michael Hiltzik: I&#8217;ve just started this, but it&#8217;s already a really interesting snapshot of a key period in the development of the personal computer.Recent recipesSmitten Kitchen&#8217;s Unfussy Sugar Cookies: These cookies did, indeed, prove to be both tasty and easy to make. If you just want some easy cookies to snack on, I absolutely recommend this recipe.Pet photos",
            "content_html": "<h2>What&#8217;s new</h2><p>This week was really intense from a work perspective. Not &#8220;bad intense&#8221;, but the kind of week where every day was spent with such a level of focus, that at 5 PM or so I found myself staring off into space and forgetting words. I think I got some good things accomplished, but my brain also felt like mush by the time the weekend came.</p><p><span id=\"more-268\"></span></p><p>This week I&#8217;m traveling to San Jose for work (I just checked into my hotel a little while ago!), so I fully expect this week to also be eaten by work. So I don&#8217;t promise anything terribly interesting for next week&#8217;s post&#8230;</p><p>However, I did take advantage of a Sunday in San Jose to visit the <a href=\"https://computerhistory.org/\">Computer History Museum</a> in Mountain View! I try to visit the museum every few years, and while a lot of the exhibits are the same, enough things change that I always get something new from the visit. Also, I&#8217;ve been doing a lot of reading about hardware development and the history thereof lately, so it was interesting to examine the museum through that new lens.</p><p>I may write more about my visit later this week &#8212; it definitely sparked some thoughts &#8212; but in the mean time, here are a few photos I took while wandering around the museum.</p><figure class=\"wp-block-image size-large is-resized\"><img alt=\"A mechanical computer built mostly of brass, with various numerical dials. A small placard labels this as a replica of the Babbage Difference Engine No. 1 Demonstration Piece.\" class=\"wp-image-282\" height=\"800\" src=\"https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_6894-768x1024.jpg\" width=\"600\" /><figcaption class=\"wp-element-caption\">The Babbage Difference Engine, and other mechanical computers, have always fascinated me.</figcaption></figure><figure class=\"wp-block-image size-large is-resized\"><img alt=\"The Cray-1, a round computer with its own built-in seating attached.\" class=\"wp-image-283\" height=\"446\" src=\"https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_6965-1024x768.jpg\" width=\"595\" /><figcaption class=\"wp-element-caption\">Can&#8217;t visit the museum without visiting the Cray-1.</figcaption></figure><figure class=\"wp-block-image size-large is-resized\"><img alt=\"The Connection Machine 1, a large black cube divided in eight sections.\" class=\"wp-image-284\" height=\"768\" src=\"https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_6973-768x1024.jpg\" width=\"576\" /><figcaption class=\"wp-element-caption\">I would have loved to have seen a CM-1 in operation, with its red LEDs showing the operation of its many single-bit CPUs.</figcaption></figure><figure class=\"wp-block-image size-large is-resized\"><img alt=\"The front panel of an Altair 8800 computer, with an array of LEDs and switches controlling the state of individual bits.\" class=\"wp-image-285\" height=\"449\" src=\"https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_7037-1024x768.jpg\" width=\"598\" /><figcaption class=\"wp-element-caption\">Having recently read Charles Petzold&#8217;s &#8220;Code&#8221;, I was struck by how closely the front panel of the Altair 8800 resembles the fictional front panel of the computer that Petzold constructs from logic gates up.</figcaption></figure><figure class=\"wp-block-image size-large is-resized\"><img alt=\"A Dell PowerEdge R710 lays on a white plastic table, top cover off, surrounded by instructions on how to disassemble it.\" class=\"wp-image-286\" height=\"467\" src=\"https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_7073-1024x768.jpg\" width=\"623\" /><figcaption class=\"wp-element-caption\">The CHM Learning Lab now includes a back room with a couple of Dell PowerEdge R710 servers, complete with instructions for how to disassemble and reassemble them. Anyone who wants can wander in and take them apart. It was great fun watching a 5-year-old kid pulling components out of one of these&#8230; As well as feeling a little weird, as I think I&#8217;ve run these in production!</figcaption></figure><h2>What I&#8217;m reading</h2><p>I don&#8217;t have a ton to share this week &#8212; honestly, the whole week feels like a blur &#8212; but here are two books that I recommend.</p><ul><li><a href=\"https://www.aliettedebodard.com/bibliography/novels/the-universe-of-xuya/the-red-scholars-wake/\">The Red Scholar&#8217;s Wake, by Aliette de Bodard</a>: As the blurb says, &#8220;Lesbian space pirates!&#8221; Also, a really wonderful novella about building a new relationship amidst grief, power differentials, politics, and space battles. I think I basically recommend everything that de Bodard writes, but especially this. And it basically stands alone! So you can read this first, without going back to the other stories in the same world.</li><li><a href=\"https://www.harpercollins.com/products/dealers-of-lightning-michael-a-hiltzik?variant=40824247779362\">Dealers of Lightning: XEROX PARC and the Dawn of the Computer Age, by Michael Hiltzik</a>: I&#8217;ve just started this, but it&#8217;s already a really interesting snapshot of a key period in the development of the personal computer.</li></ul><h2>Recent recipes</h2><ul><li><a href=\"https://smittenkitchen.com/2019/12/unfussy-sugar-cookies/\">Smitten Kitchen&#8217;s Unfussy Sugar Cookies</a>: These cookies did, indeed, prove to be both tasty and easy to make. If you just want some easy cookies to snack on, I absolutely recommend this recipe.</li></ul><h2>Pet photos</h2><figure class=\"wp-block-image size-large is-resized\"><img alt=\"Phyrne the calico cat stares down into the camera from a stairway\" class=\"wp-image-279\" height=\"414\" src=\"https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_6881-768x1024.jpg\" width=\"310\" /></figure><figure class=\"wp-block-image size-large is-resized\"><img alt=\"Close-up on the face of Percy the gray tabby cat\" class=\"wp-image-280\" height=\"420\" src=\"https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_6879-768x1024.jpg\" width=\"314\" /></figure><figure class=\"wp-block-image size-large is-resized\"><img alt=\"Benny the golden doodle curled up on a dog bed\" class=\"wp-image-281\" height=\"238\" src=\"https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_6876-1024x768.jpg\" width=\"317\" /></figure>",
            "url": "https://hpc.social/personal-blog/2022/adam-s-weekly-update-2022-12-04/",
            
            
            
            
            
            "date_published": "2022-12-05T05:49:35-07:00",
            "date_modified": "2022-12-05T05:49:35-07:00",
            
                "author": "Thinking Out Loud"
            
        },
    
        {
            "id": "https://hpc.social/personal-blog/2022/an-initial-look-at-deep-learning-io-performance/",
            "title": "An Initial Look at Deep Learning IO Performance",
            "summary": null,
            "content_text": "AbstractThis blog post describes an investigation of IO behavior of TensorFlow and PyTorch during resnet50 training running on Lambda Lab’s 8x V100 GPU instances.  Both ephemeral local NVMe storage and network attached persistent storage was tested.  The local NVMe storage was fast enough to achieve a throughput rate required to hit synthetic test targets.  The network attached persistent storage may not be able to fully saturate 8 V100 GPUs during training, though can achieve nearly the same level of performance as the local storage so long as TFRecords are utilized.  Further, there are specific behaviors and bottlenecks in TensorFlow and PyTorch that can reduce training performance when using real data from ImageNet.AcknowledgementsThank you to Michael Balaban at Lambda Labs for providing access to their GPU cloud for this testing.  Thank you to Chuan Li for the creation of his TensorFlow benchmarking tools.  Thank you also to Andrej Karpathy, Toby Boyd, Yanan Cao, Sanjoy Das, Thomas Joerg, and Justin Lebar for their excellent blog posts on deep learning and XLA performance that helped inform this article.  I hope that this post will be useful for others as your work and writing was useful for me.Introduction  …just because you can formulate your problem as RL doesn’t mean you should. If you insist on using the technology without understanding how it works you are likely to fail.          Andrej Karpathy, A Recipe for Training Neural Networks, 2019That was the phrase that stuck in my head when I first started this project.   What project you may ask?  I want to understand how deep learning experiments utilize fast storage devices.  Not just any experiments either: real ones, preferably big.  That’s how I happened upon Andrej Karpathy’s blog.  He is the former Sr. Director of AI at Tesla and knows a thing or two about training big neural networks.  I’ve spent the last decade working on Ceph and have worked on distributed systems and distributed storage for nearly 2 decades at this point.  But training neural nets?  The closest I’ve come was back in the early 2000s when I tried to build a tool to predict video game framerates.  I scraped benchmark numbers from review websites and built M5 decision trees based on hardware and video card settings.  It sort of worked, but was terribly overtrained on a small (~4000 sample) dataset.  Training with petabytes of data to teach an AI how to responsibly drive a car?  I can already feel a bit of imposter syndrome setting in.Thankfully my goal is comparatively modest.  I don’t need to build a cutting edge classifier or explore the intricacies of manually implementing back-propagation.  I simply want to understand the IO patterns that are involved when training big datasets with fast GPUs so I can help researchers speed up their work.  Up until now, my ability to do this was fairly limited.  At the day job I’ve had access to a small group of nodes with extremely modest GPUs.  I set up runs with MLPerf but the datasets (WMT G-E and CoCo) easily fit into memory. Other than a short burst of read traffic at the very beginning of training there was very little IO.  Recently I had the opportunity to meet Michael Balaban, Co-Founder of Lambda Labs.  I told him what I wanted to do and he gave me access to Lambda’s GPU cloud and beta persistent storage to give it a try.  I was able to grab one of Lambda’s 8x Tesla V100 instances (These things are incredibly popular so it’s best to grab one early in the morning!).  Not all of Lambda’s instance types currently have access to the persistent storage but the V100 instances in the Texas zone do.  Once secured, I got to work.TensorFlow - SyntheticBefore even attempting to run tests with real data, I realized I needed a baseline to start with.  Luckily, Chuan Li, Lambda’s Chief Scientific Officer, wrote a tool for running TensorFlow benchmarks and made it available on github here. One of the advantages of Lambda’s cloud is that they’ve already bundled up many popular tools for running deep-learning workloads into one package called Lambda Stack which comes pre-installed when you start an instance.  This made it fast to get started, though I did run into one issue.  Lambda Stack comes standard with TensorFlow 2, but Chuan Li’s tool relies on a TensorFlow benchmark submodule that is designed to work with TensorFlow 1.  Luckily, the parent repository was unofficially updated to work with Tensorflow 2 (with a warning that it is no longer being maintained).  A quick “git checkout master” in the “benchmarks” submodule directory got everything working.  Chuan Li’s tool makes it simple to run tests with several preconfigured templates already included.  I chose the fp16 resnet50 configuration as it should be fast at processing images and is fairly standard.TF_XLA_FLAGS=--tf_xla_auto_jit=2 ./batch_benchmark.sh X X 1 100 2 config/config_resnet50_replicated_fp16_train_synUsing the invocation provided in the benchmark README.md file, I was able to quickly run benchmarks with synthetic data on up to 8 V100 GPUs in the node.  At one point I got stuck, hitting what appeared at first to be an unexplainable 25% performance loss. I reran the tests multiple times and even monitored GPU clockspeeds/temperatures in nvidia-smi with no luck.  Ultimately I discovered my error.  In the slow cases, I had inadvertently left out the “TF_XLA_FLAGS=–tf_xla_auto_jit=2” environment variable.  It turns out that setting this allows Tensorflow compile and execute functions with XLA (Accelerated Linear Algebra) support which is a pretty big win for these tests.At this point I decided that I needed to understand how Chuan Li’s tool works.  It turns out that he is using the same base tf_cnn_benchmarks.py benchmark code that companies like Nvidia and Dell also use for benchmarking their GPU solutions.  I spent some time running it directly with Dell’s settings from their deep learning overview here.  Unfortunately those tests had mixed results, even after various tweaks.  While researching the XLA issues I mentioned earlier however, I made an even better discovery on the TensorFlow website.  I found an excellent blog post with performance data written by some of the core Tensorflow developers.  It’s now 4 years old, but still appears to be quite valid.  The tuning options used were both simpler and resulted in higher performance versus other configurations that I’ve come across.Training with synthetic data in Lambda’s cloud resulted in similar performance to what the Tensorflow developer’s reported.  In fact, using their own settings yielded slightly faster results when running on Lambda’s 8xV100 instance!  It was incredibly encouraging to me that even in Lambda’s cloud environment with virtual machine instances I could achieve performance that was as fast or faster than what the Tensorflow developers were reporting.Choosing a Real Data Set  The first step to training a neural net is to not touch any neural net code at all and instead begin by thoroughly inspecting your data.          Andrej Karpathy, A Recipe for Training Neural Networks, 2019Having convinced myself that I had Tensorflow operating reasonably efficiently in synthetic tests, it was time to start thinking about what dataset to use for “real” training.  The largest and most obvious choice is ImageNet.  ImageNet is composed of over 1.2 million categorized images that form a roughly 160GB training dataset.  It is also the largest dataset I could find that was publicly accessible. Downloading it isn’t so easy however. The only version that I could access is the ImageNet Object Localization Challenge dataset hosted on kaggle.After finally figuring out how to download the data, it was time to follow Andrej’s advice and try to learn something about it.  While ImageNet is curated and annotated, it has many images of different sizes, dimensions, and pixel counts.  Images also come from many sources with different levels of quality.  Through the power of stack-exchange I was able to find a bash one-liner script to generate a histogram of image sizes:find . -type f -print0 | xargs -0 ls -l | awk '{size[int(log($5)/log(2))]++}END{for (i in size) printf(\"%10d %3d\\n\", 2^i, size[i])}' | sort -nRoughly 80% of the images are in the 64KB or 128KB size bins. Almost all of the remaining images are smaller.  That gives us a pretty good idea of what kind of IOs to expect during classification.  Or at least…it does for frameworks that read those images directly.  In Tensorflow’s case, there’s an alternative format called TFRecord.  TFRecords are basically collections of image data sequentially laid out in much larger files.  Instead of iterating over thousands or millions of individual image files, TFRecords allow Tensorflow to instead stream fewer, larger files that each house multiple images.  It’s a one time cost to pre-process the data so Tensorflow has less work to do during training.  After I downloaded the ImageNet data I took a shot at converting the ImageNet LOC data into TensorFlow records.  Luckily, the TensorFlow tpu github repository already has a tool that can do this.  I had to manipulate the dataset slightly, but ultimately this process worked (at least for the training data):pip install gcloud google-cloud-storagepip install protobuf==3.20.1mkdir ~/data/ImageNetFooln -s ~/data/ImageNet/ILSVRC/Data/CLS-LOC/train ~/data/ImageNetFoo/trainln -s ~/data/ImageNet/ILSVRC/Data/CLS-LOC/val ~/data/ImageNetFoo/valln -s ~/data/ImageNet/ILSVRC/Data/CLS-LOC/test ~/data/ImageNetFoo/testln -s ~/data/ImageNet/LOC_synset_mapping.txt ~/data/ImageNetFoo/synset_labels.txtpython imagenet_to_gcs.py --raw_data_dir=/home/ubuntu/data/ImageNetFoo --local_scratch_dir=/home/ubuntu/ExaltedOrbs/ImageNet/tf_records --nogcs_uploadPerhaps I should say that this worked so long as the original dataset was located on the local NVMe drive.  The persistent storage didn’t fare as well.  Attempting to decompress ImageNet on the persistent storage resulted in blowing past the max number of open files allowed with errors like:OSError: [Errno 24] Too many open files.Unfortunately this couldn’t be fixed on the instance.  It appeared to be passed through from the host and the persistent storage was completely unusable until the instance was rebooted.  Recently I spoke to one of Lambda’s engineers and they are working on a fix. (It may already be implemented by the time you read this!)  I also want to note that the persistent storage is still in beta so issues like this are not entirely unexpected.  Having said that, before hitting the error it was significantly slower extracting ImageNet on the persistent storage vs on the local NVMe storage.  It’s probably best to extract ImageNet locally and then write the large TFRecords to the persistent storage during the conversion process.  Luckily extracting ImageNet to local storage was fine, and storing the original archive and the resulting TFRecords on the persistent storage worked perfectly fine as well.FIO - Baseline IO ResultsNext, I turned my attention to running baseline tests on Lambda’s local and persistent storage using fio.  Fio is a highly configurable and well respected benchmark in the storage community and perfect for generating baseline results.  I decided to use a dataset size that is roughly similar to ImageNet (200GB), the libaio engine in fio with direct IO, and an appropriately high IO depth to let the NVMe drives stretch their legs a bit.Throughput with the local NVMe drive(s) is surprisingly good.  The persistent storage is slower, but still might be fast enough at a little over 1GB/s for large reads.  16K IOPS was somewhat slower in both cases.  I chose 16K so that I could quickly compare to tests I ran in my Ceph QEMU/KVM performance blog post here.  Without getting into the details, I suspect there’s still some room for improved IOPS with Lambda’s setup.  Luckily though, converting into TFRecords should make Tensorflow throughput bound instead of latency bound.  What about PyTorch or other tools that want to read images directly though?  Fio gives us the ability to simulate it by using its ‘bssplit’ feature.  We can take the size ranges and percentiles generated when examining ImageNet and give fio a similar distribution:fio --ioengine=libaio --direct=1 --bssplit=2K/1:4K/2:8K/4:16K/8:32K/13:64K/38:128K/33:256K/1 --iodepth=128 --rw=randread --norandommap --size=200G --numjobs=1 --runtime=300 --time_based --name=fooThis isn’t exactly right as we are not reading data spread across millions of files, but it should provide something of an upper bound on what to expect.  It looks like the persistent storage can do approximately 10K reads/second at a throughput rate of around 750MB/s.  The local storage is about 3-4 times faster.  Local storage should be fast enough to support the kind of images/second throughput rates we want to hit in Tensorflow on 8 V100 GPUs, but the jury is still out for the persistent storage.Tensorflow - ImageNetRunning benchmarks with real data rather than synthetic data is fairly straightforward in Tensorflow.  You simply append data_dir and data_name flags to the CLI invocation to let it know where the TFRecords are located:sync; echo 3 | sudo tee /proc/sys/vm/drop_cachespython ./tf_cnn_benchmarks.py --batch_size=256 --num_batches=100 --model=resnet50 --optimizer=momentum --variable_update=replicated --all_reduce_spec=nccl --use_fp16=True --nodistortions --gradient_repacking=2 --compute_lr_on_cpu=True --single_l2_loss_op=True --xla_compile=True --num_gpus=8 --loss_type_to_report=base_loss --data_dir=/home/ubuntu/ImageNet-TF/train --data_name=imagenetOuch.  Much lower performance with the ImageNet data vs synthetic!  This is especially unfortunate given that 4 years ago the Tensorflow developers reported much better results.  I spent some time reading and experimenting with different settings.  Ultimately the one setting that made a substantial difference was “datasets_num_private_threads”.  In the Tensorflow benchmark source code, this setting is described as: “[The] number of threads for a private threadpool created for all datasets computation.”  I’ll go into more detail what these threads are doing in a bit. For now, let’s see how increasing the number of threads affects the results:Increasing the number of private threads has a dramatic effect on performance, though I was unable to fully match the performance achieved in the synthetic tests on either the local or persistent storage.  The local storage fared better at high thread counts gradually topping out at around 8600 images/second.  At high private thread counts the persistent storage topped out between 7000-8000 images/second with a higher degree of variability between runs.  I suspect that in this case the persistent storage has likely hit its (per instance) limit.In addition to having a dramatic effect on performance, changing the private thread count also had a large effect on the CPU consumption of the TensorFlow process.  CPU usage increases almost linearly with additional private threads up to around 30 cores.  What exactly are these private threads doing?  To answer that question, I utilized two tools that I often deploy when diagnosing CPU usage in Ceph.  When testing with a lower number of private threads, I used linux’s perf tool to look at where cycles are being consumed when the private threads are fully saturated.  At higher levels of private threads, I used my wallclock profiler uwpmp to look at how private threads spend their time when increasing the thread count no longer improves performance.In the first case with perf, we can get a good view of the work that these private threads are doing:--77.31%--tensorflow::ThreadPoolDevice::Compute          |                    |--51.19%--0x7f511a00c7d8          |          |                    |           --51.18%--tensorflow::jpeg::Uncompress          |--14.48%--tensorflow::ResizeBilinearOp&lt;Eigen::ThreadPoolDevice, unsigned char&gt;::Compute          |--5.47%--tensorflow::CastOpBase::Compute          |--2.66%--tensorflow::ReverseV2Op&lt;Eigen::ThreadPoolDevice, unsigned char, int&gt;::ComputeThe majority of the cycles consumed is in jpeg decompression and resize operations, along with a smattering of other stuff.  What happens if we look at a case with a higher private thread count but now look at wallclock time instead of cycles?  I ended up having some trouble getting the profiler to work properly and consistently get clean callgraphs, but I was able to get at least one run in that revealed some interesting information.  First, I saw time spent in the same functions that perf told us we were spending cycles in:+ 100.00% Eigen::ThreadPoolTempl&lt;tensorflow::thread::EigenEnvironment&gt;::WorkerLoop(int) + 99.90% ??? |+ 97.30% ??? ||+ 92.40% ??? |||+ 77.10% _PyEval_EvalFrameDefault ||||+ 47.20% ??? |||||+ 38.10% tensorflow::jpeg::Uncompress(void const*, int, tensorflow::jpeg::UncompressFlags const&amp;, long*, std::function&lt;unsigned char* (int, int, int)&gt;) ||||+ 12.20% tensorflow::ResizeBilinearOp&lt;Eigen::ThreadPoolDevice, unsigned char&gt;::Compute(tensorflow::OpKernelContext*) ||||+ 4.40% tensorflow::CastOpBase::Compute(tensorflow::OpKernelContext*) ||||+ 1.70% tensorflow::ReverseV2Op&lt;Eigen::ThreadPoolDevice, unsigned char, int&gt;::Compute(tensorflow::OpKernelContext*)But the wallclock profile also exposed that there may be contention in multiple areas in the private threads around some of the nsync synchronization primitives being used: |||||||    |  + 4.50% nsync::nsync_mu_semaphore_p(nsync::nsync_semaphore_s_*) |||||||    |   + 4.50% syscallThis almost always appeared nested deep inside:tensorflow::BFCAllocator::AllocateRaw(unsigned long, unsigned long, tensorflow::AllocationAttributes const&amp;)Sadly I was missing a number of debug symbols and don’t 100% trust the wallclock trace.  For now I’ll just say that the private threads are doing a significant amount of work decompressing and manipulating the image data to keep the GPUs fed.  I suspect that with newer and faster GPUs the image retrieval pipeline could become an even bigger issue when training with real image data.  The mystery for me is how The TensorFlow developers achieved such good results 4 years ago without using dedicated private threads at all.  Perhaps they had a significantly faster jpeg decompression mechanism that I am unaware of?PyTorch - ImageNetAfter running Tensorflow, I also ran some benchmarks in PyTorch using Nvidia’s “DeepLearningExamples” github repo.  First, I installed the prereqs and setup the repository:pip install 'git+https://github.com/NVIDIA/dllogger'pip install --extra-index-url https://developer.download.nvidia.com/compute/redist --upgrade nvidia-dali-cuda110git clone https://github.com/NVIDIA/DeepLearningExamplesThen, prepared ImageNet for usage in PyTorch:cd ~/data/ImageNet/ILSVRC/Data/CLS-LOC/valwget -qO- https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh | bashAnd finally ran a test:cd DeepLearningExamples/PyTorch/Classification/ConvNetssync; echo 3 | sudo tee /proc/sys/vm/drop_cachespython ./multiproc.py --nproc_per_node 1 ./main.py --arch resnet50 --label-smoothing 0.1 --run-epoch 1 --amp --static-loss-scale 256 --workspace /home/ubuntu/data/ImageNet-Scratch /home/ubuntu/data/ImageNet-Orig/ILSVRC/Data/CLS-LOC/There are a couple of differences here versus the TensorFlow tests.  First, I’m using the raw ImageNet archive instead of a preprocessed TFRecord dataset, so the read behavior is different.  Because I was unable to extract or copy the raw ImageNet archive onto the persistent storage, I’m also only testing the local NVMe drive.  Finally, I didn’t see any specific examples for running with fp16 in nVidia’s documentation, so I’m using amp (automatic mixed precision) which may be slightly slower.Given the number of differences it’s tough to draw direct comparisons with Tensorflow.  Amp is one difference, but it’s quite possible that there are tuning options that could improve performance here that I don’t know about.  I did notice that PyTorch, like Tensorflow, is using quite a bit of CPU to keep the GPUs working.  I suspect that there are ways to tweak the IO pipeline that could improve performance.  For now though, let’s compare the IO patterns on the local NVMe drive during the Tensorflow and PyTorch runs.  I was hoping to be able to use blktrace to do this, but unfortunately was unable to get any data from the virtual devices in the instance.  I was able to collect more general statistics using collectl however.Disk Read Statistics During PyTorch 8 GPU run:            Time      Name      KBytes      Merged      IOs      Size      Wait      QLen      SvcTim                  00:29:18      vda      761136      0      6746      113      58      431      0              00:29:19      vda      752172      0      6648      113      112      810      0              00:29:20      vda      747824      0      6595      113      84      604      0              00:29:21      vda      735964      0      6583      112      73      551      0              00:29:22      vda      695636      0      6237      112      102      760      0      Disk Read Statistics During TensorFlow 8 GPU run:            Time      Name      KBytes      Merged      IOs      Size      Wait      QLen      SvcTim                  00:38:45      vda      1081324      0      8440      128      0      7      0              00:38:46      vda      927512      0      7241      128      0      7      0              00:38:47      vda      913512      0      7130      128      0      7      0              00:38:48      vda      1047444      0      8186      128      0      6      0              00:38:49      vda      968776      0      7560      128      0      6      0      When just looking at the IO sizes, both runs appear similar, but that doesn’t tell the whole story.  It is likely that Tensorflow is doing much larger reads that are broken up into contiguous 128KB chunks by the block layer based on the underlying device’s max_sectors_kb setting.  The tells here are the very low queue length and wait times for the TensorFlow run versus the PyTorch run.  In both case the device service times are low (0), but in the TensorFlow case IOs are still backing up in the device queue.Interestingly, it appears that it may be possible to use nVidia’s DALI (Data Loading Library) package to read TFRecords into PyTorch.  I didn’t have time to attempt it, but potentially that could have a big effect on IO behavior and performance as well.ConclusionAs I’ve been writing this post, I realize just how complicated it is to understand the performance characteristics of training of neural networks.  Even as we talk about metrics like images/second, the options that are used (batch size for instance) can also affect convergence.  It’s very difficult to come up with a common methodology that is always better than others.  I wonder if another metric, like reaching a desired level of convergence, would be better in the end.  Having said that, I am glad for having done this exercise as I learned some valuable things:      Pre-processing data into a format like TFRecords on fast local storage is a big win from an IO perspective.  It lets storage systems that have slow metadata performance succeed so long as they have enough sequential read throughput to keep the machine learning framework busy.  This is a big win for many distributed file systems that may have substandard metadata performance (and even the good ones may still benefit).        To train on a dataset like ImageNet, you need somewhere around 1-1.3GB/s of raw disk throughput to keep 8 V100 GPUs busy when training in fp16.  For amp or fp32 the requirements are likely lower since the GPUs can’t work quite as fast.  With modern GPUs that are faster than the V100, the disk throughput requirements could be significantly higher.        Lambda’s local NVMe storage is likely fast enough to saturate 8 GPUs, even newer ones, so long as the rest of the IO path can keep up.  The persistent storage appears to become a bottleneck with sufficient GPUs and TensorFlow private threads, though can still function fairly well so long as TFRecords are used.  A concern going forward is how to ensure that the data pipeline in TensorFlow and PyTorch are fast enough to keep the GPUs fed.  The Tensorflow benchmark required a large number of private threads and showed potential evidence of contention at high thread counts.  PyTorch did not appear to natively support TFRecords, but NVidia DALI or other 3rd party code might help improve the IO path.        If it’s necessary to train directly with images rather than TFRecords, it may not make sense to host them on shared file systems.  It appears that Tensorflow and possibly PyTorch give users the ability to specify a separate training data and work directory.  If all operations against the training data are reads, it may be better to host datasets on read-only block device snapshots. For instance with Ceph, perhaps you could create a read/write RBD volume where you put a certain dataset, take a snapshot, and then map that snapshot as read only on multiple instances that all need access to the same image set.        Even with a training set as large as ImageNet, Lambda’s instances have so much memory that eventually the entire dataset becomes cached.  It was necessary to sync and drop caches before each test and keep tests short enough that they didn’t re-read the same data from buffer cache.  I was able to watch as long running tests eventually stopped performing reads and got faster as time went on.  This could make apples-to-apples comparison between different storage vendors difficult if not carefully controlled.        I’m almost certainly missing additional tweaks that can help speed up both Tensorflow and PyTorch.  This post shouldn’t be seen as the be-all/end-all for how to achieve high performance with these frameworks, but I hope it may at least help showcase some of the areas that are valuable to investigate when trying to train with real data and achieve high performance.  This wraps up my initial work looking at Deep Learning IO behavior.  I hope that next time I can come armed with a bit more knowledge about the internals of how PyTorch and Tensorflow work, focus a bit more on the quality of the training, find even larger datasets to work with, and maybe actually accomplish something useful rather than just play with ImageNet.Thanks for reading!",
            "content_html": "<h2 id=\"abstract\">Abstract</h2><p>This blog post describes an investigation of IO behavior of TensorFlow and PyTorch during resnet50 training running on Lambda Lab’s 8x V100 GPU instances.  Both ephemeral local NVMe storage and network attached persistent storage was tested.  The local NVMe storage was fast enough to achieve a throughput rate required to hit synthetic test targets.  The network attached persistent storage may not be able to fully saturate 8 V100 GPUs during training, though can achieve nearly the same level of performance as the local storage so long as TFRecords are utilized.  Further, there are specific behaviors and bottlenecks in TensorFlow and PyTorch that can reduce training performance when using real data from ImageNet.</p><h2 id=\"acknowledgements\">Acknowledgements</h2><p>Thank you to Michael Balaban at Lambda Labs for providing access to their GPU cloud for this testing.  Thank you to Chuan Li for the creation of his TensorFlow benchmarking tools.  Thank you also to Andrej Karpathy, Toby Boyd, Yanan Cao, Sanjoy Das, Thomas Joerg, and Justin Lebar for their excellent blog posts on deep learning and XLA performance that helped inform this article.  I hope that this post will be useful for others as your work and writing was useful for me.</p><h2 id=\"introduction\">Introduction</h2><blockquote>  <p><em>…just because you can formulate your problem as RL doesn’t mean you should. If you insist on using the technology without understanding how it works you are likely to fail.</em></p>  <p>        Andrej Karpathy, <a href=\"https://karpathy.github.io/2019/04/25/recipe/\">A Recipe for Training Neural Networks</a>, 2019</p></blockquote><p>That was the phrase that stuck in my head when I first started this project.   What project you may ask?  I want to understand how deep learning experiments utilize fast storage devices.  Not just any experiments either: <em>real</em> ones, preferably big.  That’s how I happened upon Andrej Karpathy’s blog.  He is the former Sr. Director of AI at Tesla and knows a thing or two about training big neural networks.  I’ve spent the last decade working on Ceph and have worked on distributed systems and distributed storage for nearly 2 decades at this point.  But training neural nets?  The closest I’ve come was back in the early 2000s when I tried to build a tool to predict video game framerates.  I scraped benchmark numbers from review websites and built M5 decision trees based on hardware and video card settings.  It sort of worked, but was terribly overtrained on a small (~4000 sample) dataset.  Training with petabytes of data to teach an AI how to responsibly drive a car?  I can already feel a bit of imposter syndrome setting in.</p><p>Thankfully my goal is comparatively modest.  I don’t need to build a cutting edge classifier or explore the intricacies of manually implementing back-propagation.  I simply want to understand the IO patterns that are involved when training big datasets with fast GPUs so I can help researchers speed up their work.  Up until now, my ability to do this was fairly limited.  At the day job I’ve had access to a small group of nodes with extremely modest GPUs.  I set up runs with MLPerf but the datasets (WMT G-E and CoCo) easily fit into memory. Other than a short burst of read traffic at the very beginning of training there was very little IO.  Recently I had the opportunity to meet Michael Balaban, Co-Founder of <a href=\"https://lambdalabs.com/\">Lambda Labs</a>.  I told him what I wanted to do and he gave me access to Lambda’s GPU cloud and beta persistent storage to give it a try.  I was able to grab one of Lambda’s 8x Tesla V100 instances (These things are incredibly popular so it’s best to grab one early in the morning!).  Not all of Lambda’s instance types currently have access to the persistent storage but the V100 instances in the Texas zone do.  Once secured, I got to work.</p><h2 id=\"tensorflow---synthetic\">TensorFlow - Synthetic</h2><p>Before even attempting to run tests with real data, I realized I needed a baseline to start with.  Luckily, Chuan Li, Lambda’s Chief Scientific Officer, wrote a tool for running TensorFlow benchmarks and made it available on github <a href=\"https://github.com/lambdal/lambda-tensorflow-benchmark\">here</a>. One of the advantages of Lambda’s cloud is that they’ve already bundled up many popular tools for running deep-learning workloads into one package called <a href=\"https://lambdalabs.com/lambda-stack-deep-learning-software\">Lambda Stack</a> which comes pre-installed when you start an instance.  This made it fast to get started, though I did run into one issue.  Lambda Stack comes standard with TensorFlow 2, but Chuan Li’s tool relies on a TensorFlow benchmark submodule that is designed to work with TensorFlow 1.  Luckily, the parent repository was unofficially updated to work with Tensorflow 2 (with a warning that it is no longer being maintained).  A quick “git checkout master” in the “benchmarks” submodule directory got everything working.  Chuan Li’s tool makes it simple to run tests with several preconfigured templates already included.  I chose the fp16 resnet50 configuration as it should be fast at processing images and is fairly standard.</p><pre><code>TF_XLA_FLAGS=--tf_xla_auto_jit=2 ./batch_benchmark.sh X X 1 100 2 config/config_resnet50_replicated_fp16_train_syn</code></pre><p>Using the invocation provided in the benchmark README.md file, I was able to quickly run benchmarks with synthetic data on up to 8 V100 GPUs in the node.  At one point I got stuck, hitting what appeared at first to be an unexplainable 25% performance loss. I reran the tests multiple times and even monitored GPU clockspeeds/temperatures in nvidia-smi with no luck.  Ultimately I discovered my error.  In the slow cases, I had inadvertently left out the “TF_XLA_FLAGS=–tf_xla_auto_jit=2” environment variable.  It turns out that setting this allows Tensorflow compile and execute functions with XLA (Accelerated Linear Algebra) support which is a pretty big win for these tests.</p><p><img alt=\"\" src=\"https://markhpc.github.io/images/2022-11-28-Lambda/Tensorflow_-_ResNet50_Synthetic_Training_fp16.svg\" /></p><p>At this point I decided that I needed to understand how Chuan Li’s tool works.  It turns out that he is using the same base tf_cnn_benchmarks.py benchmark code that companies like Nvidia and Dell also use for benchmarking their GPU solutions.  I spent some time running it directly with Dell’s settings from their deep learning overview <a href=\"https://infohub.delltechnologies.com/l/high-speed-object-storage-for-deep-learning/overview-3284\">here</a>.  Unfortunately those tests had mixed results, even after various tweaks.  While researching the XLA issues I mentioned earlier however, I made an even better <a href=\"https://blog.tensorflow.org/2018/11/pushing-limits-of-gpu-performance-with-xla.html\">discovery</a> on the TensorFlow website.  I found an excellent blog post with performance data written by some of the core Tensorflow developers.  It’s now 4 years old, but still appears to be quite valid.  The tuning options used were both simpler and resulted in higher performance versus other configurations that I’ve come across.</p><p><img alt=\"\" src=\"https://markhpc.github.io/images/2022-11-28-Lambda/Tensorflow_-_ResNet50_Synthetic_Training_fp16_blog_compare.svg\" /></p><p>Training with synthetic data in Lambda’s cloud resulted in similar performance to what the Tensorflow developer’s reported.  In fact, using their own settings yielded slightly faster results when running on Lambda’s 8xV100 instance!  It was incredibly encouraging to me that even in Lambda’s cloud environment with virtual machine instances I could achieve performance that was as fast or faster than what the Tensorflow developers were reporting.</p><h1 id=\"choosing-a-real-data-set\">Choosing a Real Data Set</h1><blockquote>  <p><em>The first step to training a neural net is to not touch any neural net code at all and instead begin by thoroughly inspecting your data.</em></p>  <p>        Andrej Karpathy, <a href=\"https://karpathy.github.io/2019/04/25/recipe/\">A Recipe for Training Neural Networks</a>, 2019</p></blockquote><p>Having convinced myself that I had Tensorflow operating reasonably efficiently in synthetic tests, it was time to start thinking about what dataset to use for “real” training.  The largest and most obvious choice is ImageNet.  ImageNet is composed of over 1.2 million categorized images that form a roughly 160GB training dataset.  It is also the largest dataset I could find that was publicly accessible. Downloading it isn’t so easy however. The only version that I could access is the ImageNet Object Localization Challenge dataset hosted on <a href=\"https://www.kaggle.com/c/imagenet-object-localization-challenge\">kaggle</a>.</p><p>After finally figuring out how to download the data, it was time to follow Andrej’s advice and try to learn something about it.  While ImageNet is curated and annotated, it has many images of different sizes, dimensions, and pixel counts.  Images also come from many sources with different levels of quality.  Through the power of stack-exchange I was able to find a bash one-liner script to generate a histogram of image sizes:</p><pre><code>find . -type f -print0 | xargs -0 ls -l | awk '{size[int(log($5)/log(2))]++}END{for (i in size) printf(\"%10d %3d\\n\", 2^i, size[i])}' | sort -n</code></pre><p><img alt=\"\" src=\"https://markhpc.github.io/images/2022-11-28-Lambda/ImageNet_-_Image_Distribution_by_Approximate_Size.svg\" /></p><p>Roughly 80% of the images are in the 64KB or 128KB size bins. Almost all of the remaining images are smaller.  That gives us a pretty good idea of what kind of IOs to expect during classification.  Or at least…it does for frameworks that read those images directly.  In Tensorflow’s case, there’s an alternative format called TFRecord.  TFRecords are basically collections of image data sequentially laid out in much larger files.  Instead of iterating over thousands or millions of individual image files, TFRecords allow Tensorflow to instead stream fewer, larger files that each house multiple images.  It’s a one time cost to pre-process the data so Tensorflow has less work to do during training.  After I downloaded the ImageNet data I took a shot at converting the ImageNet LOC data into TensorFlow records.  Luckily, the TensorFlow tpu github repository already has a <a href=\"https://github.com/tensorflow/tpu/blob/master/tools/datasets/README.md\">tool</a> that can do this.  I had to manipulate the dataset slightly, but ultimately this process worked (at least for the training data):</p><pre><code>pip install gcloud google-cloud-storagepip install protobuf==3.20.1mkdir ~/data/ImageNetFooln -s ~/data/ImageNet/ILSVRC/Data/CLS-LOC/train ~/data/ImageNetFoo/trainln -s ~/data/ImageNet/ILSVRC/Data/CLS-LOC/val ~/data/ImageNetFoo/valln -s ~/data/ImageNet/ILSVRC/Data/CLS-LOC/test ~/data/ImageNetFoo/testln -s ~/data/ImageNet/LOC_synset_mapping.txt ~/data/ImageNetFoo/synset_labels.txtpython imagenet_to_gcs.py --raw_data_dir=/home/ubuntu/data/ImageNetFoo --local_scratch_dir=/home/ubuntu/ExaltedOrbs/ImageNet/tf_records --nogcs_upload</code></pre><p>Perhaps I should say that this worked so long as the original dataset was located on the local NVMe drive.  The persistent storage didn’t fare as well.  Attempting to decompress ImageNet on the persistent storage resulted in blowing past the max number of open files allowed with errors like:</p><pre><code>OSError: [Errno 24] Too many open files.</code></pre><p>Unfortunately this couldn’t be fixed on the instance.  It appeared to be passed through from the host and the persistent storage was completely unusable until the instance was rebooted.  Recently I spoke to one of Lambda’s engineers and they are working on a fix. (It may already be implemented by the time you read this!)  I also want to note that the persistent storage is still in beta so issues like this are not entirely unexpected.  Having said that, before hitting the error it was significantly slower extracting ImageNet on the persistent storage vs on the local NVMe storage.  It’s probably best to extract ImageNet locally and then write the large TFRecords to the persistent storage during the conversion process.  Luckily extracting ImageNet to local storage was fine, and storing the original archive and the resulting TFRecords on the persistent storage worked perfectly fine as well.</p><h2 id=\"fio---baseline-io-results\">FIO - Baseline IO Results</h2><p>Next, I turned my attention to running baseline tests on Lambda’s local and persistent storage using fio.  Fio is a highly configurable and well respected benchmark in the storage community and perfect for generating baseline results.  I decided to use a dataset size that is roughly similar to ImageNet (200GB), the libaio engine in fio with direct IO, and an appropriately high IO depth to let the NVMe drives stretch their legs a bit.</p><p><img alt=\"\" src=\"https://markhpc.github.io/images/2022-11-28-Lambda/Lambda_Labs_8xv100_Storage.svg\" /></p><p>Throughput with the local NVMe drive(s) is surprisingly good.  The persistent storage is slower, but still might be fast enough at a little over 1GB/s for large reads.  16K IOPS was somewhat slower in both cases.  I chose 16K so that I could quickly compare to tests I ran in my Ceph QEMU/KVM performance blog post <a href=\"https://ceph.io/en/news/blog/2022/qemu-kvm-tuning/\">here</a>.  Without getting into the details, I suspect there’s still some room for improved IOPS with Lambda’s setup.  Luckily though, converting into TFRecords should make Tensorflow throughput bound instead of latency bound.  What about PyTorch or other tools that want to read images directly though?  Fio gives us the ability to simulate it by using its ‘bssplit’ feature.  We can take the size ranges and percentiles generated when examining ImageNet and give fio a similar distribution:</p><pre><code>fio --ioengine=libaio --direct=1 --bssplit=2K/1:4K/2:8K/4:16K/8:32K/13:64K/38:128K/33:256K/1 --iodepth=128 --rw=randread --norandommap --size=200G --numjobs=1 --runtime=300 --time_based --name=foo</code></pre><p><img alt=\"\" src=\"https://markhpc.github.io/images/2022-11-28-Lambda/Lambda_Labs_8xV100_Storage_Reads_Second_Bssplit.svg\" /></p><p>This isn’t exactly right as we are not reading data spread across millions of files, but it should provide something of an upper bound on what to expect.  It looks like the persistent storage can do approximately 10K reads/second at a throughput rate of around 750MB/s.  The local storage is about 3-4 times faster.  Local storage should be fast enough to support the kind of images/second throughput rates we want to hit in Tensorflow on 8 V100 GPUs, but the jury is still out for the persistent storage.</p><h2 id=\"tensorflow---imagenet\">Tensorflow - ImageNet</h2><p>Running benchmarks with real data rather than synthetic data is fairly straightforward in Tensorflow.  You simply append data_dir and data_name flags to the CLI invocation to let it know where the TFRecords are located:</p><pre><code>sync; echo 3 | sudo tee /proc/sys/vm/drop_cachespython ./tf_cnn_benchmarks.py --batch_size=256 --num_batches=100 --model=resnet50 --optimizer=momentum --variable_update=replicated --all_reduce_spec=nccl --use_fp16=True --nodistortions --gradient_repacking=2 --compute_lr_on_cpu=True --single_l2_loss_op=True --xla_compile=True --num_gpus=8 --loss_type_to_report=base_loss --data_dir=/home/ubuntu/ImageNet-TF/train --data_name=imagenet</code></pre><p><img alt=\"\" src=\"https://markhpc.github.io/images/2022-11-28-Lambda/Tensorflow_-_ResNet50_Real_Training_First_Attempt_fp16.svg\" /></p><p>Ouch.  Much lower performance with the ImageNet data vs synthetic!  This is especially unfortunate given that 4 years ago the Tensorflow developers reported much better results.  I spent some time reading and experimenting with different settings.  Ultimately the one setting that made a substantial difference was “datasets_num_private_threads”.  In the Tensorflow benchmark source code, this setting is described as: “[The] number of threads for a private threadpool created for all datasets computation.”  I’ll go into more detail what these threads are doing in a bit. For now, let’s see how increasing the number of threads affects the results:</p><p><img alt=\"\" src=\"https://markhpc.github.io/images/2022-11-28-Lambda/Tensorflow_-_ResNet50_ImageNet_Training_fp16_private_threads.svg\" /></p><p>Increasing the number of private threads has a dramatic effect on performance, though I was unable to fully match the performance achieved in the synthetic tests on either the local or persistent storage.  The local storage fared better at high thread counts gradually topping out at around 8600 images/second.  At high private thread counts the persistent storage topped out between 7000-8000 images/second with a higher degree of variability between runs.  I suspect that in this case the persistent storage has likely hit its (per instance) limit.</p><p>In addition to having a dramatic effect on performance, changing the private thread count also had a large effect on the CPU consumption of the TensorFlow process.  CPU usage increases almost linearly with additional private threads up to around 30 cores.  What exactly are these private threads doing?  To answer that question, I utilized two tools that I often deploy when diagnosing CPU usage in Ceph.  When testing with a lower number of private threads, I used linux’s perf tool to look at where cycles are being consumed when the private threads are fully saturated.  At higher levels of private threads, I used my wallclock profiler <a href=\"https://github.com/markhpc/uwpmp\">uwpmp</a> to look at how private threads spend their time when increasing the thread count no longer improves performance.</p><p>In the first case with perf, we can get a good view of the work that these private threads are doing:</p><pre><code>--77.31%--tensorflow::ThreadPoolDevice::Compute          |                    |--51.19%--0x7f511a00c7d8          |          |                    |           --51.18%--tensorflow::jpeg::Uncompress          |--14.48%--tensorflow::ResizeBilinearOp&lt;Eigen::ThreadPoolDevice, unsigned char&gt;::Compute          |--5.47%--tensorflow::CastOpBase::Compute          |--2.66%--tensorflow::ReverseV2Op&lt;Eigen::ThreadPoolDevice, unsigned char, int&gt;::Compute</code></pre><p>The majority of the cycles consumed is in jpeg decompression and resize operations, along with a smattering of other stuff.  What happens if we look at a case with a higher private thread count but now look at wallclock time instead of cycles?  I ended up having some trouble getting the profiler to work properly and consistently get clean callgraphs, but I was able to get at least one run in that revealed some interesting information.  First, I saw time spent in the same functions that perf told us we were spending cycles in:</p><pre><code>+ 100.00% Eigen::ThreadPoolTempl&lt;tensorflow::thread::EigenEnvironment&gt;::WorkerLoop(int) + 99.90% ??? |+ 97.30% ??? ||+ 92.40% ??? |||+ 77.10% _PyEval_EvalFrameDefault ||||+ 47.20% ??? |||||+ 38.10% tensorflow::jpeg::Uncompress(void const*, int, tensorflow::jpeg::UncompressFlags const&amp;, long*, std::function&lt;unsigned char* (int, int, int)&gt;) ||||+ 12.20% tensorflow::ResizeBilinearOp&lt;Eigen::ThreadPoolDevice, unsigned char&gt;::Compute(tensorflow::OpKernelContext*) ||||+ 4.40% tensorflow::CastOpBase::Compute(tensorflow::OpKernelContext*) ||||+ 1.70% tensorflow::ReverseV2Op&lt;Eigen::ThreadPoolDevice, unsigned char, int&gt;::Compute(tensorflow::OpKernelContext*)</code></pre><p>But the wallclock profile also exposed that there may be contention in multiple areas in the private threads around some of the nsync synchronization primitives being used:</p><pre><code> |||||||    |  + 4.50% nsync::nsync_mu_semaphore_p(nsync::nsync_semaphore_s_*) |||||||    |   + 4.50% syscall</code></pre><p>This almost always appeared nested deep inside:</p><pre><code>tensorflow::BFCAllocator::AllocateRaw(unsigned long, unsigned long, tensorflow::AllocationAttributes const&amp;)</code></pre><p>Sadly I was missing a number of debug symbols and don’t 100% trust the wallclock trace.  For now I’ll just say that the private threads are doing a significant amount of work decompressing and manipulating the image data to keep the GPUs fed.  I suspect that with newer and faster GPUs the image retrieval pipeline could become an even bigger issue when training with real image data.  The mystery for me is how The TensorFlow developers achieved such good results 4 years ago without using dedicated private threads at all.  Perhaps they had a significantly faster jpeg decompression mechanism that I am unaware of?</p><h2 id=\"pytorch---imagenet\">PyTorch - ImageNet</h2><p>After running Tensorflow, I also ran some benchmarks in PyTorch using Nvidia’s “DeepLearningExamples” github <a href=\"https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Classification/ConvNets/resnet50v1.5\">repo</a>.  First, I installed the prereqs and setup the repository:</p><pre><code>pip install 'git+https://github.com/NVIDIA/dllogger'pip install --extra-index-url https://developer.download.nvidia.com/compute/redist --upgrade nvidia-dali-cuda110git clone https://github.com/NVIDIA/DeepLearningExamples</code></pre><p>Then, prepared ImageNet for usage in PyTorch:</p><pre><code>cd ~/data/ImageNet/ILSVRC/Data/CLS-LOC/valwget -qO- https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh | bash</code></pre><p>And finally ran a test:</p><pre><code>cd DeepLearningExamples/PyTorch/Classification/ConvNetssync; echo 3 | sudo tee /proc/sys/vm/drop_cachespython ./multiproc.py --nproc_per_node 1 ./main.py --arch resnet50 --label-smoothing 0.1 --run-epoch 1 --amp --static-loss-scale 256 --workspace /home/ubuntu/data/ImageNet-Scratch /home/ubuntu/data/ImageNet-Orig/ILSVRC/Data/CLS-LOC/</code></pre><p>There are a couple of differences here versus the TensorFlow tests.  First, I’m using the raw ImageNet archive instead of a preprocessed TFRecord dataset, so the read behavior is different.  Because I was unable to extract or copy the raw ImageNet archive onto the persistent storage, I’m also only testing the local NVMe drive.  Finally, I didn’t see any specific examples for running with fp16 in nVidia’s documentation, so I’m using amp (automatic mixed precision) which may be slightly slower.</p><p><img alt=\"\" src=\"https://markhpc.github.io/images/2022-11-28-Lambda/Pytorch_-_ResNet50v15_ImageNet_Training_AMP.svg\" /></p><p>Given the number of differences it’s tough to draw direct comparisons with Tensorflow.  Amp is one difference, but it’s quite possible that there are tuning options that could improve performance here that I don’t know about.  I did notice that PyTorch, like Tensorflow, is using quite a bit of CPU to keep the GPUs working.  I suspect that there are ways to tweak the IO pipeline that could improve performance.  For now though, let’s compare the IO patterns on the local NVMe drive during the Tensorflow and PyTorch runs.  I was hoping to be able to use blktrace to do this, but unfortunately was unable to get any data from the virtual devices in the instance.  I was able to collect more general statistics using collectl however.</p><h5 id=\"disk-read-statistics-during-pytorch-8-gpu-run\">Disk Read Statistics During PyTorch 8 GPU run:</h5><table>  <thead>    <tr>      <th>Time</th>      <th>Name</th>      <th>KBytes</th>      <th>Merged</th>      <th>IOs</th>      <th>Size</th>      <th>Wait</th>      <th>QLen</th>      <th>SvcTim</th>    </tr>  </thead>  <tbody>    <tr>      <td>00:29:18</td>      <td>vda</td>      <td>761136</td>      <td>0</td>      <td>6746</td>      <td>113</td>      <td>58</td>      <td>431</td>      <td>0</td>    </tr>    <tr>      <td>00:29:19</td>      <td>vda</td>      <td>752172</td>      <td>0</td>      <td>6648</td>      <td>113</td>      <td>112</td>      <td>810</td>      <td>0</td>    </tr>    <tr>      <td>00:29:20</td>      <td>vda</td>      <td>747824</td>      <td>0</td>      <td>6595</td>      <td>113</td>      <td>84</td>      <td>604</td>      <td>0</td>    </tr>    <tr>      <td>00:29:21</td>      <td>vda</td>      <td>735964</td>      <td>0</td>      <td>6583</td>      <td>112</td>      <td>73</td>      <td>551</td>      <td>0</td>    </tr>    <tr>      <td>00:29:22</td>      <td>vda</td>      <td>695636</td>      <td>0</td>      <td>6237</td>      <td>112</td>      <td>102</td>      <td>760</td>      <td>0</td>    </tr>  </tbody></table><h5 id=\"disk-read-statistics-during-tensorflow-8-gpu-run\">Disk Read Statistics During TensorFlow 8 GPU run:</h5><table>  <thead>    <tr>      <th>Time</th>      <th>Name</th>      <th>KBytes</th>      <th>Merged</th>      <th>IOs</th>      <th>Size</th>      <th>Wait</th>      <th>QLen</th>      <th>SvcTim</th>    </tr>  </thead>  <tbody>    <tr>      <td>00:38:45</td>      <td>vda</td>      <td>1081324</td>      <td>0</td>      <td>8440</td>      <td>128</td>      <td>0</td>      <td>7</td>      <td>0</td>    </tr>    <tr>      <td>00:38:46</td>      <td>vda</td>      <td>927512</td>      <td>0</td>      <td>7241</td>      <td>128</td>      <td>0</td>      <td>7</td>      <td>0</td>    </tr>    <tr>      <td>00:38:47</td>      <td>vda</td>      <td>913512</td>      <td>0</td>      <td>7130</td>      <td>128</td>      <td>0</td>      <td>7</td>      <td>0</td>    </tr>    <tr>      <td>00:38:48</td>      <td>vda</td>      <td>1047444</td>      <td>0</td>      <td>8186</td>      <td>128</td>      <td>0</td>      <td>6</td>      <td>0</td>    </tr>    <tr>      <td>00:38:49</td>      <td>vda</td>      <td>968776</td>      <td>0</td>      <td>7560</td>      <td>128</td>      <td>0</td>      <td>6</td>      <td>0</td>    </tr>  </tbody></table><p><br />When just looking at the IO sizes, both runs appear similar, but that doesn’t tell the whole story.  It is likely that Tensorflow is doing much larger reads that are broken up into contiguous 128KB chunks by the block layer based on the underlying device’s max_sectors_kb setting.  The tells here are the very low queue length and wait times for the TensorFlow run versus the PyTorch run.  In both case the device service times are low (0), but in the TensorFlow case IOs are still backing up in the device queue.</p><p>Interestingly, it appears that it may be possible to use nVidia’s DALI (Data Loading Library) package to <a href=\"https://docs.nvidia.com/deeplearning/dali/archives/dali_170/user-guide/docs/examples/frameworks/pytorch/pytorch-various-readers.html\">read TFRecords into PyTorch</a>.  I didn’t have time to attempt it, but potentially that could have a big effect on IO behavior and performance as well.</p><h2 id=\"conclusion\">Conclusion</h2><p>As I’ve been writing this post, I realize just how complicated it is to understand the performance characteristics of training of neural networks.  Even as we talk about metrics like images/second, the options that are used (batch size for instance) can also affect convergence.  It’s very difficult to come up with a common methodology that is always better than others.  I wonder if another metric, like reaching a desired level of convergence, would be better in the end.  Having said that, I am glad for having done this exercise as I learned some valuable things:</p><ol>  <li>    <p>Pre-processing data into a format like TFRecords on fast local storage is a big win from an IO perspective.  It lets storage systems that have slow metadata performance succeed so long as they have enough sequential read throughput to keep the machine learning framework busy.  This is a big win for many distributed file systems that may have substandard metadata performance (and even the good ones may still benefit).</p>  </li>  <li>    <p>To train on a dataset like ImageNet, you need somewhere around 1-1.3GB/s of raw disk throughput to keep 8 V100 GPUs busy when training in fp16.  For amp or fp32 the requirements are likely lower since the GPUs can’t work quite as fast.  With modern GPUs that are faster than the V100, the disk throughput requirements could be significantly higher.</p>  </li>  <li>    <p>Lambda’s local NVMe storage is likely fast enough to saturate 8 GPUs, even newer ones, so long as the rest of the IO path can keep up.  The persistent storage appears to become a bottleneck with sufficient GPUs and TensorFlow private threads, though can still function fairly well so long as TFRecords are used.  A concern going forward is how to ensure that the data pipeline in TensorFlow and PyTorch are fast enough to keep the GPUs fed.  The Tensorflow benchmark required a large number of private threads and showed potential evidence of contention at high thread counts.  PyTorch did not appear to natively support TFRecords, but NVidia DALI or other 3rd party code might help improve the IO path.</p>  </li>  <li>    <p>If it’s necessary to train directly with images rather than TFRecords, it may not make sense to host them on shared file systems.  It appears that Tensorflow and possibly PyTorch give users the ability to specify a separate training data and work directory.  If all operations against the training data are reads, it may be better to host datasets on read-only block device snapshots. For instance with Ceph, perhaps you could create a read/write RBD volume where you put a certain dataset, take a snapshot, and then map that snapshot as read only on multiple instances that all need access to the same image set.</p>  </li>  <li>    <p>Even with a training set as large as ImageNet, Lambda’s instances have so much memory that eventually the entire dataset becomes cached.  It was necessary to sync and drop caches before each test and keep tests short enough that they didn’t re-read the same data from buffer cache.  I was able to watch as long running tests eventually stopped performing reads and got faster as time went on.  This could make apples-to-apples comparison between different storage vendors difficult if not carefully controlled.</p>  </li>  <li>    <p>I’m almost certainly missing additional tweaks that can help speed up both Tensorflow and PyTorch.  This post shouldn’t be seen as the be-all/end-all for how to achieve high performance with these frameworks, but I hope it may at least help showcase some of the areas that are valuable to investigate when trying to train with real data and achieve high performance.</p>  </li></ol><p>This wraps up my initial work looking at Deep Learning IO behavior.  I hope that next time I can come armed with a bit more knowledge about the internals of how PyTorch and Tensorflow work, focus a bit more on the quality of the training, find even larger datasets to work with, and maybe actually accomplish something useful rather than just play with ImageNet.</p><p>Thanks for reading!</p>",
            "url": "https://hpc.social/personal-blog/2022/an-initial-look-at-deep-learning-io-performance/",
            
            
            
            
            
            "date_published": "2022-11-28T00:00:00-07:00",
            "date_modified": "2022-11-28T00:00:00-07:00",
            
                "author": "Mark Nelson's Blog"
            
        }
    
    ]
}
