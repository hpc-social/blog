<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>hpc.social - Aggregated Personal Blog</title>
 <link href="https://hpc.social/atom.xml" rel="self"/>
 <link href="https://hpc.social/"/>
 <updated>2023-01-03T18:24:14-07:00</updated>
 <id>https://hpc.social</id>
 <author>
   <name>hpc.social</name>
   <email>info@hpc.social</email>
 </author>

 
 <entry>
   <title>Adam’s weekly (-ish) update, 2022-12-20</title>
   <link href="https://hpc.social/2022/adam-s-weekly-ish-update-2022-12-20/"/>
   <updated>2022-12-20T18:14:52-07:00</updated>
   <id>https://hpc.social/2022/adam-s-weekly-ish-update-2022-12-20</id>
   <content type="html">&lt;h2&gt;What&amp;#8217;s new&lt;/h2&gt;

&lt;p&gt;The past few weeks have been on the intense side at work, so I completely lost track of the blog and haven&amp;#8217;t had a chance to write much in that time. However, I&amp;#8217;m now on a holiday break, and finally have time to sit down at a keyboard to write more than code and Slack messages.&lt;/p&gt;

&lt;p&gt;&lt;span id=&quot;more-289&quot;&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;One of the highlights of the past few weeks was a trip to San Jose, and the NVIDIA headquarters. I changed teams at work back in July, transferring from a group that was closely integrated with product management, to a more straightforward engineering team which &lt;a href=&quot;https://blogs.nvidia.com/blog/2020/08/14/making-selene-pandemic-ai/&quot;&gt;designs and builds new high-performance computing systems&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;This was the first chance I&amp;#8217;ve had to meet up with other members of my new team in person, and it was a really wonderful experience to be in the same physical space as folks who were previously just images on my screen. I love working remotely, but it&amp;#8217;s also great to be able to stand in front of a white board with someone and brainstorm, or get coffee and just have a chat with a coworker outside of a video call with an agenda.&lt;/p&gt;

&lt;p&gt;(Plus, we were all careful and managed to avoid catching COVID from each other! Which was a win on its own.)&lt;/p&gt;

&lt;p&gt;Now, for the next two weeks I&amp;#8217;m off work, and planning to take some time to relax and spend time on projects that are harder to focus on during busy work weeks. Expect (maybe) less about computers in my blog and social feeds, and more about D&amp;amp;D, baking, and tasty cocktails.&lt;/p&gt;

&lt;h2&gt;What I&amp;#8217;m reading, watching, and listening to&lt;/h2&gt;

&lt;p&gt;I&amp;#8217;ve been a bit too scattered to focus on actual books the past few weeks, but I did find time for a few interesting articles and podcasts. In particular,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://acoup.blog/2022/12/02/collections-why-roman-egypt-was-such-a-strange-province/&quot;&gt;&amp;#8220;Why Roman Egypt was such a strange province&amp;#8221;&lt;/a&gt;, from Bret Devereaux: As usual from Devereaux, an accessible but extremely detailed discussion of why so much of what we know about the Roman empire is from Egyptian records, but why that also might not be representative of the broader empire.&lt;/li&gt;



&lt;li&gt;&lt;a href=&quot;https://willgallego.com/2022/12/18/emoji-as-incident-resolution-tools/&quot;&gt;&amp;#8220;Emoji as incident resolution tools&amp;#8221;&lt;/a&gt;, from Will Gallego: A fun discussion of how using emoji as part of a team&amp;#8217;s communication can add nuance and shared understanding during incident management, along with a discussion of the disadvantages and costs associated with the practice.&lt;/li&gt;



&lt;li&gt;&lt;a href=&quot;https://www.mikulskibartosz.name/modern-software-architecture-in-2022/&quot;&gt;&amp;#8220;What does modern software architecture look like in 2022?&amp;#8221;&lt;/a&gt;, from Bartosz Mikulski: A nice  article which discusses how service-oriented software architecture can often include an explicit expectation of change. For example, the architecture might include notes on an ongoing deprecation of a library, or might signpost the need to factor a new microservice out when overall system load gets high enough.&lt;/li&gt;



&lt;li&gt;&lt;a href=&quot;https://www.bradyheywood.com.au/podcasts/&quot;&gt;The Brady Heywood podcast&lt;/a&gt;: Found via the &lt;a href=&quot;https://oxide.computer/podcasts/oxide-and-friends/1137359&quot;&gt;Oxide and Friends podcast&lt;/a&gt;, the Brady Heywood podcast is a series on engineering disasters and their consequences from a forensic engineering firm. It&amp;#8217;s mostly not being updated any more (with the podcasters moving on to a separate series on complexity science), but it has a deep back catalog of good episodes, and includes thoughtful discussions of human factors, safety engineering, and how organizational pressures become manifest in engineering artifacts.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Recent recipes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://smittenkitchen.com/2016/12/homemade-irish-cream/&quot;&gt;Smitten Kitchen&amp;#8217;s Homemade Irish Cream&lt;/a&gt;: This is a recipe I make every year, and I often give away small bottles of it as holiday gifts. It&amp;#8217;s really ridiculously tasty, much better than Baileys  or similar, and good either on its own or in hot chocolate.&lt;/li&gt;



&lt;li&gt;&lt;a href=&quot;https://smittenkitchen.com/2014/12/fairytale-of-new-york/&quot;&gt;Smitten Kitchen&amp;#8217;s Fairytale of New York&lt;/a&gt;: This is a really tasty whiskey cocktail, and the star of the show is a &amp;#8220;winter warmth syrup&amp;#8221; that substitutes in for simple syrup. The syrup is simply very tasty, and turns what&amp;#8217;s effectively an OId Fashioned variant into a lovely holiday cocktail.&lt;/li&gt;



&lt;li&gt;Sparkling gingerbread from &lt;a href=&quot;http://www.apt2bbakingco.com/snacking-cakes&quot;&gt;Yossy Arefi&amp;#8217;s Snaking Cakes&lt;/a&gt;: This recipe takes a little more prep than most of Arefi&amp;#8217;s &amp;#8220;snacking cakes&amp;#8221;, as it includes ginger three ways (ground, fresh, and crystallized), but it&amp;#8217;s worth the few minutes of extra work.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Pet photos&lt;/h2&gt;

&lt;figure class=&quot;wp-block-image size-large is-resized&quot;&gt;&lt;img alt=&quot;A white calico cat and a gray tabby cat lounging on a large brown pet bed in front of a gas fireplace.&quot; class=&quot;wp-image-295&quot; height=&quot;512&quot; src=&quot;https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_7207-768x1024.jpeg&quot; width=&quot;384&quot; /&gt;&lt;figcaption class=&quot;wp-element-caption&quot;&gt;I&amp;#8217;m pretty sure these two want me to turn the fireplace on.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;figure class=&quot;wp-block-image size-large is-resized&quot;&gt;&lt;img alt=&quot;A gray tabby cat lounges on a dog bed, while a golden doodle lays on the floor nearby and looks forlornly at the bed.&quot; class=&quot;wp-image-294&quot; height=&quot;512&quot; src=&quot;https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_1725-1024x1024.jpeg&quot; width=&quot;512&quot; /&gt;&lt;figcaption class=&quot;wp-element-caption&quot;&gt;Just Percy bullying the dog by stealing his bed.&lt;/figcaption&gt;&lt;/figure&gt;
</content>
 </entry>
 
 <entry>
   <title>Adam’s weekly update, 2022-12-04</title>
   <link href="https://hpc.social/2022/adam-s-weekly-update-2022-12-04/"/>
   <updated>2022-12-05T05:49:35-07:00</updated>
   <id>https://hpc.social/2022/adam-s-weekly-update-2022-12-04</id>
   <content type="html">&lt;h2&gt;What&amp;#8217;s new&lt;/h2&gt;

&lt;p&gt;This week was really intense from a work perspective. Not &amp;#8220;bad intense&amp;#8221;, but the kind of week where every day was spent with such a level of focus, that at 5 PM or so I found myself staring off into space and forgetting words. I think I got some good things accomplished, but my brain also felt like mush by the time the weekend came.&lt;/p&gt;

&lt;p&gt;&lt;span id=&quot;more-268&quot;&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;This week I&amp;#8217;m traveling to San Jose for work (I just checked into my hotel a little while ago!), so I fully expect this week to also be eaten by work. So I don&amp;#8217;t promise anything terribly interesting for next week&amp;#8217;s post&amp;#8230;&lt;/p&gt;

&lt;p&gt;However, I did take advantage of a Sunday in San Jose to visit the &lt;a href=&quot;https://computerhistory.org/&quot;&gt;Computer History Museum&lt;/a&gt; in Mountain View! I try to visit the museum every few years, and while a lot of the exhibits are the same, enough things change that I always get something new from the visit. Also, I&amp;#8217;ve been doing a lot of reading about hardware development and the history thereof lately, so it was interesting to examine the museum through that new lens.&lt;/p&gt;

&lt;p&gt;I may write more about my visit later this week &amp;#8212; it definitely sparked some thoughts &amp;#8212; but in the mean time, here are a few photos I took while wandering around the museum.&lt;/p&gt;

&lt;figure class=&quot;wp-block-image size-large is-resized&quot;&gt;&lt;img alt=&quot;A mechanical computer built mostly of brass, with various numerical dials. A small placard labels this as a replica of the Babbage Difference Engine No. 1 Demonstration Piece.&quot; class=&quot;wp-image-282&quot; height=&quot;800&quot; src=&quot;https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_6894-768x1024.jpg&quot; width=&quot;600&quot; /&gt;&lt;figcaption class=&quot;wp-element-caption&quot;&gt;The Babbage Difference Engine, and other mechanical computers, have always fascinated me.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;figure class=&quot;wp-block-image size-large is-resized&quot;&gt;&lt;img alt=&quot;The Cray-1, a round computer with its own built-in seating attached.&quot; class=&quot;wp-image-283&quot; height=&quot;446&quot; src=&quot;https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_6965-1024x768.jpg&quot; width=&quot;595&quot; /&gt;&lt;figcaption class=&quot;wp-element-caption&quot;&gt;Can&amp;#8217;t visit the museum without visiting the Cray-1.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;figure class=&quot;wp-block-image size-large is-resized&quot;&gt;&lt;img alt=&quot;The Connection Machine 1, a large black cube divided in eight sections.&quot; class=&quot;wp-image-284&quot; height=&quot;768&quot; src=&quot;https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_6973-768x1024.jpg&quot; width=&quot;576&quot; /&gt;&lt;figcaption class=&quot;wp-element-caption&quot;&gt;I would have loved to have seen a CM-1 in operation, with its red LEDs showing the operation of its many single-bit CPUs.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;figure class=&quot;wp-block-image size-large is-resized&quot;&gt;&lt;img alt=&quot;The front panel of an Altair 8800 computer, with an array of LEDs and switches controlling the state of individual bits.&quot; class=&quot;wp-image-285&quot; height=&quot;449&quot; src=&quot;https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_7037-1024x768.jpg&quot; width=&quot;598&quot; /&gt;&lt;figcaption class=&quot;wp-element-caption&quot;&gt;Having recently read Charles Petzold&amp;#8217;s &amp;#8220;Code&amp;#8221;, I was struck by how closely the front panel of the Altair 8800 resembles the fictional front panel of the computer that Petzold constructs from logic gates up.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;figure class=&quot;wp-block-image size-large is-resized&quot;&gt;&lt;img alt=&quot;A Dell PowerEdge R710 lays on a white plastic table, top cover off, surrounded by instructions on how to disassemble it.&quot; class=&quot;wp-image-286&quot; height=&quot;467&quot; src=&quot;https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_7073-1024x768.jpg&quot; width=&quot;623&quot; /&gt;&lt;figcaption class=&quot;wp-element-caption&quot;&gt;The CHM Learning Lab now includes a back room with a couple of Dell PowerEdge R710 servers, complete with instructions for how to disassemble and reassemble them. Anyone who wants can wander in and take them apart. It was great fun watching a 5-year-old kid pulling components out of one of these&amp;#8230; As well as feeling a little weird, as I think I&amp;#8217;ve run these in production!&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2&gt;What I&amp;#8217;m reading&lt;/h2&gt;

&lt;p&gt;I don&amp;#8217;t have a ton to share this week &amp;#8212; honestly, the whole week feels like a blur &amp;#8212; but here are two books that I recommend.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.aliettedebodard.com/bibliography/novels/the-universe-of-xuya/the-red-scholars-wake/&quot;&gt;The Red Scholar&amp;#8217;s Wake, by Aliette de Bodard&lt;/a&gt;: As the blurb says, &amp;#8220;Lesbian space pirates!&amp;#8221; Also, a really wonderful novella about building a new relationship amidst grief, power differentials, politics, and space battles. I think I basically recommend everything that de Bodard writes, but especially this. And it basically stands alone! So you can read this first, without going back to the other stories in the same world.&lt;/li&gt;



&lt;li&gt;&lt;a href=&quot;https://www.harpercollins.com/products/dealers-of-lightning-michael-a-hiltzik?variant=40824247779362&quot;&gt;Dealers of Lightning: XEROX PARC and the Dawn of the Computer Age, by Michael Hiltzik&lt;/a&gt;: I&amp;#8217;ve just started this, but it&amp;#8217;s already a really interesting snapshot of a key period in the development of the personal computer.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Recent recipes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://smittenkitchen.com/2019/12/unfussy-sugar-cookies/&quot;&gt;Smitten Kitchen&amp;#8217;s Unfussy Sugar Cookies&lt;/a&gt;: These cookies did, indeed, prove to be both tasty and easy to make. If you just want some easy cookies to snack on, I absolutely recommend this recipe.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Pet photos&lt;/h2&gt;

&lt;figure class=&quot;wp-block-image size-large is-resized&quot;&gt;&lt;img alt=&quot;Phyrne the calico cat stares down into the camera from a stairway&quot; class=&quot;wp-image-279&quot; height=&quot;414&quot; src=&quot;https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_6881-768x1024.jpg&quot; width=&quot;310&quot; /&gt;&lt;/figure&gt;

&lt;figure class=&quot;wp-block-image size-large is-resized&quot;&gt;&lt;img alt=&quot;Close-up on the face of Percy the gray tabby cat&quot; class=&quot;wp-image-280&quot; height=&quot;420&quot; src=&quot;https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_6879-768x1024.jpg&quot; width=&quot;314&quot; /&gt;&lt;/figure&gt;

&lt;figure class=&quot;wp-block-image size-large is-resized&quot;&gt;&lt;img alt=&quot;Benny the golden doodle curled up on a dog bed&quot; class=&quot;wp-image-281&quot; height=&quot;238&quot; src=&quot;https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_6876-1024x768.jpg&quot; width=&quot;317&quot; /&gt;&lt;/figure&gt;
</content>
 </entry>
 
 <entry>
   <title>An Initial Look at Deep Learning IO Performance</title>
   <link href="https://hpc.social/2022/an-initial-look-at-deep-learning-io-performance/"/>
   <updated>2022-11-28T00:00:00-07:00</updated>
   <id>https://hpc.social/2022/an-initial-look-at-deep-learning-io-performance</id>
   <content type="html">&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;

&lt;p&gt;This blog post describes an investigation of IO behavior of TensorFlow and PyTorch during resnet50 training running on Lambda Lab’s 8x V100 GPU instances.  Both ephemeral local NVMe storage and network attached persistent storage was tested.  The local NVMe storage was fast enough to achieve a throughput rate required to hit synthetic test targets.  The network attached persistent storage may not be able to fully saturate 8 V100 GPUs during training, though can achieve nearly the same level of performance as the local storage so long as TFRecords are utilized.  Further, there are specific behaviors and bottlenecks in TensorFlow and PyTorch that can reduce training performance when using real data from ImageNet.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;Thank you to Michael Balaban at Lambda Labs for providing access to their GPU cloud for this testing.  Thank you to Chuan Li for the creation of his TensorFlow benchmarking tools.  Thank you also to Andrej Karpathy, Toby Boyd, Yanan Cao, Sanjoy Das, Thomas Joerg, and Justin Lebar for their excellent blog posts on deep learning and XLA performance that helped inform this article.  I hope that this post will be useful for others as your work and writing was useful for me.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;…just because you can formulate your problem as RL doesn’t mean you should. If you insist on using the technology without understanding how it works you are likely to fail.&lt;/em&gt;&lt;/p&gt;


  &lt;p&gt;        Andrej Karpathy, &lt;a href=&quot;https://karpathy.github.io/2019/04/25/recipe/&quot;&gt;A Recipe for Training Neural Networks&lt;/a&gt;, 2019&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;That was the phrase that stuck in my head when I first started this project.   What project you may ask?  I want to understand how deep learning experiments utilize fast storage devices.  Not just any experiments either: &lt;em&gt;real&lt;/em&gt; ones, preferably big.  That’s how I happened upon Andrej Karpathy’s blog.  He is the former Sr. Director of AI at Tesla and knows a thing or two about training big neural networks.  I’ve spent the last decade working on Ceph and have worked on distributed systems and distributed storage for nearly 2 decades at this point.  But training neural nets?  The closest I’ve come was back in the early 2000s when I tried to build a tool to predict video game framerates.  I scraped benchmark numbers from review websites and built M5 decision trees based on hardware and video card settings.  It sort of worked, but was terribly overtrained on a small (~4000 sample) dataset.  Training with petabytes of data to teach an AI how to responsibly drive a car?  I can already feel a bit of imposter syndrome setting in.&lt;/p&gt;

&lt;p&gt;Thankfully my goal is comparatively modest.  I don’t need to build a cutting edge classifier or explore the intricacies of manually implementing back-propagation.  I simply want to understand the IO patterns that are involved when training big datasets with fast GPUs so I can help researchers speed up their work.  Up until now, my ability to do this was fairly limited.  At the day job I’ve had access to a small group of nodes with extremely modest GPUs.  I set up runs with MLPerf but the datasets (WMT G-E and CoCo) easily fit into memory. Other than a short burst of read traffic at the very beginning of training there was very little IO.  Recently I had the opportunity to meet Michael Balaban, Co-Founder of &lt;a href=&quot;https://lambdalabs.com/&quot;&gt;Lambda Labs&lt;/a&gt;.  I told him what I wanted to do and he gave me access to Lambda’s GPU cloud and beta persistent storage to give it a try.  I was able to grab one of Lambda’s 8x Tesla V100 instances (These things are incredibly popular so it’s best to grab one early in the morning!).  Not all of Lambda’s instance types currently have access to the persistent storage but the V100 instances in the Texas zone do.  Once secured, I got to work.&lt;/p&gt;

&lt;h2 id=&quot;tensorflow---synthetic&quot;&gt;TensorFlow - Synthetic&lt;/h2&gt;

&lt;p&gt;Before even attempting to run tests with real data, I realized I needed a baseline to start with.  Luckily, Chuan Li, Lambda’s Chief Scientific Officer, wrote a tool for running TensorFlow benchmarks and made it available on github &lt;a href=&quot;https://github.com/lambdal/lambda-tensorflow-benchmark&quot;&gt;here&lt;/a&gt;. One of the advantages of Lambda’s cloud is that they’ve already bundled up many popular tools for running deep-learning workloads into one package called &lt;a href=&quot;https://lambdalabs.com/lambda-stack-deep-learning-software&quot;&gt;Lambda Stack&lt;/a&gt; which comes pre-installed when you start an instance.  This made it fast to get started, though I did run into one issue.  Lambda Stack comes standard with TensorFlow 2, but Chuan Li’s tool relies on a TensorFlow benchmark submodule that is designed to work with TensorFlow 1.  Luckily, the parent repository was unofficially updated to work with Tensorflow 2 (with a warning that it is no longer being maintained).  A quick “git checkout master” in the “benchmarks” submodule directory got everything working.  Chuan Li’s tool makes it simple to run tests with several preconfigured templates already included.  I chose the fp16 resnet50 configuration as it should be fast at processing images and is fairly standard.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;TF_XLA_FLAGS=--tf_xla_auto_jit=2 ./batch_benchmark.sh X X 1 100 2 config/config_resnet50_replicated_fp16_train_syn
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Using the invocation provided in the benchmark README.md file, I was able to quickly run benchmarks with synthetic data on up to 8 V100 GPUs in the node.  At one point I got stuck, hitting what appeared at first to be an unexplainable 25% performance loss. I reran the tests multiple times and even monitored GPU clockspeeds/temperatures in nvidia-smi with no luck.  Ultimately I discovered my error.  In the slow cases, I had inadvertently left out the “TF_XLA_FLAGS=–tf_xla_auto_jit=2” environment variable.  It turns out that setting this allows Tensorflow compile and execute functions with XLA (Accelerated Linear Algebra) support which is a pretty big win for these tests.&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://markhpc.github.io/images/2022-11-28-Lambda/Tensorflow_-_ResNet50_Synthetic_Training_fp16.svg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;At this point I decided that I needed to understand how Chuan Li’s tool works.  It turns out that he is using the same base tf_cnn_benchmarks.py benchmark code that companies like Nvidia and Dell also use for benchmarking their GPU solutions.  I spent some time running it directly with Dell’s settings from their deep learning overview &lt;a href=&quot;https://infohub.delltechnologies.com/l/high-speed-object-storage-for-deep-learning/overview-3284&quot;&gt;here&lt;/a&gt;.  Unfortunately those tests had mixed results, even after various tweaks.  While researching the XLA issues I mentioned earlier however, I made an even better &lt;a href=&quot;https://blog.tensorflow.org/2018/11/pushing-limits-of-gpu-performance-with-xla.html&quot;&gt;discovery&lt;/a&gt; on the TensorFlow website.  I found an excellent blog post with performance data written by some of the core Tensorflow developers.  It’s now 4 years old, but still appears to be quite valid.  The tuning options used were both simpler and resulted in higher performance versus other configurations that I’ve come across.&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://markhpc.github.io/images/2022-11-28-Lambda/Tensorflow_-_ResNet50_Synthetic_Training_fp16_blog_compare.svg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Training with synthetic data in Lambda’s cloud resulted in similar performance to what the Tensorflow developer’s reported.  In fact, using their own settings yielded slightly faster results when running on Lambda’s 8xV100 instance!  It was incredibly encouraging to me that even in Lambda’s cloud environment with virtual machine instances I could achieve performance that was as fast or faster than what the Tensorflow developers were reporting.&lt;/p&gt;

&lt;h1 id=&quot;choosing-a-real-data-set&quot;&gt;Choosing a Real Data Set&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;The first step to training a neural net is to not touch any neural net code at all and instead begin by thoroughly inspecting your data.&lt;/em&gt;&lt;/p&gt;


  &lt;p&gt;        Andrej Karpathy, &lt;a href=&quot;https://karpathy.github.io/2019/04/25/recipe/&quot;&gt;A Recipe for Training Neural Networks&lt;/a&gt;, 2019&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;Having convinced myself that I had Tensorflow operating reasonably efficiently in synthetic tests, it was time to start thinking about what dataset to use for “real” training.  The largest and most obvious choice is ImageNet.  ImageNet is composed of over 1.2 million categorized images that form a roughly 160GB training dataset.  It is also the largest dataset I could find that was publicly accessible. Downloading it isn’t so easy however. The only version that I could access is the ImageNet Object Localization Challenge dataset hosted on &lt;a href=&quot;https://www.kaggle.com/c/imagenet-object-localization-challenge&quot;&gt;kaggle&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;After finally figuring out how to download the data, it was time to follow Andrej’s advice and try to learn something about it.  While ImageNet is curated and annotated, it has many images of different sizes, dimensions, and pixel counts.  Images also come from many sources with different levels of quality.  Through the power of stack-exchange I was able to find a bash one-liner script to generate a histogram of image sizes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;find . -type f -print0 | xargs -0 ls -l | awk '{size[int(log($5)/log(2))]++}END{for (i in size) printf(&quot;%10d %3d\n&quot;, 2^i, size[i])}' | sort -n
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://markhpc.github.io/images/2022-11-28-Lambda/ImageNet_-_Image_Distribution_by_Approximate_Size.svg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Roughly 80% of the images are in the 64KB or 128KB size bins. Almost all of the remaining images are smaller.  That gives us a pretty good idea of what kind of IOs to expect during classification.  Or at least…it does for frameworks that read those images directly.  In Tensorflow’s case, there’s an alternative format called TFRecord.  TFRecords are basically collections of image data sequentially laid out in much larger files.  Instead of iterating over thousands or millions of individual image files, TFRecords allow Tensorflow to instead stream fewer, larger files that each house multiple images.  It’s a one time cost to pre-process the data so Tensorflow has less work to do during training.  After I downloaded the ImageNet data I took a shot at converting the ImageNet LOC data into TensorFlow records.  Luckily, the TensorFlow tpu github repository already has a &lt;a href=&quot;https://github.com/tensorflow/tpu/blob/master/tools/datasets/README.md&quot;&gt;tool&lt;/a&gt; that can do this.  I had to manipulate the dataset slightly, but ultimately this process worked (at least for the training data):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pip install gcloud google-cloud-storage
pip install protobuf==3.20.1

mkdir ~/data/ImageNetFoo
ln -s ~/data/ImageNet/ILSVRC/Data/CLS-LOC/train ~/data/ImageNetFoo/train
ln -s ~/data/ImageNet/ILSVRC/Data/CLS-LOC/val ~/data/ImageNetFoo/val
ln -s ~/data/ImageNet/ILSVRC/Data/CLS-LOC/test ~/data/ImageNetFoo/test
ln -s ~/data/ImageNet/LOC_synset_mapping.txt ~/data/ImageNetFoo/synset_labels.txt
python imagenet_to_gcs.py --raw_data_dir=/home/ubuntu/data/ImageNetFoo --local_scratch_dir=/home/ubuntu/ExaltedOrbs/ImageNet/tf_records --nogcs_upload
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Perhaps I should say that this worked so long as the original dataset was located on the local NVMe drive.  The persistent storage didn’t fare as well.  Attempting to decompress ImageNet on the persistent storage resulted in blowing past the max number of open files allowed with errors like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;OSError: [Errno 24] Too many open files.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Unfortunately this couldn’t be fixed on the instance.  It appeared to be passed through from the host and the persistent storage was completely unusable until the instance was rebooted.  Recently I spoke to one of Lambda’s engineers and they are working on a fix. (It may already be implemented by the time you read this!)  I also want to note that the persistent storage is still in beta so issues like this are not entirely unexpected.  Having said that, before hitting the error it was significantly slower extracting ImageNet on the persistent storage vs on the local NVMe storage.  It’s probably best to extract ImageNet locally and then write the large TFRecords to the persistent storage during the conversion process.  Luckily extracting ImageNet to local storage was fine, and storing the original archive and the resulting TFRecords on the persistent storage worked perfectly fine as well.&lt;/p&gt;

&lt;h2 id=&quot;fio---baseline-io-results&quot;&gt;FIO - Baseline IO Results&lt;/h2&gt;

&lt;p&gt;Next, I turned my attention to running baseline tests on Lambda’s local and persistent storage using fio.  Fio is a highly configurable and well respected benchmark in the storage community and perfect for generating baseline results.  I decided to use a dataset size that is roughly similar to ImageNet (200GB), the libaio engine in fio with direct IO, and an appropriately high IO depth to let the NVMe drives stretch their legs a bit.&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://markhpc.github.io/images/2022-11-28-Lambda/Lambda_Labs_8xv100_Storage.svg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Throughput with the local NVMe drive(s) is surprisingly good.  The persistent storage is slower, but still might be fast enough at a little over 1GB/s for large reads.  16K IOPS was somewhat slower in both cases.  I chose 16K so that I could quickly compare to tests I ran in my Ceph QEMU/KVM performance blog post &lt;a href=&quot;https://ceph.io/en/news/blog/2022/qemu-kvm-tuning/&quot;&gt;here&lt;/a&gt;.  Without getting into the details, I suspect there’s still some room for improved IOPS with Lambda’s setup.  Luckily though, converting into TFRecords should make Tensorflow throughput bound instead of latency bound.  What about PyTorch or other tools that want to read images directly though?  Fio gives us the ability to simulate it by using its ‘bssplit’ feature.  We can take the size ranges and percentiles generated when examining ImageNet and give fio a similar distribution:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fio --ioengine=libaio --direct=1 --bssplit=2K/1:4K/2:8K/4:16K/8:32K/13:64K/38:128K/33:256K/1 --iodepth=128 --rw=randread --norandommap --size=200G --numjobs=1 --runtime=300 --time_based --name=foo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://markhpc.github.io/images/2022-11-28-Lambda/Lambda_Labs_8xV100_Storage_Reads_Second_Bssplit.svg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This isn’t exactly right as we are not reading data spread across millions of files, but it should provide something of an upper bound on what to expect.  It looks like the persistent storage can do approximately 10K reads/second at a throughput rate of around 750MB/s.  The local storage is about 3-4 times faster.  Local storage should be fast enough to support the kind of images/second throughput rates we want to hit in Tensorflow on 8 V100 GPUs, but the jury is still out for the persistent storage.&lt;/p&gt;

&lt;h2 id=&quot;tensorflow---imagenet&quot;&gt;Tensorflow - ImageNet&lt;/h2&gt;

&lt;p&gt;Running benchmarks with real data rather than synthetic data is fairly straightforward in Tensorflow.  You simply append data_dir and data_name flags to the CLI invocation to let it know where the TFRecords are located:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sync; echo 3 | sudo tee /proc/sys/vm/drop_caches
python ./tf_cnn_benchmarks.py --batch_size=256 --num_batches=100 --model=resnet50 --optimizer=momentum --variable_update=replicated --all_reduce_spec=nccl --use_fp16=True --nodistortions --gradient_repacking=2 --compute_lr_on_cpu=True --single_l2_loss_op=True --xla_compile=True --num_gpus=8 --loss_type_to_report=base_loss --data_dir=/home/ubuntu/ImageNet-TF/train --data_name=imagenet
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://markhpc.github.io/images/2022-11-28-Lambda/Tensorflow_-_ResNet50_Real_Training_First_Attempt_fp16.svg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ouch.  Much lower performance with the ImageNet data vs synthetic!  This is especially unfortunate given that 4 years ago the Tensorflow developers reported much better results.  I spent some time reading and experimenting with different settings.  Ultimately the one setting that made a substantial difference was “datasets_num_private_threads”.  In the Tensorflow benchmark source code, this setting is described as: “[The] number of threads for a private threadpool created for all datasets computation.”  I’ll go into more detail what these threads are doing in a bit. For now, let’s see how increasing the number of threads affects the results:&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://markhpc.github.io/images/2022-11-28-Lambda/Tensorflow_-_ResNet50_ImageNet_Training_fp16_private_threads.svg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Increasing the number of private threads has a dramatic effect on performance, though I was unable to fully match the performance achieved in the synthetic tests on either the local or persistent storage.  The local storage fared better at high thread counts gradually topping out at around 8600 images/second.  At high private thread counts the persistent storage topped out between 7000-8000 images/second with a higher degree of variability between runs.  I suspect that in this case the persistent storage has likely hit its (per instance) limit.&lt;/p&gt;

&lt;p&gt;In addition to having a dramatic effect on performance, changing the private thread count also had a large effect on the CPU consumption of the TensorFlow process.  CPU usage increases almost linearly with additional private threads up to around 30 cores.  What exactly are these private threads doing?  To answer that question, I utilized two tools that I often deploy when diagnosing CPU usage in Ceph.  When testing with a lower number of private threads, I used linux’s perf tool to look at where cycles are being consumed when the private threads are fully saturated.  At higher levels of private threads, I used my wallclock profiler &lt;a href=&quot;https://github.com/markhpc/uwpmp&quot;&gt;uwpmp&lt;/a&gt; to look at how private threads spend their time when increasing the thread count no longer improves performance.&lt;/p&gt;

&lt;p&gt;In the first case with perf, we can get a good view of the work that these private threads are doing:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;--77.31%--tensorflow::ThreadPoolDevice::Compute
          |          
          |--51.19%--0x7f511a00c7d8
          |          |          
          |           --51.18%--tensorflow::jpeg::Uncompress
          |--14.48%--tensorflow::ResizeBilinearOp&amp;lt;Eigen::ThreadPoolDevice, unsigned char&amp;gt;::Compute
          |--5.47%--tensorflow::CastOpBase::Compute
          |--2.66%--tensorflow::ReverseV2Op&amp;lt;Eigen::ThreadPoolDevice, unsigned char, int&amp;gt;::Compute
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The majority of the cycles consumed is in jpeg decompression and resize operations, along with a smattering of other stuff.  What happens if we look at a case with a higher private thread count but now look at wallclock time instead of cycles?  I ended up having some trouble getting the profiler to work properly and consistently get clean callgraphs, but I was able to get at least one run in that revealed some interesting information.  First, I saw time spent in the same functions that perf told us we were spending cycles in:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;+ 100.00% Eigen::ThreadPoolTempl&amp;lt;tensorflow::thread::EigenEnvironment&amp;gt;::WorkerLoop(int)
 + 99.90% ???
 |+ 97.30% ???
 ||+ 92.40% ???
 |||+ 77.10% _PyEval_EvalFrameDefault
 ||||+ 47.20% ???
 |||||+ 38.10% tensorflow::jpeg::Uncompress(void const*, int, tensorflow::jpeg::UncompressFlags const&amp;amp;, long*, std::function&amp;lt;unsigned char* (int, int, int)&amp;gt;)
 ||||+ 12.20% tensorflow::ResizeBilinearOp&amp;lt;Eigen::ThreadPoolDevice, unsigned char&amp;gt;::Compute(tensorflow::OpKernelContext*)
 ||||+ 4.40% tensorflow::CastOpBase::Compute(tensorflow::OpKernelContext*)
 ||||+ 1.70% tensorflow::ReverseV2Op&amp;lt;Eigen::ThreadPoolDevice, unsigned char, int&amp;gt;::Compute(tensorflow::OpKernelContext*)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But the wallclock profile also exposed that there may be contention in multiple areas in the private threads around some of the nsync synchronization primitives being used:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; |||||||    |  + 4.50% nsync::nsync_mu_semaphore_p(nsync::nsync_semaphore_s_*)
 |||||||    |   + 4.50% syscall

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This almost always appeared nested deep inside:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tensorflow::BFCAllocator::AllocateRaw(unsigned long, unsigned long, tensorflow::AllocationAttributes const&amp;amp;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Sadly I was missing a number of debug symbols and don’t 100% trust the wallclock trace.  For now I’ll just say that the private threads are doing a significant amount of work decompressing and manipulating the image data to keep the GPUs fed.  I suspect that with newer and faster GPUs the image retrieval pipeline could become an even bigger issue when training with real image data.  The mystery for me is how The TensorFlow developers achieved such good results 4 years ago without using dedicated private threads at all.  Perhaps they had a significantly faster jpeg decompression mechanism that I am unaware of?&lt;/p&gt;

&lt;h2 id=&quot;pytorch---imagenet&quot;&gt;PyTorch - ImageNet&lt;/h2&gt;

&lt;p&gt;After running Tensorflow, I also ran some benchmarks in PyTorch using Nvidia’s “DeepLearningExamples” github &lt;a href=&quot;https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Classification/ConvNets/resnet50v1.5&quot;&gt;repo&lt;/a&gt;.  First, I installed the prereqs and setup the repository:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pip install 'git+https://github.com/NVIDIA/dllogger'
pip install --extra-index-url https://developer.download.nvidia.com/compute/redist --upgrade nvidia-dali-cuda110
git clone https://github.com/NVIDIA/DeepLearningExamples
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, prepared ImageNet for usage in PyTorch:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd ~/data/ImageNet/ILSVRC/Data/CLS-LOC/val
wget -qO- https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh | bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And finally ran a test:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd DeepLearningExamples/PyTorch/Classification/ConvNets
sync; echo 3 | sudo tee /proc/sys/vm/drop_caches
python ./multiproc.py --nproc_per_node 1 ./main.py --arch resnet50 --label-smoothing 0.1 --run-epoch 1 --amp --static-loss-scale 256 --workspace /home/ubuntu/data/ImageNet-Scratch /home/ubuntu/data/ImageNet-Orig/ILSVRC/Data/CLS-LOC/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are a couple of differences here versus the TensorFlow tests.  First, I’m using the raw ImageNet archive instead of a preprocessed TFRecord dataset, so the read behavior is different.  Because I was unable to extract or copy the raw ImageNet archive onto the persistent storage, I’m also only testing the local NVMe drive.  Finally, I didn’t see any specific examples for running with fp16 in nVidia’s documentation, so I’m using amp (automatic mixed precision) which may be slightly slower.&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://markhpc.github.io/images/2022-11-28-Lambda/Pytorch_-_ResNet50v15_ImageNet_Training_AMP.svg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Given the number of differences it’s tough to draw direct comparisons with Tensorflow.  Amp is one difference, but it’s quite possible that there are tuning options that could improve performance here that I don’t know about.  I did notice that PyTorch, like Tensorflow, is using quite a bit of CPU to keep the GPUs working.  I suspect that there are ways to tweak the IO pipeline that could improve performance.  For now though, let’s compare the IO patterns on the local NVMe drive during the Tensorflow and PyTorch runs.  I was hoping to be able to use blktrace to do this, but unfortunately was unable to get any data from the virtual devices in the instance.  I was able to collect more general statistics using collectl however.&lt;/p&gt;

&lt;h5 id=&quot;disk-read-statistics-during-pytorch-8-gpu-run&quot;&gt;Disk Read Statistics During PyTorch 8 GPU run:&lt;/h5&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Time&lt;/th&gt;
      &lt;th&gt;Name&lt;/th&gt;
      &lt;th&gt;KBytes&lt;/th&gt;
      &lt;th&gt;Merged&lt;/th&gt;
      &lt;th&gt;IOs&lt;/th&gt;
      &lt;th&gt;Size&lt;/th&gt;
      &lt;th&gt;Wait&lt;/th&gt;
      &lt;th&gt;QLen&lt;/th&gt;
      &lt;th&gt;SvcTim&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;00:29:18&lt;/td&gt;
      &lt;td&gt;vda&lt;/td&gt;
      &lt;td&gt;761136&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;6746&lt;/td&gt;
      &lt;td&gt;113&lt;/td&gt;
      &lt;td&gt;58&lt;/td&gt;
      &lt;td&gt;431&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;00:29:19&lt;/td&gt;
      &lt;td&gt;vda&lt;/td&gt;
      &lt;td&gt;752172&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;6648&lt;/td&gt;
      &lt;td&gt;113&lt;/td&gt;
      &lt;td&gt;112&lt;/td&gt;
      &lt;td&gt;810&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;00:29:20&lt;/td&gt;
      &lt;td&gt;vda&lt;/td&gt;
      &lt;td&gt;747824&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;6595&lt;/td&gt;
      &lt;td&gt;113&lt;/td&gt;
      &lt;td&gt;84&lt;/td&gt;
      &lt;td&gt;604&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;00:29:21&lt;/td&gt;
      &lt;td&gt;vda&lt;/td&gt;
      &lt;td&gt;735964&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;6583&lt;/td&gt;
      &lt;td&gt;112&lt;/td&gt;
      &lt;td&gt;73&lt;/td&gt;
      &lt;td&gt;551&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;00:29:22&lt;/td&gt;
      &lt;td&gt;vda&lt;/td&gt;
      &lt;td&gt;695636&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;6237&lt;/td&gt;
      &lt;td&gt;112&lt;/td&gt;
      &lt;td&gt;102&lt;/td&gt;
      &lt;td&gt;760&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h5 id=&quot;disk-read-statistics-during-tensorflow-8-gpu-run&quot;&gt;Disk Read Statistics During TensorFlow 8 GPU run:&lt;/h5&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Time&lt;/th&gt;
      &lt;th&gt;Name&lt;/th&gt;
      &lt;th&gt;KBytes&lt;/th&gt;
      &lt;th&gt;Merged&lt;/th&gt;
      &lt;th&gt;IOs&lt;/th&gt;
      &lt;th&gt;Size&lt;/th&gt;
      &lt;th&gt;Wait&lt;/th&gt;
      &lt;th&gt;QLen&lt;/th&gt;
      &lt;th&gt;SvcTim&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;00:38:45&lt;/td&gt;
      &lt;td&gt;vda&lt;/td&gt;
      &lt;td&gt;1081324&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;8440&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;00:38:46&lt;/td&gt;
      &lt;td&gt;vda&lt;/td&gt;
      &lt;td&gt;927512&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;7241&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;00:38:47&lt;/td&gt;
      &lt;td&gt;vda&lt;/td&gt;
      &lt;td&gt;913512&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;7130&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;00:38:48&lt;/td&gt;
      &lt;td&gt;vda&lt;/td&gt;
      &lt;td&gt;1047444&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;8186&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;00:38:49&lt;/td&gt;
      &lt;td&gt;vda&lt;/td&gt;
      &lt;td&gt;968776&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;7560&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;
When just looking at the IO sizes, both runs appear similar, but that doesn’t tell the whole story.  It is likely that Tensorflow is doing much larger reads that are broken up into contiguous 128KB chunks by the block layer based on the underlying device’s max_sectors_kb setting.  The tells here are the very low queue length and wait times for the TensorFlow run versus the PyTorch run.  In both case the device service times are low (0), but in the TensorFlow case IOs are still backing up in the device queue.&lt;/p&gt;

&lt;p&gt;Interestingly, it appears that it may be possible to use nVidia’s DALI (Data Loading Library) package to &lt;a href=&quot;https://docs.nvidia.com/deeplearning/dali/archives/dali_170/user-guide/docs/examples/frameworks/pytorch/pytorch-various-readers.html&quot;&gt;read TFRecords into PyTorch&lt;/a&gt;.  I didn’t have time to attempt it, but potentially that could have a big effect on IO behavior and performance as well.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;As I’ve been writing this post, I realize just how complicated it is to understand the performance characteristics of training of neural networks.  Even as we talk about metrics like images/second, the options that are used (batch size for instance) can also affect convergence.  It’s very difficult to come up with a common methodology that is always better than others.  I wonder if another metric, like reaching a desired level of convergence, would be better in the end.  Having said that, I am glad for having done this exercise as I learned some valuable things:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Pre-processing data into a format like TFRecords on fast local storage is a big win from an IO perspective.  It lets storage systems that have slow metadata performance succeed so long as they have enough sequential read throughput to keep the machine learning framework busy.  This is a big win for many distributed file systems that may have substandard metadata performance (and even the good ones may still benefit).&lt;/p&gt;

  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To train on a dataset like ImageNet, you need somewhere around 1-1.3GB/s of raw disk throughput to keep 8 V100 GPUs busy when training in fp16.  For amp or fp32 the requirements are likely lower since the GPUs can’t work quite as fast.  With modern GPUs that are faster than the V100, the disk throughput requirements could be significantly higher.&lt;/p&gt;

  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Lambda’s local NVMe storage is likely fast enough to saturate 8 GPUs, even newer ones, so long as the rest of the IO path can keep up.  The persistent storage appears to become a bottleneck with sufficient GPUs and TensorFlow private threads, though can still function fairly well so long as TFRecords are used.  A concern going forward is how to ensure that the data pipeline in TensorFlow and PyTorch are fast enough to keep the GPUs fed.  The Tensorflow benchmark required a large number of private threads and showed potential evidence of contention at high thread counts.  PyTorch did not appear to natively support TFRecords, but NVidia DALI or other 3rd party code might help improve the IO path.&lt;/p&gt;

  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If it’s necessary to train directly with images rather than TFRecords, it may not make sense to host them on shared file systems.  It appears that Tensorflow and possibly PyTorch give users the ability to specify a separate training data and work directory.  If all operations against the training data are reads, it may be better to host datasets on read-only block device snapshots. For instance with Ceph, perhaps you could create a read/write RBD volume where you put a certain dataset, take a snapshot, and then map that snapshot as read only on multiple instances that all need access to the same image set.&lt;/p&gt;

  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Even with a training set as large as ImageNet, Lambda’s instances have so much memory that eventually the entire dataset becomes cached.  It was necessary to sync and drop caches before each test and keep tests short enough that they didn’t re-read the same data from buffer cache.  I was able to watch as long running tests eventually stopped performing reads and got faster as time went on.  This could make apples-to-apples comparison between different storage vendors difficult if not carefully controlled.&lt;/p&gt;

  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;I’m almost certainly missing additional tweaks that can help speed up both Tensorflow and PyTorch.  This post shouldn’t be seen as the be-all/end-all for how to achieve high performance with these frameworks, but I hope it may at least help showcase some of the areas that are valuable to investigate when trying to train with real data and achieve high performance.&lt;/p&gt;

  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This wraps up my initial work looking at Deep Learning IO behavior.  I hope that next time I can come armed with a bit more knowledge about the internals of how PyTorch and Tensorflow work, focus a bit more on the quality of the training, find even larger datasets to work with, and maybe actually accomplish something useful rather than just play with ImageNet.&lt;/p&gt;

&lt;p&gt;Thanks for reading!&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Adam’s weekly update, 2022-11-27</title>
   <link href="https://hpc.social/2022/adam-s-weekly-update-2022-11-27/"/>
   <updated>2022-11-27T15:28:16-07:00</updated>
   <id>https://hpc.social/2022/adam-s-weekly-update-2022-11-27</id>
   <content type="html">&lt;h2&gt;What&amp;#8217;s new&lt;/h2&gt;

&lt;p&gt;The first thing that&amp;#8217;s new is&amp;#8230; this post! I&amp;#8217;m going to try to do at least a weekly post on the blog now, just a general update and some links. This will &lt;em&gt;hopefully&lt;/em&gt; help me get back into the habit of writing on the blog regularly, and maybe inspire me to write a bit more in general.&lt;/p&gt;

&lt;p&gt;&lt;span id=&quot;more-264&quot;&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;I was off work this week for the Thanksgiving holiday, and traveled Michigan to visit my parents and my brother&amp;#8217;s family. My mom has been struggling with some pretty major health issues this year, so it was really wonderful and reassuring to get to spend some time with her and my dad. I also finally got to meet my brother&amp;#8217;s three-year-old son, who was born &lt;em&gt;right&lt;/em&gt; before the pandemic started, and who I hadn&amp;#8217;t managed to meet up until now.&lt;/p&gt;

&lt;p&gt;On the tech-related front, I used this week to take a break from Twitter (mostly), and to be honest&amp;#8230; it was kinda refreshing! I had developed a pretty bad Twitter habit this year, doomscrolling for more time than I like to admit. While I really like Twitter and I&amp;#8217;ve had some nice career boosts from it, it was also a time sink that was not entirely healthy.&lt;/p&gt;

&lt;p&gt;Admittedly, that time was somewhat replaced by playing around on the &lt;a href=&quot;https://calico.social/ajdecon&quot;&gt;Fediverse / Mastodon&lt;/a&gt;. But with the lack of algorithmic suggestions, quote tweets, and other means of virality, that network so far feels a lot quieter and less time-consuming than Twitter. Tim Bray has a &lt;a href=&quot;https://www.tbray.org/ongoing/When/202x/2022/11/26/Bye-Twitter&quot;&gt;good post&lt;/a&gt; up which discusses some of the advantages and pitfalls of federated social media, and I can highly recommend reading that. I&amp;#8217;m still a bit skeptical that it will be a practical &amp;#8220;Twitter replacement&amp;#8221; for most people, but so far I&amp;#8217;m finding it pleasant.&lt;/p&gt;

&lt;h2&gt;What I&amp;#8217;m reading&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Nonfiction book: &lt;/strong&gt;&lt;a href=&quot;https://bookshop.org/p/books/code-the-hidden-language-of-computer-hardware-and-software-charles-petzold/18465738&quot;&gt;Code, Second Edition, by Charles Petzold&lt;/a&gt;. This book walks through the process of building a working computer, starting with ideas like Morse code, then working up from logic gates on up. This is technically a re-read, as I read the first edition&amp;#8230; 10+ years ago? But I&amp;#8217;m getting a lot more out of it this time around, and really enjoying it.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Fiction book: &lt;/strong&gt;&lt;a href=&quot;https://bookshop.org/p/books/the-spare-man-mary-robinette-kowal/18834426&quot;&gt;The Spare Man, by Mary Robinette Kowal&lt;/a&gt;. A cozy murder mystery on a luxury cruise to Mars. I&amp;#8217;m only a few chapters in, but already greatly enjoying myself.&lt;/li&gt;



&lt;li&gt;&lt;a href=&quot;https://ferd.ca/hiding-theory-in-practice.html&quot;&gt;&amp;#8220;Hiding theory in practice&amp;#8221;, by Fred Hebert&lt;/a&gt;. I&amp;#8217;ve been reading a lot about safety engineering and its application to computing lately, but that community can sometimes get off into the weeds about points of theory that don&amp;#8217;t have consensus in the broader computing community. This post has a good discussion of how to use the theory of safety engineering to guide decisions, without requiring that everyone working with you be handed a reading list.&lt;/li&gt;



&lt;li&gt;&lt;a href=&quot;https://cohost.org/mononcqc/post/385225-paper-repentance-as&quot;&gt;&amp;#8220;Paper: Repentance as Rebuke: Betrayal and Moral Injury in Safety Engineering&amp;#8221;, also by Fred Hebert&lt;/a&gt;. A discussion of &lt;a href=&quot;https://link.springer.com/article/10.1007/s11948-022-00412-2&quot;&gt;a paper by Dekker &lt;em&gt;et al&lt;/em&gt;&lt;/a&gt; which looks at the aftermath of the 737 MAX air disasters, and the public repentance of some of the engineers who were involved. Go read the post, it&amp;#8217;s great. And I&amp;#8217;m planning to read the original paper this week.&lt;/li&gt;



&lt;li&gt;&lt;a href=&quot;https://chipsandcheese.com/2022/11/15/cannon-lake-intels-forgotten-generation/&quot;&gt;&amp;#8220;Cannon Lake: Intel&amp;#8217;s Forgotten Generation&amp;#8221;, from &lt;em&gt;Chips and Cheese&lt;/em&gt;&lt;/a&gt;. Really I&amp;#8217;ve been reading a bunch of the technical posts from &lt;em&gt;Chips and Cheese&lt;/em&gt; lately, and they&amp;#8217;re doing pretty good analyses of recent hardware. They&amp;#8217;ve definitely earned that spot in my RSS reader.&lt;/li&gt;



&lt;li&gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/2022/11/sc22-recap.html&quot;&gt;Glenn K Lockwood&amp;#8217;s &amp;#8220;SC&amp;#8217;22 Recap&amp;#8221;&lt;/a&gt;. I was sad to miss Supercomputing this year, though enough folks have come down with COVID that I don&amp;#8217;t really regret the decision. But Glenn wrote up a really interesting recap post, with an interesting new viewpoint now that he&amp;#8217;s working at Microsoft Azure. Among other things, he included a whole section titled &lt;em&gt;The underwhelming&lt;/em&gt;, with the opening line &amp;#8220;The biggest deal appears to be that exascale is here, and it turns out that it&amp;#8217;s not that big of a deal.&amp;#8221;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Recent recipes&lt;/h2&gt;

&lt;p&gt;Because it was Thanksgiving, I did a lot of cooking this week! I&amp;#8217;m not going to list everything I made, but a few of my favorites were:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.delish.com/cooking/recipe-ideas/a23340027/cheesy-garlic-butter-rolls-recipe/&quot;&gt;Cheesy Garlic Butter Rolls from Delish&lt;/a&gt;: Nothing special, but really tasty.&lt;/li&gt;



&lt;li&gt;&lt;a href=&quot;https://smittenkitchen.com/2019/11/challah-stuffing/&quot;&gt;Challah Stuffing from Smitten Kitchen&lt;/a&gt;: This recipe was a huge winner, with most of the family coming back for seconds, and then having more the next day for leftovers. It was really good, and is probably what I&amp;#8217;ll make if I ever do stuffing again.&lt;/li&gt;



&lt;li&gt;&lt;a href=&quot;https://smittenkitchen.com/2008/09/best-challah-egg-bread/&quot;&gt;Best Challah from Smitten Kitchen&lt;/a&gt;: I baked the bread that went into the stuffing, and it was really tasty on its own! This recipe makes two loaves, and I only needed one for the stuffing. So I also made french toast with it, which worked really nicely.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Pet photos&lt;/h2&gt;

&lt;p&gt;Gotta have those pet photos.&lt;/p&gt;

&lt;figure class=&quot;wp-block-image size-large is-resized&quot;&gt;&lt;img alt=&quot;A blond golden doodle in a red harness and a blue bandanna lays on sandy dirt and looks into the camera&quot; class=&quot;wp-image-271&quot; height=&quot;233&quot; src=&quot;https://thinking.ajdecon.org/wp-content/uploads/2022/11/IMG_6863-1024x768.jpeg&quot; width=&quot;311&quot; /&gt;&lt;/figure&gt;

&lt;figure class=&quot;wp-block-image size-large is-resized&quot;&gt;&lt;img alt=&quot;A white calico cat sits on a blanket and washes her front paw&quot; class=&quot;wp-image-272&quot; height=&quot;410&quot; src=&quot;https://thinking.ajdecon.org/wp-content/uploads/2022/11/69075713241__19379770-6B0C-4780-8DD0-30C62A033C88-768x1024.jpeg&quot; width=&quot;308&quot; /&gt;&lt;/figure&gt;

&lt;figure class=&quot;wp-block-image size-large is-resized&quot;&gt;&lt;img alt=&quot;A gray-brown tabby cat wearing a green collar sitting on a wall, looking vaguely toward the camera&quot; class=&quot;wp-image-273&quot; height=&quot;405&quot; src=&quot;https://thinking.ajdecon.org/wp-content/uploads/2022/11/69073206299__DB9CA33B-0EB5-4681-96DA-8368554B6B8A-768x1024.jpeg&quot; width=&quot;304&quot; /&gt;&lt;/figure&gt;
</content>
 </entry>
 
 <entry>
   <title>SC'22 Recap</title>
   <link href="https://hpc.social/2022/sc-22-recap/"/>
   <updated>2022-11-24T02:00:00-07:00</updated>
   <id>https://hpc.social/2022/sc-22-recap</id>
   <content type="html">&lt;p&gt;The biggest annual conference in HPC, the &lt;a href=&quot;https://sc22.supercomputing.org&quot;&gt;SC conference&lt;/a&gt;, was recently held in Dallas, Texas in its second hybrid incarnation since being all-remote for the pandemic. This year attracted over 11,000 attendees which is much closer to the pre-pandemic high of 14,000 than last year's 7,000, and judging from the crushed conference rooms and busy expo floor, it looks like SC is not that much worse for wear.&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;This year's conference quite different for me since I attended for my first time as a vendor, not a researcher or practitioner, and I spent most of my days behind closed doors talking to customers. I didn't get to attend any of the keynotes, BOFs, or panels to which I wasn't invited as a result, so I'm not really qualified to give an erudite summary of the conference or expo this year.&lt;/p&gt;
&lt;p&gt;So instead, I'm just writing down what I remember in order that I remember it and not necessarily in a coherent narrative form. I'm sure I missed a lot (for example, mixed precision seemed big this year, and I heard Jack Dongarra gave a fantastic Turing Award talk) so I encourage others to write their own recaps and share with the community!&lt;span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;h2 style=&quot;text-align: left;&quot;&gt;High-level themes&lt;/h2&gt;
&lt;p&gt;I actually started writing an SC'21 recap last year which I never posted, and re-reading the intro was funny--you'd think nothing has changed in the last year.&lt;/p&gt;
&lt;h3 style=&quot;text-align: left;&quot;&gt;The underwhelming&lt;/h3&gt;
&lt;p&gt;The biggest deal appears to be that exascale is here, and it turns out that it's not that big of a deal. China let the air out of the tires by debuting their exascale systems at SC'21, and not only did they thumb their nose at Top500 by not submitting, they debuted by winning a Gordon Bell prize instead. The first US exascale system, Frontier, was debuted at ISC this year leaving its showing at SC a bit deflated too. &lt;a href=&quot;https://www.hpcwire.com/2022/11/17/2022-gordon-bell-prize-goes-to-plasma-accelerator-research/&quot;&gt;Frontier was featured in the Gordon Bell prize-winning paper&lt;/a&gt; this year, but that work required the use of four Top-10 systems, not just Frontier, painting the reality that one giant computer rarely stands on its own when it comes to advancing science.&lt;/p&gt;
&lt;p&gt;This isn't to say that deploying exascale systems isn't a noteworthy feat and worth commendation, but I felt like the hype over the last five years treated the achievement like an end state instead of a milestone. And now that we've passed the milestone, the community is grasping to figure out what comes next. So what &lt;i&gt;is&lt;/i&gt; next?&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Quantum&lt;/b&gt; had a strong and growing presence at SC, as it has for the last few years. But the conclusion of the panel &quot;&lt;a href=&quot;https://www.hpcwire.com/2022/11/19/quantum-are-we-there-or-close-yet-no-says-the-panel/&quot;&gt;Quantum Computing: A Future for HPC Acceleration&lt;/a&gt;&quot; was that no, it's not close to being ready.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Disaggregation and composability&lt;/b&gt; was another theme with growing momentum. And like quantum, there was a panel asking the same question: &quot;&lt;a href=&quot;https://www.hpcwire.com/off-the-wire/informal-poll-of-sc22-attendees-suggests-a-bright-future-for-composability/&quot;&gt;Does HPC need composability now?&lt;/a&gt;&quot; The answer, again, was no, not yet. More on that below.&lt;/p&gt;
&lt;p&gt;What about &lt;b&gt;RISC-V&lt;/b&gt;? Surely that will revolutionize the field. As it turns out, the answer there is also that &lt;a href=&quot;https://www.hpcwire.com/2022/11/18/risc-v-is-far-from-being-an-alternative-to-x86-and-arm-in-hpc/&quot;&gt;RISC-V is not ready to do anything useful for HPC yet&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The list goes on of technologies and trends that people are trying to boost now that exascale is &quot;solved.&quot; The reality, I think, is that &quot;exascale&quot; will take years to actually mature since it appears to have a ton of technical debt that accumulated during the race to be first. US Exascale rests on the shoulders of AMD and Intel, two companies whose software stacks have not caught up to the market leader, so there will be a lot of thrashing around as development practices and optimization settle out around these systems.&lt;/p&gt;
&lt;p&gt;Struggling with code porting is not very exciting to computer science Ph.D.s, so I expect future SCs to mirror this one and bifurcate into two distinct tracks: those struggling to identify the next big thing in the research space, and those struggling to use the systems that were rushed to deployment.&lt;/p&gt;
&lt;h3 style=&quot;text-align: left;&quot;&gt;The unexpected&lt;/h3&gt;
&lt;p&gt;My SC experience was very biased since I didn't get out much, but two related themes kept popping up across different meetings and the sessions I did attend.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Power efficiency is serious business now&lt;/b&gt;. It used to seem like people talked about the need for energy-efficient HPC in an abstract sense while continuing to jam more power into every rack without changing their approach to system design, facilities, and deployment models. That has hit a hard wall with energy prices soaring in Europe, though. The financial impacts of power-inefficient supercomputing have gone from a one-time capex cost to an ongoing opex cost that is putting many HPC facilities on an unsustainable cost trajectory. Even sites that aren't doing new deployments are facing sudden, sharp increases in their costs, and nobody has good answers about how they will keep the lights on.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Cloud HPC is confusing&lt;/b&gt;. With only &lt;a href=&quot;https://www.nextplatform.com/2022/11/08/hpc-follows-the-enterprise-into-the-cloud/&quot;&gt;15% of total HPC dollars winding up in the cloud&lt;/a&gt;, it's little surprise that most HPC folks are only peripherally aware of what HPC in the cloud really means. Worse yet, a subset of those folks are actively hostile towards the idea of running HPC workloads in the cloud. I spoke with my colleagues from all three major cloud service providers as well as my colleagues in DOE, NSF, and education throughout the week, and everyone painted this same general picture.&lt;/p&gt;
&lt;p&gt;There seems to be a mismatch between the expectations of on-prem HPC folks and cloud HPC folks. For example, I was asked why Windows doesn't support OpenMP very well, and after a bit of digging, I realized that the question really wasn't about using OpenMP on Windows as much as it was about using OpenMP in the cloud. There was a latent assumption that &quot;HPC in Microsoft's cloud&quot; must mean &quot;HPC on Windows&quot; which, for the record, is false--I don't even know how to use Windows anymore. Similarly, people decried the performance impacts of sharing HPC nodes with others in the cloud (they are not shared), overheads of virtualizing InfiniBand or GPUs (everyone uses PCIe passthrough or SR-IOV for HPC nodes), and other misconceptions.&lt;/p&gt;
&lt;p&gt;This isn't to say that cloud people aren't confused too; I heard stories about conversations that went sideways because a cloud folks (not from my employer, thankfully!) didn’t realize that the requirements of a traditional gov/edu HPC facility couldn’t be neatly wrapped up into a single workload with a single solution, contrary to the case across many commercial AI shops. And both sides are struggling to find models for partnership and engagement that mirror the traditional relationship between places like a DOE or NSF facility and a company like Cray. HPC departments are used to buying supercomputers and parallel file systems, while cloud providers sell computing and storage as a &lt;i&gt;service&lt;/i&gt;. The distinction may seem trivial at the surface, but there's a large divide that becomes evident once both sides start trying to drill into the details of what a partnership would look like.&lt;/p&gt;
&lt;h2 style=&quot;text-align: left;&quot;&gt;Parallel I/O in Practice Tutorial&lt;/h2&gt;
&lt;p&gt;This was my fifth year contributing to the Parallel I/O in Practice Tutorial with my colleagues at Argonne and Google, and it was our first time doing it in-person since 2019. It felt really good to be back in front of people to opine about the perils of POSIX and the greatness of the &lt;a href=&quot;https://www.mcs.anl.gov/research/projects/darshan/&quot;&gt;Darshan I/O profiling tool&lt;/a&gt;, and this year I retired out the material I used to present on burst buffers (since DataWarp and Infinite Memory Engine have lost relevance in HPC) and the &lt;a href=&quot;https://www.nersc.gov/tokio/&quot;&gt;TOKIO holistic I/O analysis framework&lt;/a&gt; (since it is no longer funded/maintained). In their stead, I presented material on &lt;a href=&quot;https://wiki.lustre.org/Lustre_User_Group_2022&quot;&gt;benchmarking with IOR and mdtest I debuted at LUG 2022 this year&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I haven't gotten feedback yet on whether this change was a net positive one, but I think it went over well. Benchmarking I/O is really challenging if you don't understand how things like page cache really work in distributed systems, and walking through some benchmark examples concretizes a lot of abstract parallel file system concepts like locking and striping. And since benchmarking is a rabbit hole of arbitrary complexity, ending the tutorial with advanced benchmarking topics turned out to be a nice way to add buffer to the end of an eight-hour stretch of carefully timed presentations. It's very easy to skip over the nuances of analyzing mdtest outputs if attendees have a lot of questions about more important things at the end of the day.&lt;/p&gt;
&lt;p&gt;The most surprising observation of the tutorial is how many attendees aren't using MPI anymore. We got a lot of questions last year about task-oriented I/O, and this year had some great questions about trying to understand or tune the I/O performed by Python-based analytics frameworks. We decided to add support for &lt;a href=&quot;https://www.mcs.anl.gov/research/projects/darshan/2019/12/11/new-experimental-version-of-darshan-available-for-instrumenting-non-mpi-applications/&quot;&gt;Darshan to profile non-MPI applications back in 2019&lt;/a&gt; which is now paying dividends by ensuring it is a relevant tool for these new analytics and AI workloads, and we'll probably have to give more attention to optimizing these workloads' I/O in the future.&lt;/p&gt;
&lt;h2 style=&quot;text-align: left;&quot;&gt;DAOS User Group&lt;/h2&gt;
&lt;p&gt;Monday morning was cold and rainy--a perfect day to attend the &lt;a href=&quot;https://daosio.atlassian.net/wiki/spaces/DC/pages/11248861216/DUG22&quot;&gt;2022 DAOS User Group&lt;/a&gt; which was held off-site at the Fairmont Hotel.&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;Whether you particularly care about DAOS or not, the cross-community HPC I/O brain trust is guaranteed to be in attendance, and this year did not disappoint. In addition to the expected stakeholders from Intel and DOE, representatives from all three big CSPs were in attendance. Google Cloud, Seagate, and HPE/Cray were all on the agenda, painting a diversifying landscape of large HPC companies investing time into DAOS and the strength and willingness of the DAOS team to partner with all comers.&lt;/p&gt;
&lt;h3 style=&quot;text-align: left;&quot;&gt;Life after Optane&lt;/h3&gt;
&lt;p&gt;The question that opened up the meeting, of course, was &quot;what is the future of DAOS since Intel cancelled Optane?&quot; Kelsey Prantis had the official statement (I'll replace the grainy photo once the DUG slides are online...):&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;The high-level project answer is that DAOS isn't going anywhere. Aurora, by virtue of still having Optane DIMMs, will not be affected, and DAOS will maintain support for Optane until Intel drops its last Optane DIMMs (Crow Pass for Sapphire Rapids) from support life sometime towards the end of this decade.&lt;/p&gt;
&lt;p&gt;For new customers who aren't going to use Optane, the answer is &quot;&lt;a href=&quot;https://daosio.atlassian.net/issues/?jql=labels%20%3D%20%22md_on_ssd%22&quot;&gt;Metadata on NVMe&lt;/a&gt;,&quot; a development being codeveloped by Intel, HPE, and Google to implement a write-ahead log (WAL) and allow DAOS to use volatile DRAM instead of Optane. It will work like a file system journal in that a compact representation of writes will be committed to NVMe immediately after landing in DRAM, and then DAOS will asynchronously write back the properly serialized representation of that transaction after it is acknowledged. Johann Lombardi had a helpful cartoon that showed how this WAL will fit into DAOS:&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;A key benefit of DAOS's implementation of this WAL is that it will be able to still service incoming writes while flushing old writes; although I don't fully grasp how this works, it is something enabled by the sophisticated I/O scheduler already implemented in DAOS.&lt;/p&gt;
&lt;p&gt;The complete implementation isn't expected to be released until Spring 2024, but it appears to touch only a few components of DAOS and doesn't affect anything above the VOS layer of the DAOS server.&lt;/p&gt;
&lt;p&gt;There was also mention of developing operability with new &lt;a href=&quot;https://news.samsung.com/global/samsung-electronics-unveils-far-reaching-next-generation-memory-solutions-at-flash-memory-summit-2022&quot;&gt;CXL-attached memory-semantic SSDs&lt;/a&gt; to keep the persistent memory capability of DAOS alive beyond Optane. I'm not sure if this would offer a performance benefit over the metadata-on-NVMe feature; early results show that metadata-on-NVMe actually delivers higher IOPS than Optane since the synchronous write path is much simpler without having to account for memory persistence. That said, I didn't really follow the full extent of options on the table for how DAOS metadata may work across different types of memory though.&lt;/p&gt;
&lt;h3 style=&quot;text-align: left;&quot;&gt;DAOS in the flesh at Argonne&lt;/h3&gt;
&lt;p&gt;Kevin Harms presented an update on Aurora's massive 220 PB DAOS installation and laid out its configuration. There are 1,024 DAOS servers based on the Intel Coyote Pass server design, each sporting&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul style=&quot;text-align: left;&quot;&gt;&lt;li&gt;2x Intel Xeon 5320 (Ice Lake) sockets&lt;/li&gt;&lt;li&gt;2x DAOS engines (one per socket)&lt;/li&gt;&lt;li&gt;16x 32GB DDR4 DIMMs&lt;/li&gt;&lt;li&gt;16x 512GB Optane DIMMs (Persistent Memory 200)&lt;/li&gt;&lt;li&gt;16x 15.36 TB Samsung PM1733 NVMe SSDs&lt;/li&gt;&lt;li&gt;2x 200 Gb/s Slingshot NICs&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;The total configuration is quoted at 220 PB usable, but Kevin pointed out that this assumes that every object is erasure coded at 16+2. Unlike virtually every other storage system out there, though, users can choose the data protection for their individual objects when they create them, meaning this 220 PB capacity is an upper limit to what users can do. Users with very hot, read-only objects may choose to replicate instead of erasure code, while others who are capacity-constrained may choose to erasure code everything at 16+2 at the cost of latency and IOPS. This flexibility is really powerful for users since they can tailor their object layout (&quot;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/articles/technical/understanding-data-redundancy-and-sharding-in-daos.html&quot;&gt;object class&lt;/a&gt;&quot; in DAOS parlance) to match the needs of their workload.&lt;/p&gt;
&lt;p&gt;Argonne will be slicing up this DAOS system by giving each scientific project its own DAOS pool, and each pool will be assigned to only 80% of the available DAOS servers by default. This seems like a nice way of providing most of the storage system performance to every user, but offering more freedom to work around bad hardware, bad users, and other performance problems that plague file systems like Lustre that distribute everything across every single server equally.&lt;/p&gt;
&lt;p&gt;Finally, I noticed that Aurora will be using Samsung SSDs, not the Intel (now Solidigm) QLC NAND that appeared in all the DAOS slides floating around two years ago. I'm not sure what happened there, but the move from Solidigm QLC to Samsung TLC couldn't have been cheap.&lt;/p&gt;
&lt;h3 style=&quot;text-align: left;&quot;&gt;New features and contributions&lt;/h3&gt;
&lt;p&gt;DAOS is starting to pick up some truly valuable features that are being developed and contributed by third parties. Of note, croit has contributed a feature which allows DAOS to serve up NVMe over Fabrics targets, and Seagate contributed an S3 gateway for DAOS. Along with the DFS file system interface, DAOS now offers the trifecta of standard object, block, and file services just like Ceph. Unlike Ceph though, performance on DAOS is a first-class citizen. While croit made it clear that the NVMeoF support still has a ways to go to improve the way it does thread pooling and provides resilience, they showed 1.4 million IOPS from a single storage client using TCP over Ethernet with minimal client-side overhead.&lt;/p&gt;
&lt;p&gt;Intel is also developing multitenant support for DFUSE, allowing a single compute node to share a DAOS mount and let permissions be enforced through UID/GID just like a regular file system. Before this update, the FUSE-based nature of DAOS allowed any unprivileged user to mount their container (good), but only one FUSE agent could be alive on a single node at a time (not good) which prevented multiple users sharing a node from both mounting their own containers.&lt;/p&gt;
&lt;p&gt;DAOS also has some longer-term enhancements that I thought were interesting:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul style=&quot;text-align: left;&quot;&gt;&lt;li&gt;expanding the range of POSIX calls supported by DAOS's intercept library to include metadata calls and memory-mapped I/O using &lt;a href=&quot;https://docs.kernel.org/admin-guide/mm/userfaultfd.html&quot;&gt;userfaultfd&lt;/a&gt;&lt;/li&gt;&lt;li&gt;implementing collaborative caching - essentially reimplementing the Linux kernel page cache in userspace so that multiple processes can share cached DAOS pages&lt;/li&gt;&lt;li&gt;supporting a computational storage paradigm by enabling offload of &lt;a href=&quot;https://github.com/rlane/ubpf&quot;&gt;userspace eBPF scripts&lt;/a&gt; to DAOS servers&lt;/li&gt;&lt;/ul&gt;
&lt;h3 style=&quot;text-align: left;&quot;&gt;DAOS in a larger data center ecosystem&lt;/h3&gt;
&lt;p&gt;Dean Hildebrand from Google Cloud then gave an overview of Google's efforts in bringing DAOS into the cloud. He had some nice performance graphs and I'll link the full presentation here once it's uploaded (it's worth a watch), but the part I found the most insightful was how they are trying to decide where a technology like DAOS fits in the larger cloud storage ecosystem. He outlined two different ways DAOS could work in GCP:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ol style=&quot;text-align: left;&quot;&gt;&lt;li&gt;&lt;b&gt;Caching&lt;/b&gt;: Google Cloud Storage (GCS) is the point of truth and DAOS is a cache&lt;/li&gt;&lt;li&gt;&lt;b&gt;Tiering&lt;/b&gt;: DAOS is a point of truth, and GCS is an archive&lt;/li&gt;&lt;/ol&gt;
&lt;p&gt;&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;He said they were leaning towards the caching model where data only lives ephemerally in DAOS, and personally, I think this is the right move since DAOS in the cloud is not resilient without Optane. However, this choice reflects a much larger tension in cloud storage for HPC:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ol style=&quot;text-align: left;&quot;&gt;&lt;li&gt;The centerpiece of every cloud's data story is a scalable, low-cost, low-performance object store which is analogous to what on-prem HPC would call campaign, community, or project storage.&lt;/li&gt;&lt;li&gt;HPC demands higher performance than what these object stores can generally deliver though.&lt;/li&gt;&lt;/ol&gt;
&lt;div&gt;To bridge the gap between these two truths, auxiliary services must bolt on to the object layer and provide higher performance, at a higher cost, for the duration of I/O-intensive HPC jobs. Some choose to provide true tiering from object into a resilient layer of flash (like &lt;a href=&quot;https://aws.amazon.com/fsx/lustre/&quot;&gt;FSx Lustre&lt;/a&gt; and &lt;a href=&quot;https://docs.weka.io/overview/data-storage&quot;&gt;Weka&lt;/a&gt; do), while others project the contents of the object through a high-performance caching layer (like &lt;a href=&quot;https://azure.microsoft.com/en-us/products/hpc-cache/#overview&quot;&gt;HPC Cache&lt;/a&gt; and &lt;a href=&quot;https://aws.amazon.com/blogs/aws/amazon-file-cache-a-high-performance-cache-on-aws-for-your-on-premises-file-systems/&quot;&gt;File Cache&lt;/a&gt;) and are never meant to persistently hold data.&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;This isn't rocket science, but I never thought deeply about the two models since campaign/community/project storage in on-prem HPC is usually fast enough to avoid needing caches or fine-grained tiering capabilities.&lt;/p&gt;
&lt;p&gt;John Bent also had a thought-provoking presentation about how Seagate's now-&quot;deprioritized&quot; CORTX object store, which once &lt;a href=&quot;https://blog.seagate.com/enterprises/seagate-and-sage-project-innovate-to-boost-hpc-and-big-data-community/&quot;&gt;competed with DAOS as Mero&lt;/a&gt;, contains ideas that can complement DAOS:&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;Whereas DAOS delivers high performance using NVMe, CORTX delivers great economics using HDDs, and their strengths are complementary to each other. While I don't fully grasp how a tiered (or caching!) system comprised of DAOS and CORTX could be implemented, John rightly pointed out that the same level of space efficiency can deliver higher data protection if multi-level erasure coding is used to stripe across durable block storage. His specific example was erasure coding at 8+1 across servers and 10+1 within servers to deliver both high efficiency and high durability. This could map to something like running DAOS atop something like CORVAULT, but I don't think all the necessary pieces are in place to realize such a harmonious coexistence yet.&lt;/p&gt;
&lt;p&gt;Of course, completely tossing Reed-Solomon for something more sophisticated (like VAST does with its locally decodable 150+4 scheme) obviates the need for multilevel erasure entirely. But DAOS has not gone down that route yet.&lt;/p&gt;
&lt;p&gt;And as with every talk John gives, there were lots of other interesting nuggets scattered throughout his presentation. Two of my favorites were:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul style=&quot;text-align: left;&quot;&gt;&lt;li&gt;A slide that pointed out that, when you buy something like Ceph as an appliance, you may be spending only 25% of the total cost on storage media and the rest is infrastructure, service, and support. This struck me as a bit on the low end, but some enterprisey NAS and midrange parallel file system appliances can go this low. Spending 60% to 90% on media is a lot nicer for the buyer (and companies like Seagate) if you can buy at scale or eschew the white-glove support, and John suggested that it's up to companies like Seagate to fix the software issues that require customers to pay for white-glove support in the first place.  After all, the less someone spends on support and licenses, the more they can spend on Seagate hard drives.&lt;/li&gt;&lt;li&gt;John's final slide pointed out that object stores were originally designed to get around the limitations of POSIX file systems, but as they've evolved over the last decade, they're starting to look a lot like file systems anyway since they require strong consistency, hierarchical namespaces, and familiar file semantics. Has all the work put into developing super-fast object stores like DAOS over the last ten years really just brought us back full circle to parallel file systems?  Companies like VAST and Weka have shown that &lt;a href=&quot;https://www.nextplatform.com/2017/09/11/whats-bad-posix-io/&quot;&gt;maybe POSIX isn't as bad as the research community (myself included!) have claimed it to be&lt;/a&gt;; it was really just low-performance implementations that nobody wanted.&lt;/li&gt;&lt;/ul&gt;
&lt;div&gt;Once John's talk is uploaded to the DUG 2022 website, I'll link it here.  Like Dean Hildebrand's talk, it is well worth watching (but for wildly different reasons!)&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;h2 style=&quot;text-align: left;&quot;&gt;PDSW 2022&lt;/h2&gt;
&lt;p&gt;I had to duck out of the DAOS User Group early to run (through the rain) to the 7th International Parallel Data Systems Workshop (PDSW 2022) on Monday afternoon.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;Much to everyone’s surprise, PDSW was only given a half day this year and everything felt a little compressed as a result. The organizers kept the work-in-progress (WIP) sessions which can often be an interesting peek into what students are pursuing, but little A/V problems and the unforgiving schedule probably did a disservice to the up-and-comers who use the WIP track to lay the groundwork for future full-length papers. Hopefully SC’23 restores PDSW to its original full-day status.&amp;lt;p&amp;gt;&amp;lt;/p&amp;gt;&lt;/p&gt;
&lt;h3 style=&quot;text-align: left;&quot;&gt;Splinters keynote from Arif Merchant at Google&lt;/h3&gt;
&lt;p&gt;The keynote presentation was given by Arif Merchant from Google about Splinters, the framework that Google Cloud uses to sample I/Os in a scalable way. The challenge they face is that it's impossible to trace and store every single I/O that hits Google's storage servers (D servers), but having an understanding of I/O patterns is essential for characterizing workload I/O behavior and planning for future infrastructure. In fact, this problem is so important that Google isn't the only cloud that's solved it!&lt;/p&gt;
&lt;p&gt;A lot of what Arif talked about is very similar to how Azure does its I/O tracing under the hood. I suppose it should not be surprise that there are only so many ways to solve the challenge of sampling individual IOPS in a way that fairly represents the aggregate workload of a huge distributed storage system. One really smart thing Splinters does that I liked was sample along two different dimensions: not only do they evenly sample across all IOPS at a fixed rate (the obvious thing), but they also sample across files at a fixed rate. In this latter case of per-file sampling, they take a tiny fraction of files and capture every I/O for that file to get a complete picture of how individual files are being accessed.&lt;/p&gt;
&lt;p&gt;This file sampling fills the huge gap that exists when randomly sampling IOPS alone. Because different I/Os have different &quot;costs&quot; (for example, reading a 1 MiB file using a single 1 MiB read op or 256x 4 KiB read ops are functionally equivalent to an application), randomly sampling ops introduces systematic biases that can be difficult to back out after the data has been sampled, subsampled, aggregated, and reduced. Splinters' approach lets you see the workload from two different angles (and biases) and answer a much larger range of questions about what's really happening across thousands of storage servers.&lt;/p&gt;
&lt;p&gt;That said, it was interesting to hear Arif describe how Splinters evolved out of a different internal Google project but wound up outliving it. Splinters is also similar to, but slightly different from, their &lt;a href=&quot;https://research.google/pubs/pub36356/&quot;&gt;Dapper&lt;/a&gt; infrastructure which also does scalable distributed system tracing. And he made overtures to &lt;a href=&quot;https://research.google/pubs/pub41344/&quot;&gt;F1&lt;/a&gt;, a scalable SQL database that is similar to (but not the same as) the SQL-like query interface that Splinters uses. I got the impression that new technologies come and go pretty quickly at Google, and there's a large appetite for creating new software systems outright rather than shoehorning an existing system into solving a new problem. I can't say one way is better than the other; I was just surprised at the contrast with my own experiences.&lt;/p&gt;
&lt;h3 style=&quot;text-align: left;&quot;&gt;Practical papers&lt;/h3&gt;
&lt;p&gt;PDSW had a healthy combination of both very-researchy papers and applied research papers this year. I could only stick around for the applied papers, and two left an impression.&lt;/p&gt;
&lt;p&gt;In the first, &lt;a href=&quot;https://jeanlucabez.io&quot;&gt;Jean Luca Bez&lt;/a&gt; presented &lt;a href=&quot;https://github.com/hpc-io/drishti&quot;&gt;Drishti&lt;/a&gt;, a tool that lives downstream of the Darshan I/O profiling library and finally does what the Darshan community has danced around for years--turning a Darshan log into an actionable set of recommendations on how to improve I/O performance. It does this by cataloguing a bunch of heuristics and using Darshan's new Python integrations to pore through a log and identify known-problematic I/O patterns. Like Jean Luca's &lt;a href=&quot;https://dxt-explorer.readthedocs.io/en/latest/&quot;&gt;DXT Explorer tool&lt;/a&gt;, Drishti has a slick user interface and greatly extends the usability and insights that can be pulled out of a Darshan log file. It probably won't win a Turing Award, but this sort of work is probably going to benefit scores of HPC end-users by making Darshan (and troubleshooting I/O problems) much more accessible to mere mortals for years to come.&lt;/p&gt;
&lt;p&gt;Adrian Jackson also presented a very tidy &lt;a href=&quot;https://arxiv.org/abs/2211.09162&quot;&gt;apples-to-apples comparison of DAOS and Lustre on the same hardware&lt;/a&gt; using both a systems-level benchmark and an application-inspired, object-oriented data model benchmark. The specific bake-off of a new curiosity (DAOS) and the decades-old incumbent (Lustre) is probably interesting to storage nerds, but I think the real novelty of the work is in its exploration of some uncomfortable realities that the HPC I/O community will have to face in the coming years:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul style=&quot;text-align: left;&quot;&gt;&lt;li&gt;Does &quot;slow memory&quot; (nonvolatile Optane or CXL-attached memory SSDs) give actual benefit to existing file systems (like Lustre), or is rethinking the entire storage stack (like DAOS did) really necessary to unlock the performance of new hardware?&lt;/li&gt;&lt;li&gt;Do applications need to rethink their approach to I/O to make use of post-POSIX storage systems like DAOS, or is performing I/O as you would on a file system (Lustre) on a post-POSIX storage system (DAOS) good enough?&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;My take from the work is that, for simple I/O patterns like checkpoint/restart, you can get pretty far by just treating something like DAOS the same as you would a parallel file system:&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;Figure from Manubens et al, &quot;&lt;a href=&quot;https://arxiv.org/abs/2211.09162&quot;&gt;Performance Comparison of DAOS and Lustre for Object Data Storage Approaches&lt;/a&gt;.&quot;&lt;/span&gt;&lt;/b&gt;&lt;/div&gt;
&lt;p&gt;But if you want your data at rest to have the same data model as how it's handled within the application, you really ought to use a storage system that supports data models that are more expressive than a stream of bytes (which is what POSIX files are).&lt;/p&gt;
&lt;p&gt;The authors didn't do a perfect job of giving Lustre its fair shake since they chose to use (abuse) directories and files to represent their application's data model on-disk instead of developing an object-file model that file systems like Lustre handle a little better. But let's be real--HPC is full of applications that do the exact same thing and represent datasets on-disk using complex hierarchies of directories and files simply because that's the easiest way to map the application's representation of data into the standard file system model. In that sense, storage systems that represent rich data models in a high-performance way should be really valuable to naive applications that map in-memory data structures directly to files and directories.&lt;/p&gt;
&lt;p&gt;Going back to John Bent's closing slide from his DAOS User Group talk, though, does any of this even matter since all answers lead back to parallel file systems? Maybe there's something to be learned about adding better back-door APIs that support more diverse data models than what POSIX file interfaces give us.&lt;/p&gt;
&lt;h2 style=&quot;text-align: left;&quot;&gt;The SC22 Expo&lt;/h2&gt;
&lt;p&gt;The expo is my favorite part of SC because it's when I get to talk to people one-on-one and learn about corners of the HPC industry that I would've never otherwise sought out. Much to my dismay, though, I had very little time to walk the floor this year--so little that I didn't get any swag. If you want to read up on what interesting technology was being showcased, I strongly recommend reading &lt;a href=&quot;https://www.servethehome.com/?s=sc22&quot;&gt;all the great content that Patrick Kennedy and his team at STH created covering the expo&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;That said, I did notice some curious trends about the show floor overall.&lt;/p&gt;
&lt;p&gt;The NVIDIA booth was notably absent this year (though they shared booth space with partners), and many of the usual top vendors had significantly smaller presence on the expo floor. Just for fun, I compiled the top ten(ish) vendors by booth size:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ol style=&quot;text-align: left;&quot;&gt;&lt;li&gt;Weka.io (3,200 sqft)&lt;/li&gt;&lt;li&gt;VAST Data, Department of Energy, Penguin Computing, HPE, and Microsoft (2,500 sqft)&lt;/li&gt;&lt;li&gt;AWS (2,000 sqft)&lt;/li&gt;&lt;li&gt;Google and TACC (1,600 sqft)&lt;/li&gt;&lt;li&gt;Supermicro, AMD, Intel, Dell, NASA, and Indiana University (1,500 sqft)&lt;/li&gt;&lt;/ol&gt;
&lt;p&gt;I think it's amazing to see all-flash storage companies at the top of the list alongside all of the Big 3 cloud service providers. I may be reading too much into this, but this may mean that the money behind SC is shifting towards companies playing in the cloud-based AI space instead of traditional big iron for simulation. Or perhaps it's a sign that most of the traditional HPC players are taking a hard look at the return they get on a big booth given the current economic climate and pulled back this year.&lt;/p&gt;
&lt;p&gt;I did chat with a couple colleagues who completely opted out of a booth this year (for reference, &lt;a href=&quot;https://hallerickson.ungerboeck.com/prod/app85.cshtml?AppCode=VFP&amp;amp;OrgCode=34&amp;amp;EvtID=5025&amp;amp;CC=SC22SM&quot;&gt;SC'21&lt;/a&gt; had 10% fewer exhibitor booths than &lt;a href=&quot;https://hallerickson.ungerboeck.com/prod/app85.cshtml?AppCode=VFP&amp;amp;OrgCode=34&amp;amp;EvtID=5020&amp;amp;CC=SC19&quot;&gt;SC'19&lt;/a&gt;), and the reasoning was consistent: they found more value in having staff meet with customers privately or attend the technical sessions and engage with people organically. Combined with a bit of bad taste left over from SC's &lt;a href=&quot;https://sc21.supercomputing.org/exhibits/exhibit-at-sc/&quot;&gt;high cost of hosting pandemic-era &quot;digital booths&quot;&lt;/a&gt; despite low return (did anyone visit digital booths at SC'20 or SC'21?), I can see why some vendors may have chosen to skip the expo this year.&lt;/p&gt;
&lt;p&gt;Whatever the reasons may be, I was a bit sad to see such a small presence from some of my favorites like IBM, Fujitsu, Atos, and NEC. Hopefully the SC Exhibits Committee (and the economy!) can find ways to bring back the pre-pandemic glory of the show floor.&lt;/p&gt;
&lt;p&gt;The expo wasn't all doom and gloom though! Even though I couldn't make my complete rounds this year, there were a couple of highlights for me.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;h3 style=&quot;text-align: left;&quot;&gt;VAST's masterful marketing&lt;/h3&gt;
&lt;p&gt;Perhaps the splashiest vendor at SC was VAST Data who had a brilliant marketing presence. First was the giant Vastronaut mascot that was the centerpiece of their booth:&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;A &lt;a href=&quot;https://twitter.com/search?q=sc22%20vast&amp;amp;f=live&quot;&gt;quick search of Twitter&lt;/a&gt; shows just how many people seized the opportunity to take a selfie at their booth. I would love to know how they transported that thing to and from the conference, but whatever the cost, I'll bet it was worth it.&lt;/p&gt;
&lt;p&gt;At the Grand Opening Gala on Monday, they also gave out delightfully tacky light-up cowboy hats that everyone seemed to be wearing:&lt;/p&gt;
&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p dir=&quot;ltr&quot; lang=&quot;en&quot;&gt;We were there! &lt;a href=&quot;https://twitter.com/hashtag/sc22?src=hash&amp;amp;ref_src=twsrc%5Etfw&quot;&gt;#sc22&lt;/a&gt; &lt;a href=&quot;https://twitter.com/hashtag/sc2022?src=hash&amp;amp;ref_src=twsrc%5Etfw&quot;&gt;#sc2022&lt;/a&gt; &lt;a href=&quot;https://twitter.com/VAST_Data?ref_src=twsrc%5Etfw&quot;&gt;@VAST_Data&lt;/a&gt; &lt;a href=&quot;https://t.co/fWhuSgBfpL&quot;&gt;pic.twitter.com/fWhuSgBfpL&lt;/a&gt;&lt;/p&gt;
— ntnu-hpc (@ntnuhpc) &lt;a href=&quot;https://twitter.com/ntnuhpc/status/1592330266932301829?ref_src=twsrc%5Etfw&quot;&gt;November 15, 2022&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;The subtle genius of this was that not only did people wear them during the gala and the &lt;a href=&quot;https://beowulfbash.com&quot;&gt;Flop Gun-themed Beowulf Bash 2022 party&lt;/a&gt; later that night, but they had to wear them on their plane rides home since they were so inconveniently bulky. Proof in point, my wife (who doesn't work in tech) sent me this text message to confirm that she was waiting for me at the right luggage carousel at San Francisco Airport:&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;I wonder how many innocent bystanders, traveling home for Thanksgiving on Thursday or Friday, saw the shiny cowboy hats at airports around the country and wondered what VAST was.&lt;/p&gt;
&lt;p&gt;The icing on the cake was VAST's CEO, Renen Hallak, parading around in an unmissable Chuck McGill-style space suit all week, clearly not taking himself too seriously and painting VAST as a work hard/play hard kind of company. Now, do flashy space suits and blinking cowboy hats alone mean VAST has a great product? I can't say&lt;sup&gt;**&lt;/sup&gt;. But marketing is an art that I appreciate, and VAST hit some great notes this year.&lt;/p&gt;
&lt;p style=&quot;font-size: xx-small;&quot;&gt;&lt;sup&gt;**&lt;/sup&gt; (Seriously, I'm not sure I wouldn't get in trouble for opining about another company here.)&lt;/p&gt;
&lt;h3 style=&quot;text-align: left;&quot;&gt;The Microsoft hardware bar&lt;/h3&gt;
&lt;p&gt;The only booth where I spent any appreciable time this year was my own employer's. I personally love booth duty and accosting strangers on the show floor, especially if there's something interesting at the booth to jumpstart a conversation. When I worked at SDSC it was a &lt;a href=&quot;https://www.sdsc.edu/News%20Items/PR111213_meteor.html&quot;&gt;Raspberry Pi cluster&lt;/a&gt;, and at the Microsoft booth this year it was the &quot;hardware bar.&quot;&lt;/p&gt;
&lt;p&gt;In addition to the customary booth presentations with giveaways, swag desk, seating area, and a fun caricature artist, the physical servers that underpin the HPC nodes in Azure were on display. &lt;a href=&quot;https://www.opencompute.org/wiki/Server/ProjectOlympus&quot;&gt;Microsoft contributes its hardware platform designs to the Open Compute Project&lt;/a&gt; so the physical hardware that runs in Azure data centers isn't entirely mysterious. Still, every cloud has its hardware secrets, so I was surprised to see these servers laid bare.&lt;/p&gt;
&lt;p&gt;The newest HPC node type (dubbed &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/virtual-machines/hbv4-series&quot;&gt;HBv4&lt;/a&gt;) on display was a node powered by AMD's Genoa processors just announced a few days earlier:&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;This wasn't a display model, either; it had real DDR5 DRAM, a real NDR InfiniBand HCA, real PCIe Gen5, and real big OCP mezzanine card with real big aluminum heat sinks and a big Microsoft sticker on top. A couple visitors commented on the way the heat piping for those Genoa CPUs was done which I guess is unusual; rather than have a giant copper block on top of each socket, heat pipes connect the socket to massive aluminum heat sinks that are closer to the chassis inlets. In retrospect it makes sense; Genoa has a whopping twelve DDR5 DIMMs per socket which leaves little extra room for heat sinks, and these 88+ core sockets have a staggering thermal design power.&lt;/p&gt;
&lt;p&gt;Another exotic piece of hardware on display was an &quot;ND MI200 v4&quot; server:&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;It's logically similar to Azure's &quot;&lt;a href=&quot;https://learn.microsoft.com/en-us/azure/virtual-machines/nda100-v4-series&quot;&gt;ND A100 v4&lt;/a&gt;&quot; server platform with two CPU sockets, eight SXM4 GPU sockets, eight 200G HDR InfiniBand HCAs, and a bunch of M.2 NVMes. But this specific server has eight MI200 GPUs on a common OAM baseboard and uses Infinity Fabric for GPU-to-GPU communication. I've never seen an OAM-socketed anything in real life before, much less eight of them on a baseboard, so I thought this was pretty great to see in the flesh.&lt;/p&gt;
&lt;p&gt;The ND A100 v4 platform was also on display and looked very similar-but-different with its eight A100 GPUs and HGX baseboard:&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;And unlike the MI200 variant, the general public can run on these nodes.&lt;/p&gt;
&lt;p&gt;I'm not sure what more I'm allowed to say, but my colleague Karl made a nice, &lt;a href=&quot;https://twitter.com/KarlPodesta/status/1593627537330126851?s=20&amp;amp;t=uthjeb7YYmTZWRVWaF4XUA&quot;&gt;quick video that runs through the entire Microsoft booth&lt;/a&gt; that's worth a watch, and more details can be had by contacting me or your favorite Microsoft account team privately.&lt;/p&gt;
&lt;p&gt;Of course, the hardware bar was just a way to lure people into the booth so I could achieve my real goal: meeting new folks. As I wrote before, one of my biggest realizations at SC this year is how generally confused people are about what HPC in the cloud really means--both people who come from traditional on-prem HPC and people who come from traditional enterprisey cloud. I found myself surprising many of the people with whom I spoke on the show floor with factoids that I have taken for granted. For example,&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul style=&quot;text-align: left;&quot;&gt;&lt;li&gt;Linux is the most common OS on these HPC node types. While you probably(?) can run Windows if you want on this stuff, I think only a few niche markets do this.&lt;/li&gt;&lt;li&gt;The usage model for an HPC cluster in the cloud can be the same as on-prem. You can have login nodes, Slurm, home directories, parallel file systems, and all that. Jobs don't have to be containerized or turned into a VM image.&lt;/li&gt;&lt;li&gt;The InfiniBand coming out of these nodes is real InfiniBand with real OFED that supports real mpich/mvapich/OpenMPI. It's the same stuff as in on-prem supercomputers. And nodes are assembled into &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/virtual-machines/sizes-hpc&quot;&gt;full-bisection fat tree InfiniBand&lt;/a&gt; clusters just like normal.&lt;/li&gt;&lt;li&gt;There's no noisy neighbor problem on compute nodes because HPC node types aren't shared between users. When you run a VM on an HPC node, you get the whole thing. Just like on large supercomputers.&lt;/li&gt;&lt;li&gt;There's no horrible loss of performance due to running in a VM. Virtualization extensions, PCIe passthrough, and SR-IOV bypass the hypervisor for most things. Inside your VM, you see real Zen cores and real Mellanox HCAs, not virtualized devices.&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;My takeaway impression is that a lot of traditional HPC folks looked at the cloud five or ten years ago, had a sour experience, and haven't paid attention since. In those last five years, though, AI has changed the game. Massive demand for the latest CPUs and accelerators, funded by live-fast-die-young venture capital, has given cloud vendors tremendous financial incentive to catch up to on-prem levels of performance efficiency for AI workloads. And it just so happens that infrastructure that's good for AI is also good for traditional modeling and simulation.&lt;/p&gt;
&lt;h2 style=&quot;text-align: left;&quot;&gt;SCinet!&lt;/h2&gt;
&lt;p&gt;One of the unexpected highlights of my SC this year arose from a chance encounter with a former coworker from NERSC, &lt;a href=&quot;https://www.nersc.gov/about/nersc-staff/networking-security/ronal-kumar/&quot;&gt;Ron Kumar&lt;/a&gt;, who gave me a whirlwind tour of SCinet.&lt;/p&gt;
&lt;p&gt;I have to confess great ignorance around SCinet in general; I always saw it was a weird technological proof of concept that the strange networking people at work would go off and do in the weeks leading up to the actual conference. I knew they did some impressive wide-area transfer demos (like the &lt;a href=&quot;https://scinet.supercomputing.org/community/documents/43/sc17-Kettimuthu-transferring_1petabyte_per_day.pdf&quot;&gt;petabyte-in-a-day demo at SC'16&lt;/a&gt;), but I didn't really get the significance.&lt;/p&gt;
&lt;p&gt;So what is SCinet? It's this yellow bundle of cables dangling from the ceiling.&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&amp;lt;p&amp;gt;The yellow cables are 144-core fiber trunks that bring over a terabit per second of bandwidth into the convention center from the Internet via the national research backbones like ESnet and Internet2 and distribute many terabits per second of capacity throughout the SC conference venue. For comparison, most HPC centers in the US only have a tenth of SCinet’s wide-area bandwidth at best since 400G infrastructure is still rolling out.&amp;lt;/p&amp;gt;&lt;/p&gt;
&lt;p&gt;Most attendees may be familiar with the row of expensive-looking networking racks behind a glass wall towards the back of the expo which is where those yellow cables dangling from the ceiling end. Here's a photo from inside that glass wall:&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;What I didn't realize is that if you go around to the back of the giant walled area behind this glass display, there's a security checkpoint that gates entry into a massive network operations center (NOC) full of laptops, spools of fiber, meeting rooms, and busily working teams in charge of all the lower layers of the networking stack.&lt;/p&gt;
&lt;p&gt;The process to get into the NOC involves an escort and being tagged in with a tamper-proof wristband, and I learned on the tour that there's millions upon millions of dollars worth of high-end networking equipment in the racks shown above. If you look closely, you can see a security camera at the end of the aisle that speaks to this; that camera was one of many.&lt;/p&gt;
&lt;p&gt;Behind the pretty public-facing side of the SCinet racks is a mess of fiber and cables:&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;I guess if you have to tear all this down after just a few weeks, there's no point in investing days in dressing it all up nicely! I particularly enjoyed the fiber panels in the third rack that appear to be affixed to the rack post with shoe laces.&lt;/p&gt;
&lt;p&gt;This year, SCinet did do a neat proof-of-concept where they demonstrated three 400G routers from three vendors (Juniper, Arista, and Cisco?) all talking the same protocol to handle what I assume is the core routing for everything in the convention center:&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;I wish I remembered exactly what was going on here, but I know enough about networking to know that, despite there being standard protocols for coordinating between networking gear, each vendor does their own implementation that is rarely easy to get interoperability from. If anyone out there knows the details of this achievement, please let me know so I can explain this a little better!&lt;/p&gt;
&lt;p&gt;In addition to networking nerd-level demonstrations, SCinet also serves up all the wifi across the convention center. That is why there were tripods with access points scattered around, and why astute attendees may have noticed janky networking equipment scattered around that looked like this:&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;Again, I get it: for a network infrastructure that's only going to last a week, I don't think it's a good use of anyone's time or money to nicely dress all the networking.&lt;/p&gt;
&lt;p&gt;One last factoid I didn't know until this year was that exhibitors can request 100 Gb/s network drops into their individual booths for demos (or downloading the latest version of a PowerPoint presentation &lt;i&gt;really fast&lt;/i&gt;). The end result of supporting both a vast wifi network and 100G fiber across the show floor is that there was a &lt;u&gt;lot&lt;/u&gt; of fiber going into the single row of SCinet equipment:&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;Finally, when I &lt;a href=&quot;https://twitter.com/glennklockwood/status/1592725187015114752?s=61&amp;amp;t=1c4Kbx75SpTJhCruzuy0Ng&quot;&gt;posted some of these photos online&lt;/a&gt; during the conference, my colleague Bilel was kind enough to post a slide from the SC22 opening presentation that had the speeds and feeds of what I had toured:&lt;/p&gt;
&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p dir=&quot;ltr&quot; lang=&quot;en&quot;&gt;Candy Culhane shared Scinet facts &lt;a href=&quot;https://twitter.com/hashtag/SC22?src=hash&amp;amp;ref_src=twsrc%5Etfw&quot;&gt;#SC22&lt;/a&gt; &lt;a href=&quot;https://twitter.com/hashtag/HPC?src=hash&amp;amp;ref_src=twsrc%5Etfw&quot;&gt;#HPC&lt;/a&gt;&lt;br /&gt;&lt;br /&gt;5.01 Tb/s of WAN capacity&lt;br /&gt;$70M in HW &amp;amp; SW, &amp;amp; services provided by 29 SCinet contrib.&lt;br /&gt;175 volunteers from 80 vol. organiz.&lt;br /&gt;&amp;gt; 450 wireless deployed&lt;br /&gt;29 network research exhibition proposals&lt;br /&gt;11.7 miles of fiber &lt;br /&gt;2384 fiber patch &lt;a href=&quot;https://t.co/JtPhjVHZJd&quot;&gt;https://t.co/JtPhjVHZJd&lt;/a&gt; &lt;a href=&quot;https://t.co/kwGl5Ydqp5&quot;&gt;pic.twitter.com/kwGl5Ydqp5&lt;/a&gt;&lt;/p&gt;
— Bilel Hadri (@mnoukhiya) &lt;a href=&quot;https://twitter.com/mnoukhiya/status/1592737463617089536?ref_src=twsrc%5Etfw&quot;&gt;November 16, 2022&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;If you know anyone involved with SCinet, I highly recommend seeing if you can get a tour at the next SC. Even as a relative networking novice, I walked away with a much greater appreciation for the annual achievement of building SCinet. And who knows? Once I get bored of this whole storage thing, maybe I'll try getting into high-performance networking.&lt;/p&gt;
&lt;h2 style=&quot;text-align: left;&quot;&gt;Composability panel&lt;/h2&gt;
&lt;p&gt;This year I was invited to participate in a panel titled &quot;Smackdown! Does HPC Need Composability Now?&quot; moderated by Addison Snell and Dan Olds from &lt;a href=&quot;https://www.intersect360.com&quot;&gt;Intersect360 Research&lt;/a&gt;. This panel was...different. Unlike the traditional SC panel where panelists take turns presenting slides and saying erudite things, this panel had two teams of panelists. And my team only had one slide to present:&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;The ground rules included &quot;personal attacks are allowed,&quot; and needless to say, the panel was about equal parts entertainment and technical discourse. That's not a bad thing, though.&lt;/p&gt;
&lt;p&gt;Addison and Dan did a phenomenal job of pulling their respective teams together and leading discussion in a format that both brought forward the key pros and cons of composability in HPC while poking fun at the thinly veiled, ego-driven personalities that often make up these sorts of panels. Rather than politely dancing around issues like sacrificing memory bandwidth by putting accelerators at the far end of a PCIe bus or gaining higher utilization by allowing users to mix and match CPU, NICs, and GPUs, us panelists were free to shoot straight (or perhaps a bit hyperbolically) and call each other out on our hidden agendas.&lt;/p&gt;
&lt;p&gt;I hope it goes without saying that all us panelists were in on the format and don't actually think people on the other side are dumb. By wrapping technical arguments in snarky comments, we could keep the level of discussion accessible to a wide audience, drive home the key points from both sides, and ensure that we weren't losing audience members who don't care about the PhD-level details as much as they want to hear what their peers are thinking about this exciting new space. I got some feedback afterwards that I didn't seem to hold back, so if anyone did take anything I said seriously, I am very sorry!&lt;/p&gt;
&lt;p&gt;On a technical level, what was the outcome?&lt;/p&gt;
&lt;p&gt;It turns out that &lt;a href=&quot;https://www.hpcwire.com/off-the-wire/informal-poll-of-sc22-attendees-suggests-a-bright-future-for-composability/&quot;&gt;there was about a 60/40 split between people who felt composability wasn't required yet and those who felt it was&lt;/a&gt; after both sides argued their case. Even among panelists, many of us were a lot less convinced about our respective positions than we let on during the panel itself. I got a chuckle when I realized that I wasn't the only one who, when invited to be on the panel, asked &quot;what side do you want me to argue?&quot; I honestly could have gone either way because the dust has not yet settled. &lt;a href=&quot;https://www.tacc.utexas.edu/about/directory/dan-stanzione&quot;&gt;Dan Stanzione, director of TACC&lt;/a&gt;, gave the truest answer to the question of &quot;will composability help HPC&quot; up front--&quot;&lt;a href=&quot;https://twitter.com/HPC_Guru/status/1592604467698241537?s=20&amp;amp;t=tn3WQBUY9M0MWSfqx1XLKA&quot;&gt;it depends&lt;/a&gt;.&quot; Maybe this is a growth opportunity, or maybe it's a lukewarm reception.&lt;/p&gt;
&lt;p&gt;Either way, composable technologies are hitting the market regardless of whether you think they'll be useful or not.  &lt;a href=&quot;https://www.nextplatform.com/2022/11/10/amd-genoa-epyc-server-cpus-take-the-heavyweight-title/&quot;&gt;AMD Genoa supports CXL 1.1 with extensions for memory pooling&lt;/a&gt;, &lt;a href=&quot;https://news.samsung.com/global/samsung-electronics-unveils-far-reaching-next-generation-memory-solutions-at-flash-memory-summit-2022&quot;&gt;Samsung has memory-semantic SSDs&lt;/a&gt;, and everyone and their mother is working on photonics to get higher bandwidths and lower latencies over longer distances. This makes it easier for people to dip their toes in the water to see if composability makes sense, and I think that's what a lot of people will wind up doing in the coming years.&lt;/p&gt;
&lt;h2 style=&quot;text-align: left;&quot;&gt;Customer meetings&lt;/h2&gt;
&lt;p&gt;Unlike in years past, my SC experience this year was dominated by customer meetings. I've been on the customer side of the table plenty of times, but I was surprised to find that it was actually more fun to be on the vendor side for a change. I'm part salesman at heart, so I found it personally gratifying to end a meeting with people nodding along rather than scratching their heads. I learned as a customer that it's very easy for vendors to go way off the rails and waste everyone's time, so I was grateful to have avoided the awkward confusion that punctuates those kinds of meetings. &lt;/p&gt;
&lt;p&gt;I also went into the week worrying that I'd be sitting in the same room, hearing the same pitch and the same jokes, and answering the same questions all week. Thankfully, I work with some great field, business, and product teams who set up interesting conversations rather than rote recitations of boring roadmap slides. Approaching the same topics from different angles helped me figure out how all the pieces of what I'm working on fit together to make a complete picture too; there weren't nearly as many opportunities to do this in the DOE world since the end-users of the HPC systems on which I worked aren't told anything until all the design decisions have already been made.&lt;/p&gt;
&lt;h2 style=&quot;text-align: left;&quot;&gt;A few personal notes&lt;/h2&gt;
&lt;p&gt;This SC was significant to me at a variety of levels; it was the first time I'd gotten on an airplane since February 2020, the first time I'd traveled since starting a new job at a new company, and the first time I'd met any of my new coworkers outside of the structure of a Teams call. During the pandemic I realized that getting out into the world and talking to people from all corners of HPC were my favorite part of my job. Not being able to go to events like SC and maintain that a sense of community involvement dramatically impacted my level of professional satisfaction for the last two years, so I'm glad I was able to finally go this year.&lt;/p&gt;
&lt;p&gt;Though customer meetings were a lot more fun than I expected them to be, I still felt bummed that I could spend so little time walking the expo, talking to folks, and attending all the BOFs normally on my &lt;a href=&quot;https://sc22.supercomputing.org/presentation/?id=bof124&amp;amp;sess=sess331&quot;&gt;must&lt;/a&gt;-&lt;a href=&quot;https://sc22.supercomputing.org/presentation/?id=bof112&amp;amp;sess=sess307&quot;&gt;attend&lt;/a&gt; &lt;a href=&quot;https://sc22.supercomputing.org/presentation/?id=bof110&amp;amp;sess=sess369&quot;&gt;list&lt;/a&gt;. Compounding this was my personal choice to not dine indoors and consequently miss out on almost all other chances to catch up with old friends and colleagues. I also decided to leave SC a day earlier than I usually do to reduce my risk of getting sick which didn't help either. There's never enough time at SC, but this year was particularly pressed.&lt;/p&gt;
&lt;p&gt;I say all this not to complain, but to say how much I appreciated the people who went out of their way to come accost me during the precious few hours I actually had on the exhibit floor. Some I'd not seen since SC'19, and some I'd never actually met since we only started working together mid-pandemic. The conference is busy for everyone, so giving me a slice of your time was very meaningful. That sense of community membership is why I go to SC, it's why I still work in this business, and it's why I try to contribute whatever I can to whomever wants it whether it be a student, engineer, salesperson, or marketer.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Converged Computing</title>
   <link href="https://hpc.social/2022/converged-computing/"/>
   <updated>2022-11-18T08:30:00-07:00</updated>
   <id>https://hpc.social/2022/converged-computing</id>
   <content type="html">&lt;p&gt;For many years, there has been a battle between cloud and HPC. The cloud side of the equation says “micro services, cloud native!”
and the HPC side says “too expensive!” Conversations often don’t progress because both sides are up-in-arms and 
focused on why they cannot work together. At best, we might get access to cloud from an HPC center,
or an company might present a product as branded for “HPC.” But it’s not truly collaborative in the way that I’d like.&lt;/p&gt;

&lt;p&gt;I’ll also step back and comment that (I do not believe) folks (myself included) on the HPC side have done enough
to sit at the table. For example, we haven’t been a voice in the Open Containers Initiative (&lt;a href=&quot;https://supercontainers.github.io/containers-wg/&quot; target=&quot;_blank&quot;&gt;although I’ve tried&lt;/a&gt;), nor have we been present (historically) for conferences that are more focused around cloud native technologies.
There is no pointing fingers or fault here - it’s just a matter of two different cultures, and it’s been challenging figuring out how to talk to one another, and how to work together. I’ve tried my best to be involved, to the best of my ability, in small ways on both sides. But I’m only one person. This isn’t to say there haven’t been small collaborations, but I believe we can do more.&lt;/p&gt;

&lt;h2 id=&quot;change-is-coming&quot;&gt;Change is Coming&lt;/h2&gt;

&lt;p&gt;I think this is going to change. The reason is because both sides of the equation have started to realize we have similar goals,
and it’s not about creating hybrid environments – having both pancakes and waffles for breakfast – but rather convergence – recognizing that pancakes and waffles are both kinds of breakfast cakes, and we can take features that we like of each to create a breakfast cake that will make everyone happy.
The idea of “Converged Computing” comes from my amazing team (see &lt;a href=&quot;https://www.youtube.com/watch?v=9VwAcSOtph0&quot; target=&quot;_blank&quot;&gt;Dan’s talk at KubeCon here&lt;/a&gt;) and is the idea that technologies from HPC can be integrated into more traditionally cloud approaches to produce a solution that
solves problems on both sides. Explicitly for these projects, it means testing the Flux Framework scheduler alongside Kubernetes. Do we still want portable workflows that can move from an HPC environment to cloud? Of course.
However, the niche or gradient that I’m interested in is the space that lives &lt;em&gt;between&lt;/em&gt; these two worlds.&lt;/p&gt;

&lt;p&gt;While I won’t go into huge detail (this would be more appropriate for a talk) the lab openly works on 
&lt;a href=&quot;https://github.com/flux-framework&quot; target=&quot;_blank&quot;&gt;Flux Framework&lt;/a&gt;, a resource manager that (in my opinion) is one of the coolest projects coming out of our space. I started working with these teams a few months ago, and am bringing my excitement and vision for (what I hope to be) a future where we are actively developing alongside other Kubernetes projects, and our work is well-known and established in this space.
What does that mean? Let me share some cool work under development. This is all being done publicly on GitHub, so there is
no issue to talk about it! My first year or so at the lab I was hired under a research project, and although I learned a lot, I haven’t felt inspired and driven until starting this work. Let’s talk about some of it! 🎉️&lt;/p&gt;

&lt;h3 id=&quot;the-flux-operator&quot;&gt;The Flux Operator&lt;/h3&gt;

&lt;div style=&quot;padding: 20px;&quot;&gt;
&lt;img src=&quot;https://flux-framework.org/flux-operator/_images/the-operator.jpg&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;If you aren’t familiar with Kubernetes Operators, let’s step back and talk about a human operator. If you are a syadmin managing apps
with associated services and databases on a cluster, you often had to do maintenance or update tasks like increasing a storage volume,
or modifying a service to a new user need. As this pattern has emerged as a common thing, they have come up with the concept of a Kubernetes Operator - an actual controller you install to your cluster that can automate this. In simple terms, after you install an operator to your cluster,
you can hand it a desired state (represented in a yaml configuration file) and the operator will do whatever it takes to reach that state. What does that means in the context of Flux? The Flux Operator is interested in creating
what we are calling a “Mini Cluster,” illustrated below.&lt;/p&gt;

&lt;div style=&quot;padding: 20px;&quot;&gt;
&lt;img src=&quot;https://flux-framework.org/flux-operator/_images/design-three-team1.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;In Kubernetes object terms this is an &lt;a href=&quot;https://kubernetes.io/docs/tasks/job/indexed-parallel-processing-static/&quot; target=&quot;_blank&quot;&gt;Indexed Job&lt;/a&gt;, a few config maps, secrets, and a &lt;a href=&quot;https://flux-framework.org/flux-restful-api/&quot; target=&quot;_blank&quot;&gt;RESTFul API&lt;/a&gt; and user interface that I designed exposed as a service.  You can read more about our current design &lt;a href=&quot;https://flux-framework.org/flux-operator/development/designs.html&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This Mini Cluster is generated from a “custom resource definition” or CRD (the yaml you provide), and it can take &lt;a href=&quot;https://flux-framework.org/flux-operator/getting_started/custom-resource-definition.html&quot; target=&quot;_blank&quot;&gt;these parameters&lt;/a&gt;. Concetually, you as the user own the Mini Cluster and can submit jobs to it (either via the web interface or the API) until you are done. When you are done, you can bring down the cluster.&lt;/p&gt;

&lt;p&gt;We are excited for this work because in the next months (to a bit longer) we are going to be testing different kinds of workloads 
running using Flux alongside this Mini Cluster, but on Kubernetes! I’ve started a small repository of dummy examples that I’m extending quickly at
&lt;a href=&quot;https://github.com/rse-ops/flux-hpc&quot; target=&quot;_blank&quot;&gt;rse-ops/flux-hpc&lt;/a&gt; and please open an issue there if you have a suggestion.&lt;/p&gt;

&lt;h3 id=&quot;stay-tuned&quot;&gt;Stay Tuned!&lt;/h3&gt;

&lt;p&gt;Stay tuned for more work in this space! I’ve been doing a ton of programming in Go, Python, and working
on a wide range of technologies, and fairly quickly, and I am very much in my happy place. Please come and join us! ❤️&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Ceph OSD CPU Scaling - Part 1</title>
   <link href="https://hpc.social/2022/ceph-osd-cpu-scaling-part-1/"/>
   <updated>2022-11-08T00:00:00-07:00</updated>
   <id>https://hpc.social/2022/ceph-osd-cpu-scaling-part-1</id>
   <content type="html">&lt;p&gt;Last summer we had a user that hit some performance issues based on a recommendation to use 2 cores per OSD in their systems.  I wanted to provide some data for the community and wrote up a blog &lt;a href=&quot;https://ceph.io/en/news/blog/2022/ceph-osd-cpu-scaling/&quot;&gt;post&lt;/a&gt; on the ceph.io website.  Please take a look!&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Containerize It, Baby!</title>
   <link href="https://hpc.social/2022/containerize-it-baby/"/>
   <updated>2022-11-03T09:30:00-06:00</updated>
   <id>https://hpc.social/2022/containerize-it-baby-</id>
   <content type="html">&lt;p&gt;I’ve just submit my &lt;a href=&quot;https://twitter.com/vsoch/status/1588215058009464832&quot; target=&quot;_blank&quot;&gt;entry&lt;/a&gt; to the HPC Guru Elevator Pitch Contest for the Supercomputing 2022 conference!&lt;/p&gt;

&lt;p&gt;I’m fairly sure (like many of these contests) it will be a politically correct winner - someone that is best appealing
to the conference, but I’ll take a stand right now that I think my submission is tops in terms of creativity
and excited energy! I mean, there is just no alternative when it comes to technologies I’m excited about.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Containerize it, baby!&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;Mic Drop!&lt;/em&gt; 🎙️&lt;/p&gt;

&lt;p&gt;Regardless of the outcome of this contest, I feel like I’ve already won - I’ve had so much fun making this and sharing with the community! 🎉️&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>happy living close (-ish) to the metal</title>
   <link href="https://hpc.social/2022/happy-living-close-ish-to-the-metal/"/>
   <updated>2022-11-02T00:18:17-06:00</updated>
   <id>https://hpc.social/2022/happy-living-close-ish-to-the-metal</id>
   <content type="html">&lt;p&gt;For various reasons, I’ve been doing a little bit of career introspection lately. One of the interesting realizations to come out of this is that, despite in practice doing mostly software work, I’ve been happiest when my work involved a strong awareness of the hardware I was running on.&lt;/p&gt;

&lt;p&gt;&lt;span id=&quot;more-247&quot;&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;I suppose it shouldn’t be a surprise, exactly, but I hadn’t exactly thought about it in those terms before! Before I got into computing, I got a bachelors degree in physics, and got through much of a PhD in materials science. While I wasn’t building computers directly, I was definitely working regularly on hardware, building experimental apparatus involving various combinations of vacuum chambers, lasers, exotic microscopes, custom electronics, and microfluidics.&lt;/p&gt;

&lt;p&gt;In terms of my computing career, I’ve generally worked in the area of “high-performance computing”, a buzzword that means I’ve focused on building fast parallel systems aimed at researchers. &lt;/p&gt;

&lt;p&gt;It’s a sub-field that lends itself to awareness of hardware: even as a new baby sysadmin, I was staring at motherboard block diagrams and thinking about the performance differences between different PCIe topologies. &lt;/p&gt;

&lt;p&gt;And because HPC is one of the areas that took the longest to embrace cloud computing, I spent a lot of years doing work in datacenters. Most of my work would usually involve writing code, doing configuration management, and managing Linux systems… but on a regular basis I’d head into a big loud room full of air conditioners and server racks, carrying a screwdriver.&lt;/p&gt;

&lt;p&gt;Amusingly, my relatively recent stint at a hyperscaler was the first time I had worked on computers, but didn’t have my office in the same building as the computers I was running! Even there I was at least somewhat cognizant of hardware specifics, and one of my early projects was performance testing on the B&lt;a href=&quot;https://www.opencompute.org/documents/facebook-bryce-canyon-storage-system-specification&quot;&gt;ryce Canyon &lt;/a&gt;storage node, to see if it was ready for use in a large-scale distributed filesystem.&lt;/p&gt;

&lt;p&gt;And these days, at NVIDIA, I’m enjoying being even closer to the metal. (At least conceptually; I still work remote…) I spend my days thinking about datacenter requirements, cable lengths, firmware upgrades, hardware health checks, and application performance tests on large clusters. And I love getting to play with these shiny toys.&lt;/p&gt;

&lt;p&gt;Anyway, this is just a ramble. But a useful one. While I’d be the first to admit that cloud has its place, and I use it for some personal projects, I really enjoy understanding the hardware I run on. I have trouble thinking of computers as remote abstractions with no underlying detail. They are pleasingly physical in my mind, even if they’re thousands of miles away.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>The web services I self-host</title>
   <link href="https://hpc.social/2022/the-web-services-i-self-host/"/>
   <updated>2022-10-30T21:59:55-06:00</updated>
   <id>https://hpc.social/2022/the-web-services-i-self-host</id>
   <content type="html">&lt;h2&gt;Why self-host anything?&lt;/h2&gt;

&lt;p&gt;In a lot of ways, self-hosting web services is signing up for extra pain. Most useful web services are available in SaaS format these days, and most people don&amp;#8217;t want to be a sysadmin just to use chat, email, or read the news.&lt;/p&gt;

&lt;p&gt;In general, I decide to self-host a service if one of two things is true:&lt;/p&gt;

&lt;p&gt;&lt;span id=&quot;more-235&quot;&gt;&lt;/span&gt;&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;Self-hosting is going to add a capability that&amp;#8217;s difficult to find in a SaaS alternative. That might be privacy, or extra compute, or just an extra degree of customization that I want.&lt;br /&gt;&lt;/li&gt;&lt;li&gt;I find it interesting or amusing to self-host it! I &lt;em&gt;have been&lt;/em&gt; a professional sysadmin, and ran production web services for over a decade. So I enjoy messing around with servers, and can have a fair amount of fun with this.&lt;/li&gt;&lt;/ul&gt;

&lt;h2&gt;Infrastructure and general tooling&lt;/h2&gt;

&lt;p&gt;Right now my self-hosted services are hosted on &lt;a href=&quot;https://www.oracle.com/cloud/&quot;&gt;Oracle Cloud Infrastructure&lt;/a&gt;, for a very simple reason: OCI includes a &lt;em&gt;very&lt;/em&gt; generous &lt;a href=&quot;https://www.oracle.com/cloud/free/&quot;&gt;Always Free tier&lt;/a&gt;, which doesn&amp;#8217;t even ask for a credit card! So I&amp;#8217;m confident I&amp;#8217;m not going to accidentally spend any money. I use ARM Ampere A1 Compute instances for service hosting.&lt;/p&gt;

&lt;p&gt;The individual services are mostly managed using &lt;a href=&quot;https://docs.docker.com/compose/&quot;&gt;Docker Compose files&lt;/a&gt;, though a few are just running bare-metal. I have so far managed to resist the urge to put everything in Kubernetes.&lt;/p&gt;

&lt;p&gt;Everything is backed up on a regular basis using &lt;a href=&quot;https://www.tarsnap.com/&quot;&gt;Tarsnap&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I also use &lt;a href=&quot;https://tailscale.com/&quot;&gt;Tailscale&lt;/a&gt; to provide a VPN between my cloud servers and my various client devices (phone, laptop, tablet). If a service needs to be exposed to the public Internet to function, I do that&amp;#8230; but otherwise, everything is only exposed within the Tailscale VPN, so that only my own devices can access them. This is both a lovely convenience (not having to manage as many DNS records), and provides an extra degree of security by hiding services that no one else needs to access.&lt;/p&gt;

&lt;h2&gt;Services that I self-host&lt;/h2&gt;

&lt;ul&gt;&lt;li&gt;&lt;strong&gt;RSS reader: &lt;/strong&gt;Despite the demise of Google Reader back in the mists of time, I&amp;#8217;ve been a consistently heavy user of RSS feed since at least 2008. At times I&amp;#8217;ve used commercial products such as &lt;a href=&quot;https://feedly.com/&quot;&gt;Feedly&lt;/a&gt;, but these days I self-host the aggregator using &lt;a href=&quot;https://freshrss.org/&quot;&gt;FreshRSS&lt;/a&gt;. I use FreshRSS partly because it&amp;#8217;s pretty easy to spin up and administer, and partly because it&amp;#8217;s compatible with &lt;a href=&quot;https://reederapp.com/&quot;&gt;Reeder&lt;/a&gt;, a Mac and iOS app that I generally use to actually read my feeds.&lt;br /&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Fediverse instance: &lt;/strong&gt;I run a &lt;a href=&quot;https://calico.social/&quot;&gt;self-hosted instance&lt;/a&gt; on the &lt;a href=&quot;https://en.wikipedia.org/wiki/Fediverse&quot;&gt;Fediverse&lt;/a&gt; ensemble of social networking sites. The best-known tool for this is &lt;a href=&quot;https://joinmastodon.org/&quot;&gt;Mastodon&lt;/a&gt;, but I currently use the &lt;a href=&quot;https://pleroma.social/&quot;&gt;Pleroma server&lt;/a&gt;, mostly because it seemed less painful to set up and configure. I run my own instance partly out of curiosity, and partly because I didn&amp;#8217;t strongly resonate with any particular topic-specific server that&amp;#8217;s already out there.&lt;br /&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;IRC bouncer: &lt;/strong&gt;I&amp;#8217;m not on IRC very much these days, but I do like to avoid losing messages, and sometimes want to be logged into the same channels on different physical clients. So I run a &lt;a href=&quot;https://wiki.znc.in/ZNC&quot;&gt;ZNC&lt;/a&gt; server to maintain persistence.&lt;br /&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Matrix server: &lt;/strong&gt;&lt;a href=&quot;https://matrix.org/&quot;&gt;Matrix&lt;/a&gt; is a decentralized messaging platform that supports end-to-end encryption. Think of it as being a little like the Fediverse, but for chat rather than microblogging. This falls pretty squarely in the category of &amp;#8220;I find this amusing to run&amp;#8221;, because I mostly chat with less-nerdy folks on other, commercial platforms.&lt;br /&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Git server: &lt;/strong&gt;I run a &lt;a href=&quot;https://gitea.io/en-us/&quot;&gt;Gitea&lt;/a&gt; server which I use to mirror my own repos, as well as a variety of other open source repos. This is mostly to ensure that I have an up-to-date backup of repos I care about, independent of Github or whatever provider.&lt;br /&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Jupyter notebooks: &lt;/strong&gt;I keep a persistent &lt;a href=&quot;https://jupyter.org/&quot;&gt;Jupyter&lt;/a&gt; notebook instance running for random code experiments and as a tiny development playground. This runs on its own VM where I also do other random software development, and it&amp;#8217;s separate from the other services mostly so I don&amp;#8217;t take down all my personal infra with an accidental OOM from a big build.&lt;br /&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Software package repository: &lt;/strong&gt;I run an instance of &lt;a href=&quot;https://www.sonatype.com/products/repository-oss-download&quot;&gt;Nexus Repository OSS&lt;/a&gt;, mostly to cache Docker images and other content that run the rest of the services above!&lt;/li&gt;&lt;/ul&gt;

&lt;h2&gt;Services where I use managed hosting but don&amp;#8217;t own the server&lt;/h2&gt;

&lt;ul&gt;&lt;li&gt;&lt;strong&gt;This website!&lt;/strong&gt; My &lt;a href=&quot;https://www.ajdecon.org&quot;&gt;regular website&lt;/a&gt; and this blog run on a shared hosting provider, mostly through inertia. (I&amp;#8217;ve used the same hosting provider for web hosting since around 2008.)&lt;br /&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Email: &lt;/strong&gt;In theory it&amp;#8217;s an open, federated system similar to the Fediverse. In practice, the combination of spam and the growth of large providers makes it increasingly painful to run a server yourself. This &lt;a href=&quot;https://cfenollosa.com/blog/after-self-hosting-my-email-for-twenty-three-years-i-have-thrown-in-the-towel-the-oligopoly-has-won.html&quot;&gt;post from Carlos Fenollosa&lt;/a&gt; does a good job of describing the difficulties.&lt;br /&gt;&lt;br /&gt;I do, however, run all my email through my own domain, though it&amp;#8217;s hosted via &lt;s&gt;Google Apps&lt;/s&gt; &lt;s&gt;GSuite&lt;/s&gt; Google Workspace. I also back up my inbox locally on a regular basis. That means that if Google ever decides to remove my account, charge obnoxious costs, or otherwise misbehave, my email address is at least portable to other providers.&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>QEMU/KVM + Ceph Librbd Performance</title>
   <link href="https://hpc.social/2022/qemu-kvm-ceph-librbd-performance/"/>
   <updated>2022-10-24T01:00:00-06:00</updated>
   <id>https://hpc.social/2022/qemu-kvm-ceph-librbd-performance</id>
   <content type="html">&lt;p&gt;Checkout my blog &lt;a href=&quot;https://ceph.io/en/news/blog/2022/qemu-kvm-tuning/&quot;&gt;post&lt;/a&gt; at the ceph.io website about tuning QEMU/KVM for high performance with librbd.  We got over 123K random read IOPs with 16K IOs from a single VM!&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Tunel- Apps for HPC</title>
   <link href="https://hpc.social/2022/tunel-apps-for-hpc/"/>
   <updated>2022-08-04T13:30:00-06:00</updated>
   <id>https://hpc.social/2022/tunel-apps-for-hpc</id>
   <content type="html">&lt;p&gt;A few months ago I was talking about &lt;a href=&quot;https://vsoch.github.io/2022/ssh-tunnels/&quot; target=&quot;_blank&quot;&gt;ssh tunnels&lt;/a&gt;. The reason was because I was looking for a solution to deploy apps (like a Jupyter notebook) onto HPC.
After an adventure I got it working, and it came down a relatively simple set of commands that I needed to just &lt;a href=&quot;https://github.com/tunel-apps/tunel/blob/main/tunel/ssh/commands.py&quot;&gt;write into my app logic&lt;/a&gt; and forget about.
The reason for this was working on my new personal project, &lt;a href=&quot;https://tunel-apps.github.io/tunel/&quot; target=&quot;_blank&quot;&gt;tunel&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Tunel is named for what it does. “Tunel” is an elegant derivation of “tunnel” and will do exactly that - create a tunnel between your local workstation and an HPC cluster.&lt;/p&gt;

&lt;/blockquote&gt;

&lt;div style=&quot;padding: 20px;&quot;&gt;
 &lt;img src=&quot;https://vsoch.github.io/assets/images/posts/tunel/tunel-docs.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;In short, tunel will provide a collection of “apps” that are easy to deploy to HPC. There are concepts called launchers, and examples are singularity, slurm, or htcondor. And we can add more! It’s the job of a launcher to take a an app recipe (a definition in yaml plus helper scripts that can be customized on the fly by the user) and get it running, whatever that means (run a job? a container? monitor something? something else?). For the most part, most apps that I’ve developing have web interfaces, as they have historically been the most challenging thing to get easily working on HPC. As a quick example, to run a jupyter notebook via Singularity on my login node, after I install tunel and have my ssh connection defined as “osg” I can do:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;tunel run-app osg singularity/socket/jupyter &lt;span class=&quot;nt&quot;&gt;--jupyterlab&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The name “singularity/socket/jupyter” is the unique identifier (and path) to the recipe and config, and I can provide custom arguments as shown above. And although this is the “singularity” launcher, we can do the same kind of interaction with a slurm launcher, going one level deeper to run the notebook on a node after we submit a job!
And in my typical way of doing things, I have automation that generates a table and documentation for each of these apps. &lt;a href=&quot;https://tunel-apps.github.io/tunel/_static/apps/&quot; target=&quot;_blank&quot;&gt;Check them out here!&lt;/a&gt;.&lt;/p&gt;

&lt;div style=&quot;padding: 20px;&quot;&gt;
 &lt;img src=&quot;https://vsoch.github.io/assets/images/posts/tunel/table.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;I’m mostly working on singularity an HTCondor apps at the moment because I use the open science grid (OSG) for development, as this is a personal project. Thanks to &lt;a href=&quot;https://twitter.com/westbynoreaster&quot; target=&quot;_blank&quot;&gt;Matthew West&lt;/a&gt; for showing me OSG - I was pretty handicapped to develop before finding it!&lt;/p&gt;

&lt;h2 id=&quot;django-template-with-a-socket&quot;&gt;Django template with a socket?&lt;/h2&gt;

&lt;p&gt;This kind of framework can be powerful if I develop a bunch of custom apps, but it’s much more powerful if I can enable YOU to easily do that too! Thus, I knew one of the first tasks I wanted to do is create a template, likely in each of Flask, Django, and FastAPI, that would plug immediately into Tunel. And while I have much work left to do, last night and this evening I figured out a technical issue that is going to empower us to make so many cool things and I wanted to share! Let’s talk about the problem, what I tried, and what ultimately worked.&lt;/p&gt;

&lt;h3 id=&quot;traditional-setup-with-uwsgi-and-nginx&quot;&gt;Traditional Setup with uwsgi and nginx&lt;/h3&gt;

&lt;p&gt;If you look at a family of Python + web interface apps, you’ll find this &lt;a href=&quot;https://uwsgi-docs.readthedocs.io/en/latest/&quot; target=&quot;_blank&quot;&gt;uwsgi&lt;/a&gt; guy in the middle (I don’t know the correct pronunciation but I say YOU-SKI). It’s a fairly rich tool, but in layman’s terms I think of it as a middleman between Python and a traditional web server. But actually, you don’t technically need the web server - and this is where things start to get interesting. For a traditonal setup, you might find a nginx (a web server) configuration file that &lt;a href=&quot;https://github.com/tunel-apps/tunel-django/blob/main/scripts/nginx/nginx.conf&quot; target=&quot;_blank&quot;&gt;looks like this&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;c1&quot;&gt;# the upstream component nginx needs to connect to&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;upstream django {&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;server unix:///tmp/tunel-django.sock;&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# configuration of the server&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;server {&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;# the port your site will be served on&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;listen      8000;&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;charset     utf-8;&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;server_name           localhost;&lt;/span&gt;

    &lt;span class=&quot;s&quot;&gt;client_max_body_size 10024M;&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;client_body_buffer_size 10024M;&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;client_body_timeout 120;&lt;/span&gt;

    &lt;span class=&quot;s&quot;&gt;...&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;location ~* \.(php|aspx|myadmin|asp)$ {&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;deny all;&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;s&quot;&gt;location /static/ {&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;autoindex on;&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;alias /var/www/static/;&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;s&quot;&gt;# Finally, send all non-media requests to the Django server.&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;location / {&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;uwsgi_pass  django;&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;uwsgi_max_temp_file_size 10024m;&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;include /code/scripts/nginx/uwsgi_params.par;&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;I’ve made a lot of web apps, and whether I use docker-compose with separate containers or a single one, I usually have to write a nginx configuration. The above gets started in the container entrypoint with my app calling uwsgi, and defining that same socket:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;uwsgi &lt;span class=&quot;nt&quot;&gt;--socket&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;socket&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; /code/scripts/uwsgi.ini
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;And of course things happen before that, but that’s the main last line. The uwsgi.ini is a configuration file
that makes it easier to define settings.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[uwsgi]
master = true
processes = 4
threads = 4
py-autoreload = 1
#socket = :3031
chdir = /code/
post-buffering = true
log-date = true
max-requests = 5000
http-timeout = 3600000
socket-timeout = 120
chmod-socket = 666
wsgi-file = tuneldjango/wsgi.py
ignore-sigpipe = true
ignore-write-errors = true
disable-write-exception = true
buffer-size=32768
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Without going into huge detail, the above says that the app that I wrote (in Python) is listening on that socket, so requests to the web server will either be directed to some static file, filtered out, or sent to our application. And we typically want to use nginx because it’s really good at serving static files and handling traffic.&lt;/p&gt;

&lt;p&gt;But now let’s step back. If you look under the server in the config above, you’ll notice we are serving
content on port 8000. This is why I can open the browser to localhost and that port and see my application.
But as we know with headless HPC, there are no ports. I can’t use this. So this was my first predicament, last night. I had created this application and it ran locally, but I needed to somehow get the entire thing routed through a tunneled socket to take a next step.&lt;/p&gt;

&lt;h3 id=&quot;uwsgi-only&quot;&gt;Uwsgi Only?&lt;/h3&gt;

&lt;p&gt;I’ll skip over the many hours of things that I tried and failed. I really liked having nginx so I first wanted to somehow send it to the user via a socket, but that never worked. I had an idea to just map the original socket and then have a second container on the host for nginx, but I decided that was too complex. What would up working is realizing that uwsgi can serve http directly, and that came down to a single addition to its config:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;listen=200
protocol=http
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Once I did that, I tried the same technique to map the socket being written to directly to a port via the ssh tunnel, and &lt;em&gt;boum&lt;/em&gt; I saw a page! But it was really ugly, because it had no style. This is where I was like OHNO I need nginx for static. But then I found &lt;a href=&quot;https://uwsgi-docs.readthedocs.io/en/latest/StaticFiles.html&quot; target=&quot;_blank&quot;&gt;this page&lt;/a&gt; and it was a message from the heavens - I could define the same static and media URls using uwsgi directly! That looked like this:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;uwsgi &lt;span class=&quot;nt&quot;&gt;--socket&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;socket&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--static-map&lt;/span&gt; /static&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/code/static /code/scripts/uwsgi-standalone.ini
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;At this point I held my breath, re-ran my app, and wow!&lt;/p&gt;

&lt;div style=&quot;padding: 20px;&quot;&gt;
 &lt;img src=&quot;https://vsoch.github.io/assets/images/posts/tunel/home.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;There it was - my entire app being served by a container running on a remote machine, only accessible to me through a physical socket. And guess what? I added a file browser, and it even worked to upload a dinosaur picture!&lt;/p&gt;

&lt;div style=&quot;padding: 20px;&quot;&gt;
 &lt;img src=&quot;https://vsoch.github.io/assets/images/posts/tunel/browser.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Here is the entire page for the app - you can see there are many flags you can add and customize to interact.&lt;/p&gt;

&lt;div style=&quot;padding: 20px;&quot;&gt;
 &lt;img src=&quot;https://vsoch.github.io/assets/images/posts/tunel/app.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;While it’s only accessible to you and there isn’t need for any kind of login, I did add the default username/password login to Django, and require it for logging in to the file browser. Of course I will eventually need this to be more formally security audited, but at least I don’t have anything interesting on my OSG home to be worried about. And is using just uwsgi a performance issue? I think probably not since the expected use case is only once person.&lt;/p&gt;

&lt;h3 id=&quot;a-future-for-apps&quot;&gt;A Future for Apps&lt;/h3&gt;

&lt;p&gt;This is just the beginning - my plan is to put together a list of use cases for a GUI on a cluster, and then just package them into the core template apps for the developer user to easilyc customize. I have big plans for working on this, and honestly I’m so excited that I find I’m staying up way too late and just egging for the work day to end so I can continue. This idea is so powerful, because it’s using existing technologies to deploy containerized apps on HPC, where you don’t need any special permission. Just to show y’all, here is what it looks like to launch my app template:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;tunel run-app osg singularity/socket/tunel-django &lt;span class=&quot;nt&quot;&gt;--tag&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;dev &lt;span class=&quot;nt&quot;&gt;--pull&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;I added the pull flag and a custom tag because I am actively developing, and my workflow is to quickly rebuild, push, and then run that command. That then shows me the ssh tunnel command that will immediately connect me to my app on a port in my browser.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;ssh &lt;span class=&quot;nt&quot;&gt;-NT&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-L&lt;/span&gt; 7789:/../tunel/singularity/singularity/socket/tunel-django/singularity-socket-tunel-django.sock sochat1@osg
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;And that’s seriously it. You as the developer user are empowered to make and deploy apps, and they have interfaces, and you don’t need to do something silly like open a port or actually deploy a web server. It’s so stupidly easy - I’m looking around at all these complex web app setups that people have made for HPC over the years and I wonder why they aren’t doing something simpler. Maybe it’s just a space of development that people gave up on, or there are some security things I’m missing. Either way, I’m going to charge forward working on this! It’s too simple, and the idea is to beautiful to do anything else by this point.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Ceph RocksDB Tuning Deep-Dive</title>
   <link href="https://hpc.social/2022/ceph-rocksdb-tuning-deep-dive/"/>
   <updated>2022-07-25T01:00:00-06:00</updated>
   <id>https://hpc.social/2022/ceph-rocksdb-tuning-deep-dive</id>
   <content type="html">&lt;p&gt;See my &lt;a href=&quot;https://ceph.io/en/news/blog/2022/rocksdb-tuning-deep-dive/&quot;&gt;post&lt;/a&gt; on the Ceph.io blog about tuning RocksDB in Ceph!&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>The Utility vs the Professional Services Firm</title>
   <link href="https://hpc.social/2022/the-utility-vs-the-professional-services-firm/"/>
   <updated>2022-07-03T01:00:00-06:00</updated>
   <id>https://hpc.social/2022/the-utility-vs-the-professional-services-firm</id>
   <content type="html">&lt;h2 id=&quot;as-research-computing-and-data-becomes-more-complex-and-diverse-we-need-more-professional-services-firms-and-fewer-utilties&quot;&gt;As research computing and data becomes more complex and diverse, we need more professional services firms and fewer utilties&lt;/h2&gt;

&lt;p&gt;(Note: This post is adapted from &lt;a href=&quot;https://www.researchcomputingteams.org/newsletter_issues/0127&quot;&gt;#127&lt;/a&gt; of the &lt;a href=&quot;https://www.researchcomputingteams.org&quot;&gt;Research Computing Teams Newsletter&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;I get to talk with a lot of research computing and data teams - software, data, and systems.  Sometimes in these conversations it’s pretty clear that some teams, or the team and their funder, or a team and I, are talking a bit past each other.  And that’s usually because they or we are (currently) operating with very different mental models of how they operate.&lt;/p&gt;

&lt;p&gt;Some research computing and data teams are operating as Utilities, and see the world through that lens; a growing number are operating as Professional Services Firms.  Others are moving from one to the other, and are at different places along that very abrupt transition.  Some kinds of groups (like bioinformatics cores) are much more likely to already be operating in service mode, while others (like research compute infrastructure teams) are more likely to still think of themselves as utilities.  It varies from place to place, though, depending on local conditions.  But they’re very different models!&lt;/p&gt;

&lt;figure style=&quot;width: 45%; float: right;&quot;&gt;
  &lt;img alt=&quot;Utility vs professional services.  Image Credit: left, John Moore (@thejmoore) at Unsplash.com; right, Jason Goodman @jasongoodman_youxventures at Unsplash.com&quot; src=&quot;https://www.dursi.ca/assets/imgs/utility-vs-professional-svc.png&quot; /&gt;
  &lt;figcaption&gt;&lt;i&gt;Utility service and professional services delivery are very different, and require different funding, management, and career development models.  Image credit: &lt;a href=&quot;https://unsplash.com/photos/0MKzwPmehRE&quot;&gt;left&lt;/a&gt; and &lt;a href=&quot;https://unsplash.com/photos/X8H8vPcelPk&quot;&gt;right&lt;/a&gt;.&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Utilities, like power companies or garbage collection or municipal potable water, were really the only sensible role models for the first decades of research computing and data teams.  Those teams were entirely about operating large equipment purchased from vendors.  Costs were mostly a big capital expense.  Everyone who needed the utility needed the same thing - undifferentiated flops and bytes, or 60Hz 120VAC.  Because everyone needed the same thing, economies of scale led to &lt;a href=&quot;https://en.wikipedia.org/wiki/Natural_monopoly&quot;&gt;natural monopolies&lt;/a&gt;; the most reasonable provision model was for the local jurisdiction/institution to own or control a single operator.  Differentiation or strategy, or gaining new customers, weren’t meaningful discussion topics.  The only thing that really makes a difference is scale, which leads to mergers.  Innovation happens slowly, top-down, at the industry-wide scale and usually from the vendors (“hey, did you hear about those new gas compressors Dyneco announced?”), and diffuses outwards.  Employees take pride in and the organization values operational skill and things ticking along smoothly.  Customers value reliability.  The only thing that matters for any individual operator is to operate effectively and to provide the standard service with the right amount of cost: high enough to absorb the available subsidy, low enough to not go broke.  If a customer needs something other than what the utility provides, rather than that being a market opportunity, it’s either an inconvenience or an irrelevance.  The power company or the water utility or the &lt;a href=&quot;https://vimeo.com/355556831&quot;&gt;old phone monopoly&lt;/a&gt; just doesn’t serve that need.&lt;/p&gt;

&lt;p&gt;Professional Service Firms — say engineering firms, or architects, or consultancies — are very different beasts.  They might very well have significant capital investment in specialized equipment, but their main selling point and their biggest cost is expertise.  Competing for and retaining that expertise, and developing that expertise in house and amongst their clients, are principal concerns.  As part of a “full-service” offering they they likely have some fairly standard services they offer at the low end, where operating cost and efficiency is vital.  But what the organization values, and the employees enjoy, is at the high-touch end — getting deeply involved with the client work, and being as much a collaborator or partner or “trusted advisor” as a service provider.  Different clients want very different things, and that high-touch high-expertise work is specialized and labour intensive, so the firms themselves need a clear focus; they &lt;em&gt;can’t&lt;/em&gt; meet all needs.  Clients can go elsewhere, so there is redundancy and competition, but less than you’d think at a distance.  In civil engineering a geotechnical firm is complementary, not competing, with one that specializes in water resource engineering.&lt;/p&gt;

&lt;p&gt;As in the rest of our lives, in research computing we need to have utilities.  As research data management matures, institutional or regional data depositories become mature and “enterprise” enough to become utilities, likely run by IT or the Library.  Teaching or CI/CD or MLOps resources for data science or software development are likely best served by this model.  The closer the operations are to standard, something that can be run by IT, the more likely it is to be a utility.  But one has to be careful.  Utilies are commodoties: they tend to get merged together wherever feasible, since scale matters and it’s all undifferentiated commodity provision.&lt;/p&gt;

&lt;p&gt;As research computing becomes broader and faster changing and more diverse, we need more and more professional services firms, too; nimble groups specialized to particular needs and ready to adapt as those needs change.  As even infrastructure is becoming less one-size-fits-all, and methods for making use of computing and data for diverse fields grow more complex and expertise intensive, the preconditions for the utility model are met in fewer situations than used to be.&lt;/p&gt;

&lt;p&gt;A lot of research computing teams are interested in providing something more like professional services, but were created in the Utility model, and are stuck there by their funders.  The institutional or external funders still have this very specific (and to their mind time tested and successful) operating model in their plans.  Utilities are funded very differently than professional services firms.  At utility scale, it doesn’t make sense to outsource things, or develop non-standard services (who wants non-standard power coming into their house!)  Funders requirements on eligible expenses may focus almost entirely on the capital spend, and not on operating funding that’s needed to make effective use of the capital, or to be more agile in how services are delivered.&lt;/p&gt;

&lt;p&gt;Even those teams who aren’t being held back by funders and who want to make the switch to professional services from their original utility model find it a hard transition. There’s no obvious, incremental path to go from providing a standard, stable commodity to changing, specialized, bundles of expertise.  Utilities operate very differently from professional services firms.  They value different things. The models for staff growth are different.  So they have to be managed quiet differently, and there’s no clear path internally from A to B.&lt;/p&gt;

&lt;p&gt;Besides funding, and internal considerations, utilities and professional services firms are also percieved and valued by their clients very differently.  Utilities’ existing customers don’t want change, and new customers aren’t yet interested in getting advanced app software development suggestions from what they perceive to still be the mobile telephony provider.&lt;/p&gt;

&lt;p&gt;But research computing and data is changing, increasingly quickly, and the utility approach only meets a piece of these growing needs.  Navigating the transition isn’t going to be easy, for RCD teams, leaders, or funders; but expressing it clearly and talking about it more will maybe mean we’re not talking past each other so often.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>SSH Tunnels</title>
   <link href="https://hpc.social/2022/ssh-tunnels/"/>
   <updated>2022-06-26T13:30:00-06:00</updated>
   <id>https://hpc.social/2022/ssh-tunnels</id>
   <content type="html">&lt;p&gt;Today I want to talk about ssh tunnels. Very abstractly, we would want to use an ssh
tunnel to securely send information. In the case of HPC, you are probably familiar with ssh,
(Secure Shell or Secure Socket Shell) when you login to your node. You might do something like this:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;ssh dinosaur@server.address.edu
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Or if you have a proper setup in your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;~/.ssh/config&lt;/code&gt; (with a named server) you might just do:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;ssh dinosaur
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;I like to use &lt;a href=&quot;https://en.wikibooks.org/wiki/OpenSSH/Cookbook/Multiplexing&quot; target=&quot;_blank&quot;&gt;ssh connection multiplexing&lt;/a&gt;
so the connection is kept alive for a bit, but I won’t go into detail because
this post isn’t specifically about the details of ssh. The use case I’m interested in (and the thing
that HPC is very bad at) is how to deploy something interactive on an HPC cluster.&lt;/p&gt;

&lt;h2 id=&quot;ssh-tunnel-with-ports&quot;&gt;SSH Tunnel with Ports&lt;/h2&gt;

&lt;p&gt;Given that a cluster has exposed ports (either the login node, or both the login node and compute nodes)
creating a tunnel is fairly straight forward! In the past I created a tool called &lt;a href=&quot;https://github.com/vsoch/forward&quot; target=&quot;_blank&quot;&gt;forward&lt;/a&gt; to handle all the manual steps to get this working, meaning:&lt;/p&gt;

&lt;ol class=&quot;custom-counter&quot;&gt;
  &lt;li&gt;Show the user &lt;a href=&quot;https://github.com/vsoch/forward#ssh-config&quot; target=&quot;_blank&quot;&gt;how to set up their ~/.ssh/config&lt;/a&gt; (once)&lt;/li&gt;
  &lt;li&gt;Define (once) parameters like a port, memory, GPUs, and if the cluster has isolated nodes&lt;/li&gt;
  &lt;li&gt;Start any number of provided apps that come with forward (e.g., jupyter, singularity, etc.)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;An interaction using forward might look like any of the following:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Run a Singularity container that already exists on your resource (recommended)&lt;/span&gt;
bash start-node.sh singularity-run /scratch/users/vsochat/share/pytorch-dev.simg

&lt;span class=&quot;c&quot;&gt;# Execute a custom command to the same Singularity container&lt;/span&gt;
bash start-node.sh singularity-exec /scratch/users/vsochat/share/pytorch-dev.simg &lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Hello World&quot;&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Run a Singularity container from a url, `docker://ubuntu`&lt;/span&gt;
bash start-node.sh singularity-run docker://ubuntu

&lt;span class=&quot;c&quot;&gt;# Execute a custom command to the same container&lt;/span&gt;
bash start-node.sh singularity-exec docker://ubuntu &lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Hello World&quot;&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# To start a jupyter notebook in a specific directory ON the cluster resource&lt;/span&gt;
bash start.sh jupyter &amp;lt;cluster-dir&amp;gt;

&lt;span class=&quot;c&quot;&gt;# To start a jupyter notebook with tensorflow in a specific directory&lt;/span&gt;
bash start.sh py3-tensorflow &amp;lt;cluster-dir&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Note that the last set of commands are pertaining to notebooks, which is where these tunnels come into play!
A notebook is going to be run on a compute node that looks something like the following:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;jupyter notebook &lt;span class=&quot;nt&quot;&gt;--no-browser&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$PORT&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;And if you ran this with a Singularity container, you’d also want to bind jovyan’s home to be the user’s, along with the jupyter config directory:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;singularity &lt;span class=&quot;nb&quot;&gt;exec&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--home&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HOME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;--bind&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HOME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;/.local:/home/jovyan/.local &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;--bind&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HOME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;/.jupyter:/home/jovyan/.jupyter &lt;span class=&quot;se&quot;&gt;\ &lt;/span&gt; 
    datascience_notebook.sif jupyter notebook &lt;span class=&quot;nt&quot;&gt;--no-browser&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$PORT&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--ip&lt;/span&gt; 0.0.0.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;As we described earlier &lt;a href=&quot;https://github.com/vsoch/forward#ssh-port-forwarding-considerations&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;,
there are subtle differences between making a tunnel (with a port) given that you have isolated nodes (or not).
You can determine this based on your ability to ssh into a non-login node (meaning where your job is running) from “the outside world”
that is your computer. If you cannot, your nodes are isolated, which we will discuss next.&lt;/p&gt;

&lt;h3 id=&quot;isolated-nodes&quot;&gt;Isolated Nodes&lt;/h3&gt;

&lt;p&gt;Let’s say that we need to create a tunnel (using ports) to an isolated node. This means that we are basically going
to establish a tunnel to the login node, and then from the login node another one to the compute node.
We might use a command that looks like this:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;ssh &lt;span class=&quot;nt&quot;&gt;-L&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$PORT&lt;/span&gt;:localhost:&lt;span class=&quot;nv&quot;&gt;$PORT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;RESOURCE&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; ssh &lt;span class=&quot;nt&quot;&gt;-L&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$PORT&lt;/span&gt;:localhost:&lt;span class=&quot;nv&quot;&gt;$PORT&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-N&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$MACHINE&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; &amp;amp;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;In the command above, the first half (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ssh -L $PORT:localhost:$PORT ${RESOURCE}&lt;/code&gt;) is executed on the local machine, which establishes a port forwarding to the login node. The “-L” in the above (from the &lt;a href=&quot;https://linuxcommand.org/lc3_man_pages/ssh1.html&quot; target=&quot;_blank&quot;&gt;man pages&lt;/a&gt;) :&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Specifies that connections to the given TCP port or Unix socket on the local (client) host are to be forwarded to the
given host and port, or Unix socket, on the remote side.
This works by allocating a socket to listen to either a TCP
port on the local side, optionally bound to the specified
bind_address, or to a Unix socket.  Whenever a connection is
made to the local port or socket, the connection is for‐
warded over the secure channel, and a connection is made to
either host port hostport, or the Unix socket remote_socket,
from the remote machine.&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;Or in layman’s terms:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Forward whatever is running on the second port on my resource to my local machine.&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;Since we are forwarding ports, this would require minimally the login node to expose ports.
The next line &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ssh -L $PORT:localhost:$PORT -N &quot;$MACHINE&quot; &amp;amp;&lt;/code&gt; is a second command run from the login node, 
and port forwards it to the compute node, since you can only access the compute node from the login nodes.
You’ll notice it looks just like the first, and this works because ssh commands can be chained.
The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-N&lt;/code&gt; says “don’t execute a remote command (and just forward the port).”
Finally, the last &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$MACHINE&lt;/code&gt; is the node that the jupyter notebook is running on.&lt;/p&gt;

&lt;h3 id=&quot;not-isolated&quot;&gt;Not Isolated&lt;/h3&gt;

&lt;p&gt;For HPCs where the compute node is not isolated from the outside world the ssh command for port forwarding first establishes a connection the login node, but then continues to pass on the login credentials to the compute node to establish a tunnel between the localhost and the port on the compute node. The ssh command in this case utilizes the flag &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-K&lt;/code&gt; that forwards the login credentials to the compute node:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;ssh &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$DOMAINNAME&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-l&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$FORWARD_USERNAME&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-K&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-L&lt;/span&gt;  &lt;span class=&quot;nv&quot;&gt;$PORT&lt;/span&gt;:&lt;span class=&quot;nv&quot;&gt;$MACHINE&lt;/span&gt;:&lt;span class=&quot;nv&quot;&gt;$PORT&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-N&lt;/span&gt;  &amp;amp;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;I’m not sure in practice how common this is anymore. At least at my current employer it’s not even the case
that ports are exposed on the login node! It’s probably better that way, because in cases where you do get ports it’s sort of a 
“pick a port above this range and hope that no other user picks the same one!” It’s messy. 
So let’s talk about the case of not having ports exposed next, since this was the entire reason I wanted to write this post!&lt;/p&gt;

&lt;h2 id=&quot;ssh-tunnel-with-socket&quot;&gt;SSH Tunnel with Socket&lt;/h2&gt;

&lt;p&gt;More than a year ago, I had this realization that a lot of people at Stanford used the “forward” tool, and just for notebooks (and this
was before they were available via Open OnDemand, which is what I’d recommend to a Stanford user at this point). I decided I wanted to make a new 
open source tool, “tunel” (an elegant derivation of “tunnel”) &lt;a href=&quot;https://github.com/vsoch/tunel&quot; target=&quot;_blank&quot;&gt;vsoch/tunel&lt;/a&gt; to make it easy
to run what I call “apps” on an HPC cluster. Are there better ways of exposing user interfaces on HPC? Yes, indeed. But not everyone
has easy access. It was also a stubborn “I want this to work” proof of concept. This new tool would be like forward, but a little nicer.
Because I, along with every other HPC developer and user, wishes we could have nice things 😭️.&lt;/p&gt;

&lt;p&gt;At this time I had just started a new role at a national lab, and I realized that none of my old techniques for launching
the job worked because of the lack of exposed ports. Thinking this was impossible, I abandoned it for a year. But then this last week I found 
&lt;a href=&quot;https://github.com/jupyter/notebook/pull/4835&quot; target=&quot;_blank&quot;&gt;this&lt;/a&gt;! I was motivated! I was excited! The basic launch command of the notebook looks like this:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;jupyter notebook &lt;span class=&quot;nt&quot;&gt;--sock&lt;/span&gt; /tmp/test.sock &lt;span class=&quot;nt&quot;&gt;--no-browser&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;And then with a different looking tunnel, we could forward this socket to the host, and map it to a port! My excitement was then brought down
by what led to two days of struggling. I first tried my entire tunel workflow, meaning launching a job on a node,
and then running that command, and providing the instruction to the user to create the tunnel as follows:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;ssh &lt;span class=&quot;nt&quot;&gt;-L&lt;/span&gt; 8888:/tmp/test.sock &lt;span class=&quot;nt&quot;&gt;-N&lt;/span&gt; user@this_host
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;That didn’t work (and remember this socket was created on the isolated node, that’s important to remember for later). So I started looking at the socket with “nc”  - “arbitrary TCP and UDP connections and listens” from the login node. The “-U” below is for UNIX sockets:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;nc &lt;span class=&quot;nt&quot;&gt;-U&lt;/span&gt; /tmp/test.sock
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;And from the head node I saw:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Ncat: Connection refused.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;So then I knew I needed a simpler, dummier example. I got rid of tunel and just ran the notebook command on the head node.
Dear reader, it still did not work. I &lt;a href=&quot;https://github.com/jupyter/notebook/issues/6459&quot; target=&quot;_blank&quot;&gt;opened an issue&lt;/a&gt; and asked &lt;a href=&quot;https://twitter.com/vsoch/status/1540546526044250112&quot; target=&quot;_blank&quot;&gt;Twitter for help&lt;/a&gt;. Someone else on Twitter reported that &lt;a href=&quot;https://twitter.com/al3x609/status/1540846694262243328&quot; target=&quot;_blank&quot;&gt;it worked for them&lt;/a&gt;, and that (in my opinion) is the challenge and story of HPC - given the huge differences in setups, it’s hard to reproduce what another person does unless you scope to a very specific
environment or technology and hugely go out of your way to do it. I’m always grateful when someone tries to help, but when the ultimate answer is just
“But it works on my machine!” I (and I think all of us) are like:&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;font-size: 50px; color: darkorchid;&quot;&gt;(╯°□°)╯︵ ┻━┻&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;🤣️&lt;/p&gt;

&lt;p&gt;Please know that is intended to be funny, and I really am grateful for the attempt to help! Anyway, the first night I was devastated because I was so excited about the possibility of this working! But of course (as it usually does) my quasi-sadness turned again into relentless stubborn-ness, and for my Saturday
I embarked on trying everything. I call this the stubborn brute force approach, and it actually leads to some pretty good outcomes?&lt;/p&gt;

&lt;h3 id=&quot;socket-from-login-node&quot;&gt;Socket from Login Node&lt;/h3&gt;

&lt;p&gt;First from the login node, I started reading about flags in detail, again from the &lt;a href=&quot;https://linuxcommand.org/lc3_man_pages/ssh1.html&quot; target=&quot;_blank&quot;&gt;man pages&lt;/a&gt;. It occurred to me that the suggested command included “-L” (discussed earlier) but there were a ton of other flags to try, and maybe I need them for my setup? The command that wound up working (after much trial and error) was just:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Running on login node&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;ssh &lt;span class=&quot;nt&quot;&gt;-NT&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-L&lt;/span&gt; 8888:/tmp/test.sock user@server
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;And here again was the suggested command:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;ssh &lt;span class=&quot;nt&quot;&gt;-L&lt;/span&gt; 8888:/tmp/test.sock &lt;span class=&quot;nt&quot;&gt;-N&lt;/span&gt; user@this_host
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;So they are very similar - and the main difference is the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-T&lt;/code&gt; is to “Disable pseudo-terminal allocation.”
So I suspect (also based on the version of ssl I’m using) that without the flag, you might be making a request for a pty to the server
(&lt;a href=&quot;https://stackoverflow.com/questions/10330678/gitolite-pty-allocation-request-failed-on-channel-0/10346575#10346575&quot; target=&quot;_blank&quot;&gt;more details here&lt;/a&gt;) and then it could abort. Adding the flag just skips this, because we don’t need that - we just need the simple forward. And yes, this indeed feels very specific to your ssh setup, version of ssh, and server configuration. Of course, this was only the beginning of figuring things out, because I had no idea how to get this working from one level deeper - an isolated compute node.&lt;/p&gt;

&lt;h3 id=&quot;socket-with-isolated-nodes&quot;&gt;Socket with Isolated Nodes&lt;/h3&gt;

&lt;p&gt;Remember that when we created the socket on the isolated node and we tried this out from the login node:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;nc &lt;span class=&quot;nt&quot;&gt;-U&lt;/span&gt; /tmp/test.sock
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;And the result was this:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Ncat: Connection refused.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;My spidey senses were telling me that this should work. Indeed, when I ssh into the isolated node from the login node,
that same command allowed me to connect (meaning it hung / there was no error output). So my first task, I decided, was to try
and “forward” this socket to the login node. Again, back to the man pages! I wound up with something like this (run from the login node):&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;ssh isolated-node &lt;span class=&quot;nt&quot;&gt;-NT&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-L&lt;/span&gt; /home/dinosaur/login-node.sock:/home/dinosaur/jupyter.sock
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The above is again using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-L&lt;/code&gt; but instead of a port (which aren’t exposed) we are using a socket! It’s kind of neat you can switch out those two. 
When I tried the same nc command from the login
node, we had progress (no connection refused message!) 🎉️ And then I moved this up one level to see if I could make this same request from my local machine, sort of combining the first command that worked with the login node notebook with this one. That looked like this (and yes this took more trial and error):&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;ssh &lt;span class=&quot;nt&quot;&gt;-NT&lt;/span&gt; user@server ssh isolated-node &lt;span class=&quot;nt&quot;&gt;-NT&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-L&lt;/span&gt; /home/dinosaur/login-node.sock:/home/dinosaur/jupyter.sock
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;And to confirm it was working, I’d ssh into the server and again run that nc command to ensure that the newly forwarded socket was readable from
the login node. After this, again with more trial and error, I tried running a second command to just forward that (now working socket) to my host.
That eventually looked like this:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# And another for the local socket&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;ssh &lt;span class=&quot;nt&quot;&gt;-NT&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-L&lt;/span&gt; 8899:/home/dinosaur/login-node.sock user@server
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;And then (all together now!) I tried putting them together.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;ssh &lt;span class=&quot;nt&quot;&gt;-NT&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-L&lt;/span&gt; 8899:/home/dinosaur/login-node.sock user@server ssh isolated-node &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
       &lt;span class=&quot;nt&quot;&gt;-NT&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-L&lt;/span&gt; /home/dinosaur/login-node.sock:/home/dinosaur/jupyter.sock
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;And then I spent some time integrating it into tunel, and &lt;em&gt;surprise!&lt;/em&gt; the first implementation didn’t work. The first bug was that I needed to clean up old sockets each time the “same” app was run (determined by the job name and organizational namespace so the user can only run one of a particular interactive app at once, and not forget about previous runs). The second issue was about opening the tunnel - it didn’t seem to work if the process exited and/or it was run in a subshell (that also probably exits). I realized that (for the time being) running this connection step on behalf of the user, since it’s something the user should have more control over, probably wasn’t the right way to go. If the user hasn’t added something like an rsa key to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;~/.ssh/authorized_keys&lt;/code&gt; on their clusters, it would also ask for a password interactively, making it harder for me to manage. So for simplicity sake, and assuming that we really should put the user in control of deciding when to start/stop the tunnel, I simply print the full ssh command in the terminal and let them copy paste it. A successful connection might then prompt them for their password for that second ssh, which (by default) I don’t think is carrying forward auth from the first.&lt;/p&gt;

&lt;p&gt;So that was my adventure! Mind you, this entire adventure was only about two days, and that included time to write this post, so I still have lots in front of me to work on. However, with these updated commands (and some nice tweaks from Python’s &lt;a href=&quot;https://github.com/Textualize/rich&quot; target=&quot;_blank&quot;&gt;rich&lt;/a&gt; library) I quickly had a nice set of commands to run and stop an app with an interactive jupyter notebook, and using sockets on isolated nodes!&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;tunel run-app server slurm/socket/jupyter
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;tunel stop-app server slurm/socket/jupyter
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;As a sidenote, one thing I like about rich is that it puts the aesthetic as a first class citizen.
So many tools just don’t consider this, and I love that with rich I can think about colors, presentation,
and even animations like spinners!&lt;/p&gt;

&lt;p&gt;Getting a socket working  means I’ll be able to continue working on this library (hooray!) so if you have ideas or requests for apps
you’d like to run on HPC, assuming just this basic technology, please give me a ping and I’d love to chat and support them.
I’m also going to be requesting an allocation on the Open Science Grid, which hopefully will give me other kinds of clusters
to test on. I hope this was interesting to read, thanks for doing that!&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Research Software Registries</title>
   <link href="https://hpc.social/2022/research-software-registries/"/>
   <updated>2022-06-19T13:15:00-06:00</updated>
   <id>https://hpc.social/2022/research-software-registries</id>
   <content type="html">&lt;p&gt;This post spurred from some original thinking about &lt;a href=&quot;https://rse-ops.github.io/proposals/proposals/drafts/research-software-registry/&quot; target=&quot;_blank&quot;&gt;research software registries&lt;/a&gt;, and my recent discovery of the &lt;a href=&quot;https://scicodes.net/&quot; target=&quot;_blank&quot;&gt;SciCodes Consortium&lt;/a&gt;, which I’m excited to find (and a bit surprised I didn’t earlier given my experience with research software and registries)! Since I’ve developed registries and been involved extensively in communities that develop standards and tooling for them, I’ve naturally been ruminating over ideas for several months, and hoping to find others that are motivated to think about similar things. This is the motivation of this post - to ruminate, share my thinking, and think together about ideas. You can read the content, or listen to the ideas below.&lt;/p&gt;

&lt;div style=&quot;font-size: 10px; color: #cccccc; overflow: hidden; white-space: nowrap; font-family: Interstate,Lucida Grande,Lucida Sans Unicode,Lucida Sans,Garuda,Verdana,Tahoma,sans-serif; font-weight: 100;&quot;&gt;&lt;a href=&quot;https://soundcloud.com/vsoch&quot; style=&quot;color: #cccccc; text-decoration: none;&quot; target=&quot;_blank&quot; title=&quot;vsoch&quot;&gt;vsoch&lt;/a&gt; · &lt;a href=&quot;https://soundcloud.com/vsoch/research-software-registries&quot; style=&quot;color: #cccccc; text-decoration: none;&quot; target=&quot;_blank&quot; title=&quot;Research Software Registries&quot;&gt;Research Software Registries&lt;/a&gt;&lt;/div&gt;

&lt;h2 id=&quot;why-do-we-want-research-software-registries&quot;&gt;Why do we want research software registries?&lt;/h2&gt;

&lt;p&gt;Research software registries have value when they are deployed for a specific context. However,
I’m not convinced that a research software registry, at the most basic form providing archives with DOIS and metadata, is a useful thing in and of itself. It’s adding complexity and redundancy to an already cluttered ecosystem. The reason is because the source of truth of software is usually the source code in version control, e.g., the GitHub repository, which often already has support for features we need to enable easy citation (CITATION.cff), tagged releases, and programmatically accessible metadata. In this context, any kind of registry that provides another identifier and points to the first is providing redundant information. The only potential benefit is grouping and curation, which I would then argue should still point to the version control and/or a specific release as a source of truth.&lt;/p&gt;

&lt;p&gt;I’m also not convinced that we have established an actual use case of “searching a registry for software.” What happens in labs and communities is that you establish communities around the software, and then there are established workflows or slack communities or GitHub organizations to join around that. Most labs already have chosen languages, and even software pipelines that new members extend or work on. I would even go as far to say that for some (myself included) I don’t find research software, but it finds me. It appears as a link in some social media or chat channel, and I click the link and then there are about 15 seconds during which I make a determination if the software can help me to solve a problem that I have, or if it looks easy, professional, and/or fun and I simply want to try it out. If the answer is “yes” then I add it to a list in a Google Document with other things to try out when I have time. If not, I close the lab and life moves on. But I want to point out that nowhere in this workflow do I explicitly go looking for software. The software often finds me, and then I keep a mental cache of “tools that I’ve seen” and go back to it when the use case arises.&lt;/p&gt;

&lt;p&gt;So being able to answer this question about wanting research software registries is especially challenging because I’m not sure I’ve ever wanted one.
Unless there is a specific kind of context around a registry (e.g., search for a specific name in a package manager to use, or look for an already assembled workflow) I haven’t been able to convince myself (yet) that I would find a use for one. I could be wrong about this, however, because as we know, people (myself included) are fairly bad at predicting the future, and perhaps there could be some future where “checking a research software registry” is a part of a daily workflow. I am skeptical because I think that a context is needed. Even if some central source of software ability truth was established, would it not be the case that a graduate student or researcher needs to go there with a use case or context in mind? I can’t imagine just mindlessly browsing for the sake of browsing. It’s akin to search engines - we are usually looking for something very specific. We don’t search without a purpose. The question here then is, what is the purpose?&lt;/p&gt;

&lt;h2 id=&quot;research-software-registries-with-a-purpose&quot;&gt;Research Software Registries with a Purpose&lt;/h2&gt;

&lt;p&gt;A very good example of purpose comes down to workflows. This is the “I need to perform this specific function and I want to use what many others have done before me and not re-invent the wheel.” The minimum example of a workflow registry would be a search interface that indexes pipelines that are perhaps stored in version control. And extended version of that includes being able to provide structured inputs, outputs, and arguments, so the registry can programmatically provide this information to tools. You can then also quickly see how changing this to be general inputs/outputs of software (and functions within) and entrypoints of containers can quickly become a more generalized registry for software that could be used by any workflow manager that knows how to consume its information. However, there is a fine line here, because when we talk about I/O we are going
squarely into workflow management territoty, and again in my opinion, we have to be careful about that scope. The closest thing that comes to mind for providing workflows as a service is something like &lt;a href=&quot;https://openneuro.org/&quot; target=&quot;_blank&quot;&gt;openneuro&lt;/a&gt; that has a beautiful idea of “Get your data into this standard format and we will serve it and provide other easy ways to analyze it.” This kind of success story tells me that perhaps there is something to say for developing anything related to processing or pipelines in the context of a community. You can’t create the perfect registry for every scientific discipline, or perhaps you can do a mediocre job at trying, but perhaps if you scope to a specific one you can do a very good job. I’ve found the same to be true with software - it’s often better to do one or few things very well than more things kind of mediocre.&lt;/p&gt;

&lt;h3 id=&quot;a-provider-of-identifiers&quot;&gt;A Provider of Identifiers?&lt;/h3&gt;

&lt;p&gt;I’m skeptical when I hear that people want to apply our traditional model of publication (e.g., having a DOI) to software. The reason isn’t because I don’t value means to support reproducibility (and knowing the exact version of something that was used) but rather that we already have means to tag specific versions of software, and means that fit into a well-established ecosystem: package managers, versions, and releases. To think that a single frozen version of software is “the correct unit to provide” I also disagree with. Software is a living, and changing entity, and when it truly does “freeze” and stops being worked on, unlike a DOI in the academic space, this is sort of its death. The correct entrypoint for a piece of software, in my opinion, is the current version on version control, from where you could decide to pin a particular release or install a particular version from a package manager. But to provide a single frozen DOI that is wrapping some other version / release of the software? It doesn’t make sense to me. It’s adding additional complexity that’s not needed. So my opinion (as I’ve shared before) is that we should be thinking more about preserving specific timepoints in package managers, and not adding on an artificially created layer of “DOI” that seems (in my opinion) more of a reflection of our need to shove things into an academic framework we are comfortable with than anything else.&lt;/p&gt;

&lt;p&gt;So (I hope) that the purpose of a research software registry would not just be to provide DOIs. That doesn’t help me get my work done at the end of the day. All that said, I don’t think there can be a singular answer for purpose. I think the purpose ultimately comes down to the institution (or community) and the specific goals of the registry. For this reason there is no one answer for what a registry should look like or provide, and it is (or will be) challenging to define attributes that “any registry should have.”&lt;/p&gt;

&lt;h3 id=&quot;what-is-my-purpose&quot;&gt;What is my purpose?&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;You cut butter&lt;/em&gt;!&lt;/p&gt;

&lt;p&gt;Just kidding :_) I’ve been ruminating on this idea for quite some time, and namely because I’m motivated to build a new kind of research software registry, but first I need to convince myself of a meaningful purpose. While I don’t have my convincing answer yet (but I do have a sense of direction) the way I’ve been thinking about this is to provide a set of questions or use cases that seem plausible. It seems like most people are asking “What kind of information should we have in a registry” but I think this isn’t exactly the question I’m interested in - I want to know:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;What do you want to do next with the software you find?&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;This is important because it’s going to drive the context and purpose of the registry. Here are a few examples:&lt;/p&gt;

&lt;ol class=&quot;custom-counter&quot;&gt;
  &lt;li&gt;&lt;strong&gt;I want to quickly try this out&lt;/strong&gt; → a registry that can deploy a developer environment&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;I want to find if this is in a package manager&lt;/strong&gt; → a reproducible install&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;I want to use this with a workflow manager&lt;/strong&gt; → this is some kind of workflow hub&lt;/li&gt;  
  &lt;li&gt;&lt;strong&gt;I want to see inputs / outputs / entrypoints&lt;/strong&gt; → support to workflow tools&lt;/li&gt;  
  &lt;li&gt;&lt;strong&gt;I want to install this on HPC&lt;/strong&gt; → I want a module deployment or similar&lt;/li&gt;  
  &lt;li&gt;&lt;strong&gt;I want to cite this&lt;/strong&gt; → use case akin to CITATION.cff&lt;/li&gt;  
  &lt;li&gt;&lt;strong&gt;I want to understand dependencies of an ecosystem&lt;/strong&gt; → a registry deploying something akin to citelang&lt;/li&gt;  
  &lt;li&gt;&lt;strong&gt;I want to see all my options to do X&lt;/strong&gt; → a domain or categorical registry&lt;/li&gt;  
  &lt;li&gt;&lt;strong&gt;I want to see new and noteworthy libraries&lt;/strong&gt; → a registry with advanced filtering and ranking&lt;/li&gt;  
  &lt;li&gt;&lt;strong&gt;I want to see change over time&lt;/strong&gt; → a registry with a layer of analysis tools&lt;/li&gt;  
&lt;/ol&gt;

&lt;p&gt;Indeed many of the above contexts require additional information. For example, if we want to be able to ask what software is specifically used to perform X, we need a set of functions that are common to a domain, and then to annotate specific software (or even functions) that do it. If we want to then ask “Which of these is the best?” we need to then generate benchmarks to measure this functionality. E.g., how long does it take to run? What are the inputs and outputs and are they correct? What are resource needs? It would be an incredibly cool thing to be able to ask these questions, but an enormous amount of work for any particular scientific domain. As an example of thinking about functional needs, we might look to brain imaging, which is arguably a subfield of neuroinformatics. We might define custom processing functions like thresholding, registration, normalization, or creating regions of interest, tag specific functions that can do each, and then collect and share metrics about the degree to which easy is successful to do each. Arguably, if I wanted to do this I would create wrappers to workflow managers (akin to Snakemake Wrappers) that not only measure metrics, but make it easy for people to quickly use it in their work.&lt;/p&gt;

&lt;h2 id=&quot;it-needs-to-be-easy&quot;&gt;It needs to be easy&lt;/h2&gt;

&lt;p&gt;Whether I’m thinking about being a user of a research software registry or a developer, it just needs to be easy. Here are some ideas around that.&lt;/p&gt;

&lt;h3 id=&quot;re-inventing-the-wheel&quot;&gt;Re-inventing the wheel?&lt;/h3&gt;

&lt;p&gt;I come with the experience of deploying a custom container registry (Singularity Hub) years ago, and then being involved in standards committees (the Open Container Initiative) that develop generalized specifications that now drive the most common software (container) registries. I’ve also developed registry proxies that do interesting things, along with a Python OCI registry, and I’m the main developer for oras-py (ORAS == OCI Registry as Storage). So believe me when I say that in terms of storing blobs and metadata about them, I don’t think we should re-invent the wheel. Any new registry I create is going to start with these standards. You might disagree, and that’s OK. But I think people have thought long and hard about these things, and we are stronger for working together on them over always making our own new thing.&lt;/p&gt;

&lt;p&gt;As a supplement to that, I want to point out one of the biggest challenges in our community. The majority of research software, I would argue, doesn’t get used beyond the lab it’s created for. Said lab might submit or include it in a paper, and then they get their publication and move on. This is reflective of many things, and I’ll review them here. The first is our funding model - we maybe can fund working on a piece of software only up until the funding dries out, and then it becomes an abandoned repository, if it’s made publicly available. The second is our incentive model - the academic community is focused on writing papers, so once you get there, you don’t have reason to consider the long term impact of the software. The third is communication. It is actually much easier to throw together your own library than to have to search and then try contributing to someone else’s.
I say this because I don’t think the way that things are are necessarily the fault of anyone - we are all agents responding to incentives and resources available.&lt;/p&gt;

&lt;p&gt;But then on the flip side - these observations beg to ask what leads to software that is successful, on a community level? I think a few things can happen. Either someone puts time and energy into establishing community, period, meaning bringing together people that are working on common goals and explicitly asking “How can we do this together,” or what I’ve seen with more commercial open source - having enough money or power that you can create strong branding and community just by way of having the funds for it.  I’ve talked about this a &lt;a href=&quot;https://vsoch.github.io/2019/transparency/&quot; target=&quot;_blank&quot;&gt;few times before&lt;/a&gt; and it’s not necessarily bad, but it’s unfair at best. Software that maybe would not be successful by its own merit rises to the top, and really great software that doesn’t have those resources does not. That said, I’ve also seen sort of mediocre software get much better and earn its reputation, so I can’t say it’s a completely wrong dynamic.&lt;/p&gt;

&lt;h3 id=&quot;is-the-answer-mooooar-metadata&quot;&gt;Is the answer Mooooar Metadata?&lt;/h3&gt;

&lt;p&gt;As we design the “perfect set of information” we want provided for any piece of software, we need to put people first.
We have to ask ourselves what are people willing to do, and generally people aren’t wanting to spend inordinate amounts of extra time defining metadata or inputs/outputs for their custom scripts. This was a point also brought up by &lt;a href=&quot;https://twitter.com/orchid00&quot; target=&quot;_blank&quot;&gt;Paula&lt;/a&gt; in the SciCodes meeting and I am 100% behind it. If we require extensive metadata about software, it needs to be done in an automated fashion. In practice when I think of archives for software, I’m just not that motivated to provide more than the absolute minimum to click the “submit” button.&lt;/p&gt;

&lt;h2 id=&quot;do-people-know-what-they-want&quot;&gt;Do people know what they want?&lt;/h2&gt;

&lt;p&gt;One of the hardest things about this kind of problem is that people don’t often know what they want. 
And actually - I’d extend that to software in general. Think of common tools like git (version control) or containers.
Could most people have told you in advance about the designs for these tools? I suspect likely not.
This is often the game that software developers play - we imagine new ways of doing things that scratch an itch
or have a problem that we have, and then hand over our duct taped laden prototype to others and we’re  like
hey, is this useful to you? And often the response in radio silence, but then sometimes it’s a resounding, “WoW, yes!”
So I’m going to throw out this idea that people generally don’t know what they want until they see it, touch it and try it.
This is also why I want to inspire you to take some time to think about your specific needs and motivation for wanting
(on a high level) to browse and interact with research software. What are the compelling reasons for this registry,
for you?&lt;/p&gt;

&lt;p&gt;This is actually really fun to think about, because what even is a research software registry? 
Is it a place to find software to plug into workflows? Does it provide ABI or more general function signatures to help you plug into workflows? Does it provide a citation? A container? An interactive environment? Dependency graph? Something else? This is inded why this problem is so hard - there are so many ways to thinkabout this basic concept. And that’s kind of what makes it fun too? But also what makes it hard. Personally speaking sinceI’m more interested in building things I find myself ruminating about details for a specific use case. And since I’m a developer and craving better support for developer environments, this tends to be where my brain goes. And have you noticed I haven’t given
a direct answer for what is a research software registry? It’s 1. because I don’t know, and 2. because we are trying to define a registry for a kind of output that we don’t even have an agreed upon definition for yet! So perhaps the definition will happen on the level of the deployment or institution? Anyway, I hope you take the opportunity to discuss with your peers, pets, and even yourself, to try and answer this question.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;To summarize, I’m spending a lot of time thinking about this, and less in an “I’m an academic that wants DOIs and metadata” and more in a “I am a software engineer that wants to build something that I actually find useful.” Might I scratch itches along the way? Sure. And I do have some early ideas that I plan to hack on before sharing publicly. In the meantime, I do hope you are interested in some of these ideas and take time to write or introspect yourself.&lt;/p&gt;

&lt;p&gt;And on a higher level, I really like this format of writing and speaking, where the speaking isn’t formal enough to be a talk that you put together and practice for weeks (I put this all together in an afternoon) but it still is a media format that literally gives a voice.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Life and leaving NERSC</title>
   <link href="https://hpc.social/2022/life-and-leaving-nersc/"/>
   <updated>2022-05-27T06:42:00-06:00</updated>
   <id>https://hpc.social/2022/life-and-leaving-nersc</id>
   <content type="html">&lt;p&gt;When word started to spread that I was leaving my job at NERSC for Microsoft, a lot of people either directly or indirectly attributed my decision to being one motivated by money.  Rationalizing my decision to leave is certainly a lot easier with this &quot;Glenn was lured away with bags of cash&quot; narrative, but that wasn't really a factor when I chose to move on.  Rather, my decision is a reflection of where I see the world of HPC going in the coming decade and where I personally wanted to position myself.  For my own therapeutic reasons (and perhaps the benefit of anyone interested in what it's like to work within, and subsequently leave, the DOE HPC complex), I'll try to write it all out here.&lt;span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;h2 style=&quot;text-align: left;&quot;&gt;Working at NERSC&lt;/h2&gt;
&lt;p&gt;First things first: NERSC has been a wonderful place to work.&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;&lt;b&gt;&amp;lt;div style=&quot;text-align: center;&quot;&amp;gt;&lt;b&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;A typical view from outside NERSC’s facility in Berkeley after work during the winter months.  Yes, it really does look like this.&lt;/span&gt;&lt;/b&gt;&amp;lt;/div&amp;gt;
&lt;/b&gt;&amp;lt;p&amp;gt;When I started in mid-2015, I came in with about three years of prior work experience (two at SDSC doing user support and one at a biotech startup) and knew a little bit about a lot of things in HPC.  But I didn’t really know the basics of I/O or storage–I couldn’t tell you what “POSIX I/O” really meant or how GPFS worked.  The fact that I got to help author &lt;a href=&quot;https://www.nersc.gov/news-publications/nersc-news/nersc-center-news/2017/new-storage-2020-report-outlines-future-hpc-storage-vision/&quot;&gt;NERSC’s ten-year strategy around storage&lt;/a&gt; in just two years, was invited to present &lt;a href=&quot;https://insidehpc.com/2019/08/designing-future-flash-storage-systems-for-hpc-and-beyond/&quot;&gt;my view on how to bridge the gap between HPC and enterprise storage&lt;/a&gt; at Samsung’s North American headquarters a year later, and was trusted to oversee &lt;a href=&quot;https://www.nextplatform.com/2021/06/07/a-35-petabyte-all-flash-balancing-act/&quot;&gt;the design and execution of the world’s first 35 petabyte all-flash Lustre file system&lt;/a&gt; through my first four years is a testament to how much opportunity is available to learn and grow at NERSC.&amp;lt;/p&amp;gt;&lt;/p&gt;
&lt;p&gt;There are a couple of reasons for this.&lt;/p&gt;
&lt;h3 style=&quot;text-align: left;&quot;&gt;Stable funding&lt;/h3&gt;
&lt;p&gt;Perhaps foremost, NERSC (and DOE's Leadership Computing Facilities, ALCF and OLCF) enjoy healthy budgets and financial stability since worldwide leadership in scientific advancement is generally a national priority by both major political parties in the US.  This means that, regardless of who is president and which party holds majorities in Congress, the DOE HPC facilities can pay their employees and deploy new supercomputers.  This solid funding makes it much easier to invest in staff development and long-term planning; I was able to become a resident I/O expert at NERSC because I was never forced to chase after the funding du jour to make ends meet.  Congress trusts NERSC to allocate its funding responsibly, and NERSC prioritized letting me learn as much as I could without distraction.&lt;/p&gt;
&lt;h3 style=&quot;text-align: left;&quot;&gt;Instant credibility and access&lt;/h3&gt;
&lt;p&gt;Second, &lt;a href=&quot;https://twitter.com/hpcprogrammer/status/1061278775353196544?s=20&amp;amp;t=_YGQXWvykuCElqltJ-x09Q&quot;&gt;having a NERSC affiliation gives you instant credibility and access&lt;/a&gt; in many cases.  It's not necessarily fair, but it's definitely true.  Within my first year at NERSC, I was invited to give &lt;a href=&quot;https://archive.siam.org/meetings/pp16/pp16_program.pdf&quot;&gt;a presentation about I/O performance monitoring in Paris&lt;/a&gt; because the organizer wanted a lineup of speakers from all the big players in HPC.  I had never been to Europe at that point in my life, but being the I/O guy from NERSC (and being able to present well!) was enough to get me there.  And it was during that trip to Paris that I got to meet--and literally have conversation over dinner with--&lt;a href=&quot;https://www.linkedin.com/in/larry-kaplan-b101936&quot;&gt;more&lt;/a&gt; &lt;a href=&quot;https://people.llnl.gov/tgamblin&quot;&gt;industry&lt;/a&gt; &lt;a href=&quot;https://en.wikipedia.org/wiki/David_E._Keyes&quot;&gt;bigshots&lt;/a&gt; that I can remember.  And that trip to Paris was not an outlier; pandemic aside, NERSC let me go to Europe at least once or twice every year I've worked there.&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;&lt;b&gt;&amp;lt;div style=&quot;text-align: center;&quot;&amp;gt;&lt;b&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;The first photo I ever took of Notre Dame on the first day I’d ever set foot in Europe.  NERSC sent me there less than a year after I started.&lt;/span&gt;&lt;/b&gt;&amp;lt;/div&amp;gt;
&lt;/b&gt;&amp;lt;p&amp;gt;Of course, this is not to say that every employee at a DOE HPC facility is wining and dining in Paris every summer.  Many of these opportunities are earned by showing the value of the work you’re doing, just like at any job.  But owing to healthy budgets, travel expenses are rarely the limiting factor in chasing after these opportunities.  In addition, going out into the world and talking about what you do is part of the job at a DOE facility; being a leader in the field of HPC is part of the mission of NERSC, ALCF, and OLCF, so doing high-risk, first-of-a-kind work &lt;i&gt;and telling the world about it&lt;/i&gt; is uniquely valued within DOE in a way that it is not in industry.&amp;lt;/p&amp;gt;&lt;/p&gt;
&lt;h3 style=&quot;text-align: left;&quot;&gt;Smart people&lt;/h3&gt;
&lt;p&gt;A product of these two factors (stable budget and instant credibility) results in coworkers and colleagues who are generally very experienced and capable.  There's an interesting mix of laissez-faire management and rigorous process-driven management as a result.&lt;/p&gt;
&lt;p&gt;Staff are generally given the freedom to choose their own destiny and focus on work that they enjoy much like in any academic environment; it's not hard to pick up passion projects or even move between groups if things get stale on a day-to-day basis.  Since everyone is working on their own slices of HPC, there's also easy access to world experts in different areas of technology if you need one.  For example, I recall once reviewing a storage system that appeared to rely on multiplexing two 12G SAS links over a single 24G SAS.  After one email and a few hours, a coworker confirmed, complete with a citation to the SCSI standards, that this was totally possible.  Even if someone in-house didn't know the answer, I had direct access to an engineering manager at a leading storage vendor who owed me a favor and definitely would've known the answer.  It's really, really hard to find as many smart people in arm's reach in most other HPC centers. &lt;/p&gt;
&lt;p&gt;At the same time, there is rigorous federal oversight on major projects and procurements to ensure that taxpayer dollars are responsibly spent.  This is a double-edged sword because all of the reporting and reviews that go into &lt;a href=&quot;https://www.energy.gov/articles/doe-build-next-generation-supercomputer-lawrence-berkeley-national-laboratory&quot;&gt;massive&lt;/a&gt; &lt;a href=&quot;https://www.ornl.gov/news/us-department-energy-and-cray-deliver-record-setting-frontier-supercomputer-ornl&quot;&gt;capital&lt;/a&gt; &lt;a href=&quot;https://www.energy.gov/articles/us-department-energy-and-intel-build-first-exascale-supercomputer&quot;&gt;projects&lt;/a&gt; make forward progress very slow at times.  All DOE HPC facilities review and re-review everything about these giant supercomputers before making a decision, so by the time the public sees a press release about a new supercomputer, lab staff have spent literal years going over every detail and risk.  It sometimes may not seem that way (how many problems has Aurora had?), but rest assured that every schedule slip or technology change the public hears was preceded by countless hours of meetings about risk and cost minimization.  On the flip-side though, you have the opportunity to learn every gory detail about the system directly from the people who designed it.&lt;/p&gt;
&lt;h3 style=&quot;text-align: left;&quot;&gt;Pay&lt;/h3&gt;
&lt;p&gt;In &lt;a href=&quot;https://www.bankrate.com/banking/federal-reserve/younger-workers-sharing-salaries/&quot;&gt;true millennial fashion&lt;/a&gt;, I think it's important to have an open discussion about the pay.  DOE labs pay more than any other HPC facility in the world as far as I am aware, and even in the San Francisco Bay Area, salary at NERSC is comparable to the base salaries offered by all the big tech companies.  You can get an idea of what entry-level salaries (think: first job after postdoc or a few years out of undergrad) by searching &lt;a href=&quot;https://h1bdata.info/&quot;&gt;H1B Visa postings&lt;/a&gt;, and anecdotally, I'd wager that a typical HPC job at NERSC pays about 2x that of the same job at a typical US university and 3x-4x that of the same job at a British or European university.  All the labs pay about the same to boot, so an HPC job at somewhere like Oak Ridge can afford you a relatively luxurious lifestyle.&lt;/p&gt;
&lt;p&gt;Don't get me wrong though; affording to buy a Bay Area house on a single NERSC salary alone would be tough in the same way that buying a Bay Area house on any single salary would be.  And while NERSC's compensation is comparable to the &lt;i&gt;base&lt;/i&gt; salary of the big tech companies, that base is about all you can get since DOE labs cannot offer equity or substantial bonuses.  This is less of a gap if you're just starting out, but anyone who's &lt;a href=&quot;https://www.levels.fyi/&quot;&gt;looked at compensation structures in tech&lt;/a&gt; knows that stock-based compensation, not base salary, dominates total compensation as you move up.&lt;/p&gt;
&lt;p&gt;So, if money wasn't an issue for me and NERSC is such a great place to work, why would I ever leave?&lt;/p&gt;
&lt;h2 style=&quot;text-align: left;&quot;&gt;The road ahead for HPC&lt;/h2&gt;
&lt;p&gt;On one hand, HPC's future has never been brighter thanks to how much life (and money!) the AI industry is bringing to the development of HPC technologies.  We have new &lt;a href=&quot;https://vastdata.com/&quot;&gt;all-flash&lt;/a&gt; &lt;a href=&quot;https://www.weka.io/&quot;&gt;file systems&lt;/a&gt;, &lt;a href=&quot;https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/&quot;&gt;gigantic GPUs&lt;/a&gt;, awesome &lt;a href=&quot;https://www.tomshardware.com/news/intels-sapphire-rapids-to-have-64-gigabytes-of-hbm2e-memory&quot;&gt;CPU memory technologies&lt;/a&gt;, and &lt;a href=&quot;https://arxiv.org/abs/2205.12182&quot;&gt;mixed-precision techniques&lt;/a&gt; in the HPC space that were all directly driven by developments primarily intended for AI workloads.  On the other hand, leadership HPC appears to be engaging in unsustainable brinkmanship while midrange HPC is having its value completely undercut by cloud vendors.  I've &lt;a href=&quot;https://glennklockwood.blogspot.com/2020/05/exascales-long-shadow-and-hpc-being.html&quot;&gt;not been shy about my overall anxiety about where HPC is going&lt;/a&gt; because of this, but I'll elaborate now that the exascale race has been won.&lt;/p&gt;
&lt;h3 style=&quot;text-align: left;&quot;&gt;The future of leadership HPC&lt;/h3&gt;
&lt;p&gt;Without some monumental breakthrough in transistor technology, there is only one path forward in continuing to build faster and faster supercomputers in the next decade: pour more and more energy (and dissipate more and more heat) into larger and larger (and more and more) GPUs.&lt;/p&gt;
&lt;p&gt;The goal post for exascale power keeps moving because that's been the easiest way to hit the mythical exaflop milestone; while the original goal was 20 MW, &lt;a href=&quot;https://www.nextplatform.com/2021/10/04/first-look-at-oak-ridges-frontier-exascaler-contrasted-to-argonnes-aurora/&quot;&gt;Frontier is coming in at 29 MW&lt;/a&gt; and &lt;a href=&quot;https://www.tomshardware.com/news/nvidia-amd-polaris-supercomputer-department-of-energy&quot;&gt;Aurora at &quot;under 60 MW.&quot;&lt;/a&gt;  Not only is this just a lot of power to feed into a single room, but the &lt;a href=&quot;https://www.olcf.ornl.gov/2020/09/23/powering-frontier/&quot;&gt;cost and effort&lt;/a&gt; of actually &lt;a href=&quot;https://www.llnl.gov/news/powering-llnl-prepares-exascale-massive-energy-and-water-upgrade&quot;&gt;building this infrastructure&lt;/a&gt; is &lt;a href=&quot;https://www.lanl.gov/asc/fous/sixty-megawatts-power-available-2025.php&quot;&gt;newsworthy&lt;/a&gt; in and of itself these days.  At the current trajectory, the cost of building a new data center and extensive power and cooling infrastructure for every new leadership supercomputer is going to become prohibitive very soon.&lt;/p&gt;
&lt;p&gt;HPC data centers situated in places where the cost of electricity and real estate (stacked atop the risk of earthquake or wildfire) further skew the economics of just adding more power are going to run up against this first.  It used to be easy to dismiss these practicality concerns by arguing that colocating scientists with supercomputers created immeasurable synergy and exchange of ideas, but the fact that science never stopped during the work-from-home days of the pandemic have taken a lot of air out of that argument.&lt;/p&gt;
&lt;p&gt;My guess is that all the 50-60 MW data centers being built for the exascale supercomputers will be the last of their kind, and that there will be no public appetite to keep doubling down.&lt;/p&gt;
&lt;p&gt;Given this, DOE's leadership computing facilities are facing an existential threat: how do you define leadership computing after exascale if you can't just add another 50% more power into your facility?  How do you justify spending another $600 million for a supercomputer that uses the same power but only delivers 15% more performance?  You can pour similarly huge amounts of money into application modernization to accelerate science, but at the end of the day, you'd still be buying a lot of hardware that's not a lot faster.&lt;/p&gt;
&lt;h3 style=&quot;text-align: left;&quot;&gt;The future of places like NERSC&lt;/h3&gt;
&lt;p&gt;NERSC is probably a little better off since its lack of an exascale machine today gives it at least one more turn of the crank before it hits a hard power limit in its data center.  That gives it the ability to deploy at least one more system after Perlmutter that is significantly (at least 2x) more capable but draws significantly more power.  However, compared to Frontier and Aurora, such a system may still look rather silly when it lands in the same way that Perlmutter looks a bit silly compared Summit, which was funded by the same agency but deployed years earlier.&lt;/p&gt;
&lt;p&gt;And therein lies the dilemma of centers like NERSC--how do you position yourself now so that by the time you deploy an HPC system that is close to maxing out on power, it is sufficiently different from a pure-FLOPS leadership system that it can solve problems that the leadership systems cannot?&lt;/p&gt;
&lt;p&gt;The easy go-to solution is to craft a story around &quot;data-centric&quot; supercomputing.  We did this when I was at the San Diego Supercomputer Center when we were budget-limited and had to differentiate our $12 million Comet supercomputer from TACC's $30 million Stampede.  You invest more in the file system than you would for a pure-FLOPS play, you provide low-cost but high-value onramps like Jupyter and science gateways to enable new science communities that have modest computing needs, and you fiddle with policies like allocations and queue priority to better suit interactive and urgent computing workloads.  From a productivity standpoint, this is can be a great story since users will always respond well to lower queue wait times and less frustrations with the file system.  From a system architect's standpoint, though, this is really boring.  The innovation happens in policies and software, not clever hardware or design, so there's very little that's new for a system designer to think about in this case.&lt;/p&gt;
&lt;p&gt;A more innovative approach is to start thinking about how to build a system that does more than just run batch jobs.  Perhaps it gives you a private, fast file system where you can store all your data in a way indistinguishable from your personal laptop.  Perhaps it gives you a convenient place to run a Jupyter notebook that has immediate access to a powerful GPU.  Or perhaps it gives you all the tools to set up an automated process where all you have to do is upload a file to trigger an automatic data analysis and reduction pipeline that returns its output to a shiny HTTP interface.  Such a system may not be able to crank out an exaflop using HPL, but does that matter if it's the only system in the country that supports such automation?&lt;/p&gt;
&lt;p&gt;There &lt;i&gt;are&lt;/i&gt; interesting system architecture questions in the latter case, so as a system designer, I much prefer it over the &quot;data-centric&quot; angle to non-exaflop supercomputing strategies.  But there remains a problem.&lt;/p&gt;
&lt;h3 style=&quot;text-align: left;&quot;&gt;The problem: cloud&lt;/h3&gt;
&lt;p&gt;Such a &quot;more than just batch jobs&quot; supercomputer actually already exists.  It's called the cloud, and it's far, far ahead of where state-of-the-art large-scale HPC is today--it pioneered the idea of providing an integrated platform where you can twist the infrastructure and its services to exactly fit what you want to get done.  Triggering data analysis based on the arrival of new data has been around for the better part of a decade in the form of serverless computing frameworks like &lt;a href=&quot;https://docs.microsoft.com/en-us/learn/modules/execute-azure-function-with-triggers/2-determine-best-trigger&quot;&gt;Azure Functions&lt;/a&gt;.  If you need to run a Jupyter notebook on a server that has a beefy GPU on it, just pop a few quarters into your favorite cloud provider.  And if you don't even want to worry about what infrastructure you need to make your Jupyter-based machine learning workload go fast, the cloud providers all have &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/machine-learning/overview-what-is-machine-learning-studio&quot;&gt;integrated machine learning development environments&lt;/a&gt; that hide all of the underlying infrastructure.&lt;/p&gt;
&lt;p&gt;And therein lies the problem: the definition of &quot;innovation&quot; as non-exaflop HPC runs up against this power wall might actually mean &quot;catching up to the cloud.&quot;&lt;/p&gt;
&lt;p&gt;This is not to say that NERSC-like HPC centers are entirely behind the cloud; all the DOE HPC facilities have bigger, faster, and more convenient parallel file systems that are generally always on and where data is always somewhere &quot;fast.&quot;  They also provide familiar, managed software environments and more egalitarian support to small- to mid-scale science projects.  DOE HPC also takes the most risk in deploying unproven technologies to shake them out before they become available to the wide market.&lt;/p&gt;
&lt;p&gt;However, those gaps are beginning to close.  You can stick &lt;a href=&quot;https://azure.microsoft.com/en-us/solutions/high-performance-computing/cray/&quot;&gt;a full Cray EX system, identical to what you might find at NERSC or OLCF, inside Azure&lt;/a&gt; nowadays and avoid that whole burdensome mess of building out a 50 MW data center.  You can also integrate such a system with all the rich infrastructure features the cloud has to offer like triggered functions.  And when it comes to being first to market for risky HPC hardware, the cloud has already caught up in many ways--&lt;a href=&quot;https://azure.microsoft.com/en-us/blog/azure-hbv3-virtual-machines-for-hpc-now-up-to-80-percent-faster-with-amd-milanx-cpus/&quot;&gt;Microsoft deployed AMD Milan-X CPUs in their data centers&lt;/a&gt; before any HPC shop did, and more recently, &lt;a href=&quot;https://www.theregister.com/2022/05/26/amd_azure_microsoft/&quot;&gt;Microsoft invested in AMD MI-200 GPUs&lt;/a&gt; before Frontier had a chance to shake them out.&lt;/p&gt;
&lt;p&gt;Given this steep trajectory, I see only two scenarios for large-scale, non-exaflop HPC facilities in the 10+ year horizon:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ol style=&quot;text-align: left;&quot;&gt;&lt;li&gt;They develop, adopt, steal, or squish cloud technologies into their supercomputers to make them functionally equivalent to cloud HPC deployments.  They may be a little friendlier to scientific users since cloud functionality wasn't designed for scientific computing alone, but they also may not be as stable, mature, or feature-rich as their cloud cousins.&lt;/li&gt;&lt;li&gt;They find better overall economics in eventually moving to &lt;a href=&quot;https://www.hpcwire.com/2021/05/13/behind-the-met-offices-procurement-of-a-billion-dollar-microsoft-system/&quot;&gt;massive, long-term, billion-dollar deals&lt;/a&gt; where flagship HPC systems and their &quot;more than just batch jobs&quot; features are colocated inside cloud datacenters sited at economically advantageous (that is, cheap power, cooling, and labor) locations in the country.&lt;/li&gt;&lt;/ol&gt;
&lt;p&gt;There's also grey area in between where national HPC facilities consolidate their physical infrastructure in cheap areas to manage costs but still self-manage their infrastructure rather than fully outsource to a commercial cloud.  &lt;a href=&quot;https://ethz.ch/en/news-and-events/eth-news/news/2021/03/we-dont-just-procure-a-new-computer.html&quot;&gt;CSCS has hinted at this model as their future plan&lt;/a&gt; since they cannot build 100 MW datacenters in Switzerland, and this is proof that leading HPC facilities around the world see the writing on the wall and need to maneuver now to ensure they remain relevant beyond the next decade.  Unfortunately, the politics of consolidating the physical infrastructure across the DOE HPC sites would likely be mired in Congressional politics and take at least a decade to work out.  Since serious work towards this hasn't started yet, I don't envision such a grey-area solution emerging before all the DOE facilities hit their power limit.&lt;/p&gt;
&lt;p&gt;Hopefully I've painted a picture of how I perceive the road ahead for large-scale HPC facilities and you can guess which one I think will win out.&lt;/p&gt;
&lt;h2 style=&quot;text-align: left;&quot;&gt;Final thoughts&lt;/h2&gt;
&lt;p&gt;I have every confidence that there will still be DOE HPC facilities in ten years and that they will still be staffed by some of the brightest minds in HPC.  And even if a cloud-based HPC facility ultimately consumes centers like NERSC, I don't think many people would be out of work.  The vast majority of what DOE's HPC people do is think carefully about technology trends, maintain a deep understanding of user requirements, provide excellent support to its thousands of users, and keep complex supercomputers running well.  Those jobs don't go away if the supercomputer is in the cloud; it's just the physical location, the hands doing physical hardware swaps, and the breadth of vendor interactions that may change.&lt;/p&gt;
&lt;p&gt;For me as a system architect though, it's become too hard for me to catch up to all the new technologies and techniques HPC needs for the future while also building up other staff to be masters of today's I/O challenges.  I found myself at a fork in the road.  One path would mean catching up on a technical level and then getting in front of where the future of HPC lies before it gets there.  The other path would mean trying to steer the entire DOE HPC ship in the right direction, as long as that may take, and have faith that the people I bring along can race far enough ahead to tell me if we're still going where we need to go.  Perhaps a bit selfishly, I chose the former.  I'm just not ready to give up on racing ahead myself yet, and the only way I could hope to catch up was to make it a full-time job.&lt;/p&gt;
&lt;p&gt;I don't claim to know the future, and a lot of what I've laid out is all speculative at best.  NERSC, ALCF, or OLCF very well may build another round of data centers to keep the DOE HPC party going for another decade.  However, there's no denying that the stakes keep getting higher with every passing year.&lt;/p&gt;
&lt;p&gt;That all said, DOE has pulled off stranger things in the past, and it still has a bunch of talented people to make the best of whatever the future holds.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Experimenting with Igor’s Bluestore WAL</title>
   <link href="https://hpc.social/2022/experimenting-with-igor-s-bluestore-wal/"/>
   <updated>2022-05-26T01:00:00-06:00</updated>
   <id>https://hpc.social/2022/experimenting-with-igor-s-bluestore-wal</id>
   <content type="html">&lt;p&gt;Igor Fedetov is one of the most knowledgable developers working on Ceph.  He’s started working on replacing our use of RocksDB’s write ahead log with a bluestore native implementation.  After tuning we can &lt;a href=&quot;https://docs.google.com/spreadsheets/d/1zETd1Nq_CbLNSh3R2II-z8efQizUjDYfHDBIcMwGNdg/edit?usp=sharing&quot;&gt;achieve&lt;/a&gt; up to 122K random write IOPS on a single OSD!  That’s nearly a 50% improvment over the current main branch and over twice as fast as Pacific!&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Interesting links I clicked this week</title>
   <link href="https://hpc.social/2022/interesting-links-i-clicked-this-week/"/>
   <updated>2022-05-14T19:35:32-06:00</updated>
   <id>https://hpc.social/2022/interesting-links-i-clicked-this-week</id>
   <content type="html">&lt;p&gt;I watched several really interesting talks from &lt;a href=&quot;https://www.usenix.org/conference/srecon22americas/program&quot;&gt;SRECon22 Americas&lt;/a&gt; this week, and in particular I&amp;#8217;d like to highlight:&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://www.usenix.org/conference/srecon22americas/presentation/desai&quot;&gt;Principled Performance Analytics&lt;/a&gt;, Narayan Desai and Brent Bryan from Google. Some interesting thoughts on quantitative analysis of live performance data for monitoring and observability purposes, moving past simple percentile analysis.&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://www.usenix.org/conference/srecon22americas/presentation/rosenthal&quot;&gt;The &amp;#8216;Success&amp;#8217; in SRE is Silent&lt;/a&gt;, Casey Rosenthal from Verica.io. Interesting thoughts here on the visibility of reliability, qualitative analysis of systems, and why regulation and certification might not be the right thing for web systems.&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://www.usenix.org/conference/srecon22americas/presentation/ryan&quot;&gt;Building and Running a Diversity-focused Pre-internship program for SRE&lt;/a&gt;, from Andrew Ryan at &lt;s&gt;Facebook&lt;/s&gt; Meta. Some good lessons-learned here from an early-career internship-like program, in its first year.&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://www.usenix.org/conference/srecon22americas/presentation/means&quot;&gt;Taking the 737 to the Max&lt;/a&gt;, Nickolas Means from Sym. A really interesting analysis of the Boeing 737 Max failures from both a technical and cultural perspective, complete with some graph tracing to understand failure modes.&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;I also ran across some other articles that I&amp;#8217;ve been actively recommending and sharing with friends and colleagues, including:&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://ferd.ca/plato-s-dashboards.html&quot;&gt;Plato&amp;#8217;s Dashboards&lt;/a&gt;, Fred Hebert at Honeycomb. This article has some great analysis of how easily-measurable metrics are often poor proxies for the information we&amp;#8217;re actually interested in, and discussing qualitative research methods as a way to gain more insight.&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://cyberlaw.stanford.edu/blog/2022/05/end-roe-will-bring-about-sea-change-encryption-debate&quot;&gt;The End of Roe Will Bring About A Sea Change In The Encryption Debate&lt;/a&gt;, Rianna Pfefferkorn from the Stanford Internet Observatory. You should absolutely go read this article, but to sum up: Law enforcement in states than ban abortion is now &lt;em&gt;absolutely&lt;/em&gt; part of the threat model that encrypted messaging defends against. No one claiming to be a progressive should be arguing in favor of &amp;#8220;exceptional access&amp;#8221; or other law enforcement access to encryption.&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Pipelib- Simple Library to Parse, Filter, and Sort Things</title>
   <link href="https://hpc.social/2022/pipelib-simple-library-to-parse-filter-and-sort-things/"/>
   <updated>2022-05-07T13:30:00-06:00</updated>
   <id>https://hpc.social/2022/pipelib-simple-library-to-parse-filter-and-sort-things</id>
   <content type="html">&lt;p&gt;In early April I added an “update” command to Singularity Registry HPC (&lt;a href=&quot;https://github.com/singularityhub/singularity-hpc/pull/538&quot; target=&quot;_blank&quot;&gt;see the pull request here&lt;/a&gt; and needed to start with a list of docker tags and
parse them into version strings to sort, and still return the original tag for later use.
I wound up creating a &lt;a href=&quot;https://github.com/singularityhub/singularity-hpc/blob/main/shpc/main/container/update/versions.py&quot; target=&quot;_blank&quot;&gt;custom class and set of functions&lt;/a&gt; that use 
&lt;a href=&quot;https://github.com/python/cpython/blob/bd030b633f98ea5d9f93ef0105a51d2faf67070d/Lib/distutils/version.py#L269&quot; target=&quot;_blank&quot;&gt;distutils.LooseVersion&lt;/a&gt; to support that, but in creating this
“hard coded thing” I stepped back and had a question.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Can we more intelligentally compose custom parsing pipelines?&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;Specifically I wanted to:&lt;/p&gt;

&lt;ol class=&quot;custom-counter&quot;&gt;
&lt;li&gt;Start with a list of container tags for an image from a registry&lt;/li&gt;
&lt;li&gt;Filter out anything that looks like a commit, but isn't a string (e.g., latest)&lt;/li&gt;
&lt;li&gt;Derive a major, minor, and patch version for each, and filter to newest&lt;/li&gt;
&lt;li&gt;Sort!&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For step 3, as an example if there was a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1.2.3-commitA&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1.2.3-commitB&lt;/code&gt; I’d only want to keep one, and the newer one of the two,
so I could ask for “unique by patch” and filter the older one out.
Ultimately of course I &lt;a href=&quot;https://twitter.com/vsoch/status/1516197732708282369&quot; target=&quot;_blank&quot;&gt;dove right in&lt;/a&gt;,
and this led to the creation of &lt;a href=&quot;https://vsoch.github.io/pipelib&quot; target=&quot;_blank&quot;&gt;Pipelib&lt;/a&gt;, which was an itch I terribly wanted to scratch! In this quick post, I want to share the overall design, because it was really fun to make.&lt;/p&gt;

&lt;div style=&quot;padding: 20px;&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/vsoch/pipelib/main/docs/assets/pipelib-small.png&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;design&quot;&gt;Design&lt;/h2&gt;

&lt;p&gt;Before we talk about the design, let me show it to you.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pipelib.steps&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pipelib.pipeline&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# A pipeline to process a list of strings
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;steps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;

   &lt;span class=&quot;c1&quot;&gt;# convert everything to lowercase
&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ToLowercase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;

   &lt;span class=&quot;c1&quot;&gt;# don't include anything with &quot;two&quot;
&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;HasPatterns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filters&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;two&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Strings to process
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;items&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'item-ONE'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'item-TWO'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'item-two-THREE'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;steps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# The updated and transformed items
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;updated&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;items&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# ['item-one']
&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;In the above, we take a pipeline object and add steps to it. That design is fairly simple,
as the Pipeline class takes an optional iterable of things to process. I say “things” because
we can give it steps, composed steps, or even entire other pipelines. Here is an example
of adding an entire other Pipeline!&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pipelib.steps&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pipelib.pipeline&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;fruits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Orange&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Melon&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Watermelon&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Fruit23&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;preprocess&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;steps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# Example of chaining steps together
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;HasMaxLength&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;HasAllLetters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Add this preprocess step alongside other steps (make lowercase)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;steps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ToLowercase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;preprocess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Create a new pipeline and run
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;steps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# We should expect orange and melon!
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;updated&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fruits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'orange'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'melon'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Implementation-wise, this is also fairly simple. We can check the underlying class of the provided object
and either add a single step, or insert a set of steps given another pipeline. In fact, pipelib comes with a
small set of “pipelines” that are ready for you to use. For example, here is one to
filter out “things that look like complete or partial git commits”&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pipelib.steps&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pipelib.pipeline&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Pre-generated sets of steps we can use
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pipelib.pipelines&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipelines&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pipelines&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;git&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RemoveCommits&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;832b1c&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;832b1c645e562d5cc6e376e5a3e058c02a40d92a&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;123-abcd&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;123-abcd&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;This is something I found useful because people sometimes use commits as Docker tags, and I don’t find this 
incredibly meaningful as a version to compare to (and want to remove them). Under the hood, it looks like this:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;n&quot;&gt;RemoveCommits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;steps&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;HasMinLength&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;HasAllLowerLettersNumbers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Do you also notice something interesting in the above? We are actually combining steps akin to logical operations.
The above “pipeline” is actually just one step that combined other steps!&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;n&quot;&gt;pipelines&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;git&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RemoveCommits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;steps&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;HasMinLength_AND_NotHasAllLowerLettersNumbers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Let’s step back and talk about some concepts that allow this.&lt;/p&gt;

&lt;h2 id=&quot;concepts&quot;&gt;Concepts&lt;/h2&gt;

&lt;h3 id=&quot;pipeline&quot;&gt;Pipeline&lt;/h3&gt;

&lt;p&gt;As we’ve seen above, a pipeline is a collection of steps that take, as input, a listing of items and return a parser and filtered list.&lt;/p&gt;

&lt;h3 id=&quot;step&quot;&gt;Step&lt;/h3&gt;

&lt;p&gt;A step is some action in a pipeline. The way this works is that we have different kinds of steps, and this makes them easy
to implement and even test. A &lt;em&gt;boolean&lt;/em&gt; step is akin to a filter, and is expected to return True or False to indicate if the item passes, e.g., False means it’s filtered out. Boolean steps are neat because they afford different kinds of logic and combination.&lt;/p&gt;

&lt;h4 id=&quot;logical-operations&quot;&gt;Logical Operations&lt;/h4&gt;

&lt;p&gt;Let’s say that we have a step that checks that an input is all letters:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;HasAllLetters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;For the above, anything that had a number (e.g., orange123) would be filtered out. But what if we wanted to inverse that, and allow passing of inputs that don’t have all letters (meaning we want numbers or special characters?) We can simply do that:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;HasAllLetters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Implementation wise, this was really fun to do! For Python to respect the logical operator &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;~&lt;/code&gt; I simply define the “&lt;strong&gt;invert&lt;/strong&gt;” function for the BooleanStep class.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__invert__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
    We can say &quot;~step&quot; and reverse the logic.
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reverse&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;It sets an attribute “reverse” to True, and returns itself, that way we use the same step, but with this variable set to be true.
What does that do? In the “run” &lt;a href=&quot;https://github.com/vsoch/pipelib/blob/69d7d4ac677a24a31ffa9322f03090cf074442c8/pipelib/steps/step.py#L217-L238&quot; target=&quot;_blank&quot;&gt;function&lt;/a&gt; of the BooleanStep we basically retrieve an outcome from the underlying step (True or False) and simply reverse it given that boolean is True! Again, it’s very simple, and allows for doing things like this:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pipelib.pipeline&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pipelib.steps&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;steps&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;steps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;HasAllLetters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;I-have-special-characters&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Idonot&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'I-have-special-characters'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;steps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;HasAllLetters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;I-have-special-characters&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Idonot&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Idonot'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;What if we wanted to combine steps? E.g., what if I want to say “has all letters” OR “has minimum length 10?” If we put the steps
side by side we would only be able to support an AND - allowing passing through of entries that have all letters and the minimum length of 10.
Pipelib supports both those operators - AND and OR as follows:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;steps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;HasAllLetters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;steps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;HasMinLength&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;HasAllLetters_AND_HasMinLength&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;thisonewillpass&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;thisoneno&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;notthisone2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'thisonewillpass'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;For both cases above, we are using the “&lt;strong&gt;and&lt;/strong&gt;” and “&lt;strong&gt;or&lt;/strong&gt; functions, respectively, and:&lt;/p&gt;

&lt;ol class=&quot;custom-counter&quot;&gt;
&lt;li&gt;Checking for class compatibility (both must be BooleanStep)&lt;/li&gt;
&lt;li&gt;Creating a list of composed steps to added to a class attribute &quot;composed&quot;&lt;/li&gt;
&lt;li&gt;Add the previous run functions too, naming based on the step class name&lt;/li&gt;
&lt;li&gt;Define a new run function that loops through the composed set, runs, updates and returns a shared result&lt;/li&gt;
&lt;li&gt;Name the class based on the combined names of the composed classes&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For step 4 above, the operation (AND or OR) will vary depending on if the initial call was to “&lt;strong&gt;and&lt;/strong&gt;” or “&lt;strong&gt;or&lt;/strong&gt;”.
The main difference between the two is that “OR” starts with a default of False (otherwise it would always return True)
and AND starts with a default of True (otherwise it would always return False).
And since we are always taking the first class “composed” attribute, this means that you can compose
steps with other steps as many times as you like - a new check is simply added to the front or back of
the list. The result (returned) is the new class that is ready to run. Here is what an OR looks like:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;steps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;HasAllLetters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;steps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;HasMinLength&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;HasAllLetters_OR_HasMinLength&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;thisonewillpass&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;veryshort&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;12345&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'thisonewillpass'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'veryshort'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;If you are interested in this function, you can see the entire thing &lt;a href=&quot;https://github.com/vsoch/pipelib/blob/832b1c645e562d5cc6e376e5a3e058c02a40d92a/pipelib/steps/step.py#L177-L241&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;transformation-operations&quot;&gt;Transformation Operations&lt;/h4&gt;

&lt;p&gt;A base step can be thought of as a transformation. Instead of expecting a boolean to be returned, we are
instead expecting a new value or None. In this respect the transform step can also act as a boolean as a return
of “None” will be removed from the list, however in most cases a transform is intended to perform an operation 
on the item passed. Here is an example of a transformation operation:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;steps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ToLowercase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;AHHHH&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ahhhh'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;h4 id=&quot;sort-operations&quot;&gt;Sort Operations&lt;/h4&gt;

&lt;p&gt;A sort operation is a step that is one level up. Instead of operating on individual items, the step
re-defines a the higher level “run” function and does operations across the iterable.
A good example from Pipelib is the use case that originally inspired me - to start with a messy
list of Docker tags, do some parsing to derive versions, and return back a sorted list.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;steps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;container&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ContainerTagSort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ascending&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;1.2.3&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;0.1.0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;8.3.2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'8.3.2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'1.2.3'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'0.1.0'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;steps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;container&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ContainerTagSort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ascending&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;1.2.3&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;0.1.0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;8.3.2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'0.1.0'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'1.2.3'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'8.3.2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;In the above we also demonstrate that steps can take parameters, such as the order of a sort!
This particular sorting step also allows you to say you want to return unique major, minor, or patch
versions.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;steps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;container&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ContainerTagSort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique_major&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;1.2.3&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;1.1.0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;8.3.2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'8.3.2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'1.2.3'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;And if you wanted to do a more comprehensive clean up and sort, you could do &lt;a href=&quot;https://vsoch.github.io/pipelib/getting_started/user-guide.html#a-real-world-example-docker-tags&quot; target=&quot;_blank&quot;&gt;something like this&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;wrapper&quot;&gt;Wrapper&lt;/h3&gt;

&lt;p&gt;Pipelib needed a way to be able to pass around some parsed version of an item, but still maintain
the original. For example, let’s say I’m parsing Docker tags into something that resembles a loose
semantic version, I might have filtered &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1.2.3-boop&lt;/code&gt; to be just &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1.2.3&lt;/code&gt;, but at the end of the
day I need the original tag to pull. Pipelib accomplishes this via wrappers.&lt;/p&gt;

&lt;p&gt;A wrapper is conceptually that - an internal wrapper class to an item that allows for storing
an original value, and still doing operations to change a current state. Wrappers are used inside 
steps and allow for things like sorting and comparison. You probably don’t need to worry about wrappers
unless you want to develop for pipelib. By default, wrappers and “extracted away” to return the basic
types. However, you can ask Pipelib to not do this unwrapping, and then you can get back
the derived and original values:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;n&quot;&gt;tags&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;1.2.3&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;1.1.0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;8.3.2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;updated&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;steps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;container&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ContainerTagSort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tags&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;unwrap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Notice that this just looks like a set of strings...
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;updated&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'8.3.2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'1.2.3'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# But actually we have wrappers, that each have an _original attribute
&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;updated&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pipelib&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;wrappers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;version&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;VersionWrapper&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;I’ve had so much fun making this library! Like many of my projects it’s probably not super useful,
but if you see a cool use case please let me know! I’m also happy to develop custom pipelines or steps
for a use case that you might be interested in. Please don’t hesitate to ask me for help, I’m always running
out of fun things to do :)&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Why should I care?&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;Arguably you could just hard code this kind of filtering and sorting, but I think the
idea of being able to customize and assemble steps is a cool one. If the steps are provided
in a library it might might it slightly easier, or your work more reproducible because 
someone else can use the steps. And if you don’t care? That’s okay too. I recognize this was
mostly a fun project, and yet-another-itch I really wanted to scratch because I’ve never
made a design like this before, either in terms of the idea or &lt;a href=&quot;https://twitter.com/vsoch/status/1521670410852442112&quot; target=&quot;_blank&quot;&gt;underlying testing and automation&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>The Research Software Ecosystem</title>
   <link href="https://hpc.social/2022/the-research-software-ecosystem/"/>
   <updated>2022-04-24T13:30:00-06:00</updated>
   <id>https://hpc.social/2022/the-research-software-ecosystem</id>
   <content type="html">&lt;p&gt;We recently published &lt;a href=&quot;https://openresearchsoftware.metajnl.com/articles/10.5334/jors.359/&quot; target=&quot;_blank&quot;&gt;the Research Software Encyclopedia&lt;/a&gt; and also have added several new parsers for obtaining new data, meaning the total collection
of &lt;a href=&quot;https://rseng.github.io/software/&quot; target=&quot;_blank&quot;&gt;curated research software&lt;/a&gt; is greater than 1500
entries. In honor of this collection, and of a library I’m working on called &lt;a href=&quot;https://vsoch.github.io/citelang/getting_started/user-guide.html&quot; target=&quot;_blank&quot;&gt;CiteLang&lt;/a&gt;, I wanted to do a small study to better understand:&lt;/p&gt;

&lt;ol class=&quot;custom-counter&quot;&gt;
&lt;li&gt;What are the most valuable dependencies in our community, across languages?&lt;/li&gt;
&lt;li&gt;What are the most valuable dependencies in our community, by language?&lt;/li&gt;
&lt;li&gt;What is the credit allocation for each repository?&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;citelang&quot;&gt;CiteLang&lt;/h2&gt;

&lt;p&gt;To step back for a second, let’s talk again about CiteLang. It has many functions - one of them
being an ability to &lt;a href=&quot;https://vsoch.github.io/2022/citelang-contrib/&quot; target=&quot;_blank&quot;&gt;assess open
source contributions&lt;/a&gt; via git, but it’s main purpose is to be a markdown syntax for citing software,
meaning that we can:&lt;/p&gt;

&lt;ol class=&quot;custom-counter&quot;&gt;
&lt;li&gt;Generate basic software credit trees, graphs, and markdown summaries.&lt;/li&gt;
&lt;li&gt;Derive a new, customizable model of credit based on published packages and dependencies.&lt;/li&gt;
&lt;li&gt;Provide a way to cite software in a paper and give credit without needing DOIs.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As a simple example, I can run CiteLang over this markdown file with CiteLang references:&lt;/p&gt;

&lt;div class=&quot;language-md highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gh&quot;&gt;# Summary&lt;/span&gt;

Portability and reproducibility of complex software stacks is essential for researchers to perform their work. 
High Performance Computing (HPC) environments add another level of complexity, where possibly conflicting 
dependencies must co-exist. Although container technologies like Singularity @conda{name=singularity} make 
it possible to &quot;bring your own environment,&quot; without any form of central strategy to manage containers, 
researchers who seek reproducibility via using containers are tasked with managing their own container 
collection, often not taking care to ensure that a particular digest or version is used. The reproducibility 
of the work is at risk, as they cannot easily install and use containers, nor can they share their software 
with others.

Singularity Registry HPC (shpc) @pypi{name=singularity-hpc} is the first of its kind to provide an easy means 
for a researcher to add their research software for sharing and collaboration with other researchers to an 
existing collection of over 200 popular scientific libraries @github{name=autamus/registry} 
@github{name=spack/spack, release=0.17}. The software installs containers as environment modules that are easy
to use and read documentation for, and exposes aliases for commands in the container that the researcher can 
add to their pipeline without thinking about complex interactions with a container. The simple addition of an 
entry to the registry maintained by shpc comes down to adding a yaml file, and after doing this, another 
researcher can easily install the same software, down to the digest, to reproduce the original work.

&lt;span class=&quot;gh&quot;&gt;# References&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;&amp;lt;!--citelang start--&amp;gt;&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;&amp;lt;!--citelang end--&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;And then run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;citelang render paper.md&lt;/code&gt; to get a &lt;a href=&quot;https://gist.github.com/vsoch/41b4559d8f87eb9d6e62945e02689428&quot; target=&quot;_blank&quot;&gt;nice rendered table alongside your paper&lt;/a&gt;! What CiteLang does is find the references in the paper, they look like this:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
@conda{name=singularity}
@pypi{name=singularity-hpc}
@github{name=autamus/registry} 
@github{name=spack/spack, release=0.17}

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Each of the references above is a package manager with a package name and (optionally) a version, and we can load in the metadata
for each and then generate a table &lt;a href=&quot;https://gist.github.com/vsoch/41b4559d8f87eb9d6e62945e02689428&quot; target=&quot;_blank&quot;&gt;that you see here&lt;/a&gt; that summarizes credit across dependencies. In this model, we give some allocation of credit 
(default is 50%) to the main work (paper or software) citing the software, and then recursively parse dependencies up to some minimum level of credit to calculate scores. Dependencies shared across libraries are averaged together. The final table represents the credit that you give not only to the top level software, but to all nested dependencies, for the work that you did. And that’s only the basics! CiteLang takes this simple ability to parse references and extends it to automation, graphs, badges, and more! You can read more about CiteLang &lt;a href=&quot;https://vsoch.github.io/citelang/getting_started/index.html&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Publish or perish? How about neither? I just need to keep writing software!&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;But do you see what is happening above? We aren’t requiring some artificial publication
in order to cite software. We are citing it based on its actual usage, as a known dependency to some other software.
In a nutshell, we don’t believe that “the traditional academic way” of citing papers makes sense for software, and instead
of using DOIs we can use package managers and metadata as a source of truth, and derive the real value of a piece of software
based on this ecosystem. This means that as a research software engineer, you can just keep doing what you are already doing, and if
someone uses CiteLang to summarize their work, given that your software is published to a package managed you’ll get credit. There
are so many cool ideas around this! But let’s start at the beginning. We first want to show how to summarize an ecosystem.
That is exactly what we are going to do in this post.&lt;/p&gt;

&lt;h2 id=&quot;the-research-software-ecosytem&quot;&gt;The Research Software Ecosytem&lt;/h2&gt;

&lt;p&gt;Starting with these curated repositories from a &lt;a href=&quot;https://rseng.github.io/rse/getting-started/scrapers/index.html&quot; target=&quot;_blank&quot;&gt; set of scrapers&lt;/a&gt; including the Journal of Open Source Software, the HAL Research Software Database, the Research Software NL Dictionary, ROpenSci, and 
The Molecular Sciences Software Institute, we can do a basic analysis to identify the most used (and thus valued) pieces of software in our ecosystem. My analysis plan was to:&lt;/p&gt;

&lt;ol class=&quot;custom-counter&quot;&gt;
&lt;li&gt;Start with the current database.&lt;/li&gt;
&lt;li&gt;For each repository, look for requirements files to parse.&lt;/li&gt;
&lt;li&gt;Derive dependency data based on this requirements file.&lt;/li&gt;
&lt;li&gt;Combine and rank to discover the top dependencies!&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This of course is limited to the subset of software in our database, and the ability of CiteLang to parse a requirements file.
Currently we parse setup.py and requirements.txt (Python), DESCRIPTION (R), go.mod (Go), package.json (npm), and Gemfile (ruby). Based on the
&lt;a href=&quot;https://rseng.github.io/rsepedia-analysis/analysis/languages/&quot; target=&quot;_blank&quot;&gt;breakdown of the languages&lt;/a&gt; found in the RSEPedia, this is a reasonable start!&lt;/p&gt;

&lt;div style=&quot;padding: 20px;&quot;&gt;
  &lt;img src=&quot;https://vsoch.github.io/assets/images/posts/citelang/languages.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;But it’s also kind of sad to see that my favorite languages (Go and Rust) are barely represented in our community. Also, the above
should tell you that the R and Python results likely have some meaningful interpretation, but the others not so much, only because we don’t have a big enough sample. So for all of the above
steps, for these 1500+ repositories and many languages, I wanted th entire process to be automated, always have potential for easy improvement,
and run at some regular interval as new software comes into the Research Software Encyclopedia (also automated) so we can derive changes over time.
If you dont’ care to read further:&lt;/p&gt;

&lt;ol class=&quot;custom-counter&quot;&gt;
&lt;li&gt;&lt;a href=&quot;https://rseng.github.io/rsepedia-analysis/&quot; target=&quot;_blank&quot;&gt;View the Research Software Ecosystem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://rseng.github.io/rsepedia-analysis/analysis/languages/&quot; target=&quot;_blank&quot;&gt;Check out Languages here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://rseng.github.io/rsepedia-analysis/analysis/dependencies/&quot; target=&quot;_blank&quot;&gt;Results for Dependencies here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://rseng.github.io/rsepedia-analysis/analysis/repos/&quot; target=&quot;_blank&quot;&gt;Individual Repositories here&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For this first publication of the interface we have the following metrics:&lt;/p&gt;

&lt;div style=&quot;padding: 20px;&quot;&gt;
  &lt;img src=&quot;https://vsoch.github.io/assets/images/posts/citelang/ecosystem.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;And I’m so excited because a tiny vision I had a few years ago to provide (and use) a community research software database is coming
to live! So without further adeiu, I’m just going to jump into the cool results! It will be fun to see how these change over time.&lt;/p&gt;

&lt;h3 id=&quot;python&quot;&gt;Python&lt;/h3&gt;

&lt;p&gt;Ladies and gents, dinosaurs and rabbits! Your Python results:&lt;/p&gt;

&lt;div style=&quot;padding: 20px;&quot;&gt;
  &lt;img src=&quot;https://vsoch.github.io/assets/images/posts/citelang/python-deps.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;So here is the first awesome insight. Is anyone really surprised to see numpy as the number one library?
The credit value here says that the average Python repository is attributing about 3% of credit to numpy, meaning it is a direct or indirect dependency. Let that sink in! Here is the irony - when is the last time you cited numpy? You probably haven’t, because you’ve cited something
that uses it. We don’t remember numpy despite the fact that it’s so core to everything that we do.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The fact that the most widely used library is rarely cited is huge evidence for why a manual “write papers and cite DOIs” approach just won’t work for software.&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;What else do we see in this list? Let me name a few things. First, we can’t be so terrible at remembering to look at or visualize
things because matplotlib is second. At least for research software, this is telling us that making plots or charts is important.
The next (possibly surprising) result is that documentation and testing is at least represented, and this might be a biased sample
because we include repositories that are peer reviewed (e.g., JoSS) and documentation and testing is necessary for that. 
Given this need for Python, sphinx and pytest come up as leaders to provide that. So here is another nugget of insight:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Some of us are so  busy focusing on domain-specific software that we forget the importance of the “less sexy” research software that helps us test, document, view things, or even create simple data structures.&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;This kind of “base” software has always been what I’ve been most interested in, and ironically what people tell me time and time again
“That’s not research software.” Oh really? So something that is entirely powering the research community is not research software?
Of course I have my own &lt;a href=&quot;https://rseng.github.io/software/repository/github/0x0f0f0f/Metatheory.jl/annotate-taxonomy/&quot; target=&quot;_blank&quot;&gt;strong opinions&lt;/a&gt; about a taxonomy for research software, but I would encourage those of you who are very dismissive to take a step back and
consider what you are really saying.&lt;/p&gt;

&lt;p&gt;The next insight is that we see a lot of libraries for data formats (e.g., pyaml, h5py, lxml, and more lower in the list) and this is an attestment to how important being able to read, serialize, and save data is.&lt;/p&gt;

&lt;p&gt;The final insight is the fact that requests is high in the list. For those of you not familiar, requests is a library for doing that, making
http requests to get content from some webby place. This is an attestment to the fact that our work is increasingly relying on external APIs,
automation, or other resources provided on the web.&lt;/p&gt;

&lt;p&gt;You can see &lt;a href=&quot;https://rseng.github.io/rsepedia-analysis/analysis/python/&quot; target=&quot;_blank&quot;&gt;the full Python results here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;r&quot;&gt;R&lt;/h3&gt;

&lt;p&gt;I’m less of an R programmer these days, but I think that these results also make sense.&lt;/p&gt;

&lt;div style=&quot;padding: 20px;&quot;&gt;
  &lt;img src=&quot;https://vsoch.github.io/assets/images/posts/citelang/r-deps.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;We don’t see any huge leaders in the same way as we see numpy in Python, but not surprisingly the leader package
for the R language is, well, R! I at first thought this was a bug, but actually R &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DESCRIPTION&lt;/code&gt; files that we parse do commonly include a pinned version of R:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-DESCRIPTION&quot;&gt;
Depends: R (&amp;gt;= 3.4.1), TailRank, 
...

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And so we actually can give credit to the language proper! If you don’t feel this is appropriate, feel free to skip this line and consider
the top package jsonlite. This is also why I think json would be represented in Python if it wasn’t part of the standard library. Us research folks - we need our json! Overall I think we see a similar pattern here as we saw with Python. The libraries that float to the top are those that involve data structures (jsonlite, yaml), webby requests or similar (httr, curl), documentation and testing (knitr, rmarkdown) and graphics or visualization.  What does this tell us about what is undervalued in research software? Again, it’s not the domain specific libraries, but rather the core stuff that enables those libraries.&lt;/p&gt;

&lt;p&gt;You can see &lt;a href=&quot;https://rseng.github.io/rsepedia-analysis/analysis/R/&quot; target=&quot;_blank&quot;&gt;the full R results here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;projects&quot;&gt;Projects&lt;/h3&gt;

&lt;p&gt;If you are interested in a specific project in the RSEPedia, we also provide a project-specific table and badge! 
You can &lt;a href=&quot;https://rseng.github.io/rsepedia-analysis/analysis/repos/&quot; target=&quot;_blank&quot;&gt;browse projects from here&lt;/a&gt;, 
and here is an example of a badge generated for a project called  &lt;a href=&quot;https://rseng.github.io/rsepedia-analysis/repos/github/ORNL/tx2/README&quot; target=&quot;_blank&quot;&gt;github.com/ORNL/tx2&lt;/a&gt; &lt;a href=&quot;https://github.com/ORNL/tx2&quot; target=&quot;_blank&quot;&gt;(and on GitHub)&lt;/a&gt;. Without even looking I can tell you we have some machine learning and/or visualization going on here (scikit-learn! umap! pandas! matplotlib)!&lt;/p&gt;

&lt;div style=&quot;padding: 20px;&quot;&gt;
  &lt;img src=&quot;https://vsoch.github.io/assets/images/posts/citelang/project.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Notice how numpy (as an example) shows up at multiple points in the tree - when we calculate an overall credit, say, for the ecosystem, we take that into account! And we can then peek at the project-specific table and sort of verify that yes, this is a Python ML/visualization project:&lt;/p&gt;

&lt;div style=&quot;padding: 20px;&quot;&gt;
  &lt;img src=&quot;https://vsoch.github.io/assets/images/posts/citelang/project-table.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;And we see some surprises! Like, the slack-sdk? What? Believe it or not, that is pulled in by &lt;a href=&quot;https://github.com/tqdm/tqdm/blob/4f208e72552c4d916aa4fe6a955349ee8b2ed353/setup.cfg#L87&quot; target=&quot;_blank&quot;&gt;tqdm&lt;/a&gt;. 
The project-specific tables (and the description at the top) also give you a better sense of how CiteLang allocates
credit. The top level package is given 50%, and then the other 50% is given to all dependencies in the same fashion.
We cut off at a value of 0.001, and we do that in case we might be parsing dependencies forever down to some infintesimally small amount.&lt;/p&gt;

&lt;p&gt;Finally, every project serves its own &lt;a href=&quot;https://rseng.github.io/rsepedia-analysis/repos/github/ORNL/tx2/data.json&quot; target=&quot;_blank&quot;&gt;raw data&lt;/a&gt;&lt;/p&gt;

&lt;div style=&quot;padding: 20px;&quot;&gt;
  &lt;img src=&quot;https://vsoch.github.io/assets/images/posts/citelang/json-data.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;and the site is searchable, because sites should be. 😄️&lt;/p&gt;

&lt;h2 id=&quot;discussion&quot;&gt;Discussion&lt;/h2&gt;

&lt;p&gt;I’m so happy (and a bit relieved, to be honest) to finally be able to show what I’ve been saying for years - that the most valuable software for research, and the software that is driving domain-specific research software, are the unsexy libraries that have to do with data structures, (maybe standards), documentation or testing, and data formats or retrieval. These are the packages that you aren’t going to remember to cite. Also, this set is totally leaving out the software we use on a day to day basis in our CI, which arguably isn’t research software but has done more for the research community than anything I can think of - containers, version control (git), and continuous integration. We’d be a mess without it. We need to be more thankful and aware of this, and for some of y’all that turn down your nose to anything that isn’t a domain-science library, perhaps take a pause. Next, let’s talk about limitations and hopes for the future.&lt;/p&gt;

&lt;h2 id=&quot;a-living-database&quot;&gt;A Living Database&lt;/h2&gt;

&lt;p&gt;I wouldn’t have been happy with myself to simply publish software at one point in time and call it a day.
The Research Software Encyclopedia is updated weekly, and so I’ve designed this analysis to do the same!
This means that while we do cache a result for a newly added piece of software, we do continue to grow the analysis 
as new software is added. And since the tool will always use the newly updated &lt;a href=&quot;https://github.com/vsoch/citelang&quot; target=&quot;_blank&quot;&gt;CiteLang&lt;/a&gt;, any improvements to the parsers there will be reflected here! And if anyone wants to run the entire thing again (outside of the limit of GitHub Actions) they can clone the repository, nuke the _repos folder, and run the scripts again.&lt;/p&gt;

&lt;h3 id=&quot;language-gaps&quot;&gt;Language Gaps&lt;/h3&gt;

&lt;p&gt;The biggest gap in the RSEPedia is with respect to what we don’t see. First, despite being a prominent language, we don’t see anything for C++, because there isn’t a package manager with an API to use it. If you have a nifty (or even hacky) idea for how to parse a requirements file, &lt;a href=&quot;https://github.com/vsoch/citelang/issues&quot; target=&quot;_blank&quot;&gt;I want to hear it&lt;/a&gt;. The RSEPedia has support for spack, but most research-oriented C++ projects are not going to go out of their way to publish their package there, and we get no signal of the package being in spack when we clone the repository. Sppaaaaaack (sorry, it’s a bit of a tic at this point!) 😁️&lt;/p&gt;

&lt;p&gt;We also don’t see standard modules or libraries provided within a language. E.g., I can almost guarantee you a ton of Python libraries are importing json, but since it’s not a package manager library we wouldn’t see it. I suspect citelang could come up with a way to derive credit for these libraries by way of abstract syntax trees or just parsing the source code, although I haven’t done this yet because I’m not convinced it’s something people are as interested in. If you want to say thank you for the Python standard library, there is a &lt;a href=&quot;https://www.python.org/psf/contrib/&quot; target=&quot;_blank&quot;&gt;donate button&lt;/a&gt; on their contribution page (or you could contribute code). There is an even deeper level of parsing (at least for Python) that looks at function signatures, and I wrote a library called &lt;a href=&quot;https://github.com/vsoch/caliper&quot; target=&quot;_blank&quot;&gt;caliper&lt;/a&gt; in early 2021 to do that, and it’s able to generate &lt;a href=&quot;https://raw.githubusercontent.com/vsoch/caliper-metrics/main/pypi/tensorflow/functiondb/functiondb-0.12.0rc1.json&quot; target=&quot;_blank&quot;&gt;function databases&lt;/a&gt; for Python software of interest. This would be cool to do for some kind of (unrelated) compatibility analysis here, but yes that’s very different.&lt;/p&gt;

&lt;h3 id=&quot;parsing-limitation&quot;&gt;Parsing Limitation&lt;/h3&gt;

&lt;p&gt;For all requirements files except for Python, we are forced to do static parsing. While not perfect because bugs can happen for niche cases of someone defining requirements in a weird way, it’s a reasonable start. There is always room for improvement, or adding more static parsers for requirements files I have not considered yet.&lt;/p&gt;

&lt;p&gt;However, this is not the case for the Python parsing (either requirements.txt or setup.py)! For Python these results are likely 
very good because we wrap the pypi package manager install command to derive a list of packages and versions from either a setup.py or requirements.txt. Don’t worry - nothing is installed, we either just parse the requirements file and return the results, or we use the solver
against a setup.py to come to an equivalent list. We originally had a static parser (and still use this as a fallback) however I talked to &lt;a href=&quot;https://github.com/alecbcs&quot; target=&quot;_blank&quot;&gt;@alecbcs&lt;/a&gt; and he had this fantastic idea! Will it likely need updates as time goes on, given
the functions are private? Sure. But I’m happy to do that to get the much more accurate listing.&lt;/p&gt;

&lt;p&gt;In practice, the only setup.py files that I was not able to parse either had a bug (e.g., trying to read a file that doesn’t exist in the repository) or they were trying to use modules outside of the standard library. For all of the cases of broken-ness, I opened issues on the respective repositories so we might have a better chance at parsing in the future! One detail is that we parse the first requirements file found. For a primary requirements file in the root of the repository, this is the best outcome. However, some repos don’t have a file in the root, and perhaps we find one in a documentation folder instead. Either way, the result represents our best effort at finding and parsing requirements given a cloned repository we don’t know the structure of in advance.&lt;/p&gt;

&lt;h3 id=&quot;final-thoughts&quot;&gt;Final Thoughts&lt;/h3&gt;

&lt;p&gt;Here are my final takeaways:&lt;/p&gt;

&lt;h4 id=&quot;publication-is-not-for-research-software&quot;&gt;Publication is not for Research Software&lt;/h4&gt;

&lt;p&gt;A system of credit that relies on software engineers to do extra manual work (to write papers) is never going to fully capture the ecosystem and give proper credit. It will only capture those that have the time and possibly privilege to take the extra time to write a paper.
Publication only makes sense given that a piece of software is paired alongside a robust result, in which case fine, write the paper and 
also champion the software.&lt;/p&gt;

&lt;h4 id=&quot;publication-does-not-actually-capture-credit&quot;&gt;Publication Does not Actually Capture Credit&lt;/h4&gt;

&lt;p&gt;A system that also only skims the superficial top (the name of one package) and does not dig deep into a dependency tree is also going to miss insights and deserved attributions of credit. As the numpy example shows, nobody is actually citing numpy, but a ton of projects are using it somewhere in their dependency tree, so it deserves a lot of credit.&lt;/p&gt;

&lt;h4 id=&quot;we-can-do-better&quot;&gt;We Can Do Better&lt;/h4&gt;

&lt;p&gt;I have a pet peeve. I’m frankly just tired of people writing about credit and attribution but not doing anything about it. We could extend that to other things, but it’s especially an issue for this topic. Ironically they are writing &lt;em&gt;papers&lt;/em&gt; and improving their publication record as they write about how publication and research software is a strained process. I may not have solved this problem, but damn at least I’m trying to actually do something about it instead of spurting gas.&lt;/p&gt;

&lt;p&gt;I find this idea exciting because there are so many directions you can go with it. When I first designed the idea I imagined a database and online interface where you could essentially connect your GitHub repository, and akin to a builder service, parse your repository on some event and derive a new credit or citation graph. Or you could have some set akin to the RSEPedia that are also updated regularly. And then, by way of having that database, we could do these same queries (that currently I’m doing statically) to say “What are the most important libraries for this language? Across the ecosystem?” or “How has this changed over time?” It would be a true way to derive the value of a library without needing people to publish papers, and totally automated and integrated with package managers, which is where people already should be putting their software.
Heck, if someone gave me a cloud and a little bit of funding I’d love to work on this. Are there good reasons or use cases? I don’t know, but maybe.&lt;/p&gt;

&lt;p&gt;So what do you think?&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Spooky Allocator Issues and Fixes</title>
   <link href="https://hpc.social/2022/spooky-allocator-issues-and-fixes/"/>
   <updated>2022-04-13T01:00:00-06:00</updated>
   <id>https://hpc.social/2022/spooky-allocator-issues-and-fixes</id>
   <content type="html">&lt;p&gt;Recently we started noticing performance issues in the main branch of Ceph that ultimately were traced back to a commit last summer that changed parts of our AVL and hybrid disk allocator implementations in bluestore.  Strangly, the issue only affected some of the NVMe drives in our test lab but not others.  The quick &lt;a href=&quot;https://github.com/ceph/ceph/pull/45884&quot;&gt;fix&lt;/a&gt; was to always update and save the allocator’s cursor position so that we don’t search (and fail) over and over in fast-fit mode for every allocation request.  Another interesting offshoot of this though is that it may be much &lt;a href=&quot;https://github.com/ceph/ceph/pull/45771&quot;&gt;nicer&lt;/a&gt; to limit fast-fit searches based on time rather than byte distance or the number of iterations.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>An unstructured rant on running long-lived software services</title>
   <link href="https://hpc.social/2022/an-unstructured-rant-on-running-long-lived-software-services/"/>
   <updated>2022-03-12T16:00:00-07:00</updated>
   <id>https://hpc.social/2022/an-unstructured-rant-on-running-long-lived-software-services</id>
   <content type="html">&lt;p&gt;&amp;#8211; Be kind to your colleagues. Be kind to your users. Be kind to yourself. This is a long haul and you’ll all fuck up.&lt;/p&gt;

&lt;p&gt;⁃ The natural environment for your code is production. It will run there longer than it does anywhere else. Design for prod first, and if possible, make your dev environment act like prod.&lt;/p&gt;

&lt;p&gt;⁃ Legacy code is the only code worth caring about.&lt;/p&gt;

&lt;p&gt;⁃ Users do weird stuff, but they usually have a very good reason, at least in their context. Learn from them.&lt;/p&gt;

&lt;p&gt;⁃ It’s 2022, &lt;em&gt;please&lt;/em&gt; do structured logging.&lt;/p&gt;

&lt;p&gt;⁃ Contexts and tracing make everyone&amp;#8217;s lives easier when it comes time to debug. At minimum, include a unique request id with every request and plumb it through the system.&lt;/p&gt;

&lt;p&gt;⁃ Do your logging in a separate thread. It sucks to find a daemon blocked and hanging because of a full disk or a down syslog server.&lt;/p&gt;

&lt;p&gt;⁃ Don’t page for individual machines going down. Do provide an easy or automated way for bad nodes to get thrown out of the system.&lt;/p&gt;

&lt;p&gt;&amp;#8211; Be prepared for your automation to be the problem, and include circuit breakers or kill switches to stop it. I&amp;#8217;ve seen health checks that started flagging every machine in the fleet as bad, whether it was healthy or not. We didn&amp;#8217;t bring down prod because the code assumed if it flagged more than 15% of the fleet as bad, the problem was probably with the test, not the service.&lt;/p&gt;

&lt;p&gt;⁃ Make sure you have a way to know who your users are. If you allow anonymous access, you&amp;#8217;ll discover in five years that a business-critical team you&amp;#8217;ve never heard of is relying on you.&lt;/p&gt;

&lt;p&gt;⁃ Make sure you have a way to turn off access for an individual machine, user, etc. If your system does anything more expensive than sending network requests, it will be possible for a single bad client to overwhelm a distributed system with thousands of servers. Turning off their access is easier than begging them to stop.&lt;/p&gt;

&lt;p&gt;⁃ If you don’t implement QOS early on, it will be hellish to add it later, and you will certainly need it if your system lasts long enough.&lt;/p&gt;

&lt;p&gt;⁃ If you provide a client library, and your system is internal only, have it send logs to the same system as your servers. This will help trace issues back to misbehaving clients so much.&lt;/p&gt;

&lt;p&gt;⁃ Track the build time for every deployed server binary and monitor how old they are. If your CI process deploys daily, week-old binaries are a problem. Month-old binaries are a major incident.&lt;/p&gt;

&lt;p&gt;⁃ If you can get away with it (internal services): track the age of client library builds and either refuse to support builds older than X, or just cut them off entirely. It sucks to support requests from year-old clients, force them to upgrade!&lt;/p&gt;

&lt;p&gt;⁃ Despite all this, you will at some point start getting requests from an ancient software version, or otherwise malformed. Make sure these requests don’t break anything.&lt;/p&gt;

&lt;p&gt;⁃ Backups are a pain, and the tooling is often bad, but I swear they will save you one day. Take the time to invest in them.&lt;/p&gt;

&lt;p&gt;⁃ Your CI process should exercise your turnup process, your decommission process, and your backups workflow. Life will suck later if you discover one of these is broken.&lt;/p&gt;

&lt;p&gt;⁃ Third party services go down. Your service goes down too, but they probably won’t happen at the same time. Be prepared to either operate without them, or mirror them yourself&lt;/p&gt;

&lt;p&gt;⁃ Your users will never, ever care if you’re down because of a dependency. Every datacenter owned by AWS could be hit by a meteor at the same time, but &lt;em&gt;your&lt;/em&gt; user will only ever ask “why doesn’t my service work?”&lt;/p&gt;

&lt;p&gt;⁃ Have good human relationships with your software dependencies. Know the people who develop them, keep in touch with them, make sure you understand each other. This is especially true internally but also important with external deps. In the end, software is made of people.&lt;/p&gt;

&lt;p&gt;⁃ If users don’t have personal buy-in to the security policy, they &lt;em&gt;will&lt;/em&gt; find ways to work around them and complain about you for making their lives harder. Take the time to educate them, or you&amp;#8217;ll be fighting them continuously.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>What I've Learned from Looking at 1,500 Jobs Leading Research Computing Teams</title>
   <link href="https://hpc.social/2022/what-i-ve-learned-from-looking-at-1-500-jobs-leading-research-computing-teams/"/>
   <updated>2022-02-26T00:00:00-07:00</updated>
   <id>https://hpc.social/2022/what-i-ve-learned-from-looking-at-1-500-jobs-leading-research-computing-teams</id>
   <content type="html">&lt;h2 id=&quot;job-numbers-continue-to-grow-lots-of-data-and-product-management-jobs-ir-groups-at-universities-becoming-bigger-employers&quot;&gt;Job numbers continue to grow; lots of data and product management jobs; IR groups at Universities becoming bigger employers&lt;/h2&gt;

&lt;p&gt;(Note: This post is adapted from &lt;a href=&quot;https://www.researchcomputingteams.org/newsletter_issues/0111&quot;&gt;#111&lt;/a&gt; of the &lt;a href=&quot;https://www.researchcomputingteams.org&quot;&gt;Research Computing Teams Newsletter&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;A year and a half ago I &lt;a href=&quot;https://www.dursi.ca/post/jobs_managing_research_computing_teams&quot;&gt;posted&lt;/a&gt; my observations on the first 500 jobs posted to &lt;a href=&quot;https://www.researchcomputingteams.org/jobs&quot;&gt;the job board&lt;/a&gt; - we’re getting close to 1,500 now, and it’s worth taking a look to see what if anything has changed in research computing team leadership and management jobs&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a class=&quot;footnote&quot; href=&quot;https://www.dursi.ca/feed.xml#fn:1&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;There are some trends that have continued since the posting.  The jobs in industry are growing vastly beyond what I would have imagined possible when I started in research computing in the 1990s.  (The number of jobs working with biomedical data of one sort or another in particular is just astonishing.)  Rather than technical computing being a niche, it’s utterly mainstream now.  There are a &lt;em&gt;lot&lt;/em&gt; of jobs out there, and I don’t even bother posting generic “data science manager” jobs unless they’re connected to some real complex research questions - which happens a lot, whether it’s fraud detection or improving financial modelling or supporting biomedical research.  Some really fun-looking jobs that would probably feel a lot like working at a research computing centre keep coming up at consultancies –– go visit a client and help them with their data science/data engineering/&lt;em&gt;etc&lt;/em&gt; needs.  There’s also a growing number of data science/engineering jobs at Universities that fall under the Provost/VP Operations rather than the VPR’s side of the house — Institutional Research, looking at (say) student success in support of the teaching mission.&lt;/p&gt;

&lt;p&gt;Because of the growth in number of jobs, it is very much a candidate’s market out there.  I’m seeing postings –– &lt;em&gt;especially&lt;/em&gt; for the traditional academic “director of research computing” jobs –— stay open for cringe-inducing periods of time.  A few in particular I’ve watched with vicarious embarrassment continue coming up in the listings for 8+ months.  That’s a bad sign for us as hiring managers - the market for individual contributors is at least as tight - but it’s amazing news for us as individuals.&lt;/p&gt;

&lt;p&gt;When I wrote that post in late 2020 it was just regulated industries like health/biotech or financial services that were developing data governance or other data management jobs, but now data management is popping up everywhere, whether it’s retail or logistics or anywhere else.   These are being joined, again first in the regulated industries, by data privacy or data risk management jobs.  Privacy-preserving data analysis jobs (and teams supporting same with software development) are also starting to be more common (and there’s a &lt;em&gt;lot&lt;/em&gt; of cool research and technology work to be done there!)&lt;/p&gt;

&lt;p&gt;I’m also (finally!) starting to see a explicitly &lt;em&gt;product&lt;/em&gt; management jobs in research computing, both academic and private-sector.  You see it around data management — bundling and curating of data into real data products — but also in software development, especially around analysis pipelines for some reason.&lt;/p&gt;

&lt;p&gt;Probably related to the growth of product &lt;em&gt;vs&lt;/em&gt; project thinking, I’m starting to see a lot of “delivery manager” jobs that would have been called “project managers” just a year ago.   Projects are defined by having clearly defined start- and end-points up-front.  “Delivery” jobs seem to focus on sustained, ongoing work, more appropriate for long-lived products.&lt;/p&gt;

&lt;p&gt;These products that keep coming up often combine data, software, and systems one way or another.  That really points to weaknesses around organizing by type of skills - the research software engineering movement, for instance - as the lines between software and systems in this DevOps, infrastructure-as-code era is very fuzzy; and as data grows more and more important, data skills are needed everywhere.&lt;/p&gt;

&lt;p&gt;Especially for us as managers or leads, but especially for individual contributors as they grow their skills, it’s important to have a pretty holistic view of research computing and data and not try to break it up into silos.  The growing number of data engineering jobs is a great example.  That work often involves all three of software, systems, and data expertise.   Data engineering is getting so broad and important that not only are there different sub-fields, in large organizations there are likely to be &lt;a href=&quot;https://medium.com/data-arena/team-topologies-for-data-engineering-teams-a15c5eb3849c&quot;&gt;completely distinct data engineering teams&lt;/a&gt; doing different work.  Trying to decide which of those jobs are “research software engineering” jobs and which aren’t is not a productive way forward, for those candidates or for us as a community.&lt;/p&gt;

&lt;p&gt;Needless to say, the growth of remote jobs has been off the charts - especially in the private sector, although the academic institutions are gamely doing what they can to keep up (often hampered by institutional policies).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Late June 2022 update&lt;/strong&gt;: At the time that I write this, there’s a slow down in hiring in tech, especially among early stage-startups.  That slowdown due to economic conditions as I write this is &lt;em&gt;not&lt;/em&gt;, as far as I can tell, affecting these more research-oriented kinds of jobs.  The job board doesn’t have a lot of jobs from startups anyway.  For larger organizations, the biotech firms or the banking firms doing fraud detection research or the computing providers or academic groups or…  clearly do not view these roles as “nice to haves” that can wait until there’s a bit more economic certainty.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;What counts as such a job?  Any job that involves leading, or mentoring people, or managing projects, programs, or products, in software, systems, or data curation/management/engineering/analysis to support the solution of research problems is a good fit.  If you are hiring for such a job, feel free to &lt;a href=&quot;https://airtable.com/shrL6QGic3Mv9JFrs&quot;&gt;submit it to the job board&lt;/a&gt;. &lt;a class=&quot;reversefootnote&quot; href=&quot;https://www.dursi.ca/feed.xml#fnref:1&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;

    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>A supportive job interview story</title>
   <link href="https://hpc.social/2022/a-supportive-job-interview-story/"/>
   <updated>2022-02-25T16:00:00-07:00</updated>
   <id>https://hpc.social/2022/a-supportive-job-interview-story</id>
   <content type="html">&lt;p&gt;(adapted from an &lt;a href=&quot;https://lobste.rs/s/1bwpi8/have_you_ever_had_given_really_good#c_1r7cs6&quot;&gt;old lobste.rs comment&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;My favorite interview ever was a systems interview that didn’t go as planned. This was for an SRE position, and while I expected the interview to be a distributed systems discussion, the interviewer instead wanted to talk kernel internals.&lt;/p&gt;

&lt;p&gt;I was not &lt;em&gt;at all&lt;/em&gt; prepared for this, and admitted it up front. The interviewer said something along the lines of, “well, why don’t we see how it goes anyway?”&lt;/p&gt;

&lt;p&gt;He then proceeded to teach me a &lt;em&gt;ton&lt;/em&gt; about how filesystem drivers work in Linux, in the form of leading me carefully through the interview question he was “asking” me. The interviewer was incredibly encouraging throughout, and we had a good discussion about why certain design decisions worked the way they did.&lt;/p&gt;

&lt;p&gt;I ended the interview (a) convinced I had bombed it, but (b) having had an excellent time anyway and having learned a bunch of new things. I later learned the interviewer had recommended to hire me based on how our conversation had gone, though I didn’t end up taking the job for unrelated reasons having to do with relocation.&lt;/p&gt;

&lt;p&gt;I’ve given a number of similar interviews since, on system design or general sysadmin skills. I’ve always tried to go into these thinking about both where I could learn, and where I could teach, and how either outcome would give the candidate a chance to shine.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Interactive Development Containers</title>
   <link href="https://hpc.social/2022/interactive-development-containers/"/>
   <updated>2022-02-15T12:30:00-07:00</updated>
   <id>https://hpc.social/2022/interactive-development-containers</id>
   <content type="html">&lt;p&gt;I’ve recently been interested in developer workflows. Aside from being a developer, I feel
like the tooling for our community, and especially for HPC or hybrid environments, is lacking.
As a simple example, let’s ask a basic question:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;How do I start developing here and move it over there?&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;For the most part, creating a development container is fairly straight forward, and we can even bind source
code to the host to work on in one editor terminal and then build and run or test in another. However,
for the moving part, it gets shoddy. Our best bet is to rebuild the container with the 
most updated source code, push to a registry, and then pull down somewhere else.
For a container that is a binary and not layers provided by a registry, we could even scp it.
If we do this right, we will have an automated build and deploy that triggers when we 
merge new code into main, but do you see the problem? What about the code that we want
to test that isn’t ready to merge? This is why we typically would need to manually
push to a registry with some kind of “work in progress” tag and then pull somewhere else.
Minimally we’d need to build fresh again, and then reproduce all the steps to set up our environment.&lt;/p&gt;

&lt;h2 id=&quot;interactive-development-containers&quot;&gt;Interactive Development Containers&lt;/h2&gt;

&lt;p&gt;Now I don’t have all the answers, but recently &lt;a href=&quot;https://github.com/alecbcs&quot; target=&quot;_blank&quot;&gt;@alecbcs&lt;/a&gt; and
I have been dreaming about what kinds of development environments we want.
functionality such as:&lt;/p&gt;

&lt;ol class=&quot;custom-counter&quot;&gt;
  &lt;li&gt;Saving the container state without leaving it.&lt;/li&gt;
  &lt;li&gt;Loading or saving or otherwise interacting with named environments.&lt;/li&gt;
  &lt;li&gt;Inspecting or interacting with container metadata, also without leaving the container.&lt;/li&gt;
  &lt;li&gt;Moving files or sizing the container without the same.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;And actually I won’t even get to answering the first question in this post about moving something
from one place to another, but rest assured it is an important one. This post is about some prototype 
or fun testing work that we’ve started around these ideas.
The playground for some of these early ideas has been &lt;a href=&quot;https://syspack.github.io/paks/&quot; target=&quot;_blank&quot;&gt;Paks&lt;/a&gt;.&lt;/p&gt;

&lt;div style=&quot;padding: 20px;&quot;&gt;
 &lt;img src=&quot;https://github.com/syspack/paks/raw/main/docs/assets/img/paks.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Paks is a Python library that I’m calling a developer wrapper for containers.
Mind you, it’s more of a playground right now to experiment with ideas. But I’ve had so
much fun even this early on that I want to share what I’ve learned.&lt;/p&gt;

&lt;h3 id=&quot;wrapper&quot;&gt;Wrapper&lt;/h3&gt;

&lt;p&gt;Because Paks is a wrapper, you will run containers using the paks command. Here are a few quick examples.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;paks run ubuntu
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;paks run &lt;span class=&quot;nt&quot;&gt;--shell&lt;/span&gt; /bin/sh busybox
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;paks run &lt;span class=&quot;nt&quot;&gt;--container-tech&lt;/span&gt; podman busybox

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;What is happening on the backend that took me a bit to figure out is that we will need to run a subprocess,
but create a &lt;a href=&quot;https://docs.python.org/3/library/pty.html&quot; target=&quot;_blank&quot;&gt;pseudo terminal&lt;/a&gt; to better
watch and interact with it. This is going to happen in the “interactive_terminal” command below. But unless you
want your terminal to get wonky, we need to use &lt;a href=&quot;https://docs.python.org/3/library/termios.html&quot; target=&quot;_blank&quot;&gt;termios&lt;/a&gt; to 
grab the current tty and make sure it gets restored no matter what at the end. That looks like this:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;interactive_command&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cmd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
        Ensure we always restore original TTY otherwise terminal gets messed up
        &quot;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# Controller to get history
&lt;/span&gt;        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;commands&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;history&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# save original tty setting then set it to raw mode
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;old_tty&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;termios&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tcgetattr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stdin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;old_pty&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;termios&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tcgetattr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stdout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_interactive_command&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cmd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;finally&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;termios&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tcsetattr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stdin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;termios&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TCSADRAIN&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;old_tty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;termios&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tcsetattr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stdout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;termios&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TCSADRAIN&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;old_pty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;What happens if you don’t do that? Your terminal gets weird and wonky. And then in the interactive
command function, this is where we launch a subprocess with a new pseudo terminal:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
        &lt;span class=&quot;n&quot;&gt;tty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setraw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stdin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fileno&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# open pseudo-terminal to interact with subprocess
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;openpty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;opentty&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;openpty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# use os.setsid() make it run in a new process group, or bash job control will not be enabled
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subprocess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Popen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;cmd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;preexec_fn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setsid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;stdin&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;opentty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;stdout&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;opentty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;stderr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;opentty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;universal_newlines&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Welcome to Paks!
&lt;/span&gt;        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;welcome&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;openpty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The &lt;a href=&quot;https://stackoverflow.com/questions/45911705/why-use-os-setsid-in-python&quot; target=&quot;_blank&quot;&gt;setsid&lt;/a&gt; as a pre-exec function
 is ensuring the child process is a new session and won’t exit, sort of akin to a daemon. 
So at face value, yes it is doing exactly what you think - we are shelling into the container
and watching the command line and looking for paks-known commands. And I didn’t use a Python keylogger because
I found that &lt;a href=&quot;https://github.com/boppreh/keyboard&quot; target=&quot;_blank&quot;&gt;keyboard&lt;/a&gt; requires sudo (like really?!) 
and &lt;a href=&quot;https://pynput.readthedocs.io/en/latest/&quot; target=&quot;_blank&quot;&gt;pynput&lt;/a&gt; is really scary because it doesn’t just get keys from the terminal - it’s watching anything you type anywhere! That gave me the heebie jeebies. I hope there is some scanner for pypi that is looking for that package
and checking it’s not being malicious.&lt;/p&gt;

&lt;p&gt;All of the above said, and all the time spent, I’m not convinced that this exact method is
the best way to be running commands from inside the container. There are other ideas
that need to be tested!&lt;/p&gt;

&lt;h3 id=&quot;structure&quot;&gt;Structure&lt;/h3&gt;

&lt;p&gt;We could have talked about this first, but let me show you the basic structure of paks
so you get an understanding of the components.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;paks

# Backends are different wrappers, so logically we start with podman and docker
├── backends
│   ├── base.py
│   ├── docker.py
│   ├── __init__.py
│   └── podman.py

# The client is what you interact with on the command line. This shows the various commands available.
├── cli
│   ├── config.py
│   ├── env.py
│   ├── __init__.py
│   └── run.py

# This is a central controller for things
├── client.py

# Here's all the built-in, interactive commands paks supports!
├── commands
│   ├── command.py
│   ├── cp.py
│   ├── env.py
│   ├── history.py
│   ├── __init__.py
│   ├── inspect.py
│   └── state.py
├── defaults.py
├── env.py
├── logger.py

# Coming soon - load your own commands!
├── plugins.py
├── schemas.py
├── settings.py
├── settings.yml
├── templates.py
├── utils
└── version.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;So that should give you the gist - we have container wrappers (backends) and then
commands that we can issue while we are inside the container. Let’s talk about them next.&lt;/p&gt;

&lt;h3 id=&quot;saving-state&quot;&gt;Saving State&lt;/h3&gt;

&lt;p&gt;The first thing I wanted to try with Paks was to save a container state, but not needing
to open a separate terminal and save from the outside. The use case for this is that given I’m in an interactive
container and I’ve made some changes, I don’t want to exit and rebuild. All y’all reproducibility folks
can stop wincing, and realize that we also need more temporary or throwaway development environments like this.
Reproducibilty is important, but mostly for the final production thing, and only up to a level of not
giving us pain. So how might I do this?&lt;/p&gt;

&lt;p&gt;For paks, while you are inside the container (let’s say ubuntu) you simply ask to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;#save&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
$ paks run ubuntu
# touch PANCAKES
# #save
Saving container...
sha256:d82aaa268feb59344cf31a757ce7f5c0caa6a6bbd10b8d0af1d55cdbc50b609b
[+] Building 0.2s (5/5) FINISHED
...
=&amp;gt; =&amp;gt; writing image sha256:f58ae524d8644400b33c078f19612cba7849ef8f3ea158e2291ac697a4129080
=&amp;gt; =&amp;gt; naming to docker.io/library/busybox-saved
Untagged: dockerio-busybox-joyous-hippo-3922-gloopy-peanut-9044:latest
Deleted: sha256:d82aaa268feb59344cf31a757ce7f5c0caa6a6bbd10b8d0af1d55cdbc50b609b
Deleted: sha256:f58ae524d8644400b33c078f19612cba7849ef8f3ea158e2291ac697a4129080
Successfully saved container! ⭐️
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;And then you can see that there is an ubuntu-saved container!&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker images | &lt;span class=&quot;nb&quot;&gt;grep &lt;/span&gt;ubuntu
ubuntu-saved                                      latest    93e336d994de   2 minutes ago   72.8MB
ubuntu                                            latest    54c9d81cbb44   7 days ago      72.8MB

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;So this has saved me some tiny bit of energy to open up another terminal, remember how to docker commit,
and then also rebuild with a squash to minimize the layers (as there is a maximum number we don’t want to hit).
What Paks could then eventually do is make it easy to move this entire container between
places, e.g., from your local machine to HPC without a hitch. I haven’t started to work on that yet
because this is a fun side project.&lt;/p&gt;

&lt;h3 id=&quot;environments&quot;&gt;Environments&lt;/h3&gt;

&lt;p&gt;One thing I do a lot is use GitHub tokens to do fun stuff with the API. I usually need to
keep this in some hidden file, then find it, open it, copy paste it, and export it in the container.
And then I do that a million times when I have to run a new container. But with Paks, we can 
create a named environment on the host (a file to source with exports):&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;paks &lt;span class=&quot;nb&quot;&gt;env &lt;/span&gt;edit github
You can also quickly show an environment:

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;paks &lt;span class=&quot;nb&quot;&gt;env &lt;/span&gt;show github
&lt;span class=&quot;nv&quot;&gt;GITHUB_TOKEN&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;xxxxxxxxxxx

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;And then in our container, as many times as we need, load it seamlessly!&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
root@9ec6c3d43591:/# &lt;span class=&quot;c&quot;&gt;#envload github&lt;/span&gt;
Loading environment...
Successfully loaded environment github

root@9ec6c3d43591:/#  &lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;GITHUB_TOKEN&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;xxxxxxxxx
root@9ec6c3d43591:/#  &lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;GITHUB_USER&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;dinosaur

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;If only my GitHub username was dinosaur! 😁️ Is it loaded?&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
root@9ec6c3d43591:/# &lt;span class=&quot;nb&quot;&gt;env&lt;/span&gt; | &lt;span class=&quot;nb&quot;&gt;grep &lt;/span&gt;GITHUB
&lt;span class=&quot;nv&quot;&gt;GITHUB_USER&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;dinosaur
&lt;span class=&quot;nv&quot;&gt;GITHUB_TOKEN&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;xxxxxxxxx

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Okay, so to be fair, there are a bunch of other commands for inspection and size,
and I’m not going to go through them all! You can see them 
&lt;a href=&quot;https://syspack.github.io/paks/getting_started/user-guide.html&quot; target=&quot;_blank&quot;&gt;in the Paks user guide&lt;/a&gt;.
And I don’t mean to say you should use this - you probably shouldn’t. But you might be interested to try it out.&lt;/p&gt;

&lt;h3 id=&quot;parsing-keystrokes&quot;&gt;Parsing Keystrokes&lt;/h3&gt;

&lt;p&gt;So the most interesting part of this project has been learning about input from the terminal,
and actually the reason I wanted to write this post to share what I learned. Let’s go back to the interactive
function where we ran subprocess and created a pseudo terminal. There actually is a pretty simple way
to watch what is being typed:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# This is the subprocess return code, keep going until we are done (e.g. have a return code)
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;poll&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Wait for io completion (e.g., see man select)
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;select&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;select&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stdin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;openpty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[])&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Was it a new input?
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stdin&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;terminal_input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stdin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fileno&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10240&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;new_char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;terminal_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;utf-8&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Do something with what you see here
&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Was it a new output?
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;openpty&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;o&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;openpty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10240&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stdout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fileno&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;I learned a lot from this! Let’s talk about it.&lt;/p&gt;

&lt;h4 id=&quot;debugging&quot;&gt;Debugging&lt;/h4&gt;

&lt;p&gt;So the first thing I learned is that my typical “import IPython” and “IPython.embed()”
isn’t going to work as easily as normal, because (at least superficially) I didn’t
see a way to have it sort of injected into the process. Anything that is interactive in
that loop is still (conceptually) running on my host. So when I use IPython
it does some weird stuff with carriage returns, but it’s still possible to interact with
a little bit. So what I wound up doing so I could easily see every keypress was to write
to file in append mode:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'/tmp/file.txt'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'a'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_char&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;This was kind of neat because I could be typing in one terminal, and then have
a file open (watching it) that updates with changes, and I’d get a sense of what
is going on. I could append anything to this file to debug. And this is also really
different from how we normally use subprocess, where maybe we will parse entire lines
at once:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subprocess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Popen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'python'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'thing.py'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stdout&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subprocess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PIPE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stdout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;readline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;because we are reading on character at a time! So what we essentially need to do
is keep a string that we continue appending to unless there is a newline, up or down,
or left or right to indicate moving the cursor.&lt;/p&gt;

&lt;h4 id=&quot;ascii-characters&quot;&gt;Ascii Characters&lt;/h4&gt;

&lt;p&gt;I started to quickly see characters that my editor didn’t know - e.g., likely
escape sequences and other ascii that showed up in the little question mark box.
I quickly realized that I was seeing &lt;a href=&quot;https://www.w3resource.com/python-exercises/python-basic-exercise-86.php&quot; target=&quot;_blank&quot;&gt;ascii&lt;/a&gt;
code (and some characters that couldn’t be parsed) so the solution was to look at the ord
of the character and compare to a number. For example, for a backspace
the number is 127. So to act on it I might do:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;c1&quot;&gt;# if we have a backspace (ord 127)
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_char&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;ord&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_char&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;127&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# This is our in progress line. If we have content, backspace!
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;string_input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;string_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# But if we don't, just write the character for the person to see and 
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# keep collecting new characters (continue in the loop)
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;string_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;openpty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;terminal_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;
    
&lt;span class=&quot;c1&quot;&gt;# Otherwise (not a backspace) add to our growing line to parse further!
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;string_input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;string_input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new_char&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The above is basically looking for a backspace, and if we find one, we remove
one character from the line we are assembling. Otherwise we just add the new character
to the line.&lt;/p&gt;

&lt;h4 id=&quot;xterm-sequences&quot;&gt;xterm sequences&lt;/h4&gt;

&lt;p&gt;And a similar thing happens for pressing up/down and right/left, except the
terminal parses them as “[A”, “[B”, “[C”, and “[D”, respectively, and often with
an escape sequence first. There are &lt;a href=&quot;https://en.wikipedia.org/wiki/ANSI_escape_code&quot; target=&quot;_blank&quot;&gt;some nice tables here&lt;/a&gt;
for the interested reader! And this was also the point that I realized how challenging parsing input is!
Along with needing to account for every character, you also need to account for platform
differences. That’s also why I view this library as mostly for development and thinking,
or at least for mostly Linux and bash shells, because I’m not sure I could ever handle them all.
So for the purposes of my library, for now I decided I’m not going to handle moving left and right,
nor do I want to deal with weird extra ascii characters that are added, so I just clean them up.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;c1&quot;&gt;# Get rid of left/right
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string_input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;string_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;[D&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;[C&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Replace weird characters and escape sequences
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string_input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Yes, that probably means some of your ninja shortcuts won’t work perfectly when running paks,
and if you absolutely want one to be parsed please let me know and we can add it.&lt;/p&gt;

&lt;h4 id=&quot;newlines&quot;&gt;Newlines&lt;/h4&gt;

&lt;p&gt;So the gold nugget of content that Paks is interested in is when you press enter.
This means you’ve finished typing something and there is some version of a newline
or carriage return. This is also a pretty variable thing depending on the platform you are
on - newlines can come in very different forms! I tried to honor the two that I see most often:&lt;/p&gt;

&lt;ol class=&quot;custom-counter&quot;&gt;
  &lt;li&gt;&lt;strong&gt;\r\n&lt;/strong&gt;: Windows &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;\n&lt;/strong&gt;: UNIX (e.g., Mac OSX)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;\r&lt;/strong&gt;: Mac (pre OSX)&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;has_newline&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;string_input&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\r&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;string_input&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;At this point, we can start acting on what we see. E.g., if the user has asked for any
kind of exit, I honor it.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Universal exit command
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;exit&quot;&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;string_input&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;has_newline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n\r&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Container exited.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n\r&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uri&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;extended_name&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The return of the name at the end is to handle cleaning up the image, which was allocated
a temporary name.&lt;/p&gt;

&lt;h3 id=&quot;history&quot;&gt;History&lt;/h3&gt;

&lt;p&gt;One of the more interesting parts of this project was realizing that people use history, a lot.
At least I do. This is going to appear as an up or down press, and only when a newline is found 
is some item in history re-executed. So first let’s look for exploring history with up/down. There are
two cases - pressing up/down without a newline:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Pressing up or down, but not enter
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;[A&quot;&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;string_input&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;[B&quot;&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;string_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;has_newline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;string_input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_history&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;openpty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;openpty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;terminal_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;And with one:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Pressing up or down with enter
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;[A&quot;&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;string_input&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;[B&quot;&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;string_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;has_newline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;string_input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_history&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;openpty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;openpty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;terminal_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;If we don’t have a newline, we add a continue to keep parsing characters the user is
typing. If we do have a newline, we let the loop keep running to keep parsing the line of history we retrieved.
But let’s step back and talk about that history. We basically want to retrieve whatever line of history that
the user is asking for, because to us it looks like up and down errors. You could imagine
restoring the previous line, and then editing it. This actually proved to be quite challenging,
because I realized (by default) when we start running a container (well, ubuntu and centos)
the history is stored in memory and not written to ~/.bash_history. This led to 
&lt;a href=&quot;https://twitter.com/vsoch/status/1492377777684639748&quot; target=&quot;_blank&quot;&gt;this thread&lt;/a&gt; and some people coming in to &lt;a href=&quot;https://twitter.com/ajdecon/status/1492381132998033409&quot; target=&quot;_blank&quot;&gt;quickly help&lt;/a&gt;
and others coming in just to say “Why are you doing this with containers it makes no sense stop.” Yeah, right. If I
listened to every person that has ever told me to stop working on something because “REASONS!” I wouldn’t
ultimately work on much at all.&lt;/p&gt;

&lt;p&gt;The short answer was that I needed a function to be able to get a line of history, and based on the 
number of times pressing up or down. For my first attempt I said “nevermind this, I’ll just save my own history!”
but that got hugely complicated very fast because it turns out, we don’t just stupidly type commands over and over,
we are constantly using more characters on the keyboard than letters and numbers, retrieving old things to edit,
updating again, and in practice I found that I could keep up with simple parsing, but it would get out of sync
for a longer session. There also is the issue that people can tweak the amount of history saved, or how it’s saved, 
and there are a set of environment &lt;a href=&quot;https://www.redhat.com/sysadmin/history-command&quot; target=&quot;_blank&quot;&gt;variables and commands&lt;/a&gt;
to do that. So most containers will start running and save history to memory and not file (and this makes
sense in case there is sensitive information) but it was problematic for me because I couldn’t parse it.
For example, when someone presses up and down a bunch of times, I might see:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;A[A[A[A[A[B[A
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;This is a reference to some previous command that I can only find in history
given I’m parsing the input/output as I am. So my second attempt (well, maybe second through
tenth) I was trying different variations of trying to be able to parse the history.
If you looked at &lt;a href=&quot;https://twitter.com/ajdecon/status/1492381132998033409&quot; target=&quot;_blank&quot;&gt;the tweet&lt;/a&gt;
you’ll see we need to run:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;history&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-a&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;to start writing what’s in memory to file. I didn’t want to do this on every command, because along
with the user seeing it and the UI being awful, it was just too much. Instead, I realized that I had a small
opportunity when the user first shells into the container (and is expecting a jump in their UI) to run whatever
I need and then clear the terminal. So I ran it there, right before a clear and welcome message.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;welcome&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;openpty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
        Welcome the user and clear terminal
        &quot;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# Don't add commands executed to history
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;openpty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot; export PROMPT_COMMAND='history -a'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\r&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;openpty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot; clear&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\r&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;openpty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot; ### Welcome to PAKS! ###&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\r&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;And with this method you aren’t aware of the extra commands at all! And did you notice the spaces above? That’s also another trick! Any command that you type with a leading
space won’t be saved to history, and this is thanks to &lt;a href=&quot;https://unix.stackexchange.com/questions/115934/why-does-bash-have-a-histcontrol-ignorespace-option&quot;&gt;HISTCONTROL&lt;/a&gt; that has an ignorespace option. I think most people / containers
set it to ignore space and to ignore duplicates:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;n&quot;&gt;root&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c268386714a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# echo $HISTCONTROL
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ignoredups&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ignorespace&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;That said, I don’t explicitly try to reset this in the container, so that could be a bug
if there is a container base that doesn’t do that. And I’m pretty sure centos doesn’t come with clear!
I’ll likely need to work on this a bit more.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;For now, please consider this only working for debian/ubuntu bases and we can inspect the other ones later!&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;Okay, so now let’s look at the function to get history (self.hist.run). For now, just ignore the command to
get the history, that’s actually done via a Paks command that we will talk about after.
Here is what is going on:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_history&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;openpty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
    Given an input with some number of up/down and newline, derive command.
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Calculate the absolute change of ups/downs
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;up&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;[A&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;down&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;[B&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;change&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;up&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;down&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# pushed down below history (maybe they are angry?)
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;change&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
       &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Retrieve history, actually via a command run from the outside to get the file
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;history&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;container_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uri&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;extended_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;openpty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;history_file&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;settings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;history_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;settings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;history&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;history&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# No history, nothing to return
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;history&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# The change is outside the length of history
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;change&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;history&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# here we are looking back up into history (negative index)
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;newline&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;history&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;change&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Add back any characters typed AFTER the up/down presses
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;newline&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;re&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;(\[A|\[B)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;newline&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The above might not be perfect, but it worked the best for everything that I tried!
This allows us to issue a command that paks knows, press up to get it again, and then edit
it and have the command work correctly. Speaking of commands…&lt;/p&gt;

&lt;h3 id=&quot;commands&quot;&gt;Commands&lt;/h3&gt;

&lt;p&gt;The core meat of paks is the commands that it recognizes. Every command has a &lt;a href=&quot;https://github.com/syspack/paks/blob/ab61458a061c555434e5d3406914612fd1d60442/paks/commands/command.py#L26&quot; target=&quot;_blank&quot;&gt;base class&lt;/a&gt;
that is going to handle parsing a line (with a main command and optional args or kwargs, depending on the command),
ensuring all required variables are passed (this is largely internal to the library and even a developer user
doesn’t need to think about it unless they want to change what is passed), and then providing functions for basic kinds of
execution. So let’s step back and first look at how we find a command (or executor). Basically, once we have a newline
and we’ve parsed it per the above (looking up history and such) we can sniff it to see if it matches a known
command pattern:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# If we have a newline (and possibly a command)
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;has_newline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run_executor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;openpty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Add derived line to the history
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;openpty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;terminal_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;string_input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The function “run_executor” is going to make this call if there is a Paks command and handle it.
And no matter what, we reset our string input to be empty given that the user pressed enter, because
they are going to start typing fresh. But before that, this function “run_executor” is going to see
if there are any known commands, and if so, to run them! That function looks like this:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;run_executor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;string_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;openpty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
    Given a string input, run executor
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Get out early if it's not a Paks command (always starts with #)
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;string_input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;string_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;[A&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;[B&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;string_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;startswith&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;#&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Do we have a matching executor?
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;executor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;commands&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_executor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;openpty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;executor&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Print any message it wants to the terminal before run...
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;executor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pre_message&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n\r&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;executor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pre_message&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Run it!
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;executor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;container_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uri&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;extended_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;original&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# And any message it wants to print after
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\r&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The result object holds what you would expect - a return code, some message,
and the basic outputs of the call. It’s up to the executor (command) to decide
what to show the user. Some might not show anything beyond commands that are run
with the executor. So what does that function “get_executor” look like?
This is where we delive into the commands module, where there is a simple lookup of
the starting prefixes of commands matched to Command classes:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;c1&quot;&gt;# lookup of named commands and settings
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;docker_commands&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;#save&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SaveContainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;#inspect&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;InspectContainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;#envload&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;EnvLoad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;#envhost&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;EnvHost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;#envsave&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;EnvSave&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;#cp&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Copy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;#size&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;When I add a load functionality, all it will need to do is update this dictionary.
And the reason those are “docker commands” is that you can imagine we eventually
support other container technologies, and the commands you run are going to vary.
Each Command actually has a class attribute for the container types that are supported.
Here is a snippet of the DockerCommands class attached to the client that we are calling “get_executor” on:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DockerCommands&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Required kwargs for any docker/podman command to run
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;required&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;container_name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;container_tech&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;command&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;container_tech&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lookup&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;docker_commands&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;parse_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cmd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;parts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cmd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot; &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\r&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;has_command&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parse_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lookup&lt;/span&gt;

    &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;property&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;history&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;History&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_executor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
        Backend is required to update history
        &quot;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parse_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lookup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lookup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;](&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;required&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;required&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;To focus on the last function, you basically see that we parse the line (name), and then
see if it’s in our lookup. If so, we return the initialized executor, and we need to add
the output source in case it needs to interact with the current terminal. The self.command
refers to the container technology (e.g., docker or podman in this case).&lt;/p&gt;

&lt;p&gt;Then we can look at a particular command (e.g., inspect) and see it’s pretty simple! We have defined
the supported container technologies along with optional messages, and a main run function. Here is the command
to inspect, which will dump out the json manifest and optionally take a section:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;InspectContainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Command&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;supported_for&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;docker&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;podman&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pre_message&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Inspecting Container...&quot;&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
        Inspect a container fully, or specific sections
        &quot;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# Always run this first to make sure container tech is valid
&lt;/span&gt;        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;check&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# These are both required for docker/podman
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;container_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;container_name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# inspect particular attributes provided as args
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;section&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run_command&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
                        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tech&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;s&quot;&gt;&quot;inspect&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;s&quot;&gt;&quot;--format&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;section&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;capitalize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;container_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Otherwise just dump the whole thing
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run_command&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tech&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;inspect&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;container_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;return_success&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;You’ll now know the main Paks trick - because we are still running on the host,
we can issue commands to the host while we are in the container! In the above, we can just type:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;c&quot;&gt;#inspect&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#inspect config&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;And see the output in the terminal! This is how a lot of the interactions with the host work.
It’s kind of simple and silly, but also really cool when you see it work on the container!
So the run function above, just as a reminder, is called by this part:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;executor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;container_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uri&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;extended_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;original&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;And honestly, that’s the majority of Paks! 🎉️&lt;/p&gt;

&lt;h2 id=&quot;discussion&quot;&gt;Discussion&lt;/h2&gt;

&lt;p&gt;Paks has  honestly been so fun to work on, despite long hours of trying to figure things out during evenings and weekends. I’m so excited
about the ideas, and I want to share them with others because I think developer tools for containers
are kind of lacking. Heck, I stayed up until like 4am writing this post. No, I don’t normally do that,
I had some things on my mind, but it was an excellent use of the time, despite the fact that I woke up 4 hours later and
I’m going to crash tonight (err tomorrow night… err now that I’m tweaking up the finishing touches to this post)!&lt;/p&gt;

&lt;h3 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h3&gt;

&lt;p&gt;I’m working on a “paks load” command that will let someone develop a Python module
with some set of commands for their custom use case. The first thing I wanted to try
was to generate sboms for spack (e.g., “Generate sboms for this spack install in the container
and save them to my host so I can upload alongside the container to a registry). I had
some &lt;a href=&quot;https://github.com/spack/spack-sbom&quot; target=&quot;_blank&quot;&gt;previous work&lt;/a&gt; to use 
spack scripting, but ultimately this weekend did a &lt;a href=&quot;https://github.com/spack/spack/pull/28909&quot; target=&quot;_blank&quot;&gt;pull request&lt;/a&gt;
to add sbom generation to spack proper. And then I’ll be able to work on the load commands.
I also want to address some of the anticipated bugs I mentioned above, like properly setting “HISTCONTROL”
to ensure we don’t save commands issued by the client to history, and possibly having a cleanup step on save
that removes the file. I haven’t added this yet is because if I’m developing in the container 
and want to say, move it from my local machine to HPC, I kind of want to have my history so I can lazily use it.&lt;/p&gt;

&lt;h3 id=&quot;but-really&quot;&gt;But Really…&lt;/h3&gt;

&lt;p&gt;We have some magic up our sleeves for what we are actually working on to inspire these ideas!
I guess you’ll just have to wait for the future, because &lt;a href=&quot;https://github.com/alecbcs&quot; target=&quot;_blank&quot;&gt;@alecbcs&lt;/a&gt; and
I are both have vision and are a great tag team! 🎉️&lt;/p&gt;

&lt;h3 id=&quot;security&quot;&gt;Security&lt;/h3&gt;

&lt;p&gt;So there are obviously security issues around a library like this - and I added notes
to the documentation that I’ll re-iterate here. Paks is intended for use by a developer
that is in their own trusted environment, whether local or on HPC. Because there is an interaction
with the host, you wouldn’t use this in production someone to give users an ability to load
environments or save. You also wouldn’t want to save a development container with something
private in history and push it. I’m still an advocate for, after development is done,
pushing changed code to GitHub and having an automated build build, test, and deploy.
Could we eventually have a production grade library to enable interactions inside the
container? Possibly, but it’s not Paks in Python in its current state. I think
that’s okay - we have to start small with ideas and go from there.&lt;/p&gt;

&lt;h3 id=&quot;didnt-i-see-paks-before&quot;&gt;Didn’t I see paks before?&lt;/h3&gt;

&lt;p&gt;Yes, you did! A previous version was intended for making spack build caches on GitHub, but that
didn’t work because you couldn’t build a spack package within a container and then
pull the same container and install it and hit the cache. I think this might work someday,
hence why I haven’t completely deleted the code, but I couldn’t let a cute logo and colorscheme go to waste!
So for now it’s on a separate branch but largely I am not working on it. If you want to see this branch,
it’s still &lt;a href=&quot;https://github.com/syspack/paks/tree/v1/spack&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;Thanks for reading friends! I hope this has been interesting and you might be inspired to
also work on better tooling for developers, even if that just means exploring the ideas.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Developing managed vs self-hosted software</title>
   <link href="https://hpc.social/2022/developing-managed-vs-self-hosted-software/"/>
   <updated>2022-02-12T16:00:00-07:00</updated>
   <id>https://hpc.social/2022/developing-managed-vs-self-hosted-software</id>
   <content type="html">&lt;p&gt;I&amp;#8217;ve done some work lately with teams that deliver their products in very different ways, and it has me thinking about how much our &amp;#8220;best practices&amp;#8221; depend on a product&amp;#8217;s delivery and operations model. I&amp;#8217;ve had a bunch of conversations about this tension&lt;/p&gt;

&lt;p&gt;On the one hand, some of the teams I&amp;#8217;ve worked with build software services that are developed and operated by the same team, and where the customers (internal or external) directly make use of the operated service. These teams try to follow what I think of as &amp;#8220;conventional&amp;#8221; SaaS best practices:&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;Their development workflow prioritizes iteration speed above all else&lt;/li&gt;&lt;li&gt;They tend to deploy from HEAD, or close to it, in their source repository&lt;ul&gt;&lt;li&gt;In almost all cases, branches are short-lived for feature development&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;They&amp;#8217;ve built good automated test suites and well-tuned CI/CD pipelines&lt;/li&gt;&lt;li&gt;Releases are very frequent&lt;/li&gt;&lt;li&gt;They make extensive use of observability tooling, often using third-party SaaS for this&lt;/li&gt;&lt;li&gt;Fast roll-back is prioritized over perfect testing ahead of time&lt;/li&gt;&lt;li&gt;While their user documentation is mostly good, their operations documentation tends to be &amp;#8220;just good enough&amp;#8221; to onboard new team members, and a lot of it lives in Slack&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;However, we also have plenty of customers who deploy our software to their own systems, whether in the cloud or on-premise. (Some of them don&amp;#8217;t even connect to the Internet on a regular basis!) The development workflow for software aimed at these customers looks rather different:&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;Deploys are managed by the customer, and release cycles are longer&lt;/li&gt;&lt;li&gt;These teams do still have CI/CD and extensive automated tests&amp;#8230; but they may also have explicit QA steps before releases&lt;/li&gt;&lt;li&gt;There tend to be lots of longer-lived version branches, and even &amp;#8220;LTS&amp;#8221; branches with their own roadmaps&lt;/li&gt;&lt;li&gt;Logging is prioritized over observability, because they can&amp;#8217;t make assumptions about the customer tooling&lt;/li&gt;&lt;li&gt;They put a lot more effort into operational documentation, because most operators will not also be developers&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;From a developer perspective, of course, this all feels much more painful! The managed service use case feels much more comfortable to develop for, and most of the community tooling and best practices for web development seems to optimize for that model.&lt;/p&gt;

&lt;p&gt;But from a sysadmin perspective, used to mostly operating third-party software, the constraints of self-hosted development are all very familiar. And even managed service teams often rely on third-party software developed using this kind of model, relying on LTS releases of Linux distributions and pinning major versions of dependencies.&lt;/p&gt;

&lt;p&gt;The biggest challenge I&amp;#8217;ve seen, however, is when a development team tries to target the same software at &lt;em&gt;both use cases&lt;/em&gt;. As far as I can tell, it&amp;#8217;s very difficult to simultaneously operate a reliable service that is being continuously developed and deployed, and to provide predictable and high-quality releases to self-hosted customers.&lt;/p&gt;

&lt;p&gt;So far, I&amp;#8217;ve seen this tension resolved in three different ways:&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;The internal service becomes &amp;#8220;just another customer&amp;#8221;, operating something close to the latest external release, resulting in a slower release cycle for the internal service&lt;/li&gt;&lt;li&gt;Fast development for the internal service gets prioritized, with external releases becoming less frequent and including bigger and bigger changes&lt;/li&gt;&lt;li&gt;Internal and external diverge completely, with separate development teams taking over (and often a name change for one of them)&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;I don&amp;#8217;t really have a conclusion here, except that I don&amp;#8217;t really love any of these results. /sigh&lt;/p&gt;

&lt;p&gt;If you&amp;#8217;re reading this and have run into similar tensions, how have you seen this resolved? Have you seen any success stories in deploying the same code internally and externally? Or alternatively &amp;#8212; any interesting stories of failure to share? &lt;img alt=&quot;😉&quot; class=&quot;wp-smiley&quot; src=&quot;https://s.w.org/images/core/emoji/14.0.0/72x72/1f609.png&quot; style=&quot;height: 1em;&quot; /&gt; Feel free to &lt;a href=&quot;mailto:ajdecon@ajdecon.org&quot;&gt;send me an email&lt;/a&gt;, I&amp;#8217;d be interested to hear from you.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Toy programs for learning a new language</title>
   <link href="https://hpc.social/2022/toy-programs-for-learning-a-new-language/"/>
   <updated>2022-01-15T16:00:00-07:00</updated>
   <id>https://hpc.social/2022/toy-programs-for-learning-a-new-language</id>
   <content type="html">&lt;p&gt;It used to be that I&amp;#8217;d get interested in a new programming language, but I wouldn&amp;#8217;t have a new project to use it for and had trouble knowing how to start. I have trouble really grasping a new language without building something in it, and &amp;#8220;X by example&amp;#8221; or working through a book don&amp;#8217;t really do the job.&lt;/p&gt;

&lt;p&gt;What&amp;#8217;s helped me lately is to build an array of &amp;#8220;standard&amp;#8221; toy programs that I understand reasonably well, and that I can use to explore the new language and figure out how to do something real in it.&lt;/p&gt;

&lt;p&gt;Right now, my toy program collection consists of:&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;A link shortening service, like &lt;a href=&quot;https://bit.ly&quot;&gt;bit.ly&lt;/a&gt; or &lt;a href=&quot;https://tinyurl.com/&quot;&gt;tinyurl&lt;/a&gt;, along with a HTTP API for adding and removing links&lt;/li&gt;&lt;li&gt;A &lt;a href=&quot;https://scipython.com/book/chapter-7-matplotlib/examples/the-two-dimensional-diffusion-equation/&quot;&gt;2D diffusion simulation&lt;/a&gt;&lt;/li&gt;&lt;li&gt;A &amp;#8220;system package inventory&amp;#8221; program, that builds a list of all the RPMs/DEBs installed on a Linux machine and shoves them into a SQLite database&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;This is almost never what I&amp;#8217;d call production-quality code. For example, when I&amp;#8217;m writing these toy programs, I rarely write unit tests (until I start exploring the test libraries for the language!). But they&amp;#8217;re still very valuable learning tools, and give me space to explore some very different use-cases.&lt;/p&gt;

&lt;p&gt;I almost always write all three in a given language, but the order depends a lot on what I think the new language will be good for. For example, I&amp;#8217;ll  write the &amp;#8220;system package inventory&amp;#8221; program first if I think the new language might be handy for system administration tools. It&amp;#8217;s a great way to see how well the language plays with a common Linux environment, how painful it is to use SQLite, and to get practice writing CLI tools in it. I&amp;#8217;ll often augment the basic &amp;#8220;scan and store&amp;#8221; functionality with a CLI to do frequent queries, like &amp;#8220;on what date was this package last upgraded&amp;#8221;.&lt;/p&gt;

&lt;p&gt;On the other hand, if I think I&amp;#8217;m going to use the new language for a bunch of numerical work, I&amp;#8217;ll start with the diffusion simulation. When I write that, I often start with a naive implementation and then start playing with profilers and other performance tools, or try to parallelize the simulation. This is also a great excuse to dig into any plotting tools commonly used with the language.&lt;/p&gt;

&lt;p&gt;These toy programs are also handy if I want to explore new ways to integrate a service into a larger production environment. For example, I might start with the link shortening service, deploying the service itself statelessly and persisting the list of links into a PostgreSQL DB. Then I start complicating things&amp;#8230;&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;Let&amp;#8217;s add logging!&lt;/li&gt;&lt;li&gt;And tracing!&lt;/li&gt;&lt;li&gt;It&amp;#8217;s always a good idea to expose Prometheus metrics&lt;/li&gt;&lt;li&gt;And wouldn&amp;#8217;t it be handy to support multiple database backends?&lt;/li&gt;&lt;li&gt;Now wrap it all in a Helm chart for handy deployment&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;I imagine I&amp;#8217;m not the only person to have a standard collection of learning projects for new languages. If you do this too, what does your project list look like?&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Cache Age Binning PR Finally Merged!</title>
   <link href="https://hpc.social/2022/cache-age-binning-pr-finally-merged/"/>
   <updated>2022-01-12T00:00:00-07:00</updated>
   <id>https://hpc.social/2022/cache-age-binning-pr-finally-merged-</id>
   <content type="html">&lt;p&gt;I’ve had this PR hanging around in various forms for years.  It’s basically the last peice of the OSD memory target code.  We can now get a “binned” view of the relative ages of items in different LRU caches and dynamically adjust target sizes for different caches.  PR is &lt;a href=&quot;https://github.com/ceph/ceph/pull/43299&quot;&gt;here&lt;/a&gt; and memory usage behavior charts are &lt;a href=&quot;https://docs.google.com/spreadsheets/d/1lSp2cLzYmRfPILDCyLMXciIfdf0OvSFngwXukQFXIqQ/edit?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Things that are Hard</title>
   <link href="https://hpc.social/2022/things-that-are-hard/"/>
   <updated>2022-01-07T12:30:00-07:00</updated>
   <id>https://hpc.social/2022/things-that-are-hard</id>
   <content type="html">&lt;p&gt;I saw a funny tweet on Twitter the other night - it was someone from a large consumer company sharing
their vision for “&lt;a href=&quot;https://hypebeast.com/2022/1/walmart-2017-mutual-mobile-metaverse-shopping-video-resurfaces&quot; target=&quot;_blank&quot;&gt;the next generation shopping experience&lt;/a&gt;” and it was a virtual person walking through a supermarket aisle and reaching out to pick up a bottle of wine.
I can’t find the specific tweet, but it said something to the effect of:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Nobody asked for this. Stop making stuff to solve problems that people don’t have&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;My dear reader, it me! 😲️ This message hit me really hard, because I am definitely one to build niche tools for use cases that likely don’t exist but seem fun or interesting to me. I also feel pretty &lt;a href=&quot;https://twitter.com/vsoch/status/1478913234136494081&quot; target=&quot;_blank&quot;&gt;disconnected&lt;/a&gt; from communities that are innovating and testing new ideas.&lt;/p&gt;

&lt;h2 id=&quot;what-is-hard&quot;&gt;What is hard?&lt;/h2&gt;

&lt;p&gt;This is a problem that a lot of us have. We build things that nobody needs. We need to focus more on the problems that people are actually facing. I would also scope that to developer workflows, which includes automation, testing, and development. Since I have a nice view into my own mental space, here is my list of things that are hard.&lt;/p&gt;

&lt;ol class=&quot;custom-counter&quot;&gt;
  &lt;li&gt;When I am trying to develop software and I can't open an interface with the code and environment I need&lt;/li&gt;
  &lt;li&gt;That my main interaction with a resource is via SSH&lt;/li&gt;
  &lt;li&gt;When a workflow or even container works in one place but not another&lt;/li&gt;
  &lt;li&gt;When I need to develop, build in CI, push to a registry, and pull. One mistake? Start from scratch&lt;/li&gt;
  &lt;li&gt;When I need to run a job and I have to interact with a job manager and it's hard and annoying&lt;/li&gt;
  &lt;li&gt;Logging or monitoring means looking at text files with cryptic names&lt;/li&gt;
  &lt;li&gt;Automated testing on HPC is not a thing. Build on GitHub and pray.&lt;/li&gt;
  &lt;li&gt;When I can't easily navigate code, documentation, or it's completely missing&lt;/li&gt;
  &lt;li&gt;When I set up everything the way I like it and I have to login to a new system and do it all over again&lt;/li&gt;
  &lt;li&gt;When I want to develop something that uses a cluster resource but there are no exposed APIs.&lt;/li&gt;
  &lt;li&gt;When it's impossible to compare between systems because they are special snowflakes&lt;/li&gt;
  &lt;li&gt;When I can't easily test across the systems that my software is intended for.&lt;/li&gt;
  &lt;li&gt;To scale anything I have to use a job manager, wait hours, and then again if there is one mistake&lt;/li&gt;
  &lt;li&gt;When it takes 2 hours or more to get a node allocated&lt;/li&gt;
  &lt;li&gt;When I can't really make tools for HPC because I'm trying to find workarounds for all these problems&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;And I’d add a “thing that is annoying” to be this obsessive focus on power and scale, in a competitive sense, and this race
to be in the top 500 and beat the other guy over all else. The constant need to rebuild clusters means we never
focus on the details of how we use them. We do the same things over again. I’ve mentioned these things before, possibly many times, but I need to point it out again.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Our current developer environments are more like handcuffs than places we are enabled to thrive.&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;The reality for me is that I tend to put myself in a new role or environment, and then think of lots of cool ways to extend a particular tool, or build something before it. This is why I’ve made a ton of visualizations, associated tools, or posts for spack - it’s literally just the thing that is right in front of me. Put something else in front of me, such as an entire infrastructure with APIs, and I’d do the same. So why can’t a nice set of developer tools be available for the resources I’m supposed to be using?&lt;/p&gt;

&lt;h2 id=&quot;develop-based-on-specific-problems&quot;&gt;Develop based on specific problems&lt;/h2&gt;

&lt;p&gt;I think I want to develop more focusing on these problems. Don’t get me wrong - I’ll definitely keep making silly projects. But my vision for the future needs to be oriented toward these pains. These in particular are the problems that I think our community needs to look at, at least given this developer perspective.
I say this because I’ve seen and used the dark side - having free rein of beautiful cloud APIs to let me automate to my heart’s content! 
I only now, without being a part of some cloud or container cluster deployed project, am aware that I don’t have access to these development tools.
 What is my solution now? I largely don’t ssh into an HPC cluster until I absolutely have to - either to scale something, or reproduce a workflow on GitHub actions that works there (but then is really challenging to get it working on the cluster resource). Indeed &lt;a href=&quot;https://twitter.com/vsoch/status/1461908217223528448&quot; target=&quot;_blank&quot;&gt;this entire thread&lt;/a&gt; resulted after a frustrating evening of exactly that.&lt;/p&gt;

&lt;p&gt;What isn’t helpful? What isn’t helpful is telling me “This center / place / person has this thing that has solved this problem.” Can I easily access it, and what about the entire research software engineering community? This kind of response shuts down the conversation 
and makes the developer (myself for example) realize that the person I’m talking to is not interested in thinking about how to inspire change.
I’ve been really frustrated recently with mentioning even an abstract idea, and getting shut down that “Oh that sounds like this other tool.”
For a project to reach this “mention status” it needs to be easy to install or use, and not have a barrier of privilege that you have to work at a certain place or know people. Telling me that there is a solution that requires some convoluted steps and permissions not only implies that it is only available to those in privilege, but that the solution is not well adopted enough or shared enough to be truly a solution for our community.&lt;/p&gt;

&lt;h2 id=&quot;inspiring-vision&quot;&gt;Inspiring Vision&lt;/h2&gt;

&lt;p&gt;If we aren’t happy with the current state of the world, what are our options? Well, we could leave our current roles to find another state that is more similar to what we want. Personally speaking, I haven’t hit that point quite yet. I want to try my hardest to formulate a vision for how I want the world to be, and then find opportunity to work on it from where I am. The wisdom here is that no specific role is perfect, and optimally we should place ourself somewhere where there are resources and open mindedness for change. it’s up to us to extend our influence as best we can to help drive some possible future. If you try that and fail? At least you tried.&lt;/p&gt;

&lt;p&gt;These are the things that are hard. I am going to try harder to have them be the focus of my thinking about the future. I want to make them easier. I’m starting to realize that possibly the reality is that I should think beyond the constraints of HPC, and more toward the kind of infrastructure that I want, and then
figure out how to slowly integrate it as a part of our culture too. We can start with a core vision for a future that we want, and then
slowly build up tooling and community around that.&lt;/p&gt;

&lt;p&gt;Happy Friday, friends!&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Researcher's Time Has Value, Too</title>
   <link href="https://hpc.social/2021/researcher-s-time-has-value-too/"/>
   <updated>2021-11-23T00:00:00-07:00</updated>
   <id>https://hpc.social/2021/researcher-s-time-has-value-too</id>
   <content type="html">&lt;h2 id=&quot;and-researchers-value-their-time&quot;&gt;..And Researchers Value Their Time&lt;/h2&gt;

&lt;p&gt;(Note: This post is adapted from &lt;a href=&quot;https://www.researchcomputingteams.org/newsletter_issues/0102&quot;&gt;#102&lt;/a&gt; of the &lt;a href=&quot;https://www.researchcomputingteams.org&quot;&gt;Research Computing Teams Newsletter&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;If you followed HPC twitter in late 2021 at all, you will have seen a &lt;a href=&quot;https://twitter.com/vsoch/status/1461908217223528448&quot;&gt;heartfelt thread&lt;/a&gt; by a well-known research software developer, one who was a key contributor to the Singularity project among others, lamenting the frankly appalling state of developer productivity in HPC - both in what tools exist, and support for them (and other tools for developers) at academic centres.  A &lt;strong&gt;lot&lt;/strong&gt; of people &lt;a href=&quot;https://twitter.com/HPC_Guru/status/1462070286983983108&quot;&gt;chimed into the discussion&lt;/a&gt;, including &lt;a href=&quot;https://twitter.com/five9a2/status/1462137427527675918&quot;&gt;one of the leading developers of the PetSC project&lt;/a&gt;, embedded software developers, some key people at big computing centres, all agreeing that there was a problem, but typically zooming in on one or another particular technical or procedural issue and not coming to any conclusion.&lt;/p&gt;

&lt;p&gt;I think the issue is a lot bigger than HPC software development workflows - it comes up in too many contexts to be about specific technical issues of running CI/CD pipelines on fixed infrastructure.  The only people to identify the correct underlying issue, in my opinion, were people with experience of both academia and the private sector, such as Brendan Bouffler at AWS:&lt;/p&gt;

&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p dir=&quot;ltr&quot; lang=&quot;en&quot;&gt;Too much reliance on “free” labour - postgrads and post docs who, invariably, decide that burning their time being mechanical turks for their “superiors” just sucks, so they come and work for us. And since we pay $$, we’re not gonna waste them on things that software can do.&lt;/p&gt;
&amp;mdash; Brendan Bouffler☁️ 🏳️‍🌈 (@boofla) &lt;a href=&quot;https://twitter.com/boofla/status/1462099372255203346?ref_src=twsrc%5Etfw&quot;&gt;November 20, 2021&lt;/a&gt;&lt;/blockquote&gt;

&lt;p&gt;The same argument got made by R&amp;amp;D research staff in the private sector.  Their time actually has value; as a result, it gets valued.&lt;/p&gt;

&lt;p&gt;In academic research computing, partly because of low salaries — especially for the endless stream of trainees — but also because we typically provide research computing systems for free, we tend to put zero value on people’s time.  Thus our “lowest-cost” approach definitely does not apply to researcher or trainee effort. If researchers have to jump through absurd hoops to get or renew their accounts, or have to distort their workflows to fit one-size-fits-all clusters and queueing systems, or postdocs have to spend hours of work by hand every month hand because tools to automate some of that work would cost $500, well, what do they expect, right?&lt;/p&gt;

&lt;p&gt;It’s not that this is an indefensible position to take, but one can’t take this position &lt;em&gt;and&lt;/em&gt; act surprised when researchers who can afford to are seriously investigating taking their projects into the commercial cloud even though it costs 2x as much.  It turns out that people’s time is worth quite a lot to them, and is certainly worth some money.  If we were to &lt;a href=&quot;https://www.dursi.ca/post/research-computing-funding-to-researchers&quot;&gt;let researchers spend their research computing and data money wherever they pleased&lt;/a&gt;, I think we’d find that significantly less than 100% of researchers would use “lowest price possible” as their sole criterion for choosing providers.  Core facilities like animal facilities, sequencing centres, and microscopy centres compete on dimensions other than being the cheapest option available.&lt;/p&gt;

&lt;p&gt;To be sure, there are process issues in academia which exacerbates the tendency to see people’s time as valueless - rules about capital vs operating costs, for instance - but those rules aren’t a law of nature.  If we were paying people in academia &lt;a href=&quot;https://www.levels.fyi/&quot;&gt;what they pay in tech&lt;/a&gt;, administration would suddenly discover some additional flexibility in the thresholds and criteria for considering something a capital expense if it meant we could be a bit more parsimonious with people’s time.&lt;/p&gt;

&lt;p&gt;Until then, one can’t be too surprised when the most talented and ambitious staff get routinely poached by the private sector, and when research groups start considering service providers that cost more but respect their time.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Ceph Crimson 2021 Q3 Project Update</title>
   <link href="https://hpc.social/2021/ceph-crimson-2021-q3-project-update/"/>
   <updated>2021-11-22T00:00:00-07:00</updated>
   <id>https://hpc.social/2021/ceph-crimson-2021-q3-project-update</id>
   <content type="html">&lt;p&gt;This is the first time we’re seeing Bluestore in Crimson beating Bluestore in Classic in some (low core count) tests.  Starting to see lower tail latency as well which is a really good sign.  Top end performance will be contingent on multi-reactor support though.  Slides available &lt;a href=&quot;https://docs.google.com/presentation/d/1eydyAFKRea8n-VniQzXKW8qkKM9GLVMJt2uDjipJjQA/edit?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>IOPS are dumb</title>
   <link href="https://hpc.social/2021/iops-are-dumb/"/>
   <updated>2021-10-24T17:56:00-06:00</updated>
   <id>https://hpc.social/2021/iops-are-dumb</id>
   <content type="html">&lt;div style=&quot;border: 1px solid black; font-size: x-small; margin-left: 2em; margin-right: 2em; padding: 1em;&quot;&gt;This post is a long-form dump of some thoughts I've had while testing all-flash file systems this past year, and bits of this appear in a &lt;a href=&quot;http://www.pdsw.org/index.shtml&quot;&gt;presentation and paper I'm presenting at PDSW'21&lt;/a&gt;&amp;nbsp;about new benchmarking techniques for testing all-flash file systems.&lt;/div&gt;
&lt;p&gt;&quot;How many IOPS do you need?&quot;&lt;/p&gt;
&lt;p&gt;I'm often asked this by storage vendors, and the question drives me a little bonkers.&amp;nbsp; I assume they ask it because their other customers bring them black-and-white IOPS requirements, but I argue that anyone would be hard-pressed to explain the scientific value of one I/O operation (versus one gigabyte) if ever called on it.&amp;nbsp; And yet, IOPS are undeniably important; the illustrious Rob Ross devoted a whole slide dedicated to this at a &lt;a href=&quot;https://science.osti.gov/ascr/ascac/Meetings/202109&quot;&gt;recent ASCAC meeting&lt;/a&gt;:&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-uoZq9awp-3E/YVS3anWGgpI/AAAAAAABWsw/tb12XvWtTScjd42nIscFJ-6U7Dr3E_TLQCLcBGAsYHQ/s2048/rob-ross-ascac-slide.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img alt=&quot;Rob Ross' perspective on why IOPS are now important for HPC I/O&quot; border=&quot;0&quot; height=&quot;226&quot; src=&quot;https://1.bp.blogspot.com/-uoZq9awp-3E/YVS3anWGgpI/AAAAAAABWsw/tb12XvWtTScjd42nIscFJ-6U7Dr3E_TLQCLcBGAsYHQ/w400-h226/rob-ross-ascac-slide.png&quot; title=&quot;Rob Ross' perspective on why IOPS are now important for HPC I/O&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;Rob Ross' perspective on why IOPS are now important for HPC I/O&lt;/span&gt;&lt;/b&gt;&lt;/div&gt;
&lt;p&gt;I agree with all of Rob's bullets and yet I disagree with the title of his slide; IOPS are dumb, and yet ignoring them when designing a performance-optimized parallel file system is even more dumb in contemporary times.&amp;nbsp; So let's talk about the grey area in between that creates this dichotomy.&lt;span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;h2 style=&quot;text-align: left;&quot;&gt;First, bandwidth is pretty dumb&lt;/h2&gt;
&lt;p&gt;If there's one constant in HPC, it's that everyone hates I/O.&amp;nbsp; And there's a good reason: it's a waste of time because every second you wait for I/O to complete is a second you aren't doing the math that led you to use a supercomputer in the first place.&amp;nbsp; I/O is the time you are doing zero computing amidst a field called &quot;high performance computing.&quot;&lt;/p&gt;
&lt;p&gt;That said, everyone appreciates the product of I/O--data.&amp;nbsp; I/O is a necessary part of preserving the results of your calculation, so nobody ever says they wish there was no I/O.&amp;nbsp; Instead, infinitely fast I/O is what people want since it implies that 100% of a scientist's time using an HPC is spent actually performing computations while still&amp;nbsp;preserving the results of that computation after the job has completed.&lt;/p&gt;
&lt;p&gt;Peeling back another layer of that onion, the saved results of that computation--data--has intrinsic value.&amp;nbsp; In a typical simulation or data analysis, every byte of input or output is typically the hard-earned product of a lot of work performed by a person or machine, and it follows that if you want to both save a lot of bytes but want to spend as little time as possible performing I/O, the true value of a parallel storage system's performance is in how many bytes per second it can read or write.&amp;nbsp; At a fundamental level, this is why I/O performance has long been gauged in terms of megabytes per second, gigabytes per second, and now terabytes per second.&amp;nbsp; To the casual observer, a file system that can deliver 100 GB/s is more valuable than a file system that can deliver only 50 GB/s assuming all things are equal for this very reason.&amp;nbsp; Easy.&lt;/p&gt;
&lt;p&gt;This singular metric of storage system &quot;goodness&quot; quickly breaks down once you start trying to set expectations around it though.&amp;nbsp; For example, let's say your HPC job generates 21 TB of valuable data that must be stored, and it must be stored so frequently that we really can't tolerate more than 30 seconds writing that data out before we start feeling like &quot;too much time&quot; is being spent on I/O instead of computation.&amp;nbsp; This turns out to be 700 GB/s--a rather arbitrary choice since that 30 seconds is a matter of subjectivity, but one that reflects the value of your 21 TB and the value of your time.&amp;nbsp; It should follow that any &lt;a href=&quot;https://www.nersc.gov/news-publications/nersc-news/nersc-center-news/2016/cori-supercomputer-now-fully-installed-at-berkeley-lab/&quot;&gt;file system that claims 700 GB/s of write capability&lt;/a&gt; should meet your requirements, and any vendor who can deliver such a system should get your business, right?&lt;/p&gt;
&lt;p&gt;Of course not.&amp;nbsp; It's no secret that obtaining those hero bandwidths, much like obtaining Linpack-level FLOPS, requires you (the end-user) to perform I/O in exactly the right way.&amp;nbsp; In the case of the aforementioned 700 GB/s file system, this means&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ol style=&quot;text-align: left;&quot;&gt;&lt;li&gt;Having each MPI process write to its own file (a single shared file will get slowed down by file system lock traffic)&lt;/li&gt;&lt;li&gt;Writing 4 MiB at a time (to exactly match the size of the network transmission buffers, remote memory buffers, RAID alignment, ...)&lt;/li&gt;&lt;li&gt;Using 4 processes per node (enough parallelism to drive the NIC, but not too much to choke the node)&lt;/li&gt;&lt;li&gt;Using 960 nodes (enough parallelism to drive all the file system drives, but not too much to choke the servers)&lt;/li&gt;&lt;/ol&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;I've never seen a scientific application perform this exact pattern, and consequentially, I don't expect that any scientific application has ever gotten that 700 GB/s of performance from a &quot;700 GB/s file system&quot; in practice.&amp;nbsp; In that sense, this 700 GB/s bandwidth metric is pretty dumb since nobody actually achieves its rated performance. Of course, that hasn't prevented me from saying&amp;nbsp;these &lt;a href=&quot;https://storageconference.us/2019/Invited/Lockwood.slides.pdf&quot;&gt;same&lt;/a&gt; &lt;a href=&quot;https://www.osti.gov/biblio/1798757&quot;&gt;dumb&lt;/a&gt; &lt;a href=&quot;https://hps.vi4io.org/_media/events/2021/iodc21-lockwood.pdf&quot;&gt;things&lt;/a&gt;&amp;nbsp;when I stump for file systems. &amp;nbsp;The one saving grace of using bandwidth as a meaningful metric of I/O performance, though, is that&amp;nbsp;&lt;b&gt;I/O patterns are a synthetic construct&lt;/b&gt;&amp;nbsp;and can be squished, stretched, and reshaped without affecting the underlying scientific data being transmitted.&lt;/p&gt;
&lt;p&gt;The value of data is in its contents, not the way it is arranged or accessed.&amp;nbsp; There's no intrinsic scientific reason why someone should or shouldn't read their data 4 MiB at a time as long as the bits eventually get to the CPU that will perform calculations on it in the correct order.&amp;nbsp; The only reason HPC users perform nice, 1 MiB-aligned reads and writes is because they learn (either in training or on the streets) that randomly reading a few thousand bytes at a time is very slow and works against their own interests of minimizing I/O time.&amp;nbsp; &amp;nbsp;This contrasts sharply with the computing side of HPC where the laws of physics generally dictate the equations that must be computed, and the order in which those computations happen dictates whether the final results accurately model some physical process or just spit out a bunch of unphysical garbage results.&lt;/p&gt;
&lt;p&gt;Because I/O patterns are not intrinsically valuable, we are free to rearrange them to best suit the strengths and weaknesses of a storage system to maximize the GB/s we can get out of it.&amp;nbsp; This is the entire foundation of MPI-IO, which receives I/O patterns that are convenient for the physics being simulated and reorders them into patterns that are convenient for the storage system.&amp;nbsp; So while saying a file system can deliver 700 GB/s is a bit disingenuous on an absolute scale, it does indicate what is possible if you are willing to twist your I/O pattern to exactly match the design optimum.&lt;/p&gt;
&lt;h2 style=&quot;text-align: left;&quot;&gt;But IOPS are particularly dumb&lt;/h2&gt;
&lt;p&gt;IOPS are what happen when you take the value out of a value-based performance metric like bandwidth.&amp;nbsp; Rather than expressing how many valuable bytes a file system can move per second, IOPS express how many arbitrary I/O operations a file system can service per second.&amp;nbsp; And since the notion of an &quot;I/O operation&quot; is completely synthetic and can be twisted without compromising the value of the underlying data, you might already see why IOPS are a dumb metric of performance.&amp;nbsp; They measure how quickly a file system can do something meaningless, where that meaningless thing (an I/O operation) is itself a function of the file system.&amp;nbsp; It's like saying you can run a marathon at five steps per second--it doesn't actually indicate how long it will take you to cover the twenty six miles.&lt;/p&gt;
&lt;p&gt;IOPS as a performance measure was relatively unknown to HPC for most of history.&amp;nbsp; &lt;a href=&quot;https://www.sdsc.edu/News%20Items/PR030512_gordon.html&quot;&gt;Until 2012&lt;/a&gt;, HPC storage was dominated by hard drives which which only delivered high-value performance for large, sequential reads and writes and the notion of an &quot;IOP&quot; was antithetical to performance.&amp;nbsp; The advent of flash introduced a new dimension of performance in its ability to read and write a lot of data at discontiguous (or even random) positions within files or across entire file systems.&amp;nbsp; Make no mistake: you still read and write more bytes per second (i.e., get more value) from flash with a contiguous I/O pattern.&amp;nbsp; Flash just raised the bottom end of performance in the event that you are unable or unwilling to contort your application to perform I/O in a way that is convenient for your storage media.&lt;/p&gt;
&lt;p&gt;To that end, when a vendor advertises how many IOPS they can deliver, they really are advertising how many discontiguous 4 KiB reads or writes they can deliver under the worst-case I/O pattern (fully random offsets).&amp;nbsp; You can convert a vendor's IOPS performance back into a meaningful value metric simply by multiplying it by 4 KiB; for example, I've been presenting a slide that claims I measured &lt;a href=&quot;https://www.osti.gov/biblio/1798757&quot;&gt;29,000 write IOPS and 1,400 read IOPS from a single ClusterStor E1000 OST array&lt;/a&gt;:&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-Aq07XkQ1A1U/YVU95I3cvwI/AAAAAAABWtA/2Z57P80DSeoWxeS2dRP42SQUlxaAjas0gCLcBGAsYHQ/s2048/Screen%2BShot%2B2021-09-29%2Bat%2B21.32.04.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img alt=&quot;Performance measurements of a single ClusterStor E1000 NVMe Lustre OST&quot; border=&quot;0&quot; height=&quot;206&quot; src=&quot;https://1.bp.blogspot.com/-Aq07XkQ1A1U/YVU95I3cvwI/AAAAAAABWtA/2Z57P80DSeoWxeS2dRP42SQUlxaAjas0gCLcBGAsYHQ/w400-h206/Screen%2BShot%2B2021-09-29%2Bat%2B21.32.04.png&quot; title=&quot;Performance measurements of a single ClusterStor E1000 NVMe Lustre OST&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;Performance measurements of a single ClusterStor E1000 NVMe Lustre OST&lt;/span&gt;&lt;/b&gt;&lt;/div&gt;
&lt;p&gt;In reality, I was able to write data at 0.12 GB/s and read data at 5.7 GB/s, and stating these performance metrics as IOPS makes it clear that these data rates reflect the worst-case scenario of tiny I/Os happening at random locations rather than the best-case scenario of sequential I/Os which can happen at 27 GB/s and 41 GB/s, respectively.&lt;/p&gt;
&lt;p&gt;Where IOPS get particularly stupid is when we try to cast them as some sort of hero number analogous to the 700 GB/s bandwidth metric discussed above.&amp;nbsp; Because IOPS reflect a worst-case performance scenario, no user should ever be asking &quot;how can I get the highest IOPS&quot; because they'd really be asking &quot;how can I get the best, worst-case performance?&quot;&amp;nbsp; Relatedly, trying to measure the &lt;i&gt;IOPS capability&lt;/i&gt; of a storage system gets very convoluted because it often requires twisting your I/O pattern in such unrealistic ways that heroic effort is required to get such terrible performance.&amp;nbsp; At some point, every I/O performance engineer should find themselves questioning why they are putting so much time into defeating every optimization the file system implements to avoid this worst-case scenario.&lt;/p&gt;
&lt;p&gt;To make this a little more concrete, let's look at this &lt;a href=&quot;https://www.lustre.org/wp-content/uploads/SC19LustreBoF_All.pdf&quot;&gt;slide I made in 2019 to discuss the IOPS projections of this exact same ClusterStor E1000 array&lt;/a&gt;:&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-xpPJ4SVoNcQ/YVVGQ4qV4WI/AAAAAAABWtI/Vpl-loGSSsomakJR69dc3xReU-0D_2AzgCLcBGAsYHQ/s2048/Screen%2BShot%2B2021-09-29%2Bat%2B22.01.19.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img alt=&quot;Projected performance of a ClusterStor E1000 NVMe Lustre OST based on a PCIe Gen3 platform&quot; border=&quot;0&quot; height=&quot;226&quot; src=&quot;https://1.bp.blogspot.com/-xpPJ4SVoNcQ/YVVGQ4qV4WI/AAAAAAABWtI/Vpl-loGSSsomakJR69dc3xReU-0D_2AzgCLcBGAsYHQ/w400-h226/Screen%2BShot%2B2021-09-29%2Bat%2B22.01.19.png&quot; title=&quot;Projected performance of a ClusterStor E1000 NVMe Lustre OST based on a PCIe Gen3 platform&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;Projected performance of a ClusterStor E1000 NVMe Lustre OST based on a PCIe Gen3 platform&lt;/span&gt;&lt;/b&gt;&lt;/div&gt;
&lt;p&gt;Somehow the random read rate went from a projected 600,000 to an astonishing 1,400,000 read IOPS--which one is the correct measure of read IOPS?&lt;/p&gt;
&lt;p&gt;It turns out that they're &lt;i&gt;both&lt;/i&gt; correct; the huge difference in measured read IOPS are the result of the the 600 KIOPS estimate coming from a measurement that&lt;/p&gt;
&lt;ol style=&quot;text-align: left;&quot;&gt;&lt;li&gt;ran for a much longer sustained period (180 seconds vs. 69 seconds)&lt;/li&gt;&lt;li&gt;used fewer client nodes (21 nodes vs. of 32 nodes)&lt;/li&gt;&lt;li&gt;wrote larger files (1,008× 8 GiB files vs. 1,024×&amp;nbsp;384 GiB files)&lt;/li&gt;&lt;/ol&gt;
&lt;p&gt;Unlike the IOPS measurements on individual SSDs which are measured using a standard tool (&lt;a href=&quot;https://github.com/axboe/fio&quot;&gt;fio&lt;/a&gt; with &lt;a href=&quot;https://pagure.io/libaio&quot;&gt;libaio&lt;/a&gt; from a single node), there is no standard method for measuring the IOPS of a parallel file system.&amp;nbsp; And just as the hero bandwidth number we discussed above is unattainable by real applications, any standardized IOPS test for a parallel file system would result in a relatively meaningless number.&amp;nbsp; And yes, this includes IO-500; &lt;a href=&quot;https://www.glennklockwood.com/benchmarks/io500.html#interpreting-results&quot;&gt;its numbers have little quantitative value&lt;/a&gt; if you want to design a parallel file system the right way.&lt;/p&gt;
&lt;p&gt;So who's to say whether a ClusterStor E1000 OST is capable of 600 kIOPS or 1,400 kIOPS?&amp;nbsp; I argue that 1,400 kIOPS is more accurate since I/O is bursty and a three-minute-long burst of completely random reads is less likely than a one-minute long one on a production system.&amp;nbsp; If I worked for a vendor though, I'm sure this would be taken to be a dishonest marketing number since it doesn't reflect an indefinitely sustainable level of performance.&amp;nbsp; And perhaps courageously, the &lt;a href=&quot;https://www.hpe.com/psnow/doc/PSN1012842049INEN.pdf&quot;&gt;official Cray ClusterStor E1000 data sheet&lt;/a&gt; doesn't even wade into these waters and avoids quoting any kind of IOPS performance expectation.&amp;nbsp; Ultimately, the true value of the random read capability is the bandwidth achievable by all of the most random workloads that will realistically be run at the same time on a file system.&amp;nbsp; Good luck figuring that out.&lt;/p&gt;
&lt;h2 style=&quot;text-align: left;&quot;&gt;Write IOPS are &lt;i&gt;really&lt;/i&gt; dumb&lt;/h2&gt;
&lt;p&gt;As I said at the outset, I cannot disagree with any of the bullets in the slide Rob presented at ASCAC.&amp;nbsp; That first one is particularly salient--there &lt;i&gt;are&lt;/i&gt; a new class of HPC workloads, particularly in AI, whose primary purpose is to randomly sample large datasets to train statistical models.&amp;nbsp; If these datasets are too large to fit into memory, you cannot avoid some degree of random read I/O without introducing biases into your weights.&amp;nbsp; For this reason, there is legitimate need for HPC to demand high random read performance from their file systems.&amp;nbsp; Casting this requirement in terms of 4 KiB random read rates to have a neat answer to the &quot;how many IOPS do you need&quot; question is dubious, but whatever.&amp;nbsp; There's little room for intellectual purity in HPC.&lt;/p&gt;
&lt;p&gt;The same can't be said for random write rates.&amp;nbsp; Write IOPS are a completely worthless and misleading performance metric in parallel file systems.&lt;/p&gt;
&lt;p&gt;In most cases, HPC applications approximate some aspect of the physical world, and mathematics and physics were created to describe this physical world in a structured way.&amp;nbsp; Whether you're computing over atoms, meshes, or matrices, there is structure to the data you are writing out and the way your application traverses memory to write everything out.&amp;nbsp; You may not write data out in a perfectly ordered way; you may have more atoms on one MPI process than another, or you may be traversing an imbalanced graph.&amp;nbsp; But there is almost always enough structure to scientific data to squish it into a non-random I/O pattern using middleware like MPI-IO.&lt;/p&gt;
&lt;p&gt;Granted, there are a few workloads where this is not true.&amp;nbsp; &lt;a href=&quot;https://www.sdsc.edu/Events/ipp_webinars/large_scale_genomics.pdf&quot;&gt;Out-of-core sorting of short-read DNA sequences&lt;/a&gt;&amp;nbsp;and &lt;a href=&quot;http://dx.doi.org/10.1016/j.future.2017.12.022&quot;&gt;in-place updates of telescope mosaics&lt;/a&gt; are two workloads that come to mind where you don't know where to write a small bit of data until you've computed on that small bit of data.&amp;nbsp; In both these cases though, the files are never read and written at the same time, meaning that these random-ish writes can be cached in memory, reordered to be less random, and written out to the file asynchronously.&amp;nbsp; And the effect of write-back caching on random write workloads is staggering.&lt;/p&gt;
&lt;p&gt;To illustrate this, consider three different ways in which IOR can be run against an all-NVMe file system to measure random 4 KiB writes:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul style=&quot;text-align: left;&quot;&gt;&lt;li&gt;In the &lt;b&gt;naïve&lt;/b&gt; case, we just write 4 KiB pages at random locations within a bunch of files (one file per MPI process) and report what IOR tells us the write IOPS were at the end.&amp;nbsp; This includes only the time spent in write(2) calls.&lt;/li&gt;&lt;li&gt;In the case where we &lt;b&gt;include fsync&lt;/b&gt;, we call fsync(2) at the end of all the writes and include the time it takes to return along with all the time spent in write(2).&lt;/li&gt;&lt;li&gt;In the &lt;b&gt;O_DIRECT&lt;/b&gt; case, we open the file with direct I/O to completely bypass the client write-back cache and ensure that write(2) doesn't return until the data has been written to the file system servers.&lt;/li&gt;&lt;/ul&gt;
&lt;div&gt;These seemingly minor changes result in write IOPS rates that differ by over 30x:&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-zShKdPu53YE/YVVW2QbVRYI/AAAAAAABWtQ/mReqH6S2lsgF0nhAmqDdlCra7-FQoywWACLcBGAsYHQ/s565/download.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img alt=&quot;Random write IOPS measured using IOR on an all-NVMe parallel file system&quot; border=&quot;0&quot; height=&quot;280&quot; src=&quot;https://1.bp.blogspot.com/-zShKdPu53YE/YVVW2QbVRYI/AAAAAAABWtQ/mReqH6S2lsgF0nhAmqDdlCra7-FQoywWACLcBGAsYHQ/w400-h280/download.png&quot; title=&quot;Random write IOPS measured using IOR on an all-NVMe parallel file system&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;Random write IOPS measured using IOR on an all-NVMe parallel file system&lt;/span&gt;&lt;/b&gt;&lt;/div&gt;
&lt;p&gt;Again we ask: which one is the right value for the file system's write IOPS performance?&lt;/p&gt;
&lt;p&gt;If we split apart the time spent in each phase of this I/O performance test, we immediately see that the naïve case is wildly deceptive:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-7r9NLXU8Cd8/YVVW9NcK52I/AAAAAAABWtU/hRmBYygTtDUkX1Q6an3iYdbMu68Ni4TMgCLcBGAsYHQ/s565/download-1.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img alt=&quot;Breakdown of time spent in I/O calls for 4K random write IOR workload&quot; border=&quot;0&quot; height=&quot;274&quot; src=&quot;https://1.bp.blogspot.com/-7r9NLXU8Cd8/YVVW9NcK52I/AAAAAAABWtU/hRmBYygTtDUkX1Q6an3iYdbMu68Ni4TMgCLcBGAsYHQ/w400-h274/download-1.png&quot; title=&quot;Breakdown of time spent in I/O calls for 4K random write IOR workload&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;Breakdown of time spent in I/O calls for 4K random write IOR workload&lt;/span&gt;&lt;/b&gt;&lt;/div&gt;
&lt;p&gt;The reason IOR reported a 2.6 million write IOPS rate is because all those random writes actually got cached in each compute node's memory, and I/O didn't actually happen until the file was closed and all cached dirty pages were flushed.&amp;nbsp; At the point this happens, the cache flushing process doesn't result in random writes anymore; the client reordered all of those cached writes into large, 1 MiB network requests and converted our random write workload into a sequential write workload.&lt;/p&gt;
&lt;p&gt;The same thing happens in the case where we include fsync; the only difference is that we're including the time required to flush caches in the denominator of our IOPS measurement.&amp;nbsp; Rather frustratingly, we actually stopped issuing write(2) calls after 45 seconds, but so many writes were cached in memory during those 45 seconds that it took almost 15 minutes to reorder and write them all out during that final fsync and file close.&amp;nbsp; What should've been 45 seconds of random writes to the file system turned into 45 seconds of random writes to memory and 850 seconds of sequential writes to the file system.&lt;/p&gt;
&lt;p&gt;The O_DIRECT case is the most straightforward since we don't cache any writes, and every one of our random writes from the application turns into a random write out to the file system.&amp;nbsp; This cuts our measured IOPS almost in half, but otherwise leaves no surprises when we expect to only write for 45 seconds. &amp;nbsp;Of course, we wrote far fewer bytes overall in this case since the effective bytes/sec during this 45 seconds was so low.&lt;/p&gt;
&lt;p&gt;Based on all this, it's tempting to say that the O_DIRECT case is the correct way to measure random write IOPS since it avoids write-back caches--but is it really?&amp;nbsp; In the rare case where an application intentionally does random writes (e.g., out-of-core sort or in-place updates), what are the odds that two MPI processes on different nodes will try to write to the same part of the same file at the same time and therefore trigger cache flushing?&amp;nbsp; Perhaps more directly, what are the odds that a scientific application would be using O_DIRECT &lt;i&gt;and&lt;/i&gt; random writes at the same time?&amp;nbsp; Only the most masochistic HPC user would ever purposely do something like this since it results in worst-case I/O performance; it doesn't take long for a user to realize this I/O pattern is terrible and reformulating their I/O pattern would increase their productive use of their supercomputer.&lt;/p&gt;
&lt;p&gt;So if no user in their right mind does truly unbuffered random writes, what's the point in measuring it in the first place?&amp;nbsp; &lt;b&gt;There is none.&amp;nbsp; Measuring write IOPS is dumb&lt;/b&gt;.&amp;nbsp; Using O_DIRECT to measure random write performance is dumb, and measuring write IOPS through write-back cache, while representative of most users' actual workloads, isn't actually doing 4K random I/Os and therefore isn't even measuring IOPS.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;h2&gt;Not all IOPS are always dumb&lt;/h2&gt;
&lt;div&gt;This all being said, measuring IOPS can be valuable in contexts outside of parallel file systems.&amp;nbsp; Two cases come to mind where measuring IOPS can be a rational yard stick.&lt;/div&gt;
&lt;h3 style=&quot;text-align: left;&quot;&gt;1. Serving up LUNs to containers and VMs&lt;/h3&gt;
&lt;div&gt;By definition, infrastructure providers shouldn't be responsible for the applications that run inside black-box containers and VMs because they are providing storage infrastructure (block devices) and not storage services (file systems).&amp;nbsp; Blocks in and blocks out are measured in IOPS, so the fit is natural.&amp;nbsp; That said, HPC users care about file systems (that is, scientific applications do not perform I/O using SCSI commands directly!), so worrying about LUN performance isn't meaningful in the HPC context.&lt;/div&gt;
&lt;h3 style=&quot;text-align: left;&quot;&gt;2. Measuring the effect of many users doing many things&lt;/h3&gt;
&lt;div&gt;While individual HPC workloads rarely perform random I/Os on purpose, if you have enough users doing many small tasks all at once, the file system itself sees a workload that approaches something random.&amp;nbsp; The more, small, independent tasks running parallel and the farther back you stand from the overall I/O load timeline, the more random it looks.&amp;nbsp; So, I argue that it is fair to measure the IOPS of a parallel file system for the purposes of measuring how much abuse a file system can take before it begins to impact everybody.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;Take, for example, these IOPS scaling I measured on a small all-flash file system using IOR:&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-TVonp3v_RWE/YW9bGX7mCrI/AAAAAAABWwQ/IWCsgpJvZYEiOAtzfntxWgnf8ZZaZyLzwCLcBGAsYHQ/s584/Unknown-1.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img alt=&quot;Scale-up IOPS benchmarking to demonstrate the saturation point of an all-flash file system&quot; border=&quot;0&quot; height=&quot;289&quot; src=&quot;https://1.bp.blogspot.com/-TVonp3v_RWE/YW9bGX7mCrI/AAAAAAABWwQ/IWCsgpJvZYEiOAtzfntxWgnf8ZZaZyLzwCLcBGAsYHQ/w400-h289/Unknown-1.png&quot; title=&quot;Scale-up IOPS benchmarking to demonstrate the saturation point of an all-flash file system&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;&lt;b&gt;Scale-up IOPS benchmarking to demonstrate the saturation point of an all-flash file system&lt;/b&gt;&lt;/span&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&amp;lt;div&amp;gt;It looks like it takes about 4,096 concurrent random readers or writers to max out the file system.  This alone isn’t meaningful until you consider what this means in the context of the whole compute and storage platform.&amp;lt;/div&amp;gt;&lt;/p&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;What fraction of the cluster's compute nodes corresponds to 4096 cores?&amp;nbsp; If you've got, say, &lt;a href=&quot;https://www.sdsc.edu/support/user_guides/expanse.html#tech_summary&quot;&gt;728 dual-socket 64-core AMD Epyc processors&lt;/a&gt;, it would only take 32 compute nodes to max out this file system.&amp;nbsp; And if another user wanted to use any of the remaining 696 compute nodes to, say, run a Python script that needed to read in random packages scattered across the file system, there would be no remaining IOPS capacity left at this point, and everyone would experience perceptible lag.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;Of course, this is the most extreme case--purely random IOPS--but you can measure the IOPS that a real workload does generate on the server side when, say, sampling a deep learning training dataset. With this, you can then figure out how much headroom that application leaves for every other random-ish workload that needs to run on the same system.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;Once you realize that a lot of the unglamorous parts of of scientific computing--reading dotfiles when you log in, loading shared objects when you launch a dynamically linked executable, or even just editing source code--are full of random-like reads, you can establish a quantitative basis for figuring out how badly an IOPS-intensive data analysis application may affect everyone else's interactive accesses on the same file system.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;This is not to say that we can easily answer the question of &quot;How many IOPS do you need?&quot; though.&amp;nbsp; How many IOPS a workload can drive is not how many IOPS that workload &lt;i&gt;needs&lt;/i&gt;--it's really how fast it can compute before it has run out of data to process and needs to read more in.&amp;nbsp; The faster your compute nodes, generally, the more data they can &lt;i&gt;consume&lt;/i&gt;.&amp;nbsp; They still &lt;i&gt;want&lt;/i&gt; all the IOPS you can give them so they can spend as much time computing (and not waiting for I/O) as possible, and how many IOPS your application can drive is a function of how quickly it runs given the full stack between it and the storage, including CPU, memory, and networking.&lt;/div&gt;
&lt;h2 style=&quot;text-align: left;&quot;&gt;If everything is dumb, now what?&lt;/h2&gt;
&lt;div&gt;Give up trying to reduce I/O performance down to a single IOPS number, because it's two degrees away from being useful.&amp;nbsp; Bandwidth is a better metric in that it's only one degree away from what actually matters, but at the end of the day, the real metric of I/O performance is how much time an application has to wait on I/O before it can resume performing meaningful computations.&amp;nbsp; Granted, most storage vendors will give you a blank stare if you take this angle to them; telling them that your application spends 50% of its time waiting on I/O isn't going to get you a better file system from a storage company alone, so think about what the real problem could be.&lt;/div&gt;
&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div style=&quot;text-align: left;&quot;&gt;&lt;b&gt;Is the application doing I/O in a pattern (random or otherwise) that prevents the storage system from delivering as many bytes/second as possible?&lt;/b&gt;&amp;nbsp; If so, ask your vendor for a storage system that delivers more bandwidth to a wider range of I/O patterns than just perfectly aligned 1 MiB reads and writes.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div style=&quot;text-align: left;&quot;&gt;&lt;b&gt;Is the storage system already running as well as it can, but it only takes a few compute nodes to max it out?&amp;nbsp;&lt;/b&gt; If so, your storage system is too small relative to your compute system, and you should ask your vendor for more servers and drives to scale out.&lt;/div&gt;
&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;b&gt;Is the storage system running at 100% CPU even though it's not delivering full bandwidth?&amp;nbsp;&lt;/b&gt; Servicing a small I/O requires a lot more CPU than a large I/O since there are fixed computations that have to happen on every read or write regardless of how big it is.&amp;nbsp; Ask your vendor for a better file system that doesn't eat up so much CPU, or ask for more capable servers.&lt;br /&gt;&lt;/div&gt;
&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div style=&quot;text-align: left;&quot;&gt;Alternatively, if you have a lot of users all doing different things and the file system is giving poor performance to everyone, ask your vendor for a file system with better quality of service.&amp;nbsp; This will ensure that one big job doesn't starve out all the small ones.&lt;/div&gt;
&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div style=&quot;text-align: left;&quot;&gt;&lt;b&gt;Is the storage system slow but you don't have the time to figure out why?&amp;nbsp;&lt;/b&gt; If so, it sounds like you work for an organization that doesn't actually value data because it's not appropriately staffed.&amp;nbsp; This isn't a storage problem!&lt;/div&gt;
&lt;div style=&quot;text-align: left;&quot;&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div style=&quot;text-align: left;&quot;&gt;Ultimately, if solving I/O problems was as easy answering how many IOPS you need, storage wouldn't be the perpetual pain point in HPC that it has been.&amp;nbsp; As with all things in computing, there is no shortcut and the proper way to approach this is by rolling up your sleeves and start ruling out problems.&amp;nbsp; You can (and should!) ask for a lot from your storage vendors--flexibility in delivering bandwidth, CPU-efficient file systems, and quality of service controls are all valid requests when buying storage.&amp;nbsp; But IOPS are not.&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Uptodate</title>
   <link href="https://hpc.social/2021/uptodate/"/>
   <updated>2021-09-19T09:30:00-06:00</updated>
   <id>https://hpc.social/2021/uptodate</id>
   <content type="html">&lt;p&gt;I recently had an itch to scratch - and that itch was writing a library in Go.
We don’t use Go much for my work, so I figured out a compelling reason to start a new personal project -
a command line tool written in Go (and matching GitHub action) to help keep things up to
date in a repository. Appropriately, I called it &lt;a href=&quot;https://vsoch.github.io/uptodate/docs/#/&quot; target=&quot;_blank&quot;&gt;uptodate&lt;/a&gt;!
It was hugely inspired from the &lt;a href=&quot;https://github.com/autamus/binoc&quot; target=&quot;_blank&quot;&gt;binoc&lt;/a&gt; (short for “binoculars”)
library that can also perform specific kinds of updates, but I wanted more of a focus on
Docker, and to have total control so I could go wild and crazy with writing Go code
without worrying about forcing it on the owner, &lt;a href=&quot;https://github.com/alecbcs&quot; target=&quot;_blank&quot;&gt;alecbcs&lt;/a&gt;, to merge my wild ideas.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;padding:20px&quot;&gt;
&lt;img src=&quot;https://vsoch.github.io/uptodate/assets/img/uptodate.png&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;uptodate&quot;&gt;Uptodate&lt;/h2&gt;

&lt;p&gt;Uptodate is a command line tool in Go and GitHub action that makes it easy to:&lt;/p&gt;

&lt;ol class=&quot;custom-counter&quot;&gt;
  &lt;li&gt; Update FROM statements in Dockerfile to have the latest shas&lt;/li&gt;
  &lt;li&gt; Update build arguments that are for spack versions, GitHub releases and commits, and container hashes.&lt;/li&gt;
  &lt;li&gt; Generate a matrix of Docker builds from a single configuration file&lt;/li&gt;
  &lt;li&gt; Generate a matrix of changed files in a repository.&lt;/li&gt;
  &lt;li&gt; List Dockerfile in a repository that have been changed.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;With all of the above, you can imagine a workflow that first updates Dockerfile
FROM statements and build args, and then re-builds and deploys these containers - 
the assumption being that the underlying dependency such as a GitHub commit
or spack version has an update. Uptodate also will take a nested structure
that I call a docker “build hierarchy” and add new folders and Dockerfile when
a new tag is detected. A kind of updater in uptodate is naturally called an “updater”
and this means for the docker build and docker hierarchy updaters, we can write
a yaml configuration file with our preferences for versions to be added, and
other metadata. You should check out the &lt;a href=&quot;https://vsoch.github.io/uptodate/docs/#/user-guide/user-guide&quot; target=&quot;_blank&quot;&gt;user guide&lt;/a&gt;
for detailed usage, or read about &lt;a href=&quot;https://vsoch.github.io/uptodate/docs/#/user-guide/github-action&quot; target=&quot;_blank&quot;&gt;the GitHub action&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-does-it-work&quot;&gt;How does it work?&lt;/h2&gt;

&lt;p&gt;I’ll give a brief overview of a few of the commands and then a quick example GitHub workflow,
and I’ll recommend that you read the documentation for the latest updates on uptodate, harharhar.
The examples below assumed that you’ve &lt;a href=&quot;https://vsoch.github.io/uptodate/docs/#/user-guide/user-guide?id=install&quot; target=&quot;_blank&quot;&gt;installed&lt;/a&gt; uptodate 
and have the binary “uptodate” in your path.&lt;/p&gt;

&lt;h3 id=&quot;dockerfile&quot;&gt;Dockerfile&lt;/h3&gt;

&lt;p&gt;If you have one or more Dockerfile in your repository you can run uptodate to update digests.
For example:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ uptodate dockerfile .
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;will find Dockerfile in the present working directory and subfolders and update.
For digests, you might see that:&lt;/p&gt;

&lt;div class=&quot;language-dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; ubuntu:20.04&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;is updated to&lt;/p&gt;

&lt;div class=&quot;language-dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; ubuntu:18.04@sha256:9bc830af2bef73276515a29aa896eedfa7bdf4bdbc5c1063b4c457a4bbb8cd79&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Note in the above we still have the digest and the tag, so subsequent updates can
further update the sha by looking up the container based on the tag.
And we can also update build arguments that match a particular format! This one,
specifically:&lt;/p&gt;

&lt;div class=&quot;language-dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;ARG&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; uptodate_&amp;lt;build-arg-type&amp;gt;_&amp;lt;build-arg-value&amp;gt;=&amp;lt;default&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The above flags the build argument for uptodate to look at using the prefix of the library
name, and then the next string after the underscore is the kind of update, followed by
specific metadata for that updater, and of course the value! A few examples are provided below.&lt;/p&gt;

&lt;h4 id=&quot;spack-build-arguments&quot;&gt;Spack Build Arguments&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/spack/spack&quot; target=&quot;_blank&quot;&gt;Spack&lt;/a&gt; is a package manager intended for HPC, and it’s
huge at the lab where I work. So naturally, it made sense for uptodate to be able to
look up the latest spack versions for some package.
To create an argument that matched to a spack package (and its version) you might see:&lt;/p&gt;

&lt;div class=&quot;language-dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;ARG&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; uptodate_spack_ace=6.5.6&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;After the updater runs, if it finds a new version 6.5.12, the line will read:&lt;/p&gt;

&lt;div class=&quot;language-dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;ARG&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; uptodate_spack_ace=6.5.12&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;This works by using the static API that is deployed alongside the &lt;a href=&quot;https://spack.github.io/packages/&quot; target=&quot;_blank&quot;&gt;Spack Packages&lt;/a&gt;
repository that I designed earlier this year. So the updater will get the latest versions
as known within the last 24 hours.&lt;/p&gt;

&lt;h4 id=&quot;github-release-build-argument&quot;&gt;GitHub Release Build Argument&lt;/h4&gt;

&lt;p&gt;If we want an updated version from a GitHub release (let’s say the spack software itself)
we might see this:&lt;/p&gt;

&lt;div class=&quot;language-dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;ARG&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; uptodate_github_release_spack__spack=v0.16.1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The above will look for new releases from spack on GitHub and update as follows:&lt;/p&gt;

&lt;div class=&quot;language-dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;ARG&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; uptodate_github_release_spack__spack=v0.16.2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;h4 id=&quot;github-commit-build-argument&quot;&gt;GitHub Commit Build Argument&lt;/h4&gt;

&lt;p&gt;Similarity, if we want more “bleeding edge” changes we can ask for a commit
from a specific branch, following this pattern:&lt;/p&gt;

&lt;div class=&quot;language-dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;ARG&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; uptodate_github_commit_&amp;lt;org&amp;gt;__&amp;lt;name&amp;gt;__&amp;lt;branch&amp;gt;=&amp;lt;release-tag&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Here is an example of asking for updates for the develop branch.&lt;/p&gt;

&lt;div class=&quot;language-dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;ARG&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; uptodate_github_commit_spack__spack__develop=NA&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;which wouldn’t care about the first “commit” NA as it would update to:&lt;/p&gt;

&lt;div class=&quot;language-dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;ARG&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; uptodate_github_commit_spack__spack__develop=be8e52fbbec8106150680fc628dc72e69e5a20be&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;And then to use it in your Dockerfile, you might pop into an environment variable:&lt;/p&gt;

&lt;div class=&quot;language-dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;ENV&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; spack_commit=${uptodate_github_commit_spack__spack__develop}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;See the &lt;a href=&quot;https://vsoch.github.io/uptodate/docs/#/user-guide/user-guide?id=dockerfile&quot; target=&quot;_blank&quot;&gt;docs&lt;/a&gt; for more detailed usage and an example for the Dockerfile updater.&lt;/p&gt;

&lt;h3 id=&quot;docker-build&quot;&gt;Docker Build&lt;/h3&gt;

&lt;p&gt;The second updater that I think is pretty useful is the Docker build updater.
This updated will read a config file, an uptodate.yaml, and then follow instructions
for version regular expressoins and different kinds of builds args to generate a matrix of
builds (intended for GitHub actions). For example, let’s say that we start with this configuration file:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;na&quot;&gt;dockerbuild&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;build_args&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# This is an example of a manual build arg, versions are required&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;llvm_version&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;

      &lt;span class=&quot;c1&quot;&gt;# The key is a shorthand used for naming (required)&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;llvm&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;versions&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
       &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;4.0.0&quot;&lt;/span&gt;
       &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;5.0.1&quot;&lt;/span&gt;
       &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;6.0.0&quot;&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# This is an example of a spack build arg, the name is the package&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;abyss_version&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;abyss&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;abyss&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;spack&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# This will be parsed by the Dockerfile parser, name is the container name&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;ubuntu_version&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;

      &lt;span class=&quot;na&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ubuntu&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ubuntu&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;container&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;startat&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;16.04&quot;&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;endat&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;20.04&quot;&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; 
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;^[0-9]+[.]04$&quot;&lt;/span&gt; 
      &lt;span class=&quot;na&quot;&gt;skips&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;17.04&quot;&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;19.04&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;You’ll see the primary section of interest is under “dockerbuild” and under this
we have three build args for a manually defined set of versions, a version from
a spack package, and a container. You could run this in a repository root
to look for these config files (and a Dockerfile that they render with in
the same directory or below it) to generate a build matrix.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;uptodate dockerbuild 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Or to only include changed uptodate.yaml files:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;uptodate dockerbuild &lt;span class=&quot;nt&quot;&gt;--changes&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;If you provide a registry URI that the containers build to, we can actually check
these containers to look at current build args (that are saved as labels and then
viewable in the image config by uptodate) to determine if an update is needed.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;uptodate dockerbuild &lt;span class=&quot;nt&quot;&gt;--registry&lt;/span&gt; ghcr.io/rse-radiuss
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;the container. I think this is one of the neatest features - it was just added
in evenings this last week! Check out an
&lt;a href=&quot;https://crane.ggcr.dev/config/ghcr.io/rse-radiuss/ubuntu:20.04&quot; target=&quot;_blank&quot;&gt;example image config&lt;/a&gt; that has these labels!
This registry URI will also be included in the output to make it easy to build
In a GitHub action, it might be used like this:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;jobs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;generate&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Generate Build Matrix&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;runs-on&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ubuntu-latest&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;dockerbuild_matrix&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;$&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;empty_matrix&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;$&lt;/span&gt;

    &lt;span class=&quot;na&quot;&gt;steps&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;uses&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;actions/checkout@v2&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;github.event_name == 'pull_request'&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;with&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
         &lt;span class=&quot;na&quot;&gt;fetch-depth&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;
         &lt;span class=&quot;na&quot;&gt;ref&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;$&lt;/span&gt;

    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;uses&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;actions/checkout@v2&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;github.event_name != 'pull_request'&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;with&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
         &lt;span class=&quot;na&quot;&gt;fetch-depth&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;

    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Generate Build Matrix&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;uses&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;vsoch/uptodate@main&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;dockerbuild&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;with&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; 
        &lt;span class=&quot;na&quot;&gt;root&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;.&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;parser&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;dockerbuild&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;flags&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;--registry&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;ghcr.io/myreponame&quot;&lt;/span&gt;

    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;View and Check Build Matrix Result&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;$&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;|&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;echo ${result}&lt;/span&gt;

  &lt;span class=&quot;na&quot;&gt;build&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;needs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;generate&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;runs-on&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ubuntu-latest&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;strategy&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;fail-fast&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;false&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;$&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;$&lt;/span&gt;

    &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Build&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;$&quot;&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;steps&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Checkout Repository&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;uses&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;actions/checkout@v2&lt;/span&gt;

    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Set up Docker Buildx&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;uses&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;docker/setup-buildx-action@v1&lt;/span&gt;

    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Build $&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;builder&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;container&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;$&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;prefix&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;$&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;filename&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;$&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;|&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;basedir=$(dirname $filename)&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;cd $basedir&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;${prefix} -t ${container} .&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Of course you’d want to login to a registry, and then also possibly calculate metrics for
the container, so consider this a very simple example.
The build matrix that is being passed between those steps has entries like this:&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ubuntu/clang/uptodate.yaml&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;container_name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ghcr.io/rse-radiuss/clang-ubuntu-20.04:llvm-10.0.0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;filename&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ubuntu/clang/Dockerfile&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;parser&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;dockerbuild&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;buildargs&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;llvm_version&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;10.0.0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;ubuntu_version&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;20.04&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;command_prefix&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;docker build -f Dockerfile --build-arg llvm_version=10.0.0 --build-arg ubuntu_version=20.04&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;description&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ubuntu/clang llvm_version:10.0.0 ubuntu_version:20.04&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&quot;git-updater&quot;&gt;Git Updater&lt;/h3&gt;

&lt;p&gt;I also like this updater because it easily generates for you a matrix of files
that are changed, according to git. Running locally it looks like this:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;./uptodate git /path/to/repo
              _            _       _       
  _   _ _ __ | |_ ___   __| | __ _| |_ ___ 
 | | | | &lt;span class=&quot;s1&quot;&gt;'_ \| __/ _ \ / _  |/ _  | __/ _ \
 | |_| | |_) | || (_) | (_| | (_| | ||  __/
  \__,_| .__/ \__\___/ \__,_|\__,_|\__\___|
       |_|                          git


  ⭐️ Changed Files ⭐️
    .github/workflows/build-matrices.yaml: Modify
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;And would generate a matrix for a GitHub action too:&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Modify&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;filename&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;cli/dockerbuild.go&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Modify&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;filename&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;parsers/common.go&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Insert&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;filename&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;parsers/docker/buildargs.go&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Modify&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;filename&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;parsers/docker/docker.go&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Modify&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;filename&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;tests/ubuntu/21.04/Dockerfile&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Modify&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;filename&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;tests/ubuntu/clang/Dockerfile&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;And of course you can change the default “main” to another branch:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;./uptodate git /path/to/repo &lt;span class=&quot;nt&quot;&gt;--branch&lt;/span&gt; master
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;and that also pipes into a GitHub action. I don’t want to redundantly reproduce the docs,
so if you are interested you can read more
at the &lt;a href=&quot;https://vsoch.github.io/uptodate/docs/#/user-guide/user-guide&quot; target=&quot;_blank&quot;&gt;user guide&lt;/a&gt;
or &lt;a href=&quot;https://vsoch.github.io/uptodate/docs/#/user-guide/github-action&quot; target=&quot;_blank&quot;&gt;GitHub action pages&lt;/a&gt;.
Mind you that the library is heavily under develop, so if you have a request for a new updater or want to report
a a bug, please &lt;a href=&quot;https://github.com/vsoch/uptodate/issues&quot; target=&quot;_blank&quot;&gt;let me know!&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;I have loved working on this library. I think it’s the first library in Go where
I’ve been proficient enough to not look everything up that I need - the code has just
flowed from my fingers! Mind you I’m still figuring out my own design preferences,
and I’m at the stage where I’ll write a new functionality, and then immediately not like
my design, and want to re-write it. But I think that means I’ll eventually get better.
But it’s always good to have one or more projects you are passionate about, because
I don’t personally see a point in being a software engineer if I don’t (yes, I know it
makes a salary, but I require more than that).&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>To Compete, Your Team Needs a Specialty</title>
   <link href="https://hpc.social/2021/to-compete-your-team-needs-a-specialty/"/>
   <updated>2021-09-11T01:00:00-06:00</updated>
   <id>https://hpc.social/2021/to-compete-your-team-needs-a-specialty</id>
   <content type="html">&lt;h2 id=&quot;and-hpc-or-research-software-development-isnt-a-specialty&quot;&gt;And ‘HPC’ or ‘Research Software Development’ isn’t a specialty&lt;/h2&gt;

&lt;p&gt;(Note: This post is adapted from &lt;a href=&quot;https://www.researchcomputingteams.org/newsletter_issues/0090&quot;&gt;#90&lt;/a&gt; of the &lt;a href=&quot;https://www.researchcomputingteams.org&quot;&gt;Research Computing Teams Newsletter&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;Quick: what’s your team’s specialty?&lt;/p&gt;

&lt;p&gt;Your team’s specialty is its reputation for what it’s good at. Not what &lt;em&gt;you&lt;/em&gt; think your team is good at; what matters is what specific thing your stakeholders (funders, clients, institutional decision makers) think your specialty is. What they recommend you for to peers, what they recommend funding you for to decision makers.&lt;/p&gt;

&lt;p&gt;In the post-pandemic world, researchers are used to getting their support remotely from anywhere. To compete, your team will need well-defined specialties; and “HPC” or “research software development” isn’t a specialty.&lt;/p&gt;

&lt;figure style=&quot;width: 45%; float: right;&quot;&gt;
  &lt;img alt=&quot;Standout from the crowd by choosing a specific path.&quot; src=&quot;https://www.dursi.ca/assets/imgs/standout_sm.jpg&quot; /&gt;
  &lt;figcaption&gt;&lt;i&gt;Stand out from the crowd by having your team choose a specific path and owning it.&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The pandemic isn’t over, but the end of this phase has begun, and with September (“academic new years”) here, it’s a good time to think about the future. Last October &lt;a href=&quot;https://www.dursi.ca/post/research-computing-in-the-aftertimes&quot;&gt;I wrote about&lt;/a&gt; what post-pandemic research computing is going to look like, and it’s holding up pretty well. With researchers now very comfortable getting research computing and data support virtually and with budgets under pressure, there is going to be a lot more competition for research computing and data teams. Research collaborations are going to be looking elsewhere more and more often - academic teams at other institutions, or with commercial companies (either commercial cloud vendors for compute, or emerging collaborations between well-known names, like &lt;a href=&quot;https://www.nag.com/news/machine-learning-expertise-new-azure-hpc-ai-collaboration-centre&quot;&gt;NAG and Azure&lt;/a&gt;, for services).&lt;/p&gt;

&lt;p&gt;This is an opportunity for well run, focussed teams to grow and prosper. But it’s going to take more planning and forethought than decades past, where one could count on having a near monopsony, of being the only available seller of services to local researchers. It’s going to take developing and maintaining a strong reputation for a small set of specialties.&lt;/p&gt;

&lt;p&gt;“HPC” may sound and feel like a specialty within the community, but to researchers and decision makers it’s incredibly generic and so meaningless. It’s not a technical term, but a term of advocacy and marketing which has been come to mean resources for anything from high throughput batch services to huge tightly coupled simulations to single-node multi-GPU code runs. Even &lt;em&gt;advocates&lt;/em&gt; for the term define it as “anything bigger than what a researcher could provide on their own” which is incredibly generic, and so necessarily meaningless. How can your team’s &lt;em&gt;specialty&lt;/em&gt; be “anything”? A team is expecting researchers to recommend them for “anything?” There’s a reason why VPRs would be just as happy contracting it out (&lt;em&gt;e.g.&lt;/em&gt; see table 2 &lt;a href=&quot;https://www.srainternational.org/blogs/srai-jra1/2019/12/09/operational-fiscal-management-of-core-facilities&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;“Services and expertise for quickly analyzing public-health bioinformatics data”, “a platform for firing off and monitoring aerospace CFD calculations”, “a centre of excellence for digital humanities data curation and archiving”: these are examples of specialities - products, services - that researchers and institutional decision makers can see the value of and be willing to put money into, services and products and teams that researchers can recommend to each other. They are areas where a team could build a strong reputation - they could be the group that researchers recommend to collaborators when they chat about research needs.&lt;/p&gt;

&lt;p&gt;“Research Software Development” at least, to its credit, doesn’t pretend to be a narrow specialty - it’s a broad area which can encompass any area of software development in support of research work. As a result, a team can’t have a specialty in “Research Software Development”; it can have a specialty in “web applications and mobile apps for data collection”, or “GIS analysis tools” or “agent-based simulations for social sciences modelling”. But almost certainly not all three at the same time.&lt;/p&gt;

&lt;p&gt;Even so, research software development is too specific in one unhelpful sense. It could be that researchers are just looking for your team to write some software for them, hand it over, and be done. But increasingly, researchers are looking not just to be delivered some software, but for a team to host the software, run it, operate it - and/or collect and curate data to be used with the tool, for tests or otherwise. Focusing solely on research software development, as a separate activity from systems operation or data analysis and management, can be overly limiting.&lt;/p&gt;

&lt;p&gt;Ok, so what does all of this have to do with competition?&lt;/p&gt;

&lt;p&gt;One of my venial weaknesses is spending too much time on twitter. I’m seeing increasing concern there from research computing teams that cloud vendors or teams using cloud vendors are coming into their institutions and winning or trying to win contracts for projects that “should” have gone to the in-house teams. I’m hearing complaints that the external bids are for amounts of money 2x or more what the in-house team says they could do it for. Incredibly (and almost certainly incorrectly) I’ve even heard 10x.&lt;/p&gt;

&lt;p&gt;Reader, as hard as it is to believe, those complaining see this as an affront&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a class=&quot;footnote&quot; href=&quot;https://www.dursi.ca/feed.xml#fn:1&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;, and a threat, rather than the enormous opportunity it is.&lt;/p&gt;

&lt;p&gt;If a contract at your institution is won - or even in serious contention - that is 2x what you estimate you could have provided the services for, that’s &lt;strong&gt;not&lt;/strong&gt; evidence that the external contractor is overcharging. It’s evidence that your team is &lt;em&gt;undercharging&lt;/em&gt;, that you could have proposed doing more to support that project and the researchers, and that you’re leaving money on the table. It’s also evidence that you haven’t fully convinced the relevant decision makers that you can provide that service; they don’t see it as being part of your specialty.&lt;/p&gt;

&lt;p&gt;Clearly your institution found it worthwhile to spend or consider spending that 2x, because they understood that it was worth at least that much to them to have those services. A bid for half that amount having failed or being questioned means that they really didn’t believe the in-house team could do it as well. That’s revealed-preferences data that you can use. (And if I truly believed someone at my institution was seriously considering spending 10x (1000%!) to work with an outside company rather than work with my team, well, that would occasion some serious soul searching.)&lt;/p&gt;

&lt;p&gt;Cloud providers and other external contractors do have advantages. They have a library of reference architectures they can deploy, so they can pitch (say) CFD solutions to the mech eng department, and bioinformatics pipeline solutions to the biology department. They can pull from a library of testimonials to demonstrate that they can do the work.&lt;/p&gt;

&lt;p&gt;But so can you. You have access to all the literature to search for how others have deployed such solutions. You have (or should have) testimonials from the people that matter - researchers at that very institution. And you have a network of deep relationships in the institution, relationships based on collaboration on research problems. Those relationships and shared expertise and history of collaboration is something the external contractors have no chance of matching.&lt;/p&gt;

&lt;p&gt;If you’re in danger of losing out on these sorts of competitions, it’s because you’re not communicating your specialities in a way that matters, in a way that’s convincing, to the people who could pay for your services. They can’t see how your “HPC batch services” connects with “a digital twinning platform for building simulation”. They don’t see “GIS exploration for private social sciences data” as being an obvious of your “Research Software Development” effort - where’s the data part?  If there’s a miscommunication there about what your team can provide, that’s on you and your team, not on the researchers or other decision makers.&lt;/p&gt;

&lt;p&gt;You have specialities - if you don’t know what they are, ask the researchers who keep coming back. How do they describe what you do? What would they say your speciality is, how do they talk about you to their colleagues? What would you have to demonstrate to them to have them recommend their colleagues to you?&lt;/p&gt;

&lt;p&gt;Similarly, you already have a million things you &lt;em&gt;don’t&lt;/em&gt; do.  You won’t fix a researcher’s printer, you don’t help them do graphic design for their posters, my guess is you don’t help them set up spreadsheets in OneDrive or set up lab webpages.  So it’s not like declaring that there’s computing stuff you do and don’t help researchers with is some completely new thing, previously utterly unknown to your organization.&lt;/p&gt;

&lt;p&gt;Once you make explicit your specialties, you can start playing to your strengths, and communicating them endlessly. You can make a point of reaching out, having your team talk at conferences in the specialties, and at departmental colloquia. You can be well-regarded enough in your institution for those specialties that external contractors pitching work within your speciality never get in the door. You can start more easily hiring people that are interested in that specialty. A specialty builds on itself, snowballs. You can start steering future work towards that specialty to build on it, and start directing work well outside the specialty to somewhere else - where it does fit inside their specialty.&lt;/p&gt;

&lt;p&gt;Yeah, that last part is scary. Sticking to this path isn’t easy. It means turning down opportunities that aren’t in or adjacent to your specialities. Especially for new teams, eager to please, this can be scary.&lt;/p&gt;

&lt;p&gt;But as anywhere in research, your team’s reputation is all that matters. Your team &lt;em&gt;has&lt;/em&gt; a reputation, has stuff it does and doesn’t do. Did you choose it, did you shape it, or are you content to just let it happen?&lt;/p&gt;

&lt;p&gt;Your team can be extremely strong in, specialize in, develop a reputation in, any of a number of things. But not all of the things. Being a manager or leader means choosing.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;And affront was taken. There were lots of dark murmurings about slick sales teams trying to fool gullible senior administrators. And, you know, I’m sure it’s comforting for the teams that might lose out on these contracts to think that the vendor mesmerized the simpleton decision makers with their entrancing slide decks, and so hoodwinked them into considering an overpriced contract. But (a) have they never &lt;em&gt;seen&lt;/em&gt; a vendor pitch? and (b) it’s self-serving twaddle to imagine that just because someone higher up made a decision to work with someone else they must clearly be dumb. Dismissing out of hand the possibility that there might be valid reasons to direct work elswhere means they’re going to end up making a lot of poor and uninformed decisions. &lt;a class=&quot;reversefootnote&quot; href=&quot;https://www.dursi.ca/feed.xml#fnref:1&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;

    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Crimson vs Classic 1 NVMe Multi-OSD Analysis</title>
   <link href="https://hpc.social/2021/crimson-vs-classic-1-nvme-multi-osd-analysis/"/>
   <updated>2021-08-30T01:00:00-06:00</updated>
   <id>https://hpc.social/2021/crimson-vs-classic-1-nvme-multi-osd-analysis</id>
   <content type="html">&lt;p&gt;&lt;a href=&quot;https://docs.google.com/spreadsheets/d/14HMaGxstvWSjobdyAlTdTG_yqhJh7K71Q5rSaxb1S6M/edit?usp=sharing&quot;&gt;Spreadsheet&lt;/a&gt; looking at Crimson vs Classic performance when scaling multiple OSDs on one NVMe drive.  Done to simulate what we can hopefully expect from multi-reactor down the road.  Includes cycles/OP comparisons as well.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Ceph Crimson 2021 Q2 Project Update</title>
   <link href="https://hpc.social/2021/ceph-crimson-2021-q2-project-update/"/>
   <updated>2021-07-29T01:00:00-06:00</updated>
   <id>https://hpc.social/2021/ceph-crimson-2021-q2-project-update</id>
   <content type="html">&lt;p&gt;Slides are available &lt;a href=&quot;https://docs.google.com/presentation/d/1S7QRmN9n7E6ffDdAIVibJipO0prd2bD2I5PFfzXMf0Y/edit?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Research Computing Funding Should Mostly Just Go To Researchers</title>
   <link href="https://hpc.social/2021/research-computing-funding-should-mostly-just-go-to-researchers/"/>
   <updated>2021-06-08T01:00:00-06:00</updated>
   <id>https://hpc.social/2021/research-computing-funding-should-mostly-just-go-to-researchers</id>
   <content type="html">&lt;p&gt;Research computing and data — supporting research efforts with
software, computer and data expertise and resources — is fundamentally
all of a piece.  Today  there’s fewer and fewer hard boundaries
between where the system requirements end and where the software
or data resource requirements begin; and teams supporting researchers
must have expertise across the stack.&lt;/p&gt;

&lt;p&gt;This convergence is a huge opportunity for research computing, but
it’s also a challenge for funders.  How to know how much to allocate to software,
and how much to hardware?  Within software, how many resources
should go to new software development or procurement, and how much
to maintenance?  In hardware, what is the right balance between
GPUs and CPUs or FPGAs, and within data, how much should we support
curation efforts vs discovery, or archival vs near-line storage?&lt;/p&gt;

&lt;p&gt;Luckily, there is a simple, robust, time-tested mechanism research
computing funders can easily take advantage of, and they should do so.
Funders for research computing and data efforts manage their portfolio
effortlessly — in exactly the same way health funders
know how to balance spending between reagents and lab staff, or the
same way physical science funders know how much to allocate to
trainee salaries vs tabletop equipment.&lt;/p&gt;

&lt;p&gt;Most research computing funding should go directly to researchers,
via traditional funding councils, and the researchers should spend
that research computing and data portion of their grants as and where
they see fit.&lt;/p&gt;

&lt;p&gt;With research computing and data funding as an integral component
of project funding, the same research review process that adjudicates
the research proposal would weigh in on the computing and data
resources requested to conduct it.  This eliminates nonsensical but
all-too-common situations where a researcher successfully wins computing
cycles for a non-funded project, or gets funding for a postdoc for
a project but doesn’t get enough compute or storage resources for
the trainee to perform the project.  It would also allow the
researcher to adjust how they were using resources mid-stream; if
after initial efforts it turned out that software development effort
to improve the code was a better use of funding than throwing
hardware at the problem, the money could be spent that way, rather
than applying ahead of time for people time and computing resources
separately and hoping that it all works out in the end.&lt;/p&gt;

&lt;figure style=&quot;width: 50%; float: left;&quot;&gt;
  &lt;img alt=&quot;A technician validates genetic variants identified through whole-exome sequencing at the Cancer Genomics Research Laboratory, part of the National Cancer Institute's Division of Cancer Epidemiology and Genetics (DCEG).&quot; src=&quot;https://www.dursi.ca/assets/research_computing_funding_to_researchers/national-cancer-institute-LxPrHCm8-TI-unsplash.jpg&quot; /&gt;
  &lt;figcaption&gt;We fund researchers to buy all kinds of complex equipment, they can handle buying research computing services.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;In this model, a researcher would include in their grant proposal
a research computing and data component where necessary.  As with
the purchasing wet lab equipment, animal experiments, or large
physical apparatus — undertakings which are no less technical or
complex than research computing — research grants would include
cost justifications for the proposed research computing services
or equipment, and funding agencies would rate the quality of the
justification and the worthiness of the proposed goals versus the
cost.&lt;/p&gt;

&lt;p&gt;A researcher whose proposal was successful would then, as with other
line items, be free to spend that research computing and data
component of their grant where they wish on for software development,
data management and analysis, or access to storage and compute
resources.   Obviously as known entities with existing working
relationships, local research computing centres — now working in a
familiar core facility model — would have a huge advantage.  But
the researcher would not be limited to working with those centres,
nor to working with only one service provider.&lt;/p&gt;

&lt;p&gt;This approach will work well for capacity computing, data, and
expertise — those needs where there are many possible service
providers.  And in those areas, having the researcher in control
of what services they can use where will help drive those vendors
to providing the kinds and quality of services that researchers
need.  But not every kind of computing or expertise capability is
available enough for researchers to be able to easily buy needed
quantities of.  Researchers can’t conjure into existence a (say)
quantum computing shared facility one investigator-led grant at a
time.  Those new and emerging capabilities have to be handled
separately, with existing funding councils setting priorities. Once
those new capabilities are operational, they can and should be
sustained with the same core-facility portable-funding model; if
they can’t, maybe they didn’t need to be built.  Other needs like
foundational infrastructures — research and education networks,
advisory bodies — will also need to be handled separately by funders.&lt;/p&gt;

&lt;p&gt;But for the bulk of research computing, for capacity support of
research using computing, data and related expertise, there’s no
longer need for endless surveys and consultations and projections
to indirectly inform decision making.  Parallel competitions for
different kinds of support for a research project have long since
stopped making sense.  Internal computing organization debates about
what kinds of services to offer should make way for researchers
allocating the funds themselves.  Let researchers decide what works
best for advancing their research.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Nobody Else Cares About Your Tech Stack</title>
   <link href="https://hpc.social/2021/nobody-else-cares-about-your-tech-stack/"/>
   <updated>2021-06-06T01:00:00-06:00</updated>
   <id>https://hpc.social/2021/nobody-else-cares-about-your-tech-stack</id>
   <content type="html">&lt;h2 id=&quot;focus-on-your-researchers-and-funders-problems-not-your-technical-solution&quot;&gt;Focus on your researchers’ and funders’ problems, not your technical solution&lt;/h2&gt;

&lt;p&gt;(Note: This post is adapted from &lt;a href=&quot;https://newsletter.researchcomputingteams.org/archive/research-computing-teams-link-roundup-22-may-2021/&quot;&gt;#75&lt;/a&gt; of the &lt;a href=&quot;https://www.researchcomputingteams.org&quot;&gt;Research Computing Teams Newsletter&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;Many of us who are managing research computing and data teams come up through the ranks doing research ourselves, and have
experience in grantwriting for open research calls.  That can actually &lt;em&gt;hold us back&lt;/em&gt; from succeeding with getting grants
for “digital research infrastructure” — building teams and infrastructure to support research.&lt;/p&gt;

&lt;p&gt;The thing is, digital research infrastructure calls, the sort that support research computing and data teams and tools,
are more like applying to grants as a nonprofit than as a researcher.  And we can learn a lot from how the nonprofit
community writes funding proposals.&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;We're not proposing a research project, we're proposing to solve problems a funder sees for a research community.&quot; src=&quot;https://www.dursi.ca/assets/nobody_tech_stack/nonprofit_not_researcher.png&quot; style=&quot;float: right; width: 50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Any funder has things they want to accomplish, and the goal as a potential fundee is to find something in the intersection of
“work that helps the funder accomplish their goals” and “work that we are able to do and that is aligned
with our goals”.   Excellent work that isn’t in that first set won’t get funding.  Money attached to work that isn’t
in the second set is at best a distraction, at worst drains your teams’ credibility.&lt;/p&gt;

&lt;p&gt;Most of us in research got our experience in grants from open disciplinary competitions where the funders and fundees goals
are aligned — be seen to be funding/doing the best research.  That means you don’t have to think about the distinction
very much.  The funder wants a portfolio of projects that are promising and could have impact - some will pan out and some
won’t, but such is research.   So everyone is focussed on “the best” work.  There’s a lot of focus on methods and technology
used, because those are relevant for assessing the best work.  A new technology or method might be why it’s important to
fund this work now - some key observation wasn’t possible before, but now it is, and the funder and team who makes the
observation now will get the impact.  And methods can sabotage a project - a team that does great work with the wrong
methods won’t get the best results.&lt;/p&gt;

&lt;p&gt;Special digital research infrastructure calls — like those that research computing projects typically fall under —
and calls by nonprofit funders, are different.  The funder has some particular change they want to see in the world;
some community they want to see better served.  They are generally much less willing to take a flyer on projects with
only a modest chance of success, because failures won’t serve the community they want to see served.  Something that
successfully serves the community can always be improved in future iterations; something that fails to meet the communities
needs may well be unsalvagable.&lt;/p&gt;

&lt;p&gt;Methods and technology matter much less to these funders.  They want to know that you can credibly deliver on the proposal,
and that you have a plan, but the nuts and bolts typically are much less interesting.&lt;/p&gt;

&lt;p&gt;A nonprofit funder absolutely wants to understand how the after-school homework tutoring program you’re proposing will
interact with the community — how it will find underserved students, how the tutoring will be delivered to the
students, what indicators will be used to measure success — but the behind the scenes tech stack like what task
management and tutor booking software you’ll use is completely irrelevant unless it’s to justify that you’ll
be able to deliver the program.  (And if you are in a position where you need details like that to justify your
credibility for delivering the program, you are probably not in serious contention for the funding).  Every paragraph
you spend talking about the cool new tutor booking software you’re going to use is a paragraph that doesn’t get spent
highlighting the funder’s goals being achieved — more underserved students doing better in school.&lt;/p&gt;

&lt;p&gt;A research computing funder who’s receptive to a “we’ll run a new research data management platform specifically
aimed at [discipline X]” proposal absolutely wants to know that you’re familiar with the underserved area, that
you’ve been successful delivering similar things before, and what metrics you’ll use for success.  They do not care
that your roadmap includes Kubernetes and some exciting new operators.  Would they be disappointed if mid-stream, you
pivoted to running the tasks on bare metal with Ansible?  If not, why draw their attention and yours to obscure and
uncertain details rather than to how your work will best advance their goals?&lt;/p&gt;

&lt;p&gt;The thing is, this same approach applies to not just research funders, but anyone you plan to work with; any research
group that contacts your team looking for something.  They have a problem; the greater the up-front focus on understanding
 and solving researcher’s problem, the better the chance of success.&lt;/p&gt;

&lt;p&gt;How will you know what the funder’s or researcher’s problems and goals are?  In the funder’s case, the call will sometimes
spell it out; in the researcher’s case, they’ll usually say something.  In both cases, it may require some question-asking
and digging deeper; the researcher’s or even the funder’s “presenting problem” may not be the underlying issue,
and the funder’s call may focus on one particular aspect rather than the overarching goals.  But the solution is the same;
just ask a bunch of questions.&lt;/p&gt;

&lt;p&gt;“Do you mean they will they just tell you?”  I know a team in a Hackathon who went to an open pre-hackathon info
session, and approached the organizer and sponsor in a gaggle afterwards.  They asked the sponsor — the lead judge — what
a successful Hackathon would be from their point of view.  The sponsor — who, again, was the &lt;em&gt;lead judge&lt;/em&gt; — answered with
a particular problem they’d like solved as an example.  That team and mystifyingly only that team delivered a partial but
promising solution to the exact problem described in detail and in public, and they of course won first prize.  How could
they not?  People organize special funding calls and hackathons &lt;em&gt;because&lt;/em&gt; &lt;em&gt;they&lt;/em&gt; &lt;em&gt;want&lt;/em&gt; &lt;em&gt;other&lt;/em&gt; &lt;em&gt;people&lt;/em&gt; &lt;em&gt;to&lt;/em&gt; &lt;em&gt;help&lt;/em&gt; &lt;em&gt;them&lt;/em&gt;
&lt;em&gt;achieve&lt;/em&gt; &lt;em&gt;their&lt;/em&gt; &lt;em&gt;goals&lt;/em&gt;.  Yes, they’ll tell you, and if you keep asking questions they’ll keep talking about it until you politely explain
that you have to leave for the evening.  They put that contact information there and run informational sessions for a reason.&lt;/p&gt;

&lt;p&gt;The stakeholder side of research computing isn’t rocket surgery.  But listening, digging in, and focussing on their goals
is still rare enough that doing it well is almost an unfair advantage.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>SRE to Solutions Architect</title>
   <link href="https://hpc.social/2021/sre-to-solutions-architect/"/>
   <updated>2021-01-02T19:06:36-07:00</updated>
   <id>https://hpc.social/2021/sre-to-solutions-architect</id>
   <content type="html">&lt;p&gt;It’s been about two years since I joined NVIDIA as a Solutions Architect, which was a pretty big job change for me! Most of my previous work was in jobs that could fall under the heading of “site reliability engineering”, where I was actively responsible for the operations of computing systems, but my new job mostly has me helping customers design and build their own systems.&lt;/p&gt;

&lt;p&gt;I’m finally starting to feel like I know what I’m doing at least 25% of the time ? so I thought this would be a good time to reflect on the differences between these roles and what my past experience brings to the table for my (sort of) new job.&lt;/p&gt;

&lt;p&gt;&lt;span id=&quot;more-147&quot;&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(Just a note: I feel like job titles for ops folks are a fraught topic. My job titles have included things like “Production Engineer”, “HPC Cluster Administrator”, and “HPC/Cloud Systems Engineer”. I tend to self-identify more with the term “sysadmin”, but I’m using “SRE” as the most current term that captures the work I’ve spent a lot of my career doing, where I generally approached ops from a software engineering perspective.  Feel free to substitute your job title of choice!)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I spent most of the past 10 years building and running large computing systems. With the exception of ~18 months working on backend storage for a &lt;a href=&quot;https://www.facebook.com&quot;&gt;fairly large website&lt;/a&gt;, I’ve mostly worked on large high-performance-computing (HPC) clusters. These systems are generally used by researchers and engineers to run simulations and data analysis. The teams I joined were generally responsible for building these clusters, keeping them running, helping the researchers who used them, and making sure they performed well.&lt;/p&gt;

&lt;p&gt;In my day-to-day work in SRE (or whatever you call it), I mostly thought about problems like:&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;Are my team’s services operating reliably and predictably, according to our defined metrics?&lt;ul&gt;&lt;li&gt;Translated: What’s broken today?! ?&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;Are our (internal) customers having a good &lt;em&gt;qualitative&lt;/em&gt; experience?&lt;/li&gt;&lt;li&gt;For any current or recently-past incidents, how can we understand what went wrong and incorporate that into our future development?&lt;/li&gt;&lt;li&gt;What major features or other changes are we hoping to release soon? How can we be confident they’ll work correctly and reliably?&lt;/li&gt;&lt;li&gt;Are we expecting to have to turn up more capacity or new systems soon? Are we ready to do so?&lt;/li&gt;&lt;li&gt;What projects can I pursue to automate anything boring that I have to work on?&lt;/li&gt;&lt;/ul&gt;

&lt;hr class=&quot;wp-block-separator&quot; /&gt;

&lt;p&gt;My role as a solutions architect is rather different, as I don’t actually have any services I’m responsible for keeping online. Instead, I’m generally working with external customers who are working with our products and using them in their own production environments. Because I’m focused on HPC and supercomputing, my customers have generally purchased NVIDIA’s hardware products, and are operating them in their own datacenters. I’m frequently talking to the SRE teams, but I’m not part of them myself.&lt;/p&gt;

&lt;p&gt;In my daily work as a solutions architect, I’m thinking more about questions like:&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;Do my (external) customers have a good understanding of what our products are and how to use them?&lt;ul&gt;&lt;li&gt;This may include products they already use, or new products that they may be planning to deploy&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;What are their pain points, and how can I feed that back to the product teams?&lt;ul&gt;&lt;li&gt;And also: What new product developments can I provide pro-active advice on before it makes it to the customer?&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;What new customer deployments are coming up, and how can I help them go smoothly?&lt;/li&gt;&lt;li&gt;How are our customers doing running their current clusters, and are they feeling a lot of pain?&lt;/li&gt;&lt;li&gt;What tools can I develop, or what content can I write, to help all of the above go well?&lt;/li&gt;&lt;/ul&gt;

&lt;hr class=&quot;wp-block-separator&quot; /&gt;

&lt;p&gt;On the one hand, I work on a lot of the same &lt;em&gt;problems&lt;/em&gt; as a solutions architect as I did in SRE. I still spend a lot of time thinking about the scalability, performance, and reliability of HPC systems. I still care a lot about making sure the systems I help build are useful and usable for researchers.&lt;/p&gt;

&lt;p&gt;On the other hand, I’m not so much on the pointy end of these problems anymore. My work is mostly focused on enabling others to run reliable systems, rather than being directly on the hook for them. And while I do help directly manage some internal lab clusters, those systems have &lt;em&gt;very&lt;/em&gt; loose SLOs. So in practice I haven’t been on call in about two years.&lt;/p&gt;

&lt;p&gt;I do think my experience in SRE has been really important in doing a good job in solutions architecture. I like to think I have a pretty good instinct for systems design at this point, and I can often help identify problems and bottlenecks in early stages. My troubleshooting skills from SRE work are incredibly helpful, as a lot of my work is helping customers understand what the heck is broken on their clusters. And I also find that it really helps to have someone who “speaks the same language” as the SRE teams for our customers, especially because I feel like so many vendor relationships neglect reliability concerns in favor of features.&lt;/p&gt;

&lt;p&gt;The transition has been really interesting, and I’m still conflicted about which kind of job I prefer. I don’t exactly miss being on call&amp;#8230; but I do miss somewhat the more visceral feeling of understanding a running system really well through sheer continuous contact with it. However, I really love helping my customers build cool systems, and I like the satisfaction of helping many different teams do well, versus focusing tightly on a single service.&lt;/p&gt;

&lt;p&gt;I’m really enjoying the solutions architect gig right now, but I also wouldn’t be surprised if I ended up doing SRE work directly again at some point.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Sketching out HPC clusters at different scales</title>
   <link href="https://hpc.social/2020/sketching-out-hpc-clusters-at-different-scales/"/>
   <updated>2020-12-14T01:03:23-07:00</updated>
   <id>https://hpc.social/2020/sketching-out-hpc-clusters-at-different-scales</id>
   <content type="html">&lt;p&gt;High-performance computing (HPC) clusters come in a variety of shapes and sizes, depending on the scale of the problems you’re working on, the number of different people using the cluster, and what kinds of resources they need to use. &lt;/p&gt;

&lt;p&gt;However, it’s often not clear what kinds of differences separate the kind of cluster you might build for your small research team:&lt;/p&gt;

&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;img alt=&quot;&quot; class=&quot;wp-image-139&quot; height=&quot;1339&quot; src=&quot;https://thinking.ajdecon.org/wp-content/uploads/2020/12/image-9.jpg&quot; width=&quot;1601&quot; /&gt;&lt;figcaption&gt;Note: do not use in production&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;From the kind of cluster that might serve a large laboratory with many different researchers:&lt;/p&gt;

&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;img alt=&quot;&quot; class=&quot;wp-image-140&quot; height=&quot;470&quot; src=&quot;https://thinking.ajdecon.org/wp-content/uploads/2020/12/img_0143.jpg&quot; width=&quot;892&quot; /&gt;&lt;figcaption&gt;The Trinity supercomputer at Los Alamos National Lab, also known as “that goddamn machine” when I used to get paged at 3am&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;There are lots of differences between a supercomputer and my toy Raspberry Pi cluster, but also a lot in common. From a management perspective, a big part of the difference is how many different specialized node types you might find in the larger system.&lt;/p&gt;

&lt;p&gt;&lt;span id=&quot;more-113&quot;&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Just a note: in this post I’m assuming we’re talking about compute clusters of the type that might be used to run simulations or data analysis jobs. This probably won’t help if you’re designing a database cluster, a Kubernetes cluster to serve a web infrastructure, etc.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Let’s start with one of the simplest ways you can build a cluster: a collection of&lt;strong&gt; compute nodes&lt;/strong&gt;, all connected to a network, with a single &lt;strong&gt;“head node&lt;/strong&gt;” that coordinates work between them:&lt;/p&gt;

&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;img alt=&quot;Diagram showing a single head node connected to five compute  nodes with a single network&quot; class=&quot;wp-image-118&quot; height=&quot;524&quot; src=&quot;https://thinking.ajdecon.org/wp-content/uploads/2020/12/image.jpg&quot; width=&quot;910&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;With this design, the head node performs most of the functions that coordinate work or provide shared services on the cluster. The compute nodes are then free for the actual compute jobs on the cluster, like simulating the weather or analyzing telescope data!&lt;/p&gt;

&lt;p&gt;Some of the shared services that most clusters provide from the head node include:&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;Running a &lt;strong&gt;job scheduler&lt;/strong&gt; that accepts requests from the users and queues them up to run on the compute nodes&lt;/li&gt;&lt;li&gt;Exporting a &lt;strong&gt;shared filesystem &lt;/strong&gt;to the other machines, so they can all access the same storage space&lt;/li&gt;&lt;li&gt;Accepting &lt;strong&gt;user logins&lt;/strong&gt; so that the people who want to run on the cluster have an access point to the cluster&lt;/li&gt;&lt;li&gt;Acting as a &lt;strong&gt;management node&lt;/strong&gt; that the cluster sysadmins can use to help maintain the rest of the cluster&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;This kind of design can scale remarkably well, and it’s probably the most common kind of cluster out there. But at some point, you might find that the head node is doing too much, and you need to split its functions across multiple machines.&lt;/p&gt;

&lt;p&gt;The first thing you’ll often see is moving user logins onto their own dedicated &lt;strong&gt;login node&lt;/strong&gt;:&lt;/p&gt;

&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;img alt=&quot;Diagram showing a login node, a management node, and five compute nodes connected on the same network&quot; class=&quot;wp-image-120&quot; height=&quot;643&quot; src=&quot;https://thinking.ajdecon.org/wp-content/uploads/2020/12/image-1.jpg&quot; width=&quot;1317&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;All the other functions are still on the head node (which is often explicitly called a &lt;strong&gt;management node&lt;/strong&gt; at this point). But by moving user logins to their own node, it becomes easier to do maintenance or make changes to the larger system without disturbing your users. &lt;/p&gt;

&lt;p&gt;(It also means that if your users accidentally crash the login node, they’re less likely to take down all those shared services on the management node&amp;#8230;)&lt;/p&gt;

&lt;p&gt;If you have lots of users, you can also easily add more login nodes! These scale pretty well because the shared services are all still on the management node, but your users get more interactive nodes for their development work&lt;/p&gt;

&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;img alt=&quot;Diagram showing three login nodes, a management node, and five compute nodes on the same network&quot; class=&quot;wp-image-122&quot; height=&quot;631&quot; src=&quot;https://thinking.ajdecon.org/wp-content/uploads/2020/12/image-2.jpg&quot; width=&quot;1269&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;At this point, you might also set up a second management node in order to provide redundancy or failover in case your primary management node fails:&lt;/p&gt;

&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;img alt=&quot;Diagram showing three login nodes, two management nodes, and five compute nodes&quot; class=&quot;wp-image-124&quot; height=&quot;703&quot; src=&quot;https://thinking.ajdecon.org/wp-content/uploads/2020/12/image-3.jpg&quot; width=&quot;1291&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;At this point we have a lot of compute nodes, redundant management nodes, and a nice collection of login nodes for the users to use for their work. What else might we need as we scale up?&lt;/p&gt;

&lt;p&gt;Well, for one thing, the shared filesystem is still on the management node. We might want to split it off onto its own machine to provide better performance:&lt;/p&gt;

&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;img alt=&quot;Diagram showing three login nodes, two management nodes, a storage node, and five compute nodes on the same network&quot; class=&quot;wp-image-126&quot; height=&quot;777&quot; src=&quot;https://thinking.ajdecon.org/wp-content/uploads/2020/12/image-4.jpg&quot; width=&quot;1300&quot; /&gt;&lt;figcaption&gt;Following tradition, storage is represented as a poorly-drawn cylinder to match the shape of a hard drive platter ?&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Or if we want to scale our performance higher than a single storage server can provide, we might want to use a &lt;strong&gt;distributed filesystem&lt;/strong&gt; like Lustre, BeeGFS, or GPFS and provide a whole tier of dedicated storage machines:&lt;/p&gt;

&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;img alt=&quot;Replace single storage node with three storage nodes in a cluster&quot; class=&quot;wp-image-128&quot; height=&quot;741&quot; src=&quot;https://thinking.ajdecon.org/wp-content/uploads/2020/12/image-5.jpg&quot; width=&quot;1341&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;You might also notice that we’re using the same network for everything! Communication between compute nodes, access to storage, and management services are all competing to send messages over the same network. This could be a problem if, for example, the application wants to simultaneously read lots of data from storage and exchange messages with neighboring compute nodes. &lt;/p&gt;

&lt;p&gt;At this point we may want to split these different types of traffic onto their own networks:&lt;/p&gt;

&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;img alt=&quot;Same diagram, but add a separate application network connecting only the compute nodes , and a separate storage network connecting storage and compute only&quot; class=&quot;wp-image-130&quot; height=&quot;1044&quot; src=&quot;https://thinking.ajdecon.org/wp-content/uploads/2020/12/image-6.jpg&quot; width=&quot;1590&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;Depending on how much you need to optimize (or how much you want to spend!), you may have several different networks connecting all the machines in the cluster, separated by function. You may have dedicated networks for functions like:&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;&lt;strong&gt;High-speed network&lt;/strong&gt; (or &lt;strong&gt;application network&lt;/strong&gt;): This is a dedicated network for user applications to communicate between compute nodes, and is often built using specialized hardware like Infiniband or a vendor-proprietary technology. This is especially important if you use technologies like MPI in your applications, which rely heavily on inter-node communication.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Storage network&lt;/strong&gt;: This is a dedicated network for access to storage. If you rely on especially fast network storage, you might use Infiniband or another very fast network here too.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Management network&lt;/strong&gt;: This is often the “everything else” network, used for job scheduling, SSH, and other miscellaneous traffic. This is often a less-performant network, using 1Gb or 10Gb Ethernet, because we expect the heavier usage to be on the application or storage networks.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Out-of-band management network&lt;/strong&gt;: Many datacenter environments have methods for managing individual servers outside their operating systems, such as accessing the baseboard management controllers. However, this kind of access can be a security risk, and it’s often put on its own network to restrict access.&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;All these different networks may be on their own hardware, for the best performance; or they may be virtual networks (VLANs) sharing the same physical connections. &lt;/p&gt;

&lt;p&gt;Once you get past this point, there are many different ways to continue splitting off or adding special-purpose functions, but these are less common outside of very large sites.&lt;/p&gt;

&lt;p&gt;For example, you may have multiple independent storage systems you want to access:&lt;/p&gt;

&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;img alt=&quot;Add a second storage cluster, separate from the first, on the storage network&quot; class=&quot;wp-image-132&quot; height=&quot;1293&quot; src=&quot;https://thinking.ajdecon.org/wp-content/uploads/2020/12/image-7.jpg&quot; width=&quot;1600&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;Or your cluster may depend on fast access to an external resource, and you want to attach a dedicated tier of network routers:&lt;/p&gt;

&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;img alt=&quot;Add a pair of router nodes on the management node. The router nodes also have connections to the internet &quot; class=&quot;wp-image-134&quot; height=&quot;1398&quot; src=&quot;https://thinking.ajdecon.org/wp-content/uploads/2020/12/image-8.jpg&quot; width=&quot;1492&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;Or you may even have some slower tier of storage that you need to move data in and out of, such as S3 or a tape system, and build a set of dedicated machines for data movement:&lt;/p&gt;

&lt;figure class=&quot;wp-block-image&quot;&gt;&lt;img alt=&quot;Add a pair of data movement nodes connected to the management nodes. The data movement nodes also have a connection to an external storage system&quot; class=&quot;wp-image-135&quot; height=&quot;1600&quot; src=&quot;https://thinking.ajdecon.org/wp-content/uploads/2020/12/53d60f83-a8c7-447a-b528-43e61ff5e300-3966-00000359711b33bb_file.jpg&quot; width=&quot;1600&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;In other words, you can add as much complexity as you like! ? Or, as much as your users and workloads require. Very complex environments serving many researchers may have many different tiers of dedicated machines, for data movement, network routing, managing software licenses, and more. But not every environment will need this type of complexity.&lt;/p&gt;

&lt;p&gt;In all cases, the general strategy is the same: if your work is being bottlenecked by some you special-purpose function, you may consider moving that work to dedicated machines to get better performance. &lt;/p&gt;

&lt;p&gt;This needs to be balanced, though, against the costs of doing so, in money, power, rack space, or other constraints. Frequently, there’s a trade-off between adding special-purpose machines and adding more compute machines, and your users might prefer to just have more compute!&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>When Research Infrastructure Is and Isn't Maintained</title>
   <link href="https://hpc.social/2020/when-research-infrastructure-is-and-isn-t-maintained/"/>
   <updated>2020-12-04T00:00:00-07:00</updated>
   <id>https://hpc.social/2020/when-research-infrastructure-is-and-isn-t-maintained</id>
   <content type="html">&lt;p&gt;(Note: This post is adapted from &lt;a href=&quot;https://www.researchcomputingteams.org/newsletter_issues/0053&quot;&gt;#53&lt;/a&gt; of the &lt;a href=&quot;https://www.researchcomputingteams.org&quot;&gt;Research Computing Teams Newsletter&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;There were two big stories in the news this week (as I write this, at the end of 2020) about what’s possible with sustained research infrastructure funding and what happens when research infrastructure isn’t sustained.&lt;/p&gt;

&lt;p&gt;In the first, you’ve probably read about AlphaFold, Google Brain’s efforts to bring deep learning to protein folding. &lt;a href=&quot;https://www.the-scientist.com/news-opinion/deepmind-ai-speeds-up-the-time-to-determine-proteins-structures-68221&quot;&gt;It did very well&lt;/a&gt; in the 14th annual Critical Assessment of (protein) Structure Prediction (CASP) contest. Predictably but unfortunately, Google’s press releases wildly overhyped the results - “Protein Folding Solved”.&lt;/p&gt;

&lt;p&gt;Most proteins fold very robustly in the chaotic environment of the cell, and so it’s expected that there should be complex features that predict how the proteins folded configurations look. We still don’t know anything about the model AlphaFold used - other than it did very well on these 100 proteins - or how it was trained. There are a lot of questions of how it will work with more poorly behaved proteins - a wrong confident prediction could be much worse than no prediction. But it did get very good results, and with a very small amount of computational time to actually make the predictions. That raises a lot of hope for the scope of near-term future advances.&lt;/p&gt;

&lt;p&gt;But as &lt;a href=&quot;https://twitter.com/aledmedwards/status/1333754396530847745&quot;&gt;Aled Edwards points out on twitter&lt;/a&gt;, the real story here is one of long term, multi-decadal, investment in research infrastructure including research data infrastructure by the structural biology community. The &lt;a href=&quot;https://www.wwpdb.org&quot;&gt;protein data bank&lt;/a&gt; was set up 50 years ago (!!); and a culture of data sharing of these laboriously solved protein structures was set up, with a norm of contributing to (and helping curate) the data bank. That databank has been continuously curated and maintained, new techniques developed, eventually leading to the massive database now on which methods can be trained and results compared.&lt;/p&gt;

&lt;p&gt;It’s the sustained funding and support - monetarily but also in terms of aligning research incentives like credit - which built the PDB. The other big story we heard this week tells us that you can’t just fund a piece of infrastructure, walk away, and expect the result to be self-sustaining. On December 1st, the iconic &lt;a href=&quot;https://www.the-scientist.com/news-opinion/famous-arecibo-radio-telescope-in-puerto-rico-collapses-68219&quot;&gt;Arecibo Radio Telescope in Puerto Rico collapsed&lt;/a&gt;. The telescope was considered important enough to keep running - there was no move to decommission it until late November - but not important enough to keep funding the maintenance to keep it functioning.&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;Overhead image of a broken Arecibo Telescope&quot; src=&quot;https://www.dursi.ca/assets/imgs/arecibo-collapsed.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Digital research infrastructure - software, data resources, computing systems - fall apart at least as quickly without ongoing funded effort to maintain them.  It’s not about whether these digital pieces of infrastructure are “sustainable”; it’s whether or not they are &lt;em&gt;sustained&lt;/em&gt;. Too many critical pieces of our digital research infrastructure are not being sustained.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>SC'20 Recap</title>
   <link href="https://hpc.social/2020/sc-20-recap/"/>
   <updated>2020-11-23T13:00:00-07:00</updated>
   <id>https://hpc.social/2020/sc-20-recap</id>
   <content type="html">&lt;p&gt;The &lt;a href=&quot;https://sc20.supercomputing.org/&quot;&gt;HPC industry's biggest conference, SC&lt;/a&gt;, was held virtually over the last two weeks. Although the original plan to hold it in Atlanta was supplanted by all-virtual format, it still managed to be a whirlwind show full of product showcases, research presentations, and interesting talks, panels, and workshops. The virtual format certainly wasn't the same as attending in-person, but some of the conference buzz and tone could still be sensed by following the &lt;a href=&quot;https://twitter.com/search?q=%23SC20&quot;&gt;#SC20 tag on Twitter&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As &lt;a href=&quot;https://glennklockwood.blogspot.com/2019/11/sc19-recap.html&quot;&gt;with SC'19&lt;/a&gt;, the conference seemed subdued in part due to the fact that many attendees were still being pulled away by their daily lives while attending and in part because the HPC community is still waiting for exascale to finally get here. The community's conversion to remote work has also smeared a lot of the usual vendor briefings and big announcements out over the entire five-month period since ISC'19, causing most of the hot news at SC this year to seem incremental over years past.&lt;/p&gt;
&lt;p&gt;Still, I picked up on a few themes that I thought were noteworthy, and what follows is a recap of some of the highlights from the conference as I saw them.&lt;/p&gt;
&lt;p&gt;All the standard disclaimers apply to the remainder of this post: these are just my personal opinion and do not represent the viewpoint of anyone other than me. I'm not an expert on many (most?) of these topics, so my observations may be misinformed or downright wrong--feel free to get in touch if I stand to be corrected. Also bear in mind that what I find interesting is colored by my day job as a storage architect; I don't pay close attention to the scientific or application spaces in HPC and instead focus on hardware, architecture, systems design, integration, and I/O. As such, I'm sure I missed all sorts of topics that others find exciting.&lt;/p&gt;
&lt;h2 style=&quot;text-align: left;&quot;&gt;Table of Contents&lt;/h2&gt;
&lt;ol&gt;&lt;li&gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/feeds/posts/default/-/hpc?alt=rss#bigsplash&quot;&gt;Big Splashes&lt;/a&gt;&lt;ol&gt;  &lt;li&gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/feeds/posts/default/-/hpc?alt=rss#bigsplash-whatsnew&quot;&gt;What's new&lt;/a&gt;&lt;/li&gt;  &lt;li&gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/feeds/posts/default/-/hpc?alt=rss#bigsplash-whatsmissing&quot;&gt;What's missing&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/feeds/posts/default/-/hpc?alt=rss#themes&quot;&gt;High-level Themes&lt;/a&gt;&lt;ol&gt;  &lt;li&gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/feeds/posts/default/-/hpc?alt=rss#comptechfutures&quot;&gt;Computing Technologies Futures&lt;/a&gt;&lt;/li&gt;  &lt;li&gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/feeds/posts/default/-/hpc?alt=rss#storagetechfutures&quot;&gt;Storage Technologies Futures&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/feeds/posts/default/-/hpc?alt=rss#actualfutures&quot;&gt;Actual Future Directions&lt;/a&gt;&lt;ol&gt;  &lt;li&gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/feeds/posts/default/-/hpc?alt=rss#actualfutures-hpcai&quot;&gt;The Relationship of HPC and AI&lt;/a&gt;&lt;/li&gt;  &lt;li&gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/feeds/posts/default/-/hpc?alt=rss#actualfutures-disagg&quot;&gt;Disaggregation in Practice&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/feeds/posts/default/-/hpc?alt=rss#ssug&quot;&gt;Spectrum Scale User Group vs. Lustre BOF&lt;/a&gt;&lt;ol&gt;  &lt;li&gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/feeds/posts/default/-/hpc?alt=rss#ssug-1&quot;&gt;Enterprisey features that organizations may care about&lt;/a&gt;&lt;/li&gt;  &lt;li&gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/feeds/posts/default/-/hpc?alt=rss#ssug-2&quot;&gt;Manageability features that administrators may care about&lt;/a&gt;&lt;/li&gt;  &lt;li&gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/feeds/posts/default/-/hpc?alt=rss#ssug-3&quot;&gt;Performance, scalability, and reliability features that end users may care about&lt;/a&gt;&lt;/li&gt;  &lt;li&gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/feeds/posts/default/-/hpc?alt=rss#ssug-4&quot;&gt;Interface features that platform developers may care about&lt;/a&gt;&lt;/li&gt;  &lt;li&gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/feeds/posts/default/-/hpc?alt=rss#ssug-overall&quot;&gt;Overall Impressions&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/feeds/posts/default/-/hpc?alt=rss#io500&quot;&gt;IO-500 BOF&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/feeds/posts/default/-/hpc?alt=rss#conclusion&quot;&gt;Concluding Thoughts&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;
&lt;h2 id=&quot;bigsplash&quot; style=&quot;text-align: left;&quot;&gt;Big Splashes&lt;/h2&gt;
&lt;p&gt;Although there weren't any earth-shattering announcements this year, there were a few newsworthy developments that received a healthy amount of press attention.&lt;/p&gt;
&lt;h3 id=&quot;bigsplash-whatsnew&quot; style=&quot;text-align: left;&quot;&gt;What's new&lt;/h3&gt;
&lt;p&gt;&lt;b&gt;RIKEN's Fugaku machine&lt;/b&gt; made its debut at ISC'20 in June this year, but I felt a lot of its deserved fanfare was muted by the the newness of the pandemic and the late-binding decision to convert ISC'20 to being all remote. SC'20 was when Fugaku got to really shine; it improved benchmark results for HPL, HPCG, and Graph500 relative to its ISC'20 numbers:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://lh3.googleusercontent.com/-h7Z74v-IiMQ/X7s3qep3YbI/AAAAAAABPnk/fJLP0QrjIFAIL_IQ0Vj3_9pfII91KVdGQCLcBGAsYHQ/Em-YT_1VcAA82Fj.jpeg&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img alt=&quot;Fugaku performance improvements since July 2020&quot; height=&quot;226&quot; src=&quot;https://lh3.googleusercontent.com/-h7Z74v-IiMQ/X7s3qep3YbI/AAAAAAABPnk/fJLP0QrjIFAIL_IQ0Vj3_9pfII91KVdGQCLcBGAsYHQ/w400-h226/Em-YT_1VcAA82Fj.jpeg&quot; title=&quot;Fugaku performance improvements since July 2020&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;Fugaku performance improvements since July 2020 from &lt;a href=&quot;https://event.on24.com/wcc/r/2825195/3357A8DF10E6050DE025D69073348677&quot;&gt;Prof. Matsuoka's FLATS keynote&lt;/a&gt;&lt;/span&gt;&lt;/b&gt;&lt;/div&gt;
&lt;p&gt;But RIKEN and Fujitsu had a number of early science success stories to showcase around how the machine was being cited in scientific studies towards better understanding COVID-19.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Intel announced the Ice Lake Xeon architecture&lt;/b&gt; as well and put a lot of marketing behind it. And by itself, Ice Lake is a &lt;a href=&quot;https://www.hpcwire.com/solution_content/intel/the-ice-lake-top-10/&quot;&gt;major advancement&lt;/a&gt; since it's Intel's first server part that uses their 10 nm process and provides a PCIe Gen4 host interface, and it includes support for 2nd generation 3D XPoint DIMMs (Barlow Pass) and 8 DDR4 memory channels.&lt;/p&gt;
&lt;p&gt;Unfortunately, Ice Lake is late to the party in the context of its competition; Intel's benchmark results &lt;a href=&quot;https://newsroom.intel.com/news-releases/intel-building-future-high-performance-computing/&quot;&gt;position Ice Lake as a competitor to AMD Rome&lt;/a&gt; which matches Ice Lake's 8-channel/PCIe Gen4-based platform despite being over a year old at this point. For reference:&lt;/p&gt;
&lt;div&gt;    &lt;table class=&quot;tg&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot;&gt;    &lt;thead&gt;      &lt;tr&gt;        &lt;th class=&quot;tg-6ic8&quot;&gt;&lt;/th&gt;        &lt;th class=&quot;tg-7btt&quot;&gt;Intel Ice Lake&lt;sup&gt;&lt;a href=&quot;https://newsroom.intel.com/news-releases/intel-building-future-high-performance-computing&quot;&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/th&gt;        &lt;th class=&quot;tg-7btt&quot;&gt;AMD Rome&lt;sup&gt;&lt;a href=&quot;https://www.amd.com/en/products/cpu/amd-epyc-7h12&quot;&gt;[2]&lt;/a&gt;&lt;/sup&gt;&lt;/th&gt;      &lt;/tr&gt;    &lt;/thead&gt;    &lt;tbody&gt;      &lt;tr&gt;        &lt;td class=&quot;tg-l2oz&quot;&gt;&lt;b&gt;Shipping&lt;/b&gt;&lt;/td&gt;        &lt;td class=&quot;tg-baqh&quot;&gt;4Q2020&lt;/td&gt;        &lt;td class=&quot;tg-baqh&quot;&gt;3Q2019&lt;/td&gt;      &lt;/tr&gt;      &lt;tr&gt;        &lt;td class=&quot;tg-9j3s&quot;&gt;&lt;b&gt;Cores&lt;/b&gt;&lt;/td&gt;        &lt;td class=&quot;tg-c3ow&quot;&gt;up to 32&lt;/td&gt;        &lt;td class=&quot;tg-9wq8&quot;&gt;up to 64&lt;/td&gt;      &lt;/tr&gt;      &lt;tr&gt;        &lt;td class=&quot;tg-9j3s&quot;&gt;&lt;b&gt;Memory&lt;/b&gt;&lt;/td&gt;        &lt;td class=&quot;tg-c3ow&quot;&gt;8x DDR4-3200&lt;/td&gt;        &lt;td class=&quot;tg-9wq8&quot;&gt;8x DDR4-3200&lt;/td&gt;      &lt;/tr&gt;      &lt;tr&gt;        &lt;td class=&quot;tg-9j3s&quot;&gt;&lt;b&gt;Host Interface&lt;/b&gt;&lt;/td&gt;        &lt;td class=&quot;tg-c3ow&quot;&gt;?x PCIe Gen4&lt;/td&gt;        &lt;td class=&quot;tg-9wq8&quot;&gt;128x PCIe Gen4&lt;/td&gt;      &lt;/tr&gt;    &lt;/tbody&gt;  &lt;/table&gt;&lt;/div&gt;
&lt;p&gt;By the time Ice Lake starts shipping, AMD will be launching its next-generation Milan server processors, so it's difficult to get excited about Ice Lake if one isn't married to the Intel software ecosystem or doesn't have specific use for the new AVX512 instructions being introduced.&lt;/p&gt;
&lt;p&gt;The Intel software ecosystem is not nothing though, and Intel does seem to remain ahead on that front. Intel had its &lt;a href=&quot;https://www.oneapi.com/events/devcon2020/&quot;&gt;inaugural oneAPI Dev Summit during SC'20&lt;/a&gt;, and although I don't follow the application developer space very closely, my perception of the event is that it focused on showcasing the building community momentum around oneAPI rather than delivering splashy announcements. That said, this oneAPI Dev Summit seems to have sucked the air out of the room for other Intel software-centric events; &lt;a href=&quot;https://www.ixpug.org/events&quot;&gt;IXPUG had no discernible presence at SC'20&lt;/a&gt; despite IXPUG changing its name from &quot;Intel Xeon Phi User Group&quot; to &quot;Intel eXtreme Performance User Group&quot; when Xeon Phi was sunset. However one dev event is better than none; I did not hear of any equivalent events hosted by AMD at SC'20.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;NVIDIA also announced new SKU of its Ampere A100 data center GPU&lt;/b&gt; with a whopping 80 GB of HBM2. This was surprising to me since the A100 with 40 GB of HBM2 was only first unveiled two quarters ago. The A100 chip itself is the same so there's no uptick in flops; they just moved to HBM2e stacks which allowed them to double the capacity and get an incremental increase in memory bandwidth.&lt;/p&gt;
&lt;p&gt;So, who's this part for? Doubling the HBM capacity won't double the price of the GPU, but the A100-80G part will undoubtedly be more expensive despite there being no additional FLOPS. My guess is that this part was released for&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ol style=&quot;text-align: left;&quot;&gt;  &lt;li&gt;People who just want to fit bigger working sets entirely in GPU memory. Larger deep learning models are the first thing that come to my mind.&lt;/li&gt;  &lt;li&gt;People whose applications can't fully utilize A100's flops due to suboptimal memory access patterns; higher HBM2e bandwidth may allow such apps to move a little higher along the roofline.&lt;/li&gt;  &lt;li&gt;People who may want to purchase AMD's next-generation data center GPU (which will undoubtedly also use HBM2e) but probably be released before the follow-on to Ampere is ready.&lt;/li&gt;&lt;/ol&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;NVIDIA also upgraded its Selene supercomputer to include these A100-80G parts, moving its Top500 position to #5 and demonstrating that these parts exist and deliver as advertised.&lt;/p&gt;
&lt;h3 id=&quot;bigsplash-whatsmissing&quot; style=&quot;text-align: left;&quot;&gt;What's missing&lt;/h3&gt;
&lt;p&gt;&lt;b&gt;HPE/Cray was pretty quiet&lt;/b&gt; on announcements, especially after two SCs in a row with Shasta (now &quot;Cray EX&quot;) news. HPE undoubtedly has its head down readying its first large Shasta installations, and given the fact that the primary manufacturing facilities for Cray Shasta are located in a &lt;a href=&quot;https://www.wctrib.com/news/education/6749479-Chippewa-County-has-states-highest-14-day-COVID-19-case-rate-several-area-counties-also-see-increases&quot;&gt;COVID hotspot in the US&lt;/a&gt;, maybe this was to be expected--this autumn has not been the time to rush anything.&lt;/p&gt;
&lt;p&gt;That said, we know that Cray EX systems have been shipping since July 2020:&lt;/p&gt;
&lt;blockquote class=&quot;twitter-tweet&quot;&gt;  &lt;p dir=&quot;ltr&quot; lang=&quot;en&quot;&gt;A wee video for Friday afternoon. Watch the installation of the four-cabinet Shasta Mountain system, the first phase of the &lt;a href=&quot;https://twitter.com/ARCHER2_HPC?ref_src=twsrc%5Etfw&quot;&gt;@ARCHER2_HPC&lt;/a&gt; 23-cabinet system.&lt;a href=&quot;https://t.co/DqYRDJi39B&quot;&gt;https://t.co/DqYRDJi39B&lt;/a&gt;&lt;a href=&quot;https://twitter.com/Cray_Inc?ref_src=twsrc%5Etfw&quot;&gt;@Cray_Inc&lt;/a&gt; &lt;a href=&quot;https://t.co/8D4Hv5Msmt&quot;&gt;pic.twitter.com/8D4Hv5Msmt&lt;/a&gt;&lt;/p&gt;
  — EPCCed (@EPCCed) &lt;a href=&quot;https://twitter.com/EPCCed/status/1289177495304990721?ref_src=twsrc%5Etfw&quot;&gt;July 31, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;So it is a little surprising that HPE was not promoting any early customer or science success stories yet, and the only Cray EX/Shasta system to appear on Top500 was &lt;a href=&quot;https://top500.org/system/179900/&quot;&gt;Alps, a modest 4.6 PF Rome-based system at CSCS&lt;/a&gt;. Next year--either at the &lt;a href=&quot;https://www.isc-hpc.com/press-releases/ISC-2021-will-take-place-virtually-in-the-summer-of-2021.html&quot;&gt;all-virtual ISC'21&lt;/a&gt; or SC'21--will likely be the year of Cray EX.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Intel was also pretty quiet about Aurora&lt;/b&gt;, perhaps for the same reason as HPE/Cray. The fact that Intel's biggest hardware news was around Ice Lake suggests that Intel's focus is on fulfilling the promises of disclosures they made at SC'19 rather than paving new roads ahead. There was a healthy amount of broad-stroke painting about exascale, but aside from the oneAPI buzz I mentioned above, I didn't see anything technically substantive.&lt;/p&gt;
&lt;p&gt;Sadly, &lt;b&gt;IBM was the most quiet&lt;/b&gt;, and it was perhaps the most prominent appearance of IBM in this year's official program was in&lt;a href=&quot;https://www.llnl.gov/news/llnl-ibm-win-sc20-test-time-blue-genel&quot;&gt;winning the Test of Time Award for the Blue Gene/L architecture&lt;/a&gt;. It was almost a eulogy of IBM's once-dominant position at the forefront of cutting-edge HPC research and development, and this feeling was perhaps underscored by the &lt;a href=&quot;https://twitter.com/science_dot/status/1329544810915479553?s=21&quot;&gt;absence of perhaps the most noteworthy IBMer&lt;/a&gt; involved in the creation of Blue Gene. This isn't to say IBM had no presence at SC'20 this year; it's just clear that their focus is on being at the forefront of hybrid cloud and cognitive computing rather than supercomputing for supercomputing's sake.&lt;/p&gt;
&lt;h2 id=&quot;themes&quot; style=&quot;text-align: left;&quot;&gt;High-level Themes&lt;/h2&gt;
&lt;p&gt;The most prevalent theme that I kept running into was not the technology on the horizon, but rather the technology further off. There were a few sessions devoted to things like &quot;Post Moore's Law Devices&quot; and &quot;Exotic Technology&quot; in 2035, and rather than being steeped in deep technical insight, they leaned more towards either &lt;a href=&quot;https://insidehpc.com/2019/02/john-shalf-and-thomas-sterling-to-keynote-isc-2019-in-frankfurt/&quot;&gt;recitations of similar talks&lt;/a&gt;&amp;nbsp;given in years past (&lt;a href=&quot;https://twitter.com/HPC_Guru/status/1329132708593766402?s=20&quot;&gt;one speaker presented slides&lt;/a&gt; that were &lt;a href=&quot;https://www.nextplatform.com/2016/06/24/alchemy-cant-save-moores-law/&quot;&gt;literally five years old&lt;/a&gt;)or outlandish claims that hinged on, in my opinion, incomplete views of how technology evolves.&lt;/p&gt;
&lt;p&gt;I found the latter talks a bit disturbing to find in the SC program since they contained very little technical insight and seemed more focused on entertainment value--the sort of thing usually relegated to post-conference hotel bar conversation. So rather than repeat their predictions as gospel, I'll present my critical take on them. I realize that it's far easier for me to throw stones at people at the top of the hill than to climb there myself, and I'm perfectly willing to accept that my opinions below are completely wrong. And, if you'd like to throw stones at me yourself, I contributed my position to &lt;a href=&quot;https://sc20.supercomputing.org/?post_type=page&amp;amp;p=3479&amp;amp;id=pan116&amp;amp;sess=sess187&quot;&gt;a panel on tiered storage&lt;/a&gt;&amp;nbsp;this year against which all are welcome to argue.&lt;/p&gt;
&lt;h3 id=&quot;comptechfutures&quot; style=&quot;text-align: left;&quot;&gt;Computing Technologies Futures&lt;/h3&gt;
&lt;p&gt;This year's focus on far-flung technologies at SC made me wonder--are these sorts of talks filling out the program because there's no clear path beyond exascale? Is it possible that the HPC community's current focus on climbing the exascale mountain is taking our minds off of the possibility that there's nothing past that mountain except desert?&lt;/p&gt;
&lt;p&gt;For example, Shekhar Borkar gave his five-year outlook on memory technologies:&lt;/p&gt;
&lt;div&gt;  &lt;blockquote class=&quot;twitter-tweet&quot;&gt;    &lt;p dir=&quot;ltr&quot; lang=&quot;en&quot;&gt;Memory Technology Score Card &lt;br /&gt;&lt;br /&gt;According to Shekhar it is SRAM, DRAM, Flash &amp;amp; PCM for the next 5 years.&lt;br /&gt;&lt;br /&gt;Other technologies are OK for research but not ready for prime time yet.&lt;a href=&quot;https://twitter.com/hashtag/SC20?src=hash&amp;amp;ref_src=twsrc%5Etfw&quot;&gt;#SC20&lt;/a&gt; &lt;a href=&quot;https://twitter.com/hashtag/HPC?src=hash&amp;amp;ref_src=twsrc%5Etfw&quot;&gt;#HPC&lt;/a&gt; &lt;a href=&quot;https://twitter.com/hashtag/AI?src=hash&amp;amp;ref_src=twsrc%5Etfw&quot;&gt;#AI&lt;/a&gt; &lt;a href=&quot;https://t.co/q7sjCp2DFH&quot;&gt;pic.twitter.com/q7sjCp2DFH&lt;/a&gt;&lt;/p&gt;
    — HPC Guru (@HPC_Guru) &lt;a href=&quot;https://twitter.com/HPC_Guru/status/1329129023314616320?ref_src=twsrc%5Etfw&quot;&gt;November 18, 2020&lt;/a&gt;  &lt;/blockquote&gt;  &lt;/div&gt;
&lt;p&gt;SRAM and DRAM are decades-old staples in the HPC industry, and even NAND has been used in production HPC for a decade now. The statement that PCM &lt;i&gt;may&lt;/i&gt; be useful in the next five years is quite striking since &lt;a href=&quot;https://arstechnica.com/information-technology/2017/03/intels-first-optane-ssd-375gb-that-you-can-also-use-as-ram/&quot;&gt;PCM products have been shipping in volume since 2017&lt;/a&gt;--from this, I take that the future is going to look an awful lot like the present on the memory and storage front. The biggest change, if any, will likely be the economics of NAND and 3D integration evolving to a point where we can afford more all-flash and all-HBM systems in the coming years.&lt;/p&gt;
&lt;p&gt;On the computational front, many of the soothsayers leaned heavily on using cryogenics for the post-Moore's Law chip designs. Ultra-low-temperature CMOS and superconductors for supercomputers are a low-hanging fruit to pick when conjecturing about the future since (1) their physics are well understood, and (2) they have clear and nonlinear benefits over the CMOS technologies baked into chips today, as shown by Borkar:&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-zuV0y49SHK8/X7nNeszLoAI/AAAAAAABPhc/_iiH0vrRZ6ke0KMwhX6oygdmT38Ubu-hACLcBGAsYHQ/s1884/Screen%2BShot%2B2020-11-21%2Bat%2B18.30.43.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img alt=&quot;The benefits of low-temperature computing according to Shekhar Borkar&quot; border=&quot;0&quot; height=&quot;216&quot; src=&quot;https://1.bp.blogspot.com/-zuV0y49SHK8/X7nNeszLoAI/AAAAAAABPhc/_iiH0vrRZ6ke0KMwhX6oygdmT38Ubu-hACLcBGAsYHQ/w400-h216/Screen%2BShot%2B2020-11-21%2Bat%2B18.30.43.png&quot; title=&quot;The benefits of low-temperature computing according to Shekhar Borkar&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;The benefits of low-temperature computing according to &lt;a href=&quot;https://sc20.supercomputing.org/?post_type=page&amp;amp;p=3479&amp;amp;id=inv105&amp;amp;sess=sess298&quot;&gt;Shekhar Borkar&lt;/a&gt;&lt;/span&gt;&lt;/b&gt;&lt;/div&gt;
&lt;p&gt;The problem, of course, is that you won't ever be able to buy a cryogenic supercomputer unless a company can make enough money selling a cryogenic supercomputer to (1) pay down the non-recurring engineering costs, (2) recuperate the costs of productizing the design, and (3) make enough profit to make the shareholders or venture capitalists underwriting (1) and (2) happy.&lt;/p&gt;
&lt;p&gt;Realize that cryogenics at scale are dangerous and messy--compared to water cooling, there is no municipal supply of liquid helium, and the market for building pumps and piping for cryogenic plumbing is virtually zero compared to water-based plumbing. When you add the fact that the vast majority of data centers--including the hyperscalers who drive much of the data center market--don't want to touch water-cooled infrastructure, the HPC market would have to bear the cost of cryogenic computing at-scale entirely on its own for the foreseeable future.&lt;/p&gt;
&lt;p&gt;That all said, remember that this is just my own personal opinion. For a helpful and mostly objective perspective, &lt;a href=&quot;https://twitter.com/hpc_guru/status/1329129023314616320?s=21&quot;&gt;@HPC_Guru posted a thread that captures the general sentiment of these sessions&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For the sake of entertainment, I'll include some of the more outlandish slides that I saw on this topic.&lt;/p&gt;
&lt;p&gt;Erik DeBenedictis had the following predictions of the future in 2006 for 2020:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://lh3.googleusercontent.com/-3nEnwAHYAuM/X7nRmA9sYPI/AAAAAAABPho/eBlqvifpZhIKBYcZFfhM43Plj7XySdijQCLcBGAsYHQ/Screen%2BShot%2B2020-11-21%2Bat%2B18.48.05.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img alt=&quot;The future of yesterday - a 2006 prediction of what HPC will look like in 2020 by Erik DeBenedictis&quot; height=&quot;226&quot; src=&quot;https://lh3.googleusercontent.com/-3nEnwAHYAuM/X7nRmA9sYPI/AAAAAAABPho/eBlqvifpZhIKBYcZFfhM43Plj7XySdijQCLcBGAsYHQ/w400-h226/Screen%2BShot%2B2020-11-21%2Bat%2B18.48.05.png&quot; title=&quot;The future of yesterday - a 2006 prediction of what HPC will look like in 2020 by Erik DeBenedictis&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;The future of yesterday - a 2006 prediction of what HPC will look like in 2020 by &lt;a href=&quot;https://sc20.supercomputing.org/presentation/?sess=sess188&amp;amp;id=pan106&quot;&gt;Erik DeBenedictis&lt;/a&gt;&lt;/span&gt;&lt;/b&gt;&lt;/div&gt;
&lt;p&gt;DeBenedictis' primary oversight in this prediction was failing to see the end of Dennard scaling due to physics. Had power consumption continued to drop with node size, we could very well be at 20 GHz today, and the fact that his core counts, flops/socket, and system peak were reasonable is a testament to good forecasting. However Dennard scaling is what forced CPUs towards longer vectors (which is how a 40-core socket can still get 1.6 TF without running at 20 GHz) and what motivated the development of the more power-efficient architecture of GPGPUs. DeBenedictis' predictions for the future, though, don't look as reasonable to me:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://lh3.googleusercontent.com/-4723-WwU21I/X7nUFGo0uaI/AAAAAAABPh0/llCQVvmd_4sLtS5ORsUI0UanOxS9mAb_wCLcBGAsYHQ/Screen%2BShot%2B2020-11-21%2Bat%2B18.59.07.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img alt=&quot;The future of HPC is hybrid quantum/classical systems according to DeBenedictis&quot; height=&quot;226&quot; src=&quot;https://lh3.googleusercontent.com/-4723-WwU21I/X7nUFGo0uaI/AAAAAAABPh0/llCQVvmd_4sLtS5ORsUI0UanOxS9mAb_wCLcBGAsYHQ/w400-h226/Screen%2BShot%2B2020-11-21%2Bat%2B18.59.07.png&quot; title=&quot;The future of HPC is hybrid quantum/classical systems according to DeBenedictis&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;The future of HPC is hybrid quantum/classical systems according to &lt;a href=&quot;https://sc20.supercomputing.org/presentation/?sess=sess188&amp;amp;id=pan106&quot;&gt;DeBenedictis&lt;/a&gt;&lt;/span&gt;&lt;/b&gt;&lt;/div&gt;
&lt;p&gt;While quantum/classical hybrid machines may very well exist in 2035, they aren't exactly solving the same problems that today's supercomputers can. In a sense, he chose to make a meta-prediction that science will change to fit the technology available--or perhaps he chose to redefine supercomputing to mean something even more niche than it does today.&lt;/p&gt;
&lt;p&gt;Thomas Sterling also gave his 200 GHz yottaflop prediction:&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-yBR4kFo5hKI/X7nWYm3GamI/AAAAAAABPiI/lAVBcUoqlpUqHlCLCbvMFZ9w1RJNsllxgCLcBGAsYHQ/s2048/Screen%2BShot%2B2020-11-21%2Bat%2B19.08.41.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img alt=&quot;Thomas Sterling's gonzo predictions of HPC in 2035&quot; border=&quot;0&quot; height=&quot;226&quot; src=&quot;https://1.bp.blogspot.com/-yBR4kFo5hKI/X7nWYm3GamI/AAAAAAABPiI/lAVBcUoqlpUqHlCLCbvMFZ9w1RJNsllxgCLcBGAsYHQ/w400-h226/Screen%2BShot%2B2020-11-21%2Bat%2B19.08.41.png&quot; title=&quot;Thomas Sterling's gonzo predictions of HPC in 2035&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;&lt;a href=&quot;https://sc20.supercomputing.org/presentation/?sess=sess188&amp;amp;id=pan106&quot;&gt;Thomas Sterling&lt;/a&gt;'s gonzo predictions of HPC in 2035&lt;/span&gt;&lt;/b&gt;&lt;/div&gt;
&lt;p&gt;which hasn't changed since &lt;a href=&quot;https://twitter.com/glennklockwood/status/1012014953765752832?s=20&quot;&gt;he predicted a superconducting yottaflop at ISC'18&lt;/a&gt;. Unlike DeBenedictis, Sterling chose not to redefine HPC to fit the available technology but instead predict a physical, economical, and practical fantasy about the future.  Not there's anything wrong with that. Everyone's got to have a goal.&lt;/p&gt;
&lt;p&gt;Kathy Yelick offered the most pragmatic 15-year prediction:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://lh3.googleusercontent.com/-LRIzyo-1ku4/X7nX66YUeAI/AAAAAAABPiU/a57rttDmfFQObPLtfPWJHvPIWaRY8LlFACLcBGAsYHQ/Screen%2BShot%2B2020-11-21%2Bat%2B19.15.25.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img alt=&quot;Kathy Yelick's predictions of HPC in 2035&quot; height=&quot;226&quot; src=&quot;https://lh3.googleusercontent.com/-LRIzyo-1ku4/X7nX66YUeAI/AAAAAAABPiU/a57rttDmfFQObPLtfPWJHvPIWaRY8LlFACLcBGAsYHQ/w400-h226/Screen%2BShot%2B2020-11-21%2Bat%2B19.15.25.png&quot; title=&quot;Kathy Yelick's predictions of HPC in 2035&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;&lt;a href=&quot;https://sc20.supercomputing.org/presentation/?sess=sess188&amp;amp;id=pan106&quot;&gt;Kathy Yelick&lt;/a&gt;'s predictions of HPC in 2035&lt;/span&gt;&lt;/b&gt;&lt;/div&gt;
&lt;p&gt;and I can't poke holes in any of these predictions because there is a clear path from today to this vision for the future. That said, if you actually attach flops and hertz to these predictions, the future does not look nearly as exciting as superconducting yottaflops do.&lt;/p&gt;
&lt;p&gt;As dissatisfying as it may be, Shekhar Borkar had a slide that is probably the pathway into the future of HPC:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://lh3.googleusercontent.com/-qX6kB89e_sQ/X7nt9dKqLjI/AAAAAAABPjU/yV52iV_aqvwiHkXXkWDk3DlAdhSn-KcSwCLcBGAsYHQ/Screen%2BShot%2B2020-11-21%2Bat%2B18.14.34.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img alt=&quot;Moore's Law will survive as long as we change what it means&quot; height=&quot;216&quot; src=&quot;https://lh3.googleusercontent.com/-qX6kB89e_sQ/X7nt9dKqLjI/AAAAAAABPjU/yV52iV_aqvwiHkXXkWDk3DlAdhSn-KcSwCLcBGAsYHQ/w400-h216/Screen%2BShot%2B2020-11-21%2Bat%2B18.14.34.png&quot; title=&quot;Moore's Law will survive as long as we change what it means&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;Moore's Law will survive as long as we change what it means according to &lt;a href=&quot;https://sc20.supercomputing.org/?post_type=page&amp;amp;p=3479&amp;amp;id=inv105&amp;amp;sess=sess298&quot;&gt;Borkar&lt;/a&gt;&lt;/span&gt;&lt;/b&gt;&lt;/div&gt;
&lt;p&gt;The only way the future of HPC will be predictable is if you're willing to define what HPC is to fit whatever the available technologies are. Yelick expressed the same sentiment with her &quot;Not sure, but it will be called OpenMP&quot; bullet, and to his credit, Sterling himself did this with his Beowulf cluster. If the market just gives you a pile of parts, strap it together and call it HPC. And if transistor scaling has no more steam, find something that still has legs and call it Moore's Law.&lt;/p&gt;
&lt;h3 id=&quot;storagetechfutures&quot; style=&quot;text-align: left;&quot;&gt;Storage Technologies Futures&lt;/h3&gt;
&lt;p&gt;On the storage front, the predictions from 2006 for 2020 storage technology were pretty reasonable as well. Dr. Mark Kryder (of Kryder's Law fame) predict that Kryder's Law would hold:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://lh3.googleusercontent.com/-BXV0smrShPA/X7ne46LTdfI/AAAAAAABPi8/PHj37Wz1UP47WZOFJWu5Zu_sZ9BjAvYWQCLcBGAsYHQ/Screen%2BShot%2B2020-11-21%2Bat%2B19.44.28.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img alt=&quot;Mark Kryder's vision for HDDs in 2020 as told in 2006&quot; height=&quot;226&quot; src=&quot;https://lh3.googleusercontent.com/-BXV0smrShPA/X7ne46LTdfI/AAAAAAABPi8/PHj37Wz1UP47WZOFJWu5Zu_sZ9BjAvYWQCLcBGAsYHQ/w400-h226/Screen%2BShot%2B2020-11-21%2Bat%2B19.44.28.png&quot; title=&quot;Mark Kryder's vision for HDDs in 2020 as told in 2006&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;&lt;a href=&quot;https://sc20.supercomputing.org/presentation/?sess=sess186&amp;amp;id=pan105&quot;&gt;Mark Kryder&lt;/a&gt;'s vision for HDDs in 2020 as told in 2006&lt;/span&gt;&lt;/b&gt;&lt;/div&gt;
&lt;p&gt;However he mispredicted how it would hold--his assumption was that surface bit density would keep skyrocketing, and this is why his bandwidth number was so far off. Packing magnetic bits ever more closely together turns out to be a very difficult problem, so the hard disk drive industry chose to increase capacities by solving the easier problem of packing more platters into a single 3.5&quot; half-height form factor.&lt;/p&gt;
&lt;p&gt;The flash predictions of Richard Freitas (who passed away in 2016) were also very reasonable:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://lh3.googleusercontent.com/-8HqWsOCKPdo/X7neIKciEKI/AAAAAAABPi0/jInrObVCr_UxJxgQuCFnfboCjnfnNH5MACLcBGAsYHQ/Screen%2BShot%2B2020-11-21%2Bat%2B19.42.20.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img alt=&quot;Predictions for solid-state storage in 2020 from Rich Freitas in 2006&quot; height=&quot;233&quot; src=&quot;https://lh3.googleusercontent.com/-8HqWsOCKPdo/X7neIKciEKI/AAAAAAABPi0/jInrObVCr_UxJxgQuCFnfboCjnfnNH5MACLcBGAsYHQ/w400-h233/Screen%2BShot%2B2020-11-21%2Bat%2B19.42.20.png&quot; title=&quot;Predictions for solid-state storage in 2020 from Rich Freitas in 2006&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;Predictions for solid-state storage in 2020 from &lt;a href=&quot;https://sc20.supercomputing.org/presentation/?sess=sess186&amp;amp;id=pan105&quot;&gt;Rich Freitas&lt;/a&gt; in 2006&lt;/span&gt;&lt;/b&gt;&lt;/div&gt;
&lt;p&gt;His biggest miscalculation was not realizing that solid-state storage would bifurcate into the two tiers we now call RAM and flash. He predicted &quot;storage class memory&quot; based on the assumption that it would be block-based (like flash) but use a simple and low-latency bus (like RAM). We enjoy higher bandwidth and capacity than his prediction due to the increased parallelism and lower cost of NAND SSDs, but relying on PCIe instead of a memory bus and the low endurance of NAND (and therefore significant back-end data management and garbage collection) drove up the latency.&lt;/p&gt;
&lt;p&gt;Predictions for the future were more outlandish. Kryder's prediction for 2035 was a bit too much for me:&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://lh3.googleusercontent.com/-Y7vKyB56q6s/X7nk1xlRpiI/AAAAAAABPjI/PK71S5rkidw8Bljc6kUnNJEQF_h2U7DnwCLcBGAsYHQ/Screen%2BShot%2B2020-11-21%2Bat%2B20.10.59.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img alt=&quot;Kryder's 15-year outlook for HDDs&quot; height=&quot;226&quot; src=&quot;https://lh3.googleusercontent.com/-Y7vKyB56q6s/X7nk1xlRpiI/AAAAAAABPjI/PK71S5rkidw8Bljc6kUnNJEQF_h2U7DnwCLcBGAsYHQ/w400-h226/Screen%2BShot%2B2020-11-21%2Bat%2B20.10.59.png&quot; title=&quot;Kryder's 15-year outlook for HDDs&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;&lt;a href=&quot;https://sc20.supercomputing.org/presentation/?sess=sess186&amp;amp;id=pan105&quot;&gt;Kryder's 15-year outlook for HDDs&lt;/a&gt; with a heaping serving of &quot;oof&quot;&lt;/span&gt;&lt;/b&gt;&lt;/div&gt;
&lt;p&gt;Extrapolating Kryder's Law another 15 years puts us at 1.8 petabytes per hard drive, but this rests on the pretty shaky foundation that there's something holy about hard disk drive technology that will prevent people from pursuing different media. Realizing this requires two things to be true:&lt;/p&gt;
&lt;div&gt;  &lt;ol style=&quot;text-align: left;&quot;&gt;    &lt;li&gt;The HDD industry remains as profitable in the next fifteen years as it is today. Seeing as how parts of the HDD industry are already going extinct due to flash (remember when personal computers had hard drives?) and &lt;a href=&quot;https://blog.westerndigital.com/host-managed-smr-dropbox/&quot;&gt;hyperscale is taking more ownership of drive controller functionality&lt;/a&gt; and eating into manufacturers' margins, I just don't see this as being likely.&lt;/li&gt;    &lt;li&gt;The cost to develop the required recording techniques (two-dimensional magnetic recording and bit-patterned media) is both as fast and as cheap as HAMR was. If it's not, see #1 above--there won't be the money or patience to sustain the HDD market.&lt;/li&gt;  &lt;/ol&gt;&lt;/div&gt;
&lt;p&gt;This doesn't even consider the appeal of dealing with 1.8 PB drives as a system architect; at Kryder's forecasted numbers, it would take eleven days to fill, rebuild, or scrub one of these drives. As a system designer, why would I want this? Surely there are better ways to assemble spindles, motors, actuators, and sheet metal to increase my bandwidth and reduce my blast radius than cramming all these platters into a 3.5&quot; form factor.&lt;/p&gt;
&lt;p&gt;My bet (and note--I was not invited to contribute it, as I am not an expert!) is that the HDD market will continue to slow down as it falls off the Kryder Law curve due to scaling limitations. This will result in a slow but downward spiral where R&amp;amp;D slows because it is starved of funding, and funding is starved because HDDs fall further and further off of the economics curve. HDDs won't be gone by 2035, but they will fit in the small gap between that exists between low-cost write-once-read-many media (like ultra-dense trash flash) and low-cost write-once-read-never media (like tape).&lt;/p&gt;
&lt;p&gt;Kryder essentially acknowledged that his projection relies on something intrinsically special about HDDs; he commented that the technological advancements required to reach 1.8 PB HDDs will happen because HDD engineers don't want to lose their jobs to the flash industry. Personally, I'd take a new job with an exciting future over a gold watch any day of the week. Maybe that's the millennial in me.&lt;/p&gt;
&lt;p&gt;I found this general theme of wildly projecting into the future rather yucky this SC, and I won't miss it if it's gone for another fifteen years. &amp;nbsp;By their very nature, these panels are &lt;i&gt;exclusive&lt;/i&gt;, not inclusive--someone literally has to &lt;i&gt;die&lt;/i&gt; in order for a new perspective to be brought on board. &amp;nbsp;There was an element to this in the Top500 BOF as well, and &lt;a href=&quot;https://twitter.com/glennklockwood/status/1328450538929758208?s=21&quot;&gt;one slide in particular made me cringe&lt;/a&gt; at how such a prominent good-ol-boys club was being held up before the entire SC community. &amp;nbsp;These sorts of events are looking increasingly dated and misrepresentative of the HPC community amidst the backdrop of SC putting diversity front and center.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;h2 id=&quot;actualfutures&quot; style=&quot;text-align: left;&quot;&gt;Actual Future Directions&lt;/h2&gt;
&lt;p&gt;Although wild projections of the future felt like fashionable hot topics of the year, a couple of previous hot topics seemed to be cooling down and transitioning from hype to reality. Two notable trends popped out at me: the long-term relationship between HPC and AI and what disaggregation may really look like.&lt;/p&gt;
&lt;h3 id=&quot;actualfutures-hpcai&quot; style=&quot;text-align: left;&quot;&gt;The Relationship of HPC and AI&lt;/h3&gt;
&lt;p&gt;As has been the norm for a few years now, deep learning (now more broadly &quot;AI&quot;) was peppered across the SC program this year. Unlike previous years, though, the AI buzz seemed to be tempered by a little more pragmatism as if it were coming down the hype curve. Perhaps the best talk that captured this was an invited talk by Cliff Young of Google about the possibility of a&lt;a href=&quot;https://sc20.supercomputing.org/presentation/?id=inv111&amp;amp;sess=sess299&quot;&gt;Virtuous Cycle of HPC and AI&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The &quot;convergence of HPC and AI&quot; has been talked about in the supercomputing community since HPC-focused GPUs were reinvented as an AI accelerator. If you look at who's been selling this line, though, you may realize that the conversation is almost entirely one-way; the HPC industry pines for this convergence. The AI industry, frankly, doesn't seem to care what the HPC industry does because they're too busy monetizing AI and bankrolling the development of the N+1th generation of techniques and hardware to suit &lt;i&gt;their&lt;/i&gt; needs, not those of the HPC industry.&lt;/p&gt;
&lt;p&gt;Dr. Young's talk closed this loop by examining what the AI industry can learn from HPC; the so-called &quot;Cambrian explosion&quot; of accelerators is somewhere near its peak which has resulted in a huge architectural design space to explore:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://lh3.googleusercontent.com/-f_O7zZRTwjs/X7n3n9mIWBI/AAAAAAABPjg/eRZU5bsRimQ4F08dctGqLpG0ODNE1wzrwCLcBGAsYHQ/Screen%2BShot%2B2020-11-18%2Bat%2B11.37.08.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img alt=&quot;How ML can learn from HPC according to Cliff Young&quot; height=&quot;226&quot; src=&quot;https://lh3.googleusercontent.com/-f_O7zZRTwjs/X7n3n9mIWBI/AAAAAAABPjg/eRZU5bsRimQ4F08dctGqLpG0ODNE1wzrwCLcBGAsYHQ/w400-h226/Screen%2BShot%2B2020-11-18%2Bat%2B11.37.08.png&quot; title=&quot;How ML can learn from HPC according to Cliff Young&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;How ML can learn from HPC according to Cliff Young&lt;/span&gt;&lt;/b&gt;&lt;/div&gt;
&lt;p&gt;When cast this way, HPC actually has a lot of experience in driving progress in these areas; the 4x4 systolic array design point has its genesis in the HPC-specific MIC architecture, and the HPC industry drove the productization of the DRAM-backed HBM memory hierarchy implemented by IBM for the Summit and Sierra systems. These HPC-led efforts presumably contributed to Google's ability to bet on much larger array sizes starting with its first-generation TPU.&lt;/p&gt;
&lt;p&gt;In addition, it sounds like training has begun to reach some fundamental limits of data-parallel scalability:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://lh3.googleusercontent.com/-gU7jpzPEiJ0/X7n5NmusV9I/AAAAAAABPjs/oSjkuLcqrlQw4bf0HMyw8yXmglvEWRPowCLcBGAsYHQ/Screen%2BShot%2B2020-11-18%2Bat%2B11.38.54.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img alt=&quot;Limitations being faced by machine learning&quot; height=&quot;226&quot; src=&quot;https://lh3.googleusercontent.com/-gU7jpzPEiJ0/X7n5NmusV9I/AAAAAAABPjs/oSjkuLcqrlQw4bf0HMyw8yXmglvEWRPowCLcBGAsYHQ/w400-h226/Screen%2BShot%2B2020-11-18%2Bat%2B11.38.54.png&quot; title=&quot;Limitations being faced by machine learning&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;Limitations being faced by machine learning&lt;/span&gt;&lt;/b&gt;&lt;/div&gt;
&lt;p&gt;HPC has long dealt with the scalability limitations of allreduce by developing technologies like complex low- and high-radix fabric topologies and hardware offloading of collective operations. Whether the AI industry simply borrows ideas from HPC and implements its own solutions or contributes to existing standards remains to be seen, but standards-based interfaces into custom interconnects like &lt;a href=&quot;https://aws.amazon.com/about-aws/whats-new/2018/11/introducing-elastic-fabric-adapter/&quot;&gt;AWS Elastic Fabric Adapter&lt;/a&gt; are a promising sign.&lt;/p&gt;
&lt;p&gt;Another &quot;hard problem&quot; area in which HPC is ahead is in sparse matrices:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://lh3.googleusercontent.com/-4dJ0BpxDwys/X7n650KJ8EI/AAAAAAABPj4/7KTjJpjXf38AB7Ts1JeVjzi66w1L6S0ugCLcBGAsYHQ/Screen%2BShot%2B2020-11-18%2Bat%2B11.45.24.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img alt=&quot;Impending challenges brought by moving to sparse methods in ML&quot; height=&quot;208&quot; src=&quot;https://lh3.googleusercontent.com/-4dJ0BpxDwys/X7n650KJ8EI/AAAAAAABPj4/7KTjJpjXf38AB7Ts1JeVjzi66w1L6S0ugCLcBGAsYHQ/w400-h208/Screen%2BShot%2B2020-11-18%2Bat%2B11.45.24.png&quot; title=&quot;Impending challenges brought by moving to sparse methods in ML&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;Impending challenges brought by moving to sparse methods in ML&lt;/span&gt;&lt;/b&gt;&lt;/div&gt;
&lt;p&gt;Young's position is that, although &quot;sparse&quot; means different things to AI (50-90% sparse) than it does to HPC (&amp;gt;95% sparse), HPC has shown that there are algorithms that can achieve very high fractions of peak performance on sparse datasets.&lt;/p&gt;
&lt;p&gt;His concluding slide was uplifting in its suggestion that the HPC-AI relationship may not be strictly one-way forever:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://lh3.googleusercontent.com/-DwLW3Y4S89U/X7n8gutH4jI/AAAAAAABPkE/zzgpxV2v9aQ5Q_UixG-o1X-l4-MHfpxSACLcBGAsYHQ/Screen%2BShot%2B2020-11-18%2Bat%2B11.51.26.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img alt=&quot;How HPC and ML can work together to advance technology&quot; height=&quot;226&quot; src=&quot;https://lh3.googleusercontent.com/-DwLW3Y4S89U/X7n8gutH4jI/AAAAAAABPkE/zzgpxV2v9aQ5Q_UixG-o1X-l4-MHfpxSACLcBGAsYHQ/w400-h226/Screen%2BShot%2B2020-11-18%2Bat%2B11.51.26.png&quot; title=&quot;How HPC and ML can work together to advance technology&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;How HPC and ML can work together to advance technology&lt;/span&gt;&lt;/b&gt;&lt;/div&gt;
&lt;p&gt;He specifically called out promise in the use of mixed precision; AI already relies on judicious use of higher-precision floating point to stabilize its heavy use of 16-bit arithmetic, and &lt;a href=&quot;https://sos23.ornl.gov/wp-content/uploads/2019/03/II_6_Dongarra2.pdf&quot;&gt;scientific computing is finding algorithms in which 16-bit precision can be tolerated&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Being more hardware- and infrastructure-minded myself, I was particularly surprised to see this nod to liquid cooling early on:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://lh3.googleusercontent.com/-vmn8uJbdUDI/X7n_1ikQ22I/AAAAAAABPkQ/4wUYGZhPS1I_99OnOW1gup7hDpNdltwZACLcBGAsYHQ/Screen%2BShot%2B2020-11-21%2Bat%2B22.02.14.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img alt=&quot;Liquid cooling in hyperscale - one of few areas in which HPC is ahead&quot; height=&quot;226&quot; src=&quot;https://lh3.googleusercontent.com/-vmn8uJbdUDI/X7n_1ikQ22I/AAAAAAABPkQ/4wUYGZhPS1I_99OnOW1gup7hDpNdltwZACLcBGAsYHQ/w400-h226/Screen%2BShot%2B2020-11-21%2Bat%2B22.02.14.png&quot; title=&quot;Liquid cooling in hyperscale - one of few areas in which HPC is ahead&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;Liquid cooling in hyperscale - one of few areas in which HPC is ahead&lt;/span&gt;&lt;/b&gt;&lt;/div&gt;
&lt;p&gt;Google's TPU v3 was its first foray into direct liquid cooling, a data center technology that HPC has been using for decades (think: &lt;a href=&quot;https://www.computerhistory.org/revolution/supercomputers/10/68/275&quot;&gt;Cray-2's waterfall&lt;/a&gt;). While this may not seem spectacular to any PC enthusiast who's done liquid cooling, the difficulty of scaling these systems up to rack-, row-, and data center-scale are not always linear. Young explicitly acknowledged HPC's expertise in dealing with liquid-cooled infrastructure, and if hyperscale is driven in this direction further, HPC will definitely benefit from the advances that will be enabled by a new and massive market driver.&lt;/p&gt;
&lt;h3 id=&quot;actualfutures-disagg&quot; style=&quot;text-align: left;&quot;&gt;Disaggregation in Practice&lt;/h3&gt;
&lt;p&gt;The promise of disaggregation--having pools of CPU, persistent memory, GPUs, and flash that you can strap together into a a single node--has been around for a long time and had steadily gained attention as a potential candidate for an exascale technology. However I don't think there was a realistic hope for this until IBM's AC922 node--the one that comprises the Summit and Sierra systems--hit the market and demonstrated a unified, hardware-enabled coherent memory space across CPUs and GPUs.&lt;/p&gt;
&lt;p&gt;The actual story there wasn't great though; coherence between CPU and GPU was enabled using NVIDIA's proprietary NVLink protocol while the CPU and NIC were connected via a different coherence protocol, OpenCAPI, over the same physical interface. CCIX and GenZ also emerged as high-speed protocols for cache coherence and disaggregation, and the story only got worse when Intel put forth CXL as its standard for coherence and disaggregation.&lt;/p&gt;
&lt;p&gt;Fortunately, the dust is now settling and it appears that CXL and GenZ are emerging at the front of the pack. There was an amicable panel session where members of these two consortia presented a unified vision for CXL and GenZ which &lt;i&gt;almost&lt;/i&gt; appeared credible: CXL would be the preferred protocol for inside a chassis or rack, and GenZ would be the preferred protocol between chassis and racks. Key features of the finalized CXL 2.0 standard were unveiled which largely revolved around support for &lt;i&gt;CXL switches&lt;/i&gt;:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://lh3.googleusercontent.com/-ERLI8khJdMM/X7oNek5C1RI/AAAAAAABPkk/o2ijd7QmsbQzvnTiCRd8gzzVXQJ9y6MuQCLcBGAsYHQ/Screen%2BShot%2B2020-11-18%2Bat%2B10.10.31.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img alt=&quot;Roles played by CXL 2.0's switch capability&quot; height=&quot;226&quot; src=&quot;https://lh3.googleusercontent.com/-ERLI8khJdMM/X7oNek5C1RI/AAAAAAABPkk/o2ijd7QmsbQzvnTiCRd8gzzVXQJ9y6MuQCLcBGAsYHQ/w400-h226/Screen%2BShot%2B2020-11-18%2Bat%2B10.10.31.png&quot; title=&quot;Roles played by CXL 2.0's switch capability&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;Roles played by CXL 2.0's switch capability from&lt;span style=&quot;text-align: left;&quot;&gt;&lt;a href=&quot;https://sc20.supercomputing.org/presentation/?id=pan117&amp;amp;sess=sess185&quot;&gt;Debendra Das Sharma&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/b&gt;&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;These switches function not only as port expanders to allow many devices to plug into a single host, but also as true switches that enable multi-root complexes that pool hosts and devices to dynamically map devices to hosts using CXL's managed hotplug capability. There's also support for a &lt;i&gt;CXL Fabric Manager&lt;/i&gt; that moderates something that looks a lot like SR-IOV; a single physical device can be diced up and mapped to up to sixteen different hosts. At its surface, this looks like a direct, open-standard competitor to NVLink, NVSwitch, and MIG.&lt;/p&gt;
&lt;p&gt;What these new CXL switches &lt;i&gt;do not&lt;/i&gt; support is inter-switch linking; all CXL devices much share a single switch to maintain the low latency for which CXL was designed. This is where GenZ fits in since it is a true switched fabric, and this is why the &lt;a href=&quot;https://www.businesswire.com/news/home/20200402005187/en/CXL-Consortium™-Gen-Z-Consortium-Announce-MOU-Agreement&quot;&gt;CXL and GenZ consortia have signed a memorandum of understanding (MOU)&lt;/a&gt; to design their protocols towards mutual compatibility and interoperability so that the future of disaggregated systems will be composed of pooled CXL devices bridged by a GenZ fabric. A direct parallel was drawn to PCIe and Ethernet, where a future disaggregated system may see CXL assume the role of PCIe, and GenZ may assume the role currently filled by Ethernet. &lt;/p&gt;
&lt;p&gt;When it came time for Q&amp;amp;A, the panel got more interesting.&lt;/p&gt;
&lt;p&gt;A lot of the audience questions revolved around what standards CXL is planning to wipe off the face of the planet. The Intel (and CXL) panelist, Debendra Das Sharma, fielded the bulk of these questions and made it clear that&lt;/p&gt;
&lt;p&gt;(1) &lt;b&gt;CXL will not replace DDR&lt;/b&gt; as a local memory interface; it is a complementary technology. This sounded a little disingenuous given the following slide was also shown to highlight CXL 1.0's latency being on par with DRAM latency:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://lh3.googleusercontent.com/-6u64pO4Rr_Y/X7oZhQKD-aI/AAAAAAABPkw/AiwWLBV8uXI87soGMA2JT5PcirzQKR_sgCLcBGAsYHQ/Screen%2BShot%2B2020-11-18%2Bat%2B10.12.25.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img alt=&quot;Latency of CXL in the context of storage devices&quot; height=&quot;226&quot; src=&quot;https://lh3.googleusercontent.com/-6u64pO4Rr_Y/X7oZhQKD-aI/AAAAAAABPkw/AiwWLBV8uXI87soGMA2JT5PcirzQKR_sgCLcBGAsYHQ/w400-h226/Screen%2BShot%2B2020-11-18%2Bat%2B10.12.25.png&quot; title=&quot;Latency of CXL in the context of storage devices&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;Latency of CXL in the context of storage devices&lt;/span&gt;&lt;/b&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;(2) &lt;b&gt;CXL will not replace PCIe&lt;/b&gt; as a host I/O interface; is a superset of PCIe and many devices will remain happy with PCIe’s load/store semantics. Of course, this is what I would say too if I had effective control over both the CXL standard and the PCIe SIG. &amp;lt;p&amp;gt;&amp;lt;/p&amp;gt;&lt;/p&gt;
&lt;p&gt;When asked directly if Intel had joined the GenZ consortium though, Sharma gave a terse &quot;no&quot; followed by &quot;no comment&quot; as to why. He then immediately followed that with a very carefully crafted statement:&lt;/p&gt;
&lt;blockquote style=&quot;border: none; margin: 0px 0px 0px 40px; padding: 0px;&quot;&gt;  &lt;p style=&quot;text-align: left;&quot;&gt;&quot;While we have not joined the GenZ consortium, we are fully supportive of making the CXL enhancements that will help GenZ.&quot;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The panelists also commented that the MOU was designed to make transitioning from CXL to GenZ protocols smooth, but when asked exactly how the CXL-to-GenZ bridge would be exposed, Tim Symons (representing Microchip and GenZ) could not offer an answer since this bridging function is still being defined. These sorts of answers left me with the impression that CXL is in the driver's seat and GenZ has been allowed to come along for the ride.&lt;/p&gt;
&lt;p&gt;Reading between the lines further, there was a striking absence of HPE people on the panel given the fact that &lt;a href=&quot;https://www.nextplatform.com/2019/09/09/inside-hpes-gen-z-switch-fabric/&quot;&gt;GenZ originated within HPE's &quot;The Machine&quot; project&lt;/a&gt;. It remains unclear where GenZ fits now that HPE owns Slingshot, a different high-performance scale-out switched fabric technology. What would be the benefit of having a three-tier Slingshot-GenZ-CXL fabric? If CXL 2.0 adopted a single-hop switch and fabric manager, what's to stop CXL 3.0 from expanding its scope to a higher radix or multi-hop switch that can sensibly interface directly with Slingshot?&lt;/p&gt;
&lt;p&gt;Given that CXL has already eaten a part of GenZ's lunch by obviating the need for GenZ host interfaces, I wouldn't be surprised if GenZ eventually meets the same fate as The Machine and gets cannibalized for parts that get split between future versions of Slingshot and CXL. CXL has already effectively killed CCIX, and IBM's decision to join CXL suggests that it may be positioning to &lt;a href=&quot;https://www.nextplatform.com/2020/09/03/the-memory-area-network-at-the-heart-of-ibms-power10/&quot;&gt;merge OpenCAPI's differentiators into CXL after Power10&lt;/a&gt;. This is pure speculation on my part though.&lt;/p&gt;
&lt;h2 id=&quot;ssug&quot; style=&quot;text-align: left;&quot;&gt;Spectrum Scale User Group vs. Lustre BOF&lt;/h2&gt;
&lt;p&gt;Because SC'20 was smeared over two weeks instead of one, I got to attend both the Lustre BOF and one of the Spectrum Scale User Group (SSUG) sessions. I also came equipped with a much more meaningful technical understanding of Spectrum Scale this year (I've spend the last year managing a group responsible for Spectrum Scale at work), and it was quite fascinating to contrast the two events and their communities' respective priorities and interests.&lt;/p&gt;
&lt;p&gt;The Spectrum Scale User Group featured a presentation on &quot;&lt;a href=&quot;https://www.spectrumscaleug.org/wp-content/uploads/2020/11/episode-11-what-is-new-in-5-1.pdf&quot;&gt;What is new in Spectrum Scale 5.1.0&lt;/a&gt;&quot; and the &lt;a href=&quot;https://sc20.supercomputing.org/presentation/?sess=sess316&amp;amp;id=bof149&quot;&gt;Lustre BOF&lt;/a&gt; had its analogous Feature Discussion. I broadly bucketize the new features presented at both events into four categories:&lt;/p&gt;
&lt;h3 id=&quot;ssug-1&quot; style=&quot;text-align: left;&quot;&gt;1. Enterprisey features that organizations may care about&lt;/h3&gt;
&lt;p&gt;For Spectrum Scale, this included support for newer releases of RHEL, SLES, Ubuntu, AIX(!), and Windows (!!). IBM also noted that Spectrum Scale also now supports the &lt;a href=&quot;https://www.ibm.com/downloads/cas/AM1PYZBB&quot;&gt;zEDC hardware compression unit on the z15 mainframe processor&lt;/a&gt;:&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://lh3.googleusercontent.com/-g04tb8_FLa0/X7qwLXzgx6I/AAAAAAABPlM/8xK525c4h7UZ_5lX0jrIGSHuW2a8zyGQQCLcBGAsYHQ/Spectrum%2BScale%2B5.1%2BPlatform%2BUpdates.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img alt=&quot;https://www.spectrumscaleug.org/wp-content/uploads/2020/11/episode-11-what-is-new-in-5-1.pdf&quot; height=&quot;226&quot; src=&quot;https://lh3.googleusercontent.com/-g04tb8_FLa0/X7qwLXzgx6I/AAAAAAABPlM/8xK525c4h7UZ_5lX0jrIGSHuW2a8zyGQQCLcBGAsYHQ/w400-h226/Spectrum%2BScale%2B5.1%2BPlatform%2BUpdates.png&quot; title=&quot;https://www.spectrumscaleug.org/wp-content/uploads/2020/11/episode-11-what-is-new-in-5-1.pdf&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;Spectrum Scale 5.1 platform updates&lt;/span&gt;&lt;/b&gt;&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;The Lustre discussion presented their equivalent OS support slide with a similar set of supported enterprise Linux distributions (RHEL, SLES, Ubuntu). No support for AIX or Z (s390x) though:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://lh3.googleusercontent.com/-e7LAXSxdS8Y/X7qx0Ta_1NI/AAAAAAABPlY/JA4x86Z3vbk55oIafOmeOPDSyTZV8EAbACLcBGAsYHQ/Screen%2BShot%2B2020-11-22%2Bat%2B10.45.30.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;225&quot; src=&quot;https://lh3.googleusercontent.com/-e7LAXSxdS8Y/X7qx0Ta_1NI/AAAAAAABPlY/JA4x86Z3vbk55oIafOmeOPDSyTZV8EAbACLcBGAsYHQ/w400-h225/Screen%2BShot%2B2020-11-22%2Bat%2B10.45.30.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;Lustre 2.14 platform updates&lt;/span&gt;&lt;/b&gt;&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;If nothing else, this was a reminder to me that the market for Spectrum Scale is a bit broader than just HPC like Lustre is. I have to assume they have enough AIX, Windows, and Z customers to justify the their support for those platforms. That said, wacky features like hardware-assisted compression is not unique to Spectrum Scale on Z; &lt;a href=&quot;https://www.eofs.eu/_media/events/lad17/14_andreas_dilger_lad2017-zfs_improvements.pdf&quot;&gt;Lustre picked up hardware-assisted compression&lt;/a&gt; back in 2017 thanks to Intel.&lt;/p&gt;
&lt;p&gt;New improvements to Spectrum Scale's security posture were also presented that were a little alarming to me. For example, one no longer has to add &lt;span style=&quot;font-family: courier;&quot;&gt;scp&lt;/span&gt; and &lt;span style=&quot;font-family: courier;&quot;&gt;echo&lt;/span&gt; to the sudoers file for Spectrum Scale to work (yikes!). There was also a very harsh question from the audience to the effect of &quot;why are there suddenly so many security fixes being issued by IBM?&quot; and the answer was similarly frightening; Spectrum Scale is now entering markets with stringent security demands which has increased IBM's internal security audit requirements, and a lot of new vulnerabilities are being discovered because of this.&lt;/p&gt;
&lt;p&gt;It's ultimately a good thing that Spectrum Scale is finding a fixing a bunch of security problems, since the prior state of the practice was just not performing stringent audits. I assume that Lustre's approach to security audits is closer to where Spectrum Scale was in years past, and should Lustre ever enter these &quot;new markets&quot; to compete with Spectrum Scale, I expect a similarly uncomfortable quantity of security notices would come to light. This is all speculative though; the only definite is that IBM is moving GPFS towards role-based access control which is a positive direction.&lt;/p&gt;
&lt;p&gt;Overall, Spectrum Scale seemed considerably more focused on developing these enterprisey features than Lustre.&lt;/p&gt;
&lt;h3 id=&quot;ssug-2&quot; style=&quot;text-align: left;&quot;&gt;2. Manageability features that administrators may care about&lt;/h3&gt;
&lt;p&gt;Spectrum Scale also revealed a bunch of smaller features that are nice to have for administrators including&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul style=&quot;text-align: left;&quot;&gt;  &lt;li&gt;&lt;b&gt;Faster failing of hung RDMA requests&lt;/b&gt; - you can now set a maximum time that an RDMA request can hang (e.g., if an endpoint fails) before its thread is killed by Spectrum Scale itself. This avoids having to wait for lower-level timeouts and seems like a nice-to-have knob for a file system that supports a lot of path and endpoint diversity. Lustre may be ahead on this front with its &lt;a href=&quot;https://wiki.whamcloud.com/display/LNet/LNet+Health+User+Documentation#LNetHealthUserDocumentation-lnet_transaction_timeout&quot;&gt;lnet_transaction_timeout parameter&lt;/a&gt;, but it's unclear exactly how these two settings differ.&lt;/li&gt;  &lt;li&gt;&lt;b&gt;Safeguards against administrator error&lt;/b&gt; - Spectrum Scale added features that warn the administrator about doing something that may be a mistake, such as accidentally breaking quorum by downing a node or mapping incorrect drive slots to RAID groups. There's not really equivalent functionality in Lustre; these are the places where Lustre solution providers (think HPE/Cray ClusterStor) get to value-add management software on top of open-source Lustre (think &lt;a href=&quot;https://pubs.cray.com/bundle/ClusterStor_CSCLI_Command_Reference_Guide_42_S9922/page/About_CSCLI_Command_Reference_Guide_E1000.html&quot;&gt;cscli&lt;/a&gt;)&lt;/li&gt;  &lt;li&gt;&lt;b&gt;GUI and REST API changes&lt;/b&gt; - you can do an increasing amount of management operations using the Spectrum Scale GUI or its underlying control-plane REST API. Lustre has the &lt;a href=&quot;https://wiki.lustre.org/Integrated_Manager_for_Lustre&quot;&gt;IML GUI&lt;/a&gt;, but it isn't treated as a first-class citizen in the same way that Spectrum Scale does and it was not mentioned at the Lustre BOF at all. Again, this is an area where vendors usually value-add their own management on top of community Lustre.&lt;/li&gt;  &lt;li&gt;&lt;b&gt;Improved monitoring, reporting, and phone-home&lt;/b&gt; - a framework called &quot;MAPS&quot; was recently introduced to essentially do what Nagios does in most DIY environments--raise alarms for crashes, resource exhaustion, misconfiguration, and the like. It also does performance monitoring and historical data aggregation. As with the other manageability features mentioned, Lustre relies on third-party tools for these features.&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;For resilience, Spectrum Scale announced new tunable parameters to improve parallel journal recovery:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://lh3.googleusercontent.com/-Jipe0aQYLmA/X7sgoE-yvKI/AAAAAAABPnY/JHUiSL5s0V8V4eUaMVlMJ3QedqsAdFqugCLcBGAsYHQ/Screen%2BShot%2B2020-11-22%2Bat%2B18.38.06.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img alt=&quot;Spectrum Scale's latest advancements in improving recovery performance&quot; height=&quot;226&quot; src=&quot;https://lh3.googleusercontent.com/-Jipe0aQYLmA/X7sgoE-yvKI/AAAAAAABPnY/JHUiSL5s0V8V4eUaMVlMJ3QedqsAdFqugCLcBGAsYHQ/w400-h226/Screen%2BShot%2B2020-11-22%2Bat%2B18.38.06.png&quot; title=&quot;Spectrum Scale's latest advancements in improving recovery performance&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;Spectrum Scale's latest advancements in improving recovery performance&lt;/span&gt;&lt;/b&gt;&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;whereas Lustre announced parallel fsck with major performance improvements to speed up recovery:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://lh3.googleusercontent.com/-kYUWJhID2Fo/X7rTdizIz2I/AAAAAAABPmM/6_seKzcajo4LR7wg6peBfLgAawE8STESQCLcBGAsYHQ/Screen%2BShot%2B2020-11-22%2Bat%2B13.09.03.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img alt=&quot;Lustre's latest advancements in improving recovery performance&quot; height=&quot;225&quot; src=&quot;https://lh3.googleusercontent.com/-kYUWJhID2Fo/X7rTdizIz2I/AAAAAAABPmM/6_seKzcajo4LR7wg6peBfLgAawE8STESQCLcBGAsYHQ/w400-h225/Screen%2BShot%2B2020-11-22%2Bat%2B13.09.03.png&quot; title=&quot;Lustre's latest advancements in improving recovery performance&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;Lustre's latest advancements in improving recovery performance&lt;/span&gt;&lt;/b&gt;&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;Finally, Spectrum Scale showcased its vision to allow Spectrum Scale to be mounted inside containerized environments:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://lh3.googleusercontent.com/-V_E2gaE1ukA/X7q2JeDMkUI/AAAAAAABPlk/h6wwdezVob8M3-EsjB8XhGkRZQj6i-UhACLcBGAsYHQ/Container%2Bnative%2Bstorage%2Baccess%2B-%2Bcoming%2Bsoon.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img alt=&quot;Spectrum Scale vision for containerized application access&quot; height=&quot;226&quot; src=&quot;https://lh3.googleusercontent.com/-V_E2gaE1ukA/X7q2JeDMkUI/AAAAAAABPlk/h6wwdezVob8M3-EsjB8XhGkRZQj6i-UhACLcBGAsYHQ/w400-h226/Container%2Bnative%2Bstorage%2Baccess%2B-%2Bcoming%2Bsoon.png&quot; title=&quot;Spectrum Scale vision for containerized application access&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;b&gt;The Spectrum Scale vision for containerized application access&lt;/b&gt;&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;This is actually somewhere that Lustre is quite a bit ahead in some regards because it has long had features like &lt;a href=&quot;https://wiki.lustre.org/UID/GID_Mapping&quot;&gt;UID/GID mapping&lt;/a&gt; and &lt;a href=&quot;https://www.ddn.com/blog/technology-innovation/leveraging-isolation-lustre-file-systems/&quot;&gt;subdirectory mounts&lt;/a&gt; that allow for a greater degree of isolation that maps well to untrusted containers.&lt;/p&gt;
&lt;p&gt;That all said, Lustre's focus is not on taking on more of these nice-to-have manageability features. When asked about adding basic manageability features like supporting easy addition/removal of Lustre OSTs and OSSes to enable evergreen Lustre systems analogous to Spectrum Scale's &lt;span style=&quot;font-family: courier;&quot;&gt;mmrestripefs&lt;/span&gt; command, the answer was effectively &quot;no.&quot; The reason given is that (1) Lustre clients are where files get stitched together, so migration will always have to involve client access, and (2) &lt;span style=&quot;font-family: courier;&quot;&gt;lfs find&lt;/span&gt; and &lt;span style=&quot;font-family: courier;&quot;&gt;lfs migrate&lt;/span&gt; already provide the tools necessary to move data files in theory. From this, I take away that stitching those two &lt;span style=&quot;font-family: courier;&quot;&gt;lfs&lt;/span&gt; commands together into a tool that actually does what &lt;span style=&quot;font-family: courier;&quot;&gt;mmfsrestripe&lt;/span&gt; does is an exercise left to the viewer--or a company who can value-add such a tool on top of their Lustre offering.&lt;/p&gt;
&lt;h3 id=&quot;ssug-3&quot; style=&quot;text-align: left;&quot;&gt;3. Performance, scalability, and reliability features that end users may care about&lt;/h3&gt;
&lt;p&gt;Spectrum Scale didn't have a huge amount to offer in the user-facing performance/scalability/reliability features this year. They improved their support for QOS (which is admittedly fantastic when compared to &lt;a href=&quot;https://doc.lustre.org/lustre_manual.xhtml#dbdoclet.tbftuning&quot;&gt;Lustre's Token Bucket Filter QOS&lt;/a&gt;which cannot limit IOPS like Spectrum Scale can) from an administrator standpoint, and they have begun to think about how to incorporate TRIMming into flash-based Spectrum Scale deployments to offer reliable performance.&lt;/p&gt;
&lt;p&gt;By comparison, Lustre's new features really shine in this department. Andreas Dilger presented this slide near the beginning of his talk:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://lh3.googleusercontent.com/-7n7BUOKsHUY/X7rW7UtjrRI/AAAAAAABPmc/-xhGvLpzb9oZCXtRBRPEw6NfyxUZ82FWQCLcBGAsYHQ/Screen%2BShot%2B2020-11-22%2Bat%2B11.30.36.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img alt=&quot;Some of Lustre's many upcoming performance improvements&quot; height=&quot;226&quot; src=&quot;https://lh3.googleusercontent.com/-7n7BUOKsHUY/X7rW7UtjrRI/AAAAAAABPmc/-xhGvLpzb9oZCXtRBRPEw6NfyxUZ82FWQCLcBGAsYHQ/w400-h226/Screen%2BShot%2B2020-11-22%2Bat%2B11.30.36.png&quot; title=&quot;Some of Lustre's many upcoming performance improvements&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;Some of Lustre's many upcoming performance improvements&lt;/span&gt;&lt;/b&gt;&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;which reflects significant attention being paid to improving the performance of emerging noncontiguous and otherwise adversarial I/O pattern--perhaps motivated by storage-hungry AI and genomics markets.&lt;/p&gt;
&lt;p&gt;Lustre is also introducing features aimed at both scale-up and scale-out, with a 30x speedup in the time it takes to &lt;a href=&quot;https://jira.whamcloud.com/browse/LU-12988&quot;&gt;mount petabyte OSTs&lt;/a&gt; (likely in preparation for the &lt;a href=&quot;https://www.globenewswire.com/news-release/2019/06/17/1869364/0/en/Cray-to-Deliver-First-Exabyte-Storage-System-for-the-Frontier-Exascale-System-at-ORNL.html&quot;&gt;exascale Lustre installations coming in the next year or two&lt;/a&gt;) and automated directory metadata &lt;a href=&quot;https://jira.whamcloud.com/browse/LU-11025&quot;&gt;sharding&lt;/a&gt;, &lt;a href=&quot;https://jira.whamcloud.com/browse/LU-12051&quot;&gt;shrinking&lt;/a&gt;, and &lt;a href=&quot;https://jira.whamcloud.com/browse/LU-12624&quot;&gt;balancing&lt;/a&gt;. From this, it's clear that the primary focus of Lustre continues to be extreme scale and performance above all else, but it's unclear how much of this effort is putting Lustre ahead of Spectrum Scale as much as it is catching up to all the effort that went into making Spectrum Scale scale out to 250 PB for the Summit system.&lt;/p&gt;
&lt;h3 id=&quot;ssug-4&quot; style=&quot;text-align: left;&quot;&gt;4. Interface features that platform developers may care about&lt;/h3&gt;
&lt;p&gt;The newest release of Spectrum Scale introduces improvements to NFS (by adding v4.1 support), CSI (incremental improvements), SMB (incremental improvements), and most surprising to me, HDFS. By comparison, I don't think Lustre directly supports any of these interfaces--you have to use third-party software to expose these protocols--and if they are supported, they aren't under active development.&lt;/p&gt;
&lt;h3 id=&quot;ssug-overall&quot; style=&quot;text-align: left;&quot;&gt;Overall Impressions&lt;/h3&gt;
&lt;p&gt;These two presentations pointed to a sharp contrast between how Spectrum Scale and Lustre position themselves as storage systems; IBM's vision for Spectrum Scale is as a high-capacity data lake tier against which a diversity of apps (HPC, containerized services, map-reduce-style analytics) can consume and product data. They even said as much while talking about their HDFS support:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://lh3.googleusercontent.com/-DUmUI2S33cE/X7q2gcuGz-I/AAAAAAABPls/rtIMHLuNukY6pusiBZj4DC65OBAi4kzHwCLcBGAsYHQ/Screen%2BShot%2B2020-11-22%2Bat%2B11.05.34.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img alt=&quot;Spectrum Scale's vision as a hub for all data in the enterprise&quot; height=&quot;226&quot; src=&quot;https://lh3.googleusercontent.com/-DUmUI2S33cE/X7q2gcuGz-I/AAAAAAABPls/rtIMHLuNukY6pusiBZj4DC65OBAi4kzHwCLcBGAsYHQ/w400-h226/Screen%2BShot%2B2020-11-22%2Bat%2B11.05.34.png&quot; title=&quot;Spectrum Scale's vision as a hub for all data in the enterprise&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;Spectrum Scale's vision as a hub for all data in the enterprise&lt;/span&gt;&lt;/b&gt;&lt;/div&gt;
&lt;p&gt;Spectrum Scale AFM improvements were also touted at the user group presentation as a means to enable workflows that span on-premise and public cloud for workloads involving HPC, containerized services, file, and object--no matter where you operate, Spectrum Scale will be there. They showed this logo soup diagram which spoke to this:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://lh3.googleusercontent.com/-xwCG993aFH8/X7q37HCe9sI/AAAAAAABPl4/NNdT8W0oK_8ZKeoSp0jmkqaWSIBiXCeeQCLcBGAsYHQ/Screen%2BShot%2B2020-11-22%2Bat%2B11.11.29.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img alt=&quot;Spectrum Scale logo soup supporting complex workflows and hybrid cloud&quot; height=&quot;226&quot; src=&quot;https://lh3.googleusercontent.com/-xwCG993aFH8/X7q37HCe9sI/AAAAAAABPl4/NNdT8W0oK_8ZKeoSp0jmkqaWSIBiXCeeQCLcBGAsYHQ/w400-h226/Screen%2BShot%2B2020-11-22%2Bat%2B11.11.29.png&quot; title=&quot;Spectrum Scale logo soup supporting complex workflows and hybrid cloud&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;Spectrum Scale logo soup supporting complex workflows and hybrid cloud&lt;/span&gt;&lt;/b&gt;&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;and it's clearly aligned with IBM's hybrid cloud corporate strategy. I can see how this vision could be useful based on my experience in industry, but at the same time, this looks like a Rube Goldberg machine with a lot of IBM-specific linchpins that concentrates risk on IBM product support (and licensing costs!) progressing predictably.&lt;/p&gt;
&lt;p&gt;Lustre, by comparison, appears to be focused squarely on performance and scale. There was no logo soup or architectural vision presented at the Lustre BOF itself.  This is likely a deliberate effort by the Lustre community to focus on being an open-source piece to a larger puzzle that others can package up by anyone with the need or business acumen to do so. Just as Linux itself is just a community effort around which companies like Red Hat (IBM) or SUSE build and market a solution, Lustre should be just one part of an organization's overall data management strategy whereas Spectrum Scale is trying to be the entire answer.&lt;/p&gt;
&lt;p&gt;This isn't a value judgment for or against either; Lustre offers more architectural flexibility at the cost of having to do a lot of day-to-day lifting and large-scale architectural design oneself, while Spectrum Scale is a one-stop shop that likely requires fewer FTEs and engineering effort to build infrastructure for complex workflows. The tradeoff, of course, is that Spectrum Scale and its surrounding ecosystem is priced for enterprises, and absent a new pricing scheme that economically scales cost with capacity (hypothetically referred to as &quot;data lake pricing&quot; at the SSUG), the choice of whether to buy into Spectrum Scale or Lustre as a part of a larger data strategy may come down to how expensive your FTEs are.&lt;/p&gt;
&lt;p&gt;On a non-technical note, the Lustre BOF certainly felt more community-oriented than the Spectrum Scale UG; the dialog was more collegial and there were no undertones of &quot;customers&quot; demanding answers from &quot;vendors.&quot; This is not to say that the SSUG wasn't distinctly more friendly than a traditional briefing; it just felt a bit more IBM-controlled since it was on an IBM WebEx whose registration was moderated by IBM and where all the speakers and question answerers were IBM employees. Perhaps there's no other way in a proprietary product since the vendor ultimately holds the keys to the kingdom.&lt;/p&gt;
&lt;h2 id=&quot;io500&quot; style=&quot;text-align: left;&quot;&gt;IO-500 BOF&lt;/h2&gt;
&lt;p&gt;The IO-500 BOF is one of my favorite events at both ISC and SC each year, but as with the rest of SC'20, this year's IO-500 BOF felt like a quiet affair. I noticed two noteworthy themes:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ol style=&quot;text-align: left;&quot;&gt;  &lt;li&gt;&lt;b&gt;I/O performance is being awarded in dimensions beyond just peak I/O bandwidth&lt;/b&gt;. There are six awards now being given for first place: 10-node bandwidth, 10-node metadata, 10-node overall, total bandwidth, total metadata, and total overall. This contrasts with Top500 which treats performance in a single dimension (peak HPL) and implicitly perpetuates the position that HPL performance is the only aspect of performance that defines &quot;#1.&quot; I quite like the IO-500 approach because it makes it easier to see a multidimensional picture of I/O performance and apply your own value system to the list to decide what combination of hardware and storage system software qualifies as #1.&lt;/li&gt;  &lt;li&gt;&lt;b&gt;The importance of system configuration is elevating in the IO-500 community&lt;/b&gt;--defining a system hardware schema, presenting the data uniformly, and establishing standard tools and techniques for collecting this data from the systems running the IO500 benchmark are all on the roadmap for the IO-500 benchmark. Again, this makes the list much more valuable for the purposes of &lt;i&gt;learning&lt;/i&gt; something since a properly annotated set of submissions would allow you to understand the effects of, for example, choosing NVMe over SAS SSDs or declustered parity over RAID6 on nonvolatile media.&lt;/li&gt;&lt;/ol&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;The &lt;a href=&quot;https://io500.org/site/submissions/full/sc20&quot;&gt;final IO-500 list for SC'20&lt;/a&gt; itself didn't change much this time; experimental and proof-of-concept file systems remain dominant in the top 10 positions, and DAOS, WekaFS, and IME carry most of the weight. However the #1 position &lt;i&gt;was&lt;/i&gt; a surprise:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://lh3.googleusercontent.com/-cTwVFaPx2Ls/X7sDjKJv9AI/AAAAAAABPnA/NKiMql4zqAQ46XnyF8BLn85YZCEsSSqHQCLcBGAsYHQ/Screen%2BShot%2B2020-11-22%2Bat%2B15.23.25.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img alt=&quot;Overall winner for the IO-500 full list was Pengcheng Laboratory's MadFS&quot; height=&quot;226&quot; src=&quot;https://lh3.googleusercontent.com/-cTwVFaPx2Ls/X7sDjKJv9AI/AAAAAAABPnA/NKiMql4zqAQ46XnyF8BLn85YZCEsSSqHQCLcBGAsYHQ/w400-h226/Screen%2BShot%2B2020-11-22%2Bat%2B15.23.25.png&quot; title=&quot;Overall winner for the IO-500 full list was Pengcheng Laboratory's MadFS&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;The overall winner for the IO-500 full list was Pengcheng Laboratory's MadFS&lt;/span&gt;&lt;/b&gt;&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;A new file system called &quot;MadFS&quot; took the top spot with some ridiculous performance numbers, and frustratingly, there have been no public disclosures about what this file system is or how it works. The IO-500 committee said that they spoke privately with the submitters and felt comfortable that the entry was legitimate, but they were not at liberty to disclose many details since Pengcheng Laboratory is preparing to present MadFS at another venue. They did hint that MadFS drew inspiration from DAOS, but they didn't offer much more.&lt;/p&gt;
&lt;p&gt;Peeling the MadFS submission apart does reveal a few things:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul style=&quot;text-align: left;&quot;&gt;  &lt;li&gt;It is a file system attached to &lt;a href=&quot;https://www.globaltimes.cn/content/1171676.shtml&quot;&gt;Pengcheng Laboratory's Cloudbrain-II system&lt;/a&gt;, which is a &lt;a href=&quot;https://e.huawei.com/us/products/cloud-computing-dc/atlas/atlas-900-ai&quot;&gt;Huawei Atlas 900&lt;/a&gt; supercomputer packed with &lt;a href=&quot;https://en.wikichip.org/wiki/hisilicon/kunpeng/920-6426&quot;&gt;Huawei Kungpeng 920 ARM CPUs&lt;/a&gt; and&amp;nbsp;&lt;a href=&quot;https://www.hotchips.org/hc31/HC31_1.11_Huawei.Davinci.HengLiao_v4.0.pdf&quot;&gt;Huawei Ascend 910 coprocessors&lt;/a&gt;. Cloudbrain-II is a huge system with a huge budget, so it should have a very capable storage subsystem.&lt;/li&gt;  &lt;li&gt;72 processes were run on each of the 255 client nodes, reaching a peak of2,209,496 MiB/second. This translates to 73 Gbit/sec out of each 100 Gb/s node--pretty darned efficient.&lt;/li&gt;  &lt;li&gt;The MadFS file system used is 9.6 PB in size, and the fastest-running tests (ior-easy-*) ran for a little over six minutes. This corresponds to863 TB read and written in the best case, which is reasonable.&lt;/li&gt;  &lt;li&gt;The ior-easy tests were run using a transfer size of2,350,400 bytes which is a &lt;i&gt;really&lt;/i&gt; weird optimization point. Thus, it's unlikely that MadFS is block-based; it probably runs entirely in DRAM or HBM, is log-structured, and/or relies on persistent memory to buffer byte-granular I/O from any underlying block devices.&lt;/li&gt;  &lt;li&gt;The submission indicates that 254 metadata nodes were used, and each node had six storage devices. The submission also says that data servers (of an undefined quantity) has 2 TB NVMe drives.&lt;/li&gt;  &lt;ul&gt;    &lt;li&gt;Since 255 clients and 254 metadata servers were used, this may suggest that metadata is federated out to the client nodes. This would explain why the metadata rates are so astonishing.&lt;/li&gt;    &lt;li&gt;If the 9.6 PB of NVMe for data was located entirely on the 255 clients, this means each compute node would've had to have had over 37 TB of NVMe after parity. This seems unlikely.&lt;/li&gt;    &lt;li&gt;From this, we might guess that MadFS stores metadata locally but data remotely. This would be a very fragile architecture for important data, but a reasonable one for ephemeral storage akin to &lt;a href=&quot;https://unifyfs.readthedocs.io/en/latest/&quot;&gt;UnifyFS&lt;/a&gt;.&lt;/li&gt;  &lt;/ul&gt;  &lt;li&gt;MadFS is not ready for prime time, as its &lt;span style=&quot;font-family: courier;&quot;&gt;statfs(2)&lt;/span&gt; returns nonsense data. For example, the MadFS ior-easy-* runs report the file system has zero inodes, while the ior-hard-* runs reported268 trillion inodes all of which are used.&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;Until more disclosures are made about MadFS and the Cloudbrain-II system though, there's little intellectual value in this IO-500 submission. However the waters are definitely chummed, and I for one will be keeping an eye out for news about this Chinese system.&lt;/p&gt;
&lt;p&gt;Finally, although not part of the IO-500 BOF, Microsoft Azure released some benchmark results shortly after about their successful demonstration of &lt;a href=&quot;https://www.hpcwire.com/off-the-wire/azure-hpc-reports-1-tb-s-cloud-parallel-filesystem/&quot;&gt;over 1 TB/sec using BeeGFS in Azure&lt;/a&gt;. This wasn't run to the IO-500 spec so it wouldn't have been a valid submission, but it is the single fastest IOR run in the cloud of which I am aware. This bodes well for the future of parallel file systems in the cloud as a blessed BeeGFS/Azure configuration would compete directly with &lt;a href=&quot;https://aws.amazon.com/fsx/lustre/&quot;&gt;Amazon FSx for Lustre&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;conclusion&quot; style=&quot;text-align: left;&quot;&gt;Concluding Thoughts&lt;/h2&gt;
&lt;p&gt;Virtual SC this year turned out to be far more exhausting than I had anticipated despite the fact that I never had to leave my chair. On the upside, I got to attend SC with my cat for the first time:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://lh3.googleusercontent.com/-i1kSsZQhShg/X7sSEEW7DMI/AAAAAAABPnM/7p5U0Fkkb6sOI3fwjF4VCbzg__06L2hNACLcBGAsYHQ/IMG_0446.JPG&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img alt=&quot;Harriet dialing into the Women in HPC Workshop&quot; height=&quot;300&quot; src=&quot;https://lh3.googleusercontent.com/-i1kSsZQhShg/X7sSEEW7DMI/AAAAAAABPnM/7p5U0Fkkb6sOI3fwjF4VCbzg__06L2hNACLcBGAsYHQ/w400-h300/IMG_0446.JPG&quot; title=&quot;Harriet dialing into the Women in HPC Workshop&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;Harriet dialing into the Women in HPC Workshop with me&lt;/span&gt;&lt;/b&gt;&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;and I didn't find myself getting as sweaty running between sessions. On the downside, the whole conference was just &lt;i&gt;weird&lt;/i&gt;. The only conference buzz I felt was through the Twitter community due to the total lack of chance encounters, late nights out, early morning briefings, and copious free coffee. The content felt solid though, and I admit that I made heavy use of pause, rewind, and 2x replay to watch things that I would have otherwise missed in-person.&lt;/p&gt;
&lt;p&gt;In my past SC recaps I remarked that I get the most out of attending the expo and accosting engineers on the floor, and the complete absence of that made SC feel a lot less whole. As a speaker, the lack of engagement with the audience was very challenging too. The 45-second delay between live video and Q&amp;amp;A made dialog challenging, and there was no way to follow up on questions or comments using the virtual platform. I suppose that is the price to be paid for having an otherwise robust virtual event platform.&lt;/p&gt;
&lt;p&gt;Although COVID forced us all into a sub-optimal SC venue this year, I think it also took away a lot of advancements, discussions, and dialog that would've fed a richer SC experience as well. With any luck SC can be in-person again next year and the community will have bounced back and made up for the time lost this year. When SC'21 rolls around, we should have at least one exascale system hitting the floor in the US (and perhaps another in China) to talk about, and the Aurora system should be very well defined. We'll have a few monster all-flash file systems on the I/O front to boot (including one in which I had a had a hand!), and the world will be opening up again--both in the technological sense and the literal sense. The future looks bright.&lt;/p&gt;
&lt;p&gt;As always, I owe my sincerest thanks to the organizers of SC this year for putting together the programs that spurred this internal monologue and the dialogues in which I engaged online these past two weeks. I didn't name every person from whom I drew insight, but if you recognize a comment that you made and would like attribution, please do let me know.&lt;/p&gt;
&lt;p&gt;Finally, if you'd like to read more, see my recaps of the &lt;a href=&quot;https://glennklockwood.blogspot.com/2020/11/pdsw20-recap.html&quot;&gt;PDSW'20 workshop&lt;/a&gt;, &lt;a href=&quot;https://www.nersc.gov/news-publications/staff-blogs/sc20-tiered-storage-panel-recap/&quot;&gt;my tiered storage panel&lt;/a&gt;, and the forthcoming DAOS User Group.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Buckle up, CPUs are going to get weirder</title>
   <link href="https://hpc.social/2020/buckle-up-cpus-are-going-to-get-weirder/"/>
   <updated>2020-11-22T00:00:00-07:00</updated>
   <id>https://hpc.social/2020/buckle-up-cpus-are-going-to-get-weirder</id>
   <content type="html">&lt;h2 id=&quot;the-m1-is-a-good-test-run-lets-get-ready&quot;&gt;The M1 is a good test run, let’s get ready&lt;/h2&gt;

&lt;p&gt;(Note: This post is adapted from last week’s &lt;a href=&quot;https://newsletter.researchcomputingteams.org/archive/5246c80f-2211-470c-94cb-d25496e8d5e8&quot;&gt;issue 51&lt;/a&gt; 
of the &lt;a href=&quot;https://www.researchcomputingteams.org&quot;&gt;resarch computing teams newsletter&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;The big news of the past month has been Apple’s new &lt;a href=&quot;https://www.anandtech.com/show/16252/mac-mini-apple-m1-tested/7&quot;&gt;M1
CPU&lt;/a&gt;.
The M1’s specs in and of themselves kind of interesting, but more
important to us in research computing is that the M1 is an example
of how CPUs are going to get more different as time goes on, and
that will have impacts on our teams.  The M1 going to be a trial run for
a future of more diverse computing architectures that we’d do well
to get ready for.&lt;/p&gt;

&lt;p&gt;Large-scale research computing systems have all been about “co-design”
for ages, but the truth is that in the mainstream, big-picture CPU
design choices have been pretty fixed, with most of co-design
being about choice of accelerators or mix and match between CPU,
memory. and acceleration.  Now that the market has accepted ARM as
a platform — and with &lt;a href=&quot;https://riscv.org&quot;&gt;RISC-V&lt;/a&gt; on its way — we
can expect to start seeing bolder choices for CPU design being
shipped, with vendors making very different tradeoffs than have
been made in the past.  So whether or not you see yourself using
Apple hardware in the future, M1’s choices and their consequences
are interesting.&lt;/p&gt;

&lt;p&gt;M1 makes two substantially different trade-offs.  The first is
having DRAM on socket.  This sacrifices extensibility — you can’t
just add memory — for significantly better memory performance and
lower power consumption.  Accurately moving bits back and forth
between chips takes a surprising amount of energy, and doing it
fast takes a lot of power!   The results are striking:&lt;/p&gt;

&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p dir=&quot;ltr&quot; lang=&quot;ja&quot;&gt;M1 MacBook AirでLINPACK動かして電力測定をしてみた。USB PD電力計＋iOS用Linpackという謎アプリのため参考値だが34.01 GFlops/W。まともに測るべきだしスケールしないやり方なので比べられる値ではないが、点灯したLCD込みでGreen 500の1位は超えていることに… うーん、正しいのか？ &lt;a href=&quot;https://t.co/ldEroByfxt&quot;&gt;pic.twitter.com/ldEroByfxt&lt;/a&gt;&lt;/p&gt;
&amp;mdash; Ohtsuji (@ohtsuji) &lt;a href=&quot;https://twitter.com/ohtsuji/status/1328768907461623808?ref_src=twsrc%5Etfw&quot;&gt;November 17, 2020&lt;/a&gt;&lt;/blockquote&gt;

&lt;p&gt;LINPACK - solving a set of linear equations - is a pretty flawed
benchmark, but it’s widely understood.  The performance numbers
here are pretty healthy for a chip with four big cores, but the
&lt;em&gt;efficiency&lt;/em&gt; numbers are startling.  They’re not unprecedented
except for the context; these wouldn’t be surprising numbers for a
GPU, which also have DRAM-on-socket, and are similarly non-extensible.
But they are absurdly high for something more general-purpose like
a CPU.&lt;/p&gt;

&lt;p&gt;Having unified on-socket memory between CPU and integrated GPU also
makes possible some &lt;a href=&quot;https://blog.tensorflow.org/2020/11/accelerating-tensorflow-performance-on-mac.html&quot;&gt;great Tensorflow
performance&lt;/a&gt;,
simultaneously speeds up and lowers power consumption for &lt;a href=&quot;https://www.macrumors.com/2020/11/17/apple-silicon-m1-compiles-code-as-fast-as-mac-pro/&quot;&gt;compiling
code&lt;/a&gt;,
and does weirdly well at running
&lt;a href=&quot;https://info.crunchydata.com/blog/postgresql-benchmarks-apple-arm-m1-macbook-pro-2020&quot;&gt;postgreSQL&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The second tradeoff has some more immediate effects for research
computing teams. Apple, as is its wont, didn’t worry too much about
backwards-looking compatibility, happily sacrificing that for
future-looking capabilities.  The new Rosetta (x86 emulation) seems
to work seamlessly and is &lt;a href=&quot;https://twitter.com/pmelsted/status/1329934691944816640&quot;&gt;surprisingly
performant&lt;/a&gt;.  But
if you want to take full advantage of the architecture of course
you have to compile natively.  And on the day of release, a lot of
key tools and libraries didn’t just “automatically” work the way
they seemed to when most people first started using other ARM chips.
(Though that wasn’t magic either; the ecosystem had spent years
slowly getting ready for adoption by the mainstream.)&lt;/p&gt;

&lt;p&gt;“Freaking out” wouldn’t be too strong a way to describe
the reaction in some corners; one user claimed that GATK would
&lt;a href=&quot;https://twitter.com/biocrusoe/status/1328704001039339521&quot;&gt;“never
work”&lt;/a&gt; on
Apple silicon (because a build script mistakenly assumed that an
optional library that had Intel-specific optimizations would be
present - they’re on it), and the absence of a free fortran compiler
on the day of hardware release worried other people (there’s already
&lt;a href=&quot;https://github.com/fxcoudert/gfortran-for-macOS/releases/tag/11-arm-alpha1&quot;&gt;experimental gfortran
builds&lt;/a&gt;).
Having come of computational science age in the 90s when new chips
took months to get good tooling for, the depth of concern seemed a
bit overwrought.&lt;/p&gt;

&lt;p&gt;This isn’t to dismiss the amount of work that’s needed to get
software stacks working on new systems.  Between other ARM systems
and M1, a lot of research software teams are going to have to put
in a lot of time porting new low-level libraries and tools to the
new architectures.  Many teams that haven’t had to worry about this
sort of thing before are going to have to refactor architecture-specific
optimizations out and into libraries.  Some code will simply have
to be rewritten - some R code has depended on &lt;a href=&quot;https://developer.r-project.org/Blog/public/2020/11/02/will-r-work-on-apple-silicon/&quot;&gt;Intel-specific NaN
handling&lt;/a&gt;
to implement NA semantics (which are &lt;a href=&quot;https://blog.revolutionanalytics.com/2016/07/understanding-na-in-r.html&quot;&gt;similar to but different
from&lt;/a&gt;
NaN) that M1 does not honour, so natively compiled R needs extra
checks on M1.&lt;/p&gt;

&lt;p&gt;It’s also not to dismiss the complexity that people designing and
running computing systems will have to face.  Fifteen years ago,
the constraints on a big computing system made things pretty clear — 
you’d choose a whackload of x86 with some suitably fast (for your application)
network. The main question were how fat are the nodes, what’s
the mix of low, medium, and high-memory nodes, and what your storage
system is like.  It’s been more
complex for a while with accelerators, and now with entirely different
processor architectures in the mix, it will get harder.  Increasingly,
there is no “best” system; a system has to be tuned to favour some
specific workloads.  And that necessarily means disfavouring others,
which centres have been loathe to do.&lt;/p&gt;

&lt;p&gt;So the point here isn’t M1.  Is M1 a good choice for your research
computing support needs?  Almost certainly not if you run on clusters.
And if you’re good with your laptop or desktop, well, then lots of
processors will work well enough.&lt;/p&gt;

&lt;p&gt;But even so, a lot of software is going to now have to support these
new chips. And this is just the start of “weird” CPUs 
coming for research computing.&lt;/p&gt;

&lt;p&gt;CPUs will keep coming that will make radically different tradeoffs
than choices than seemed obvious before.  That’s going to make
things harder for research software and research computing systems
teams for a while.  A lot of “&lt;a href=&quot;https://encyclopedia2.thefreedictionary.com/vaxocentrism&quot;&gt;all the world’s an
x86&lt;/a&gt;”
assumptions - some that are so ingrained they are currently hard
to see - are going to get upended, and setting things back right
is going to take work.  The end result will be more flexible and
capable code, build systems, and better-targeted systems, but it’ll
take a lot of work to get there.   If you haven’t already started
using build and deployment workflows and processes that can handle
supporting multiple architectures, now is a good time to start.&lt;/p&gt;

&lt;p&gt;But the new architectures, wider range of capabilities, and different
tradeoff frontiers are also going to expand the realm of what’s
possible for research computing.  And isn’t that why we got into
this field?&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>PDSW'20 Recap</title>
   <link href="https://hpc.social/2020/pdsw-20-recap/"/>
   <updated>2020-11-20T06:00:00-07:00</updated>
   <id>https://hpc.social/2020/pdsw-20-recap</id>
   <content type="html">&lt;p&gt;This year was the first all-virtual &lt;a href=&quot;http://www.pdsw.org/index.shtml&quot;&gt;Parallel Data Systems Workshop&lt;/a&gt;, and despite the challenging constraints imposed by the pandemic, it was remarkably engaging. &amp;nbsp;The program itself was contracted relative to past years and only had time for three Work-In-Progress (WIP) presentations, so it was a little difficult to pluck out high-level research trends and themes. &amp;nbsp;However, this year's program did seem more pragmatic, with talks covering very practical topics that had clear connection to production storage and I/O. The program also focused heavily on the HPC side of the community, and the keynote address was perhaps the only talk that focused squarely on the data-intensive data analysis side of what used to be PDSW-DISCS. &amp;nbsp;Whether this is the result of PSDW's return to the short paper format this year, shifting priorities from funding agencies, or some knock-on effect of the pandemic is impossible to say.&lt;/p&gt;
&lt;p&gt;Although there weren't any strong themes that jumped out at me, &lt;a href=&quot;https://glennklockwood.blogspot.com/2019/11/sc19-recap.html#pdsw&quot;&gt;last year's theme of using AI to optimize I/O performance&lt;/a&gt; was much more muted this year. &amp;nbsp;&lt;a href=&quot;http://www.pdsw.org/pdsw20/papers/ws_pdsw_S2_P1_Rosario.pdf&quot;&gt;Eliakin del Rosario presented a paper&lt;/a&gt; describing a clustering and visual analysis tool he developed that underpins &lt;a href=&quot;https://sc20.supercomputing.org/?post_type=page&amp;amp;p=3479&amp;amp;id=ws_ross106&amp;amp;sess=sess226&quot;&gt;a study applying machine learning to develop an I/O performance model&lt;/a&gt; presented in the main SC technical program, but there was no work in the direction of applying AI to directly optimize I/O. &amp;nbsp;Does this mean that these ideas have climbed over the hype curve and are now being distilled down into useful techniques that may appear in production technologies in the coming years? &amp;nbsp;Or was the promise of AI to accelerate I/O just a flash in the pan?&lt;/p&gt;
&lt;p&gt;In the absence of common themes to frame my recap, what follows are just my notes and thoughts about some of the talks and presentations that left an impression. &amp;nbsp;I wasn't able to attend the WIP session or cocktail hour due to non-SC work obligations (it's harder to signal to coworkers that you're &quot;on travel to a conference&quot; when you're stuck at home just like any other workday) so I undoubtedly missed things, but all slides and papers are available on &lt;a href=&quot;http://www.pdsw.org/index.shtml&quot;&gt;the PDSW website&lt;/a&gt;, and anyone with an SC workshop pass can re-&lt;a href=&quot;https://cdmcd.co/P4WY7Y&quot;&gt;watch the recorded proceedings on the SC20 digital platform&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;h2 style=&quot;text-align: left;&quot;&gt;Keynote - Nitin Agrawal&lt;/h2&gt;
&lt;p&gt;This year’s keynote by &lt;a href=&quot;http://pages.cs.wisc.edu/~nitina/&quot;&gt;Nitin Agrawal&lt;/a&gt; was a long-form research presentation on SummaryStore, an “approximate storage system” that doesn't store the data you put in it so much as it stores the data you will probably want to get back out of it at a later date. &amp;nbsp;This notion of a storage system that doesn't actually store things sounds like an affront at a glance, but when contextualized properly, it makes quite a lot of sense:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://lh3.googleusercontent.com/-A-w5SKA256s/X7X5EMOW3xI/AAAAAAABPeI/zM_zV6TtemYefCZIKTbu7Gr_GlI8AW37QCLcBGAsYHQ/How%2Bnot%2Bto%2Bdrown%253A%2Bdemocratizing%2Bstorage.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img height=&quot;225&quot; src=&quot;https://lh3.googleusercontent.com/-A-w5SKA256s/X7X5EMOW3xI/AAAAAAABPeI/zM_zV6TtemYefCZIKTbu7Gr_GlI8AW37QCLcBGAsYHQ/w400-h225/How%2Bnot%2Bto%2Bdrown%253A%2Bdemocratizing%2Bstorage.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;p&gt;There are cases where the data being stored doesn't have high value. &amp;nbsp;For example, data may become less valuable as it ages, or data may only be used to produce very rough guesses (e.g., garbage out) so inputting rough data (garbage in) is acceptable. &amp;nbsp;In these cases, the data may not be worth the cost of the media on which it is being stored, or its access latency may be more important than its precision; these are the cases where an approximate storage system may make sense.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;The specific case presented by Dr. Agrawal, SummaryStore, strongly resembled a time series database to feed a recommendation engine that naturally weighs recent data more heavily than older data. &amp;nbsp;The high-level concept seemed a lot like existing time series telemetry storage systems where high-frequency time series data are successively aggregated as they age so that new data may be sampled every few seconds while old data may be sampled once an hour.&lt;/p&gt;
&lt;p&gt;For example, LMT and mmperfmon are time series data collection tools for measuring the load on Lustre and Spectrum Scale file systems, respectively. &amp;nbsp;The most common questions I ask of these tools are things like&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul style=&quot;text-align: left;&quot;&gt;&lt;li&gt;What was the sum of all write bytes between January 2018 and January 2019?&lt;/li&gt;&lt;li&gt;How many IOPS was the file system serving between 5:05 and 5:10 this morning?&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;By comparison, it’s very rare to ask “How many IOPS was the file system serving between 5:05 and 5:10 two years ago?”  It follows that the storage system underneath LMT and mmperfmon can be “approximate” to save space and/or improve query performance.  Dr. Agrawal’s presentation included this pictorial representation of this:&amp;lt;p&amp;gt;&amp;lt;/p&amp;gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://lh3.googleusercontent.com/-P1Sm1HEPVpI/X7X75BfDWaI/AAAAAAABPeU/myD6BnYdV_IP7vxthcSWRwD12W4kjQhhwCLcBGAsYHQ/Time-decayed%2Bstream%2Bapproximation.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img height=&quot;225&quot; src=&quot;https://lh3.googleusercontent.com/-P1Sm1HEPVpI/X7X75BfDWaI/AAAAAAABPeU/myD6BnYdV_IP7vxthcSWRwD12W4kjQhhwCLcBGAsYHQ/w400-h225/Time-decayed%2Bstream%2Bapproximation.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;Because these approximate storage systems are specifically designed with an anticipated set of queries in mind, much of Agrawal's presentation really spoke to implementation-specific challenges he faced while implementing SummaryStore--things like how SummaryStore augmented bloom filter buckets with additional metadata to allow approximations of sub-bucket ranges to be calculated. &amp;nbsp;More of the specifics can be found in the &lt;a href=&quot;http://www.pdsw.org/pdsw20/slides/pdsw_keynote_2020_Nitin.pdf&quot;&gt;presentation slides&lt;/a&gt; and references therein.&lt;/p&gt;
&lt;p&gt;This notion of approximate storage is not new; it is preceded by years of research into &lt;i&gt;semantic file systems&lt;/i&gt;, where the way you store data is driven by the way in which you intend to access the data. &amp;nbsp;By definition, these are data management systems that are tailor-made for specific, high-duty cycle I/O workloads such as web service backends.&lt;/p&gt;
&lt;p&gt;What I took away from this presentation is that semantic file systems (and approximate storage systems by extension) aren't intrinsically difficult to build for these specific workloads. &amp;nbsp;Rather, making such a system sufficiently generic &lt;i&gt;in practice&lt;/i&gt; to be useful beyond the scope of such a narrow workload is where the real challenge lies. &amp;nbsp;Tying this back to the world of HPC, it's hard to see where an approximate storage system could be useful in most HPC facilities since their typical workloads are so diverse. &amp;nbsp;However, two thoughts did occur to me:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ol style=&quot;text-align: left;&quot;&gt;&lt;li&gt;If the latency and capacity characteristics of an approximate storage system are so much better than generic file-based I/O when implemented on the same storage hardware (DRAM and flash drives), an approximate storage system could help solve problems that traditionally were limited by memory capacity. &amp;nbsp;DNA sequence pattern matching (think &lt;a href=&quot;https://blast.ncbi.nlm.nih.gov&quot;&gt;BLAST&lt;/a&gt;) or de novo assembly could feasibly be boosted by an approximate index.&lt;/li&gt;&lt;li&gt;Since approximate storage systems are purpose-built for specific workloads, the only way they fit into a general-purpose HPC environment is using purpose-built composable data services. &amp;nbsp;Projects like &lt;a href=&quot;https://press3.mcs.anl.gov/mochi/&quot;&gt;Mochi&lt;/a&gt; or &lt;a href=&quot;https://github.com/excelab/bespokv&quot;&gt;BespoKV&lt;/a&gt; provide the building blocks to craft and instantiate such purpose-built storage systems, and software-defined storage orchestration in the spirit of &lt;a href=&quot;https://cug.org/proceedings/cug2016_proceedings/includes/files/pap105s2-file1.pdf&quot;&gt;DataWarp&lt;/a&gt; or the &lt;a href=&quot;https://www.hpc.cam.ac.uk/research/data-acc&quot;&gt;Cambridge Data Accelerator&lt;/a&gt; would be needed to spin up an approximate storage service in conjunction with an application that would use it. &amp;nbsp;&lt;/li&gt;&lt;/ol&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;I'm a big believer in #2, but #1 would require a forcing function coming from the science community to justify the effort of adapting an application to use approximate storage.&lt;/p&gt;
&lt;h2 style=&quot;text-align: left;&quot;&gt;Keeping It Real: Why HPC Data Services Don't Achieve I/O Microbenchmark Performance&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://www.pdsw.org/pdsw20/papers/ws_pdsw_S1_P1_Carns.pdf&quot;&gt;Phil Carns (Argonne) presented a lovely paper&lt;/a&gt; full of practical gotchas and realities surrounding the idea of establishing a roofline performance model for I/O. &amp;nbsp;The goal is simple: measure the performance of each component in an I/O subsystem's data path (application, file system client, network, file system server, storage media), identify the bottleneck, and see how close you can get to hitting the theoretical maximum of that bottleneck:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://lh3.googleusercontent.com/-couyuEn261I/X7YGwPqk4uI/AAAAAAABPeg/M_HHYQMrdnM5q1qrdKX9KPBjTPw6J9FjgCLcBGAsYHQ/Screen%2BShot%2B2020-11-12%2Bat%2B08.11.48.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img height=&quot;225&quot; src=&quot;https://lh3.googleusercontent.com/-couyuEn261I/X7YGwPqk4uI/AAAAAAABPeg/M_HHYQMrdnM5q1qrdKX9KPBjTPw6J9FjgCLcBGAsYHQ/w400-h225/Screen%2BShot%2B2020-11-12%2Bat%2B08.11.48.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;br /&gt;&lt;/div&gt;
&lt;p&gt;The thesis of the paper was that even though this sounds simple, there’s a lot more than meets the eye.  I won’t recite the presentation (see the &lt;a href=&quot;http://www.pdsw.org/pdsw20/papers/ws_pdsw_S1_P1_Carns.pdf&quot;&gt;paper&lt;/a&gt; and &lt;a href=&quot;http://www.pdsw.org/pdsw20/slides/pdsw_S1_P1_%20Carns.pdf&quot;&gt;slides&lt;/a&gt;–they’re great), but I thought some of the more interesting findings included:&amp;lt;p&amp;gt;&amp;lt;/p&amp;gt;&lt;/p&gt;
&lt;div&gt;&lt;ol style=&quot;text-align: left;&quot;&gt;&lt;li&gt;There's a 40% performance difference between the standard OSU MPI bandwidth benchmark and what happens when you make the send buffer too large to fit into cache. &amp;nbsp;It turns out that actually writing data over the network from DRAM (as a real application would) is demonstrably slower than writing data from a tiny cacheable memory buffer.&lt;/li&gt;&lt;li&gt;Binding MPI processes to cores is good for MPI latency but can be bad for I/O bandwidth. &amp;nbsp;Highly localized process placement is great if those processes talk to each other, but if they have to talk to something off-chip (like network adapters), the more spread out they are, the greater the path diversity and aggregate bandwidth they may have to get out of the chip.&lt;/li&gt;&lt;li&gt;O_DIRECT bypasses page cache but not device cache, while O_SYNC does not bypass page cache &amp;nbsp;but flushes both page and device caches. &amp;nbsp;This causes O_DIRECT to reduce performance for smaller I/Os which would benefit from write-back caching when used by itself, but increase performance when used with O_SYNC since one less cache (the page cache) has to be synchronized on each write. Confusing &lt;i&gt;and&lt;/i&gt; wild. &amp;nbsp;And also completely nonstandard since these are Linux-specific flags.&lt;/li&gt;&lt;/ol&gt;&lt;h2 style=&quot;text-align: left;&quot;&gt;Towards On-Demand I/O Forwarding in HPC Platforms&lt;/h2&gt;&lt;/div&gt;
&lt;div&gt;&lt;a href=&quot;http://www.pdsw.org/pdsw20/papers/ws_pdsw_S1_P2_Bez.pdf&quot;&gt;Jean Luca Bez (UFRGS) presented a neat userspace I/O forwarding service&lt;/a&gt;, FORGE, that got me pretty excited since the field of &lt;a href=&quot;https://www.glennklockwood.com/data-intensive/storage/io-forwarding.html&quot;&gt;I/O forwarding has been pretty stagnant&lt;/a&gt; since IOFSL came out ten years ago.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;The high-level concept is simple: take the intelligence of collective I/O operations implemented in ROMIO and, instead of running them inside the same MPI application performing I/O, offload that functionality to discrete nodes:&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://lh3.googleusercontent.com/-vQtcMpV9y_w/X7YLc2Ti4HI/AAAAAAABPes/Z3WbhdeFEVYaHRETWwmwAEwWf2VjEGkrwCLcBGAsYHQ/Screen%2BShot%2B2020-11-12%2Bat%2B08.40.44.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img height=&quot;225&quot; src=&quot;https://lh3.googleusercontent.com/-vQtcMpV9y_w/X7YLc2Ti4HI/AAAAAAABPes/Z3WbhdeFEVYaHRETWwmwAEwWf2VjEGkrwCLcBGAsYHQ/w400-h225/Screen%2BShot%2B2020-11-12%2Bat%2B08.40.44.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;br /&gt;This FORGE service is ephemeral in that it is spun up at the same time your MPI application is spun up and persists for the duration of the job. &amp;nbsp;However unlike traditional MPI-IO-based collectives, it runs on dedicated nodes, and it relies on &lt;i&gt;a priori&lt;/i&gt; knowledge of the application's I/O pattern to decide what sorts of I/O reordering would benefit the application.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;This is perhaps a bit wasteful since nodes are being held idle until I/O happens, but the promise of this idea is much larger. &amp;nbsp;Many large HPC systems have dedicated I/O forwarding nodes because they have to--for example, LNet routers or DVS servers exist in Cray-based HPC systems to do the network protocol conversion to allow InfiniBand-based Lustre and Spectrum Scale file systems to be mounted on Aries-based compute nodes. &amp;nbsp;There's no reason these same nodes couldn't also be used to run FORGE-like services on-demand to buffer and reorder I/Os in transit. &amp;nbsp;And if you stick some NVMe into these protocol conversion nodes, you suddenly have something that looks an awful lot like a transparent burst buffer akin to DDN Infinite Memory Engine.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;Taking this a step further, this idea also further motivates having reconfigurable storage infrastructure within an HPC system; with a little bit of knowledge about your I/O workload, one could reconfigure the parallelism and compute power available along the I/O data path itself to optimally balance the limited resources of nodes and the performance benefit. &amp;nbsp;A couple examples:&lt;/div&gt;
&lt;div&gt;&lt;ul style=&quot;text-align: left;&quot;&gt;&lt;li&gt;Have a very IOPS-heavy, many-file workload? &amp;nbsp;Since these tend to be CPU-limited, it would make sense to allocate a lot of FORGE nodes to this job so that you have a lot of extra CPU capacity to receive these small transactions, aggregate them, and drive them out to the file system.&lt;/li&gt;&lt;li&gt;Have a bandwidth-heavy shared-file workload? &amp;nbsp;Driving bandwidth doesn't require a lot of FORGE nodes, and fewer nodes means fewer potential lock conflicts when accessing the shared file.&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;This intelligent I/O forwarding naturally maps to file system architectures that incorporate I/O forwarding and stateless components--like &lt;a href=&quot;https://glennklockwood.blogspot.com/2019/02/vast-datas-storage-system-architecture.html&quot;&gt;VAST&lt;/a&gt;--where more network and computational parallelism can be sloshed into a compute node's data path to deal with more complex or adversarial I/O patterns.&lt;/div&gt;
&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h2 style=&quot;text-align: left;&quot;&gt;Fractional-Overlap Declustered Parity&lt;/h2&gt;
&lt;div&gt;&lt;a href=&quot;http://www.pdsw.org/pdsw20/papers/ws_pdsw_S2_P2_%20Ke.pdf&quot;&gt;Huan Ke (U Chicago) presented a paper&lt;/a&gt; that tried to bridge the gap between RAID implementations that use declustered parity, which has really fast rebuild but a huge failure domain, and traditional (clustered) parity which has very slow rebuilds but a very small failure domain.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;The special sauce proposed by Ke is being judicious about how stripes are laid out across a declustered group. &amp;nbsp;Using Latin squares to map RAID blocks to physical drives, one can control how many unique stripes would be affected by a failure (termed the &lt;i&gt;overlap fraction&lt;/i&gt;):&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://lh3.googleusercontent.com/-q4m6ljXvG7I/X7YTBI1-j_I/AAAAAAABPe4/acpRWMeZsx4agTJ96k_6OSTp8CcWAcN1gCLcBGAsYHQ/Screen%2BShot%2B2020-11-12%2Bat%2B09.46.02.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img height=&quot;225&quot; src=&quot;https://lh3.googleusercontent.com/-q4m6ljXvG7I/X7YTBI1-j_I/AAAAAAABPe4/acpRWMeZsx4agTJ96k_6OSTp8CcWAcN1gCLcBGAsYHQ/w400-h225/Screen%2BShot%2B2020-11-12%2Bat%2B09.46.02.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
This is usually where I stop being able to keep up in these sorts of parity scheme talks; however, I quickly realized that this parity scheme relies on the same principle that engineers use to design cost-efficient parameter sweep experiments. &amp;nbsp;In fact, I made a &lt;a href=&quot;https://www.glennklockwood.com/materials-science/statistical-design.html&quot;&gt;webpage about this exact topic in the context of optimizing a hypothetical chemical vapor deposition experiment&lt;/a&gt; when I was an undergraduate in materials science, and it's really not as complicated as I thought. &amp;nbsp;&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;What it boils down to is defining a set of experiments (or mappings between RAID blocks and drives) where you vary all the parameters (temperature, pressure etc--or which RAID block maps to which drive) but ensure that the same parameter value is never repeated twice (e.g., don't have two experiments with temperature held at 30C, or have two RAID layouts where block #2 is never placed on drive #3). &amp;nbsp;Orthogonal arrays (which are composed of Latin squares) provide an analytical method for coming up with these unique combinations.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;In the engineering context, you essentially never repeat an experiment if you can infer the result of varying one parameter using a combination of other experiments. &amp;nbsp;In the parity placement scheme, you never use a block mapping if a combination of drive failures will break all your RAID stripes. &amp;nbsp;The neat idea behind what Ke presented is a method to vary this constraint so that you can find layout schemes that have any mix of blast radius (how many stripes are lost on an unrecoverable failure) against rebuild time.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h2 style=&quot;text-align: left;&quot;&gt;NVIDIA GPUDirect Storage Support in HDF5&lt;/h2&gt;
&lt;div&gt;&lt;a href=&quot;http://www.pdsw.org/pdsw20/papers/ws_pdsw_S2_P3_Ravi.pdf&quot;&gt;John Ravi presented his work&lt;/a&gt; implementing support for NVIDIA's brand new &lt;a href=&quot;https://developer.nvidia.com/blog/gpudirect-storage/&quot;&gt;GPUDirect Storage&lt;/a&gt; (which allows data transfer between GPU memory and an NVMe device without ever touching host memory using &lt;a href=&quot;https://www.kernel.org/doc/html/latest/driver-api/pci/p2pdma.html&quot;&gt;peer-to-peer PCIe&lt;/a&gt;) in HDF5. &amp;nbsp;Much of the talk focused on the implementation details specific to HDF5, but he did present some performance results which I found quite interesting:&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://lh3.googleusercontent.com/-60ItLuyV3bQ/X7c29whuOgI/AAAAAAABPfM/texW-SpqU5UgY2OQKkKlk1fgF_pZhBzXQCLcBGAsYHQ/Screen%2BShot%2B2020-11-12%2Bat%2B10.15.45.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img height=&quot;225&quot; src=&quot;https://lh3.googleusercontent.com/-60ItLuyV3bQ/X7c29whuOgI/AAAAAAABPfM/texW-SpqU5UgY2OQKkKlk1fgF_pZhBzXQCLcBGAsYHQ/w400-h225/Screen%2BShot%2B2020-11-12%2Bat%2B10.15.45.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;br /&gt;In the above diagram, &quot;SEC2&quot; refers to the default POSIX interface, &quot;DIRECT&quot; is POSIX using O_DIRECT, and &quot;GDS&quot; is GPUDirect Storage. &amp;nbsp;What surprised me here is that all of the performance benefits were expressed in terms of bandwidth, not latency--I naively would have guessed that not having to bounce through host DRAM would enable much higher IOPS. &amp;nbsp;These results made me internalize that the performance benefits of GDS lie in not having to gum up the limited bandwidth between the host CPU and host DRAM. &amp;nbsp;Instead, I/O can enjoy the bandwidth of HBM or GDDR to the extent that the NVMe buffers can serve and absorb data. &amp;nbsp;I would hazard that in the case of IOPS, the amount of control-plane traffic that has to be moderated by the host CPU undercuts the fast data-plane path enabled by GDS. &amp;nbsp;This is consistent with literature from &lt;a href=&quot;https://www.ddn.com/blog/ddn-ai-storage-gets-faster-simpler-gpudirect-storage/&quot;&gt;DDN&lt;/a&gt;&amp;nbsp;and &lt;a href=&quot;https://vastdata.com/resources/lightspeed-e-book/&quot;&gt;VAST&lt;/a&gt; about their performance boosts from GDS.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h2 style=&quot;text-align: left;&quot;&gt;Fingerprinting the Checker Policies of Parallel File Systems&lt;/h2&gt;
&lt;div&gt;The final PDSW talk that struck a chord was by &lt;a href=&quot;http://www.pdsw.org/pdsw20/papers/ws_pdsw_S3_P3_Han.pdf&quot;&gt;Runzhou Han who presented a methodology for exercising parallel file systems' fsck tools&lt;/a&gt; using targeted fault injection. &amp;nbsp;He intentionally corrupted different parts of the data structures used by BeeGFS and Lustre to store metadata, then ran fsck to see how well those mistakes were caught. &amp;nbsp;I think the biggest intellectual contribution of the work was formalizing a taxonomy of different types of corruption events (junk data, zeros written, duplicate data, and out-of-sync data) and ways in which fsck does or does not cope with them:&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://lh3.googleusercontent.com/-eQLpeKUzuwM/X7dKJ7pWKmI/AAAAAAABPfY/qyIoWhHODycUmF1ubU74p6cz34h5u-6QACLcBGAsYHQ/Screen%2BShot%2B2020-11-12%2Bat%2B12.34.07.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img height=&quot;225&quot; src=&quot;https://lh3.googleusercontent.com/-eQLpeKUzuwM/X7dKJ7pWKmI/AAAAAAABPfY/qyIoWhHODycUmF1ubU74p6cz34h5u-6QACLcBGAsYHQ/w400-h225/Screen%2BShot%2B2020-11-12%2Bat%2B12.34.07.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;br /&gt;The practical outcome of this work is that it identified a couple of data structures and corruption patterns that are particularly fragile on Lustre and BeeGFS. &amp;nbsp;Alarmingly, two cases triggered kernel panics in lfsck which led me to beg the question: why isn't simple fault injection like this part of the regular regression testing performed on Lustre? &amp;nbsp;As someone who's been adjacent to several major parallel file system outages that resulted from fsck not doing a good job, hardening the recovery process is a worthwhile investment since anyone who's having to fsck in the first place is already having a bad day.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;That said, this paper seemed much more practical than foundational and it was unclear where this goes once the immediate issues discovered are addressed. &amp;nbsp;To that end, I could see why hardening fsck isn't getting a lot of research attention.&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>What will Post-Pandemic Academic Research Computing Look Like?</title>
   <link href="https://hpc.social/2020/what-will-post-pandemic-academic-research-computing-look-like/"/>
   <updated>2020-10-24T01:00:00-06:00</updated>
   <id>https://hpc.social/2020/what-will-post-pandemic-academic-research-computing-look-like-</id>
   <content type="html">&lt;p&gt;We’re nowhere near the endgame yet.  But even now in the middle of the COVID-19 times it is not too soon to think about what research computing will look like when the threat of infection by SARS-CoV-2 no longer shapes our work lives.  While the future looks good for research computing team individual contributors who are willing to learn on the fly, the coming years
will be treacherous for teams as organizations, and their managers.&lt;/p&gt;

&lt;h2 id=&quot;what-hath-2020-wrought&quot;&gt;What hath 2020 wrought&lt;/h2&gt;

&lt;p&gt;There’s a few pretty unambiguous “inputs” from 2020 that will have consequences for years to come:&lt;/p&gt;

&lt;h3 id=&quot;institutional-and-governmental-coffers-are-depleted&quot;&gt;Institutional and governmental coffers are depleted&lt;/h3&gt;

&lt;p&gt;Entire sectors of the economy are in bad shape.  Institutional budgets have suffered across the board.  There have been large unforeseen costs for dealing with the pandemic, while normal operating costs haven’t gone down much except in tiny budget lines like travel.&lt;/p&gt;

&lt;p&gt;At Universities, international student tuitions have dropped less than expected, but there are well-founded worries that they will continue dropping and not bounce back.   In a lot of jurisdictions,  dollops of one-off support for educational institution came from governments.  Those governments will be tightening their budget as soon as they can, and reducing rather than increasing payouts over the course of many years to claw their way back to budget balance.&lt;/p&gt;

&lt;h3 id=&quot;clients-are-now-used-to-research-computing-teams-being-distant&quot;&gt;Clients are now used to research computing teams being distant&lt;/h3&gt;

&lt;p&gt;We’ve all been working from home over the course of months.  A lot of previously unquestioned assumptions about how important it is to have certain groups or equipment “here” with the research groups so that they could be accessible are now known to be mistaken.  Researchers, VPRs, and funders are seeing that virtual teams for research computing can support research perfectly well with some controls in place.  Yes, it’s handy to sit down beside someone to get things sorted sometimes but we’ve learned we can do pretty well without that.&lt;/p&gt;

&lt;h3 id=&quot;primacy-of-health-research&quot;&gt;Primacy of health research&lt;/h3&gt;

&lt;p&gt;Life sciences has been an increasingly important part of research computing since quantitative molecular methods took off, and even more since the human genome project’s completion.  During the pandemic, centres have dramatically shifted towards prioritizing various kinds of health research workloads, which in turn has boosted capacity (and expectations) of lots of health related research groups and their funders.&lt;/p&gt;

&lt;h3 id=&quot;importance-of-data-and-data-sharing-better-understood&quot;&gt;Importance of data and data sharing better understood&lt;/h3&gt;

&lt;p&gt;With most of the world daily monitoring case counts, “excess deaths”, case fatality rate vs infection fatality rate and the like, the importance of clean, high-quality data has never been more widely understood.  And the limits of what “AI” or advanced analysis techniques can do with poor quality data is very clear.&lt;/p&gt;

&lt;p&gt;And as data’s importance becomes clearer the importance of pooling data has never been more obvious, even in disciplines typically very reluctant to do so (sometimes for good reasons, sometimes not).  That’s very unlikely to rapidly change back.&lt;/p&gt;

&lt;h3 id=&quot;the-best-research-computing-teams-have-learned-to-communicate-a-lot-better&quot;&gt;The best research computing teams have learned to communicate a lot better&lt;/h3&gt;

&lt;p&gt;The research computing and data teams that have come through this pretty well and with satisfied clients have really had to up their games in communications - internally and externally, synchronous and asynchronous.  Many of these teams already had experience sucessfully working with distributed collaborators and partners, and built on those strengths.&lt;/p&gt;

&lt;p&gt;But not all research computing and data teams have come through this experience with satisfied client researchers.&lt;/p&gt;

&lt;h2 id=&quot;consequences-2021-and-beyond&quot;&gt;Consequences: 2021 and beyond&lt;/h2&gt;

&lt;p&gt;None of the changes I’ve described above are particularly subtle or ambiguous, and I think the short-term consequences are almost as clear.  Some short and mid-term consequences will be, roughly in order of certainty:&lt;/p&gt;

&lt;h3 id=&quot;research-computing-teams-are-never-going-back-to-100-work-from-office&quot;&gt;Research computing teams are never going back to 100% work-from-office&lt;/h3&gt;

&lt;p&gt;This one is so obvious it hardly needs to be said, but let’s say it.  Space on University campuses has always been tight, and 2020 has shown us that research computing teams don’t need to be on campus.  While each team will have to figure out its own approach - fully distributed, rotating with hot-desking, hybrid - we’re never going back to routinely being all together on campus.&lt;/p&gt;

&lt;h3 id=&quot;research-budgets-are-mostly-going-to-shrink-except-in-health&quot;&gt;Research budgets are mostly going to shrink, except in health&lt;/h3&gt;

&lt;p&gt;Governments worldwide will start trying to get their finances back into balance after the huge COVID-19 expenditures and shrunken tax revenues of 2020 and early(?) 2021.  While research budgets probably won’t be drastically cut, they certainly won’t grow.&lt;/p&gt;

&lt;p&gt;On the other hand, even once the pandemic is well and truly over, funding for health and health research will be extremely popular, voters will be wary of another pandemic, and COVID-19 long-term effects will still need to be studied and monitored.  Health and health research will have an even larger claim to priority over stagnant research funding than before, and institutions will be eager to support such efforts.&lt;/p&gt;

&lt;h3 id=&quot;research-support-budgets-are-going-to-shrink&quot;&gt;Research support budgets are going to shrink&lt;/h3&gt;

&lt;p&gt;With research budgets flat and institutions facing declining government funding and possibly international enrolments, there is going to be pressure to make cuts wherever possible.  “Overheads” for the basic missions of teaching and research are going to be under increasing scrutiny.&lt;/p&gt;

&lt;p&gt;Any research computing team that can’t communicate very clearly its value to VPRs and university administration in terms of research dollars and other outcomes the administration cares about is going to be facing a lot of very uncomfortable questions.  Any cuts to research support services that won’t result in months and months worth of  of angry phone calls are going to look pretty attractive to administrations trying to figure out what to cut without firing faculty or teaching staff.&lt;/p&gt;

&lt;h3 id=&quot;research-computing-teams-will-consolidate&quot;&gt;Research computing teams will consolidate&lt;/h3&gt;

&lt;p&gt;VPRs have long eyed various kinds of core facilities and wondered if they could be contracted out&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a class=&quot;footnote&quot; href=&quot;https://www.dursi.ca/feed.xml#fn:1&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.  A year from now, with VPRs earnestly looking for budget cuts, researchers increasingly comfortable with getting research computing support over Zoom and Miro, an increased emphasis on data-sharing and thus remote data infrastructures, and some research computing teams better able to communicate their value than others, there will be consolidation and sorting of researching data computing teams.&lt;/p&gt;

&lt;p&gt;Very small groups - a couple of specialists embedded in a large (especially health-related) research group, or a handful of research computing experts in a large corporate IT shop - are likely safe as long as they support research that continues to be funded as they’re too small a target to be worth cutting.   But medium-sized centres with vague goals and priorities who can’t communicate the value they bring are going to be called upon to justify their existence.&lt;/p&gt;

&lt;p&gt;As this shakes out, funding will favour small, hyper-specialized teams who deeply understand some segment of local needs, and large regional centres with diversified funding sources, excellent communications, and clear goals and priorities that enter contracts with other institutions and groups.&lt;/p&gt;

&lt;p&gt;There isn’t going to be a dramatic “big bang” of closures, dissolutions, or mergers. Instead, straitened circumstances and very broad acceptance of virtual research support and data infrastructure will accelerate trends that have already been visible.  And it’s going to be lead by individual contributors who are about to realize their employment options have significantly increased.&lt;/p&gt;

&lt;h3 id=&quot;more-adoption-of-industry-best-practices-for-running-computer-systems&quot;&gt;More adoption of industry best practices for running computer systems&lt;/h3&gt;

&lt;p&gt;Research software quality takes a lot of of (unjustified) guff, but the truth is that with version control, unit tests, CI/CD, and packaging, research software development is &lt;em&gt;much&lt;/em&gt; closer to industry best practices than research computing systems operations is.&lt;/p&gt;

&lt;p&gt;With health data applications becoming increasingly important, that will have to change.  Privacy restrictions around PHI will require better controls, documentation, and processes, including security incident reporting.  Emphasis on data sharing and availability will push teams towards higher availability SLAs, which will push towards on-calls and practices like, if not chaos-engineering, at least routine testing of failures as with &lt;a href=&quot;https://slack.engineering/disasterpiece-theater-slacks-process-for-approachable-chaos-engineering/&quot;&gt;“disasterpiece theatre”&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;portfolios-of-research-computing-systems-are-going-to-be-rebalanced-away-from-big-metal&quot;&gt;Portfolios of research computing systems are going to be rebalanced away from “big metal”&lt;/h3&gt;

&lt;p&gt;As with research computing teams, this isn’t going to be a big bang or a sudden pivot, but an acceleration of trends already in place.&lt;/p&gt;

&lt;p&gt;With greater emphasis on data and health applications, very large-scale physical science simulations (my own background) will be an even smaller, while still important, use case for research computing.  With greater emphasis on remote data infrastructures, remote teams, and data sharing, commercial cloud adoption in research computing will continue to grow.  On-premises infrastructure is going to continue to tilt away from being able to support small numbers of large simulations towards architectures which can provide more flexibility for a wider range of computing and data applications.&lt;/p&gt;

&lt;h2 id=&quot;what-does-it-mean-for-us&quot;&gt;What does it mean for us?&lt;/h2&gt;

&lt;p&gt;Like the mainstreaming of telemedicine, many of the consequences of the pandemic will just be pushing forward something that was always going to happen eventually but had lacked an impetus until now.  And for many (most?) research computing team individual contributors, things will look pretty good - work-from-home will open up more job opportunities, even if the portfolio of projects they support starts looking different.&lt;/p&gt;

&lt;p&gt;But for research computing teams as organizations, and for their managers, the coming years will be treacherous.  If the research computing team supporting University research groups doesn’t have to be on campus any more, why do they have to be University employees at all?  If a neighbouring centre has better-run systems with better availability and already handle PHI, why not just use them for research software development support too?&lt;/p&gt;

&lt;p&gt;It is not too early to start upping your game when it comes to the adminstration, your researchers, and your team members.  For the administration, you’re going to have to ensure that you can justify every budget item in terms the administration recognize and value, and that you have clear and focussed goals and priorities. For researchers, you can start making sure that your systems, processes, and practices are as high-quality and researcher-focussed and -friendly as possible.  For your team members, if you’re not regularly communicating with them to make sure they’re happy in their current roles and with their career development, this is the time to start.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;See for instance table 2 of &lt;a href=&quot;https://www.srainternational.org/blogs/srai-jra1/2019/12/09/operational-fiscal-management-of-core-facilities&quot;&gt;Carter &lt;em&gt;et al&lt;/em&gt;.&lt;/a&gt;, where VPRs 2:1 would prefer service contracts for HPC centres over in-house options (of an admittedly small sample). &lt;a class=&quot;reversefootnote&quot; href=&quot;https://www.dursi.ca/feed.xml#fnref:1&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;

    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Things I Learned from Looking at 500 Research Computing Manager Jobs over 10 Months</title>
   <link href="https://hpc.social/2020/things-i-learned-from-looking-at-500-research-computing-manager-jobs-over-10-months/"/>
   <updated>2020-10-14T01:00:00-06:00</updated>
   <id>https://hpc.social/2020/things-i-learned-from-looking-at-500-research-computing-manager-jobs-over-10-months</id>
   <content type="html">&lt;p&gt;I write a weekly &lt;a href=&quot;https://newsletter.researchcomputingteams.org&quot;&gt;newsletter&lt;/a&gt;
for research computing managers, team leads, or those aspiring to
those roles.  One of the things I’ve wanted to emphasize in the
newsletter is that managing research computing teams is a profession
in and of itself, and worth doing well.  Part of that is emphasizing
the existence of career opportunities.&lt;/p&gt;

&lt;p&gt;So since the beginning I’ve included job listings and  maintained
a &lt;a href=&quot;https://www.researchcomputingteams.org/jobs&quot;&gt;job board&lt;/a&gt;,
posting about 500 such jobs over the past 10 months and removing
them as they become filled or otherwise unavailable.  My main
criteria for such jobs are whether or not I would describe the work
as principally about managing or leading a research computing team - 
admittedly a fuzzy definition.&lt;/p&gt;

&lt;p&gt;Over the course of examining those 500 jobs - and looking through
many many more that never made it to the board - I’ve learned some
things:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;There are a lot of jobs out there for people managing research
computing teams&lt;/strong&gt;.  I’ve &lt;em&gt;never&lt;/em&gt; had any trouble finding some weekly
to put in the job board or with highlights interesting enough to
list at the end of the newsletter.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;There are certainly many more I’m missing&lt;/strong&gt;.  As the field matures
there are starting to be &lt;a href=&quot;https://us-rse.org/jobs/&quot;&gt;job&lt;/a&gt;
&lt;a href=&quot;https://society-rse.org/careers/vacancies/&quot;&gt;boards&lt;/a&gt; for research
software development or for particular sub-fields of research
computing like
&lt;a href=&quot;https://bioinformatics.ca/job-postings/#/?&amp;amp;order=desc&quot;&gt;bioinformatics&lt;/a&gt;.
But, consistent with research’s neglect of management as something
that needs to be done and done well, no such resources exist for
the managers of those important roles. So I have a go-to list of
google and other searches for jobs which I go through a couple of
times a week.&lt;/p&gt;

&lt;p&gt;In research, when you’re doing a literature search and you start
hitting the same papers again and again, you’re pretty sure you’ve
got a mostly complete list of references as a starting point.  I’m
nowhere near that with my managing research computing teams job
list, largely because the names we use for these roles vary so
widely.  So I’m confident that I only see a fraction of these jobs.
(You can help out by &lt;a href=&quot;https://airtable.com/shrL6QGic3Mv9JFrs&quot;&gt;submitting any
jobs&lt;/a&gt; you know about).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Research computing teams are broadening, and so is the need for
managers&lt;/strong&gt;.  Where this is most obvious is in data science or data
engineering teams, which have spread to every sector and every
industry.  Generic “Manager, Data Science” jobs are so plentiful
that I don’t list most of them - many of them are more operational
rather than “jobs leading research computing teams” - but even the
ones that make the cut are in sectors from health to transportation
to retail to engineering.  There are increasingly data engineering,
cloud architecture, etc roles for supporting research computing
efforts, to say nothing of ML/AI jobs.  And there are countless
management/team lead jobs for specialist research computing in
health, biology, and biomedicine.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Research data management is increasingly employable&lt;/strong&gt;.  As the
initial data science and data engineering work in organizations
mature, many institutions are realizing that they now need principled
approaches to data governance, stewardship, and modelling.  This
is happening most rapidly in heavily regulated industries —
health, finance — but is starting to percolate outwards.
Those who have maintained and curated data resources for research,
or who have supported those that do, will be surprised at the number
of jobs in the private sector for doing similar work.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;“Traditional” research computing team management jobs remain, and
they take forever to fill&lt;/strong&gt;: There are definitely still routinely
“Director of Research Computing, University of Somethingorother”
jobs out there.  And I don’t know whether it’s because of the
pandemic, or because of the competition from other sectors, but
such jobs are taking forever to fill this year.  I routinely see
them open for months, and then reposted one or more times.  I see
this in both for managers of teams running on-premises hardware and
for teams mainly doing software development.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Despite the talk of RSE units, most research computing jobs within
academic institutions are lone outposts&lt;/strong&gt;:  While in companies
research computing - data science, computing resource management,
software development - tends to be centralized (even if it is
matrixed out or embedded into other teams), in academia we’re
definitely not there - most of the team leads/manger jobs I see in
Universities are for small teams embedded in a single institute or
project.  I think that’s a shame; it greatly reduces the opportunity
for cross-pollination, learning, and developing best practices,
makes work less efficient and less satisfying, and it makes teams
more management heavy than they need to be.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>White Managers in Research Computing, We Need to be Speaking Out About Racism, then Listening and Advocating</title>
   <link href="https://hpc.social/2020/white-managers-in-research-computing-we-need-to-be-speaking-out-about-racism-then-listening-and-advocating/"/>
   <updated>2020-06-05T01:00:00-06:00</updated>
   <id>https://hpc.social/2020/white-managers-in-research-computing-we-need-to-be-speaking-out-about-racism-then-listening-and-advocating</id>
   <content type="html">&lt;p&gt;Many people in our research computing community — and in the broader research community we serve — are in pain this week.  There’s another video of another Black man, George Floyd, begging for his life while being murdered by a police officer in Minneapolis.  Here in Toronto a Black woman, Regis Korchinski-Paquet, died when what should have been a routine call resulted in a mystifying number of police officers showing up.  With only police officers present in her apartment, she went over her high-rise balcony to her death, &lt;a href=&quot;https://www.cbc.ca/news/canada/toronto/regis-korchinski-paquet-toronto-police-1.5590296&quot;&gt;with her last words being, repeatedly, “Mom, help”&lt;/a&gt;.  This is all taking place during a pandemic which is disproportionately killing and incapacitating Black people, Indigenous people, and people of colour because they have less access to jobs that can be worked from home, and are more likely to be living in overcrowded multi-generational homes.&lt;/p&gt;

&lt;p&gt;So with news and social media being dominated by the consequences of systemic racism, anti-Black violence in particular, and &lt;a href=&quot;https://brutality.glideapp.io&quot;&gt;police violence&lt;/a&gt; in reaction to anti-police-brutality protests, a lot of people are feeling despair and anguish.&lt;/p&gt;

&lt;p&gt;As managers, we are leaders of communities.  Small communities, but nonetheless.  We have a responsibility to members of those communities to let them know we support them and are here for them.  It doesn’t take much to be small bit of genuine help to &lt;a href=&quot;https://twitter.com/FutureDrDukes/status/1267508084143865859&quot;&gt;someone really struggling&lt;/a&gt;.  But we have to initiate the conversations.  Our community members won’t open up to us about these topics until we’ve demonstrated we can have some kind of adult conversation about racism.&lt;/p&gt;

&lt;p&gt;Doing or saying something is scary for many of us in research computing — who are overwhelmingly not Black and mostly white, which is a related conversation we need to have — because we are worried, reasonably, about getting it wrong.   And it’s easy to make the excuse that because we don’t have Black team members (which… you know, same) it’s not something we need to address.&lt;/p&gt;

&lt;p&gt;Most of us don’t have team members who have gotten sick with COVID-19 either, but we’ve certainly been addressing that.  It’s been hard and uncomfortable and we didn’t get it all right the first time around and we did it anyway.  You don’t necessarily know who’s hurting in your team and community or why.  Not addressing a topic dominating the news and social media now doesn’t project professionalism, it just suggests discomfort or indifference.&lt;/p&gt;

&lt;p&gt;I do not have great suggestions about what to say or do.  I can offer some articles and collections of resources I’m finding useful:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://blacktechpipeline.substack.com/p/hey-employers-do-black-lives-matter&quot;&gt;How Black employees want to be supported by their employers during times of protest for BlackLivesMatter&lt;/a&gt; - Pariss Athena&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/the-establishment/welcome-to-the-anti-racism-movement-heres-what-you-ve-missed-711089cb7d34&quot;&gt;Welcome To The Anti-Racism Movement — Here’s What You’ve Missed&lt;/a&gt; - Ijeoma Oluo&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.google.com/document/d/1a-lzdtxOlWuzYNGqwlYwxMWADtZ6vJGCpKhtJHHrS54/preview?pru=AAABcpnrylg*2vEMb2In8-9aRyfg0OKSuA&quot;&gt;Anti-Racist Resource Guide&lt;/a&gt; - Victoria Alexander&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.google.com/document/d/1BRlF2_zhNe86SGgHa6-VlBO-QgirITwCTugSfKie5Fs/preview?pru=AAABcocJWsk*HmXb3HkF-szmJL5SmeHugg&quot;&gt;Anti-Racism Resources&lt;/a&gt; - Sarah Sophie Flicker, Alyssa Klein&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.google.com/document/d/1PrAq4iBNb4nVIcTsLcNlW8zjaQXBLkWayL8EaPlh0bc&quot;&gt;Scaffolded Anti-Racism Resources&lt;/a&gt; - Anna Stamborski, Nikki Zimmermann, Bailie Gregory&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I can also tell you what I’m doing at work.  I’ve raised the issue at our all hands meeting using words much like the above, and let people know they can talk to me about it if they need to.  Unhelpfully, I sounded a bit awkward, even after practicing, but the next conversation will be easier.  I’ve made a point of checking in a little deeper with people during one-on-ones, and doing a lot of listening,  I’m listening for feedback even when it’s uncomfortable, and I’ll keep reading those materials, and others, to see what I can do better and how I can support change.&lt;/p&gt;

&lt;p&gt;That’s not the best or even a particularly good way to address what’s going on now and what’s been going on for a very long time. It’s the bare minimum, and started too late. The challenge will come when making changes, then advocating for more change to peers and upwards. But it’s a start.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;From &lt;a href=&quot;https://buttondown.email/ljdursi/archive/ed9c986b-6007-426d-a8c7-9f49b3b0d107&quot;&gt;issue #27&lt;/a&gt; of the &lt;a href=&quot;https://buttondown.email/ljdursi/archive/&quot;&gt;Research Computing Teams newsletter&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Exascale's long shadow and the HPC being left behind</title>
   <link href="https://hpc.social/2020/exascale-s-long-shadow-and-the-hpc-being-left-behind/"/>
   <updated>2020-05-20T18:33:00-06:00</updated>
   <id>https://hpc.social/2020/exascale-s-long-shadow-and-the-hpc-being-left-behind</id>
   <content type="html">&lt;p&gt;The delivery of Japan’s all-CPU Fugaku machine and the disclosure of the UK’s all-CPU ARCHER 2 system amidst the news, solidly “pre-Exascale” machines with pre-exascale budgets, is opening old wounds around the merits of deploying all-CPU systems in the context of leadership HPC.  Whether a supercomputer can truly be “leadership” if it is addressing the needs of today using power-inefficient, low-throughput technologies (rather than the needs of tomorrow, optimized for efficiency) is a very fair question to ask, and Filippo took this head-on:&lt;br /&gt;&lt;br /&gt;&amp;lt;blockquote class=&quot;twitter-tweet&quot; style=&quot;display: block; margin: auto;&quot;&amp;gt;&amp;lt;div dir=&quot;ltr&quot; lang=&quot;en&quot;&amp;gt;Unfortunately take codes from Tier-2 with GPU to Tier-1 without GPU is a &lt;em&gt;huge&lt;/em&gt; step backward. These calls are holding back the true potential of &lt;a href=&quot;https://twitter.com/hashtag/GPU?src=hash&amp;amp;ref_src=twsrc%5Etfw&quot;&gt;#GPU&lt;/a&gt; computing in accelerating scientific discovery! &lt;a href=&quot;https://t.co/qVVEWFDXt1&quot;&gt;https://t.co/qVVEWFDXt1&lt;/a&gt;&amp;lt;/div&amp;gt;
— Filippo Spiga (@filippospiga) &lt;a href=&quot;https://twitter.com/filippospiga/status/1263072225781047297?ref_src=twsrc%5Etfw&quot;&gt;May 20, 2020&lt;/a&gt;&amp;lt;/blockquote&amp;gt;&lt;br /&gt;&lt;br /&gt;Of course, the real answer depends on your definition of “leadership HPC.”  Does a supercomputer qualify as “leadership” by definition if its budget is leadership-level?  Or does it need to enable science at a scale that was previously unavailable?  And does that science necessarily have to require dense floating point operations, as the Gordon Bell Prize has historically incentivized?  Does simulation size even have anything to do with the actual impact of the scientific output?&lt;br /&gt;&lt;br /&gt;While I do genuinely believe that the global exascale effort has brought nearly immeasurable good to the HPC industry, it’s now casting a very stark shadow that brings contrast to the growing divide between energy-efficient, accelerated computing (and the science that can make use of it) and all the applications and science domains that do not neatly map to dense linear algebra.  This growing divide causes me to lose sleep at night because it’s splitting the industry into two parts with unequal share of capital.  The future is not bright for infrastructure for long-tail HPC funded by the public, especially since the cloud is aggressively eating up this market.&lt;br /&gt;&lt;br /&gt;Because this causes a lot of personal anxiety about the future of the industry in which I am employed, I submitted the following whitepaper in response to an NSCI RFI issued in 2019 titled “&lt;a href=&quot;https://www.federalregister.gov/d/2019-12866&quot;&gt;Request for Information on Update to Strategic Computing Objectives&lt;/a&gt;.”  To be clear, I wrote this entirely on my personal time and without the permission or knowledge of anyone who pays me–to that extent, &lt;a href=&quot;https://twitter.com/hpcprogrammer/status/1261480678866259974?s=21&quot;&gt;I did not write this as a GPU- or DOE-apologist&lt;/a&gt; company man, and I did not use this as a springboard to advance my own research agenda as often happens with these things.  I just care about my own future and am continually trying to figure out how much runway I’ve got.&lt;br /&gt;&lt;br /&gt;The TL;DR is that I am very supportive of efforts such as Fugaku and Crossroads (contrary to &lt;a href=&quot;https://twitter.com/hpcprogrammer/status/1261483277506019335?s=21&quot;&gt;accusations otherwise&lt;/a&gt;), which are looking to do the hard thing and advance the state of the art in HPC technology without leaving wide swaths of traditional HPC users and science domains behind. Whether or not efforts like Fugaku or Crossroads are enough to keep the non-Exascale HPC industry afloat remains unclear.  For what it’s worth, I never heard of any follow-up to my response to this RFI and expect it fell on deaf ears.&lt;br /&gt;&lt;br /&gt;&amp;lt;h2&amp;gt;Response to “Request for Information on Update to Strategic Computing Objectives”&amp;lt;/h2&amp;gt;G. K. Lockwood&lt;br /&gt;August 17, 2019&lt;br /&gt;&lt;br /&gt;&amp;lt;h3&amp;gt;Preface&amp;lt;/h3&amp;gt;This document was written as a direct response to the Request for Information on Update to Strategic Computing Objectives (Document Number 2019-12866) published on June 18, 2019.  All views expressed within are the personal opinion of its author and do not represent the views or opinions of any individuals or organizations with whom the author may or may not be associated in any professional or personal capacities.  This document was authored without the support, knowledge, or input of any such individuals or organizations, and any similarity between the opinions expressed here and any other individuals or organizations is purely coincidental.&lt;br /&gt;&lt;br /&gt;&amp;lt;h3&amp;gt;Question 1. What are emerging and future scientific and technical challenges and opportunities that are central to ensuring American leadership in Strategic Computing (SC), and what are effective mechanisms for addressing these challenges?&amp;lt;/h3&amp;gt;&lt;br /&gt;While the NSCI Strategic Plan identified four overarching principles which are undeniably required to maintain continued American leadership, its five strategic objectives are, in many ways, mutually incompatible with each other.&lt;br /&gt;&lt;br /&gt;In the three years following the initial NSCI plan towards delivering capable exascale, the outcomes of the Aurora and CORAL-2 procurements within DOE have made undeniably clear that the definition of “capable exascale” necessarily requires the use of GPU technologies.  Because GPUs are, in many ways, accelerators specifically suited for scientific problems that can be reduced to dense linear algebra, this has effectively signaled that scientific challenges which are not reducible to dense linear algebra (and therefore incompatible with GPU technologies) are, by definition, no longer of strategic significance.&lt;br /&gt;&lt;br /&gt;By bifurcating science domains based on whether they are or are not compatible with GPU-based acceleration, we are now at a crossroads where entire classes of domain science research that have historically run at-scale on CPU-based leadership computing systems will be left behind.  To be clear, this is not simply a matter of engineering—many important classes of scientific challenges are fundamentally incompatible with the GPU accelerator model of computation, and no amount of code modernization will change this fact.  Yet these same science domains, which rely on complex multiphysics applications that are core to strategic areas such as stockpile stewardship and climate science, are of undeniably critical importance to both national security and society at large.&lt;br /&gt;&lt;br /&gt;Thus, there is now a clear and growing gap between NSCI’s ambition to deliver capable exascale and the larger mission to maintain leadership in entirety of truly strategically important computing in the nation.  There are technical challenges intrinsic in this growing gap which include pursuing research in hardware and software technologies that approach strategic computing more holistically rather than exclusively from a FLOPS perspective.  The community has long acknowledged that the scope of HPC has surpassed simply performing floating point operations, and the definition of capability computing now includes enabling science that, for example, may require tremendous data analysis capabilities (e.g., moving, transforming, and traversing massive data sets) but have relatively low floating point requirements.  The DOE Crossroads procurement and the Japanese leadership program and its Fugaku system embody this more balanced approach, and there is little doubt that both Crossroads and Fugaku will demonstrate a number of world’s-firsts and, by definition, demonstrate leadership in strategic computing without making all of the sacrifices required to meet today’s definition of capable exascale.&lt;br /&gt;&lt;br /&gt;Both Crossroads and Fugaku have required significant R&amp;amp;D investment to enable these dimensions of capability, and the NSCI would do well to explicitly call out the need for continued investment in such directions that are orthogonal to exaflop-level capability.&lt;br /&gt;&lt;br /&gt;&amp;lt;h3&amp;gt;Question 2. What are appropriate models for partnerships between government, academia and industry in SC, and how can these partnerships be effectively leveraged to advance the objectives of SC?&amp;lt;/h3&amp;gt;&lt;br /&gt;The most impactful models for industry-government partnership in HPC have come in the form of close collaboration between the HPC facilities that deploy extreme-scale systems and the technology providers in industry that create and support the required hardware and software solutions.  Strategy necessarily involves taking input from user requirements, workload characterization, and technology trends to inform future directions, and HPC facilities are uniquely qualified to speak to both user requirements (by virtue of the fact that they directly interact with users in support of HPC systems) and workload characterization (by virtue of the fact that they manage HPC systems).  Complementarily, industry technology providers (vendors) are uniquely qualified to speak to technology directions, marketability, and sustainability in the larger technology market.&lt;br /&gt;&lt;br /&gt;This effective collaboration can take the form of non-recurring engineering such as those contracts associated with large system procurements (often to address more tactical challenges towards strategic computing) or standalone programs such as DOE PathForward (which addresses longer-term technology development towards strategic computing).  In both cases though, industry (not HPC facilities or academic researchers) propose the initial scope of work based on their own understanding of both (1) HPC-specific requirements and (2) larger market and profit prospects.  This latter point is critical because the HPC market alone is simply not large enough to sustain purpose-built technologies, and sustaining new technologies and their peripheral enabling ecosystems requires buy-in from multiple markets.&lt;br /&gt;&lt;br /&gt;The role of academia in research is more complex, as academic research in HPC can be either basic or applied in nature.  Basic research (such as in applied mathematics and algorithm development) has stood on its own historically since such work results in a larger base of knowledge from which specific technology solutions (whether developed by industry or HPC facilities) can be composed both today and in the future.  The federal agencies participating in NSCI can claim credit for funding the basic research outcomes that have been incorporated into innumerable software and hardware technologies in use today. &lt;br /&gt;&lt;br /&gt;On the other hand, applied research (such as developing new software systems that may implement the outcomes of basic research) has had very mixed outcomes.  It is often the case that applied researchers who have no direct relationship with neither HPC facilities nor technology providers formulate research projects based on second-hand HPC requirements and technology trends.  It follows that their interpretation of such requirements is incomplete, and their research outcomes are misaligned with the actual needs of HPC facilities and industry.  Barring cases where academic applied research outcomes are so valuable that they stand on their own (of which there are many examples including OpenMPI and Tau), applied research in the absence of such a sustainability path results in a tremendous amount of software that has virtually no long-term (i.e., strategic) value to SC.&lt;br /&gt;&lt;br /&gt;This speaks to a gap between applied research in academia and those who apply research in practice that must be closed.  This gap has been perpetuated by a lack of HPC practitioners (domain scientists and applied researchers directly attached to HPC facilities or technology providers) on the committees that evaluate the merit of research.  Thus, a more effective engagement model would involve coupling the academic research pipeline to HPC facilities and industry more closely.  This may be something as informal as increasing the diversity of review panels and program committees to include representatives from facilities and industry to a formal requirement that successful research proposals have a clearly defined connection to a specific industry or facility partner.  Regardless of the solution though, funding applied research that will be “thrown over the wall” to HPC facilities and vendors without their input is not compatible with SC.&lt;br /&gt;&lt;br /&gt;&amp;lt;h3&amp;gt;Question 3. How do we develop and nurture the capable workforce with the necessary skill and competencies to ensure American leadership in SC? What are effective nontraditional approaches to lowering the barriers to knowledge transfer?&amp;lt;/h3&amp;gt;&lt;br /&gt;Although virtually every report discussing strategic directions and future requirements of HPC call for knowledge transfer and building a larger workforce through training and outreach (e.g., see the complete set of &lt;a href=&quot;https://exascaleage.org/&quot;&gt;DOE Exascale Requirements Reviews&lt;/a&gt;), such reports generally neglect two critical realities of employing and retaining a talented workforce at production HPC facilities and in industry.&lt;br /&gt;&lt;br /&gt;The first reality is that the problems intrinsic to modern HPC (solving problems at extreme scales) are no longer exclusive to HPC.  The ubiquity of technology in modern life now means that the entire technology industry must deal with problems at scale as a matter of course.  As such, the HPC community is now competing with well-capitalized commercial entities that have increased the absolute value of a skilled engineer to levels that the scientific research community simply cannot afford.&lt;br /&gt;&lt;br /&gt;Thus, the perceived lack of skilled workforce in HPC is not a failing of the workforce development strategy in place; in fact, it may be a great indicator of its success, as it has created a workforce whose skills have value that far outstrip the investment put into workforce development.  However, this also means that the talented individuals who eschew the higher pay and amenities of working in the larger technology industry do so for non-monetary reasons (work-life balance, attraction to the science mission, geographic locality).  It is therefore critically important that strategic computing identify these motivators and built upon them to the greatest possible degree to maintain an edge in an extremely competitive hiring landscape.&lt;br /&gt;&lt;br /&gt;The second reality is that the key to an exceptional workforce is not simply a matter of technical knowledge.  There is no shortage of individuals who understand parallel programming in the world, and it is of little strategic value to pursue workforce development strategies that prioritize knowledge transfer as the principal outcome.  Rather, strategic computing requires a workforce that is capable of critical thinking and has a natural drive to solve problems that have never been solved before.  These traits should be emphasized to a far greater degree than the current pedagogical emphasis on material that can be learned from a manual by anyone with a curious mind.&lt;br /&gt;&lt;br /&gt;By definition, very few people in the world have prior experience in world-class HPC.  There are very limited opportunities to build a credible work history in extreme-scale HPC for individuals who are ineligible for student internships or postdoctoral appointments.  As a result, world-class HPC facilities rarely see qualified applicants for open positions when “qualified” is defined on the basis of relevant work experience; a mid-career developer or systems engineer working in a campus-scale HPC organization simply has no opportunities to demonstrate his or her intellectual capability in a way that is outstanding to the facilities that deliver strategic computing resources.&lt;br /&gt;&lt;br /&gt;Thus, an integrative approach to workforce development that (1) emphasizes problem-based learning rather than rote reiteration of manuals and standards documents in an environment where (2) representatives from NSCI constituent agencies can engage with trainees (i.e., potential employees) in a fashion with less formality and pretense than a typical “CV-phone screen-interview” pipeline may reveal a much broader potential workforce whose strengths more closely align with strategic computing.  Such an approach may manifest in the form of intensive boot camps such as the DOE ATPESC program, grants for mid-career retraining in partnership with a leadership computing facility, or sabbatical support for technical staff at the nation’s mid-scale computing facilities.&lt;br /&gt;&lt;br /&gt;&amp;lt;h3&amp;gt;Question 4. How can technical advances in SC and other large government and private initiatives, including infrastructure advances, provide new knowledge and mechanisms for executing next generation research?&amp;lt;/h3&amp;gt;&lt;br /&gt;No response.&lt;br /&gt;&lt;br /&gt;&amp;lt;h3&amp;gt;Question 5. What are the future national-level use cases that will drive new computing paradigms, and how will new computing paradigms yield new use cases?&amp;lt;/h3&amp;gt;It is easy to claim that artificial intelligence will be the most important future national use case to drive new computing paradigms.  However, this is a very dangerous statement to make without qualification, as the actual level of readiness for applying AI to solve scientific problems is very low, and the actual scales, aggregate demand, and algorithmic motifs required by such workloads for scientific discovery are poorly undefined.  More generally, the requirements of AI workloads at large remain uncertain; for example, the Facebook uses a variety of AI techniques in production and have found that each application area requires different computational, storage, and network resources (see &lt;i&gt;&lt;a href=&quot;https://research.fb.com/wp-content/uploads/2017/12/hpca-2018-facebook.pdf&quot;&gt;Applied Machine Learning at Facebook: A Datacenter Infrastructure Perspective&lt;/a&gt;&lt;/i&gt;).  Outside of the large hyperscale datacenters, industry consensus suggests that production AI workloads remain largely at single-server scales.  As such, it is difficult to confidently assert what the rate of scale-out AI will be for strategic computing.&lt;br /&gt;&lt;br /&gt;The current leading technique for AI at scale is deep learning, yet scientific discovery is at odds with the black-box nature of this method.  Alternative methods such as decision trees offer much more insight into why a trained model behaves as it does and is more compatible with applying physical constraints to which physical systems being modeled (e.g., see &lt;i&gt;&lt;a href=&quot;https://doi.org/10.1073/pnas.1711236115&quot;&gt;Iterative random forests to discover predictive and stable high-order interactions&lt;/a&gt;&lt;/i&gt;).  However, the relative importance of such non-block-box learning techniques in HPC are completely unknown, as are the general optimization points for such techniques in the context of scientific computing.  There is a danger that the similarities between deep learning and many HPC problems (GEMM-heavy workloads) place an artificially high importance on the role of deep learning in SC.  It may be the case that deep learning is the most effective method for applying AI to address problems in scientific computing, but caution must be taken to ensure that major challenges in SC not all look like deep-learning nails simply because GPUs are a very effective hammer.&lt;br /&gt;&lt;br /&gt;From a domain science perspective, there are very few domain sciences where AI can replace traditional simulation-driven workflows wholesale.  As such, the role of AI in SC will be largely supplementary; scientific workflows may integrate an AI component to generate starting conditions, replace humans in the loop during steering, or identify areas of interest in the results of a primary simulation.  However, it is very unlikely that AI will grow to be of greater significance to scientific computing than modeling and simulation.  Instead, it will be the source of new computational resource requirements that simply did not exist in the past because those tasks were carried out by humans.  The road towards integrating AI into scientific workflows will also be a long and tortuous one, as the field is evolving far more rapidly in industry than scientific computing traditionally has.  Care must be taken that SC not tie itself too closely to a method (and its associated hardware configurations) that may be deprecated in short order.&lt;br /&gt;&lt;br /&gt;&amp;lt;h3&amp;gt;Question 6. What areas of research or topics of the 2016 NSCI Strategic Plan should continue to be a priority for federally funded research and require continued Federal R&amp;amp;D investments? What areas of research or topics of the 2016 Strategic Plan no longer need to be prioritized for federally funded research?&amp;lt;/h3&amp;gt;&lt;br /&gt;The five objectives outlined in the 2016 NSCI Strategic Plan all gravitate around elements of topics that require continued federal R&amp;amp;D investments, but they do require realignment with the technological, scientific, and economic landscape as it exists now.&lt;br /&gt;&lt;br /&gt;&amp;lt;h4&amp;gt;Objective 1: accelerating the development of capable exascale by the mid-2020s&amp;lt;/h4&amp;gt;The 2016 NSCI report correctly stated that capable exascale technologies would not be available until the mid-2020s, but DOE pulled its exascale system deliveries into the early 2020s.  As a result, the delivery of exascale had to be accelerated at significantly higher costs: there have been significant capital costs (the first US exascale systems will cost between 2x and 10x their immediate predecessors, either setting a new bar for the cost of future leadership HPC systems or resulting in a bubble in funding for all post-exascale machines), operational costs (the power budgets may exceed the original 20 MW goal by 50%), and opportunity cost (only two of the three CORAL labs actually deployed a CORAL-1 machine).&lt;br /&gt;&lt;br /&gt;Notably absent here is a commensurate increase (2x-10x, 1.5x, or 1.3x as above) in R&amp;amp;D efforts towards making these exascale systems widely accessible to applications that do not fall under the umbrella of ECP funding.  As such, NSCI must continue to emphasize the importance of funding R&amp;amp;D to enable the “capable” component of this objective through the mid-2020s at minimum.&lt;br /&gt;&lt;br /&gt;&amp;lt;h4&amp;gt;Objective 2: Developing a coherent platform for modeling, simulation, and data analytics&amp;lt;/h4&amp;gt;The convergence of HPC and Big Data was a popular point of discussion when the 2016 report was written, but there has yet to be a compelling, quantitative analysis that demonstrates the difference between a “Big Data” system and an “HPC” system despite the best efforts of several leadership-scale HPC facilities.  The challenge is not one of technology and system architecture; rather, the principal design point for “Big Data” systems outside of the HPC world has simply been one of cost (e.g., scaling out cheap hardware over a cheap network for a very well-defined bulk data access pattern) over performance.  There is absolutely nothing that stops the typical “Big Data” application stacks, both old (e.g., Hadoop and Spark; see &lt;a href=&quot;https://doi.org/10.1109/BigData.2016.7840606&quot;&gt;this paper&lt;/a&gt;) and new (e.g., TensorFlow; see &lt;a href=&quot;https://dl.acm.org/doi/10.5555/3291656.3291724&quot;&gt;this paper&lt;/a&gt;) from running at scale on any modern HPC systems, and both have been demonstrated at scale on systems that were sensibly designed.&lt;br /&gt;&lt;br /&gt;As such, this objective need not be emphasized in the future.  Rather, engineering work is required to enable the “Big Data” stacks in use outside of HPC to work efficiently on the HPC systems of tomorrow.  This remains a software, not architectural, problem, and very much an engineering, not research, challenge.&lt;br /&gt;&lt;br /&gt;&amp;lt;h4&amp;gt;Objective 3: R&amp;amp;D towards post-CMOS technologies and new paradigms&amp;lt;/h4&amp;gt;It is not the role of NSCI constituent agencies to fund the development of new materials systems explicitly for post-CMOS computing, because these agencies, their review committees, and the academic researchers they fund do not have the insight into the realities of logistics, material costs, and manufacturing required to predict what combination of materials and microarchitectures could actually be turned into a marketable product that can be sustained by the larger technology industry.  In the absence of this insight, R&amp;amp;D towards post-CMOS technologies is likely to produce interesting demonstrations that are impractical for the purposes of actually developing leadership-scale computing systems.  Instead, such research should be funded using facility-industry partnerships as discussed previously in Question 2.&lt;br /&gt;&lt;br /&gt;Investing in R&amp;amp;D towards new paradigms in computing should also be considered not with respect to enabling new scientific applications, but rather accelerating existing scientific workloads that are incompatible with exascale technologies (GPUs).  As discussed in response to Question 1, there is a very real risk of leaving entire domains of computational science behind as the definition of leadership computing (when equated to exascale) becomes increasingly narrow in scope.  Developing new accelerator technologies that are of benefit to complex application workflows (e.g., multiphysics simulations) are of critical importance in the coming years missions such as stockpile stewardship and climate science fall by the wayside.&lt;br /&gt;&lt;br /&gt;&amp;lt;h4&amp;gt;Objective 4: Improving application development and workforce development&amp;lt;/h4&amp;gt;The DOE Exascale Computing Project (ECP) has demonstrated a highly effective way of integrating researchers, application code teams, and facilities towards improving application development.  Providing a coherent ecosystem of recommended methods (such as its IDEAS project; e.g., see &lt;a href=&quot;https://ideas-productivity.org/ideas-ecp/&quot;&gt;ECP-IDEAS&lt;/a&gt;), development tools (funded under its Software Technologies area), algorithm-application partnerships (through its co-design centers), and application integration efforts (funded under Hardware and Integration area) are an excellent blueprint for improving application development.  Developing a more generic model for establishing and supporting this style of development beyond the timeline of the ECP funding should be pursued.&lt;br /&gt;&lt;br /&gt;Improving workforce development should reduce its focus on basic technical training and more on improving critical thinking as described in the response to Question 3 above.&lt;br /&gt;&lt;br /&gt;&amp;lt;h4&amp;gt;Objective 5: Broadening public-private partnership&amp;lt;/h4&amp;gt;As described in the response to Question 2 above, public-private partnership is absolutely critical to sustain SC in the coming years.  The financial incentives driving technology development from the world outside of HPC have come to outstrip the resources available to HPC to exist independently.  SC efforts must engage with both technology providers and the primary market forces (the enterprise and hyperscale computing industries) to better understand where technologies, solutions, and opportunities can be pursued in partnership rather than in parallel.&lt;br /&gt;&lt;br /&gt;&amp;lt;h3&amp;gt;Question 7. What challenges or objectives not included in the 2016 NSCI Strategic Plan should be strategic priorities for the federally funded SC R&amp;amp;D? Discuss what new capabilities would be desired, what objectives should guide such research, and why those capabilities and objective should be strategic priorities?&amp;lt;/h3&amp;gt;The mission of providing capable exascale as described in the 2016 NSCI Strategic Plan is proving to be not a sustainable long-term path.  As described in the response to Question 1 above, the first exascale machines stand to accelerate scientific problems that can be cast as dense matrix-matrix multiplication problems, but there are large swaths of scientific problems to which this does not apply.  If one considers the Graph500 BFS list, three of the top five systems are over seven years old and will be retired in 2019.  While graph problems are not prolific in SC, the fact that such little progress has been made in accelerating extreme-scale graph traversal during the seven years that exascale has been aggressively pursued is indicative of some classes of HPC problems being abjectly left behind.&lt;br /&gt;&lt;br /&gt;Thus, a primary objective towards capable exascale must be examining the opportunity costs of the current strategic direction.  If it is determined that there is simply no way to bring forward those types of computational problems that are incompatible with GPU-based acceleration, then a clearer strategy must be formulated to ensure that the scientific challenges being solved by those computational problems do not stagnate.  As it stands, the public discourse surrounding the first-generation US exascale architectures is not universally positive because of this perceived scientific exclusivity of the chosen architectures, and such exclusivity is at odds with both capable computing and computing leadership.&lt;br /&gt;&amp;lt;div&amp;gt;&lt;br /&gt;&amp;lt;/div&amp;gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>COBOL, Imperial College, Bursty Maintenance, and Sustained Scientific Software</title>
   <link href="https://hpc.social/2020/cobol-imperial-college-bursty-maintenance-and-sustained-scientific-software/"/>
   <updated>2020-04-18T01:00:00-06:00</updated>
   <id>https://hpc.social/2020/cobol-imperial-college-bursty-maintenance-and-sustained-scientific-software</id>
   <content type="html">&lt;p&gt;We’ve all read about the huge rise in unemployment claims causing
unprecedented loads on US state software systems, with the situation
so dire that the governor of New Jersey put out &lt;a href=&quot;https://qz.com/1832988/covid-19-results-in-new-jersey-desperately-needing-cobol-coders/&quot;&gt;an urgent call
for COBOL programmers&lt;/a&gt;.
It’s worth looking at this from the point of view of research
software, where we need software to be sustainable and reproducible
for long periods of time.&lt;/p&gt;

&lt;p&gt;The systems that need suddenly need COBOL developers have often
been chugging away with maintenance and tweaks for 40–50
years.  This is an almost unfathomable success in the world of
software. So the current issue clearly isn’t with the quality of
the software itself &lt;em&gt;per se&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Is COBOL being “obsolete” the problem?  I mean, look
at that record of success again.  COBOL is a proven, &lt;a href=&quot;https://hackernoon.com/i-took-a-cobol-course-and-it-wasnt-the-worst-z1ba3yrp&quot;&gt;perfectly
serviceable&lt;/a&gt;,
domain-specific language for these sorts of batch tasks. There’s
ways to connect to tools and services written in other languages,
so it can coexist with other systems.  The lack of (say) a vibrant and
rapidly-evolving ecosystem of third-party packages isn’t necessarily
a bad thing here. (How innovative and cutting-edge do you want the
system that sends out your pension cheques to be, exactly, when the
time comes? Do you really want someone to accidentally
&lt;a href=&quot;https://qz.com/646467/how-one-programmer-broke-the-internet-by-deleting-a-tiny-piece-of-code/&quot;&gt;leftpad&lt;/a&gt;
your bank account?)&lt;/p&gt;

&lt;p&gt;Yes, people coming in to maintain the software for the first time
will have to familiarize themselves with a new, old, language.  But
people in research or open-source software learn an unfamiliar language to
contribute to a code base every day. Even if they knew the language,
they would still have to learn the codebase itself, the idioms, and
the problem domain. All of those things can be quickly learned by
new developers if there is documentation and tests, and especially
if there are people who have recently been maintaining the code
base to help.  And that’s the issue here.&lt;/p&gt;

&lt;p&gt;These COBOL systems weren’t poorly designed, or obsolete, or a bad
match to their requirements.  Easily handling 100x the previously
expected maximum rate of applications isn’t a feature, it’s a symptom
of giddy overengineering.  The requirements just changed suddenly.
And when that happened, the people, procedures, and resources weren’t
in place to do the necessary maintenance.&lt;/p&gt;

&lt;p&gt;There is no such thing as infrastructure which does not require
maintenance, and the need for that maintenance is often quite bursty.
This is just as true in research software as it is in governmental
systems.  Research software which goes into production needs to be
written in a maintainable fashion, but that’s not enough.  There
has to be funding support to keep in place the people, procedures,
and resources necessary to maintain that software, likely in bursts.
And those resources have to remain in place between bursts.&lt;/p&gt;

&lt;p&gt;The bursty nature of necessary maintenance has also come up in
research software, in the saga of the &lt;a href=&quot;https://twitter.com/neil_ferguson/status/1241835454707699713&quot;&gt;Imperial College epidemic
modelling
software&lt;/a&gt;.
When COVID-19 arrived, this tool suddenly moved from a mildly
interesting research code to a key input into UK domestic policy.
Transparency and flexibility leapt from being nice-to-haves to key
requirements, and the people, procedures, documentation, tests, and
resources weren’t in place to add them.&lt;/p&gt;

&lt;p&gt;The importance and urgency of epidemic modelling meant that expertise
and resources from many places were made available to extend and
eventually rewrite the code. But this isn’t a sustainable model for
research computing software, any more than it is for unemployment
application processing systems.&lt;/p&gt;

&lt;p&gt;We still genuinely don’t know how to reliably provide maintenance, bursty
or otherwise, for software, shared databases, or systems in
our research communities.  Our funding models are all built around
supporting experiments, observations, or theoretical works —
short-term projects which start, proceed, result in publications
and other research outputs, and are then done.  Mechanisms for ongoing support of evolving
research &lt;em&gt;inputs&lt;/em&gt; isn’t even a work in progress — it’s absent.&lt;/p&gt;

&lt;p&gt;If experimental methods work develops new kinds of equipment or
reagents which are useful to other researchers, then a vendor starts
manufacturing and selling those items to researchers, with money
that comes out of their grants — and that’s the sustainability
model.  We don’t have that for ongoing efforts in software, databases,
or even reliably for hardware shared at a larger scale than a single
organization yet.&lt;/p&gt;

&lt;p&gt;For software undergoing active development, there are at least
plausible approaches proposed.  Some of them look,
reasonably enough, like the research equipment model above.  Add a
modest amount of money to grants earmarked for distribution to
software, databases, or systems that the research group relies on.
Maybe that would work!  But it would almost certainly preferentially
fund projects that are being actively worked on, taking feature
requests and bug reports for software or new submissions for
databases.&lt;/p&gt;

&lt;p&gt;For mature, quiescent resources that “just work” and
so fade into the background, the tools that don’t need development
until they suddenly do, we need other solutions.  Likely we need
centres of expertise in research computing, populated by professionals
as advocated by &lt;a href=&quot;https://society-rse.org&quot;&gt;RSE societies&lt;/a&gt; &lt;a href=&quot;https://us-rse.org&quot;&gt;around
the world&lt;/a&gt;, with named maintainers even for
research tools actively used but not actively developed.&lt;/p&gt;

&lt;p&gt;People —
&lt;a href=&quot;https://bssw.io/blog_posts/maintainers-drive-software-sustainability&quot;&gt;maintainers&lt;/a&gt;,
with the tools to do their job — are what drive software
sustainability, not language choices or technologies.  As a research
community we need to find and retain funding to retain, develop,
and empower those people to do their work.  Otherwise we’re going
to waste time and effort urgently re-learning and re-creating tools
when individually unforeseeable but collectively predictable bursts
in maintenance are needed.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>How To Quickly Start One-on-Ones with your Research Computing Team- A One-Week Plan of Action</title>
   <link href="https://hpc.social/2020/how-to-quickly-start-one-on-ones-with-your-research-computing-team-a-one-week-plan-of-action/"/>
   <updated>2020-03-24T01:00:00-06:00</updated>
   <id>https://hpc.social/2020/how-to-quickly-start-one-on-ones-with-your-research-computing-team-a-one-week-plan-of-action</id>
   <content type="html">&lt;p&gt;Research computing teams around the world are finding themselves working completely remotely suddenly.  As a manager, you’ve gotten over the first hump and made sure everyone has the tools they need - software, VPN access, accounts on whatever chat and videoconferencing tools you’ll need.  Now what?&lt;/p&gt;

&lt;p&gt;We all know that &lt;a href=&quot;https://www.extension.harvard.edu/professional-development/blog/challenges-managing-virtual-teams-and-how-overcome-them&quot;&gt;remote teams need more communication&lt;/a&gt; than on-site teams, so you’ll need to start communicating more.  This is a perfect time to start doing one-on-ones if you haven’t been doing them already.&lt;/p&gt;

&lt;p&gt;What follows is a one-week plan to get started doing one-on-ones with your newly-remote research computing team.  For each weekday, there’s about 10 minutes of reading and another 10-15 minutes of homework to do to get you started doing one-on-ones with your team starting one week from when you begin.  There’s follow-up activities in weeks two and three to take stock, make tweaks, and start thinking about tools that will help.&lt;/p&gt;

&lt;p&gt;This document is available in &lt;a href=&quot;https://www.dursi.ca/assets/quickstart-one-on-ones/quickstart-one-on-one.pdf&quot;&gt;pdf&lt;/a&gt; and &lt;a href=&quot;https://www.dursi.ca/assets/quickstart-one-on-ones/quickstart-one-on-one.epub&quot;&gt;epub&lt;/a&gt; formats.  You can also sign up below to get 
the material sent to you one day at a time in a series (your email won’t
be used for &lt;em&gt;anything&lt;/em&gt; else except sending you the material below.)&lt;/p&gt;

&lt;h1 id=&quot;day-1---background-and-planning&quot;&gt;Day 1 - Background and Planning&lt;/h1&gt;

&lt;p&gt;Even on-site, one of the most important things a manager can do with their teams is to have regular one-on-one meetings with each of their team members.  This practice is almost ubiquitous in tech companies and many other industries.  The fact that there are tools, websites, podcasts, and videos about it might lead you to think they’re complicated; they’re not. They’re super simple.  Those resources all exist because one-on-ones are important and people are trying to help.  Some of those resources are quite good, and I’ll provide some pointers to some of them that I think are particularly relevant in our research computing context; but you don’t &lt;em&gt;need&lt;/em&gt; any of them.&lt;/p&gt;

&lt;p&gt;One-on-ones are just meetings with each individual team member and you; they get a half hour of your completely undivided attention, every week (or at &lt;strong&gt;worst&lt;/strong&gt;, every other week).  The basic principles of successful one-on-one meetings are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The meeting is scheduled and at a regular time every week.&lt;/li&gt;
  &lt;li&gt;This is about building working relationships.&lt;/li&gt;
  &lt;li&gt;This isn’t a status update: the meeting is about your team member, not you.&lt;/li&gt;
  &lt;li&gt;So, the the team member and their agenda goes first, every time.&lt;/li&gt;
  &lt;li&gt;Take notes in a way that shows you’re paying attention.&lt;/li&gt;
  &lt;li&gt;Followup is crucial.&lt;/li&gt;
  &lt;li&gt;When in doubt, imagine having one-on-ones with someone you report to.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And that’s it.  There’s no tricks or particularly hard parts!  If you follow the principles above when making decisions, and are disciplined enough to keep the meetings going even (especially) when things are busy, pay attention to what you’re hearing, and follow up, you’re going to to have successful one-on-ones.&lt;/p&gt;

&lt;p&gt;Simple as they might be, these meetings are going to be most effective way to achieve four important things:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Understand your team member better and so build solid working relationships.&lt;/li&gt;
  &lt;li&gt;Build trust with your team member.&lt;/li&gt;
  &lt;li&gt;Make your team member feel more important and engaged, and likely to raise issues with you.&lt;/li&gt;
  &lt;li&gt;Learn &lt;em&gt;much&lt;/em&gt; more about what’s actually going on with the work your team is doing.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;On top of those benefits, most managers find that these meetings actually save them time - people will save up questions they have for the one-on-ones rather than asking them as they come up, you’ll be able to better match people to tasks (and be able to find out how better to direct them on those tasks), and if anything starts to go sideways you’ll find out about it faster.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Your assignment:&lt;/strong&gt;  Let’s get things started by finding potential one-on-one slots on your calendar starting a week from today; in a couple of days you’ll be sending the list of those timeslots out to your team members to sign up for their one-on-ones.  Look for a few more slots than team members - if you have 8 team members, aim to find say 12 slots.  Identify 30-minute slots on your calendar for that week, ideally with a bit of padding on either side to prepare and review your notes.  Prioritize slots that are normally free in coming weeks, and start doing what you can to fence those times off for a bit.  Normally we’d be doing this for three weeks out or so when our calendars are mostly empty except for recurring meetings - here we’re doing them in a hurry, and I know you already have a bunch of things lined up for next week.  But this is important, so if you have to, consider rescheduling or cancelling some other low priority meetings next week to make room.   List the slots in a google spreadsheet, or start a doodle poll with them as the entries, and in a couple of days we’ll get people to sign up for them.&lt;/p&gt;

&lt;p&gt;Also: if you’re having a weekly team meeting today, give them a heads up that because you’re now all remote you’d like to start meeting with them one-on-one every week, and you’ll be sending an email out.&lt;/p&gt;

&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/@stephen_younge/your-team-just-went-remote-now-what-b643e58fad61&quot;&gt;Your Team Just Went Remote, Now What&lt;/a&gt; by Stephen Younge talks about the importance of communications and one-on-ones in the current moment&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://shop.oreilly.com/product/0636920056843.do&quot;&gt;The Manager’s Path&lt;/a&gt; and &lt;a href=&quot;https://www.goodreads.com/book/show/324750.High_Output_Management&quot;&gt;High Output Management&lt;/a&gt; are new and classic tech-industry management books respectively that talk about extensively and early on about one-on-ones&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;faqs&quot;&gt;FAQs&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Q: I have 12 people directly reporting to me - I just don’t think I can find 6 hours a week to spend on this.&lt;/strong&gt;&lt;br /&gt;
A: I promise you that this is one of the most productive ways you can be spending your management time — certainly in difficult periods like this, but even when things get back to normal.  You don’t have to take my word for it - ask around if you know anyone doing one-on-ones in their team, and see what they say.  Doing these meetings will mean you’ll be less stressed as a manager, have a better handle on what’s going on in your team, be able to put fires out sooner, have a forum for giving your team members feedback and coaching, be better able guide your team members skills and career development, and your team members will know that you’re listening to them.  If you’re still skeptical, phase it in - start every-other week and move to weekly after you have a few rounds under your belt.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q: Ok, 12, sure, but I’m a PI with a lab of 30 people.  How’s that supposed to work?&lt;/strong&gt;&lt;br /&gt;
A: Thirty people is just too many people to give effective management to - you aren’t meaningfully tracking the needs, career development, and progress of thirty people &lt;em&gt;now&lt;/em&gt; on top of everything else you need to do, and your lab members already know it.  One-on-ones aren’t magic; they can’t fix that.  So you’ll have to pick and choose between some options.&lt;/p&gt;

&lt;p&gt;Perhaps you’ll prioritize trainees and some senior staff: have one-on-ons with them, and after a couple of rounds so that they understand how it works, have the senior staff start one-on-ones with more junior staff, even if there’s no formal reporting relationship there.   That’s not as good as them having one-on-one time with you, but it’s better than no one having one-on-one time with anyone, and it starts preparing senior staff or trainees for future management duties.  Every so often you could make sure you have “skip-level one-on-ones” with the staff or trainees who are having one-on-ones with these senior lab members - individually or as a group meeting - to make sure things are going well.&lt;/p&gt;

&lt;p&gt;Alternately, you could just have bi-weekly one-on-ones with everyone; that’s 7.5 hours a week providing direct hands-on management with your team members.  Again, it’s not as good as weekly one-on-ones but it is significantly better than not having them.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q: So if biweekly is ok, can I do these monthly?&lt;/strong&gt;&lt;br /&gt;
A: A useful way to think about one-on-ones from your team members point of view is to imagine the situation with you having one-on-ones with someone more senior to you - your boss if you’re a staff manager, or a department chair or dean if you’re a PI.  How would you treat a regular monthly half-hour one-on-one meeting with someone you report to?&lt;/p&gt;

&lt;p&gt;I don’t think that this is a stretch to imagine.  You’d want this precious 30 min a month to go as well as possible.  You’d spend some time preparing a dog-and-pony show, probably some slides or something, and prioritize and hone a small number of questions you need answers to.  It would be A Big Deal, and so kind of stressful, each time.&lt;/p&gt;

&lt;p&gt;Your boss would get a sanitized view of what’s going on.  You’d get a chance to look good to a boss whose been around a while and recognizes this as the highly polished view that it is.  You’d maybe get a few answers you needed, or a promise to get back to you on those questions, which is good - but at the cost of significant preparation and stress.&lt;/p&gt;

&lt;p&gt;Monthly &lt;em&gt;just isn’t worth doing&lt;/em&gt;.  Bi-weekly isn’t great - you won’t save as much time on interruptions and questions, because short questions that can wait three days for an answer often can’t wait a week or more - but it’s not bad.  Weekly is the best.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q: My schedule gets changed at the last minute a lot - I’m not sure I can always keep the same time every week for these meetings!&lt;/strong&gt;&lt;br /&gt;
A: It’s ok!  Your team members know you’re busy.  Stuff gets rescheduled. They understand, really.  The important thing isn’t that you’re meeting Frieda at 2pm every Thursday come what may; the important thing is that Frieda knows she always has 30 minutes on your schedule every week.  Just reschedule them as needed, just like you’d do with any other important meeting - for the same week, and as soon as you know something’s coming up.&lt;/p&gt;

&lt;h1 id=&quot;day-2---understanding-what-the-one-on-ones-will-look-like&quot;&gt;Day 2 - Understanding What The One-on-Ones will Look Like&lt;/h1&gt;

&lt;p&gt;You know the basic principles of each one-on-one meeting now; what does this mean for how the meetings will go?&lt;/p&gt;

&lt;p&gt;First, let’s talk about the medium.  This is about building working relationships, and while it’s probably not literally impossible to do that over text-based chat, it’s hugely slower and more prone to missteps.  On top of that, there are going to eventually be sensitive topics discussed in these one-on-ones: you want the team member to be able to tell you about problems they’re having with co-workers, or that they didn’t do so great at something, or the stress of working at home with the uncertainty of the pandemic, and they might reasonably be reluctant to put these concerns into writing and send them off into the internet.&lt;/p&gt;

&lt;p&gt;So the options are some kind of video/teleconference, or just phonecalls.  Videoconferencing is better, because it lets you show facial expression and some body language; that goes a long way towards conveying intent and reducing miscommunications.  You probably already use some tools for videoconferencing, and whatever you use is fine.  I personally have no problem recommending:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://This is about building working relationships.&quot;&gt;Zoom.us&lt;/a&gt; - rock solid, requires installation of an app on desktop or mobile, free for 40-minute  one-on-one calls which works perfectly here.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://slack.com/video-conferencing&quot;&gt;Slack videoconference&lt;/a&gt; - requires a paid plan which is extremely reasonably priced for academic or non-profit work; less solid but works reasonably well and if you’re already using slack it’s one fewer application to juggle.  A downside is that you can’t do video on mobile.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://whereby.com/user&quot;&gt;Whereby&lt;/a&gt; - Free for up to four people, works in browser on desktop or on mobile via free apps; pretty solid, only requires the host to register an account.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But really, if you already use something else and your team members have access to it, just use it, it’s fine.  If you’re not already using something, pick whereby or zoom. And if for some reason none of those are available, just plan to make phone calls.&lt;/p&gt;

&lt;p&gt;So that’s the medium, what’s the message?  There are a number of suggested high-level agendas for one-on-ones out there.  I’m going to recommend going with the &lt;a href=&quot;https://www.manager-tools.com/map-universe/basics&quot;&gt;Manager Tools One-on-Ones&lt;/a&gt; agenda: it’s super simple, I’ve seen it work very well in a number of research computing contexts, it’s well motivated by empirical data, and I think it makes the best starting point.  If you’ve seen and used a different agenda that’s worked well, feel free to use it instead; otherwise, use theirs.&lt;/p&gt;

&lt;p&gt;The Manger Tools agenda is:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;10 minutes for what your team member wants to talk about&lt;/li&gt;
  &lt;li&gt;10 minutes for items on your agenda&lt;/li&gt;
  &lt;li&gt;10 minutes to focus on their career or skills development&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let’s tackle these in order:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Their agenda:&lt;/strong&gt; 10 minutes for what they want to talk about.&lt;/p&gt;

&lt;p&gt;It’s hard to give you much information here, since your various team members will have different topics they want to cover.  A number will misunderstand and try to give updates on their tasks; try to gently discourage that and nudge them toward higher-level topics.&lt;/p&gt;

&lt;p&gt;They will also likely have questions about the effort, their role, what’s coming next; this is a great opportunity for discussion.  They’ll have questions about the current situation that you are unlikely to be able to answer, and you’ll just have to be honest about what you do and don’t know.  They’ll talk about how things are currently working with the team and may bring up things you want to follow up on.&lt;/p&gt;

&lt;p&gt;But there will also be a lot of not particularly work-related discussion they want to have with you.  Maybe it’ll be about their new home-office setup, or their pets, or their family, or the world situation.  They may want to lament the loss of sports (and talk about their favourite team), or talk about a new dish they’re learning to cook now that they can’t go to one of their neighbourhood restaurants.  Remember that the purpose of this meeting is to build a working relationship with your team member, to understand them better and to open up lines of communications.  It’s a privilege to have them sharing this with you.  Take notes and ask lots of questions — you’ll learn a lot from the answers — and share in kind if you feel comfortable doing so.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Your agenda:&lt;/strong&gt; 10 minutes for your topics.&lt;/p&gt;

&lt;p&gt;Remember, the one-on-one isn’t a status update meeting, and using it as one wastes an opportunity.  You’re the boss; you can ask for status updates any time you want, and it’s something easily covered over chat or email.  These meetings are better for higher level topics - talking about &lt;em&gt;why&lt;/em&gt; they’re having troubles, or great successes, with particular tasks, what’s coming next, and the like.  If you want, you can routinely check on status of tasks before the one-on-one, and then that can inform the discussion.  The goal here isn’t to get a weekly update on what tasks are slipping behind - the goal here is in the medium term to have fewer tasks slipping behind because you better understand the work and your team members.&lt;/p&gt;

&lt;p&gt;This is a good opportunity to give positive or corrective feedback on things you’ve seen over the past few days; in times of change like now, this feedback can be a useful way to reinforce expectations around things like code reviews or meetings that may look a little different now then they did several weeks ago when there was more in-person interaction.&lt;/p&gt;

&lt;p&gt;You might well want to followup from things from previous meetings, or things that have come up from the past weeks work; to ask questions about things you saw.  In general, even during your time, the more questions you can be asking that draw information from the team member, rather than just telling them stuff, the better.&lt;/p&gt;

&lt;p&gt;It’s also a great time to share some updates about the big-picture view of how things are going in your department or effort, and how it will effect them and their work, or opportunities it might give them in the future.  It’s a great opportunity to gauge their level of enthusiasm for new aspects of upcoming work.&lt;/p&gt;

&lt;p&gt;Don’t use one on ones to share information that you’re going to be sharing with several members of the team, though.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;It’s not a good use of this time.  This meeting is about the individual team member, so it should cover things specifically about them.  (Covering how something you’ve updated the entire team about will effect &lt;em&gt;them in particular,&lt;/em&gt; however, is completely on-topic.)&lt;/li&gt;
  &lt;li&gt;It’s not a good use of your time.  If it’s something several team members need to know about, cover it at your weekly team meeting and save yourself some time.&lt;/li&gt;
  &lt;li&gt;It’s surprisingly prone to causing problems.  Efficiency aside, you’re probably going to accidentally convey slightly different messages each time - at the very least, they’ll each hear slightly different things.  The &lt;em&gt;best&lt;/em&gt; case is that this leads to unnecessary confusion.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Professional Development:&lt;/strong&gt; 10 minutes&lt;/p&gt;

&lt;p&gt;This isn’t something that comes out of your time or their time, because professional development is a shared responsibility between the two of you.  A first one-on-one is a great chance to update your baseline on what their long-term career goals are, and give them any guidance or resources they ask for; and to find out if there are particular aspects of the research they’d like to get more involved in or new technologies they’d like to learn.  In future one-on-ones, you can cover upcoming conferences or presentation opportunities, try to find work opportunities that will help them build experience they’re looking for, or coach them on particular skill development.&lt;/p&gt;

&lt;p&gt;Once you’ve been doing those for a while, you’ll find that there usually isn’t going to be 10 minutes worth of things to say about their career or skills development - their development needs or long-term career goals just aren’t going to change much week-to-week.  So after a while it’ll more typically be 15 minutes for them, 15 minutes for you.  But it’s still hugely valuable to have a recurring slot for these topics to be discussed, and your first few one-on-ones there’ll probably be more than enough topics there to have it take the time.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Your assignment:&lt;/strong&gt;  For each of your team members, start a document or (a page in a document) listing them, what you know of their career plans, what they’re working on, things you’ve been meaning to talk with them about, your current understanding of their strengths and weaknesses, how you imagine their role might change over the coming year… anything that would be useful to touch on in a one-on-one.  It doesn’t need to be anything like comprehensive - it’ll get added to over the course of many one-on-ones - but it’ll be a good starting point to preparing for the first round of meetings.&lt;/p&gt;

&lt;p&gt;Also: if today’s your weekly team meeting today, give them a heads up that because you’re now all remote you’d like to start meeting with them one-on-one every week, and you’ll be sending an email out.&lt;/p&gt;

&lt;h2 id=&quot;resources-1&quot;&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;The entire Manager Tools Basics series, and especially their &lt;a href=&quot;https://www.manager-tools.com/map-universe/basics&quot;&gt;one-on-ones podcasts&lt;/a&gt;, are extremely widely followed and are definitely worth listening to.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;faqs-1&quot;&gt;FAQs&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Q: What if they don’t have anything to say or ask for their 10-15 minutes?&lt;/strong&gt;&lt;br /&gt;
A: Team members that are kind of distrustful about this new practice might be kind of reticent to talk or ask questions.  It may take them several meetings to really start trusting this whole one-on-one thing and start opening up. That’s ok; one of the purposes of having these meetings is exactly to open up lines of communications.&lt;/p&gt;

&lt;p&gt;Long silences can be helpful here; ask them what they’d like to talk about, smile patiently and look at the camera, and count out 10 seconds in your head.  That 10 seconds will feel like a while to you, and an absolute eternity to them, but it’s very convincing that you’re interested in waiting to hear what they have to say.  If they’re still not coming up with anything, you can try prompting them with very broad questions — “How is this whole remote work going for you?” “How are you dealing with the COVID-19 situation?” “Do you feel our approach to working remote is going well?”.  It’s better if the topics and questions come unprompted, but that may take some time.  It’s alright.&lt;/p&gt;

&lt;p&gt;Frankly though, by far the more common issue is:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q: What if they go long?&lt;/strong&gt;&lt;br /&gt;
A: This is &lt;em&gt;way&lt;/em&gt; more common than there being not enough to say, especially at the beginning.  If they’re not used to having your undivided attention, they’re going to have a lot to say, especially for the first few meetings.  My suggestion is just let them go long for the first few.  If there are things you really want to cover on your agenda, gently interrupt them towards the end with time to cover your priorities - but remember, you’re the boss, you can talk to them whenever you want about stuff; this is their half hour to bring things to you.  If they keep going long week after week, start giving them feedback about that and gradually over the weeks get them to 10-15 minutes.  But don’t be too forceful about it; these meetings are about opening channels of communication.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q: I just have something quick to ask, can we start with that before we get into their stuff?&lt;/strong&gt;&lt;br /&gt;
A: Absolutely, categorically, no.  I don’t blame you, I’ve slipped and done this myself, but it sabotages the whole meeting - now it’s just another meeting with the boss quizzing the team member about stuff they want to know about.  Seriously, you can wait 15 minutes - or just ask them on slack (or whatever) before the meeting.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q: Can I ask about status of their tasks at all?&lt;/strong&gt;&lt;br /&gt;
A: Talking about tasks’ status isn’t completely off-limits, but it’s &lt;em&gt;really easy&lt;/em&gt; for status questions to slowly take over the one-on-ones.  If there’s a task or project you two haven’t talked about for a while, by all means you can take this opportunity to ask for a quick update, but try to make sure that’s the exception not a rule.  This is a better venue for identifying problems that keep coming up and coaching them on dealing with those, or building on strengths they’ve shown in dealing with other tasks - high level things rather than quick updates.  Again, you’re the boss - you can ask for status updates any time you want.  This time is for them.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q: What if they offer status updates during their time?&lt;/strong&gt;&lt;br /&gt;
A: Listen, and take notes; and pay attention to anything they’re saying beyond the status update (are they pointing out things they had to overcome that you didn’t know about?  Are they talking in a way that suggests they’re not normally getting enough credit?).  If it is really just a status update, thank them but explain that this isn’t a status update meeting, you really want to focus on higher-level issues — whether they have what they need to succeed, whether there are things they want to know about the program as a whole, how things are going with their coworkers, what have they learned working on project X, are there things that they’re concerned about for the coming weeks.  If a few weeks of gentle redirection isn’t enough, you can be more direct (or you can try to short-circuit things by directly asking for status updates before the one-on-one).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q: What if I don’t have a full 10-15 minutes of things to cover?&lt;/strong&gt;&lt;br /&gt;
A: That’s ok!  The main purpose of this meeting is for them to talk.  We’ll introduce later some general questions you can raise in your time if you don’t have specific things you need to address; but otherwise, if your list is short this week, give them a heads up so they can use more time this time; and if the meetings are short sometimes, it’s fine.&lt;/p&gt;

&lt;h1 id=&quot;day-3---sending-out-the-invitations&quot;&gt;Day 3 - Sending Out The Invitations&lt;/h1&gt;

&lt;p&gt;You’ve done the hardest parts of preparation now - found spots in your calendar, and identified the starting points for discussions with each of your team members.  Today’s easy, but it’s the key - you send out the email (or slack message, or..) announcing that this is coming and get people to start signing up for the slots you set aside two days ago.&lt;/p&gt;

&lt;p&gt;Make sure that the times you’ve looked up two days ago are ready to be signed up for.  Pick your favourite way of doing that, in rough order of least work-for-you to most:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Google doc sign-up spreadsheet&lt;/strong&gt;: Set up a google doc spreadsheet (or Office 365 Excel sheet or whatever you use) with the times in one column and the next column being who gets that slot.  Make it nice-looking if you want; you now have a signup sheet.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Doodle poll&lt;/strong&gt; - In Doodle under Poll Settings you can “Limit the number of choices per option” to one, so first person to pick the option gets it.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Email&lt;/strong&gt; - just have them email you with their preferred time.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With that done you’re ready to send off the announcement and begin the signups!&lt;/p&gt;

&lt;p&gt;Here’s a perfectly good email  announcing the one-on-ones and asking for signups.  If the letter strikes you as good enough, then fill in the blanks and send it off.  If you want to rework or rewrite it to be in your own voice, absolutely do so: but send it off.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Hi team:&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;With us working remotely now, we don’t get as many opportunties to talk with each other, hear how things are going, and ask questions of each other.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;So I’d like to start having weekly half-hour one-on-one meetings with each of you individually.  These aren’t status update meetings; it’s a meeting for you to tell me how things are going, ask questions about our projects, let me know what you need help with, or to tell me things you need from me.  I’ll have some questions for you about how things are going, and will give any input I have on what’s gone on in the past week.  And we’ll have an opportunity to spend some time each meeting talking about your professional development and career goals.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;We’ll have the same agenda each meeting:&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;First 10 minutes - your agenda.  Whatever you want to talk about or ask questions about.  If there’s a question I don’t know the answer to off hand, I’ll have it for you by the next meeting.  If you want to ask me about something you’re doing and it would help to screen share a plot or diagram, by all means, but please this isn’t something to prepare slides for.  It’s just us talking.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Second 10 minutes - my topics for the week.  I’ll talk about things that have come up in the past week, share updates on efforts that have specific relevance to the work you’re doing, and follow up on things we discussed in earlier one-on-ones.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Third 10 minutes - your professional and career development.  We’ll talk about your career goals and what we can both do to help you get there.  Maybe there are areas of research you’d like to be more involved in, or technologies you’d like to learn more about; if we’re discussing these regularly then I’ll be better able to take them into account when opportunities come up.  After our first few meetings this may not be something we have items to discuss every single week; for those weeks it’ll be 15 minutes/15 minutes.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;We’ll have these meetings over [videoconference].  [Details].&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;I’ve setup a sign up sheet for time slots at [google doc/doodle poll/here in the rest of the email]; let me know your first choice for times (first come, first served!)  After a couple weeks we can adjust the times if we need to.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;We’ll begin on [start date]; I’m looking forward to starting having these conversations with you.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Let me know if you have any questions.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Best wishes,&lt;/em&gt;
    &lt;em&gt;[You]&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Your assignment&lt;/strong&gt;: If today is your weekly team meeting, cover the content of the letter before sending it out.   Then get the times signup ready in whatever format you like.  Rework the email as necessary,  and send it off.  You’re done!&lt;/p&gt;

&lt;h2 id=&quot;resources-2&quot;&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://letterstoanewdeveloper.com/2020/03/16/how-to-manage-one-to-ones/&quot;&gt;How to manage one to ones&lt;/a&gt; - One-on-ones from the team members point of view; you could include this in the email if you wanted.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;faqs-2&quot;&gt;FAQs&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Q: Isn’t this… not a lot of notice for my team?&lt;/strong&gt;&lt;br /&gt;
A: Honestly, yes - we’re ramping this up in a week total.  Normally I’d suggest starting a couple of weeks out to socialize the idea and answer questions (and to make sure your calendar was less full already), but you’re working remotely now and need this a little more quickly.&lt;/p&gt;

&lt;p&gt;And it’s not really &lt;em&gt;that&lt;/em&gt; sudden.  There’s a 60% chance your weekly team meeting has already happened and you’ve gotten a chance to give them a heads-up it’s coming; otherwise you’ll have one coming up and have a chance to talk with them about it.&lt;/p&gt;

&lt;p&gt;Finally - your team knows that things are very uncertain right now, and there have been (and will continue to be) a lot of changes coming up.  That both makes this more reasonable and mean your team will be motivated to want more face to face time with you.&lt;/p&gt;

&lt;p&gt;If you &lt;em&gt;really&lt;/em&gt; think it’s going to be an issue, bump the first few meeting times to the following week to give you and your team an extra couple of days to talk it through.  But it’s not like they have to prepare a presentation or something for the first meeting.  It’s you and each of them talking.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q: What if one of my team members won’t sign up?&lt;/strong&gt;&lt;br /&gt;
A: So imagine you - possibly younger you - getting an offer for more and regular face time with your boss, especially during a time of uncertainty.  Do you run away, or jump on it instantly?&lt;/p&gt;

&lt;p&gt;Once you send this out, most if not all of your team members are going to sign up before the electrons in your email have cooled down.  We’ll talk tomorrow and the next day about how to handle the theoretically-possible occasional reluctant team member; but remember that asking your team members to attend one half-hour meeting a week is a &lt;em&gt;perfectly reasonable thing to ask,&lt;/em&gt; even with short notice, and even under completely normal circumstances.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q: I’m still not sure about this scheduled meetings thing - can’t I just play it by ear every week and schedule them on the fly?&lt;/strong&gt;&lt;br /&gt;
A: I mean, you can &lt;strong&gt;do&lt;/strong&gt; anything you want.  But if you want valuable one-on-one meetings, this is the way to do it.&lt;/p&gt;

&lt;p&gt;“Playing it by ear” just isn’t efficient; if you thought finding gaps in your calendar and matching them to people took some time during this process, imagine doing it every single week.&lt;/p&gt;

&lt;p&gt;But more importantly, these meetings are about your team members, and what matters to them is the certainty of knowing they have time on your schedule.&lt;/p&gt;

&lt;p&gt;Yes, your schedule might get moved around if things get busy — but when things are busy, stuff that’s scheduled probably gets done (even if it gets done at a different time than originally planned), and stuff that isn’t on your schedule likely just gets dropped.&lt;/p&gt;

&lt;p&gt;These meetings are for and about your team members, and for them to be valuable, your team members need the certainty of having first dibs on a slot in your calendar.  Catch-as-catch-can is probably how you’re doing things now; the value in this is setting up something more regular.&lt;/p&gt;

&lt;h1 id=&quot;day-4---answering-signups-and-preparing-notes&quot;&gt;Day 4 - Answering Signups and Preparing Notes&lt;/h1&gt;

&lt;p&gt;So you’ve sent out the email yesterday, and you’re starting to get answers and maybe some questions.  Today you’ll respond to some of those questions and get ready to start.&lt;/p&gt;

&lt;p&gt;First, notice how many of your slots are already filled.  People by and large want time with you, and are happy to have it.  If you’re using google docs for a signup sheet, check out in “version history” the time that first slot got snapped up.  Fast, right?&lt;/p&gt;

&lt;p&gt;Start putting the signups in your calendar as recurring events, with no end date.  (You can always move around individual events, or add an end date later).  If you use email invitations yourself, include the team member on the event and send them an invite, even if you don’t usually do that internally, so they know they’re on your calendar.  Thank them for signing up so quickly, and tell them you look forward to your conversation.&lt;/p&gt;

&lt;p&gt;You might be getting some questions from some team members: what to expect from these meetings, they are a so new, do they need to prepare anything.  Those could be over email/slack, or if you have your weekly team meeting today it could be in person.  These questions are (qualitative) data!  Make note of them in the document you started two days ago for each team member.  These are team members who might need help with change — and there’s a lot of change going on right now, so that’s good to know about — or maybe they are team members you’ll need to work a little harder with developing trust with, and the one-on-ones will help with that.  On the other hand they were comfortable enough with you to raise the question to you directly.  Don’t try to interpret the data yet - just note that they had questions about the one-on-ones and what they were.&lt;/p&gt;

&lt;p&gt;Answer the questions that you get (and if you get several versions of the same question, it might also be useful to send out the answer to the whole team).  Also, it might be good to send out a reminder note to nudge the stragglers.&lt;/p&gt;

&lt;p&gt;The other thing to do today is to get some note-taking forms ready for use during the one-on-one.   The details of the form don’t matter much - there needs to be a place to take notes on what they told you; to put reminders for things you want to talk about; and place to write down things that have to be done for followup (either by you or by them).  We’ll populate the “things you want to talk about” portion tomorrow.&lt;/p&gt;

&lt;p&gt;The key to taking the notes once you’re in the meeting is to (a) take notes like it’s a meeting, not like it’s a class - more about this in a minute - and (b) take notes on paper.&lt;/p&gt;

&lt;p&gt;Yes, on paper.  Like an animal.&lt;/p&gt;

&lt;p&gt;Or on electronic paper - I use my iPad + stylus.&lt;/p&gt;

&lt;p&gt;The key here is to take notes in a way that show you’re paying attention.  Even remotely via teleconference, typing notes in a document doesn’t do that.  Again, when in doubt, imagine yourself in one-on-ones with someone you report to.  You’re talking to them, they’re nodding and typing along; you stop talking, and they keep typing for a minute or so, then look up.  How paid-attention-to do you feel?  I mean, sure, they’re &lt;em&gt;probably&lt;/em&gt; taking notes - or maybe they’re firing off a quick response to an email or sending someone a slack message.&lt;/p&gt;

&lt;p&gt;Remember, a principle goal of these meetings is the team member understanding that they have your undivided attention for a non-trivial amount of time every week.  You leaning off to the side, scribbling something down, and coming up with a pen in your hand occasionally is completely unambiguous.&lt;/p&gt;

&lt;p&gt;On my iPad, I have a notebook for each team member (I use Goodnotes, there’s a lot of other good ones out there too), and before each meeting I set up one page with the date on it for stuff they’re telling me about, one page for me that I can put things that I want to talk about, and I use the bottom of each page for follow-up items.&lt;/p&gt;

&lt;p&gt;The other key note-taking thing is to take notes like you would for a meeting, &lt;strong&gt;not&lt;/strong&gt; like you would for a class.  We in academia-adjacent fields can be really bad about this.  We’ve spent a lot of time in class and lectures and symposia and colloquia writing down all the information being presented like any of it might be on the exam.  That’s easier when you’re &lt;em&gt;doing&lt;/em&gt; the note taking - you don’t have to make decisions about what’s important and what’s not - but it means that it’s more work to &lt;em&gt;use&lt;/em&gt; the notes later, since everything’s there no matter how important it was.  And in our line of work we know that stuff is read many more times than it’s written.&lt;/p&gt;

&lt;p&gt;So while you’re taking notes, try to focus on the important things; specific things they say or do that you want to remember, things that need to be followed up on, surprises, etc.  This will make you a more careful listener, too.  If you’re not sure if something was important, ask a followup question about it and you’ll find out pretty quickly.  Useful multi-purpose followup prompts every research computing manager should have in their toolbox include: “Tell me more about that.” “That sounds tricky.” and “What are some options for handling that?”&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Your assignment&lt;/strong&gt;: As above; add the meetings to your calendar as the signups come in, and answer any questions that you get.  If there are people who haven’t signed up, send a reminder message.  Get some note taking forms ready for your meetings; we’ll populate them with some initial questions and topics tomorrow.  Just make sure there’s some room at the &lt;strong&gt;start&lt;/strong&gt; of the form for what they tell or ask you (because they go first), a place for you to write down things you want to talk about (we’ll put stuff there tomorrow), and a place to take note of things that need to be followed-up on, by either of you.&lt;/p&gt;

&lt;h2 id=&quot;resources-3&quot;&gt;Resources&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.manager-tools.com/2005/07/the-single-most-effective-management-tool-part-1&quot;&gt;Manager-Tools podcast&lt;/a&gt; - At the end of the page (look for downloads) there’s a template form for taking notes and question prompts: the “1-on-1 Key Points and Prep Form”, in PDF or word.  It’s a good starting point for your notes form if you want to use it.&lt;/p&gt;

&lt;h2 id=&quot;faqs-3&quot;&gt;FAQs&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Q: Seriously, on paper?&lt;/strong&gt;&lt;br /&gt;
A: Like an animal, yes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q: C’mon.&lt;/strong&gt;&lt;br /&gt;
A: You c’mon.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q: Ok, fine.  What if one of the filled slots has already been stomped on by a meeting that’s come up for me?&lt;/strong&gt;&lt;br /&gt;
A: It happens, and there’s no sense in pretending it won’t.  Just let the team member know; it’ll give them a model of what might happen in the future.  “Hey Lawrence - I’ve got you in my calendar for 11:30am on Tuesdays now - thanks for signing up!  Something just came up for our slot this week - it’s a one-off and I tried to have it rescheduled but I can’t avoid it.  Can we do 3pm on Thursday for just this week?”  This is one of the reasons you made sure there were a few extra slots.  Maybe X-out that replacement slot from the signup sheet if people are still signing up.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q: Should we set up some kind of shared agenda so we can each see what topics we each have?&lt;/strong&gt;&lt;br /&gt;
A: Some teams find that extremely useful, and some don’t.  Tools inevitably shape the work they’re used for, so you and your team still need to figure out the best way to run these for your particular work first.  Hold off on integrating tooling into these meetings for now until you have a better sense of what you need.  After a couple weeks of one-on-ones is a good time to take stock and see might would be helpful.&lt;/p&gt;

&lt;h1 id=&quot;day-5---preparing-for-the-first-week-of-one-on-ones&quot;&gt;Day 5 - Preparing for the First Week of One-On-Ones&lt;/h1&gt;

&lt;p&gt;This is almost it!  All, or at least most, of your team has signed up for one-on-ones; by the end of the day you will have had your team meeting at some point this week where you’ll have either given the team members a heads-up or answered some questions; you have note sheets ready to be filled out.  Today you’ll handle anyone who hasn’t signed up yet, and figure out how to fill out the forms with your topics for the first one-on-ones.&lt;/p&gt;

&lt;p&gt;It’s really unlikely that more than a full day afterwards you still have someone who hasn’t signed up for a one-on-one slot after you’ve answered questions and sent out a reminder.  I’m including what to do if it &lt;em&gt;does&lt;/em&gt;  happen, because people will ask otherwise, but understand that this isn’t the common case.&lt;/p&gt;

&lt;p&gt;If you do have someone dragging their feet, this is the time to follow up with them.  (And the fact that you have to follow up with them is also data which is worth recording.  Maybe it’s reluctance, maybe they’re overwhelmed, maybe it’s a lot of things — you don’t know yet, but there’s &lt;em&gt;something&lt;/em&gt; there).  Find out if they have questions; be genuinely curious about their reasons for not signing up, don’t assume a reason (this is a time of a lot of change and disruption, we’re all going through a lot right now).  Try to get to the reason it hasn’t been done yet, do what you can to address it, and directly ask them to choose one of the remaining times by the end of the day.&lt;/p&gt;

&lt;p&gt;If by the end of the day they still haven’t signed up, sign them up for one of the remaining slots - the latest one that’s still available, ideally - and send them the invitation.  Yes, this is a little heavy-handed, but you’ve asked them three times now within 48 hours to complete a very simple and reasonable task.  Either they are reluctant because of work-related reasons, or overwhelmed because of potentially non-work-related-reasons, and either way having individual conversations with them is the right thing to do, and your duty as their manager.&lt;/p&gt;

&lt;p&gt;Ok, so by the end of the day you will have everyone signed up for one-on-one slots.  Now it’s time to make sure you know what you’ll be talking about in your time.&lt;/p&gt;

&lt;p&gt;Pull out the document you wrote three days ago covering each team member, and the forms from yesterday, and pick a few easy things you’d like to cover in your first one-on-one together with each of the team members.  Maybe it’s about something coming up that’s relevant to them, maybe it’s a check in on a project that hasn’t come up in a while, maybe you’ve been thinking about getting them to give more presentations and you wonder if that’s something they’d be up for.  Make a note to ask how they’re doing, and whether they think the team is working well in this new mode, and if they have any suggestions for things the team could do differently or start doing.&lt;/p&gt;

&lt;p&gt;You may also want to review some one-on-one question lists (there’s some in the resources section below) to get some ideas for big picture questions you could ask that would draw out information that would help you.  Open-ended questions that can lead into followup questions are often very eye-opening, and you’ll find that ten minutes flies by.&lt;/p&gt;

&lt;p&gt;Note that this is really &lt;strong&gt;not&lt;/strong&gt; the time to bring up that thing that really bugs you but you haven’t mentioned yet because you’ve been waiting to have “the talk”.  This is not the time for “the talk”.  If you start a new practice like one-on-ones and then immediately drop something like that on them, especially if it’s from a while ago and they had no idea, they’ll feel ambushed — and they won’t be wrong.&lt;/p&gt;

&lt;p&gt;Now I’m a big believer in giving corrective feedback (even though I still find it &lt;em&gt;really&lt;/em&gt; hard!) and in research computing, not giving enough negative feedback is a much bigger problem than giving too much.  But this meeting series is about them, and developing a solid working relationship and trust.  When that’s done, then among other things it will be easier for you to give and them to receive respectful, helpful, negative feedback, and have it taken in the spirit it’s intended.  But building that relationship and trust, especially with everything else changing around us, will take time.&lt;/p&gt;

&lt;p&gt;Lean &lt;em&gt;way&lt;/em&gt; into the positive for now.  (Again, big believer in the usefulness of good negative feedback, but people often forget how powerful and effective positive feedback is in encouraging the behaviour that the team needs and helping a team member grow to reach their potential.)  If they’ve been working at anything even approaching their normal productivity these past weeks, there’s lots to give positive feedback about!  It’s ok to point out some small negative things you saw in the last week or so — nudge them about being better with “mute” on Zoom, remind them about that code review that’s still pending, ask them to be on time to the next team call, whatever — but don’t poison these meetings for the team member by introducing some big negative topic early on, and do &lt;em&gt;not&lt;/em&gt; dig up anything that’s more than a couple of weeks old.&lt;/p&gt;

&lt;p&gt;For this week, preparing for the career development section is really easy.  Unless you’ve had this conversation very recently, just make a note to ask them what their medium and long term career goals are now, and what skills and experience they’d like to develop to get there.&lt;/p&gt;

&lt;p&gt;And that’s it.  Everyone’s signed up; the one-on-one forms are ready and waiting; next week the one-on-ones start.  You’re all ready — you can do this!&lt;/p&gt;

&lt;h2 id=&quot;resources-4&quot;&gt;Resources&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://knowyourteam.com/blog/2020/02/19/how-to-coach-employees-ask-these-1-on-1-meeting-questions/&quot;&gt;How to Coach Employees? Ask these One-on-One Meeting Questions&lt;/a&gt; - Claire Lew, KnowYourTeam
&lt;a href=&quot;https://www.peoplebox.ai/t/one-on-one-meeting-template-manager-questions-list&quot;&gt;The Ultimate 1-on-1 Meeting Questions Template&lt;/a&gt; - PeopleBox
&lt;a href=&quot;https://one-on-ones.app&quot;&gt;One-on-Ons app&lt;/a&gt; - Random one-on-one questions&lt;/p&gt;

&lt;h2 id=&quot;faqs-4&quot;&gt;FAQs&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Q: More of a comment than a question - This whole thing is a lot of work?&lt;/strong&gt;&lt;br /&gt;
A: Kind of, yeah.  But it’s a lot more work getting them started than keeping them going.  Once everything is set up, it only takes a few minutes a week per team member in addition to the meetings to get all of the benefits of one-on-ones — which will help your team members and help you.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q: I’ve gone back and forth with one team member and answered their questions and they still seem reluctant; what should I say?&lt;/strong&gt;&lt;br /&gt;
A: “I look forward to speaking more about this with you Wednesdays at 1pm.”&lt;/p&gt;

&lt;p&gt;Seriously, there are &lt;em&gt;very few times&lt;/em&gt; when “because I’m the boss” is a good enough reason; these sort of process details about how the team will work together when you’re now all suddenly working remotely is &lt;em&gt;exactly&lt;/em&gt; one of those times.  It’s good to hear their concerns if they have any, you should respect those concerns, and you should expect them to show up for the one-on-one.&lt;/p&gt;

&lt;h1 id=&quot;day-6---last-minute-reminders&quot;&gt;Day 6 - Last minute Reminders&lt;/h1&gt;

&lt;p&gt;Congratulations - this is the day the one-on-ones start!  You’ve done the hardest part, trust me.&lt;/p&gt;

&lt;p&gt;The first few one-on-ones meetings may seem a little awkward and stilted, but they’ll quickly grow more comfortable as you get the hang of them.&lt;/p&gt;

&lt;p&gt;Keep in mind the principles:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The meeting is scheduled and at a regular time every week.&lt;/li&gt;
  &lt;li&gt;This is about building working relationships.&lt;/li&gt;
  &lt;li&gt;This isn’t a status update: the meeting is about your team member, not you.&lt;/li&gt;
  &lt;li&gt;So, the the team member and their agenda goes first, every time.&lt;/li&gt;
  &lt;li&gt;Take notes in a way that shows you’re paying attention.&lt;/li&gt;
  &lt;li&gt;Followup is crucial.&lt;/li&gt;
  &lt;li&gt;When in doubt, imagine having one-on-ones with someone you report to.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You’ve already got the first one done, by creating the scheduling, and you’ve got the note-taking sorted.  Now you just have conversations with your team members.&lt;/p&gt;

&lt;p&gt;If I could recommend any tactics for the conversations, I’d just say:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;They go first.  Kick off with “What would you like to talk about?”  or something similar to hand over the agenda to them.&lt;/li&gt;
  &lt;li&gt;Listen a lot more than you speak, and ask a lot of high-level questions and followup questions.&lt;/li&gt;
  &lt;li&gt;Let them go long for their part if they want to this time.  The career development conversation can wait another week.  If it gets to 20-25 minutes and you really want to cover your topics this week, see if you can gently interrupt; but remember, you’re the boss, you can talk to them about your stuff any time you want.&lt;/li&gt;
  &lt;li&gt;Take notes and highlight the things to follow up on.  Make sure those followup items end up on your todo list.&lt;/li&gt;
  &lt;li&gt;Focus mostly on the positive for the first few meetings.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Your assignment&lt;/strong&gt;: Have some one-on-ones!&lt;/p&gt;

&lt;h1 id=&quot;day-8---hows-it-going&quot;&gt;Day 8 - How’s It Going?&lt;/h1&gt;

&lt;p&gt;Hey, it’s the middle of one-on-ones week — congratulations!  You got them started, and fast!  How’s it going?&lt;/p&gt;

&lt;p&gt;This is a good time to glance through the notes from the first few one-on-ones.  What are you learning that you didn’t know this time last week?  Have you already helped some of your team members solve a problem they were having?&lt;/p&gt;

&lt;p&gt;Be sure to add the things you said you’d followup on to whatever task list system you use.  Having the conversations with your team members builds a good working relationship; but it’s following up on the things you said you’d do that builds trust.  Did the team member ask you for some information?  To contact someone for them?  To get something un-stuck in administration?  Add it to your list and get them done before the next one-on-one.   That, more than anything else, will prove to them you were listening and care about what you heard.&lt;/p&gt;

&lt;h1 id=&quot;day-10---reviewing-week-one---what-went-well-and-planning-for-week-two&quot;&gt;Day 10 - Reviewing Week One - What Went Well, and Planning for Week Two&lt;/h1&gt;

&lt;p&gt;You’re done your first week of one-on-ones, just 10 work days after starting the process.  Congratulations, this is a big milestone.&lt;/p&gt;

&lt;p&gt;So the benefits I listed on day one of getting started with one-on-ones were:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Understand your team member better and so build solid working relationships.&lt;/li&gt;
  &lt;li&gt;Build trust with your team member.&lt;/li&gt;
  &lt;li&gt;Make your team member feel more important and engaged, and likely to raise issues with you.&lt;/li&gt;
  &lt;li&gt;Learn &lt;em&gt;much&lt;/em&gt; more about what’s actually going on with the work your team is doing.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Building trust will take more than a week; setting that one aside, do you feel that there are team members you already understand a bit better?  How did the team members seem to react to having your attention for 30 minutes?  Did you learn anything about the work being done that surprised you?&lt;/p&gt;

&lt;p&gt;Scan those one-on-one notes again and update the document  on your team members with things you learned.  It could be very work-related things like career goals, or it could be things like the names of their pets - if they told you about it, it’s important to them, so it’s important to you.&lt;/p&gt;

&lt;p&gt;Of the team members, who had a lot to say?  Did some go better than others?   How are you doing with the follow up tasks?  Do those followup tasks suggest new topics of discussion for next week?&lt;/p&gt;

&lt;p&gt;Now start putting together the one-on-one notes for next week.  Have you learned something - maybe a question - that worked really well with one team member and you want to try with others?  What’s come up over the last week that you’d like to talk about?&lt;/p&gt;

&lt;p&gt;Congratulations - you’re done with week one, and already ready for week two!  Preparing this does take a little time each week, and it will always take time, but it will be easier as the weeks go on.&lt;/p&gt;

&lt;h1 id=&quot;day-15---reviewing-week-two---what-went-well-and-thinking-of-future-one-on-ones&quot;&gt;Day 15 - Reviewing Week Two - What Went Well, and Thinking of Future One-on-Ones&lt;/h1&gt;

&lt;p&gt;Fantastic.  Two weeks of one-on-ones!&lt;/p&gt;

&lt;p&gt;You’re now starting to get the hang of this, and seeing what works and what doesn’t.&lt;/p&gt;

&lt;p&gt;This might be a good time to take stock and see if there are things that would help the process go more smoothly.  Have there been topics that have come up that some preparation would have been good for?  Maybe it would be useful to have some kind of shared agenda.  Some groups just have a google doc for each team member shared with the manager, and that can work nicely.&lt;/p&gt;

&lt;p&gt;If that would be useful, consider raising it at the next team meeting.  But for heaven’s sake, before you put any item on it, think about it from the direct’s point of view.  If your one-on-one is a Thursday,  and on Monday you enter an agenda item like “Performance on Project X” or “Your future in the department”, your team member is going to have a &lt;strong&gt;&lt;em&gt;very bad week.&lt;/em&gt;&lt;/strong&gt;  **Be much more explicit and include context: “Project X review: 5 things that went well, one thing to tweak for next time”, or “Opportunities opening up elsewhere in the department”.&lt;/p&gt;

&lt;p&gt;If you’d like more specialized tools, there’s a bunch of promising seeming ones; I’ll list some of heard of below in the resources.&lt;/p&gt;

&lt;p&gt;Those tools might be helpful to you, or might not; our team doesn’t even use a shared agenda, but research computing is incredibly diverse and your teams needs will be different from ours.  If there are any tools you find that make your teams’ one-on-ones easier and more successful, by all means use them (and let me know!)&lt;/p&gt;

&lt;p&gt;You’re done!  This is the end of the 3-week run of emails on starting one-on-ones in a hurry.  If this was valuable to you, consider signing up for the &lt;a href=&quot;https://www.dursi.ca/newsletter.html&quot;&gt;Research Computing Teams Newsletter&lt;/a&gt;; any time something like this is posted it will show up in the newsletter, along with a weekly roundup of management, community, and technology news relevant to you as the manager of a research computing team.&lt;/p&gt;

&lt;p&gt;Congratulations again, and best of luck to you and your research computing team!&lt;/p&gt;

&lt;h2 id=&quot;resources-5&quot;&gt;Resources&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://soapboxhq.com/&quot;&gt;https://soapboxhq.com/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://getlighthouse.com&quot;&gt;getlighthouse.com&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://knowyourteam.com/&quot;&gt;https://knowyourteam.com/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.fellow.app/&quot;&gt;https://www.fellow.app/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://teambit.io/&quot;&gt;https://teambit.io/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.fellow.app/&quot;&gt;https://www.fellow.app/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.small-improvements.com/&quot;&gt;https://www.small-improvements.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;faqs-5&quot;&gt;FAQs&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Q:  So this isn’t as bad as I thought it was going to be, but I’m still not convinced.  Should I just drop them?&lt;/strong&gt;&lt;br /&gt;
A: Do me a favour?  Keep them going for two months.  Have them become part of the routine way you manage.  Get input from your team members.  Then do what you think is best.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>SC'19 Recap</title>
   <link href="https://hpc.social/2019/sc-19-recap/"/>
   <updated>2019-11-27T09:59:00-07:00</updated>
   <id>https://hpc.social/2019/sc-19-recap</id>
   <content type="html">&lt;p&gt;Last week was the annual &lt;a href=&quot;https://sc19.supercomputing.org/&quot;&gt;Supercomputing conference, held this year in Denver&lt;/a&gt;, and it was its usual whirlwind of big product announcements, research presentations, vendor meetings, and catching up with old colleagues.  As is the case every year, SC was both too short and too long; there is a long list of colleagues and vendors with whom I did not get a chance to meet, yet at the same time I left Denver on Friday feeling like I had been put through a meat grinder.&lt;br /&gt;&lt;br /&gt;All in all it was a great conference, but it felt like it had the same &lt;a href=&quot;https://glennklockwood.blogspot.com/2019/06/isc19-recap.html&quot;&gt;anticipatory undertone I felt at ISC 2019&lt;/a&gt;.  There were no major changes to the Top 500 list (strangely, that &lt;a href=&quot;https://www.scmp.com/tech/policy/article/3015997/china-has-decided-not-fan-flames-super-computing-rivalry-amid-us&quot;&gt;mysterious 300+ PF Sugon machine that was supposed to debut at ISC&lt;/a&gt; did not make an appearance in Denver).  AMD Rome and memory-channel Optane are beginning to ship, but it seems like everyone’s got their nose to the grindstone in pursuit of achieving capable exascale by 2021.&lt;br /&gt;&lt;br /&gt;As with every major HPC conference, I approached SC this year with the following broad objectives:&lt;br /&gt;&amp;lt;ol&amp;gt;&amp;lt;li&amp;gt;&lt;b&gt;Sharing knowledge and ideas&lt;/b&gt; by contributing to the technical program and its workshops, tutorials, and BOFs with the goal of getting more momentum behind good ideas and steering research and roadmaps in a direction best aligned with where I think the HPC industry needs to go&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&lt;b&gt;Gathering intelligence&lt;/b&gt; across different technologies and market verticals to stay ahead of where technology and the community may be driving as a result of other parallel industries&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&lt;b&gt;Contributing to community development&lt;/b&gt; amongst storage and I/O researchers and practitioners with the goal of broadening the community and bringing more people and ideas to the table&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&lt;b&gt;Building and maintaining relationships&lt;/b&gt; with individual vendor representatives and peers so that I know to whom I can turn when new opportunities or challenges come up&amp;lt;/li&amp;gt;&amp;lt;/ol&amp;gt;The things I took away from the conference are colored by these goals and the fact that I mostly work in high-performance storage systems design.  If I missed any major themes or topics in this recap post, it was likely a reflection of the above goals and perspective.&lt;br /&gt;&lt;br /&gt;&amp;lt;h2 id=&quot;before&quot;&amp;gt;Before the conference&amp;lt;/h2&amp;gt;SC’19 started back in the early spring for me since I served on the technical papers committee and co-chaired the Parallel Data Systems Workshop this year.  That all amounted to a predictable amount of work throughout the year, but there were two surprises that came up in October with respect to SC that are worth mentioning before we dive into the technical contents of the conference.&lt;br /&gt;&lt;br /&gt;&amp;lt;h3&amp;gt;The “I am HPC Guru” campaign&amp;lt;/h3&amp;gt;&lt;a href=&quot;https://twitter.com/JimCownie&quot;&gt;Jim Cownie&lt;/a&gt; had the brilliant idea in early October to launch a covert campaign to create “I am HPC Guru” pins for SC, and he enlisted a group of willing members of the HPC Twitter community to pitch in.  I was fortunate enough to be invited to participate in the fun, and judging by the reach of the &lt;a href=&quot;https://twitter.com/search?q=%23IAmHPCGuru&quot;&gt;#IAmHPCGuru&lt;/a&gt; tag on Twitter during the conference, it was a wild success.&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td&amp;gt;&lt;a href=&quot;https://1.bp.blogspot.com/-oP5ANNrwSMM/XdsQydbfYAI/AAAAAAABHeE/Wuf7R75LQK4UPrl7vyyNDWSDQQGn0iU9QCLcBGAsYHQ/s1600/293D77AB-1BCA-44F6-AC92-F883782F4534_1_201_a.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;320&quot; src=&quot;https://1.bp.blogspot.com/-oP5ANNrwSMM/XdsQydbfYAI/AAAAAAABHeE/Wuf7R75LQK4UPrl7vyyNDWSDQQGn0iU9QCLcBGAsYHQ/s320/293D77AB-1BCA-44F6-AC92-F883782F4534_1_201_a.jpeg&quot; width=&quot;240&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;font-size: 12.800000190734863px;&quot;&amp;gt;An allotment of “I am HPC Guru” pins.  People who pitched in also got a commemorative larger-sized pin (shown outside the bag above) which was a calling card for members of the secret society.&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&lt;br /&gt;Hats off to Jim for conceiving this great idea, seeing through the design and shipment of the pins, and being so inclusive with the whole idea.  There are now hundreds of HPC_Guru pins all over the world thanks to Jim’s efforts (and a couple dozen still with me here in California…), and I think it was a really positive way to build the Twitter-HPC community.&lt;br /&gt;&lt;br /&gt;&amp;lt;h3&amp;gt;The new job&amp;lt;/h3&amp;gt;Life also threw me a bit of a curve ball in late October when I took on a new set of responsibilities at NERSC and changed from contributing to an R&amp;amp;D group to leading an operational storage team.  This meant that, in addition to all the pre-conference commitments I had made with an eye towards longer-term storage technology strategy, I suddenly had to contextualize my goals with respect to a completely new role in tactical planning and deployment.&lt;br /&gt;&lt;br /&gt;Whereas I’ve historically written off sales-oriented meetings at SC, having good relationships with vendor sales teams in addition to their engineers and product managers is now an essential component of my new position.  As a result of wearing these two hats instead of one, the number of hard commitments I had over the course of the conference about doubled over what it usually had been.  About half of these meetings were private (and not things about which I could write), and they also reduced the time I could’ve otherwise getting into the weeds about upcoming technologies.&lt;br /&gt;&lt;br /&gt;Because the conference was so broken up into private and public meetings for me this year, a chronological recounting of the conference (as I did for &lt;a href=&quot;https://glennklockwood.blogspot.com/2018/11/a-week-in-life-of-sc-attendee.html&quot;&gt;my SC’18 recap&lt;/a&gt;) would be full of odd gaps and not make a whole lot of sense.  Instead, I will focus around a few of the juiciest topics I took away from the conference:&lt;br /&gt;&amp;lt;ol&amp;gt;&amp;lt;li&amp;gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/2019/11/sc19-recap.html#trends&quot;&gt;High-level trends that seemed to pop up repeatedly over the week&lt;/a&gt;&lt;span id=&quot;goog_846935845&quot;&gt;&lt;/span&gt;&lt;span id=&quot;goog_846935846&quot;&gt;&lt;/span&gt;&lt;a href=&quot;https://www.blogger.com/&quot;&gt;&lt;/a&gt;&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/2019/11/sc19-recap.html#splash&quot;&gt;Intel’s disclosures around the Aurora/A21 system&lt;/a&gt;&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/2019/11/sc19-recap.html#pdsw&quot;&gt;Outcomes from the 2019 Parallel Data Systems Workshop (PDSW 2019)&lt;/a&gt;&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/2019/11/sc19-recap.html#e1kf&quot;&gt;The Perlmutter all-NVMe storage node architecture&lt;/a&gt;&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/2019/11/sc19-recap.html#daos&quot;&gt;DAOS and the 2019 DAOS User Group meeting&lt;/a&gt;&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/2019/11/sc19-recap.html#else&quot;&gt;Everything else&lt;/a&gt;&amp;lt;/li&amp;gt;&amp;lt;/ol&amp;gt;&amp;lt;div&amp;gt;&lt;br /&gt;&amp;lt;/div&amp;gt;&lt;/p&gt;
&lt;h2 id=&quot;trends&quot;&gt;High-level trends&lt;/h2&gt;
&lt;p&gt;It’s difficult to group together all of the disparate things I heard and learned over the week into crisp bundles that I would consider emerging trends, but there were a few broad topics that kept popping up that suggested the following:&lt;br /&gt;&lt;br /&gt;&lt;b&gt;#1 - Memory-channel 3D XPoint is now out in the wild at sufficient scale that a picture is beginning to form around where it fits in the I/O stack&lt;/b&gt;.  The &lt;a href=&quot;http://nextgenio.eu/&quot;&gt;NEXTGenIO project&lt;/a&gt; and &lt;a href=&quot;https://daos-stack.github.io/&quot;&gt;Intel DAOS&lt;/a&gt; both demonstrated the performance achievable when 3D XPoint is integrated into larger systems this year, and the acceleration it offers can be staggering when a sensible software framework is built upon around persistent memory to bridge it with other media (like flash) and higher-level functionality (like parallel storage).  Michèle Weiland and Adrian Jackson presented their successes with the NEXTGenIO project throughout the week, most notably in the technical papers track (see “&lt;a href=&quot;https://dl.acm.org/citation.cfm?id=3356159&quot;&gt;An early evaluation of Intel’s Optane DC persistent memory module and its impact on high-performance scientific applications&lt;/a&gt;”) and across several smaller events (e.g., Adrian presented performance results, &lt;a href=&quot;https://www.epcc.ed.ac.uk/blog/2019/10/30/precision-persistent-programming&quot;&gt;detailed in his EPCC blog post&lt;/a&gt;, at the Multi-Level Memory BOF).  DAOS also made a splash on IO-500; more on this below.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;#2 - The I/O ecosystem developed in preparation for the manycore era is making the transition from pure research to practical engineering effort.&lt;/b&gt;  As the first generation of 7nm CPUs hit the market with KNL-like core counts and massive scale-up GPU node architectures are being announced by every major HPC silicon provider, latency-hiding techniques for I/O are becoming a hot topic.  Asynchronous I/O—that is, techniques that allow an application to continue computing while a write I/O operation is still happening—came up a few times, and this technique is also moving up in the software stack from system software (such as DAOS, WekaIO, and VAST) into middleware (MPI-IO and HDF5).  I touch on this in the PDSW section below.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;#3 - Innovation in HPC storage is moving away from the data plane and towards full data life cycle.  &lt;/b&gt;Whereas focus in HPC I/O has traditionally revolved around making I/O systems as fast as possible, research and product announcements this year seemed to gravitate towards data management—that is, how to manage the placement of data before, during, and after I/O.  Proprietary frameworks for data migration, policy management, tiering, and system-level analytics and intelligence (backed by serious vendor investment; see &lt;a href=&quot;https://investors.cray.com/news-releases/news-release-details/cray-introduces-clusterstor-e1000-storage-fuel-converged&quot;&gt;Cray ClusterStor Data Services&lt;/a&gt; and &lt;a href=&quot;https://www.ddn.com/press-releases/ddn-unveils-exa5-hpc-big-data-ai-acceleration-multicloud-data-management-isc19/&quot;&gt;DDN STRATAGEM&lt;/a&gt;) are popping up across the storage appliance market as a differentiator atop open-source software like Lustre, and research around applying AI to optimize data placement is maturing from novel research into product engineering.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;#4 - Scientific workflows—and the parallels they have with enterprise and hyperscale markets—are starting to be taken seriously by technology providers.  &lt;/b&gt;Vendors have begun to take ownership of the data movement challenges that exist &lt;i&gt;between&lt;/i&gt; bursts of compute-intensive jobs. Advances aimed at edge computing are becoming surprisingly relevant to HPC since decentralized data that is far away from compute is, in a sense, how HPC has done storage for decades.  Whether they be sensors distributed across billions of cell phones, thousands of non-volatile storage media distributed across an exascale computing system, or detectors deployed at giant telescopes relying on a supercomputer for image processing, there are a common set of data management, movement, and remote processing challenges whose solutions can be applied across the board.&lt;br /&gt;&lt;br /&gt;&amp;lt;h2 id=&quot;splash&quot;&amp;gt;Intel’s big splash&amp;lt;/h2&amp;gt;Following on their big system-level disclosures at ISC’19, Intel’s disclosure of the ALCF exascale system node architecture and the unveiling of their software strategy seemed to be the biggest splash of SC’19.  I was not actually at the Intel DevCon keynote where Raja Koduri made the announcements, but his slides on &lt;a href=&quot;https://s21.q4cdn.com/600692695/files/doc_presentations/2019/11/DEVCON-2019_16x9_v13_FINAL.pdf&quot;&gt;Xe and oneAPI are available online&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;The node architecture is, at a glance, very similar to the Summit node architecture today:&lt;br /&gt;&amp;lt;blockquote class=&quot;twitter-tweet&quot;&amp;gt;&amp;lt;div dir=&quot;ltr&quot; lang=&quot;en&quot;&amp;gt;Aurora &lt;a href=&quot;https://twitter.com/hashtag/supercomputer?src=hash&amp;amp;ref_src=twsrc%5Etfw&quot;&gt;#supercomputer&lt;/a&gt; &lt;a href=&quot;https://twitter.com/argonne?ref_src=twsrc%5Etfw&quot;&gt;@argonne&lt;/a&gt; will have nodes with 2 Sapphire Rapids CPUs and 6 Ponte Vecchio GPUs with unified memory architecture&lt;a href=&quot;https://twitter.com/hashtag/SC19?src=hash&amp;amp;ref_src=twsrc%5Etfw&quot;&gt;#SC19&lt;/a&gt; &lt;a href=&quot;https://twitter.com/hashtag/HPC?src=hash&amp;amp;ref_src=twsrc%5Etfw&quot;&gt;#HPC&lt;/a&gt; &lt;a href=&quot;https://twitter.com/hashtag/AI?src=hash&amp;amp;ref_src=twsrc%5Etfw&quot;&gt;#AI&lt;/a&gt; &lt;a href=&quot;https://twitter.com/hashtag/Exascale?src=hash&amp;amp;ref_src=twsrc%5Etfw&quot;&gt;#Exascale&lt;/a&gt; &lt;a href=&quot;https://twitter.com/hashtag/GPU?src=hash&amp;amp;ref_src=twsrc%5Etfw&quot;&gt;#GPU&lt;/a&gt; &lt;a href=&quot;https://t.co/HTGMnYh7AY&quot;&gt;pic.twitter.com/HTGMnYh7AY&lt;/a&gt;&amp;lt;/div&amp;gt;
— HPC Guru (@HPC_Guru) &lt;a href=&quot;https://twitter.com/HPC_Guru/status/1196219238328881152?ref_src=twsrc%5Etfw&quot;&gt;November 18, 2019&lt;/a&gt;&amp;lt;/blockquote&amp;gt;From the slide and accompanying discussion on Twitter, there was quite a lot unveiled about the node architecture.  Each node will have:&lt;br /&gt;&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;Two Sapphire Rapids Xeons (which appear to have 8 channels of DDR in the aforementioned slide) and six Ponte Vecchio Intel GPUs&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;A CXL-based “Xe Link” router provides all-to-all connectivity between the GPUs, presumably comparable to (but more standards-based than) NVLink/NVSwitch, for a unified memory space&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Eight Slingshot NIC ports per node, which is 1.6 Tbit/sec of injection bandwidth&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;A “&lt;a href=&quot;https://twitter.com/david_schor/status/1196216301716307968&quot;&gt;Rambo Cache&lt;/a&gt;” that sits between HBM, GPU, and CPU that presumably reduces NUMA effects for hot data that is being touched by many computing elements&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;A “&lt;a href=&quot;https://twitter.com/david_schor/status/1196215105450496000&quot;&gt;matrix engine&lt;/a&gt;” (which sounds an awful lot like NVIDIA’s tensor cores) in each GPU&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;div&amp;gt;This was an extremely daring release of information, as Intel has now publicly committed to a 7nm GPU part (comparable to TSMC’s 5nm process), along with a high-yield EMIB process (their chiplet interconnect for HBM integration) and Foveros (their 3D die stacking for Rambo integration), in 2021.&amp;lt;/div&amp;gt;&lt;/p&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;Intel also released the beta version of their&amp;nbsp;&lt;a href=&quot;https://software.intel.com/oneapi&quot;&gt;Intel oneAPI&lt;/a&gt; which appears to be &lt;a href=&quot;https://twitter.com/david_schor/status/1196212194339250176&quot;&gt;a mixture of re-branded Intel developer products&lt;/a&gt; (Fortran and C++ compilers, TBB, MKL, DAL, MPI, VTune, etc) with their new SYCL-based Data Parallel C++ compiler. &amp;nbsp;The novelty here is that Intel is committing to supporting this entire stack for CPUs, GPUs, FPGAs, and matrix accelerators so that, for example, you could feasibly write a single application with a single set of tools that runs across all accelerator types.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;There was a lot of interest in SYCL at the Performance Portability and Productivity workshop, P3HPC, on Friday. &amp;nbsp;There were two talks of particular interest in the parts I attended; the first, presented by Balint Joo of Jefferson Lab, presented the &lt;a href=&quot;https://drive.google.com/file/d/1rBIzzdGWvVHrQKTwA44o8OLhOSA4nW2P/view?usp=sharing&quot;&gt;performance of a quantum chromodynamics kernel when implemented using Kokkos, accelerator-specific libraries, and SYCL&lt;/a&gt;:&lt;br /&gt;&lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-YHXs6YL4WSo/Xd2KjpB4JII/AAAAAAABHg8/PPkgPTxcGtw-s7_vonpLPuuOFhNJofGLACLcBGAsYHQ/s1600/EC1521A9-3E0F-4BA6-AED1-AB51182E4C13_1_201_a.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;300&quot; src=&quot;https://1.bp.blogspot.com/-YHXs6YL4WSo/Xd2KjpB4JII/AAAAAAABHg8/PPkgPTxcGtw-s7_vonpLPuuOFhNJofGLACLcBGAsYHQ/s400/EC1521A9-3E0F-4BA6-AED1-AB51182E4C13_1_201_a.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;SYCL vs. Kokkos vs. native on NVIDIA and Intel architectures&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;These early results are promising, and with the exception of KNL, the SYCL ecosystem is already showing promise as a performance-portable framework. &amp;nbsp;The same is generally true for more complex computational kernels as well, as presented by &lt;a href=&quot;https://drive.google.com/file/d/12asEc4DddbEOJ1YQR9EV2QXZlBQs70si/view?usp=sharing&quot;&gt;Istvan Reguly from Pázmány Péter Catholic University&lt;/a&gt;:&lt;br /&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-YA4TpniLg4o/Xd2LqntEV1I/AAAAAAABHhE/q53qMlVGTvkW1FnnvtO1XTGNMHwreIqUwCLcBGAsYHQ/s1600/2FAA7FDA-5100-4F79-B9F5-B912BC43EFA0_1_201_a.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;300&quot; src=&quot;https://1.bp.blogspot.com/-YA4TpniLg4o/Xd2LqntEV1I/AAAAAAABHhE/q53qMlVGTvkW1FnnvtO1XTGNMHwreIqUwCLcBGAsYHQ/s400/2FAA7FDA-5100-4F79-B9F5-B912BC43EFA0_1_201_a.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Performance portability figure of merit for a complex kernel using different performance-portable parallel runtimes.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;Intel's choice to back an open standard rather than develop its own proprietary APIs for each accelerator type was a very smart decision, as it looks like they are already making up lost ground against NVIDIA in building a robust software ecosystem around their accelerator technologies. &amp;nbsp;The fact that these presentations were given by application scientists, not Intel engineers, really underscores this.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;Strangely, AMD kept a low profile at SC by comparison despite the fact that Rome is beginning to enter the market and, by all accounts I heard on the show floor, selling like gangbusters. &amp;nbsp;One major procurement I heard about switched from an Intel CPU-based plan of record to AMD processor as a result of a schedule slip by Intel; this wound up resulting the system obtaining 50% more cores at the same cost (plus the added benefit of PCIe Gen4) which is a testament to the advantage that AMD currently has in the near term.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;By comparison, very few large HPC centers seem to be biting on Intel's Cascade Lake-AP despite Intel's&amp;nbsp;&lt;a href=&quot;https://medium.com/performance-at-intel/hpc-leadership-where-it-matters-real-world-performance-b16c47b11a01&quot;&gt;very aggressive marketing against Rome&lt;/a&gt;. &amp;nbsp;Combined with the above observation that the Aurora architecture's Sapphire Rapids processors will only have eight memory channels per socket suggests that Cascade Lake-AP's 12-channel socket was likely released as a stopgap to have an answer to Rome while 10nm Xeon part production is scaling up.&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&amp;lt;h2 id=&quot;pdsw&quot;&amp;gt;PDSW 2019&amp;lt;/h2&amp;gt;This year I had the great honor of co-chairing the Parallel Data Systems Workshop, the premiere data and storage workshop at SC, along with the esteemed Phil Carns (creator of &lt;a href=&quot;https://www.mcs.anl.gov/research/projects/darshan/&quot;&gt;Darshan&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/OrangeFS&quot;&gt;PVFS2/OrangeFS&lt;/a&gt;, among other things).  We tried to broaden the scope of the workshop to be more inclusive of “cloudy” storage and data topics, and we also explicitly tried to build the program to include discussion about data management that ran tangential to traditional HPC-focused storage and I/O.&lt;br /&gt;&lt;br /&gt;The proceedings are already online in an &lt;a href=&quot;https://conferences.computer.org/sc19w/2019/#!/toc/16&quot;&gt;interim location hosted by ACM&lt;/a&gt;, and the full proceedings will be published by IEEE TCHPC.  Slides are available on the &lt;a href=&quot;http://www.pdsw.org/&quot;&gt;PDSW website&lt;/a&gt;, and I tried to tag my realtime thoughts using &lt;a href=&quot;https://twitter.com/search?q=%23pdsw19&quot;&gt;#pdsw19 on Twitter&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;&amp;lt;h3&amp;gt;Alluxio Keynote&amp;lt;/h3&amp;gt;Our keynote speaker was Haoyuan Li, founder of &lt;a href=&quot;https://www.alluxio.io/&quot;&gt;Alluxio&lt;/a&gt;, who gave a brilliant talk about the data orchestration framework he developed at &lt;a href=&quot;https://amplab.cs.berkeley.edu/&quot;&gt;AMPLab&lt;/a&gt; and went on to commercialize.  It is an abstraction that stitches together different storage resources (file systems, object stores, etc) into a single namespace that applications can use to read and write data in a way that hides the complexity of tiered storage.  It was designed towards the beginning of the “Big Data revolution” with a specific eye towards providing a common interface for data accessibility; by writing an application against the Alluxio API, it would be made future-proof if the HDFS or S3 APIs fizzled since Alluxio normalizes the specific API and semantics of a native storage interface from user applications.&lt;br /&gt;&lt;br /&gt;Had something like this existed in the early days of HPC, there’s a good chance that we would not be stuck using POSIX I/O as the least common denominator for data access.  That said, Alluxio does solve a slightly easier problem in that it targets analytics workloads that are read-intensive—for example, it does not provide a means for applications to do random writes, and so it provides only a subset of the full semantics that some more general-purpose I/O interfaces (such as file access) may provide.  In making this trade-off though, it is able to aggressively cache data from any storage backend in a distributed memory space, and Alluxio has a configurable cache eviction policy for predictable workflows.&lt;br /&gt;&lt;br /&gt;In describing the motivation for the Alluxio design, Haoyuan had some interesting insights.  In particular, he pointed out that there is a growing movement away from the hyperconverged hardware architecture that motivated Hadoop and HDFS:&lt;br /&gt;&lt;br /&gt;&amp;lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&amp;gt;&lt;a href=&quot;https://1.bp.blogspot.com/-2qbf6V-KeiM/XdokMSW19kI/AAAAAAABHcQ/6O8UfJFwx_sCnIdJl0rW7lXJ4PulFOSTgCLcBGAsYHQ/s1600/IMG_8272.jpeg&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;298&quot; src=&quot;https://1.bp.blogspot.com/-2qbf6V-KeiM/XdokMSW19kI/AAAAAAABHcQ/6O8UfJFwx_sCnIdJl0rW7lXJ4PulFOSTgCLcBGAsYHQ/s400/IMG_8272.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/div&amp;gt;
&lt;br /&gt;The whole “move compute to where the data is!” model for Hadoop has always struck me as rather fanciful in practice; it only works in single-tenant environments where there’s no chance of someone else’s compute already existing where your data is, and it imposes a strict coupling between how you scale data and analytics.  As it turns out, the data analytics industry is also waking up to that, and as Haoyuan’s slide above shows, separating storage from compute gives much more flexibility in how you scale compute with respect to data, but at the cost of increased complexity in data management.  The whole point of Alluxio is to minimize that cost of complexity by making data look and feel local by (1) providing a single namespace and API, and (2) using distributed memory caching to make data access perform as well as if compute and memory were colocated.&lt;br /&gt;&lt;br /&gt;This is a bit ironic since HPC has been disaggregating storage from compute for decades; HPC systems have tended to scale compute capability far faster than storage.  However, the HPC community has yet to address the added complexity of doing this, and we are still struggling to simplify storage tiering for our users.  This is only getting worse as some centers slide back into hyperconverged node designs by incorporating SSDs into each compute node.  This causes different tiers to spread data across multiple namespaces &lt;i&gt;and&lt;/i&gt; also further complicate data access since the semantics across those namespaces differ.  For example, it’s not sufficient to know that&lt;br /&gt;&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;&lt;span&gt;/local&lt;/span&gt; is the fastest tier&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&lt;span&gt;/scratch&lt;/span&gt; is less fast&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&lt;span&gt;/home&lt;/span&gt; is slow&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;since&lt;br /&gt;&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;&lt;span&gt;/local&lt;/span&gt; is only coherent with other processes sharing the same physical compute node&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&lt;span&gt;/scratch&lt;/span&gt; is globally coherent&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&lt;span&gt;/home&lt;/span&gt; is globally coherent&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;Alluxio is not the solution to this problem at present because it is optimized for write-once, read-many workloads whereas HPC does have to support random writes.  That said, HPC storage systems that incorporate the same design goals as Alluxio (connecting many types of storage under a single namespace, providing a restricted set of semantics, and applying aggressive caching to deliver local-like performance) hold a lot of promise.  Perhaps it’s no surprise that every serious parallel file system on the market is beginning to implement features like this—think Lustre &lt;a href=&quot;http://wiki.lustre.org/File_Level_Redundancy_Solution_Architecture&quot;&gt;File-Level Redundancy (FLR)&lt;/a&gt; and &lt;a href=&quot;https://dl.acm.org/citation.cfm?id=3356139&quot;&gt;Persistent Client Caching (LPCC)&lt;/a&gt;, &lt;a href=&quot;http://files.gpfsug.org/presentations/2017/Manchester/02-1_AFM.pdf&quot;&gt;Spectrum Scale AFM&lt;/a&gt;, and the core two-tier design of &lt;a href=&quot;https://www.weka.io/pdf-content/wekaio-hpc-storage-architecture/&quot;&gt;WekaIO&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;Haoyuan also presented a few case studies that showcased the ability of Alluxio to ease the transition from on-premise infrastructure (like Hadoop with HDFS) to hybrid cloud (e.g., run Presto across datasets both in older on-prem HDFS and newer S3 buckets).  It seems to be very fashionable to run analytics directly against data in object stores in industry, and Alluxio essentially gives such data more dynamism by being the place where active data can be staged for processing on demand.  Because it is a stateless orchestration layer rather than a storage system itself, Alluxio also seems nicely compatible with dynamic provisioning of compute resources.  In this sense, it may be an interesting internship project to see if Alluxio could be deployed on an HPC system to bridge a large data analytics job with an off-system object store.  Get in touch with me if you know a student who may want to try this!&lt;br /&gt;&lt;br /&gt;&amp;lt;h3&amp;gt;Asynchronous I/O&amp;lt;/h3&amp;gt;Middleware for asynchronous I/O came up in two different papers this year.  The first, “&lt;a href=&quot;https://sc19.supercomputing.org/proceedings/workshops/workshop_pages/ws_pdsw109.html&quot;&gt;Enabling Transparent Asynchronous I/O using Background Threads&lt;/a&gt;” by Tang et al., described a new pluggable runtime for HDF5 that processes standard HDF5 I/O requests asynchronously.  It does this by copying I/O requests and their metadata into a special buffer, putting those requests on a queue that is managed by the asynchronous runtime, building a directed graph of all requests’ dependencies, and dispatching I/Os alongside regular application execution using a lightweight (Argobots-based) asynchronous worker pool.&lt;br /&gt;&lt;br /&gt;What this amounts to is that a standard HDF5 write call wouldn’t block until the I/O has been committed to disk somewhere; instead, it returns immediately after the async runtime makes a copy of the data to be written into its own private memory buffer.  The application is then free to continue computing, while an Argobots thread begins buffering and dispatching outstanding asynchronous I/O calls.  The performance that results from being able to overlap I/O with computation is remarkable:&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;https://1.bp.blogspot.com/-FsMoNOoV-Xc/XdrLI2LR_WI/AAAAAAABHcg/oYZFSLVLYIANmINOJSzxRGzaDU_diJuSACLcBGAsYHQ/s1600/169FBEAD-B394-48D4-9646-91A7CFD21747_1_201_a.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;298&quot; src=&quot;https://1.bp.blogspot.com/-FsMoNOoV-Xc/XdrLI2LR_WI/AAAAAAABHcg/oYZFSLVLYIANmINOJSzxRGzaDU_diJuSACLcBGAsYHQ/s400/169FBEAD-B394-48D4-9646-91A7CFD21747_1_201_a.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;I/O speedup at scale as a result of the asynchronous runtime backend for HDF5 presented by Tang et al.&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&lt;br /&gt;What’s more impressive, though, is that this backend is almost entirely transparent to the user application; in its simplest form, it can be enabled by setting a single environment variable.&lt;br /&gt;&lt;br /&gt;Later in the day, Lucho Ionkov presented a much more novel (research-y?) asynchronous I/O runtime in his paper, “&lt;a href=&quot;https://sc19.supercomputing.org/proceedings/workshops/workshop_pages/ws_pdsw101.html&quot;&gt;A Foundation for Automated Placement of Data&lt;/a&gt;” which glued together &lt;a href=&quot;https://github.com/lanl/DRepl&quot;&gt;DRepl&lt;/a&gt; (an abstraction layer between scientific applications and storage architectures, vaguely similar to what Alluxio aims to do), &lt;a href=&quot;https://dl.acm.org/citation.cfm?id=3085484&quot;&gt;TCASM&lt;/a&gt; (a Linux kernel modification that allows processes to share memory), and &lt;a href=&quot;https://link.springer.com/chapter/10.1007/978-3-319-20119-1_22&quot;&gt;Hop&lt;/a&gt; (an expressive key-value store with tunable performance/resilience requirements).  The resulting runtime provides a high-level interface for applications to express I/O and data placement as a series of attach, publish, and re-attach operations to logical regions of memory.  The runtime then manages the actual data movement (whether it be between nodes or to persistent storage) asynchronously.&lt;br /&gt;&lt;br /&gt;Again, the net result in speedup as the problem size scales up is impressive:&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;https://1.bp.blogspot.com/-S--IJmBfKj8/XdrRdtryjmI/AAAAAAABHcs/CoKVjetWhvYXjFHYGdwqOZf3BnI8FMDPQCLcBGAsYHQ/s1600/D8B1E642-9B22-484E-99F9-C83DE81B9AC4_1_201_a.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;300&quot; src=&quot;https://1.bp.blogspot.com/-S--IJmBfKj8/XdrRdtryjmI/AAAAAAABHcs/CoKVjetWhvYXjFHYGdwqOZf3BnI8FMDPQCLcBGAsYHQ/s400/D8B1E642-9B22-484E-99F9-C83DE81B9AC4_1_201_a.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;I/O speedup at scale using the asynchronous I/O runtime presented by Iokov in Otstott et al.&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;As with the asynchronous HDF5 paper, performance gets better with scale as the increasing costs of doing I/O at scale are amortized by overlapping it with computation.  In contrast to HDF5 though, this runtime comes with a completely new application API, so one would need to convert an application’s critical I/O routines to use this framework instead of POSIX I/O.  The runtime is also pretty heavyweight in that it requires a separate global data placement “nameserver,” a custom Linux kernel, and buy-in to the new memory model.  In that sense, this is a much more research-oriented framework, but the ideas it validates may someday appear in the design of a fully framework that incorporates both an application runtime and a storage system.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Why is this important?&lt;/b&gt;  These asynchronous I/O runtimes are making a lot more sense in the era of heterogeneous computing where accelerators (think GPUs) really aren’t good at driving a full kernel-based I/O pipeline.  Instead of running a full I/O stack and enforcing strict consistency (i.e., serializing I/O) on a lightweight accelerator core, having an asynchronous runtime running on a fat core that simply copies an I/O buffer from accelerator memory to slower memory before releasing program control back to the accelerator allows the accelerator tp spend less time doing what it’s terrible at doing (ordering I/O operations) and more time computing.  At the same time, the fat core that is running the asynchronous I/O runtime can then operate on that copied I/O buffer on its own time, reorder and serialize operations to ensure consistency, and jump into and out of the kernel to enforce file permissions without interrupting the accelerator:&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;https://1.bp.blogspot.com/-egf3u1VCO1w/XdrhXLQcqxI/AAAAAAABHc4/V5AP2pTwnsUXzNKkMmMHCS1uWuBFXy6TgCLcBGAsYHQ/s1600/async-io-runtime.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;https://1.bp.blogspot.com/-egf3u1VCO1w/XdrhXLQcqxI/AAAAAAABHc4/V5AP2pTwnsUXzNKkMmMHCS1uWuBFXy6TgCLcBGAsYHQ/s640/async-io-runtime.png&quot; width=&quot;100%&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;Sketch of how an asynchronous I/O runtime might map to a heterogeneous node architecture&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&lt;br /&gt;Ron Oldfield did raise a really great consideration during PDSW about this though: at the end of the day, the asynchronous I/O runtime still has to share network resources with the application’s message passing runtime (e.g., MPI).  He alluded to work done a decade ago that found that asynchronous I/O was often stomping on MPI traffic since both MPI and I/O could happen at the same time.  Without some kind of awareness or coordination between the asynchronous I/O runtime and the application communication runtime, this sort of scheme is prone to self-interference when running a real application.&lt;br /&gt;&lt;br /&gt;Given this, the right place to integrate an asynchronous I/O runtime might be inside the message passing runtime itself (e.g., MPI-IO).  This way the asynchronous I/O scheduler could consider outstanding asynchronous messages it must pass as well and be smart about dispatching too many competing network transfers at the same time.  Unfortunately this then places a complex burden of serialization and synchronization on the runtime, and this starts to look a lot like just throwing messages at the NIC and letting it figure out the correct ordering.  The principal advantage here would be that the runtime has a lot more visibility into user intent (and may have more spare processing capacity if most of the application time is spent on an accelerator), so it could afford to be smarter about how it builds its dependency graph.&lt;br /&gt;&lt;br /&gt;&amp;lt;h3&amp;gt;Analytics for Runtime and Operations&amp;lt;/h3&amp;gt;No computing-related workshop would be complete without a smattering of artificial intelligence and machine learning, and PDSW was no different this year.  Two papers were presented that attempted to use machine learning to predict parallel I/O performance in slightly different ways.&lt;br /&gt;&lt;br /&gt;Suren Byna presented “&lt;a href=&quot;https://sc19.supercomputing.org/proceedings/workshops/workshop_pages/ws_pdsw103.html&quot;&gt;Active Learning-based Automatic Tuning and Prediction of Parallel I/O Performance&lt;/a&gt;” where the authors developed an approach for autotuning parallel I/O (specifically using MPI-IO hints and Lustre striping parameters) using active learning to predict the optimal values for their tuning parameters.  They used two different approaches, and the faster one uses predicted performance to infer optimal tuning values.  Given how many factors actually come to play in parallel I/O performance on production systems, their model was able to predict I/O performance quite well under a range of I/O patterns:&lt;br /&gt;&lt;br /&gt;&amp;lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&amp;gt;&lt;a href=&quot;https://1.bp.blogspot.com/--PIWB0B9dS4/Xdrl62VwRUI/AAAAAAABHdE/iAmCZ17la0ky3riAvdOd4hs6OjO5q1q3QCLcBGAsYHQ/s1600/87F85F2F-A8B5-40AC-A9AD-4A06D8AAECBA_1_201_a.jpeg&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;300&quot; src=&quot;https://1.bp.blogspot.com/--PIWB0B9dS4/Xdrl62VwRUI/AAAAAAABHdE/iAmCZ17la0ky3riAvdOd4hs6OjO5q1q3QCLcBGAsYHQ/s400/87F85F2F-A8B5-40AC-A9AD-4A06D8AAECBA_1_201_a.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/div&amp;gt;
&lt;br /&gt;Bing Xie et al presented “&lt;a href=&quot;https://sc19.supercomputing.org/proceedings/workshops/workshop_pages/ws_pdsw108.html&quot;&gt;Applying Machine Learning to Understand Write Performance of Large-scale Parallel Filesystems&lt;/a&gt;” which pursued a similar line of work—using machine learning to predict I/O performance—but with a slightly different goal.  Xie’s goal was to identify the factors which most strongly affect predicted I/O performance, and she found that write performance was most adversely affected by metadata load and load imbalance on Blue Gene/Q and GPFS, whereas Cray XK7 and Lustre were more affected by aggregate file system load and load imbalance.  This system-centric work laid out a more sophisticated blueprint for identifying causal relationships between poor I/O performance and system-level health events, and I think applying these approaches to the &lt;a href=&quot;https://www.nersc.gov/research-and-development/tokio/a-year-in-the-life-of-a-parallel-file-system/&quot;&gt;dataset I published last year with my Year in the Life of a Parallel File System paper&lt;/a&gt; might identify some interesting emergent relationships between bad performance and the subtle factors to which they can be attributed.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Why is this important?&lt;/b&gt;  Industry is beginning to take notice that it is no longer sufficient to just report there here-and-now of how parallel file systems are behaving, and more sophisticated analytics engines are being co-deployed with very large systems.  For example, the &lt;a href=&quot;https://www.hpcwire.com/2019/10/03/summit-has-real-time-analytics-heres-how-it-happened-and-whats-next/&quot;&gt;Summit system at Oak Ridge made a splash in October by announcing the real-time analytics engine&lt;/a&gt; that was implemented on top of it, and &lt;a href=&quot;https://www.cray.com/products/storage/clusterstor/view&quot;&gt;Cray View&lt;/a&gt; is a similar analytics-capable engine built atop Lustre that Cray offers as a part of its ClusterStor lineup.  I’m not sure if DDN has something comparable, but their recent purchase of Tintri and its &lt;a href=&quot;https://www.tintri.com/products/tintri-analytics&quot;&gt;robust, enterprise-focused analytics engine&lt;/a&gt; means that they hold IP that can be undoubtedly be applied to its HPC-focused storage product portfolio.&lt;br /&gt;&lt;br /&gt;Being able to predict performance (and the conditions that cause it to degrade!) is the holy grail of parallel I/O systems management, and it’s a sure bet that all the HPC storage vendors are watching research in this area very closely to see what ideas they can pluck from the community to add value to their proprietary analytics engines.  The fact that AI is being applied to production system data and yielding useful and actionable outcomes gives legs to this general idea of AI for self-driving systems.  The talks at PDSW this year were only demonstrations, not hardened products, but these ad-hoc or small-scale demonstrations are moving us in the right direction.&lt;br /&gt;&lt;br /&gt;&amp;lt;h3&amp;gt;My Talk on Data Motion&amp;lt;/h3&amp;gt;I also coauthored and presented a paper at PDSW this year that was an exploratory study of how we can understand data movement throughout an entire data center.  The goal of the entire paper, “&lt;a href=&quot;https://sc19.supercomputing.org/proceedings/workshops/workshop_pages/ws_pdsw106.html&quot;&gt;Understanding Data Motion in the Modern HPC Data Center&lt;/a&gt;,” was to generate this diagram that shows how much data flows between different systems at NERSC:&lt;br /&gt;&lt;br /&gt;&amp;lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&amp;gt;&amp;lt;/div&amp;gt;&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&amp;lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&amp;gt;&lt;a href=&quot;https://1.bp.blogspot.com/-hUn48BmW3Cg/Xdr8OwIV87I/AAAAAAABHdw/cNkutk-zRR8DQNwW6Ac-fGjo65MgHEWKACLcBGAsYHQ/s1600/datacenter-graph.jpg&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;300&quot; src=&quot;https://1.bp.blogspot.com/-hUn48BmW3Cg/Xdr8OwIV87I/AAAAAAABHdw/cNkutk-zRR8DQNwW6Ac-fGjo65MgHEWKACLcBGAsYHQ/s400/datacenter-graph.jpg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/div&amp;gt;&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;I won’t recount the technical content of the talk here, but the paper is open access for those interested.  The essence of the study is that we showed that it is possible to examine data motion beyond the context of individual jobs and begin tying together entire workflows, but there’s a lot of supporting work required to shore up the tools and telemetry from which this analysis draws.  The paper was very much a long-form work in progress, and I’d be interested in hearing from anyone who is interested in pursuing this work further.&lt;br /&gt;&lt;br /&gt;&amp;lt;h2 id=&quot;e1kf&quot;&amp;gt;Scale-up highly available NVMe hardware&amp;lt;/h2&amp;gt;Although it didn’t make a many headlines (as storage rarely does), &lt;a href=&quot;https://investors.cray.com/news-releases/news-release-details/cray-introduces-clusterstor-e1000-storage-fuel-converged&quot;&gt;Cray announced its new ClusterStor E1000 platform shortly before SC&lt;/a&gt; and had some of their E1000-F all NVMe enclosures on display at a few booths.  I normally don’t care too much about storage enclosures (it’s all just sheet metal, right?), but this announcement was special to me because it is the hardware platform that is going into NERSC’s Perlmutter system in 2020, and I’ve been involved with the different iterations of this hardware design for over a year now.&lt;br /&gt;&lt;br /&gt;It’s very gratifying to see something start out as a CAD drawing and a block diagram and grow up into actual hardware:&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;https://1.bp.blogspot.com/-mcihJuv4KHg/Xd1wvLcecuI/AAAAAAABHfU/FOZbYoHwzz0OsHQRd-WblLCOwKffw4XCgCLcBGAsYHQ/s1600/051E2FB3-E18E-4CB0-A128-C9643DAF38E8.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;300&quot; src=&quot;https://1.bp.blogspot.com/-mcihJuv4KHg/Xd1wvLcecuI/AAAAAAABHfU/FOZbYoHwzz0OsHQRd-WblLCOwKffw4XCgCLcBGAsYHQ/s400/051E2FB3-E18E-4CB0-A128-C9643DAF38E8.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;The E1000-F all-NVMe enclosure&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&lt;br /&gt;Torben Kling Petersen gave a talk at the Exhibitor Forum disclosing the details of the hardware design on behalf of Cray, and it looks like they’ve made just about everything surrounding the E1000 public:&lt;br /&gt;&lt;br /&gt;&amp;lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&amp;gt;&lt;a href=&quot;https://1.bp.blogspot.com/-uloI_lQ3Ff8/Xd1xGElhp0I/AAAAAAABHfc/4tnlBHFKMY4uBb4nMYORjYksZwJdBOl2gCLcBGAsYHQ/s1600/08FF9AF4-E0B9-4304-A510-C6505C65B31E_1_201_a.jpeg&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;300&quot; src=&quot;https://1.bp.blogspot.com/-uloI_lQ3Ff8/Xd1xGElhp0I/AAAAAAABHfc/4tnlBHFKMY4uBb4nMYORjYksZwJdBOl2gCLcBGAsYHQ/s400/08FF9AF4-E0B9-4304-A510-C6505C65B31E_1_201_a.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/div&amp;gt;
&lt;br /&gt;The foundation for this platform is the E1000-F high-availability enclosure as shown in the above slide.  It has two separate Rome-based servers (“controllers”) and 24 U.2 NVMe slots capable of PCIe Gen4.  Each Rome controller has slots for up to three 200 Gbit NICs; doing the math, this gives a very nicely balanced design that is implemented entirely without PCIe switches:&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;https://1.bp.blogspot.com/-pm3ADq968-c/Xd12_h5Yi8I/AAAAAAABHfo/IdytQRpXcPgHl_OfPmfezFd67wF9uQwvACLcBGAsYHQ/s1600/e1kf-cartoon.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;325&quot; src=&quot;https://1.bp.blogspot.com/-pm3ADq968-c/Xd12_h5Yi8I/AAAAAAABHfo/IdytQRpXcPgHl_OfPmfezFd67wF9uQwvACLcBGAsYHQ/s400/e1kf-cartoon.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;Cartoon block diagram for one half of the E1000-F chassis.  Note that the NVMe read rates (violet text) are assumed based on Samsung PM1733 specs and performance projections that Petersen presented.  Also note that each NVMe drive is 2x2 PCIe Gen4 with multipath to the other Rome controller (not shown).&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;I visited the booth of the ODM with whom Cray worked to develop this node design and was fortunate enough to meet the node architects from both sides who gave me a really helpful breakdown of the design.  Physically, the 2U chassis is laid out something like this:&lt;br /&gt;&lt;br /&gt;&amp;lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&amp;gt;&lt;a href=&quot;https://1.bp.blogspot.com/-O6Z1S401OCw/Xd164wyS9nI/AAAAAAABHf0/eFZZFOhQG6gKMkuHjRjQJrQVzjVhQlOoACLcBGAsYHQ/s1600/e1kf-layout.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;253&quot; src=&quot;https://1.bp.blogspot.com/-O6Z1S401OCw/Xd164wyS9nI/AAAAAAABHf0/eFZZFOhQG6gKMkuHjRjQJrQVzjVhQlOoACLcBGAsYHQ/s400/e1kf-layout.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/div&amp;gt;
&lt;br /&gt;Just about everything is both hot-swappable and fully redundant.  The entire system can be powered and cooled off of a single 1.2 kW(?) power supply, and all the fans are hot-swappable and configured in a 5+1:&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;https://1.bp.blogspot.com/-z-UKjbgSVGY/Xd17dG-hWRI/AAAAAAABHf8/fOlOtBODrxYlmjibKtES2P-47WZzSbrWgCLcBGAsYHQ/s1600/775BB1C2-4008-4DBA-86EA-3C8601803A90_1_201_a.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;300&quot; src=&quot;https://1.bp.blogspot.com/-z-UKjbgSVGY/Xd17dG-hWRI/AAAAAAABHf8/fOlOtBODrxYlmjibKtES2P-47WZzSbrWgCLcBGAsYHQ/s400/775BB1C2-4008-4DBA-86EA-3C8601803A90_1_201_a.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;Fans are all individually replaceable and configured in 5+1.  You can also see the NVMe backplanes, attached to an active midplane (not shown), through the open fan slot.&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&lt;br /&gt;All the fans are on the same pulse-width modulator (PWM), so they all operate at the same speed and provide even airflow as long as they are properly powered.  My recollection from what the architect told me is that the PWM signal is provided by an FPGA on the midplane which also handles drive power-up.  Because there is only a single midplane and this power/cooling controller lives on it, this power/cooling FPGA is also configured redundantly as 1+1.  Thus, while the midplane itself is not redundant or field-replaceable, the active components on it are, and it would take physical damage (e.g., someone punching a hole through it and breaking the PCB traces) to knock the whole chassis offline.&lt;br /&gt;&lt;br /&gt;Each chassis has two independent node boards that are hot-pluggable and self-contained:&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;https://1.bp.blogspot.com/-SWMov-I2DwU/Xd180WGiAHI/AAAAAAABHgI/iFdEtpuQfdoF1GuyrQQH_ZZR07yjC6bRACLcBGAsYHQ/s1600/BD7CFE49-BDC8-46B6-B8FC-02D7BD22F9AC.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;300&quot; src=&quot;https://1.bp.blogspot.com/-SWMov-I2DwU/Xd180WGiAHI/AAAAAAABHgI/iFdEtpuQfdoF1GuyrQQH_ZZR07yjC6bRACLcBGAsYHQ/s400/BD7CFE49-BDC8-46B6-B8FC-02D7BD22F9AC.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;One of the E1000-F node sleds with its cover popped off at the Cray booth&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;Each node board is wrapped in a sheet metal sled and has a screwed-on lid.  The whole node sled was designed by the ODM to be a field-replaceable unit (FRU), so doing something like a DIMM swap does require a screwdriver to remove the top cover.  However it’s ultimately up to OEMs to decide how to break down FRUs.&lt;br /&gt;&lt;br /&gt;The ODM had a bare controller board at its booth which looks like this:&lt;br /&gt;&lt;br /&gt;&amp;lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&amp;gt;&amp;lt;/div&amp;gt;&lt;/p&gt;
&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-e-X_hhgvnlo/Xd2BNNo48MI/AAAAAAABHgc/i2w91323rtkhcshMkr6j1ebuWnu1k-CwACLcBGAsYHQ/s1600/IMG_8312.JPG&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;178&quot; src=&quot;https://1.bp.blogspot.com/-e-X_hhgvnlo/Xd2BNNo48MI/AAAAAAABHgc/i2w91323rtkhcshMkr6j1ebuWnu1k-CwACLcBGAsYHQ/s400/IMG_8312.JPG&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;E1000-F bare controller board&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;There are two M.2 PCIe Gen4 slots for mirrored boot drives and a pair of big hot-plug block connectors in the front of the board for redundant power and 48 lanes of PCIe Gen4 for the 24x U.2 drives hanging off the midplane.  There’s a single riser slot for two standard HHHL PCIe add-in cards where two NICs plug in, and a third OCP-form factor slot where the third NIC can slot in.  The rear of the controller sled shows this arrangement:&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;https://1.bp.blogspot.com/-S-Vk3p6O3iI/Xd2CElcLokI/AAAAAAABHgk/0fF_Uz6Ke2MgoO9M6Anrh0NEnSZw6FPAACLcBGAsYHQ/s1600/435EE91C-A15A-4FCD-AAFD-F88CD1D37A1B.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;300&quot; src=&quot;https://1.bp.blogspot.com/-S-Vk3p6O3iI/Xd2CElcLokI/AAAAAAABHgk/0fF_Uz6Ke2MgoO9M6Anrh0NEnSZw6FPAACLcBGAsYHQ/s400/435EE91C-A15A-4FCD-AAFD-F88CD1D37A1B.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;Rear view of a single Rome controller&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;It looks like there’s a single RJ45 port (for LOM?), a power and reset button, a single USB-3, and a mini DisplayPort for crash carting.&lt;br /&gt;&lt;br /&gt;When Cray announced the E1000-F, &lt;a href=&quot;https://www.hpcwire.com/2019/10/30/cray-debuts-clusterstor-e1000-finishing-remake-of-portfolio-for-exascale-era/&quot;&gt;HPCwire ran a block diagram of the complete chassis design&lt;/a&gt; that suggested that heartbeating would be done through a non-transparent bridge (NTB) implemented on the AMD Rome host interface.  This was a little worrisome since AMD has yet to release the proper drivers to enable this NTB for Linux in a functional way; this simple fact is leading other ODMs towards a more conservative node design where a third-party nonblocking PCIe switch is added simply to provide a functioning NTB.  When I asked the architect about this, though, he revealed that the E1000-F also has an internal gigabit Ethernet loop between both controllers for heartbeating which completely obviates the need to rely on any NTB for failover.&lt;br /&gt;&lt;br /&gt;Another interesting thing I learned while talking to the E1000-F designers is that the power supply configuration gives a lot of runway for the overall system design:&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;https://1.bp.blogspot.com/-9X6oywadOEw/Xd2EgIttYZI/AAAAAAABHgw/L22x4eng6YYI0T9YSwCbRr50y5SCA0-bwCLcBGAsYHQ/s1600/950DFDAE-C3E8-46CC-85AF-851023FB4EBB.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;400&quot; src=&quot;https://1.bp.blogspot.com/-9X6oywadOEw/Xd2EgIttYZI/AAAAAAABHgw/L22x4eng6YYI0T9YSwCbRr50y5SCA0-bwCLcBGAsYHQ/s400/950DFDAE-C3E8-46CC-85AF-851023FB4EBB.jpeg&quot; width=&quot;300&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;One of the two power supply sleds for the E1000-F chassis.  Lots of free real estate remains and is currently occupied by bus bars.&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;The current power supply is (I believe) ~1200 W, and the carrier sled on which it is mounted is mostly empty space taken up by two fat bus bars that reach all the way to the front of it.  In leaving all of this space in the sled, it will be fully possible to build a physically compatible PSU sled that delivers significantly more power to the U.2 NVMe drives and host controllers if the power consumption of the controllers or the NVMe drives increases in the future.  The ODM confirmed that the cooling fans have similar headroom and should allow the whole enclosure to support a higher power and thermal load by just upgrading the power and controller FRUs.&lt;br /&gt;&lt;br /&gt;This point is important because the performance of PCIe Gen4 SSDs are actually capped by their power consumption—if you look at product sheets for ruler SSDs (M.2, NF1, and E1.S), you will find that their performance is universally lower than their U.2 and HHHL variants due to the fact that the ruler standards limit power to 8-12W compared to U.2/HHHL’s ~25W.  This E1000-F chassis is designed as-is for 25W U.2 drives, but there are already &lt;a href=&quot;https://146a55aca6f00848c565-a7635525d40ac1c70300198708936b4e.ssl.cf1.rackcdn.com/images/3e94b383b566fc8bfb7814ec6cad5dc88f2bad7c.pdf&quot;&gt;proposals to push individual SSD power up to 40W&lt;/a&gt; and beyond.  Given this trend and the high bandwidth available over a PCIe Gen4 x4 connector, it’s entirely possible that there will be a demand for higher-power NVMe enclosures as Gen4 matures and people want to drive Gen4 NVMe at line rate.&lt;br /&gt;&lt;br /&gt;&amp;lt;h2 id=&quot;daos&quot;&amp;gt;DAOS User Group&amp;lt;/h2&amp;gt;The &lt;a href=&quot;https://wiki.hpdd.intel.com/display/DC/DUG19&quot;&gt;2019 DAOS User Group&lt;/a&gt; was held on Wednesday in a hotel adjacent to the main convention center. Contrary to previous years in which I attended, this meeting felt like a real user group; there were presenters from several different organizations, none of whom directly contribute to or are contractual customers of DAOS.  There were also real performance data which largely centered around the &lt;a href=&quot;https://www.vi4io.org/io500/start&quot;&gt;insanely high IO-500 benchmark score that DAOS posted earlier in the week&lt;/a&gt;:&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;https://1.bp.blogspot.com/-BkuxsxoIhbc/Xd36EaomTYI/AAAAAAABHhg/aEXJklOVKbIrI8W9U8m1B76hXL9KfNNrACLcBGAsYHQ/s1600/2A2C3168-663C-404A-A8BC-2644E7626D5D_1_201_a.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;300&quot; src=&quot;https://1.bp.blogspot.com/-BkuxsxoIhbc/Xd36EaomTYI/AAAAAAABHhg/aEXJklOVKbIrI8W9U8m1B76hXL9KfNNrACLcBGAsYHQ/s400/2A2C3168-663C-404A-A8BC-2644E7626D5D_1_201_a.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;Bandwidth spread on the IO-500’s IOR test suite&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;These numbers are using a pretty modest server environment and client count (&lt;a href=&quot;https://twitter.com/geomark/status/1197297380535808000?s=21&quot;&gt;24 DAOS servers, 26 client nodes, 28 ranks per client, dual-rail OPA100&lt;/a&gt;) and use the native DAOS API.  What I didn’t snap a photo of are the crazy metadata rates which posted a geometric mean of 4.7 million IOPS; by comparison, the 250 PB Alpine file system attached to the Summit supercomputer at Oak Ridge posted 1.2 million IOPS using more than 500 clients.  To the extent that it was meant to address the IOPS limitations intrinsic to traditional parallel file systems, the DAOS design is looking like a resounding success.&lt;br /&gt;&lt;br /&gt;According to the speaker, the metadata performance of this IO-500 run was not limited by any server-side resources, so adding more clients (like WekaIO’s top-scoring run with 345 clients) could have pushed this number higher.  It was also stated that the staggering IOR read performance was limited by the aggregate Optane DIMM bandwidth which is a testament to how highly optimized the data path is.&lt;br /&gt;&lt;br /&gt;&amp;lt;h3&amp;gt;Actually using DAOS&amp;lt;/h3&amp;gt;This is all using the DAOS native API though, and unless you intend to rewrite all your &lt;span&gt;open()&lt;/span&gt;s and &lt;span&gt;write()&lt;/span&gt;s as &lt;span&gt;daos_pool_connect()&lt;/span&gt; + &lt;span&gt;daos_cont_open()&lt;/span&gt; + &lt;span&gt;daos_array_open()&lt;/span&gt;s and &lt;span&gt;daos_array_write()&lt;/span&gt;s, it’s hard to tell what this really means in terms of real-world performance.  Fortunately there was a great set of talks about the &lt;a href=&quot;https://wiki.hpdd.intel.com/display/DC/DUG19?preview=/114950685/117310261/3_DUG19_middleware.pdf&quot;&gt;DAOS POSIX compatibility layer and related middleware&lt;/a&gt;.  I described the POSIX middleware a little in &lt;a href=&quot;https://glennklockwood.blogspot.com/2019/06/isc19-recap.html&quot;&gt;my recap of ISC’19&lt;/a&gt;, but it’s much clearer now exactly how a POSIX application may be adapted to use DAOS.  Ultimately, there are three options that DAOS provides natively:&lt;br /&gt;&lt;br /&gt;&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;&lt;b&gt;libdfs&lt;/b&gt;, which is a DAOS library that provides a POSIX-like (but not POSIX-compatible) API into DAOS.  You still have to connect to a pool and open a container, but instead of reading and writing to arrays, you read and write arbitrary buffers to byte offsets within file-like objects.  These objects exist in a hierarchical namespace, and there are functions provided by libdfs that map directly to POSIX operations like mkdir, rmdir, statfs, etc.  Using libdfs, you would still have to rewrite your POSIX I/O calls, but there would be a much smaller semantic gap since POSIX files and directories resemble the files and directories provided by libdfs.  A great example of what libdfs looks like can be found in the &lt;a href=&quot;https://github.com/hpc/ior/blob/master/src/aiori-DFS.c&quot;&gt;IOR DFS backend code&lt;/a&gt;.&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&lt;b&gt;dfuse&lt;/b&gt;, which is a FUSE client written on top of libdfs.  With this, you literally get a file system mount point which POSIX applications can interact with natively.  Because this uses FUSE though, such accesses are still generating system calls and memory copies which come with steep latency penalties.&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&lt;b&gt;libioil&lt;/b&gt;, which is a POSIX interception library.  This is what you’d LD_PRELOAD in front of a standard application, and it does the remapping of genuine POSIX API calls into libdfs-native calls without ever going through the kernel.&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;br /&gt;&amp;lt;div&amp;gt;Cedric Milesi from HPE presented benchmark slides that showed that using the DFS (file-based) API over the native (array-based) API has no effect on performance:&amp;lt;/div&amp;gt;&lt;/p&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-R12yuhpqwnw/Xd4F1kooVII/AAAAAAABHiE/m8Kp2vuhzz0xyG9hkhhG2OXGoycJPEqRgCLcBGAsYHQ/s1600/FD351A70-C5B7-4B43-B58F-427E991FA99F_1_201_a.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;300&quot; src=&quot;https://1.bp.blogspot.com/-R12yuhpqwnw/Xd4F1kooVII/AAAAAAABHiE/m8Kp2vuhzz0xyG9hkhhG2OXGoycJPEqRgCLcBGAsYHQ/s400/FD351A70-C5B7-4B43-B58F-427E991FA99F_1_201_a.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Performance scaling of the native DAOS API (which encodes array objects) to the DAOS DFS API (which encodes file and directory objects). &amp;nbsp;No discernible performance difference.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;&lt;br /&gt;&amp;lt;div&amp;gt;Thus, there is no performance difference whether you treat DAOS like an array store (its original design) or a file/directory store (through the libdfs API) as far as bandwidth is concerned.  This is excellent news, as even though libdfs isn’t a drop-in replacement for POSIX I/O, it implements the POSIX data model (data is stored as streams of bits) which is a more comfortable look and feel for a storage system than storing typed arrays.  And since libioil is a shim atop libdfs, the above performance data suggests that POSIX applications won’t pay significant bandwidth overheads by preloading the POSIX intercept library to get DAOS compatibility out of the box.&amp;lt;/div&amp;gt;&lt;/p&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;What's less clear is what the metadata overheads of libdfs are. &amp;nbsp;Because the whole metadata model of DFS (files and directories) is very different from native DAOS (arrays), it's impossible to do a head-to-head comparison of metadata performance. &amp;nbsp;That said, DFS metadata is only a subset of the full POSIX metadata so it should be faster even on identical hardware. &amp;nbsp;For example, DAOS only enforces permissions when opening a container, so I would not expect DFS to have any notion of file-level or directory-level ownership or permissions bits. &amp;nbsp;As such, DFS would not incur the cost of doing an expensive recursive permission check on dfs_open(), and the open rate should be much higher than something that adheres to POSIX.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;&lt;a href=&quot;https://wiki.hpdd.intel.com/display/DC/DUG19?preview=/114950685/117310295/5_DUG19_Argonne_harms_v2.pdf&quot;&gt;Kevin Harms from ALCF also presented a really enlightening slide&lt;/a&gt; containing very early performance tests from their internal DAOS testbed using dfuse and libioil:&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-KKpue81Rplw/Xd4CH_zW1BI/AAAAAAABHh4/8Ycwv0QkgVs_rEO-D5wM6IWQ4cbFfPW4gCLcBGAsYHQ/s1600/E7AA229C-FE13-411A-BA27-2F843CDC7C0C_1_201_a.jpeg&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;300&quot; src=&quot;https://1.bp.blogspot.com/-KKpue81Rplw/Xd4CH_zW1BI/AAAAAAABHh4/8Ycwv0QkgVs_rEO-D5wM6IWQ4cbFfPW4gCLcBGAsYHQ/s400/E7AA229C-FE13-411A-BA27-2F843CDC7C0C_1_201_a.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;This slide is a treasure trove of interesting information:&lt;/div&gt;
&lt;div&gt;&lt;ol&gt;&lt;li&gt;It implicitly confirms that the verbs provider for libfabric not only works, but works well. &amp;nbsp;Recall that the Intel testbed from which IO-500 was run used Intel OmniPath 100, whereas the Argonne testbed uses a competitor's fabric, InfiniBand.&lt;/li&gt;&lt;li&gt;Single-stream performance of DAOS using the dfuse interface is 450 MB/sec which isn't terrible. &amp;nbsp;For comparison, single-stream performance of Lustre on Cray Aries + FDR InfiniBand is about the same.&lt;/li&gt;&lt;li&gt;Using the libioil POSIX interface dramatically increases the single-stream performance which shines a light on how costly using the Linux VFS kernel interface (with FUSE on top) really is. &amp;nbsp;Not using FUSE, avoiding an expensive context switch into kernel mode, and avoiding a memcpy from a user buffer into a kernel buffer gives a 3x performance boost.&lt;/li&gt;&lt;/ol&gt;&lt;div&gt;Again, in the sense that DAOS was meant to address the performance impacts of using a kernel-based storage system for I/O, it looks like DAOS is meeting expectation.&lt;/div&gt;
&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;&lt;div&gt;Finally,&amp;nbsp;&lt;a href=&quot;https://wiki.hpdd.intel.com/display/DC/DUG19?preview=/114950685/117310261/3_DUG19_middleware.pdf&quot;&gt;Mohamad Chaarawi also spent some time talking about the Lustre/DAOS integration&lt;/a&gt;&amp;nbsp;which uses DAOS dfuse to stitch together a Lustre namespace with DAOS DFS namespaces. &amp;nbsp;I mentioned this in my ISC recap, but there's now a pretty detailed slide about how this will look in practice:&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-XNASGvs6FN8/Xd4SMLDN2iI/AAAAAAABHiQ/bBIeXxA2ySgV1Occ09dOgDlKTcqSnVuCgCLcBGAsYHQ/s1600/5FD6F079-D02B-4CD3-9D11-C63B599F53D7_1_201_a.jpeg&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;300&quot; src=&quot;https://1.bp.blogspot.com/-XNASGvs6FN8/Xd4SMLDN2iI/AAAAAAABHiQ/bBIeXxA2ySgV1Occ09dOgDlKTcqSnVuCgCLcBGAsYHQ/s400/5FD6F079-D02B-4CD3-9D11-C63B599F53D7_1_201_a.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;This Lustre integration won't be quite as rosy as I described earlier since DFS namespaces don't seamlessly merge into the Lustre namespace. &amp;nbsp;Instead, it looks like DFS namespaces will be mounted in a separate directory hierarchy governed by their pool UUID (&quot;PUUID&quot; in above slide) and container UUID (&quot;CUUID&quot;), and the Lustre namespace will contain symlinks to the DFS mounts. &amp;nbsp;What exactly creates and destroys these symlinks is unclear; in July it had sounded like Lustre foreign layouts would dynamically stitch DAOS objects into Lustre using the Lustre control plane, but now it sounds like DAOS will behave more like autofs on top of Lustre.&lt;/div&gt;
&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h3&gt;The burgeoning DAOS community&lt;/h3&gt;
&lt;div&gt;Although the progress and increasing tangibility of DAOS is impressive, I was most struck by the diversity of stakeholders represented at the DAOS User Group meeting. &amp;nbsp;In particular, the participation of HPE (the non-Cray part, no less!) and Lenovo was a surprise to me since neither has an immediate interest in the Argonne exascale system which has been the biggest driver for DAOS development. &amp;nbsp;Lenovo in particular made the bold statement that they want to sell a DAOS appliance in 4Q2020/1Q2021 called the &quot;DSS-D Integrated Solution with DAOS.&quot;&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;Oddly enough, the Cray part of HPE was not obviously present at the DAOS User Group despite their involvement in Argonne's Aurora system and activity on the DAOS mailing lists. &amp;nbsp;This may just be a reflection of Cray's historic reluctance to send engineering staff to SC, but their absence was quite notable in contrast to Lenovo's head-first dive into announcing a DAOS appliance. &amp;nbsp;There were also no loud voices supporting all of the work that DAOS has put into integrating with Apache Spark, nor were there any vocal supporters of Intel's newly stated ambition to create a native &lt;a href=&quot;https://en.wikipedia.org/wiki/SEG-Y&quot;&gt;SEG-Y interface&lt;/a&gt; (a format used by oil and gas) for DAOS.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h2 id=&quot;else&quot;&gt;Everything else&lt;/h2&gt;
&lt;div&gt;There were some interesting tidbits that I picked up at SC this year don't fit neatly anywhere else in this post but are worth writing down.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h3&gt;Technical tidbits - the Cray Shasta cabinet&lt;/h3&gt;
&lt;div&gt;Much like the Cray E1000-F storage enclosure, I have also watched the Cray Shasta cabinet design evolve from a set of CAD diagrams to living, breathing behemoth of sheet metal and coolant tubing. &amp;nbsp;SC'19 was the debut of a finished Cray Shasta compute cabinet, and it's a sight to behold:&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-9uFvpqZqQVk/Xd47QZMmxsI/AAAAAAABHio/ZOEKu5Jn5tAnquxMm7W7qCRE7SLuQjJegCLcBGAsYHQ/s1600/DAE5AE65-2E30-4174-8B71-73CC32B5159E.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;400&quot; src=&quot;https://1.bp.blogspot.com/-9uFvpqZqQVk/Xd47QZMmxsI/AAAAAAABHio/ZOEKu5Jn5tAnquxMm7W7qCRE7SLuQjJegCLcBGAsYHQ/s400/DAE5AE65-2E30-4174-8B71-73CC32B5159E.jpeg&quot; width=&quot;300&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;The front end of the new Cray Shasta compute cabinet&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;div&gt;These new cabinets are all direct liquid cooled, and the water tubing to each blade from the center manifold is all done up in the above photo. &amp;nbsp;Compute blades slot in vertically, and each cabinet has French doors that open in directions opposite to each other. &amp;nbsp;The back end is a little less neat at a glance:&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-6s8gW6eBBmY/Xd475Yd4pUI/AAAAAAABHiw/5OlBsyNcNXAzcCZI2a388X2rMSzxAIS8wCLcBGAsYHQ/s1600/95BB7E4A-0691-4CA9-B4D5-27516D40CFA3.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;400&quot; src=&quot;https://1.bp.blogspot.com/-6s8gW6eBBmY/Xd475Yd4pUI/AAAAAAABHiw/5OlBsyNcNXAzcCZI2a388X2rMSzxAIS8wCLcBGAsYHQ/s400/95BB7E4A-0691-4CA9-B4D5-27516D40CFA3.jpeg&quot; width=&quot;300&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;The back end of the new Cray Shasta compute cabinet&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;div&gt;As with the front end, it opens up with French doors, and interestingly, the rear doors look identical to the front doors. &amp;nbsp;Although I didn't ask explicitly, my guess is that this means that both the front and rear of the cabinets could feature giant cabinet graphics if so desired.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;The rear cabling is almost all copper 200 Gb/s:&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-ZeHIjtCDiAk/Xd48iKhreSI/AAAAAAABHi8/QF9rBirRDBgFCpPZIiGXthajR8c6OUKcwCLcBGAsYHQ/s1600/846CD644-BE1A-4ABE-BFC8-6DC43FE35B64.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;300&quot; src=&quot;https://1.bp.blogspot.com/-ZeHIjtCDiAk/Xd48iKhreSI/AAAAAAABHi8/QF9rBirRDBgFCpPZIiGXthajR8c6OUKcwCLcBGAsYHQ/s400/846CD644-BE1A-4ABE-BFC8-6DC43FE35B64.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Cray Slingshot switch blade and Cray chassis management module&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;div&gt;And, in a departure from the XC and XT/XE lines, all of this copper cabling uses a standard QSFP-DD connectors to carry 2x200 Gb. &amp;nbsp;In the above photo, you can see a genuine Cray Slingshot switch blade slotted in horizontally (cf. the vertically slotted compute blades) and the water coupling for the liquid-cooled switch blade and management module. &amp;nbsp;There are no fancy coolant waterfalls with Shasta, but that's probably not a bad thing. &amp;nbsp;As I've heard it told, the Cray-2 waterfall was a case of making lemonade from lemons; apparently fluorinert reacts corrosively with curved plastic surfaces.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h3&gt;Less-technical tidbits&lt;/h3&gt;
&lt;div&gt;SC isn't purely about the technology, and truth be told, the personalities and community are the principal reason I attend every year. &amp;nbsp;It follows that a number of personal highlights for me weren't directly related to HPC at all but were nevertheless very valuable bits of information that I took away from Denver.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;For example, I met two of the big marketing minds behind a major HPC company who really floored me by attributing value to my support of the HPC industry and community through social media. &amp;nbsp;Social media is really how I got my start in this industry (I started as a hobbyist), so it's gratifying to hear that I might be contributing in a way that is meaningful to kindred spirits who also got into the HPC field from unconventional paths. &amp;nbsp;It was also a reminder that there are always real people behind every corporate Twitter account, and you very well may meet them at a conference like SC. &amp;nbsp;When that happens, it can be a really positive experience (&quot;Great to meet the person behind the handle!&quot;) or an embarrassing one (&quot;I really did say that three years ago, didn't I?&quot;). &amp;nbsp;This year was the first time it became clear that, in trying to avoid the latter case as a matter of course, the former becomes more prevalent without a whole lot of added effort.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;I also met what may have been the world's slickest corporate sales team, whose brilliantly staged choreography of chance encounters over drinks only became apparent to me as I was walking back to my hotel. &amp;nbsp;I know that plenty of people dislike interacting with sales, but being a great salesperson is really a craft in and of itself, and I respect people who are masters of their trade regardless of what it is. &amp;nbsp;And now if I ever find myself in a situation where I need to win someone over cold, I know from whom I can draw inspiration to unleash my inner &quot;customer success manager.&quot; &amp;nbsp;It's a careful balance of drawing out concerns, driving open-ended complaints towards something actionable, and knowing where to cut through red tape and just get the right people talking.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;Another non-technical area in which I was looking for information this year was management philosophy. &amp;nbsp;I've had the pleasure of working with and for some very talented managers who recognize management as a distinct vocation in and of itself, and I made it a point to get time with a few such people who've consistently built me up over the years. &amp;nbsp;One of the more pithy philosophies I took away from one colleague is that there are times when neither &quot;asking for permission&quot; nor &quot;asking for forgiveness&quot; is the right approach—rather, sometimes you have to &quot;radiate intent.&quot; &amp;nbsp;I'd never heard this before, but it makes sense in that it allows others the opportunity to say &quot;no&quot; and take explicit ownership of inaction, but it doesn't require the inverse of saying &quot;yes&quot; and taking responsibility for the outcomes.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h3&gt;Staying organized&lt;/h3&gt;
&lt;div&gt;Finally, I am always trying to figure out the optimal &quot;workflow&quot; for keeping organized at SC, and this year was no different. &amp;nbsp;A few years ago I fully committed to simply not bringing my laptop to the conference venue every day in lieu of bringing a much lighter and more versatile iPad Pro, and this worked fine with two exceptions:&lt;/div&gt;
&lt;div&gt;&lt;ul&gt;&lt;li&gt;For the Parallel I/O in Practice tutorial I co-presented, I brought my laptop so that all four presenters could project from it and I could use my iPad for keeping realtime notes.&lt;/li&gt;&lt;li&gt;For PDSW, I brought my laptop just in case, knowing that I would be in the same room all day. &amp;nbsp;I wound up presenting from it simply because it provided a better viewing angle from the podium; the room arrangements in Denver were such that it was impossible for a speaker at the podium to see the slides being projected, so he or she would have to rely on the device driving the projector to tell what content was actually being projected.&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;I did have to use the laptop at the hotel on Saturday night to make some final modifications to my PDSW talk (there are a few obscure features in PowerPoint that simply aren't exposed in the iOS version), but the rest of the conference (including a couple of BOF talks) that were iPad-only.&lt;/div&gt;
&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;For notetaking, I started storing all of my notes in &lt;a href=&quot;https://agenda.com/&quot;&gt;Agenda&lt;/a&gt;, and where appropriate, used Agenda's feature to create a single note for each calendar entry corresponding to a formal meeting. &amp;nbsp;For unstructured conversations on the expo floor or between sessions, I kept one catch-all note per day in which I typed everything I could remember as soon as the conversation ended. &amp;nbsp;For example, the conversation I had with the designers of the E1000-F enclosure was saved as a combination of obscure written details I took as soon as I left the booth and photos I snapped during the conversation.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;In places where typing on an iPad was not possible (e.g., in most technical sessions, where there were no tables), I used &lt;a href=&quot;https://www.nebo.app/&quot;&gt;Nebo&lt;/a&gt; and an Apple Pencil to take handwritten notes. &amp;nbsp;As it turns out, hand-writing on an iPad sitting on your knee is far more productive than either trying to type text letter-by-letter into the on-screen iPad keyboard or awkwardly balancing the folded-out iPad Pro keyboard on a lap or bag. &amp;nbsp;Nebo is really good at converting handwriting into ASCII, and that ASCII easily copies out and into an Agenda note.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;This workflow supplanted my approach last year which relied exclusively on using &lt;a href=&quot;https://www.gingerlabs.com/&quot;&gt;Notability&lt;/a&gt; and hand-written notes with OCR. &amp;nbsp;In meetings where a table &lt;i&gt;was&lt;/i&gt; available (i.e., vendor briefings), being able to type rather than handwrite was far more effective in capturing every nuance in spoken word. &amp;nbsp;I've found that I rarely ever get a copy of the slides shown at SC briefings, so being able to quickly capture exact hardware specs or release dates as someone is trying to gloss over some unflattering details is really not possible when writing everything by hand.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;For tracking action items, I've started used &lt;a href=&quot;https://culturedcode.com/things/&quot;&gt;Things 3&lt;/a&gt; (which is admittedly crazy expensive) but is really good at capturing to-do items in under five seconds so that they can be more formally sorted, assigned a start/complete date, etc at the end of the day or after the conference.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;This all mostly worked, but I did run into a major issue with Agenda where all my ad-hoc notes vanished when I got home from Denver and my home computer decided to sync. &amp;nbsp;The good news is that Agenda uses internal versioning so the notes' contents weren't truly lost, and their support team was extremely responsive in both recovering my lost notes and releasing a fix within a week. &amp;nbsp;Not a great first experience with the app, but I'm not sure that'll stop me from using it.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h2&gt;Concluding thoughts&lt;/h2&gt;
&lt;div&gt;As always seems to be the case, the week of SC was over before I knew it. &amp;nbsp;There's a lot I know that I didn't get to see in terms of colleagues, exhibitors, and technical program sessions. &amp;nbsp;Of everything I&amp;nbsp;&lt;i&gt;did&lt;/i&gt;&amp;nbsp;get to see, there's there's plenty that I wasn't sure I'd be allowed to write up. &amp;nbsp;So if you happened to get this far and are wondering why I didn't write about the most interesting thing that you got out of the conference this year, odds are that I didn't see it, or if I did, I wasn't sure I was allowed to write about it. &amp;nbsp;And if I&amp;nbsp;&lt;i&gt;did&lt;/i&gt;&amp;nbsp;write about you and you won't get in trouble for being attributed by name, please let me know and I'd be happy to update this post to give you credit.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;Denver was the city of the first SC I ever attended, so I was glad to be back. &amp;nbsp;I was also happy to get to see snow at least once this year:&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-deMUJP9tRlc/Xd5GYZ0K3HI/AAAAAAABHjI/FiiiB_olwWkWTEakUzZbJZ5aQWFMAFz2wCLcBGAsYHQ/s1600/80A3C5C3-E02A-47E8-95A3-8A6A2ADE5241.jpeg&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;300&quot; src=&quot;https://1.bp.blogspot.com/-deMUJP9tRlc/Xd5GYZ0K3HI/AAAAAAABHjI/FiiiB_olwWkWTEakUzZbJZ5aQWFMAFz2wCLcBGAsYHQ/s400/80A3C5C3-E02A-47E8-95A3-8A6A2ADE5241.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;and the convention center did an excellent job of providing space, AV support, catering, and gigantic coffee urns:&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-2oQTWgax8t8/Xd5HFhTgg4I/AAAAAAABHjQ/Mabh8dy-lHE-6Kd_8eVIS9nte6BP0kNfACLcBGAsYHQ/s1600/EE0A8732-B484-4FD1-A531-33B543EAA2B8.jpeg&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;300&quot; src=&quot;https://1.bp.blogspot.com/-2oQTWgax8t8/Xd5HFhTgg4I/AAAAAAABHjQ/Mabh8dy-lHE-6Kd_8eVIS9nte6BP0kNfACLcBGAsYHQ/s400/EE0A8732-B484-4FD1-A531-33B543EAA2B8.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;I got less sleep on average this year than any SC prior (around 6 hours a night), and yet I feel like I accomplished less of what was on my list than ever before. &amp;nbsp;I suppose that's just a sign that the conference (or perhaps my ambition!) continues to grow, and I should expect SC'20 to be even bigger, better, and exhausting.&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>The Purpose of Research Computing is the Research, not the Computing</title>
   <link href="https://hpc.social/2019/the-purpose-of-research-computing-is-the-research-not-the-computing/"/>
   <updated>2019-11-06T00:00:00-07:00</updated>
   <id>https://hpc.social/2019/the-purpose-of-research-computing-is-the-research-not-the-computing</id>
   <content type="html">&lt;p&gt;Absolutely everyone in research computing will agree that supporting
research is their centre’s highest goal.  And they’re not lying,
but at many centres I’ve visited, they aren’t really correct, either.&lt;/p&gt;

&lt;p&gt;The day-to-day work in such a centre, naturally enough, is all about
technical operations - keeping the computers running, updating
software, making sure &lt;code&gt;/scratch&lt;/code&gt; has enough space free, answering
emails.  And of course, it has to be.  But without internal champions
actively and continually turning the focus back to the &lt;em&gt;purpose&lt;/em&gt;
of those activities - the research outcomes that those activities
are meant to support - the internal, technical, activities &lt;em&gt;become&lt;/em&gt;
the purpose of the centre.&lt;/p&gt;

&lt;p&gt;Pretty quickly, you end up with centres that are ranking their
performance quarter to quarter with cluster utilization numbers,
or having all researcher interactions occurring via “tickets” and
measuring volume and mean-time-to-closure of those tickets (because
shorter conversations with researchers are better, right?)  And
once that’s happened, it becomes very hard to change; bytes and
flops and closure rates have become the reason for coming to work.
It’s baked into the reporting, the funding, the staff’s annual
performance review.  Sure, many of these same centres do collect
and in some way report publications, but if publication rates
resulting from work with the centre are down 5% last year because
two productive groups need new capabilities but the centre has
decided to grow current capability, no one is getting an uncomfortable
call from the boss at these centres.  Ticket closure rates going
down 5% though… maybe you’re getting a call.&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;Organizations that care about their clients make their offerings very clear.&quot; src=&quot;https://www.dursi.ca/assets/purpose_research_computing/pexels_inside-a-store-2199190_crop.jpg&quot; style=&quot;float: left; width: 33%; padding: 15px 15px 15px 0px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It doesn’t take very long to spot centres like this, even from the
outside.  On their websites, most prominently of all, are the
statistics that their biggest cluster premiered at position X on
the Top 500, it has such-and-such much disk space, umpty-ump GPUs,
and even more CPUs.  There are elaborate multi-stage sign-up
procedures which make the centre’s own reporting easy but getting
a graduate student started on the cluster tedious.  Their website
will show a couple of dated research success stories, but if a
researcher is visiting the website for the first time and wants to
know basic facts relevant to them, things like “What is a list of
services that the centre offers”, “Can you help my grad student do
X and if so how long would it take”,  “What is current wait times
for resources/for software installation”, the researcher is out of
luck - they’re just directed to a “contact us” email address (which,
of course, feeds into a ticket tracker).&lt;/p&gt;

&lt;p&gt;(Have you ever visited a restaurant webpage and needed like 4 or 5
clicks to get to the menu and their hours?  If the restaurant took
the menu off the website entirely and you instead had to file a
ticket so you could ask specifically if they made spaghetti carbonara,
that’s what most research computing centre websites are like for
researchers.  Organizations that care about their customers make
their offerings very clear.)&lt;/p&gt;

&lt;p&gt;The thing is, using metrics like utilization, tickets, storage and
the like to measure how much research support is happening is
madness, and we all know it’s madness.  You can goose utilization
numbers by letting researchers run jobs inefficiently, by prioritizing
job size distributions that may or may not represent researcher
needs, or by having staff do a lot of benchmarks “just to make
everything’s still good”.  You can keep ticket closure rates up by
having something that should be clarified or automated or fixed and
instead leaving it vague or manual or broken so that there’s a
stream of tickets coming in that are easily closed; or by irrelevantly
dicing what could be a long, productive discussion with a researcher
into a series of shorter “tickets”.&lt;/p&gt;

&lt;p&gt;It’s madness because neither utilization, nor ticket closure rates,
nor storage use, nor even training course enrolment are valuable
to research &lt;em&gt;in and of themselves&lt;/em&gt;.  They are &lt;em&gt;inputs&lt;/em&gt; to the process
of supporting research via computing; not the purpose, not the
desired outcomes.  Being guided by metrics of those inputs and just
hoping that as long as those numbers stay good the best possible
science outcomes will just happen of their own accord is an abdication
of responsibility, and a squandering of scarce research-support
resources.&lt;/p&gt;

&lt;p&gt;And it’s worse than that, of course.  Even a focus on inputs, if
it was being honest, would focus on &lt;em&gt;all&lt;/em&gt; the inputs, and certainly
the most valuable and hardest-to-replace inputs - the technical
staff.   What’s the “utilization” of the staff?  What fraction of
that Ph.D. chemist’s time over there is spent actually enabling
research projects, versus updating software packages or responding
to “why is my job still in the queue” tickets?  How much time does
our data centre monitoring expert spend swapping memory and cables?
Is that up this quarter, or down; and if it’s down, why?  What
fraction of the expertise of the support staff is being used?  What
is the meaningful contribution rate?&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;Inputs produce outpus which produce outcoms which produce impact.  The inputs are not what you should measure.&quot; src=&quot;https://www.dursi.ca/assets/purpose_research_computing/shutterstock_input_outcome.jpg&quot; style=&quot;float: right; width: 50%; padding: 15px 0px 15px 15px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The reason that those staff input metrics aren’t being measured and
others are is simple, and clarifying.  The hardware inputs aren’t
being used as metrics due to a (false) belief that they are meaningful
in and of themselves, nor because of an (incorrect) understanding
that they are they can be taken in a principled way as a proxy for
the desired research outcomes.  They’re used because they’re easy
to gather.  And they’re comfortable to use because they don’t really
require centre managers to make any hard choices.&lt;/p&gt;

&lt;p&gt;Focussing on the inputs instead of the outputs - or even better,
outcomes - isn’t only a research computing thing, of course.  It’s
an absolutely classic mistake in a lot of sectors; a google search
for &lt;a href=&quot;https://www.google.com/search?q=focus+on+outcomes%2C+not+inputs&amp;amp;oq=focus+on+outcomes%2C+not+inputs&quot;&gt;focus on outcomes, not
inputs&lt;/a&gt;
returns 139 million results.&lt;/p&gt;

&lt;p&gt;There are two prototypical reasons why it happens.  If I were feeling
in a twitter-ranty mood again, I might be tempted to draw the analogy
to the first case - lack of competition, due to private- or
public-sector monopolies, reducing the urgency of focusing on
customer’s needs.  You see this in internal departments of large
organizations, where the “customer base” is locked in, or in other
places where there’s no real competition (Hello most departments
of motor vehicles, or cable companies, or Google Mail support!).
These departments end up developing a relentless internal focus,
having cryptic and opaque internal decision-making processes seemingly
unrelated to what their clients actually want, and famously make
clients jump through hoops to get their needs met.  This isn’t
caused by malevolence, or even indifference; it couldn’t be for it
to be so widespread.  It’s just that, absent any real driver to
focus on &lt;em&gt;customer&lt;/em&gt; outcomes, it is almost impossible to drive
internal priorities towards anything other than internal efficiencies.
Those few companies in this situation that &lt;em&gt;do&lt;/em&gt; manage to maintain
a focus on client outcomes are doing so by constantly expending
almost heroic levels of unseen effort inside the organization.&lt;/p&gt;

&lt;p&gt;But I don’t actually think that’s what driving some research computing
centres inputs focus when it comes to operations and technical
decision making.  I think it comes almost from the other direction,
the other classic case; that of small nonprofits, typically enormously
concerned with their clients, who focus first on a very basic need
and then don’t know how to generalize beyond that as they grow.&lt;/p&gt;

&lt;p&gt;Imagine a small nonprofit, passionately committed to helping people,
that gets its start meeting a very basic need - let’s say they’re
providing before-school breakfasts to children in or near poverty.
At that level, the activity &lt;em&gt;is&lt;/em&gt; the outcome; they can count the
number of breakfasts served, try to get better at serving breakfasts
with a given amount of donations, work on raising money to fund
more breakfasts, maybe expand to different schools or supplying a
wider range of breakfasts to be inclusive of students with particular
dietary needs.  They are &lt;em&gt;super&lt;/em&gt; committed to their clients.&lt;/p&gt;

&lt;p&gt;But as that nonprofit starts expanding, it becomes clear their
client base needs a wider range of services.  It starts partnering
with food banks, to help fight student hunger at home; its staff
participate in some after-school tutoring programs.  But it has no
way to prioritize these activities.  Is it a hunger-fighting
nonprofit?  Is it a help-underprivledged-students-succeed-at-school
nonprofit?  If they could double the tutoring efforts at the cost
of slowing the growth of the breakfast program next year, is that
the right thing to do, or not?  How would they know?&lt;/p&gt;

&lt;p&gt;This is a terrifying transition for a nonprofit to go through.
Before, it knew exactly what it was doing, and had very clear metrics
for success.  In this intermediate stage, it probably has some
earmarked resources to participate in the tutoring and foodbanks,
and it touts that work, but it doesn’t know how to do anything but
report on school breakfasts.  To go beyond this means making choices
about what it will prioritize - and more scarily, what it will &lt;em&gt;not&lt;/em&gt;
prioritize - and working on program evaluation plans for the much
more ambitious but more ambiguous goals of “fighting hunger” or
“helping students succeed at school”.   Many nonprofits never make
that transition.  Some stay small and focussed, which works well
but limits their impact; many stay in limbo in that uncomfortable
intermediate state until they are overtaken by events or other
organizations.&lt;/p&gt;

&lt;p&gt;At most research computing centres, I think the story is more like
that of the nonprofit.  Except let’s be honest, while providing
breakfasts is inherently meaningful and has very few organizations
willing to do it, providing cycles and storage isn’t, and has many
alternate providers.&lt;/p&gt;

&lt;p&gt;But going beyond meeting the basic needs of providing research
computing cycles and storage, which was a much greater need in the
90s than it is today, is genuinely hard.  It’s very labour intensive -
it requires going out to the entire research community you aim
to support, including those who you’ve never had a working relationship
with, and understanding needs.  It’s very uncomfortable - you have
to then prioritize those needs based on their value to the larger
enterprise and to where you can make the most difference, and that
means having awkward conversations bout &lt;em&gt;not&lt;/em&gt; prioritizing other
needs.  And it’s incredibly uncertain - it means going from evaluations
based on numbers on a dashboard that are largely under your control,
to unfamiliar qualitative evaluations and doing the hard work of
trying to measure research outcomes.&lt;/p&gt;

&lt;p&gt;But there’s a relatively straightforward approach to get there
starting from where you are.  It takes some work, but just going
through the process is clarifying.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;What do you do now?&lt;/strong&gt;  You know, broadly, what services you offer to
researchers, you’ve just never had to make it explicit.  Start to put
together a very simple &lt;a href=&quot;https://www.cherwell.com/library/blog/7-steps-to-defining-and-designing-an-effective-service-catalog/&quot;&gt;service catalog&lt;/a&gt;.
It doesn’t have to be very complicated; figure out internally what
services you offer, at quite a high level, in language that researchers
would care about.  Get staff to offer suggestions.  For each service,
for internal consumption only, figure out what’s involved in providing it - what are 
typical amount of hours involved, who has to coordinate with whom, &lt;em&gt;etc.&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;How do those services help researchers?&lt;/strong&gt;  Again, you have a broad
sense of this, but make it concrete.  Is it more publications?
Higher-impact publications?  &lt;em&gt;Faster&lt;/em&gt; publication?  Better collaboration
opportunties?  Higher funding success rates?  Better job prospects for
students and postdocs?  More successful faculty or postdoc recruitment?
Friendly users or your VPR can help with this. Come up with a handful
that seem most important in your user community.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Connect services and benefits.&lt;/strong&gt; Come up with some concrete
examples of how you’ve provided each of those benefits with the services you
make available.  You may find benefits that you can’t yet justify claiming
you provide, or services you’ve forgotten about.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Refine your services and benefits lists.&lt;/strong&gt; Start talking about
these benefits and services, in talks at user groups or when doing
outreach to departments, new hires, incoming graduate student
training, and the like.  Find out which ones attract attention,
which ones don’t.  Ask for suggestions for new items for the lists,
and new conncetions between the two.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Start thinking about indicators and evaluation.&lt;/strong&gt;  Besides anecdotes,
how could you convince your funder, or senior leadership at your institution,
that you provide those benefits?  How could you show you were getting
better? How could you convince them that a 15% increase in funding would
provide some meaningful improvement to research institution?  The answer
will depend on the benefits you’ve chosen, but there are lots of
&lt;a href=&quot;https://www.councilofnonprofits.org/tools-resources/evaluation-and-measurement-of-outcomes&quot;&gt;resources&lt;/a&gt;
out &lt;a href=&quot;https://managementhelp.org/evaluation/outcomes-evaluation-guide.htm&quot;&gt;there&lt;/a&gt;
to help you with this.  Closer to home, I absolutely promise you
there are people at your instution who will talk to you about program
evaluation until you want to pass out just to enjoy some quiet.
What you come up with will seem quite different to you.  They won’t
be instruments with 3 decimal places of accuracy; they
may be citation counts or randomly sampled surveys or qualitative
interviws.  Measuring research is hard - but everyone in research
knows and understands this.  Approaches like short surveys or
interviews are labour intensive, but provide amazing information -
they will provide a constant stream of incoming success stories that
you can make use of, and less successful storis you can learn from.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Start thinking about rebalancing your service offerings&lt;/strong&gt;.  Once
you have these lists, and approaches to evaluations, then and only
then do you have a principled way to make decisions about in which services
to invest more, and in which to invest less.  And you’ll have very
convincing arguments to take to funders and leadership.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I get it that going through this process to the point where you
can meaningfully what ask the right next thing to do to help
research isn’t easy.  It absolutely isn’t.  It’s a lot of work,
and while it is useful in many different ways, it still doesn’t
make things easy - if anything, it forces you to confront tradeoffs
and hard choices that focusing on inputs may have let you avoid.  A centre that hasn’t
been thinking this way for a while will have some low-hanging fruit
that can be picked to start, but after that there will be  multiple
ways for a centre to be supporting research, and no clear answer
which is the “best”.   Making those choices will require knowing
the strengths of the centre and knowing where those strengths are
needed in the research community it serves — and not all research
needs are the same!  But &lt;em&gt;those&lt;/em&gt; are questions that team leaders
need to be wrestling with.&lt;/p&gt;

&lt;p&gt;The alternative, just running a set of computers for the same
friendly user group of people year after year, isn’t research
support; it’s a hobby.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Computational Science Collaborations Train Great Managers - But Trainees Might Need Help To Become Good Managers First</title>
   <link href="https://hpc.social/2019/computational-science-collaborations-train-great-managers-but-trainees-might-need-help-to-become-good-managers-first/"/>
   <updated>2019-09-29T01:00:00-06:00</updated>
   <id>https://hpc.social/2019/computational-science-collaborations-train-great-managers-but-trainees-might-need-help-to-become-good-managers-first</id>
   <content type="html">&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;What I write below likely applies to fields of theoretical
and observational science that involve collaborations, too. I think
the experiences that trainees in laboratory science are likely
significantly different, as are those people who spent a large
amount of time working in a single group in a well-defined large
project.  I’d certainly like to hear from colleagues from those
areas; are there similarities, or are things quite different?&lt;/em&gt;&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;We don’t like to talk about it much, but the natural career path 
in academia - from undergrad to research assistant to
graduate student, postdoc, Junior PI and beyond - involves a
progression first towards and then away from the actual doing of
science to the coordinating, orchestrating, planning of, and
advocating for scientific projects; or, in other words, management
(though we’d &lt;em&gt;never&lt;/em&gt; call it that; academia is full of
open disdain for anything that smacks of management, marketing,
or other suspiciously real-world activities).&lt;/p&gt;

&lt;p&gt;But computational-science academic environments are pretty particular places, with different approaches to working in a team than, say, in much of business.&lt;/p&gt;

&lt;p&gt;First, academic work is largely performed by trainees like students, who
have a very different relationship to their academic supervisor than an
employee does to their manager.  At its best, when an academic lab
is run by ethical and competent leadership&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a class=&quot;footnote&quot; href=&quot;https://www.dursi.ca/feed.xml#fn:1&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;, significant effort
is put into developing those trainees, giving them increasing
responsibilities, and looking for opportunities for them to apply
those emerging skills to new problems.&lt;/p&gt;

&lt;p&gt;Second, since much of the work is on open research problems, it’s
very difficult to judge how long something “should” take, so deadlines
in assigning tasks is relatively uncommon; updates tend to sound like
“here’s what I managed to get done this week,” and it is what it is.&lt;/p&gt;

&lt;p&gt;Third, due to the open-endedness, the trainee/mentor relationship,
and modelling the extreme independence of senior academics, there
is a norm of collegiality.  Directing someone’s work
comes across as very heavy handed; the final work output can be
ruthlessly assessed, but path to get there, the work process, is
somewhat off-limits.&lt;/p&gt;

&lt;p&gt;Fourth, it’s common - maybe even the norm - for projects to be
tackled with others outside of not only the team, but in different
institutions entirely.&lt;/p&gt;

&lt;p&gt;Finally, the independence of researchers, the dynamic nature of
research, and the fact that so many coworkers are elsewhere mean
many working relationships are comparatively
short-lived.&lt;/p&gt;

&lt;p&gt;So imagine that you are a postdoc - the most senior trainee - in a
computational lab, routinely working in multi-institutional
collaborations, and this is where you’re developing your people and
project management chops.  You are directing some particular piece
of research that will lead to publications key for your career.
You aim to be the lead author on one or more of those papers and
you have a clear idea of the path to get there, and you’re driven —
you’ll be on the job market next year and you learned at a
conference that a competitor’s lab is looking at some of the same
questions.&lt;/p&gt;

&lt;p&gt;But your “project team” are peers or even academics
more senior than you, and many are outside your institution entirely;
getting them to do anything is a matter of persuasion.  Your local,
more junior, trainees are just beginning their journey, and for
them to contribute meaningfully you have to find ways to develop
their skills and the best ways to benefit from those skills.
You want everyone to be invested, contributing, and growing their
skills, and you don’t have time to direct people in how to do
their work even if you were so inclined. And the clock is ticking.&lt;/p&gt;

&lt;p&gt;What kind of skills are you developing as you’re thrust into this situation?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Much of the computing and technical community is teaching itself
things about management that the rest of the world has known for a
half-century.  Famously, Google’s Project Oxygen, which was at
aimed least in part at “proving” that management does not matter,
started in 2008 looking at management across teams in the organization,
and found that it really does.  (Surprise!).  Their original findings
identified &lt;a href=&quot;https://www.inc.com/marcel-schwantes/the-8-biggest-things-that-google-managers-do-to-su.html&quot;&gt;8 characteristics of managers of successful
teams&lt;/a&gt;,
and 3 pitfalls that managers in less successful teams fell into.&lt;/p&gt;

&lt;p&gt;Those characteristics of good managers, in decreasing order of importance:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;They’re good coaches.&lt;/li&gt;
  &lt;li&gt;They empower their team and don’t micro-manage.&lt;/li&gt;
  &lt;li&gt;They express interest in their team members’ success and personal well-being.&lt;/li&gt;
  &lt;li&gt;They’re productive and results-oriented.&lt;/li&gt;
  &lt;li&gt;They’re good communicators and they listen to the team.&lt;/li&gt;
  &lt;li&gt;They help employees with career development.&lt;/li&gt;
  &lt;li&gt;They have a clear vision and strategy for the team.&lt;/li&gt;
  &lt;li&gt;They have key technical skills that help them advise the team.&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a class=&quot;footnote&quot; href=&quot;https://www.dursi.ca/feed.xml#fn:2&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;How will our postdoc rate against those criteria?  Well:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;They are going to be very concerned with skills development in their direct reports, encouraging them on to bigger and better things — so the postdoc learns to be a good coach;&lt;/li&gt;
  &lt;li&gt;They certainly won’t micromanage — they’ll let team members decide how to approach their work;&lt;/li&gt;
  &lt;li&gt;They’ll be very aware of the need to develop their team member’s success, partly due to norms around credit, and partly due to expectations that people move up and on in their careers;&lt;/li&gt;
  &lt;li&gt;They’re very focussed on high-quality work and the steps to get there;&lt;/li&gt;
  &lt;li&gt;Because they’ve never been able to rely on role power or organizational authority to coordinate the getting work done, they will have developed the communication skills necessary to communicate with and listen to the team, at least around tasks;&lt;/li&gt;
  &lt;li&gt;Thinking about team-members career advancement needs and goals is going to be second nature, although they might take it for granted that they know what the team members goals are;&lt;/li&gt;
  &lt;li&gt;They’ll have quite clear goals and a vision, even if they’re not used to having to communicate it explicitly in an environment where everyone knows that the goal is discrete publications and can clearly see where their work fits in;&lt;/li&gt;
  &lt;li&gt;They will typically be quite proficient in their relevant technical skills.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;That’s just about a clean sweep!&lt;/p&gt;

&lt;p&gt;So I claim that the sort of training that I’ve seen people get on projects in
computational (or observational or theoretical) science collaborations equips
people with the advanced skills to become great managers.&lt;/p&gt;

&lt;p&gt;But there’s a downside.  The very hands-off approach to management
(indeed, the refusal to countenance that “management” is even an
appropriate thing for scientists to stoop to) means that some of
the more basic, fundamental, skills are lacking.  The same early
work at Google pointed out key shortcomings of their less successful
managers:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Have trouble making a transition to the team.&lt;/li&gt;
  &lt;li&gt;Lack a consistent approach to performance management.&lt;/li&gt;
  &lt;li&gt;Spend too little time managing and communicating.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;And those start to look like real sticking points for our postdoc.
Almost no one is taught management skills before their first
management position, but scientists are more or less taught that
management &lt;em&gt;shouldn’t&lt;/em&gt; be done; “we’re not that sort of people”.
So:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Making the transition to being the manager of the team is going to be doubly difficult for our postdoc — both in internalizing their role as a manager, and in putting the time in to develop really solid working relationships with the team members.&lt;/li&gt;
  &lt;li&gt;Performance communications - giving people feedback (positive and negative) on their work often and regularly, rather than waiting weeks or months for some big sub-project to be done and then assessing the finished project — is going to be completely foreign, if not anathema, to them.&lt;/li&gt;
  &lt;li&gt;Our postdoc is going to spend little to no time actually managing, or communicating about the things they haven’t had to communicate about before like sharing the vision aind aims of the team, or finding out the specific career goals of individual team members.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So while many or even all of the advanced skills might well be in
place for scientist trainees to excel at management outside of the
academy, the basic skills — or even models of what the basic skills
would look like — are often going to be lacking.&lt;/p&gt;

&lt;p&gt;But those basic skills are the easiest to address!  Anyone can learn
them, and someone who’s spent a good chunk of their career in the
sciences &lt;em&gt;certainly&lt;/em&gt; can.&lt;/p&gt;

&lt;p&gt;So many computational scientists do end up becoming good —
and so quickly become great — managers successfully on their
own, but it can take a lot of trial and error, and be stressful for
all involved.  (My own transition towards becoming good has been….  uneven.)&lt;/p&gt;

&lt;p&gt;I don’t think that transition &lt;em&gt;has&lt;/em&gt; to be so difficult; today there are 
some fantastic resources out
there to help.  And maybe it’s where I’ve been looking or which
materials resonate with me, but a lot of the strongest sources
that really nail the basics come from people on the more technical
side.  I’m a huge fan of &lt;a href=&quot;https://www.manager-tools.com&quot;&gt;Manager Tools&lt;/a&gt;,
which have amassed a huge library a “here’s a number of steps you
can start taking today” podcasts and a couple of books, with data
to back them up.  A number of colleagues really like &lt;a href=&quot;https://www.oreillyå.com/library/view/the-managers-path/9781491973882/&quot;&gt;The Manager’s
Path&lt;/a&gt;
which takes a career-long view of stepping up the career ladder
(set in the tech industry but most of the material would carry over
to other fields) and the different skills and responsibilities
needed at each stage.  And Google’s ongoing organizational reasearch and
resulting training materials at
&lt;a href=&quot;https://rework.withgoogle.com&quot;&gt;rework.withgoogle.com&lt;/a&gt; are well
worth reading.&lt;/p&gt;

&lt;p&gt;Scientists learn a lot of transferrable skills in their training,
and the world needs more of their input in teams and projects
across all sectors.  There’s a stereotype about scientists being
nerdy introverts, but collaborations across institutions build really 
quite advanced communications and management skills that can serve
them very well almost anywhere.  And if they need a little help
adjusting to the different skills needed for managment of 
projects or teams outside of academia, there are resources out there now to
help them succeed.  If there are some that have especially helped
you, please do share them with me and I’ll list them here.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;Which, more visibly now than ever before, cannot be taken as a given. &lt;a class=&quot;reversefootnote&quot; href=&quot;https://www.dursi.ca/feed.xml#fnref:1&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;

    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;Note that at Google, technical skills are &lt;em&gt;dead last&lt;/em&gt; as a skill for managers; but that largely results from Google having such a high technical bar for hiring. &lt;a class=&quot;reversefootnote&quot; href=&quot;https://www.dursi.ca/feed.xml#fnref:2&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;

    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>ISC'19 Recap</title>
   <link href="https://hpc.social/2019/isc-19-recap/"/>
   <updated>2019-06-27T01:31:00-06:00</updated>
   <id>https://hpc.social/2019/isc-19-recap</id>
   <content type="html">&lt;p&gt;I was fortunate enough to attend the &lt;a href=&quot;https://www.isc-hpc.com/&quot;&gt;ISC HPC conference&lt;/a&gt; this year, and it was a delightful experience from which I learned quite a lot.  For the benefit of anyone interested in what they have missed, I took the opportunity on the eleven-hour flight from Frankfurt to compile my notes and thoughts over the week.&lt;br /&gt;&lt;br /&gt;I spent most of my time in and around the sessions, BOFs, and expo focusing on topics related to I/O and storage architecture, so that comprises the bulk of what I’ll talk about below.  Rather than detail the conference chronologically as &lt;a href=&quot;https://glennklockwood.blogspot.com/2018/11/a-week-in-life-of-sc-attendee.html&quot;&gt;I did for SC’18&lt;/a&gt; though, I’ll only mention a few cross-cutting observations and trends here.&lt;br /&gt;&lt;br /&gt;I’ll also not detail the magnificent &lt;a href=&quot;https://hps.vi4io.org/events/2019/iodc&quot;&gt;HPC I/O in the Data Center workshop&lt;/a&gt; here, but anyone reading this who cares about storage or I/O should definitely flip through the slides on the &lt;a href=&quot;https://hps.vi4io.org/events/2019/iodc#agenda&quot;&gt;HPC-IODC workshop website&lt;/a&gt;!  This year HPC-IODC and &lt;a href=&quot;https://wopsss.org/&quot;&gt;WOPSSS&lt;/a&gt; merged their programs, resulting in a healthy mix of papers (in both CS research and applied research), expert talks, and fruitful discussion.&lt;br /&gt;&lt;br /&gt;&amp;lt;h2&amp;gt;High-level observations&amp;lt;/h2&amp;gt;As is often the case for ISC, there were a few big unveilings early in the week.  Perhaps the largest was the disclosure of several key architectural details surrounding the &lt;a href=&quot;https://aurora.alcf.anl.gov/&quot;&gt;Aurora exascale system to be deployed at Argonne in 2021&lt;/a&gt;.  &lt;a href=&quot;https://www.tacc.utexas.edu/systems/frontera&quot;&gt;TACC’s Frontera system&lt;/a&gt;, a gigantic Dell cluster stuffed with Intel Cascade Lake Xeons, made its debut on the Top500 list as well.  In this sense, Intel was in good form this year.  And Intel has to be, since only one of the handful of publicly disclosed pre-exascale (&lt;a href=&quot;https://www.nextplatform.com/2018/10/30/berkeley-lab-first-in-line-for-cray-shasta-supercomputers/&quot;&gt;Perlmutter&lt;/a&gt; and &lt;a href=&quot;https://www.nextplatform.com/2018/06/21/details-emerge-on-post-k-exascale-system-with-first-prototype/&quot;&gt;Fugaku&lt;/a&gt;) and exascale systems (&lt;a href=&quot;https://www.nextplatform.com/2019/05/07/cray-amd-tag-team-on-1-5-exaflops-frontier-supercomputer/&quot;&gt;Frontier&lt;/a&gt;) will be using Intel parts.&lt;br /&gt;&lt;br /&gt;The conference had also had an anticipatory undertone as these pre-exascale and exascale systems begin coming into focus.  The promise of ARM as a viable HPC processor technology is becoming increasingly credible as &lt;a href=&quot;https://vanguard.sandia.gov/astra/index.html&quot;&gt;Sandia’s Astra machine&lt;/a&gt;, an all-ARM cluster integrated by HPE, appeared throughout the &lt;a href=&quot;https://2019.isc-program.com/&quot;&gt;ISC program&lt;/a&gt;.  These results are paving the way for Fugaku (the “post-K” machine), which will prove ARM and its SVE instruction set at extreme scale.&lt;br /&gt;&lt;br /&gt;Also contributing to the anticipatory undertone was a lot of whispering that occurred outside of the formal program.  The recently announced acquisition of Cray by HPE was the subject of a lot of discussion and conjecture, but it was clear that the dust was far from settled and nobody purported to have a clear understanding of how this would change the HPC market.  There was also some whispering about a new monster Chinese system that was on the cusp of making this year’s ISC Top500.  Curiously, the Wuxi supercomputer center (where Tianhe-2 is housed) had a booth on the show floor, but it was completely vacant.&lt;br /&gt;&lt;br /&gt;Also noticeably absent from the show floor was NVIDIA, although they certainly sent engineers to participate in the program.  By comparison, AMD was definitely present, although they were largely promoting the impending launch of Rome rather than their GPU lineup.  A number of HPC solutions providers were excited about Rome because of both high customer demand and promising early performance results, and there wasn’t a single storage integrator with whom I spoke that wasn’t interested in what doors will open with an x86 processor and a PCIe Gen4 host interface.&lt;br /&gt;&lt;br /&gt;&amp;lt;h2&amp;gt;Intel disclosures about Aurora 2021&amp;lt;/h2&amp;gt;Perhaps the biggest news of the week was a “&lt;a href=&quot;https://2019.isc-program.com/presentation/?id=inv_sp184&amp;amp;sess=sess212&quot;&gt;special event&lt;/a&gt;” presentation given by Intel’s Rajeeb Hazra which disclosed a number of significant architectural details around the Aurora exascale system being deployed at Argonne National Laboratory in 2021.&lt;br /&gt;&lt;br /&gt;&amp;lt;h3&amp;gt;Nodes will be comprised of Intel Xeon CPUs and multiple Intel GPUs&amp;lt;/h3&amp;gt;Intel has confirmed that Aurora will be built on Intel-designed general-purpose GPUs based on the “Xe” architecture with multiple GPUs per node.  With this disclosure and the knowledge that nodes will be connected with Cray’s Slingshot interconnect, it is now possible to envision what a node might look like.  Furthermore, combining the disclosure of a high GPU:CPU ratio, the Aurora power budget, and some vague guessing at the throughput of a 2021 GPU narrows down the number of nodes that we may expect to see in Aurora.&lt;br /&gt;&lt;br /&gt;Although no specific features of the Intel GPUs were disclosed, Intel was also promoting their new &lt;a href=&quot;https://en.wikichip.org/wiki/x86/avx512vnni&quot;&gt;AVX512-VNNI instructions&lt;/a&gt; to position their latest top-bin Xeon cores as the best option for inference workloads.  Coupled with what we can assume will be highly capable GPUs for training acceleration, Intel is building a compelling story around their end-to-end AI portfolio.  Interestingly, news that &lt;a href=&quot;https://www.nextplatform.com/2019/06/17/nvidia-makes-arm-a-peer-to-x86-and-power-for-gpu-acceleration/&quot;&gt;NVIDIA is partnering with ARM&lt;/a&gt; dropped this past week, but NVIDIA’s noted absence from ISC prevented a comparable ARM-NVIDIA AI solution from shining through.&lt;br /&gt;&lt;br /&gt;&amp;lt;h3&amp;gt;System will have over 10 PB of system memory&amp;lt;/h3&amp;gt;Aurora will have a significant amount of memory presumably comprised of a combination of HBM, DDR, and/or Optane persistent memory.  The memory capacity is markedly higher than that of the AMD-based Frontier system, suggesting that Intel may be leveraging Optane persistent memory (which has a lower cost per bit than DDR) to supplement the HBM that is required to feed such a GPU-heavy architecture.&lt;br /&gt;&lt;br /&gt;&amp;lt;h3&amp;gt;The storage subsystem will deliver over 230 PB of capacity at over 25 TB/sec&amp;lt;/h3&amp;gt;Perhaps the most interesting part of Aurora is its I/O subsystem, which will use an object store and an all-solid-state storage architecture instead of the traditional parallel file system.  This will amount to 230 PB of usable flash capacity that can operate in excess of 25 TB/sec.  Although I’ll describe this storage architecture in more depth below, combining the performance point of 25 TB/sec with the aforementioned high GPU:CPU ratio suggests that each compute node will be able to inject a considerable amount of I/O traffic into the fabric.  This points to very capable Xeon cores and very capable NICs.&lt;br /&gt;&lt;br /&gt;&amp;lt;h3&amp;gt;The programming model for the system will utilize SYCL&amp;lt;/h3&amp;gt;Intel has announced that its “One API’ relies on the &lt;a href=&quot;https://www.khronos.org/sycl/&quot;&gt;Khronos Group’s SYCL standard&lt;/a&gt; for heterogeneous programming in C++ rather than the incumbent choices of OpenMP, OpenACC, or OpenCL.  This does not mean that OpenMP, OpenACC, and/or OpenCL won’t be supported, but it does reveal where Intel intends to put all of its efforts in enabling its own GPUs and FPGAs for HPC.  They further emphasized their desire to keep these efforts open, standards-based, and portable, undoubtedly demonstrating stark contrast with the incumbent GPU vendors.  This is an interesting long-term differentiator, but time will tell whether SYCL is able to succeed where OpenCL has failed and gain a foothold in the HPC ecosystem.&lt;br /&gt;&lt;br /&gt;&amp;lt;h2&amp;gt;DAOS will be HPC’s gateway drug to object stores&amp;lt;/h2&amp;gt;DAOS (the “Distributed Asynchronous Object Store,” pronounced like it’s spelled) is an object store that Intel has been developing for the &lt;a href=&quot;https://www.theregister.co.uk/2012/07/11/doe_fastforward_amd_whamcloud/&quot;&gt;better part of a decade in collaboration with the US Department of Energy&lt;/a&gt;.  The DAOS name has become overloaded in recent years as a result of it changing scope, focus, and chief architects, and the current version is quite different from the original DAOS that was prototyped as a part of the DOE Fast Forward program (e.g., only &lt;a href=&quot;https://www.snia.org/sites/default/files/SDC15_presentations/dist_sys/EricBarton_DAOS_Architecture_Extreme_Scale.pdf&quot;&gt;one of three original DAOS components, DAOS-M, survives&lt;/a&gt;).  A few key features remain the same, though:&lt;br /&gt;&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;It remains an object store at its core, but various middleware layers will be provided to expose alternate access APIs and semantics&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;It is specifically designed to leverage Intel Optane persistent memory and NAND-based flash to deliver extremely high IOPS in addition to high streaming bandwidth&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;It relies on user-space I/O via &lt;a href=&quot;http://mercury-hpc.github.io/&quot;&gt;Mercury&lt;/a&gt; and &lt;a href=&quot;https://spdk.io/&quot;&gt;SPDK&lt;/a&gt; to enable its extreme I/O rates&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Its &lt;a href=&quot;https://github.com/daos-stack/daos/blob/master/doc/storage_model.md#4.1&quot;&gt;storage architecture&lt;/a&gt; is still based on a hierarchy of servers, pools, containers, and objects&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;Object stores have historically not found success in HPC due to HPC apps’ general dependence on POSIX-based file access for I/O, but the Aurora DAOS architecture cleverly bridges this gap.  I was lucky enough to run into Johann Lombardi, the DAOS chief architect, at the Intel booth, and he was kind enough to walk me through a lot of the details.&lt;br /&gt;&lt;br /&gt;DAOS will provide seamless integration with a POSIX namespace by using &lt;a href=&quot;https://jira.whamcloud.com/browse/LU-11376&quot;&gt;Lustre’s new foreign layout feature&lt;/a&gt; which &lt;a href=&quot;https://www.eofs.eu/_media/events/lad18/15_johann_lombardi_intel_cross_tier_unified_namespace_v3.pdf&quot;&gt;allows an entity in the Lustre namespace to be backed by something that is not managed by Lustre&lt;/a&gt;.  In practice, a user will be able to navigate a traditional file namespace that looks like any old Lustre file system using the same old ls and cd commands.  However, some of the files or directories in that namespace may be &lt;a href=&quot;https://github.com/daos-stack/daos/blob/master/src/client/dfs/README.md&quot;&gt;special DAOS objects&lt;/a&gt;, and navigating into a DAOS-based object transparently switches the data path from one that uses the traditional Lustre client stack to one that uses the DAOS client stack.  In particular,&lt;br /&gt;&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;Navigating into a directory that is backed by a DAOS container will cause the local DAOS agent to mount that DAOS container as a POSIX namespace using FUSE and junction it into the Lustre namespace.  Files and subdirectories contained therein will behave as regular POSIX files and subdirectories for the most part, but they will only honor a subset of the POSIX consistency semantics.&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Accessing a file that is backed by a DAOS container (such as an HDF5 file) will cause the client to access the contents of that object through whatever API and semantics the DAOS adapter for that container format provides.&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;DAOS also includes a preloadable library which allows performance-sensitive applications to bypass the FUSE client entirely and map POSIX API calls to DAOS native API calls.  For applications that use middleware such as HDF5 or MPI-IO, I/O will be able to entirely bypass the POSIX emulation layer and get the highest performance through DAOS-optimized backends.  In the most extreme cases, applications can also write directly against the DAOS native object API to control I/O with the finest granularity, or use one of DAOS’s addon APIs that encapsulate other non-file access methods such as key-value or array operations.&lt;br /&gt;&lt;br /&gt;A significant amount of this functionality is already implemented, and Intel was showing DAOS performance demos at its booth that used both IOR (using the DAOS-native backend) and Apache Spark:&lt;br /&gt;&lt;br /&gt;&amp;lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&amp;gt;&lt;a href=&quot;https://1.bp.blogspot.com/-pQCEzVp5fKE/XRP6nbvlVrI/AAAAAAABFdE/VhnOgDCJmKUvdp6NQfuF29oCWvnZvGy5ACLcBGAs/s1600/IMG_6381.JPG&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;320&quot; src=&quot;https://1.bp.blogspot.com/-pQCEzVp5fKE/XRP6nbvlVrI/AAAAAAABFdE/VhnOgDCJmKUvdp6NQfuF29oCWvnZvGy5ACLcBGAs/s320/IMG_6381.JPG&quot; width=&quot;240&quot; /&gt;&lt;/a&gt;&amp;lt;/div&amp;gt;
&lt;br /&gt;&lt;br /&gt;The test hardware was a single DAOS server with Intel Optane DIMMs and two Intel QLC NAND SSDs and demonstrated over 3 GB/sec on writes and over a million read IOPS on tiny (256-byte) transfers.  Johann indicated that their testbed hardware is being scaled up dramatically to match their &lt;a href=&quot;https://wiki.hpdd.intel.com/display/DC/roadmap&quot;&gt;extremely aggressive development schedule&lt;/a&gt;, and I fully expect to see performance scaling results at SC this November. &lt;br /&gt;&lt;br /&gt;This is all a far cry from the original Fast Forward DAOS, and this demo and discussion on the show floor was the first time I felt confident that DAOS was not only a good idea, but it was a solution that can realistically move HPC beyond the parallel file system.  Its POSIX compatibility features and Lustre namespace integration provide enough familiarity and interoperability to make it something usable for the advanced HPC users who will be using the first exascale machines.&lt;br /&gt;&lt;br /&gt;At the same time, it applies a number of new technologies in satisfying ways (Mercury for user-space network transport, &lt;a href=&quot;https://www.pdl.cmu.edu/PDL-FTP/PDSI/CMU-PDL-08-110.pdf&quot;&gt;GIGA+ for subtree sharding&lt;/a&gt;, Optane to coalesce tiny I/Os, …) that, in most ways, puts it at technological parity with other high-performance all-flash parallel storage systems like &lt;a href=&quot;https://www.weka.io/&quot;&gt;WekaIO&lt;/a&gt; and &lt;a href=&quot;https://www.vastdata.com/&quot;&gt;VAST&lt;/a&gt;.  It is also resourced at similar levels, with DOE and Intel investing money and people in DAOS at levels comparable to the venture capital that has funded the aforementioned competitors.  Unlike its competitors though, it is completely open-source and relies on standard interfaces into hardware (&lt;a href=&quot;https://ofiwg.github.io/libfabric/&quot;&gt;libfabric&lt;/a&gt;, &lt;a href=&quot;http://spdk.io/&quot;&gt;SPDK&lt;/a&gt;) which gives it significant flexibility in deployment.&lt;br /&gt;&lt;br /&gt;As with everything exascale, only time will tell how DAOS works in practice.  There are plenty of considerations peripheral to performance (data management policies, system administration, and the like) that will also factor into the overall viability of DAOS as a production, high-performance storage system.  But so far DAOS seems to have made incredible progress in the last few years, and it is positioned to shake up the HPC I/O discussion come 2021.&lt;br /&gt;&lt;br /&gt;&amp;lt;h2&amp;gt;The Cloud is coming for us&amp;lt;/h2&amp;gt;This ISC also marked the first time where I felt that the major cloud providers were converging on a complete HPC solution that could begin eroding campus-level and mid-range HPC.  Although application performance in the cloud has historically been the focus of most HPC-vs-cloud debate, compute performance is largely a solved problem in the general sense.  Rather, data—its accessibility, performance, and manageability—has been the single largest barrier between most mid-range HPC users and the cloud.  The convenience of a high-capacity and persistent shared namespace is a requirement in all HPC environment, but there have historically been no painless ways to produce this environment in the cloud.&lt;br /&gt;&lt;br /&gt;AWS was the first to the table with a solution in &lt;a href=&quot;https://aws.amazon.com/fsx/lustre/&quot;&gt;Amazon FSx&lt;/a&gt;, which is a managed Lustre-as-a-service that makes it much easier to orchestrate an HPC workflow that relies on a high-performance, high-capacity, shared file system.  This has prompted the other two cloud vendors to come up with competing solutions:  Microsoft Azure’s partnership with Cray is resulting in a &lt;a href=&quot;https://www.cray.com/solutions/supercomputing-as-a-service/cray-clusterstor-in-azure&quot;&gt;ClusterStor Lustre appliance in the cloud&lt;/a&gt;, and Google Cloud will be offering &lt;a href=&quot;https://cloud.google.com/blog/products/storage-data-transfer/competing-with-supercomputers-hpc-in-the-cloud-becomes-reality&quot;&gt;DDN’s EXAScaler Lustre appliances as a service&lt;/a&gt;.  And Whamcloud, the company behind Lustre, offers its own &lt;a href=&quot;https://wiki.whamcloud.com/display/PUB/Cloud+Edition+for+Lustre+Software&quot;&gt;Lustre Cloud Edition&lt;/a&gt; on all three major cloud platforms.&lt;br /&gt;&lt;br /&gt;In addition to the big three finally closing this gap, a startup called &lt;a href=&quot;https://kmesh.io/&quot;&gt;Kmesh&lt;/a&gt; burst on to the I/O scene at ISC this year and is offering a cloud-agnostic solution to providing higher-touch parallel file system integration and management in the cloud for HPC.  Vinay Gaonkar, VP of Products at Kmesh, gave insightful presentations at several big I/O events during the week that spoke to the unique challenges of designing Lustre file systems in a cloud ecosystem.  While architects of on-prem storage for HPC are used to optimizing for price-performance on the basis of purchasing assets, optimizing price-performance from ephemeral instance types often defies conventional wisdom; he showed that instance types that may be considered slow on a computational basis may deliver peak I/O performance at a lower cost than the beefiest instance available:&lt;br /&gt;&lt;br /&gt;&amp;lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&amp;gt;&lt;a href=&quot;https://1.bp.blogspot.com/-b12cCvM1gvo/XRQEGgHTWaI/AAAAAAABFdQ/1jrm3GjLeTkHsd-zJywV4KN1QAnVGkdSwCLcBGAs/s1600/IMG_6395.jpg&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;239&quot; src=&quot;https://1.bp.blogspot.com/-b12cCvM1gvo/XRQEGgHTWaI/AAAAAAABFdQ/1jrm3GjLeTkHsd-zJywV4KN1QAnVGkdSwCLcBGAs/s320/IMG_6395.jpg&quot; width=&quot;320&quot; /&gt;&lt;/a&gt;&amp;lt;/div&amp;gt;
&lt;br /&gt;Vinay’s slides are available online and &lt;a href=&quot;https://hps.vi4io.org/_media/events/2019/hpciodc-hpc_on_public_clouds_vinay_gaonkar.pdf&quot;&gt;offer a great set of performance data for high-performance storage in the public clouds&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;The fact that there is now sufficient market opportunity to drive these issues to the forefront of I/O discussion at ISC is an indicator that the cloud is becoming increasingly attractive to users who need more than simple high-throughput computing resources.&lt;br /&gt;&lt;br /&gt;Even with these sorts of parallel file systems-as-a-service offerings though, there are still non-trivial data management challenges when moving on-premise HPC workloads into the cloud that result from the impedance mismatch between scientific workflows and the ephemeral workloads for which cloud infrastructure is generally designed.  At present, the cost of keeping active datasets on a persistent parallel file system in the cloud is prohibitive, so data must continually be staged between an ephemeral file-based working space and long-term object storage.  This is approximately analogous to moving datasets to tape after each step of a workflow, which is unduly burdensome to the majority of mid-scale HPC users.&lt;br /&gt;&lt;br /&gt;However, such staging and data management issues are no longer unique to the cloud; as I will discuss in the next section, executing workflows across multiple storage tiers is no longer a problem unique to the biggest HPC centers.  The solutions that address the burdens of data orchestration for on-premise HPC are likely to also ease the burden of moving modest-scale HPC workflows entirely into the cloud.&lt;br /&gt;&lt;br /&gt;&amp;lt;h2&amp;gt;Tiering is no longer only a problem of the rich and famous&amp;lt;/h2&amp;gt;Intel started shipping Optane persistent memory DIMMs earlier this year, and the rubber is now hitting the road as far as figuring out what I/O problems it can solve at the extreme cutting edge of HPC.  At the other end of the spectrum, flash prices have now reached a point where meat-and-potatoes HPC can afford to buy it in quantities that can be aggregated into a useful tier.  These two factors resulted in a number of practical discussions about how tiering can be delivered to the masses in a way that balances performance with practicality.&lt;br /&gt;&lt;br /&gt;The &lt;a href=&quot;http://www.sagestorage.eu/&quot;&gt;SAGE2&lt;/a&gt; project featured prominently at the high-end of this discussion.  Sai Narasimhamurthy from Seagate presented the &lt;a href=&quot;https://dl.acm.org/citation.cfm?doid=3203217.3205341&quot;&gt;Mero software stack&lt;/a&gt;, which is the Seagate object store that is being developed to leverage persistent memory along with other storage media.  At a distance, its goals are similar to those of the original DAOS in that it provides an integrated system that manages data down to a disk tier.  Unlike the DAOS of today though, it takes on the much more ambitious goal of providing a &lt;a href=&quot;https://dl.acm.org/citation.cfm?id=3127024.3127034&quot;&gt;PGAS-style memory access model into persistent storage&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;On the other end of the spectrum, a number of new Lustre features are rapidly coalescing into the foundation for a capable, tiered storage system.  At the Lustre/EOFS BOF, &lt;a href=&quot;http://wiki.lustre.org/File_Level_Redundancy_Solution_Architecture#Phase_4:_Erasure_Coded_Striped_Files&quot;&gt;erasure coded files&lt;/a&gt; were shown on the roadmap for the Lustre 2.14 release in 2Q2020.  While the performance of erasure coding probably makes it prohibitive as the default option for new files on a Lustre file system, erasure coding in conjunction with Lustre’s file-level replication will allow a Lustre file system to store, for example, hot data in an all-flash pool that uses striped mirrors to enable high IOPS and then tier down cooler data to a more cost-effective disk-based pool of erasure-coded files.&lt;br /&gt;&lt;br /&gt;In a similar vein, Andreas Dilger also discussed future prospects for Lustre at the &lt;a href=&quot;https://hps.vi4io.org/events/2019/iodc&quot;&gt;HPC I/O in the Data Center workshop&lt;/a&gt; and showed &lt;a href=&quot;https://hps.vi4io.org/_media/events/2019/hpc-iodc-lustre_next_20_years-dilger.pdf&quot;&gt;a long-term vision for Lustre&lt;/a&gt; that is able to interact with both tiers within a data center and tiers across data centers:&lt;br /&gt;&lt;br /&gt;&amp;lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&amp;gt;&lt;a href=&quot;https://1.bp.blogspot.com/-f8WoFmxIh7Y/XRQItNt8k_I/AAAAAAABFdc/LjfajPmyIkEBnfiZAUHTaHAiYBOcLlInQCLcBGAs/s1600/IMG_6400.jpg&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;239&quot; src=&quot;https://1.bp.blogspot.com/-f8WoFmxIh7Y/XRQItNt8k_I/AAAAAAABFdc/LjfajPmyIkEBnfiZAUHTaHAiYBOcLlInQCLcBGAs/s320/IMG_6400.jpg&quot; width=&quot;320&quot; /&gt;&lt;/a&gt;&amp;lt;/div&amp;gt;
&lt;br /&gt;&lt;br /&gt;Many of these features already exist and serve as robust building blocks from which a powerful tiering engine could be crafted.&lt;br /&gt;&lt;br /&gt;Finally, tiering took center stage at the Virtual Institute for I/O and IO-500 BOF at ISC with the &lt;a href=&quot;https://www.vi4io.org/io500/list/19-06/start&quot;&gt;Data Accelerator at Cambridge beating out OLCF Summit as the new #1 system&lt;/a&gt;.  A key aspect of Data Accelerator’s top score arose from the fact that it is an &lt;a href=&quot;https://www.eofs.eu/_media/events/lad18/07_alasdair_king_cam-bb_data_accelerator-lad18.pdf&quot;&gt;ephemeral burst buffer system&lt;/a&gt;; like Cray DataWarp, it dynamically provisions parallel file systems for short-term use.  As a result of this ephemeral nature, it could be provisioned with no parity protection and deliver a staggering amount of IOPS.&lt;br /&gt;&lt;br /&gt;&amp;lt;h2&amp;gt;Impressions of the industry&amp;lt;/h2&amp;gt;As I’ve described before, I often learn the most by speaking one-on-one with engineers on the expo floor.  I had a few substantive discussions and caught on to a few interesting trends.&lt;br /&gt;&lt;br /&gt;&amp;lt;h3&amp;gt;No winners in EDSFF vs. NF.1&amp;lt;/h3&amp;gt;It’s been over a year since Samsung’s NF.1 (formerly M.3 and NGSFF) and Intel’s EDSFF (ruler) SSD form factor SSDs, and most integrators and third-party SSD manufacturers remain completely uncommitted to building hardware around one or the other.  Both form factors have their pros and cons, but the stalemate persists by all accounts so far.  Whatever happens to break this tie, it is unlikely that it will involve the HPC market, and it seems like U.2 and M.2 remain the safest bet for the future.&lt;br /&gt;&lt;br /&gt;&amp;lt;h3&amp;gt;Memory Landscape and Competition&amp;lt;/h3&amp;gt;The HBM standard has put HMC (hybrid memory cube) in the ground, and I learned that Micron is committed to manufacturing HBM starting at the 2e generation.  Given that SK Hynix is also now manufacturing HBM, Samsung may start to face competition in the HBM market as production ramps up.  Ideally this brings down the cost of HBM components in the coming years, but the ramp seems to be slow, and Samsung continues to dominate the market.&lt;br /&gt;&lt;br /&gt;Perhaps more interestingly, 3DXPoint may be diversifying soon.  Although the split between Intel and Micron has been well publicized, I failed to realize that Intel will also have to start manufacturing 3DXPoint in its own fabs rather than the shared facility in Utah.  Micron has also announced its commitment to the NVDIMM-P standard which could feasibly blow open the doors on persistent memory and non-Intel processor vendors to support it.  However, Micron has not committed to an explicit combination of 3DXPoint and NVDIMM-P.&lt;br /&gt;&lt;br /&gt;Realistically, the proliferation of persistent memory based on 3DXPoint may be very slow.  I hadn’t realized it, but not all Cascade Lake Xeons can even support Optane DIMMs; there are separate SKUs with the requisite memory controller, suggesting that persistent memory won’t be ubiquitous, even across the Intel portfolio, until the next generation of Xeon at minimum.  Relatedly, none of the other promising persistent memory technology companies (Crossbar, Everspin, Nantero) had a presence at ISC.&lt;br /&gt;&lt;br /&gt;&amp;lt;h3&amp;gt;China&amp;lt;/h3&amp;gt;The US tariffs on Chinese goods are on a lot of manufacturers’ minds.  Multiple vendors remarked that they are either&lt;br /&gt;&lt;br /&gt;&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;thinking about moving more manufacturing from China into Taiwan or North America,&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;already migrating manufacturing out of China into Taiwan or North America,&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;under pressure to make shorter-term changes to their supply chains (such as stockpiling in the US) in anticipation of deteriorating conditions&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;br /&gt;I was not expecting to have this conversation with as many big companies as I did, but it was hard to avoid.&lt;br /&gt;&lt;br /&gt;Beyond worrying about the country of origin for their components, though, none of the vendors with whom I spoke were very concerned about competition from the burgeoning Chinese HPC industry.  Several commented that even though some of the major Chinese integrators have very solid packaging, they are not well positioned as solutions providers.  At the same time, customers are now requiring longer presales engagements due to the wide variety of new technologies on the market.  As a result, North American companies playing in the HPC vertical are finding themselves transitioning into higher-touch sales, complex custom engineering, and long-term customer partnerships.&lt;br /&gt;&lt;br /&gt;&amp;lt;h2&amp;gt;Concluding thoughts&amp;lt;/h2&amp;gt;&amp;lt;div&amp;gt;This year’s ISC was largely one of anticipation of things to come rather than demonstrations that the future has arrived.  Exascale (and the pre-exascale road leading to it) dominated most of the discussion during the week.  Much of the biggest hype surrounding exascale has settled down, and gone are the days of pundits claiming that the sky will fall when exascale arrives due to constant failures, impossible programming models, and impossible technologies.  Instead, exascale is beginning to look very achievable and not unduly burdensome: we know how to program GPUs and manycore CPUs already, and POSIX file-based access will remain available for everyone.  Instead, the challenges are similar to what they’ve always been–continuing to push the limits of scalability in every part of the HPC stack.&amp;lt;/div&amp;gt;&lt;/p&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;I owe my sincerest thanks to the organizers of ISC, its sessions, and the HPC-IODC workshop for putting together the programs that spurred all of the interesting discourse over the week. &amp;nbsp;I also appreciate the technical staff at many of the vendor booths with whom I spoke. &amp;nbsp;I didn't name every person with whom I drew insights on the expo floor, but if you recognize a comment that you made to me in this post and want credit, please do let me know--I'd be more than happy to. &amp;nbsp;I also apologize to all the people with whom I spoke and sessions I attended but did not include here; not everything I learned last week fit here.&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>VAST Data's storage system architecture</title>
   <link href="https://hpc.social/2019/vast-data-s-storage-system-architecture/"/>
   <updated>2019-02-27T05:23:00-07:00</updated>
   <id>https://hpc.social/2019/vast-data-s-storage-system-architecture</id>
   <content type="html">&lt;p&gt;&lt;a href=&quot;https://www.vastdata.com/&quot;&gt;VAST Data, Inc&lt;/a&gt;, an interesting new storage company, unveiled their new all-flash storage system today amidst a good amount of hype and fanfare.  There’s no shortage of marketing material and trade press coverage out there about their company and the juiciest features of their storage architecture, so to catch up on what all the talk has been about, I recommend taking a look at&lt;br /&gt;&amp;lt;div&amp;gt;&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;The &lt;a href=&quot;https://www.vastdata.com/app/pdf/datasheet.pdf&quot;&gt;VAST “Universal Storage” datasheet&lt;/a&gt;&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;The Next Platform’s article, “&lt;a href=&quot;https://www.nextplatform.com/2019/02/26/vast-data-clustered-flash-storage-bans-the-disk-from-the-datacenter/&quot;&gt;VAST Data Clustered Flash Storage Bans The Disk From The Datacenter&lt;/a&gt;”&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Chris Mellor’s piece, “&lt;a href=&quot;https://blocksandfiles.com/2019/02/26/vast-datas-extinction-level-event-for-disk-drives-and-tiering/&quot;&gt;VAST Data: The first thing we do, let’s kill all the hard drives&lt;/a&gt;”&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;/div&amp;gt;&lt;/p&gt;
&lt;div&gt;The reviews so far are quite sensational in the literal sense since VAST is one of very few storage systems being brought to market that have been designed from top to bottom to use modern storage technologies (containers, NVMe over Fabrics, and byte-addressable non-volatile memory) &lt;i&gt;and&lt;/i&gt; tackle the harder challenge of file-based (not block-based) access.&lt;/div&gt;
&lt;div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;In the interests of grounding the hype in reality, I thought I would share various notes I've jotted down based on my understanding of the VAST architecture. &amp;nbsp;That said, I have to make a few disclaimers up front:&lt;/div&gt;
&lt;div&gt;&lt;ol&gt;&lt;li&gt;I have no financial interests in VAST, I am not a VAST customer, I have never tested VAST, and everything I know about VAST has come from just a few conversations with a limited number of people in the company. &amp;nbsp;This essentially means I have no idea what I'm talking about.&lt;/li&gt;&lt;li&gt;I do not have any NDAs with VAST and none of this material is confidential. &amp;nbsp;Much of it is from public sources now. &amp;nbsp;I am happy to provide references where possible. &amp;nbsp;If you are one of my sources and want to be cited or credited, please let me know.&lt;/li&gt;&lt;li&gt;These views represent my own personal opinions and not those of my employer, sponsors, or anyone else.&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;
&lt;div&gt;With that in mind, what follows is a semi-coherent overview of the VAST storage system as I understand it. &amp;nbsp;If you read anything that is wrong or misguided, rest assured that it is not intentional. &amp;nbsp;Just let me know and I will be more than happy to issue corrections (and provide attribution if you so desire).&lt;/div&gt;
&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;(&lt;b&gt;Update on May 12, 2020&lt;/b&gt;: There is now an &lt;a href=&quot;https://vastdata.com/whitepaper&quot;&gt;authoritative whitepaper on how VAST works under the hood&lt;/a&gt; on the VAST website. &amp;nbsp;Read that, especially &quot;How It Works,&quot; for a better informed description than this post.)&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;&lt;h2&gt;Relevant Technologies&lt;/h2&gt;&lt;div&gt;A VAST storage system is comprised of two flavors of building blocks:&lt;/div&gt;
&lt;div&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;JBOFs&lt;/b&gt; (VAST calls them &quot;d boxes&quot; or &quot;HA enclosures&quot;). &amp;nbsp;These things are what contain the storage media itself.&lt;/li&gt;&lt;li&gt;&lt;b&gt;I/O servers&lt;/b&gt; (VAST calls them &quot;cnodes,&quot; &quot;servers,&quot; &quot;gateways&quot; or, confusingly, &quot;compute nodes&quot;). &amp;nbsp;These things are what HPC cluster compute nodes talk to to perform I/O via NFS or S3.&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;
&lt;div&gt;Tying these two building blocks together is an RDMA fabric of some sort--either InfiniBand or RoCE. &amp;nbsp;Conceptually, it would look something like this:&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://2.bp.blogspot.com/-fwRspws0SAk/XHYc4Oj6tjI/AAAAAAABD98/oxVEAYQMJgAUGgSlX7CfjLokiCHo-Yt0QCLcBGAs/s1600/The%2BVAST%2BData%2BArchitecture.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;338&quot; src=&quot;https://2.bp.blogspot.com/-fwRspws0SAk/XHYc4Oj6tjI/AAAAAAABD98/oxVEAYQMJgAUGgSlX7CfjLokiCHo-Yt0QCLcBGAs/s400/The%2BVAST%2BData%2BArchitecture.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Conceptual diagram of how VAST Data's storage system (IOS, storage fabric, and JBOFs) might fit into a canonical HPC system. &amp;nbsp;Interestingly, strongly resembles old-school block-based SAN architectures.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;For the sake of clarity, we'll refer to the HPC compute nodes that run applications and perform I/O through an NFS client as &quot;clients&quot; hereafter. &amp;nbsp;We'll also assume that all I/O to and from VAST occurs using NFS, but remember that VAST also supports S3.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h3&gt;JBOFs&lt;/h3&gt;&lt;div&gt;JBOFs are dead simple and their only job is to expose each NVMe device attached to them as an NVMe over Fabrics (NVMeoF) target. &amp;nbsp;They are not truly JBOFs because they do have (from &lt;a href=&quot;https://www.vastdata.com/app/pdf/datasheet.pdf&quot;&gt;the VAST spec sheet&lt;/a&gt;):&lt;/div&gt;
&lt;div&gt;&lt;ol&gt;&lt;li&gt;2x embedded active/active servers, each with two Intel CPUs and the necessary hardware to support failover&lt;/li&gt;&lt;li&gt;4x 100 gigabit NICs, either operating using RoCE or InfiniBand&lt;/li&gt;&lt;li&gt;38x 15.36 TB U.2 SSD carriers. &amp;nbsp;These are actually carriers that take multiple M.2 SSDs.&lt;/li&gt;&lt;li&gt;18x 960 GB U.2 Intel Optane SSDs&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;
&lt;div&gt;However they are not intelligent. &amp;nbsp;They are not RAID controllers nor do they do &lt;i&gt;any&lt;/i&gt; data motion between the SSDs they host. &amp;nbsp;They literally serve each device out to the network and that's it.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h3&gt;I/O Servers&lt;/h3&gt;&lt;div&gt;I/O servers are where the magic happens, and they are physically discrete servers that&amp;nbsp;&lt;/div&gt;
&lt;div&gt;&lt;ol&gt;&lt;li&gt;share the same SAN fabric as the JBOFs and speak NVMeoF on one side, and&lt;/li&gt;&lt;li&gt;share a network with client nodes and talk NFS on the other side&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;
&lt;div&gt;These I/O servers are completely stateless; all the data stored by VAST is stored in the JBOFs. &amp;nbsp;The I/O servers have no caches; their job is to turn NFS requests from compute nodes into NVMeoF transfers to JBOFs. &amp;nbsp;Specifically, they perform the following functions:&lt;/div&gt;
&lt;div&gt;&lt;ol&gt;&lt;li&gt;Determine which NVMeoF device(s) to talk to to serve an incoming I/O request from an NFS client. &amp;nbsp;This is done using a hashing function.&lt;/li&gt;&lt;li&gt;Enforce file permissions, ACLs, and everything else that an NFS client would expect.&lt;/li&gt;&lt;li&gt;Transfer data to/from SSDs, and transfer data to/from 3D XPoint drives.&lt;/li&gt;&lt;li&gt;Transfer data between SSDs and 3D XPoint drives. &amp;nbsp;This happens as part of the regular write path, to be discussed later.&lt;/li&gt;&lt;li&gt;Perform &quot;global compression&quot; (discussed later), rebuilds from parity, and other maintenance tasks.&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;
&lt;div&gt;It is also notable that I/O servers do not have an affinity to specific JBOFs as a result of the hash-based placement of data across NVMeoF targets. &amp;nbsp;They are all simply stateless worker bees that process I/O requests from clients and pass them along to the JBOFs. &amp;nbsp;As such, they do not need to communicate with each other or synchronize in any way.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h3&gt;System Composition&lt;/h3&gt;&lt;div&gt;Because I/O servers are stateless and operate independently, they can be dynamically added (and removed) from the system at any time to increase or decrease the I/O processing power available to clients. &amp;nbsp;VAST's position is that the peak I/O performance to the JBOFs is virtually always CPU limited since the data path between CPUs (in the I/O servers) and the storage devices (in JBOFs) uses NVMeoF. &amp;nbsp;This is a reasonable assertion since NVMeoF is extremely efficient at moving data as a result of its use of RDMA and simple block-level access semantics.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;At the same time, this design requires that every I/O server be able to communicate with every SSD in the entire VAST system via NVMeoF. &amp;nbsp;This means that each I/O server mounts every SSD at the same time; in a relatively small two-JBOF system, this results in 112x NVMe targets on every I/O server. &amp;nbsp;This poses two distinct challenges:&lt;/div&gt;
&lt;div&gt;&lt;ol&gt;&lt;li&gt;From an implementation standpoint, this is &lt;b&gt;pushing the limits of how many NVMeoF targets a single Linux host can effectively manage&lt;/b&gt; in practice. &amp;nbsp;For example, a 10 PB VAST system will have over 900 NVMeoF targets mounted on every single I/O server. &amp;nbsp;There is no fundamental limitation here, but this scale will exercise pieces of the Linux kernel in ways it was never designed to be used.&lt;/li&gt;&lt;li&gt;From a fundamental standpoint, this &lt;b&gt;puts tremendous pressure on the storage network&lt;/b&gt;. &amp;nbsp;Every I/O server has to talk to every JBOF as a matter of course, resulting in a network dominated by all-to-all communication patterns. &amp;nbsp;This will make performance extremely sensitive to topology, and while I wouldn't expect any issues at smaller scales, high-diameter fat trees will likely see these sensitivities manifest. &amp;nbsp;The Lustre community turned to fine-grained routing to counter this exact issue on fat trees. &amp;nbsp;Fortunately, InfiniBand now has adaptive routing that I expect will bring much more forgiveness to this design.&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div&gt;This said, VAST has tested their architecture at impressively large scale and has an aggressive scale-out validation strategy.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h3&gt;Shared-everything consistency&lt;/h3&gt;
&lt;div&gt;Mounting every block device on every server may also sound like anathema to anyone familiar with block-based SANs, and generally speaking, it is. &amp;nbsp;NVMeoF (and every other block-level protocol) does not really have locking, so if a single device is mounted by two servers, it is up to those servers to communicate with each other to ensure they aren't attempting to modify the same blocks at the same time. &amp;nbsp;Typical shared-block configurations manage this by simply assigning exclusive ownership of each drive to a single server and relying on heartbeating or quorum (e.g., in HA enclosures or GPFS) to decide when to change a drive's owner. &amp;nbsp;StorNext (formerly CVFS) allows all clients to access all devices, but it uses a central metadata server to manage locks.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;VAST can avoid a lot of these problems by simply not caching any I/Os on the I/O servers and instead passing NFS requests through as NVMeoF requests. &amp;nbsp;This is not unlike how parallel file systems like PVFS (now OrangeFS) avoided the lock contention problem; not using caches dramatically reduces the window of time during which two conflicting I/Os can collide. &amp;nbsp;VAST also claws back some of the latency penalties of doing this sort of direct I/O by issuing all writes to nonvolatile memory instead of flash; this will be discussed later.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;For the rare cases where two I/O servers are asked to change the same piece of data at the same time though, there is a mechanism by which an extent of a file (which is on the order of 4 KiB) can be locked. &amp;nbsp;I/O servers will flip a lock bit for that extent in the JBOF's memory using an atomic RDMA operation before issuing an update to serialize overlapping I/Os to the same byte range. &amp;nbsp;&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;VAST also uses redirect-on-write to ensure that writes are always consistent. &amp;nbsp;If a JBOF fails before an I/O is complete, presumably any outstanding locks evaporate since they are resident only in RAM. &amp;nbsp;Any changes that were in flight simply get lost because the metadata structure that describes the affected file's layout only points to updated extents after they have been successfully written. &amp;nbsp;Again, this redirect-on-complete is achieved using an atomic RDMA operation, so data is always consistent. VAST does not need to maintain a write journal as a result.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;It is not clear to me what happens to locks in the event that an I/O server fails while it has outstanding I/Os. &amp;nbsp;Since I/O servers do not talk to each other, there is no means by which they can revoke locks or probe each other for timeouts. &amp;nbsp;Similarly, JBOFs are dumb, so they cannot expire locks.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h2&gt;The VAST write path&lt;/h2&gt;
&lt;div&gt;I think the most meaningful way to demonstrate how VAST employs parity and compression while maintaining low latency is to walk through each step of the write path and show what happens between the time an application issues a write(2) call and the time that write call returns.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;First, an application on a compute node issues a write(2) call on an open file that happens to reside on an NFS mount that points to a VAST server. &amp;nbsp;That write flows through the standard Linux NFS client stack and eventually results in an NFS RPC being sent over the wire to a VAST server. &amp;nbsp;Because VAST clients use the standard Linux NFS client there are a few standard limitations. &amp;nbsp;For example,&lt;br /&gt;&lt;ol&gt;&lt;li&gt;There is no parallel I/O from the client. &amp;nbsp;A single client cannot explicitly issue writes to multiple I/O servers. &amp;nbsp;Instead, some sort of &lt;a href=&quot;https://www.emc.com/collateral/hardware/white-papers/h8316-wp-smartconnect.pdf&quot;&gt;load balancing technique&lt;/a&gt; must be inserted between the client and servers.&lt;/li&gt;&lt;li&gt;VAST violates POSIX because it only ensures NFS close-to-open consistency. &amp;nbsp;If two compute nodes try to modify the same 4 KiB range of the same file at the same time, the result will be corrupt data. &amp;nbsp;VAST's server-side locking cannot prevent this because it happens at the client side. &amp;nbsp;The best way around this is to force all I/O destined to a VAST file system to use direct I/O (e.g., open with O_DIRECT).&lt;/li&gt;&lt;/ol&gt;&lt;div&gt;Pictorially, it might look something like this:&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://2.bp.blogspot.com/-u0n8JTcmsYk/XHd-DF6RrvI/AAAAAAABD-M/Iy8PRzE0Yootom8IkmLkNoA8-i5YlWpwgCLcBGAs/s1600/vast-write-1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;323&quot; src=&quot;https://2.bp.blogspot.com/-u0n8JTcmsYk/XHd-DF6RrvI/AAAAAAABD-M/Iy8PRzE0Yootom8IkmLkNoA8-i5YlWpwgCLcBGAs/s640/vast-write-1.png&quot; width=&quot;100%&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Step 1 of VAST write path: client issues a standard NFS RPC to a VAST I/O server&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;div&gt;Then the VAST I/O server receives the write RPC and has to figure out to which NVMeoF device(s) the data should be written. &amp;nbsp;This is done by first determining on which NVMe device the appropriate file's metadata is located. &amp;nbsp;This metadata is stored in B-tree like data structures with a very wide fan-out ratio and whose roots are mapped to physical devices algorithmically. &amp;nbsp;Once an I/O server knows which B-tree to begin traversing to find a specific file's metadata algorithmically, it begins traversing that tree to find the file, and then find the location of that file's extents. &amp;nbsp;The majority of these metadata trees live in 3D XPoint, but very large file systems may have their outermost levels stored in NAND.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;A key aspect of VAST's architecture is that writes always land on 3D XPoint first; this narrows down the possible NVMeoF targets to those which are storage-class memory devices.&lt;br /&gt;&lt;br /&gt;Pictorially, this second step may look something like this:&lt;br /&gt;&lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-2abTaRik0t0/XHd_M0XVBlI/AAAAAAABD-U/zwLZSNXiNfEUEVyGzDskuU_bMMjCJcUSQCLcBGAs/s1600/vast-write-2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;323&quot; src=&quot;https://3.bp.blogspot.com/-2abTaRik0t0/XHd_M0XVBlI/AAAAAAABD-U/zwLZSNXiNfEUEVyGzDskuU_bMMjCJcUSQCLcBGAs/s640/vast-write-2.png&quot; width=&quot;100%&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Step 2 of VAST write path: I/O server forwards write to 3D XPoint devices. &amp;nbsp;Data is actually triplicated at this point for reasons that will be explained later.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;VAST uses 3D XPoint for two distinct roles:&lt;/div&gt;
&lt;div&gt;&lt;ol&gt;&lt;li&gt;Temporarily store all incoming writes&lt;/li&gt;&lt;li&gt;Store the metadata structures used to describe files and where the data for files reside across all of the NVMe devices&lt;/li&gt;&lt;/ol&gt;&lt;div&gt;VAST divides 3D XPoint used for case #1 into buckets. &amp;nbsp;Buckets are used to group data based on how long that data is expected to persist before being erased; incoming writes that will be written once and never erased go into one bucket, while incoming writes that may be overwritten (erased) in a very short time will go into another. &amp;nbsp;VAST is able to make educated guesses about this because it knows many user-facing features of the file (its parent directory, extension, owner, group, etc) to which incoming writes are being written, and it tracks file volatility over time.&lt;/div&gt;
&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;Data remains in a 3D XPoint bucket until that bucket is full. &amp;nbsp;The bucket is full when its size can be written to the NAND SSDs such that entire SSD erase blocks (which VAST claims can be on the order of a gigabyte in size) can be written down to NAND at once. &amp;nbsp;Since JBOFs are dumb, this actually results in I/O servers reading back a full bucket out of 3D XPoint:&lt;br /&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-RJGVUcyTBiE/XHeAKVtyEFI/AAAAAAABD-k/PBimD6KGY5YVIBgiaQtp7pLKnr9kTFhUACLcBGAs/s1600/vast-write-3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;323&quot; src=&quot;https://3.bp.blogspot.com/-RJGVUcyTBiE/XHeAKVtyEFI/AAAAAAABD-k/PBimD6KGY5YVIBgiaQtp7pLKnr9kTFhUACLcBGAs/s640/vast-write-3.png&quot; width=&quot;100%&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Step 3 of VAST write path: Once sufficient writes have been received to fill a bucket and create a full stripe, the I/O server must read it from 3D XPoint. &amp;nbsp;Note that this diagram may be misleading; it is unclear if a single bucket resides on a single 3D XPoint device, or if a bucket is somehow sharded. &amp;nbsp;My guess is the former (as shown).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;The I/O server then bounces that bucket back out to NAND devices:&lt;br /&gt;&lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-tNBnyVf_y3Y/XHeBQ2sXi9I/AAAAAAABD-s/0coxApTRWvoeUBhSmplAgzaiufm-gSqBwCLcBGAs/s1600/vast-write-4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;323&quot; src=&quot;https://3.bp.blogspot.com/-tNBnyVf_y3Y/XHeBQ2sXi9I/AAAAAAABD-s/0coxApTRWvoeUBhSmplAgzaiufm-gSqBwCLcBGAs/s640/vast-write-4.png&quot; width=&quot;100%&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Step 4 of VAST write path: Once a full stripe has been formed in 3D XPoint and the I/O node has read it into DRAM, it actually writes that stripe down across many NAND devices. &amp;nbsp;Again, this diagram is probably inaccurate as a result of my own lack of understanding; the relationship between a bucket (which maps to a single SSD's erase block) and a stripe (which must touch N+M SSDs) is unclear to me.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;By writing out an entire erase block at once, VAST avoids the need for the SSD to garbage collect and amplify writes, since erase blocks are never only partially written. &amp;nbsp;Erase blocks are also presumably rarely (or never?) only partially erased either; this is a result of&lt;/div&gt;
&lt;div&gt;&lt;ol&gt;&lt;li&gt;the combined volatility-based bucketing of data (similarly volatile data tends to reside in the same erase block), and&lt;/li&gt;&lt;li&gt;VAST's redirect-on-write nature (data is never overwritten; updated data is simply written elsewhere and the file's metadata is updated to point to the new data).&lt;/li&gt;&lt;/ol&gt;&lt;div&gt;Because VAST relies on cheap consumer NAND SSDs, the data is not safe in the event of a power loss even after the NAND SSD claims the data is persisted. &amp;nbsp;As a result, VAST then forces each NAND SSD to flush its internal caches to physical NAND. &amp;nbsp;Once this flush command returns, the SSDs have guaranteed that the data is power fail-safe. &amp;nbsp;VAST then deletes the bucket contents from 3D XPoint:&lt;/div&gt;
&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-bsKdxL1chJs/XHeDA8F0cJI/AAAAAAABD-4/FK-cCC1S6DYZOOa73C_Iq2G1QDbdrmrDwCLcBGAs/s1600/vast-write-5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;324&quot; src=&quot;https://1.bp.blogspot.com/-bsKdxL1chJs/XHeDA8F0cJI/AAAAAAABD-4/FK-cCC1S6DYZOOa73C_Iq2G1QDbdrmrDwCLcBGAs/s640/vast-write-5.png&quot; width=&quot;100%&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Step 5 of the VAST write path: Once data is truly persisted and safe in the event of power loss, VAST purges the original copy of that bucket that resides on the 3D XPoint.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;div&gt;The metadata structures for all affected files are updated to point at the version of the data that now resides on NAND SSDs, and the bucket is free to be filled by the next generation of incoming writes.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h3&gt;Data Protection&lt;/h3&gt;&lt;div&gt;These large buckets also allow VAST to use extremely wide striping for data protection. &amp;nbsp;As writes come in and fill buckets, large stripes are also being built with a minimum of 40+4 parity protection. &amp;nbsp;Unlike in a traditional RAID system where stripes are built in memory, VAST's use of nonvolatile memory (3D XPoint) to store partially full buckets allow very wide stripes to be built over larger windows of time without exposing the data to loss in the event of a power failure. &amp;nbsp;Partial stripe writes never happen because, by definition, a stripe is only written down to flash once it is full.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;Bucket sizes (and by extension, stripe sizes) are variable and dynamic. &amp;nbsp;VAST will opportunistically write down a stripe as erase blocks become available. &amp;nbsp;As the number of NVMe devices in the VAST system increases (e.g., more JBOFs are installed), stripes can become wider. &amp;nbsp;This is advantageous when one considers the erasure coding scheme that VAST employs; rather than use a Reed-Solomon code, they have developed their own parity algorithm that allows blocks to be rebuilt from only a subset of the stripe. &amp;nbsp;An example stated by VAST is that a 150+4 stripe only requires 25% of the remaining data to be read to rebuild. &amp;nbsp;As pointed out by &lt;a href=&quot;https://glennklockwood.blogspot.com/2019/02/vast-datas-storage-system-architecture.html?showComment=1551464397653#c6795285534302272414&quot;&gt;Shuki Bruck though&lt;/a&gt;, this is likely a derivative of the &lt;a href=&quot;https://doi.org/10.1109/TIT.2012.2227110&quot;&gt;Zigzag coding scheme introduced by Tamo, Wang, and Bruck in 2013&lt;/a&gt;, where a data coded using N+M parity only require (N+M)/M reads to rebuild.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;To summarize, parity-protected stripes are slowly built in storage-class memory over time from bits of data that are expected to be erased at roughly the same time. &amp;nbsp;Once a stripe is fully built in 3D XPoint, it is written down to the NAND devices. &amp;nbsp;As a reminder, I/O servers are responsible for moderating all of this data movement and parity generation; the JBOFs are dumb and simply offer up the 3D XPoint targets.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;To protect data as stripes are being built, the contents of the 3D XPoint layer are simply triplicated. &amp;nbsp;This is to say that every partially built stripe's contents appear on three different 3D XPoint devices.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h3&gt;Performance Expectations&lt;/h3&gt;&lt;div&gt;This likely has a profound effect on the write performance of VAST; if a single 1 MB write is issued by an NFS client, the I/O server must write 3 MB of data to three different 3D XPoint devices. &amp;nbsp;While this would not affect latency by virtue of the fact that the I/O server can issue NVMeoF writes to multiple JBOFs concurrently, this means that the NICs facing the backend InfiniBand fabric must be able to inject data three times as fast as data arriving from the front-end, client-facing network. &amp;nbsp;&lt;b&gt;Alternatively, VAST is likely to carry an intrinsic 3x performance penalty to writes versus reads.&lt;/b&gt;&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;There are several factors that will alter this in practice:&lt;/div&gt;
&lt;div&gt;&lt;ul&gt;&lt;li&gt;Both 3D XPoint SSDs and NAND SSDs have higher read bandwidth than write bandwidth as a result of the power consumption associated with writes. &amp;nbsp;This will further increase the 3:1 read:write performance penalty.&lt;/li&gt;&lt;li&gt;VAST always writes to 3D XPoint but may often read from NAND. &amp;nbsp;This closes the gap in theory, since 3D XPoint is significantly faster at both reads and writes than NAND is at reads in most cases. &amp;nbsp;However the current 3D XPoint products on the market are PCIe-attached and limited to PCIe Gen3 speeds, so there is not a significant bandwidth advantage to 3D XPoint writes vs. NAND reads.&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;It is also important to point out that VAST has yet to publicly disclose any performance numbers. &amp;nbsp;However, using replication to protect writes is perhaps the only viable strategy to deliver extremely high IOPS without sacrificing data protection. &amp;nbsp;WekaIO, which also aims to deliver extremely high IOPS, showed a similar 3:1 read:write performance skew in &lt;a href=&quot;https://www.vi4io.org/io500/list/19-01/10node&quot;&gt;their IO-500 submission in November&lt;/a&gt;. &amp;nbsp;While WekaIO uses a very different approach to achieving low latency at scale, their benchmark numbers indicate that scalable file systems that optimize for IOPS are likely to sacrifice write throughput to achieve this. &amp;nbsp;VAST's architecture and choice to replicate writes is in line with this expectation, but until VAST publishes performance numbers, this is purely speculative. &amp;nbsp;I would like to be proven wrong.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Other Bells and Whistles&lt;/h2&gt;The notes presented above are only a small part of the full VAST architecture, and since I am no expert on VAST, I'm sure there's even more that I don't realize I don't know or fully understand. &amp;nbsp;That said, I'll highlight a few examples of which I am tenuously aware:&lt;br /&gt;&lt;br /&gt;Because every I/O server sees every NVMe device, it can perform &lt;b&gt;global compression&lt;/b&gt;. &amp;nbsp;Typical compression algorithms are designed only to compress adjacent data within a fixed block size, which means similar but physically disparate blocks cannot be reduced. &amp;nbsp;VAST tracks a similarity value for extents in its internal metadata and will group these similar extents before compressing them. &amp;nbsp;I envision this to work something like a Burrows-Wheeler transformation (it is definitely not one though) and conceptually combines the best features of compression and deduplication. &amp;nbsp;I have to assume this compression happens somewhere in the write path (perhaps as stripes are written to NAND), but I don't understand this in any detail.&lt;br /&gt;&lt;br /&gt;The exact compression algorithm is one of VAST's own design, and it is not block-based as a result of VAST not having a fixed block size. &amp;nbsp;This means that decompression is also quite different from block-based compression; according to VAST, their algorithm can decompress only a local subset of data such that reads do not require similar global decompression. &amp;nbsp;The net result is that read performance of compressed data is not significantly compromised. &amp;nbsp;VAST has a very compelling example where they compressed data that was already compressed and saw a significant additional capacity savings as a result of the global nature of their algorithm. &amp;nbsp;While I normally discount claims of high compression ratios since they never hold up for scientific data, the conceptual underpinnings of VAST's approach to compression sounds promising.&lt;br /&gt;&lt;br /&gt;VAST is also very closely tied to byte-addressable nonvolatile storage from top to bottom, and much of this is a result of their &lt;b&gt;B-tree-based file system metadata structure&lt;/b&gt;. &amp;nbsp;They refer to their underlying storage substrate as an &quot;element store&quot; (which I imagine to be similar to a key-value store), and it sounds like it is designed to store a substantial amount of metadata per file. &amp;nbsp;In addition to standard POSIX metadata and the pointers to data extents on various NVMe devices, VAST also stores user metadata (in support of their S3 interface) and internal metadata (such as heuristics about file volatility, versioning for continuous snapshots, etc). &amp;nbsp;This element store API is not exposed to customers, but it sounds like it is sufficiently extensible to support a variety of other access APIs beyond POSIX and S3.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Take-away Messages&lt;/h2&gt;VAST is an interesting new all-flash storage system that resulted from taking a green-field approach to storage architecture. &amp;nbsp;It uses a number of new technologies (storage-class memory/3D XPoint, NAND, NVMe over fabrics) in intellectually satisfying ways, and builds on them using a host of byte-granular algorithms. &amp;nbsp;It looks like it is optimized for both cost (in its intelligent optimization of flash endurance) and latency (landing I/Os on 3D XPoint and using triplication) which have been traditionally difficult to optimize together.&lt;br /&gt;&lt;br /&gt;Its design does rely on an extremely robust backend RDMA fabric, and the way in which every I/O server must mount every storage device sounds like a path to scalability problems--both in terms of software support in the Linux NVMeoF stack and fundamental sensitivities to topology inherent in large, high-diameter RDMA fabrics. &amp;nbsp;The global all-to-all communication patterns and choice to triplicate writes make the back-end network critically important to the overall performance of this architecture.&lt;br /&gt;&lt;br /&gt;That said, the all-to-all (&quot;shared everything&quot;) design of VAST brings a few distinct advantages as well. &amp;nbsp;As the system is scaled to include more JBOFs, the global compression scales as well and can recover an increasing amount of capacity. &amp;nbsp;Similarly, data durability increases as stripes can be made wider and be placed across different failure domains. &amp;nbsp;In this sense, the efficiency of the system increases as it gets larger due to the global awareness of data. &amp;nbsp;VAST's choice to make the I/O servers stateless and independent also adds the benefit of being able to scale the front-end capability of the system independently of the back-end capacity. &amp;nbsp;Provided the practical and performance challenges of scaling out described in the previous paragraph do not manifest in reality, this bigger-is-better design is an interesting contrast to the mass storage systems of today which, at best, do not degrade as they scale out. &amp;nbsp;Unfortunately, VAST has not disclosed any performance or scaling numbers, so the proof will be in the pudding.&lt;br /&gt;&lt;br /&gt;However, VAST has hinted that the costs are &quot;one fifth to one eighth&quot; of enterprise flash today; by their own estimates of today's cost of enterprise flash, this translates to a cost of between $0.075 and $0.12 per gigabyte of flash when deployed in a VAST system. &amp;nbsp;This remains 3x-5x more expensive than spinning disk today, but the cost of flash is dropping far faster than the cost of hard drives, so the near-term future may truly make VAST cost-comparable to disk. &amp;nbsp;As flash prices continue to plummet though, the VAST cost advantage may become less dramatic over datacenter flash, but their performance architecture will remain compelling when compared to a traditional disk-oriented networked file system.&lt;br /&gt;&lt;br /&gt;As alluded above, VAST is not the first company to develop a file-based storage system designed specifically for flash, and they share many similar architectural design patterns with their competition. &amp;nbsp;This is creating gravity around a few key concepts:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;Both flash and RDMA fabrics handle kilobyte-sized transfers with grace, so the days of requiring megabyte-sized I/Os to achieve high bandwidth are nearing an end.&lt;/li&gt;&lt;li&gt;The desire to deliver high IOPS makes replication an essential part of the data path which will skew I/O bandwidth towards reads. &amp;nbsp;This maps well for read-intensive workloads such as those generated by AI, but this does not bode as well for write-intensive workloads of traditional modeling and simulation.&lt;/li&gt;&lt;li&gt;Reserving CPU resources exclusively for driving I/O is emerging as a requirement to get low-latency and predictable I/O performance with kilobyte-sized transfers. &amp;nbsp;Although not discussed above, VAST uses containerized I/O servers to isolate performance-critical logic from other noise on the physical host. &amp;nbsp;This pattern maps well to the notion that in exascale, there will be an abundance of computing power relative to the memory bandwidth required to feed computations.&lt;/li&gt;&lt;li&gt;File-based I/O is not entirely at odds with very low-latency access, but this file-based access is simple one of many interfaces exposed atop a more flexible key-value type of data structure. &amp;nbsp;As such, as new I/O interfaces emerge to serve the needs of extremely latency-sensitive workloads, these flexible new all-flash storage systems can simply expose their underlying performance through other non-POSIX APIs.&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;Finally, if you've gotten this far, it is important to underscore that I am in no way speaking authoritatively about anything above. &amp;nbsp;If you are really interested in VAST or related technologies, don't take it from me; talk to the people and companies developing them directly.&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>What Should a National Research Computing Platform Be?</title>
   <link href="https://hpc.social/2019/what-should-a-national-research-computing-platform-be/"/>
   <updated>2019-01-19T00:00:00-07:00</updated>
   <id>https://hpc.social/2019/what-should-a-national-research-computing-platform-be-</id>
   <content type="html">&lt;h1 id=&quot;what-is-a-national-research-computing-platform-for-in-2019&quot;&gt;What is a National Research Computing Platform &lt;em&gt;For&lt;/em&gt; in 2019?&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;Computers are everywhere now, but computing is still hard. Canada
should build on its competitive advantage by
strengthening existing efforts to provide
expertise, skills and training to researchers and
scholars across the country, and let others provide the increasingly
commodity hardware.  The result will be a generation of trainees
with deep research and cloud experience, and a critical mass of
talent at centres focussed on building enabling technologies.&lt;/em&gt;&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;As R&amp;amp;D becomes increasingly intertwined with computational techniques,
the need for advanced R&amp;amp;D computing support to power research and
scholarship has grown enormously.  What that support looks like,
however, and the kind of services that researchers most need, has
changed radically over the past decades.&lt;/p&gt;

&lt;h2 id=&quot;the-history-of-providing-computers-for-research&quot;&gt;The history of providing computers for research&lt;/h2&gt;

&lt;p&gt;In the 1990s and 2000s, the overwhelming need was simply access to
computers.  With no other providers for computing or storage, it
fell to individual research groups to supply their own.  But a
natural economy of scale starts to play out with computational
resources.  Purchasing and operating hardware becomes more
cost-effective in bulk; and what was even then the most scarce and
valuable resource - the expertise to operate and make effective use
of the hardware - actually &lt;em&gt;grows&lt;/em&gt;, rather than is diminished, by
being involved in different research problems.  So quickly individual
researcher “clusters in a closet” gave way to departmental, then
institutional, and finally regional or national platforms for
computational research and data science support.  In Canada, the
vast majority of such support is offered through &lt;a href=&quot;https://www.computecanada.ca&quot;&gt;Compute
Canada&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As we enter 2019, this landscape looks quite different than it did
in the 90s.  Computing resources adequate for research are thick
on the ground.  Indeed, as the range of problems researchers tackle
with computing and data broaden, many extremely active areas of
compute- and data-powered research require nothing more than a
powerful desktop.&lt;/p&gt;

&lt;p&gt;And for larger needs, the unavoidable logic of economies of scale
for computers and storage has now entered the marketplace.  A
competitive range of commercial vendors provide access to
computing resources that can meet the vast majority of other
researchers needs.  While it’s true that those commercial cloud
providers charge a premium (50%-100%, slowly declining over time)
over what it costs to provide the resources in academic research
environments, that premium pays for enormous benefits in improved
uptime, flexibility, and currency of the hardware, all of which
have real value for researchers.  Increasingly, even niche technologies
like FPGAs, RDMA-enabled networking, and ARM processors are readily
available on commercial cloud providers, leaving fewer and fewer
use cases where in house provision of computer resources remains a
necessity.  Those use cases are important — they include 
multi-rack HPC users, and the stewardship and analysis
of data with the strictest regulatory on-premises requirements —
but they represent a minority of computational science needs.&lt;/p&gt;

&lt;h2 id=&quot;the-need-for-higher-level-support&quot;&gt;The need for higher-level support&lt;/h2&gt;

&lt;p&gt;&lt;img alt=&quot;We advance research more powerfully by providing clarity than clusters&quot; src=&quot;https://www.dursi.ca/assets/what_is_ardc_for/shutterstock_clarity.jpg&quot; style=&quot;float: right; width: 50%;&quot; /&gt;
But even while &lt;em&gt;computers&lt;/em&gt; for research become ever more accessible,
research &lt;em&gt;computing&lt;/em&gt;   for cutting edge research remains a barrier to too many.  Scientists and scholars are trained to be experts in
their field, not necessarily experts in computer science or the
latest computer hardware.  Even keeping track of the latest
computational methods, which frequently come from neighbouring
fields if not different disciplines entirely, can be a challenge.
Researchers greatly need assistance from and collaborations with
experts in research computation itself.  It is the skills, not the
infrastructure, that is scarcest.&lt;/p&gt;

&lt;p&gt;The good news is that the Compute Canada federation has a network
of roughly 200 computational experts, many at the Ph.D. level,
available to directly enable science projects.  The bad news is that
the priorities of the organization, and thus most of its effort and
energies, are focussed on procuring and operating on-premises commodity
computing and storage hardware - to the extent that many of those
experts spend most of their time answering basic help-desk
questions or performing routine operational duties for those systems.&lt;/p&gt;

&lt;h2 id=&quot;what-should-todays-rd-computing-support-focus-on&quot;&gt;What should today’s R&amp;amp;D computing support focus on?&lt;/h2&gt;

&lt;p&gt;With academic institutions now being just one player amongst
many for computing and storage resources, there are a few possible
futures for Canada’s computing centres – centres that have 
grown up primarily
focused on purchasing, operating, and providing access to hardware 
for researchers.  They could downsize, shrinking to focus on those
sorts of hardware not well covered by other providers.  Alternatively,
they could double down on the “discount provider” model,
emphasizing low cost, ‘no frills’ access to compute and
storage, competing on price.&lt;/p&gt;

&lt;p&gt;Either of these approaches represent a scandalous squandering of
opportunity, wasting invaluable and nearly irreplaceable expertise
and experience in applying computational techniques to open research
problems.   Instead, we should do something different.  We should
pursue our competitive advantage by taking the existing network of
computational science advisors that we already have and make those
higher level expert services the primary offering, letting other
providers focus on the lower level procurement and operating of
most computing and storage hardware.&lt;/p&gt;

&lt;h3 id=&quot;skills-beat-hardware&quot;&gt;Skills beat hardware&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dursi.ca/assets/what_is_ardc_for/pixabay_mechanics-3310067.png&quot; style=&quot;float: right; width: 50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The goal of a research computing support platform is to enable
research, and to help develop the next generation of research talent.
Knowledge transfer and skills development are by far the most
valuable work that a computing team can to to meet those goals -
because skills have longest lasting impact, because it addresses 
real needs in Canada’s R&amp;amp;D ecosystem, and simply because no one 
else can do it at scale.&lt;/p&gt;

&lt;p&gt;First, deep training with research methods pay
long-lasting dividends. Even in a rapidly changing fields like data 
and computational science, skills and experience don’t depreciate the
way computing hardware does.  New methods come, but old methods don’t
really go; and fluency in the previous generation of methods makes
learning – or even creating – those newer methods easier.&lt;/p&gt;

&lt;p&gt;And it’s actually even better than that, because not only do the
skills that come from that research experience and training remain
useful in their field for long periods from time, they transfer
to other disiplines extremely well.  Methods for solving equations,
or pulling information out of data, have strong relationships with
each other and can often be applied with modest modifications to 
problems well outside the fields in which they were first developed.
These broad areas of effort - Data Science, Informatics, 
Simulation Science, and the Data Engineering or cloud computing
tools needed for them - are enabling research technologies which 
can empower research in many fields.  And there lies the second
reason for the importance of the skills devevelopment; these
research-enabling technologies are areas in which Canada
currently lags.  A recent report on the 
&lt;a href=&quot;http://new-report.scienceadvice.ca/assets/report/Competing_in_a_Global_Innovation_Economy_FullReport_EN.pdf&quot;&gt;State of Science and Technology and Industrial R&amp;amp;D&lt;/a&gt; specifically calls out “enabling technologies”
as a current area of weakness for Canada which is holding high
impact research in other areas back.  Focussing on such highly
transferrable skills and talent development in our research computing
platform would help build a critical mass of such expertise both
in the research computing centres themselves and in the community as
a whole.&lt;/p&gt;

&lt;p&gt;Finally, there just aren’t other options for providing high-level
data and computational science collaboration and training to Canada’s
scholars and researchers consistently and across disciplines.  We in the
research community know that availability of a collaborator with
complementary interests and skills can make the difference between
a research project happening or not.  Unlike access to commodity
computing hardware, the skills involved in making sure researchers
have access to the best methods for their research, and in training
emerging research talent in the computational side of their discipline,
are very much not commodity skills, and cannot be purchased or rented
from somewhere else.&lt;/p&gt;

&lt;h3 id=&quot;the-cloud-premium-is-a-price-worth-paying&quot;&gt;The cloud premium is a price worth paying&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dursi.ca/assets/what_is_ardc_for/pixabay_cloud-computing-2001090.jpg&quot; style=&quot;float: right; width: 50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The benefits of further efforts in skills development and training
are fairly clear, and this alone would justify redirecting some
effort from hardware to research services, and using comercial 
cloud providers to fill the gap. But having substantial commercial
cloud resources available for researchers is worthwhile on its own merits.&lt;/p&gt;

&lt;p&gt;Firstly, cloud provides more flexibility for rapidly changing research.
The resource mix can be much broader and change much more rapidly
than traditional procurement cycles would allow; what’s more, those
changes can be in response to demonstrated researcher needs, rather
than making predictions and assumptions about the next five years
based on existing research users.  Like owning systems, dynamically
taking advantage of this flexibility requires top operational staff.
And the uptime availability and hardware currency of these resources
will generally be significantly better than what can be provided in
house.&lt;/p&gt;

&lt;p&gt;Secondly, trainees and staff benefit from gaining extremely relevant 
commercial cloud expertise.  This goes back to skills development
a bit, but in this case it’s the system tools – the experience
working with commercial cloud services and building data systems 
solutions using them – that are valuable in and of themselves,
and will be attractive skills to have in whatever career they
move on to.&lt;/p&gt;

&lt;p&gt;Finally, commercial engagement can proceed much more smoothly, and
be more attractive from the point of view of the commercial partner,
when the collaboration happens in the commercial cloud. The success
of efforts like &lt;a href=&quot;https://www.theubercloud.com&quot;&gt;Uber Cloud&lt;/a&gt; provides
some validation of this.  Most companies that would participate in such
engagement either already have or are planning commercial cloud projects,
and are likely more comfortable with such offerings that using 
academic systems.&lt;/p&gt;

&lt;h2 id=&quot;how-to-proceed&quot;&gt;How to proceed&lt;/h2&gt;

&lt;p&gt;Making significant changes to priorities and indeed how we 
provision basic services can seem daunting.  It may not seem clear
how to get there from here, but there are some basic approaches
and guidelines that can help.&lt;/p&gt;

&lt;dl&gt;
  &lt;dt&gt;&lt;strong&gt;No need to do it all at once&lt;/strong&gt;&lt;/dt&gt;
  &lt;dd&gt;This is a change that can and should be made incrementally.  A 
team can be quite straightforwardly trained at a new,
small, “national site” to provide access to a slowly
growing range of cloud resources.  This can start as a modestly
scaled pilot, expanding in response to researcher needs.&lt;/dd&gt;
  &lt;dt&gt;&lt;strong&gt;Make the hardware you own really count by advancing the mission&lt;/strong&gt;&lt;/dt&gt;
  &lt;dd&gt;Many hardware needs are readily outsourceable, whether to commerical
entities or by “buying in” with other academic R&amp;amp;D 
computing partners.  However, some resources will likely stay in-house.
The way to choose is to ensure that every decision to own
rapidly-depreciating, expensive-to-operate equipment directly
supports the mission of excellent research support and research skills
development.  In-house equipment should be significantly better
at that mission than what can be procured from elsewhere.  That
may mean making cutting-edge infrastructure that is
in itself publication worthy, or buying still-prototype 
experimental systems to evaluate, and to build and share expertise
on.&lt;/dd&gt;
  &lt;dt&gt;&lt;strong&gt;Use the right tools for the job&lt;/strong&gt;&lt;/dt&gt;
  &lt;dd&gt;Helpdesk requests and fixing software bugs both
are short-term tasks that benefit from a “ticket tracking”
approach; an issue is identified, someone fixes it and 
“closes” the ticket, and the faster the ticket is closed,
the better the service was.  That isn’t the right way to think about
higher-level services like collaborations and knowledge transfer,
and using tools for one to manage interactions like the other distorts
both the tool and the interactions.  Consulting firms use case
managment software, not ticket trackers, to manage engagements,
and use the effectiveness of the collaboration rather than the duration
of the engagement to judge success.  Since interactions with the
researchers are vitally important to the success of the mission,
the best available case management software (and helpdesk software where
appropriate) should be used.&lt;/dd&gt;
  &lt;dt&gt;&lt;strong&gt;Make the expertise really count by building a unified national team&lt;/strong&gt;&lt;/dt&gt;
  &lt;dd&gt;Once the right tools are in place, other lessons can be learned
from successful consultancies.  The most successful collaborations
will combine staff from across the country with the appropriate 
expertise, and staff that are local to the researcher.  To achieve
that, the computational experts across the country must be able
to find each other, self-assemble into teams as needed, and collaborate
seamlessly.  While the technical infratructure for this exists,
the organizational incentives are still for staff at a site to support 
primarily “their” researchers.  Such siloing is completely
counter to supporting national research.&lt;/dd&gt;
&lt;/dl&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dursi.ca/assets/what_is_ardc_for/shutterstock_collaboration.jpg&quot; style=&quot;float: right; width: 50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The goal of a research computing support platform - any research
support resource, really - is to enable research, and to help develop
the next generation of research talent.  With that primary mission
in mind, the reasons for focussing the time and effort of computational
science experts on collaboration and skills development rather than
operating commodity hardware could not be clearer:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Collaboration across disciplines - domain science and computational/data expertise - enables better Canadian research;&lt;/li&gt;
  &lt;li&gt;Computational and data skills maintain their value, while hardware rapidly depreciates; and&lt;/li&gt;
  &lt;li&gt;Building a critical mass of expertise and talent focussed on emerging data science and computational methods will strengthen Canadian competitiveness not just in research but in innovation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are costs to this approach; it will cost somewhat more to
have someone else run much of that hardware.  But even those costs
have upsides:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Cloud provides more flexibility for rapidly changing research; capability mixes and system configurations can be changed much faster than hardware procurement cycles;&lt;/li&gt;
  &lt;li&gt;Commercial cloud infrastructure provides much better uptime and currency for researchers;&lt;/li&gt;
  &lt;li&gt;Both the computational experts and the research trainees benefit from gaining extremely relevant cloud expertise that will benefit them in any future career; and&lt;/li&gt;
  &lt;li&gt;Industrial engagement will be much more straightforward around 
commercial cloud providers than academic infrastructure.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The prospect of moving to such a different service model may seem
daunting, but it needn’t be:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Move one step at a time, with a new, small, “national site” being a collection of cloud resources;&lt;/li&gt;
  &lt;li&gt;Not all hardware can be outsourced; make what you do retain an ownership stake in count by having it be best-in-class, enable experimentation and development of new approaches, or otherwise having owning it rather than renting it &lt;em&gt;directly&lt;/em&gt; advance the mission;&lt;/li&gt;
  &lt;li&gt;Choose the best possible tools for staff/researcher interactions; and&lt;/li&gt;
  &lt;li&gt;Build the best possible computational science team by having them collaborate internally, as well, and ensuring researchers and trainees get the most relevant help and collaboration possible.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These changes will not be easy; they will require participation from
funders, staff, researchers, and all stakeholders.  But the research
computing world of today is not that of the 1990s, and how we support
computational research should take advantage of that.&lt;/p&gt;

&lt;p&gt;Images courtesy of &lt;a href=&quot;https://www.shutterstock.com/home&quot;&gt;shutterstock&lt;/a&gt; and &lt;a href=&quot;https://pixabay.com&quot;&gt;pixabay&lt;/a&gt;, used under license&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>A week in the life of an SC attendee</title>
   <link href="https://hpc.social/2018/a-week-in-the-life-of-an-sc-attendee/"/>
   <updated>2018-11-24T01:44:00-07:00</updated>
   <id>https://hpc.social/2018/a-week-in-the-life-of-an-sc-attendee</id>
   <content type="html">&lt;p&gt;Last week was the annual &lt;a href=&quot;https://sc18.supercomputing.org/&quot;&gt;Supercomputing conference, held this year in Dallas&lt;/a&gt;, and it was as busy as they always are.  Every year I take plenty of photos and post plenty of tweets throughout the week, but this year I thought it might be fun to share some of those photos (and the related things I learned) now that the dust has settled.  Since some people might also be interested in how someone might approach the conference from a technical and philosophical perspective, I figured I’d write a more general piece documenting my entire SC experience this year.&lt;br /&gt;&lt;br /&gt;This post wound up being a massive, meandering, chronological documentary of a week in my life that includes both technical and non-technical commentary.  For anyone who is only interested in the technical insights I gained during SC, check out the items prefixed with (tech) in this table of contents:&lt;br /&gt;&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/2018/11/a-week-in-life-of-sc-attendee.html#before-conf&quot;&gt;Before the Conference&lt;/a&gt;&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/2018/11/a-week-in-life-of-sc-attendee.html#saturday&quot;&gt;Saturday&lt;/a&gt;&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/2018/11/a-week-in-life-of-sc-attendee.html#sunday&quot;&gt;Sunday&lt;/a&gt;&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/2018/11/a-week-in-life-of-sc-attendee.html#monday&quot;&gt;Monday&lt;/a&gt;&amp;lt;/li&amp;gt;&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;(tech) &lt;a href=&quot;https://glennklockwood.blogspot.com/2018/11/a-week-in-life-of-sc-attendee.html#pdsw&quot;&gt;PDSW-DISCS 2018 Highlights&lt;/a&gt;&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/2018/11/a-week-in-life-of-sc-attendee.html#gala&quot;&gt;SC Exhibition Gala&lt;/a&gt;&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/2018/11/a-week-in-life-of-sc-attendee.html#bash&quot;&gt;The Beowulf Bash&lt;/a&gt;&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;li&amp;gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/2018/11/a-week-in-life-of-sc-attendee.html#tuesday&quot;&gt;Tuesday&lt;/a&gt;&amp;lt;/li&amp;gt;&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;(tech) &lt;a href=&quot;https://glennklockwood.blogspot.com/2018/11/a-week-in-life-of-sc-attendee.html#tuesdaytechprog&quot;&gt;Technical Program, Data and Storage Paper Track Highlights&lt;/a&gt;&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/2018/11/a-week-in-life-of-sc-attendee.html#tuesdayinterlude&quot;&gt;Interlude of Meetings&lt;/a&gt;&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;(tech) &lt;a href=&quot;https://glennklockwood.blogspot.com/2018/11/a-week-in-life-of-sc-attendee.html#tuesdayexpo&quot;&gt;Cray and Fujitsu’s Exascale System Hardware on the Expo Floor&lt;/a&gt;&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;(tech) &lt;a href=&quot;https://glennklockwood.blogspot.com/2018/11/a-week-in-life-of-sc-attendee.html#paralleliobof&quot;&gt;Analyzing Parallel I/O BOF Highlights&lt;/a&gt;&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/2018/11/a-week-in-life-of-sc-attendee.html#crayparty&quot;&gt;The Cray Celebration&lt;/a&gt;&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;li&amp;gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/2018/11/a-week-in-life-of-sc-attendee.html#wednesday&quot;&gt;Wednesday&lt;/a&gt;&amp;lt;/li&amp;gt;&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/2018/11/a-week-in-life-of-sc-attendee.html#wednesdaymorning&quot;&gt;SC Student Career Fair and a Booth Talk&lt;/a&gt;&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;(tech) &lt;a href=&quot;https://glennklockwood.blogspot.com/2018/11/a-week-in-life-of-sc-attendee.html#wednesdayexpo&quot;&gt;Flash, Disk, and Tape Technologies on the Expo Floor&lt;/a&gt;&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;(tech) &lt;a href=&quot;https://glennklockwood.blogspot.com/2018/11/a-week-in-life-of-sc-attendee.html#io500bof&quot;&gt;Recap of the IO-500/VI4IO BOF&lt;/a&gt;&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;li&amp;gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/2018/11/a-week-in-life-of-sc-attendee.html#thursday&quot;&gt;Thursday&lt;/a&gt;&amp;lt;/li&amp;gt;&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;(tech) &lt;a href=&quot;https://glennklockwood.blogspot.com/2018/11/a-week-in-life-of-sc-attendee.html#thursdaytechprog&quot;&gt;WekaIO and Micron at the Exhibitor Forum&lt;/a&gt;&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/2018/11/a-week-in-life-of-sc-attendee.html#nsfbof&quot;&gt;NSF Future Directions BOF&lt;/a&gt;&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/2018/11/a-week-in-life-of-sc-attendee.html#mypaper&quot;&gt;My SC Paper&lt;/a&gt;&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/2018/11/a-week-in-life-of-sc-attendee.html#perot&quot;&gt;SC Technical Program Reception at the Perot Museum&lt;/a&gt;&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;li&amp;gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/2018/11/a-week-in-life-of-sc-attendee.html#friday&quot;&gt;Friday&lt;/a&gt;&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/2018/11/a-week-in-life-of-sc-attendee.html#after-conf&quot;&gt;After the Conference&lt;/a&gt;&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;br /&gt;Everything that’s not labeled (tech) is part diary and part career development perspective.  Hopefully someone will find something in here that’s of some value.&lt;br /&gt;&lt;br /&gt;Finally, disclosures:&lt;br /&gt;&amp;lt;ul style=&quot;font-size: xx-small;&quot;&amp;gt;&amp;lt;li&amp;gt;I omitted some names in the interests of respecting the privacy of the folks who took the time to talk to me one-on-one.  If you’re part of this story and don’t mind having your name out there, I’d be happy to include it.&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Everything I paraphrase here is public information or conjecture on my part.  Nothing in this post is either confidential or sensitive.  That said, check your references before citing anything here.  I don’t know what I’m talking about.&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Everything here is my personal opinion and does not necessarily reflect the viewpoint of my employer or its funding agency.  I attended the conference as a part the regular course of business in which I am employed.  However I took all photos for personal purposes, and the entirety of this post was written on my own personal time.&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;br /&gt;&amp;lt;h2 id=&quot;before-conf&quot;&amp;gt;&lt;span&gt;&lt;/span&gt;Before the conference&amp;lt;/h2&amp;gt;Everyone’s SC experience is different because it draws such a diverse range of professionals.  There are plenty of activities for everyone ranging from students and early-career staff to senior management and leadership, and people on different career tracks (e.g., facilities staff, computer science researchers, program managers, product sales) are likely to be drawn to very different parts of the conference agenda.  My priorities during the week of SC are definitely shaped by where I am in my career, so when filling out my calendar a few weeks ahead of the conference, I considered the following:&lt;br /&gt;&lt;br /&gt;&lt;b&gt;My job is half research and half facilities staff.&lt;/b&gt;  50% of my time is funded by grant money to do applied research in characterizing parallel I/O systems.  The other half of my time is spent staying current on emerging technologies in computing and storage.  These two responsibilities mean that my SC is usually a mix of attending technical program sessions (to see what my peers in research are doing and see what research ideas might turn up in future technologies) and engaging with vendors.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;I work in advanced technologies.&lt;/b&gt;  This means I am generally not in the trenches directly feeling the pains of operating HPCs today; instead, my job is to identify technologies that will cause less problems tomorrow.  This also means that I don’t have purchasing authority, and I am less likely to be involved with anything that’s going to hit the floor in the next year.  As such, I generally don’t do vendor sales meetings or briefings at SC because they are generally focused on nearer-term products and sales.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;I did not get to where I am by myself.&lt;/b&gt;  I first heard about SC in 2010 when I was a graduate student, and it sounded almost infinitely more exciting than the materials science conferences I was attending.  I had no experience in HPC at the time, but it made me realize what I really wanted to pursue as a career.  I relied heavily on the good will of the online HPC community to learn enough to get my first HPC job at SDSC, and after that, the faith of a great many more to get me to where I am now.  SC is often the only time I get to see people who have helped me out in my early career, and I always make time connect with them.&lt;br /&gt;&lt;br /&gt;The net result of these goals was a pretty full schedule this year:&lt;br /&gt;&lt;br /&gt;&amp;lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&amp;gt;&amp;lt;/div&amp;gt;&lt;/p&gt;
&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;http://2.bp.blogspot.com/-TzDXu652VHs/W_c4dxJBnXI/AAAAAAABCn8/-deME8LsYgwntvDZo4V_p12MG1ji4L_hwCK4BGAYYCw/s1600/Screen%2BShot%2B2018-11-22%2Bat%2B10.12.27.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;400&quot; src=&quot;https://2.bp.blogspot.com/-TzDXu652VHs/W_c4dxJBnXI/AAAAAAABCn8/-deME8LsYgwntvDZo4V_p12MG1ji4L_hwCK4BGAYYCw/s400/Screen%2BShot%2B2018-11-22%2Bat%2B10.12.27.png&quot; width=&quot;292&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;My SC'18 schedule. &amp;nbsp;Note that the time zone is PST, or two hours behind Dallas time.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;I mark everything that I &lt;i&gt;must&lt;/i&gt; attend (usually because I’m a speaker) in red to know the immovable obligations. Blue items are things I &lt;i&gt;will&lt;/i&gt; attend unless an emergency comes up, and grey things are events I &lt;i&gt;should&lt;/i&gt; attend because they sound interesting.&lt;br /&gt;&lt;br /&gt;White space is very important to me too; between 10am and 6pm, white spaces are when I can walk the expo floor.  A lot of people write off the expo as a waste of time, but I actually feel that it’s one of the most valuable parts of SC.  Since my job is to understand emerging technology (and the market trends that drive them), accosting a pre-sales engineer or product manager in a strategically important technology provider can yield an invaluable peek into the markets they’re serving.  White space in the evenings are equally important for engagements of opportunity or working on slides that have to be presented the next day.&lt;br /&gt;&amp;lt;div&amp;gt;&lt;br /&gt;&amp;lt;/div&amp;gt;&lt;/p&gt;
&lt;h2 id=&quot;saturday&quot;&gt;Saturday, November 10&lt;/h2&gt;
&lt;p&gt;I always fly to SC on the Saturday before the conference starts.  I have historically opted to do workshops on both Sunday and Monday, as I really enjoy attending both &lt;a href=&quot;http://www.pmbsworkshop.org/&quot;&gt;PMBS&lt;/a&gt; and &lt;a href=&quot;http://www.pdsw.org/&quot;&gt;PDSW-DISCS&lt;/a&gt;.  I bring a suitcase with has extra room for conference swag, and doing so this year was critically important because I opted to &lt;a href=&quot;https://twitter.com/glennklockwood/status/1061337582858956800&quot;&gt;bring along a pair of cowboy boots&lt;/a&gt; that I knew I would not want to wear on the flight home.&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://4.bp.blogspot.com/-ZUaEGhTWOJA/W_b_laf-LnI/AAAAAAABClE/BH6sxJc8GLI1YhAxfdw1WgfctK1mLbFiACK4BGAYYCw/s1600/IMG_4844.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;320&quot; src=&quot;https://4.bp.blogspot.com/-ZUaEGhTWOJA/W_b_laf-LnI/AAAAAAABClE/BH6sxJc8GLI1YhAxfdw1WgfctK1mLbFiACK4BGAYYCw/s320/IMG_4844.jpeg&quot; width=&quot;320&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;My brown kicks.  Also Harriet the cat.&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&lt;br /&gt;On just about every work flight I’m on, I’ve got PowerPoint slides to review; this trip was no different, and I spent the 3.5-hour flight time reviewing the slides I had to present the next day. Once in Dallas and at my hotel, I carried out my usual work-travel night-of-arrival ritual: order the specialty pizza from a local pizza joint, text home saying I arrived safely, and iron my clothes while watching Forensic Files.&lt;br /&gt;&lt;br /&gt;&amp;lt;h2 id=&quot;sunday&quot;&amp;gt;Sunday, November 11&amp;lt;/h2&amp;gt;This year I had the honor of presenting one part of &lt;a href=&quot;https://sc18.supercomputing.org/presentation/?id=tut121&amp;amp;sess=sess238&quot;&gt;the famed Parallel I/O in Practice tutorial at SC&lt;/a&gt; along with Rob Ross, Brent Welch, and Rob Latham.  This tutorial has been running for over fifteen years now, and at some point over those years, it picked up the curious ritual of being kicked off with some juggling:&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://4.bp.blogspot.com/-aeDAd6dJWf8/W_cFOyOjQAI/AAAAAAABClc/1e6hwAvpkscqRo7E6SJY18Uremnqb4-pwCK4BGAYYCw/s1600/IMG_4857.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;320&quot; src=&quot;https://4.bp.blogspot.com/-aeDAd6dJWf8/W_cFOyOjQAI/AAAAAAABClc/1e6hwAvpkscqRo7E6SJY18Uremnqb4-pwCK4BGAYYCw/s320/IMG_4857.jpeg&quot; width=&quot;240&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;Brent leading up to the tutorial start time with some juggling.  He brought the pins with him.&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&lt;br /&gt;The tutorial itself is really comprehensive and includes everything from device-level performance behavior to parallel file systems architecture and I/O middleware.  Even though I can proudly say that I knew 95% of the material being presented throughout the day (as I probably should since I was a presenter!), I found &lt;a href=&quot;https://twitter.com/glennklockwood/status/1061751272339070976&quot;&gt;this particular slide that Rob Latham presented&lt;/a&gt; particularly insightful:&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;https://2.bp.blogspot.com/-cmNtrmcyeNE/W_cIF0rHUwI/AAAAAAABCl0/1NyV2lPD3FoagXus54zTCwbbPy4ckfgywCLcBGAs/s1600/IMG_4860.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;300&quot; src=&quot;https://2.bp.blogspot.com/-cmNtrmcyeNE/W_cIF0rHUwI/AAAAAAABCl0/1NyV2lPD3FoagXus54zTCwbbPy4ckfgywCLcBGAs/s400/IMG_4860.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;The ease and portability of using I/O middleware comes without sacrificing performance!  Sorry for the odd angle; this is the screen as us presenters were able to view it.&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&lt;br /&gt;It makes the case that there is no significant performance penalty for using higher-level I/O libraries (like PnetCDF or parallel HDF5) despite how much easier they are to use than raw MPI-IO.  One of the biggest take-home messages of the entire tutorial is to use I/O middleware wherever possible; doing so means that understanding parallel file system architecture isn’t prerequisite to getting good I/O performance.&lt;br /&gt;&lt;br /&gt;&amp;lt;h2 id=&quot;monday&quot;&amp;gt;Monday, November 12&amp;lt;/h2&amp;gt;&amp;lt;div&amp;gt;Monday was the official first day of SC.  Workshops and tutorials went on throughout the day, and the opening keynote and exhibition hall opening gala started in the evening.&amp;lt;/div&amp;gt;&lt;/p&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h3 id=&quot;pdsw&quot;&gt;PDSW-DISCS 2018&lt;/h3&gt;
&lt;p&gt;The &lt;a href=&quot;http://www.pdsw.org/&quot;&gt;3rd Joint International Workshop on Parallel Data Storage &amp;amp; Data Intensive Scalable Computing Systems (PDSW-DISCS)&lt;/a&gt; was on Monday, and I had the honor of being asked to serve as its Publicity Chair this year.&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://1.bp.blogspot.com/-DujP5Fbmxeg/W_cDr2Kw8TI/AAAAAAABClQ/QVtCDkG_JPQUlSxwJGtalamSkn9k7dacQCK4BGAYYCw/s1600/IMG_4863.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;320&quot; src=&quot;https://1.bp.blogspot.com/-DujP5Fbmxeg/W_cDr2Kw8TI/AAAAAAABClQ/QVtCDkG_JPQUlSxwJGtalamSkn9k7dacQCK4BGAYYCw/s320/IMG_4863.jpeg&quot; width=&quot;240&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;The PDSW-DISCS full-day workshop agenda&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&lt;br /&gt;It’s a really great workshop for people working in I/O, storage, and data and always draws a large crowd:&lt;br /&gt;&lt;br /&gt;&amp;lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://2.bp.blogspot.com/-1BGZXlbQsxQ/W_cMDZ1LK_I/AAAAAAABCmQ/CC046rCDsP49GYPyCgpBPbERtiJId4kjgCK4BGAYYCw/s1600/IMG_4872.jpeg&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;225&quot; src=&quot;https://2.bp.blogspot.com/-1BGZXlbQsxQ/W_cMDZ1LK_I/AAAAAAABCmQ/CC046rCDsP49GYPyCgpBPbERtiJId4kjgCK4BGAYYCw/s400/IMG_4872.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/div&amp;gt;
&lt;br /&gt;For researchers, it’s a great venue for short papers that IEEE or ACM publishes, and it also has a really nice Work-in-Progress track where a page-long abstract gives you a seven minute spot to pitch your work.  For attendees, it’s always chock full of good talks that range from pure research to applied development.&lt;br /&gt;&lt;br /&gt;This year’s keynote speaker was &lt;a href=&quot;https://www.linkedin.com/in/rangan/&quot;&gt;Rangan Sukumar&lt;/a&gt;, Cray’s analytics guru.  His talk was interesting in that it approached the oft-mentioned convergence between HPC and AI (which has become an over-used trope by itself) from the perspective of a system architect (which is where the rubber meets the road):&lt;br /&gt;&lt;br /&gt;&amp;lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://4.bp.blogspot.com/-FUxOI01WZAQ/W_cNqcINlnI/AAAAAAABCmc/TxatMQ-ANK0yHmGv5RMzrbvBz3MBz_vCgCK4BGAYYCw/s1600/IMG_4866.jpeg&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;225&quot; src=&quot;https://4.bp.blogspot.com/-FUxOI01WZAQ/W_cNqcINlnI/AAAAAAABCmc/TxatMQ-ANK0yHmGv5RMzrbvBz3MBz_vCgCK4BGAYYCw/s400/IMG_4866.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/div&amp;gt;
&lt;br /&gt;As many great keynote speakers are, Rangan used hyperbole at times to contrast HPC and “Big Data” workloads, and this &lt;a href=&quot;https://twitter.com/glennklockwood/status/1062002965630910470&quot;&gt;stimulated some discussion online&lt;/a&gt;.  Although the slides alone tell only part of the story, you can download them from the &lt;a href=&quot;http://www.pdsw.org/&quot;&gt;PDSW-DISCS’18 website&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;Later in the morning, Margaret Lawson (University of Illinois, Sandia Labs) presented a follow-on to the &lt;a href=&quot;https://dx.doi.org/10.1145/3149393.3149403&quot;&gt;EMPRESS metadata system she presented last year&lt;/a&gt;:&lt;br /&gt;&lt;br /&gt;&amp;lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://4.bp.blogspot.com/-CRKTR3ccrGU/W_cRcIo-pRI/AAAAAAABCmo/yNwqAJnhiDoKKNKyjdrkSQR8CK1XPcXjACK4BGAYYCw/s1600/IMG_4874.jpeg&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;225&quot; src=&quot;https://4.bp.blogspot.com/-CRKTR3ccrGU/W_cRcIo-pRI/AAAAAAABCmo/yNwqAJnhiDoKKNKyjdrkSQR8CK1XPcXjACK4BGAYYCw/s400/IMG_4874.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/div&amp;gt;
&lt;br /&gt;Last year, EMPRESS seemed a little too researchy for me (as a facilities person) to sink my teeth into.  This year though, the picture seems a lot more complete and I quite like the architectural framework.  Although EMPRESS may not ever be a household name, the concept of separating data streams and metadata streams underneath some sort of I/O middleware is really solid.  I think that storing data and metadata in different, architecturally distinct storage systems that map to the unique access patterns of data and metadata is ultimately the right way to approach large-scale data and metadata management in HPC, and I expect to see this design pattern proliferate as scientific data analysis becomes a bigger part of large-scale HPC workloads.&lt;br /&gt;&lt;br /&gt;In the afternoon, researchers from OSU offered a rare peak into Alibaba through a high-level analysis of SSD failure data provided by the Chinese hyperscaler:&lt;br /&gt;&lt;br /&gt;&amp;lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://4.bp.blogspot.com/-dxNudSspkiM/W_cUTSdBV2I/AAAAAAABCm0/lnCQWJ4BdYccKgd9O9iO3NNy-MapHSZvACK4BGAYYCw/s1600/IMG_4879.jpeg&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;225&quot; src=&quot;https://4.bp.blogspot.com/-dxNudSspkiM/W_cUTSdBV2I/AAAAAAABCm0/lnCQWJ4BdYccKgd9O9iO3NNy-MapHSZvACK4BGAYYCw/s400/IMG_4879.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/div&amp;gt;
&lt;br /&gt;&lt;br /&gt;The most alarming finding to me was that 20% of SSD failures were caused by humans yanking the wrong SSD.  This immediately made me wonder who Alibaba is hiring to do routine operational support at their data centers; if people are causing a significant fraction of storage faults, either they aren’t hiring with the same standards as their US counterparts, or their data centers are a mess.  The speaker’s proposed remedy was to use a different SSD form factor for each logical use case for SSDs so that operators could visually identify an SSD reserved for metadata versus one reserved for data.  I personally think a label maker, a barcode scanner, and a decent salary is an easier, standards-based solution.&lt;br /&gt;&lt;br /&gt;Other highlights included&lt;br /&gt;&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;&lt;i&gt;Characterizing Deep-Learning I/O Workloads in TensorFlow&lt;/i&gt;, presented by Stefano Markidis of KTH.  The first time I’ve seen an I/O-centric evaluation of how deep learning workflows will affect storage requirements of future systems.  I learned a lot.&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&lt;i&gt;Toward Understanding I/O Behavior in HPC Workflows&lt;/i&gt;, presented by Jakob Lüttgau of DKRZ/ANL.  Rather than analyze the I/O pattern of a single MPI job, this paper began examining the I/O patterns of related jobs that all work towards a single scientific objective.  Again, one of the first research papers I’ve seen that takes a critical look at end-to-end workflows from an I/O perspective.&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&lt;i&gt;Methodology for the Rapid Development of Scalable HPC Data Services&lt;/i&gt;, presented by Matthieu Dorier of ANL.  I think this paper is intended to be the canonical reference for &lt;a href=&quot;https://press3.mcs.anl.gov/mochi/&quot;&gt;the Mochi project&lt;/a&gt;, which I was glad to finally see.  The idea of enabling quickly composable, purpose-built I/O services that are optimized for next-generation media and interconnects is a brilliant one, and I am a huge believer that this approach will be what demonstrates the earliest scientific successes that rely on storage-class memory at scale.&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;br /&gt;There were a number of really promising ideas presented at the WIP sessions as well, and recapping the entirety of the workshop is a blog post in and of itself.  Fortunately, all the papers and slides are openly available on the &lt;a href=&quot;http://www.pdsw.org/&quot;&gt;PDSW-DISCS website&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;&amp;lt;h3 id=&quot;gala&quot;&amp;gt;SC Opening Keynote and Gala&amp;lt;/h3&amp;gt;I’ve actually stopped going to the SC keynotes over the last year since they’re increasingly focused on the societal impacts enabled by HPC rather than HPC itself.  While I’m definitely not knocking that theme–it’s a great way to inspire early-career individuals, big-picture program management types, and disenchanted technical folks in the trenches–it’s just not why I attend SC.  Instead, I make use of my exhibitor badge and head into the expo floor before it opens to the public; this is the only time during the conference where I seem to be able to reliably find the people I want to meet at their booths.&lt;br /&gt;&lt;br /&gt;This year I visited a few small businesses with whom I’ve fostered good will over the last few years to say hello, then dropped in on the SDSC booth to catch up with the latest news from my former coworkers.  They also happen to have free beer on the opening night.&lt;br /&gt;&lt;br /&gt;Once the expo floor opens to the public following the opening keynote, booth activity goes from zero to eleven really quickly.  Every booth has a big splash during the gala which makes it hard to choose just one, but my decision this year was made easier by Cray choosing to unveil its new exascale HPC platform, Shasta, and celebrate its &lt;a href=&quot;http://investors.cray.com/phoenix.zhtml?c=98390&amp;amp;p=irol-newsarticle&amp;amp;ID=2374181&quot;&gt;first sale of a Shasta system to NERSC&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://4.bp.blogspot.com/-aNYQ3TPdHb8/W_ceLrO4wUI/AAAAAAABCnA/mSzGaeTYLEwQMvIC4k-Hsbo5-1YFM-kmACK4BGAYYCw/s1600/IMG_4890.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;300&quot; src=&quot;https://4.bp.blogspot.com/-aNYQ3TPdHb8/W_ceLrO4wUI/AAAAAAABCnA/mSzGaeTYLEwQMvIC4k-Hsbo5-1YFM-kmACK4BGAYYCw/s400/IMG_4890.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;Cray CEO Pete Ungaro at the Shasta unveiling ceremony&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&lt;br /&gt;This new system, named &lt;a href=&quot;http://www.nersc.gov/systems/perlmutter/&quot;&gt;Perlmutter&lt;/a&gt;, will be delivered in 2020 and has a bunch of really slick new technologies incorporated into it.&lt;br /&gt;&lt;br /&gt;After Cray CEO Pete Ungaro unveiled the prototype Shasta blades, there was a celebratory toast and both NERSC and Cray staff donned their “ASK ME ABOUT SAUL” pins:&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://2.bp.blogspot.com/-WZaeoBPQOD0/W_cfwQuPqeI/AAAAAAABCnM/s4WPrAzqC3og8NZtGbmArI0OkuujzKFgACK4BGAYYCw/s1600/IMG_1897.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;200&quot; src=&quot;https://2.bp.blogspot.com/-WZaeoBPQOD0/W_cfwQuPqeI/AAAAAAABCnM/s4WPrAzqC3og8NZtGbmArI0OkuujzKFgACK4BGAYYCw/s200/IMG_1897.jpeg&quot; width=&quot;200&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;NERSC and Cray staff got these VIP pins to promote NERSC’s next system, named after astrophysicist, Nobel laureate, and Berkeley Lab scientist Saul Perlmutter.&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&lt;br /&gt;I stuck around to shake hands with my colleagues at Cray (including the CEO himself!  Haven’t washed my hand since) and catch up with some of my counterparts in storage R&amp;amp;D there.&lt;br /&gt;&lt;br /&gt;&amp;lt;h3 id=&quot;bash&quot;&amp;gt;The Beowulf Bash&amp;lt;/h3&amp;gt;The gala shut down at 9 PM, at which time I headed over to the &lt;a href=&quot;https://beowulfbash.com/&quot;&gt;Beowulf Bash&lt;/a&gt; to try to find other some colleagues who said they would be there.  I generally don’t prioritize parties at SC for a couple reasons:&lt;br /&gt;&amp;lt;ol&amp;gt;&amp;lt;li&amp;gt;Shouting over music all night is a great way to burn out one’s voice.  This is not good when I have to present something the next day.&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;The crowds and lines often undercut my enjoyment of catching up with old colleagues (and meeting new ones).&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;I almost always have slides that need to be finished by the end of the night.&amp;lt;/li&amp;gt;&amp;lt;/ol&amp;gt;&amp;lt;div&amp;gt;I make an exception for the Bash because I personally value many of the people behind organizing and sponsoring it, and it captures the scrappier side of the HPC community which helped me get my foot in the door of the industry.  This year I specifically went to catch up with my colleagues at &lt;a href=&quot;https://www.nextplatform.com/&quot;&gt;The Next Platform&lt;/a&gt;; Nicole and Tim are uncommonly insightful and talented writers and editors, and they always have wacky anecdotes to share about some of the more public figures in our industry.&amp;lt;/div&amp;gt;&lt;/p&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;More generally and self-servingly though, maintaining a good relationship with members of the HPC trade press at large has tremendous value over time regardless of your affiliation or job title. &amp;nbsp;Behind every interesting HPC news article is an editor with incomparable access to a broad network of people in the industry. &amp;nbsp;Despite this though, they still are subject to the same haters as anyone else who puts something out in the spotlight, so I have to imagine that putting in a kind word in-person will is always worth it.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;At around midnight, only the die-hards were still around.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;http://1.bp.blogspot.com/-ZdI6xLEacbM/W_cnS_b4VhI/AAAAAAABCnY/J1EGks-vYbI8bw6o6OKlBZl6eHcmSr5YwCK4BGAYYCw/s1600/IMG_4891.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;225&quot; src=&quot;https://1.bp.blogspot.com/-ZdI6xLEacbM/W_cnS_b4VhI/AAAAAAABCnY/J1EGks-vYbI8bw6o6OKlBZl6eHcmSr5YwCK4BGAYYCw/s400/IMG_4891.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Late night Beowulf Bash at Eddie Deen's Ranch.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;Regrettably, I barely had any time to catch up with my colleagues from the FreeNode HPC community at the Bash (or at all). &amp;nbsp;Maybe at ISC.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;After getting back to the hotel, I realized I hadn't eaten anything since lunch. &amp;nbsp;I also learned that absolutely nothing that delivers food in the downtown Dallas area is open after midnight. &amp;nbsp;After waiting an hour for a food delivery that wound up going to a restaurant that wasn't even open, I had to settle for a hearty dinner of Hot Pockets from the hotel lobby.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;http://3.bp.blogspot.com/-Mo_QP_NnEow/W_coivLrvoI/AAAAAAABCnk/rStboNb1iAQ2GLWPIAElzC3IdCoRqvmcQCK4BGAYYCw/s1600/56378490119__E6748A65-8655-4DDC-8502-639F0A830956.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;320&quot; src=&quot;https://3.bp.blogspot.com/-Mo_QP_NnEow/W_coivLrvoI/AAAAAAABCnk/rStboNb1iAQ2GLWPIAElzC3IdCoRqvmcQCK4BGAYYCw/s320/56378490119__E6748A65-8655-4DDC-8502-639F0A830956.jpg&quot; width=&quot;240&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;I hadn't eaten a Hot Pocket since graduate school. &amp;nbsp;Still taste the same.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;Fortunately my Tuesday was relatively light on hard obligations.&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&amp;lt;h2 id=&quot;tuesday&quot;&amp;gt;Tuesday, November 13&amp;lt;/h2&amp;gt;&amp;lt;div&amp;gt;Tuesday was the first day in which the SC technical program and expo were both in full swing.  I split the day between paper talks, meetings, and the expo floor.&lt;br /&gt;&lt;br /&gt;&amp;lt;/div&amp;gt;&lt;/p&gt;
&lt;h3 id=&quot;tuesdaytechprog&quot;&gt;Technical Program, Part 1 - Data and Storage&lt;/h3&gt;
&lt;p&gt;My Tuesday morning began at 10:30 AM with the &lt;a href=&quot;https://sc18.supercomputing.org/session/?sess=sess179&quot;&gt;Data and Storage paper presentation session&lt;/a&gt; in the technical program.  Of note, the &lt;a href=&quot;https://twitter.com/glennklockwood/status/1062385999026814976&quot;&gt;first two papers presented were about cloud-centric storage&lt;/a&gt; paradigms, and only the third one was clearly focused on scientific HPC workloads.&lt;br /&gt;&lt;br /&gt;&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;&lt;a href=&quot;https://sc18.supercomputing.org/presentation/?id=pap165&amp;amp;sess=sess179&quot;&gt;SP-Cache: Load-Balanced, Redundancy-Free Cluster Caching with Selective Partition&lt;/a&gt; by Yu et al was a paper squarely aimed at reducing tail latency of reads.  Very important if you want to load an old GMail message without waiting more than a few seconds for it to load.  Less useful for most scientific HPC workloads.&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&lt;a href=&quot;https://sc18.supercomputing.org/presentation/?id=pap585&amp;amp;sess=sess179&quot;&gt;BESPOKV: Application Tailored Scale-Out Key-Value Stores&lt;/a&gt; by Anwar et al was a paper presenting a framework that is uncannily similar to the Mochi paper presented at PDSW on the day before.  The premise was to allow people to compose their own Cassandra-like KV store with specific consistency and durability balance without having to reinvent the basic building blocks.&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&lt;a href=&quot;https://sc18.supercomputing.org/presentation/?id=pap450&amp;amp;sess=sess179&quot;&gt;Scaling Embedded In Situ Indexing with DeltaFS&lt;/a&gt; by Zheng et al was the talk I really wanted to hear but I had to miss on account of a conflicting meeting.  The DeltaFS work being done by CMU and LANL is a really innovative way to deal with the scalability challenges of parallel file system metadata, and I think it’s going to ultimately be where many of the nascent software-defined storage technologies aimed at HPC will converge.&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;div&amp;gt;Unfortunately I had to cut out of the session early to meet with a vendor partner at a nearby hotel.&amp;lt;/div&amp;gt;
&lt;br /&gt;&amp;lt;h3 id=&quot;tuesdayinterlude&quot;&amp;gt;Interlude of Meetings&amp;lt;/h3&amp;gt;The first of my two vendor meetings at this year’s SC was less a sales call and more about continuing a long-running discussion about technology futures in the five-to-ten year timeframe.  No sane vendor will commit to any roadmap that far out, especially given the uncertainty surrounding post-Moore’s Law technologies, but they are receptive to input from customers who are formulating their own strategic directions for the same time period.  Maintaining these sorts of ongoing conversations is a major part of what falls under my job title in “advanced technologies.”&lt;br /&gt;&lt;br /&gt;Unfortunately that vendor meeting overlapped with the Lustre BOF, but other staff from my institution were able to attend and ensure that our interests were represented.  I was also able to attend the Lustre Lunch that followed the BOF which was very fruitful; in addition to simply being present to remind the Lustre community that I (and the institution I represent) am a part of it, I happened to connect in-person with &lt;a href=&quot;https://twitter.com/rajgautam&quot;&gt;someone I’ve known for a few years via Twitter and make a valuable connection&lt;/a&gt;.  Unfortunately I had to leave the Lustre Lunch early to make another meeting, unrelated to SC, that allowed a geographically distributed committee to meet face-to-face.&lt;br /&gt;&lt;br /&gt;After that committee meeting, I seized the free hour I had to visit the show room floor.&lt;br /&gt;&lt;br /&gt;&amp;lt;h3 id=&quot;tuesdayexpo&quot;&amp;gt;Expo Floor, Part 1&amp;lt;/h3&amp;gt;The first photo-worthy tech I saw was the Shasta blade at the &lt;b&gt;Cray booth&lt;/b&gt;.  Because the booth was mobbed with people during the previous night’s gala, this was actually my first time seeing Shasta hardware up close.  Here’s the compute blade:&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://1.bp.blogspot.com/-RC-DE7ZI8CY/W_dAcEULkyI/AAAAAAABCoM/yv5pEDrWxrAzyWFn3IlvIfM6zODvsfwgwCK4BGAYYCw/s1600/IMG_4899.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;300&quot; src=&quot;https://1.bp.blogspot.com/-RC-DE7ZI8CY/W_dAcEULkyI/AAAAAAABCoM/yv5pEDrWxrAzyWFn3IlvIfM6zODvsfwgwCK4BGAYYCw/s400/IMG_4899.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;Part of a Cray Shasta compute blade up-close&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&lt;br /&gt;Unlike the Cray XC blade of today’s systems which uses a combination of forced-air convection and heat exchangers to enable liquid cooling, these Shasta blades have direct liquid cooling which is rapidly becoming a de facto minimum requirement for an exascale-capable rack and node design.  I had some questions, so I struck up a conversation with a Cray employee at the booth and learned some neat things about the Shasta packaging.&lt;br /&gt;&lt;br /&gt;For the sake of clarity, here is a hand-drawn, annotated version of the same photo:&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://1.bp.blogspot.com/-1x8QsBpe6ok/W_dAql81IwI/AAAAAAABCoU/pRyu5minI_0XK8_k_iFW3kSh6vc_nadJQCK4BGAYYCw/s1600/IMG_4899%2B2.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;300&quot; src=&quot;https://1.bp.blogspot.com/-1x8QsBpe6ok/W_dAql81IwI/AAAAAAABCoU/pRyu5minI_0XK8_k_iFW3kSh6vc_nadJQCK4BGAYYCw/s400/IMG_4899%2B2.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;Part of a Cray Shasta compute blade up-close with my annotations&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&lt;br /&gt;What stood out to me immediately was the interesting way in which the DIMMs were direct-liquid cooled.  Unlike IBM’s attempt at this with the POWER 775 system (the PERCS system of Blue Waters infamy) where cold plates were attached to every DIMM, Cray has opted to use what looks like a heat-conductive foam that wraps copper cooling lines.  To service the DIMMs, the entire copper cooling complex that runs between the two rows of two DIMMs unfastens and lifts up.  There’s enough slack in the liquid cooling lines (highlighted in purple) so that DIMMs (and presumably every other field-replaceable part in the blade) can be serviced without draining the coolant from the blade.&lt;br /&gt;&lt;br /&gt;The NIC is also pretty interesting; it is a commercial high-end data center Ethernet NIC that’s manufactured in a custom form factor to fit this blade.  It looks like a second CPU is housed underneath the NIC so that it may be the case that the NIC and one of the CPUs shares a common cooling block.  The NIC is also positioned perpendicular to the long edge of the blade, meaning that there are probably some pretty good cable runs going from the front-most NIC all the way to the rear of the blade.  Finally, because the NIC is on a discrete mezzanine card, the networking technology is no longer soldered to the compute as it is with Aries on today’s XC.&lt;br /&gt;&lt;br /&gt;The network switch (which &lt;a href=&quot;https://twitter.com/ernstdj/status/1062425074425315328&quot;&gt;I did not photograph, but others did&lt;/a&gt;) is another blade that slots into the rear of the Shasta cabinet and mates perpendicularly with a row of compute blades such that a single switch blade can service a fully populated compute chassis.  The engineer with whom I spoke said that these Shasta cabinets have no actual midplane; the compute blades connect directly to the switch blades through a bunch of holes cut out of the sheet metal that separates the front of the cabinet from the rear.  Without a midplane there is presumably one less single point of failure; at the same time though, it wasn’t clear to me how out-of-band management works without a centralized controller somewhere in the chassis.&lt;br /&gt;&lt;br /&gt;At this point I should point out that all of the above information is what I learned by talking to a Cray booth employee at SC without any special privilege; although I’m sure that more details are available under non-disclosure, I frankly don’t remember any of it because I don’t work on the compute side of the system.&lt;br /&gt;&lt;br /&gt;My next big stop on the show room floor was at the &lt;b&gt;Fujitsu booth&lt;/b&gt;, where they had their post-K prototype hardware on display.  Of particular note was their A64FX engineering sample:&lt;br /&gt;&lt;br /&gt;&amp;lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://1.bp.blogspot.com/-SxfvUEu4-a8/W_dHNdImJcI/AAAAAAABCok/6AVXnJCCjMgZ2bd1z1Xyg6xBttqofVXSACK4BGAYYCw/s1600/IMG_4903.jpeg&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;225&quot; src=&quot;https://1.bp.blogspot.com/-SxfvUEu4-a8/W_dHNdImJcI/AAAAAAABCok/6AVXnJCCjMgZ2bd1z1Xyg6xBttqofVXSACK4BGAYYCw/s400/IMG_4903.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/div&amp;gt;
&lt;br /&gt;&lt;br /&gt;If you look very carefully, you can see the four stacks of high-bandwidth memory (HBM) on-die along with the ARM, which is fantastically historic in that it’s the first general-purpose CPU (of which I am aware) that has integrated HBM2.  What’s not present is any indication of how the on-chip Tofu NIC is broken out; I guess I was expecting something like Intel’s -F series KNLs with on-package OmniPath.&lt;br /&gt;&lt;br /&gt;A sample node of the post-K system was also on display:&lt;br /&gt;&lt;br /&gt;&amp;lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://1.bp.blogspot.com/-E-nmVR8mYuk/W_dIVsPZPqI/AAAAAAABCow/wiOV3yLXH60Q3EoqKilPsgD4xkbMnGssACK4BGAYYCw/s1600/IMG_4902.jpeg&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;300&quot; src=&quot;https://1.bp.blogspot.com/-E-nmVR8mYuk/W_dIVsPZPqI/AAAAAAABCow/wiOV3yLXH60Q3EoqKilPsgD4xkbMnGssACK4BGAYYCw/s400/IMG_4902.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/div&amp;gt;
&lt;br /&gt;Seeing as how both this post-K system and Cray Shasta are exascale-capable system architectures, it’s interesting to compare and contrast them.  Both have direct liquid cooling, but the post-K compute blade does not appear to have any field-replaceable units.  Instead, the entire board seems to be a single FRU, so CPUs must be serviced in pairs.  I think the A64FX lacks any cache coherence bus, meaning that two CPUs correspond to two nodes per FRU.&lt;br /&gt;&lt;br /&gt;That all said, the post-K design does not appear to have any DDR DRAM, and the NIC is integrated directly into the CPU.  With those two components out of the picture, the rate of a single component failure is probably a lot lower in post-K than it would be in Shasta.  Hopefully the post-K HBM has ECC though!&lt;br /&gt;&lt;br /&gt;In chatting with a Fujitsu engineer about the post-K node architecture at their booth, I also met &lt;a href=&quot;https://twitter.com/hei_nyan&quot;&gt;a Fujitsu engineer&lt;/a&gt; who just happened to be developing LLIO, the post-K system’s burst buffer service:&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://2.bp.blogspot.com/-IsqISZrmqBw/W_dNKiGmH2I/AAAAAAABCo8/FFgBybUIUD4JfINVq6BRK7Q6Yait4nbRACK4BGAYYCw/s1600/IMG_4904.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;262&quot; src=&quot;https://2.bp.blogspot.com/-IsqISZrmqBw/W_dNKiGmH2I/AAAAAAABCo8/FFgBybUIUD4JfINVq6BRK7Q6Yait4nbRACK4BGAYYCw/s400/IMG_4904.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;LLIO burst buffer slide shown at the Fujitsu booth&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&lt;br /&gt;It sounds a lot like DataWarp in terms of features, and given that Fujitsu is also developing a new Lustre-based file system (FEFS 2.0?) for post-K, we might see a tighter integration between the LLIO burst buffer layer and the FEFS back-end disk storage.  This is definitely a technology that wasn’t on my radar before SC but is definitely worth keeping an eye on as 2021 approaches.&lt;br /&gt;&lt;br /&gt;As I was racing between a few other booths, I also happened upon my boss (and NERSC-9 chief architect) presenting the Perlmutter system architecture at the &lt;b&gt;NVIDIA booth&lt;/b&gt;:&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://1.bp.blogspot.com/-lsEkqobGmZI/W_dht1RUO-I/AAAAAAABCpI/FRUPAFyIpJYHrRlODSyeFVp8Gma8w4JqACK4BGAYYCw/s1600/IMG_4905.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;225&quot; src=&quot;https://1.bp.blogspot.com/-lsEkqobGmZI/W_dht1RUO-I/AAAAAAABCpI/FRUPAFyIpJYHrRlODSyeFVp8Gma8w4JqACK4BGAYYCw/s400/IMG_4905.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://www.nersc.gov/about/nersc-staff/advanced-technologies-group/nicholas-wright/&quot;&gt;NERSC’s Nick Wright&lt;/a&gt;, chief architect of the Perlmutter system, describing its architecture at the NVIDIA booth&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&lt;br /&gt;&lt;br /&gt;The talk drew a crowd–I’m glad to see people as jazzed about the new system as I am.&lt;br /&gt;&lt;br /&gt;&amp;lt;h3 id=&quot;paralleliobof&quot;&amp;gt;Analyzing Parallel I/O BOF&amp;lt;/h3&amp;gt;The &lt;a href=&quot;https://sc18.supercomputing.org/presentation/?id=bof123&amp;amp;sess=sess382&quot;&gt;Analyzing Parallel I/O BOF&lt;/a&gt; is a must-attend event for anyone in the parallel I/O business, and this year’s BOF was especially good.  Andreas Dilger (of Lustre fame; now CTO of Whamcloud) gave a brief but insightful retrospective on understanding I/O performance:&lt;br /&gt;&lt;br /&gt;&amp;lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://3.bp.blogspot.com/-MgBl_XxRySg/W_djpH7FX7I/AAAAAAABCpU/gyigkciDcgEHKl314Li6Qa-9xsD9L637gCK4BGAYYCw/s1600/IMG_4908.jpeg&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;225&quot; src=&quot;https://3.bp.blogspot.com/-MgBl_XxRySg/W_djpH7FX7I/AAAAAAABCpU/gyigkciDcgEHKl314Li6Qa-9xsD9L637gCK4BGAYYCw/s400/IMG_4908.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/div&amp;gt;
&lt;br /&gt;Unfortunately I did not take a picture of Andreas’ second slide (available on &lt;a href=&quot;https://hps.vi4io.org/events/2018/bof-analyzing&quot;&gt;the Analyzing Parallel I/O BOF’s website&lt;/a&gt;) which is a “what is needed?” slide which largely revolves around better integration between storage system software (like Lustre) and user applications.  I/O middleware seems to be at the center of most of the bullets that called for increased development which bodes well for scientific application developers who attended the Parallel I/O in Practice tutorial on Sunday–recall that this was my key takeaway.  It’s good to know that the lead of Lustre development agrees with this vision of the future, and I hope Whamcloud moves Lustre in this direction so users and middleware developers can meet the storage system software somewhere in the middle.&lt;br /&gt;&lt;br /&gt;The BOF took a darker turn after this, starting with a presentation from Si Liu of TACC about the Optimal Overloaded IO Protection System, or OOOPS.  It’s a library that wraps the standard POSIX I/O calls:&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://2.bp.blogspot.com/-QqCtiINUB4o/W_dnlx7FK_I/AAAAAAABCpg/Acch-MI1AKoWQLt2G53_fOJ2UUqI2o8XQCK4BGAYYCw/s1600/IMG_4909.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;240&quot; src=&quot;https://2.bp.blogspot.com/-QqCtiINUB4o/W_dnlx7FK_I/AAAAAAABCpg/Acch-MI1AKoWQLt2G53_fOJ2UUqI2o8XQCK4BGAYYCw/s320/IMG_4909.jpeg&quot; width=&quot;320&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;OOOPS operates by hijacking standard I/O calls and lagging them.&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&lt;br /&gt;&lt;br /&gt;But in addition to passively monitoring how an application performs I/O, it purposely injects latency to throttle the rate at which I/O operations get issued by an application.  That is, it purposely slows down I/O from clients to reduce server-side load and, by extension, the effects of a single bad actor on the I/O performance of all the other users.&lt;br /&gt;&lt;br /&gt;Ideologically, I have a lot of problems with an HPC facility inserting itself into the user’s workflow and reducing the efficiency with which he or she can accomplish their science relative to the peak capability of the HPC resource.  If a storage system allows a single user to accidentally deny service to other users in pursuit of peak performance, that is a problem with the storage system and it should be addressed at the system level.  And as Andreas pointed out in the BOF, tools exist to allow storage systems to accomplish fair sharing, which is distinctly different from explicitly penalizing users.  Granted, TACC is also the facility where one of its staff went on record as saying that the R language should not be used by anyone since it is a waste of energy.  Perhaps they have an institutionally different relationship with their user community.&lt;br /&gt;&lt;br /&gt;Fortunately, anything that relies on LD_PRELOAD can be circumvented by users, so OOOPS is unlikely to be used to enforce any kind of resource usage policy as it was pitched during the BOF.  I do see a lot of value in using it to fence data analysis workflows that may hit a pathological condition as a result of their inputs, and being able to trigger changes in application behavior by tracking I/O rates is a technique that could be useful in auto-tuning I/O middleware.&lt;br /&gt;&lt;br /&gt;Rosemary Francis, CEO of Ellexus, also spoke at the BOF and spoke for the need to make I/O performance analysis a little more accessible for the end users.  I was quite delighted by the visualizations she presented (presumably from her company’s Breeze product) which used both color and human-readable “bad” I/O patterns to create a pie graph that quickly shows how much time an application spent doing I/O in various good, bad, and neutral ways.  Darshan, the tried-and-true open source I/O profiling library, operates at a slightly lower level and assumes a slightly higher level of user sophistication by comparison.&lt;br /&gt;&lt;br /&gt;The discussion half of the BOF was packed with engagement from the audience–so much so that I didn’t find any moments of silence to seize the opportunity to stump for my own view of the world.  The combination of OOOPS and Rosemary’s I/O war stories did steer the discussion towards ways to punish bad users though.  I can appreciate HPC operators’ frustration in novice users causing system-wide problems, but I don’t think shaming users who do bad I/O is a great solution.  Rather, something between OOOPS’ automatic identification of bad I/O at runtime and Ellexus’ user-centric reporting and feedback, combined with storage systems capable of enforcing QOS, is where we need to go.&lt;br /&gt;&lt;br /&gt;&amp;lt;h3 id=&quot;crayparty&quot;&amp;gt;The Cray Celebration&amp;lt;/h3&amp;gt;I wrote earlier that I normally don’t do the SC vendor party circuit, but the Cray party this year was another exception for two reasons: (1) we had just announced Perlmutter along with Cray’s Shasta unveiling which is worth celebrating, and (2) there were specific Cray staff with whom I wanted to confer sometime during the week.  So after the Parallel I/O BOF, I headed over to the event venue:&lt;br /&gt;&lt;br /&gt;&amp;lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://4.bp.blogspot.com/-RVFQ8qG9FLg/W_d4w-ZzXpI/AAAAAAABCpw/rj2edkzFiMc5PQYOM6tAWh-475M7BdlQgCK4BGAYYCw/s1600/IMG_4910.jpeg&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;400&quot; src=&quot;https://4.bp.blogspot.com/-RVFQ8qG9FLg/W_d4w-ZzXpI/AAAAAAABCpw/rj2edkzFiMc5PQYOM6tAWh-475M7BdlQgCK4BGAYYCw/s400/IMG_4910.jpeg&quot; width=&quot;300&quot; /&gt;&lt;/a&gt;&amp;lt;/div&amp;gt;
&lt;br /&gt;The event was quite nice in that it was not held at a loud bar (which made conversation much easier), it had plenty of food (no need for 2 AM Hot Pockets), and the format was conducive to moving around and meeting a lot of different people.  The event was awash with representatives from all the major Cray customers including the DOE labs, the big oil &amp;amp; gas companies, and the regional leadership computing centers in EMEA including CSCS and KAUST, as well as alumni of all those employers and Cray itself.  I’ve only worked at a Cray customer site for three years now, but I couldn’t walk ten feet without running into someone I knew; in that sense, it felt a little like an event at the annual Cray User Group meeting but with a broader range of attendees.&lt;br /&gt;&lt;br /&gt;I don’t know what this event would’ve been like if I was a student or otherwise didn’t already know many of the regular faces within the Cray user community and instead had to start conversations cold.  That said, I was busy the entire evening getting to know the people behind all the conference calls I’m on; I find that getting to know my industry counterparts as people rather than just vendor reps really pays dividends when surprises happen and conflicts need to be resolved.  Events like this at SC are invaluable for building and maintaining these sorts of relationships.&lt;br /&gt;&lt;br /&gt;&amp;lt;h2 id=&quot;wednesday&quot;&amp;gt;Wednesday, November 14&amp;lt;/h2&amp;gt;My Wednesday began bright and early with a quick run-around of the expo floor to figure out who I needed to visit before the end of the week.&lt;br /&gt;&lt;br /&gt;&amp;lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://4.bp.blogspot.com/-SSHpcrqDu3o/W_eXyaZ5sxI/AAAAAAABCp8/ptdxmFa9eh4_-QgJvcmQqZtCQBYbP5d2wCK4BGAYYCw/s1600/IMG_4913.jpeg&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;155&quot; src=&quot;https://4.bp.blogspot.com/-SSHpcrqDu3o/W_eXyaZ5sxI/AAAAAAABCp8/ptdxmFa9eh4_-QgJvcmQqZtCQBYbP5d2wCK4BGAYYCw/s400/IMG_4913.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/div&amp;gt;
&lt;br /&gt;The expo floor was awkwardly laid out this year, so I really needed to do this to make sure I didn’t spin my tires trying to find certain booths once the crowd showed up.  Incidentally, I did witness a sales person violate the unwritten rule of keeping everything friendly until the expo floor opened to the public–a sales rep selling “the world’s fastest storage system” tried to stir up cold sales leads at my employer’s booth at 8 AM while we were all still drinking our coffee and catching up on e-mail.  If you do this, shame on you!  Respect the exhibitor access and don’t put your game face on until the public is allowed in.&lt;br /&gt;&lt;br /&gt;&amp;lt;h3 id=&quot;wednesdaymorning&quot;&amp;gt;SC Student Career Fair and Booth Talk&amp;lt;/h3&amp;gt;My first meeting was a chat over coffee with &lt;a href=&quot;https://www.vastdata.com/&quot;&gt;VAST Data&lt;/a&gt;, a storage technology company that has some really innovative and exciting ideas in the pipeline, to keep up to date with the latest news as they approach public launch.&lt;br /&gt;&lt;br /&gt;My second obligation was volunteering at my employer’s booth at the SC Career Fair.  I generally enjoy booth duty and talking to students, and this year I was doubly motivated by my desire to fill some career and student job openings related to my responsibilities.  A diverse cross section of students dropped by our booth looking for both summer internships and full-time jobs; many seemed very well rehearsed in their cold pitch, while some others were a little more casual or cautious.  Although I’m not particularly qualified to give career advice, I will say that knowing how to sell yourself cold can be a valuable skill in your early career.  If you are seeking employment, be prepared to respond to a request to “tell me about yourself” in a way that makes you stand out.&lt;br /&gt;&lt;br /&gt;After the Career Fair, I wound up hunkering down at the SDSC booth to have lunch with my former coworkers and review the slides I volunteered to present at the adjacent DDN booth.&lt;br /&gt;&lt;br /&gt;At 2 PM I took the stage (booth?) and one of my colleagues was not only kind enough to sit in on this booth talk, but also &lt;a href=&quot;https://twitter.com/suhaibkhan/status/1062797409963724800&quot;&gt;share this photo he took&lt;/a&gt; right before I started:&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://3.bp.blogspot.com/-psgfnpOmiNs/W_eg1Ao9N2I/AAAAAAABCqI/BX_TzjeqBXsPg73VMO7OzW5p2K-wP1lfACK4BGAYYCw/s1600/IMG_1881.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;300&quot; src=&quot;https://3.bp.blogspot.com/-psgfnpOmiNs/W_eg1Ao9N2I/AAAAAAABCqI/BX_TzjeqBXsPg73VMO7OzW5p2K-wP1lfACK4BGAYYCw/s400/IMG_1881.jpg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;Beginning of my talk at the DDN booth.  Photo credit goes to Suhaib Khan via Twitter.&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&lt;br /&gt;I continue to be humbled that anyone would go out of their way to come hear what I have to say, especially when my talk is as unvetted as booth talks tend to be.  Talking at booths rarely goes well for me; the audio is always a wildcard, the audience is often unwitting, and auditory and visual distractions are literally everywhere.  The DDN booth was my sole booth talk of this year and it went about as well as I would have expected.  On the up side, quite a few attendees seemed genuinely interested to hear what I had to say about the variety of ways one can deploy flash in an HPC system.  Unfortunately, I ran a few minutes long and got derailed by external distractions several times during the presentation though.  Flubbing presentations happens, and none of the audience members seemed to mind.&lt;br /&gt;&lt;br /&gt;Shortly after the booth talk, I had to find a quiet spot to jump on a telecon.  This was no easy task; since cell phones killed the public phone booth, there are very few places to take a call on the expo floor.&lt;br /&gt;&lt;br /&gt;&amp;lt;h3 id=&quot;wednesdayexpo&quot;&amp;gt;Expo Floor, Part 2&amp;lt;/h3&amp;gt;The afternoon afforded me two more hours to race around the expo floor.  Despite my planning earlier in the morning, I wound up spinning my tires looking for a few key vendors who simply didn’t show up to SC this year, including&lt;br /&gt;&lt;br /&gt;&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;Samsung and SK Hynix, two of the top three DRAM vendors and the sole manufacturers of HBM2&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Seagate, one of two hard disk drive manufacturers&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Broadcom/Avago, the company manufacturing most of the serdes used in the upcoming 200G and 400G network devices&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Juniper, one of the major players in the 400 GbE space&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;AdvancedHPC, one of the few US integrators selling BeeGFS&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;br /&gt;I’m not really sure why so many vendors didn’t show up this year, but it made getting a holistic view of the storage and networking technologies markets impossible.  That said, I still saw a few noteworthy things.&lt;br /&gt;&lt;br /&gt;One of the big open questions in high-performance storage revolves around the battle between the NF1 (formerly NGSFF, promoted by Samsung) and EDSFF (promoted by Intel) form factors for NVMe.  It’s clear that these long-and-skinny NVMe designs are going to have to replace the thermally inefficient 2.5” U.2 and unserviceable HHHL PCIe form factors, but the dust is far from being settled.  On the one hand, Samsung leads flash storage sales worldwide, but their NF1 form factor caps the power consumption (and therefore performance) of its devices to levels that are squarely aimed at cheaper data center flash.  On the other, the EDSFF form factor being pushed by Intel has a short version (competing directly with NF1) and a longer version that allows higher power.&lt;br /&gt;&lt;br /&gt;The &lt;b&gt;Supermicro booth&lt;/b&gt; had actual EDSFF drives on display, and this was the first time I could actually see one up-close:&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://1.bp.blogspot.com/-7jvyY9w2JfI/W_evRjopdII/AAAAAAABCqY/9HnWCAv9ovAImo6oeokaY_fcnbvZrXaDQCK4BGAYYCw/s1600/IMG_4915.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;300&quot; src=&quot;https://1.bp.blogspot.com/-7jvyY9w2JfI/W_evRjopdII/AAAAAAABCqY/9HnWCAv9ovAImo6oeokaY_fcnbvZrXaDQCK4BGAYYCw/s400/IMG_4915.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;A long-type EDSFF NVMe drive at the Supermicro booth.  The aluminum casing is actually required to meet the thermals.&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&lt;br /&gt;&lt;br /&gt;What I didn’t realize is that the higher thermal specification enabled by the long-version EDSFF drives requires that the entire SSD circuit board be enclosed in the aluminum casing shown to enable better heat dissipation.  This has the nasty side effect of reducing density; while a standard 19” 1U chassis can fit up to 36 NF1 SSDs, the aluminum casing on long EDSFFs reduces the equivalent density to 32 SSDs.  Although long EDSFF drives can compensate for this by packing more NAND dies on the physically longer EDSFF board, supporting these longer SSDs requires more engineering on the chassis design to fit the same amount of compute into a smaller area.&lt;br /&gt;&lt;br /&gt;Similarly but differently, the &lt;b&gt;Lenovo booth&lt;/b&gt; was showcasing their D3284 JBOD which packs 84x 3.5” HDDs into a double-decker 5U chassis.  I had naively assumed that all of these super-dense 84-drive enclosures were top-loading such that each drive mates to a backplane that is mounted to the floor of the chassis, but it turns out that’s not the case:&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://1.bp.blogspot.com/-hBOA2lL4nlw/W_e1CJZPqoI/AAAAAAABCqk/KC1o-eaOaNYbbvmibNYiD7vfoExdk6jlQCK4BGAYYCw/s1600/IMG_4918.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;300&quot; src=&quot;https://1.bp.blogspot.com/-hBOA2lL4nlw/W_e1CJZPqoI/AAAAAAABCqk/KC1o-eaOaNYbbvmibNYiD7vfoExdk6jlQCK4BGAYYCw/s400/IMG_4918.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;Lenovo’s 5U84 JBOD&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&lt;br /&gt;Instead, each 3.5” drive goes into its 2.5U shelf on its side, and each drive attaches to a carrier that has to be slid slightly toward the front of the JBOD to release the drive, and then slide towards the back of the JBOD to secure it.  This seems a little harder to service than a simple top-load JBOD, but I assume there are thermal efficiencies to be gained by this layout.&lt;br /&gt;&lt;br /&gt;The &lt;b&gt;Western Digital booth&lt;/b&gt; had a pretty broad portfolio of data center products on display.  Their newest gadget seems to be a planar NAND-based U.2 device that can present itself as DRAM through a custom hypervisor.  This sounds like a direct competitor to Intel’s Memory Drive offering which uses ScaleMP’s hypervisor to expose flash as DRAM to a guest VM.  The combination of exposing flash as very slow memory and relying on software virtualization to do this lends this to being a technology not really meant for HPC, and the engineer with whom I spoke confirmed as much.  Virtualized big-and-slow memory is much more appealing to in-memory databases such as SAP HANA.&lt;br /&gt;&lt;br /&gt;Perhaps more interestingly was the lack of any mention of Western Digital’s investment in storage-class memory and microwave-assisted magnetic recording (MAMR) disk drives.  When I prodded about the state of MAMR, I was assured that the technology will work because there is no future of hard drives without some form of energy-assisted magnetic recording.  However, product announcements are still 18-24 months away, and the capacity for these drives will enter the market at the rather underwhelming range of ~20 TB.  Conveniently, this matches Seagate’s recent cry of wolf that they will &lt;a href=&quot;https://www.theregister.co.uk/2018/03/21/seagate_to_drop_multiactuator_hamr_in_2020/&quot;&gt;launch HAMR drives in 2020 at a 20 TB capacity point&lt;/a&gt;.  Western Digital also made no mention of multi-actuator drives, and asking about it only got me a sly grin; this suggests that Western Digital is either playing slow and steady so as not to over-promise, or Seagate has a slight technological lead.&lt;br /&gt;&lt;br /&gt;My last substantive stop of the afternoon was at the IBM booth, where they had one of their new TS4500 tape libraries operating in demo mode.  The window was too reflective to take a vide of the robotics, but I will say that there was a perceptible difference between the robotics in IBM’s enterprise tape library and the robotics in another vendor’s LTO tape library.  The IBM enterprise robotics are downright savage in how forcefully they slam tapes around, and I now fully believe IBM’s claims that their enterprise cartridges are constructed to be more physically durable than standard LTO.  I’m sure there’s some latency benefit to being able to ram tapes into drives and library slots at full speed, but it’s unnerving to watch.&lt;br /&gt;&lt;br /&gt;IBM also had this cheeky infographic on display that was worth a photo:&lt;br /&gt;&lt;br /&gt;&amp;lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://4.bp.blogspot.com/-3ADWFs9y1M8/W_e-W-67hfI/AAAAAAABCqw/6j4stvBWBici6SvDRTx17gIhMCHQNGjOACK4BGAYYCw/s1600/IMG_4919.jpeg&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;400&quot; src=&quot;https://4.bp.blogspot.com/-3ADWFs9y1M8/W_e-W-67hfI/AAAAAAABCqw/6j4stvBWBici6SvDRTx17gIhMCHQNGjOACK4BGAYYCw/s400/IMG_4919.jpeg&quot; width=&quot;300&quot; /&gt;&lt;/a&gt;&amp;lt;/div&amp;gt;
&lt;br /&gt;If I built a tape drive that was still operating after forty years in outer space, I’d want to brag about it too.  But there are a couple of factual issues with this marketing material that probably made every physical scientist who saw it roll their eyes.&lt;br /&gt;&lt;br /&gt;Over at the compute side of the IBM booth, I learned that the Summit and Sierra systems sitting at the #1 and #2 positions on Top500 are built using node architectures that IBM is selling commercially.  There are 2 CPU + 6 GPU nodes (which is what Summit at OLCF has) which require liquid cooling, and 2 CPU + 4 GPU nodes (which is what Sierra at LLNL has) which can be air- or liquid-cooled.  I asked an IBM technologist which configuration is more commercially popular, and the Sierra configuration is currently leading sales due to the relative lack of infrastructure to support direct liquid cooling in commercial data centers.&lt;br /&gt;&lt;br /&gt;This has interesting implications for the exascale technologies I looked at on Tuesday; given that the exascale-capable system designs presented by both the Fujitsu and Cray rely on direct liquid cooling, bridging the gap between achieving exascale-level performance and delivering a commercially viable product is pretty wide from a facilities perspective.  Fortunately, the &lt;a href=&quot;https://twitter.com/ProfMatsuoka/status/1062771762721644544&quot;&gt;Fujitsu A64FX chip usually runs below 200 W&lt;/a&gt; and can feasibly be air-cooled with lower-density packaging, and &lt;a href=&quot;https://www.nextplatform.com/2018/10/30/cray-slingshots-back-into-hpc-interconnects-with-shasta-systems/&quot;&gt;Cray’s Shasta will support standard air-cooled 19” racks&lt;/a&gt; via lower-density nodes.&lt;br /&gt;&lt;br /&gt;&amp;lt;h3 id=&quot;io500bof&quot;&amp;gt;The IO-500/VI4IO BOF&amp;lt;/h3&amp;gt;The second must-attend BOF for people working in I/O is the IO-500 and Virtual Institute for I/O BOF.  It’s a very pragmatic BOF where people discuss system architecture, benchmarking, and various related community efforts, and since 2017, also began to include the semiannual unveiling of the IO-500 list.&lt;br /&gt;&lt;br /&gt;This year was exciting in that the top system, a DDN IME installation at JCAHPC, was unseated by the monstrous storage system attached to the Summit system at Oak Ridge and sustained an astounding 2 TiB/sec and 3 million opens/sec.  In fact, the previous #1 system dropped to #4, and each of the new top three systems was of a different architecture (Spectrum Scale at Oak Ridge, IME at KISTI, and Lustre at Cambridge).&lt;br /&gt;&lt;br /&gt;Perhaps the most interesting of these new submissions was the #3 system, the Data Accelerator at Cambridge, which is a home-grown whitebox system that was designed to be functionally equivalent to DataWarp’s scratch mode:&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://3.bp.blogspot.com/-PmYCsLJDJkg/W_g0D2xheFI/AAAAAAABCrE/4vq_SFQavBMN3jN66nfFXhRaSC7rW4WWQCK4BGAYYCw/s1600/IMG_4927.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;300&quot; src=&quot;https://3.bp.blogspot.com/-PmYCsLJDJkg/W_g0D2xheFI/AAAAAAABCrE/4vq_SFQavBMN3jN66nfFXhRaSC7rW4WWQCK4BGAYYCw/s400/IMG_4927.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;Alasdair King presenting the Data Accelerator design at the IO-500 BOF&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&lt;br /&gt;&lt;br /&gt;The hardware are just Dell boxes with six NVMe drives and one OPA NIC per socket, and the magic is actually handled by a cleanroom reimplementation of the interface that Slurm uses to instantiate DataWarp partitions on Cray XC systems.  Rather than use a sophisticated orchestration system as DataWarp does though, the Data Accelerator translates Slurm #DW pragmas into Ansible plays that spin up and tear down ephemeral Lustre file systems.&lt;br /&gt;&lt;br /&gt;The fact that the #3 fastest storage system in the world is a whitebox NVMe system is really remarkable, and my hat is off to the team at Cambridge that did this work.  As all-flash parallel file systems go from the realm of being a high-end boutique solution and become affordably mainstream, relatively scrappy but innovative engineering like the Cambridge system are surely going to cause a rapid proliferation of flash adoption in HPC centers.&lt;br /&gt;&lt;br /&gt;DDN also presented their software-defined IO-500 submission, this time run in Google Cloud and landing in the #8 position:&lt;br /&gt;&lt;br /&gt;&amp;lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://2.bp.blogspot.com/-k8mAG6N4jfM/W_g2VNcP7eI/AAAAAAABCrQ/KVIbo_dGmq0fYeCtLQUiYVRGVxldRJGAACK4BGAYYCw/s1600/IMG_4929%2B2.jpeg&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;300&quot; src=&quot;https://2.bp.blogspot.com/-k8mAG6N4jfM/W_g2VNcP7eI/AAAAAAABCrQ/KVIbo_dGmq0fYeCtLQUiYVRGVxldRJGAACK4BGAYYCw/s400/IMG_4929%2B2.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/div&amp;gt;
&lt;br /&gt;Since DDN’s embedded SFA product line already runs virtual machines on their controller hardware, it doesn’t seem like a big stretch to run the same SFA VMs in the cloud.  While this sounds a little counterproductive to DDN’s biggest differentiator in providing a fully integrated hardware platform, this idea of running SFA in Google Cloud arose from the growing need for parallel file systems in the cloud.  I can only assume that this need is being largely driven by AI workloads which require a combination of high I/O bandwidth, high IOPS, and POSIX file interfaces.&lt;br /&gt;&lt;br /&gt;&amp;lt;h2 id=&quot;thursday&quot;&amp;gt;Thursday, November 15&amp;lt;/h2&amp;gt;&amp;lt;div&amp;gt;The conference was showing signs of winding down by Thursday, as many attendees brought their luggage with them to the convention center so they could head back home that night.  The expo floor also closes in the mid-afternoon on Thursday.&amp;lt;/div&amp;gt;&lt;/p&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h3 id=&quot;thursdaytechprog&quot;&gt;Technical Program, Part 2 - Exhibitor Forum&lt;/h3&gt;
&lt;p&gt;My Thursday began at 10:30 AM with the &lt;a href=&quot;https://sc18.supercomputing.org/session/?sess=sess270&quot;&gt;HPC Storage and Memory Architectures session&lt;/a&gt; of the Exhibitor Forum.  Liran Zvibel, former CTO and now CEO of WekaIO was the first presenter and gave a surprisingly technical description of the &lt;b&gt;WekaIO Matrix parallel file system&lt;/b&gt; architecture:&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://4.bp.blogspot.com/-OspyBccJT8w/W_hL_EChGsI/AAAAAAABCrc/9qCH5J61FuopiqwpxaMILtA98g_uP8hagCK4BGAYYCw/s1600/IMG_4933.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;300&quot; src=&quot;https://4.bp.blogspot.com/-OspyBccJT8w/W_hL_EChGsI/AAAAAAABCrc/9qCH5J61FuopiqwpxaMILtA98g_uP8hagCK4BGAYYCw/s400/IMG_4933.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;WekaIO’s Matrix file system architecture block diagram.  Surprising amount of detail can be cleaned by examining this carefully.&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&lt;br /&gt;In terms of building a modern parallel file system from the ground up for all-flash, WekaIO checks off almost all of the right boxes.  It runs almost entirely in user space to keep latency down, it runs in its own reserved pool of CPU cores on each client, and capitalizes on the approximate parity between NVMe latency and modern high-speed network latency.  They make use of a lot of the smart ideas implemented in the enterprise and hyperscale storage space too and are one of the few really future-looking storage companies out there who are really thinking about the new possibilities in the all-flash world while still courting the HPC market.&lt;br /&gt;&lt;br /&gt;There is a fair amount of magic involved that was not broken down in the talk, although I’ve found that the WekaIO folks are happy to explain some of the more complex details if asked specific questions about how their file system works.  I’m not sure what is and isn’t public though, so I’ll save an architectural deep-dive of their technology for a later date.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Andreas Schlapka of Micron Technology&lt;/b&gt; was the next speaker, and his talk was quite a bit more high-level.  Aside from the grand statements about how AI will transform technology though, he did have a couple of nice slides that filled some knowledge gaps in my mind.  For example:&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://1.bp.blogspot.com/-LgZtubR8iyw/W_hUGRxlP2I/AAAAAAABCrs/nwHlQOb8ESwX5xLHUJ0KDFvfhBkpCfnqQCK4BGAYYCw/s1600/IMG_4934.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;300&quot; src=&quot;https://1.bp.blogspot.com/-LgZtubR8iyw/W_hUGRxlP2I/AAAAAAABCrs/nwHlQOb8ESwX5xLHUJ0KDFvfhBkpCfnqQCK4BGAYYCw/s400/IMG_4934.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;Broad strokes highlighting the different computational (and architectural) demands of training and inference workloads&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&lt;br /&gt;Training is what the vast majority of casual AI+HPC pundits are really talking about when extolling the huge compute requirements of deep learning.  Part of that is because GPUs are almost the ideal hardware solution to tackle the mathematics of training (dense matrix-matrix multiplication) and post impressive numbers; the other part is that inference can’t happen without a well-trained model, and models are continually being refined and re-trained.  What I hadn’t fully appreciated is that inference is much more of an interesting computational problem in that it more closely resembles the non-uniform and latency-bound workloads of scientific computing.&lt;br /&gt;&lt;br /&gt;This has interesting implications for memory technology; while HBM2 definitely delivers more bandwidth than DDR, it does this by increasing the channel width to 128 bits and hard-wiring 8 channels into each stack.  The extra bandwidth helps feed GPUs for training, but it’s not doing much for the inference side of AI which, presumably, will become a much more significant fraction of the cycles required overall.  In my mind, increasing the size of SRAM-based caches, scratchpads, and register files are the more obvious way to reduce latency for inference, but we haven’t really seen a lot of fundamentally new ideas on how to effectively do that yet.&lt;br /&gt;&lt;br /&gt;The speaker went on to show the following apples-to-apples system-level reference:&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://4.bp.blogspot.com/-n5FVQeMoPsk/W_hYIrYjrfI/AAAAAAABCr4/YLsuO00mIlYqJYEFoz2wi0vIP29Ag3e_gCK4BGAYYCw/s1600/IMG_4935.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;300&quot; src=&quot;https://4.bp.blogspot.com/-n5FVQeMoPsk/W_hYIrYjrfI/AAAAAAABCr4/YLsuO00mIlYqJYEFoz2wi0vIP29Ag3e_gCK4BGAYYCw/s400/IMG_4935.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;System-level speeds and feeds of the memory products available now or in the near future as presented by Micron&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&lt;br /&gt;It’s not terribly insightful, but it lets you back out the bus width of each memory technology (bandwidth / data rate / device #) and figure out where its bandwidth is coming from:&lt;br /&gt;&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;DDR4 and DDR5 use 64-bit channels and relies on increasing channel-level parallelism to improve bandwidth.  This is now putting them in a place where you wind up having to buy way more capacity than you may want just to get sufficient bandwidth.  This is analogous to where HDDs are in the HPC storage hierarchy today; it’s rapidly becoming uneconomical to rely on DDR for bandwidth.&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;GDDR uses narrower channels (32 bits) but more of them to get better bandwidth.  They also rely on phenomenally high data rates per pin; I don’t really understand how this is possible since they rely on inefficient single-ended signaling.&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;HBM uses both wide (128 bits) and plentiful channels to get its performance; the table is a misleading in this regard since &lt;a href=&quot;https://twitter.com/ernstdj/status/1066178570748420096&quot;&gt;each “device” (HBM stack) contains eight channels&lt;/a&gt;.  &lt;strike&gt;This is fine for feeding highly parallel arithmetic units like vector ALUs, but this offers no benefit to latency-bound workloads that, for example, chase pointers to traverse a graph.&lt;/strike&gt; &lt;span style=&quot;font-size: xx-small;&quot;&gt;(it turns out HBM is just fine for pointer chasing–thanks to &lt;a href=&quot;https://twitter.com/ernstdj&quot;&gt;one of the HPC’s memory-wizards-at-large&lt;/a&gt; for pointing this out to me!)&lt;/span&gt;&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;Micron also made the strange assertion that they are the only company that offers the entire range of memory products.  I guess since Samsung and SK Hynix both opted to skip SC, Micron can say whatever it likes; however, Samsung is currently the only company shipping commercial quantities of HBM, and Hynix’s HBM capability just came online.  As far as I know, Micron has never manufactured a stack of HBM since they spent years promoting the competing-but-now-defunct Hybrid Memory Cube technology.&lt;br /&gt;&lt;br /&gt;&amp;lt;h3 id=&quot;nsfbof&quot;&amp;gt;The NSF Future Directions BOF&amp;lt;/h3&amp;gt;I opted to see what was new with National Science Foundation’s Office of Advanced Cyberinfrastructure (OAC) at their noon BOF.  Despite having left the NSF world when I left San Diego, I still care deeply about NSF computing because they pay for many of the most accessible HPC resources in the US.  I certainly got my start in HPC on the NSF’s dime at SDSC, and I got to see firsthand the huge breadth of impact that SDSC’s XSEDE resources had in enabling smaller research groups at smaller institutions to perform world-class research.  As such, it’s also no surprise that the NSF leads the pack in developing and deploying many of the peripheral technologies that can make HPC accessible such as federated identity, science gateways, and wide-area file systems.&lt;br /&gt;&lt;br /&gt;That all said, actually listening to the NSF HPC strategic vision makes me rather grumpy since the directions of such an important federal office sometimes appear so scattershot.  And judging by the audience questions at the end of the BOF, I am not the only one–Very Important People(tm) in two different national-level HPC consortia asked very pointed questions of Manish Parashar, the NSF OAC director, that highlighted the dichotomy between OAC’s strategic vision and where it was actually putting money.  I really believe in the critical importance of NSF investment in maintaining national cyberinfrastructure which is probably why I keep showing up to these BOFs and do my best to support my colleagues at SDSC and the other XSEDE SPs.&lt;br /&gt;&lt;br /&gt;After sitting through this Future Directions BOF, I could write &lt;a href=&quot;https://glennklockwood.blogspot.com/2015/01/thoughts-on-nsf-future-directions.html&quot;&gt;another updated rant about how I feel about the NSF’s direction in HPC&lt;/a&gt; and get myself in trouble.  Instead, I’ll instead share just a few slides I photographed from afar along with some objective statements and leave it at that.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;The future directions summary slide:&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://4.bp.blogspot.com/-dN7IrgBNqMM/W_hh9cs4GoI/AAAAAAABCsE/z1APs1eMzEEpMU00PdclYjYMqUv8jK8swCK4BGAYYCw/s1600/IMG_4938.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;225&quot; src=&quot;https://4.bp.blogspot.com/-dN7IrgBNqMM/W_hh9cs4GoI/AAAAAAABCsE/z1APs1eMzEEpMU00PdclYjYMqUv8jK8swCK4BGAYYCw/s400/IMG_4938.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;NSF OAC’s future directions&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;Performance, capability computing, and global leadership are not mentioned in the above slides.  Terms like “agility, responsiveness, accessibility”) are often used to describe the cloud.&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;“reduce barriers to CI adoption” indicates that NSF wants to serve more users.  NSF is not increasing investment in capital acquisition (i.e., more or larger HPC systems beyond the status quo of technology refreshes).&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;“Prioritize investments to maximize impact” does not define what impacts are to be maximized.&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;br /&gt;&lt;b&gt;The Frontera slide:&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://4.bp.blogspot.com/-Z6fVo1VzObU/W_hjwTwsYsI/AAAAAAABCsQ/V7shT5OZQsMkmuQiXKL-E0NgFeD5H6UcwCK4BGAYYCw/s1600/IMG_4939.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;225&quot; src=&quot;https://4.bp.blogspot.com/-Z6fVo1VzObU/W_hjwTwsYsI/AAAAAAABCsQ/V7shT5OZQsMkmuQiXKL-E0NgFeD5H6UcwCK4BGAYYCw/s400/IMG_4939.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;NSF’s next leadership-class HPC, Frontera, to be deployed by TACC&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;The award amount was $60M.  The previous Track-1 solicitation that funded Blue Waters was $200M.  Stampede was $30M, and Stampede 2 was another $30M.&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;“leadership-class … for all [science and engineering] applications” either suggests that all science and engineering applications are leadership-capable, or this leadership-class system is not primarily designed to support a leadership computing workload.&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;It is unclear what the significance of the “CPU” qualifier in “largest CPU system” is in the larger context of leadership computing.&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;There is mention of “leadership-class” computing.  There is no mention of exascale computing.  There is nothing that acknowledges leveraging the multi-billion-dollar investment the US has made into the Exascale Computing Project.  An audience member politely asked about this omission.&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;div&amp;gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;b&gt;The Midscale Research Infrastructure slide:&lt;/b&gt;&amp;lt;/div&amp;gt;&lt;/p&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;http://2.bp.blogspot.com/-hn8mLFrBDC4/W_hmRyzMrKI/AAAAAAABCsc/FiAtDwgi1zMvGYb9JnfcuZJRlmUFGQ--QCK4BGAYYCw/s1600/IMG_4940.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;225&quot; src=&quot;https://2.bp.blogspot.com/-hn8mLFrBDC4/W_hmRyzMrKI/AAAAAAABCsc/FiAtDwgi1zMvGYb9JnfcuZJRlmUFGQ--QCK4BGAYYCw/s400/IMG_4940.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Upcoming solicitations for research cyberinfrastructure&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;ul&gt;&lt;li&gt;NSF OAC expects to issue one $6M-$20M solicitation and another $20M-$70M solicitation &quot;soon&quot; to fund HPC systems and the associated infrastructure.&lt;/li&gt;&lt;li&gt;$6M-$20M is on the same order of magnitude as the Track-2 solicitations that funded SDSC's Gordon ($10M) and Comet ($12M).&lt;/li&gt;&lt;li&gt;$20M-$70M is on the same order of magnitude as the Track-2 solicitations that funded TACC's Stampede 1 and 2 ($30M). &amp;nbsp;NSF's next leadership-class investment (Frontera) is $60M.&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;&lt;br /&gt;&amp;lt;h3 id=&quot;mypaper&quot;&amp;gt;My SC Paper&amp;lt;/h3&amp;gt;The next major item on my agenda was presenting my paper, &lt;a href=&quot;https://sc18.supercomputing.org/presentation/?id=pap206&amp;amp;sess=sess186&quot;&gt;A Year in the Life of a Parallel File System&lt;/a&gt;, as the final talk in the final session of the paper track.&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://4.bp.blogspot.com/-CRKvFoWeftI/W_hpfC0RUyI/AAAAAAABCso/FqDL12Rd1l4eL0Rutvcs6xYO7PQqIP9yACK4BGAYYCw/s1600/IMG_4941.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;400&quot; src=&quot;https://4.bp.blogspot.com/-CRKvFoWeftI/W_hpfC0RUyI/AAAAAAABCso/FqDL12Rd1l4eL0Rutvcs6xYO7PQqIP9yACK4BGAYYCw/s400/IMG_4941.jpeg&quot; width=&quot;300&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;My name in lights–or something like that.&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&lt;br /&gt;I was admittedly bummed out when I found out that I was going to be the conference closer since a significant number of SC attendees tend to fly out on Thursday night and, presumably, would not stick around for my presentation.  As a result, I didn’t take preparation for it as seriously in the weeks leading up to SC as I normally would have.  I knew the presentation was a 30-35 minute talk that had to be fit into a 25-minute slot, but I figured I would figure out how to manage that on the night before the talk and mostly wing it.&lt;br /&gt;&lt;br /&gt;What I realized after arriving at SC was that a bunch of people–most of whom weren’t the expected audience of storage researchers–were looking forward to hearing the talk.  This left me scrambling to seriously step up the effort I was going to put into making sure the presentation was well composed despite needing to drop ten minutes of material and fit it into the 25 minutes I was given.  I documented my general approach to crafting presentations in my &lt;a href=&quot;https://glennklockwood.blogspot.com/2014/04/being-successful-researcher.html&quot;&gt;patented Glenn K. Lockwood Five Keys to Being a Successful Researcher (FKBSR) method&lt;/a&gt;, but I’ll mention some of my considerations for the benefit of anyone who is interested in how others approach public speaking.&lt;br /&gt;&amp;lt;ol&amp;gt;&amp;lt;li&amp;gt;I absolutely could not overshoot the timing because some attendees had to leave at 5 PM to catch 7 PM flights.  This meant that it would be better for me to undershoot the time and either draw out the conclusions and acknowledgments slides to finish on time or finish early and leave extra time for questions.&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;The people I met at SC who indicated interest in my talk were storage systems people, not statisticians.  This meant I could probably tone down the statistical rigor in the presentation without offending people’s scientific sensibilities.&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Similarly, because attendees were already familiar with typical HPC I/O systems and the relevant technologies, I could gloss over the experimental setup and description of the different compute and storage systems.&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Given the above considerations, a reasonable approach would be to punt as many non-essential details into the Q&amp;amp;A after the talk and let people try to poke holes in my methods only if they really cared.&amp;lt;/li&amp;gt;&amp;lt;/ol&amp;gt;&amp;lt;div&amp;gt;I also know two things about myself and the way I present:&amp;lt;/div&amp;gt;&lt;/p&gt;
&lt;div&gt;&lt;ol&gt;&lt;li&gt;I can present either at a casual pace where I average ~70 seconds per slide or in turbo mode where I average ~50 seconds per slide. &amp;nbsp;Orating at turbo speed requires a lot more preparation because it requires speaking through slide transitions rather than pausing to reorient after each slide transition.&lt;/li&gt;&lt;li&gt;I get distracted easily, so I would rather have people begin to leave after my monologue ended and Q&amp;amp;A began than have the commotion of people getting up derail the tail end of my presentation.&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;As a result of all these factors, I opted to both cut a lot of details to get the talk down to ~25-30 minutes when presented at a casual pace, then prepare to present in turbo mode just in case the previous speakers went long (I was last of three speakers), there were A/V issues (they were prolific at this SC, especially for Mac users), or there were any audience interruptions.&lt;br /&gt;&lt;br /&gt;I also opted to present from my iPad rather than a full laptop since it did a fine job earlier at both PDSW-DISCS and the IO-500/VI4IO BOF.  In sticking with this decision though, I learned two valuable things during the actual presentation:&lt;br /&gt;&amp;lt;ol&amp;gt;&amp;lt;li&amp;gt;&lt;b&gt;The iOS “do not disturb” mode does not suppress Twitter notifications&lt;/b&gt;.  A couple of people were kind enough to tweet about my presentation as I was giving it, but this meant that my presenter view was blowing up with Twitter noise as I was trying to present!  Fortunately I only needed to look down at my iPad when transitioning between slides so it didn’t derail me.&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&lt;b&gt;There’s no usefully sized timer or clock in PowerPoint for iOS’s presenter view&lt;/b&gt;, and as a result, I had no idea how I was doing on time as I entered the final third of my slides.  This became a distraction because I was fully expecting a five-minute warning from the session moderator at some point and got worried that I wasn’t going to get one.  As such, I didn’t want to slow down the tail of the presentation without knowing how close I was getting to the target.  It turned out that I didn’t get a five-minute warning because I was already concluding at that point.&amp;lt;/li&amp;gt;&amp;lt;/ol&amp;gt;&amp;lt;div&amp;gt;Fortunately the audience was sufficiently engaged to pad out the Q&amp;amp;A period with many of the questions that would’ve been answered by the slides I had dropped.  Afterwards I got feedback that indicated the presentation was noticeably short to the audience (not great) but that the narrative remained understandable to most attendees throughout the entire presentation (good).&amp;lt;/div&amp;gt;&lt;/p&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;As far as the technical content of the presentation though, I won't recap that here--until I write up the high-level presentation as another blog post, you may have to read the paper (or invite me to present it at your institution!).&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h3 id=&quot;perot&quot;&gt;SC Technical Program Reception&lt;/h3&gt;
&lt;div&gt;I've never attended the reception that wraps up the last full day of SC for a variety of reasons, and I was going to skip it again this year to fit some me-time into the otherwise frantic week. &amp;nbsp;However the venue (the Perot Museum) and its close proximity to my hotel lured me out.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-tI-MQMV8TfY/W_iNlk_tK9I/AAAAAAABCs0/7AgSWy_4EkMETBoz4y2vYAmKtPhInqbPQCLcBGAs/s1600/IMG_4944.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;300&quot; src=&quot;https://1.bp.blogspot.com/-tI-MQMV8TfY/W_iNlk_tK9I/AAAAAAABCs0/7AgSWy_4EkMETBoz4y2vYAmKtPhInqbPQCLcBGAs/s400/IMG_4944.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;The entryway to the Perot Museum&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;I am not a &quot;never eat alone&quot; kind of person because I find that my ability to be at the top of my game diminishes without at least some intermittent time to sit back and digest. &amp;nbsp;As such, I approached the reception with very selfish intent: I wanted to see the museum, learn about something that had nothing to do with supercomputing, have a drink and a meal, and then go back to my hotel. &amp;nbsp;So I did just that.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;The dinosaurs seemed like a major feature of the museum:&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-0SzrBWWPg0U/W_iQbFgdV0I/AAAAAAABCtA/pfbaJXLCVKwcY4SBUKh-uO7mCdH8BFkQgCLcBGAs/s1600/IMG_4947.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;300&quot; src=&quot;https://3.bp.blogspot.com/-0SzrBWWPg0U/W_iQbFgdV0I/AAAAAAABCtA/pfbaJXLCVKwcY4SBUKh-uO7mCdH8BFkQgCLcBGAs/s400/IMG_4947.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Rapetosaurus skeleton on display at the Perot Museum&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;The archaeological diversity of the dinosaur room reminded me of &lt;a href=&quot;https://www.royalsaskmuseum.ca/trex&quot;&gt;the dinosaur museum near my wife's hometown&lt;/a&gt; in the Canadian prairies, but the exhibit seemed to be largely reproduction fossils that blended science with entertainment.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;More impressive to me was the extensive mineral collection:&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://4.bp.blogspot.com/-gXjf9Sexq68/W_iRxM_ABNI/AAAAAAABCtM/EA0wfJOoOvgqTwtOwWxt5Y42vFFM-1hvQCLcBGAs/s1600/IMG_4949.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;400&quot; src=&quot;https://4.bp.blogspot.com/-gXjf9Sexq68/W_iRxM_ABNI/AAAAAAABCtM/EA0wfJOoOvgqTwtOwWxt5Y42vFFM-1hvQCLcBGAs/s400/IMG_4949.jpeg&quot; width=&quot;300&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;I'm a sucker for quartz. &amp;nbsp;I did my PhD research on silicates.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;Not only were the minerals on display of remarkable quality, but many of them were found in Texas. &amp;nbsp;In fact, the museum overall had a remarkably Texas-focused set of exhibits which really impressed me. &amp;nbsp;The most interesting exhibit that caught my attention was a mini-documentary on the geologic history of Texas that explained how plate tectonics and hundreds of millions of years resulted in the world-famous oil and gas reserves throughout the state.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;Having learned something and enjoyed some delightful food at the museum, I then called it quits and cashed out.&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&amp;lt;h2 id=&quot;friday&quot;&amp;gt;Friday, November 16&amp;lt;/h2&amp;gt;&amp;lt;div&amp;gt;The last day of SC is always a bit odd because the expo has already wrapped up, most of the vendors and casual attendees have gone home, and the conference is much more quiet and focused.  My day started with a surreal shuttle ride to the conference center in what appeared to be a 90’s-era party bus:&amp;lt;/div&amp;gt;&lt;/p&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-gVuBN4kPkJk/W_iTzpl1fgI/AAAAAAABCtY/EYRjvFdgORAW8buCAMOEPaOgajKt4yTOQCLcBGAs/s1600/IMG_4956.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;300&quot; src=&quot;https://3.bp.blogspot.com/-gVuBN4kPkJk/W_iTzpl1fgI/AAAAAAABCtY/EYRjvFdgORAW8buCAMOEPaOgajKt4yTOQCLcBGAs/s400/IMG_4956.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Conference shuttle, complete with taped-together audio system, faux leather sofa, and a door that had to be poked with a broom stick to open.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;Only six concurrent half-day workshops and a panel were on the agenda:&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://2.bp.blogspot.com/-CcpDEs4gg0g/W_iUdw0L9oI/AAAAAAABCtg/aifOyN4nBx8Lc4eEa5Gs7v19EB6hsvqTgCLcBGAs/s1600/IMG_4957.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;320&quot; src=&quot;https://2.bp.blogspot.com/-CcpDEs4gg0g/W_iUdw0L9oI/AAAAAAABCtg/aifOyN4nBx8Lc4eEa5Gs7v19EB6hsvqTgCLcBGAs/s320/IMG_4957.jpeg&quot; width=&quot;239&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;The entire Friday agenda fit on a single screen&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;I stuck my head into the&amp;nbsp;&lt;a href=&quot;https://sc18.supercomputing.org/session/?sess=sess145&quot;&gt;P3HPC workshop&lt;/a&gt;'s first panel discussion to catch the age-old but ever-lively argument over someone's proposed definition of performance portability and productivity either being too broad or too narrow. &amp;nbsp;I/O performance portability generally does not have a place in these sorts of conversations (which I don't fault--algorithmic complexity in I/O is usually hidden from user applications) so I attended only as an interested observer and wasn't as fastidious about taking notes as I was earlier in the week.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;At 10:30 AM I headed over to the &lt;a href=&quot;https://sc18.supercomputing.org/presentation/?id=pan105&amp;amp;sess=sess306&quot;&gt;Convergence between HPC and Big Data: The Day After Tomorrow&lt;/a&gt; panel discussion which had a star-studded speaker lineup. &amp;nbsp;&lt;a href=&quot;http://www.nersc.gov/about/nersc-staff/center-leadership/katie-antypas/&quot;&gt;NERSC's Katie Antypas&lt;/a&gt; gave a great overview of the NERSC-9/Perlmutter architecture which fit the panel topic uncannily well since it is a system design from the ground up to meet the needs of both traditional HPC and large-scale data analysis.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-D7BDhWU4Ww8/W_iXlgPQFUI/AAAAAAABCts/ZUG5vfMr7TIVeDqPmijx8iZBE_YjrPJWwCLcBGAs/s1600/IMG_4959.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;225&quot; src=&quot;https://1.bp.blogspot.com/-D7BDhWU4Ww8/W_iXlgPQFUI/AAAAAAABCts/ZUG5vfMr7TIVeDqPmijx8iZBE_YjrPJWwCLcBGAs/s400/IMG_4959.jpeg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;The NERSC-9 Project Director describing how the Perlmutter system embodies the convergence of HPC and Big Data in front of a remarkably big crowd in the final session of SC.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;Unfortunately I had to duck out shortly after she spoke to get to my last meeting of the week with an old colleague for whom I always make time at SC. &amp;nbsp;Incidentally, some of the most valuable time you can spend at SC is talking to &lt;a href=&quot;https://www.nag.com/&quot;&gt;industry&lt;/a&gt; &lt;a href=&quot;https://bioteam.net/&quot;&gt;consultants&lt;/a&gt;. &amp;nbsp;Not unlike getting to know members of the trade press, good consultants have exposure to a tremendous breadth of problem and solution spaces. &amp;nbsp;They can give you all manner of interesting insights into different vendors, industry verticals, and market trends in an otherwise brief conversation.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;After my final meeting was cut short by my colleague's need to run to the airport, I had a quick bite with another Friday holdout then made my own way to the airport to catch up on a week's worth of e-mails. &amp;nbsp;The flight back to Oakland was one of the rare occasions where I was just too worn out to try to catch up on some delinquent report writing and just watched three hours of Dark Tourist on Netflix.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h2 id=&quot;after-conf&quot;&gt;After the Conference&lt;/h2&gt;
&lt;div&gt;It was technically Saturday by the time I finally got home, but the family was happy to see me (and the swag I had in tow):&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-u5u1MUSi0_I/W_iln_CwlEI/AAAAAAABCt8/W-ssev6VpyIDSMG8XQsHcNM84xTGqDJCgCEwYBhgL/s1600/IMG_4967.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;240&quot; src=&quot;https://1.bp.blogspot.com/-u5u1MUSi0_I/W_iln_CwlEI/AAAAAAABCt8/W-ssev6VpyIDSMG8XQsHcNM84xTGqDJCgCEwYBhgL/s320/IMG_4967.jpeg&quot; width=&quot;320&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;George fully appreciating the giant pile of conference swag with which I came home&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;This was definitely the busiest SC of my career, but in many ways it was also the most productive. &amp;nbsp;I owe sincere thanks to everyone in the HPC community who made it such a worthwhile conference to attend--vendors, presenters, old colleagues, and even the new colleagues who occasionally just wanted to introduce themselves and express that they enjoy reading the nonsense I post on Twitter. &amp;nbsp;I always leave SC more amazed and humbled by all the bright minds with whom I connect, and I hope that I am doing my part to pay that experience forward for others now and in the SC conferences to come.&lt;/div&gt;
&lt;p&gt;&lt;span&gt;&lt;!--more--&gt;&lt;/span&gt;&lt;span&gt;&lt;!--more--&gt;&lt;/span&gt;&lt;span&gt;&lt;!--more--&gt;&lt;/span&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>A Killer Feature for Scientific Development Frameworks- An Incremental Path To Maturity</title>
   <link href="https://hpc.social/2018/a-killer-feature-for-scientific-development-frameworks-an-incremental-path-to-maturity/"/>
   <updated>2018-07-16T01:00:00-06:00</updated>
   <id>https://hpc.social/2018/a-killer-feature-for-scientific-development-frameworks-an-incremental-path-to-maturity</id>
   <content type="html">&lt;p&gt;( &lt;strong&gt;Note&lt;/strong&gt;: This is a bit of a work in progress; even more so than usual, comments/criticisms/additions welcome )&lt;/p&gt;

&lt;h3 id=&quot;the-stages-of-research-software-development&quot;&gt;The Stages of Research Software Development&lt;/h3&gt;

&lt;p&gt;Research software development covers a lot of ground — it’s the development of software for research,
and research is a broad endeavour that covers a lot of use cases.&lt;/p&gt;

&lt;p&gt;The part of research software development that I find the most interesting is the part that 
&lt;em&gt;is a research effort itself&lt;/em&gt;; the creation of new simulation methods, new data analysis techniques,
new ways to combining different sorts of approaches.  Like any new tools, this work
can enable people to ask entirely new questions, or answer old questions in new ways, pushing
scholarship forward along previously unexplored paths.&lt;/p&gt;

&lt;p&gt;But for new methods to live up to their potential and have that impact, they have to be developed
and disseminated.  As a community, we’re still developing the training and tool chains that 
make this routine; without them, there are still too many bottlenecks in the method development
pipeline that mean good ideas for new tools get delayed, sometimes indefinitely, before adoption.&lt;/p&gt;

&lt;p&gt;Computational tools for science and scholarship go through stages of development like any experimental technique:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Will this even work?&lt;/strong&gt;  Testing the core ideas out, usually interactively&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Will this answer my question?&lt;/strong&gt;  Developing a very early prototype on your own data set/conditions&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Is this an interesting question to others?&lt;/strong&gt;  Sharing a more robust prototype with friendly collaborators who think it might be useful&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Becoming Research Infrastructure&lt;/strong&gt; The robust, usable, automatable tool becomes something strangers start to use routinely in their own research&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These steps can be thought of as a sort of an internal-to-the-research-endeavour version of 
the &lt;a href=&quot;https://en.wikipedia.org/wiki/Technology_readiness_level&quot;&gt;Technology Readiness Levels&lt;/a&gt; 
that are used to describe the maturity of technologies and tools, now often used when talking
about commercialization.&lt;/p&gt;

&lt;p&gt;Not every idea has to go through all four stages to be successful; sometimes a tool will be a ‘one-off’
or nearly so, used for one or two projects and that’s it.  This isn’t at all a bad thing, 
if it served its one purpose well.&lt;/p&gt;

&lt;p&gt;But each transition between stages represents a potential barrier for ideas becoming new tools,
a jump in level of development skills and effort required.  Every tool that stalls at between 
stages solely because there isn’t training or tooling to allow incremental progress along 
the pipeline is a tool that is unnecessarily lost to researchers who might have made use of it.&lt;/p&gt;

&lt;h3 id=&quot;training-research-software-developers-to-tackle-all-stages&quot;&gt;Training Research Software Developers To Tackle all Stages&lt;/h3&gt;

&lt;p&gt;The set of techniques that we mean when we talk about “Software
Engineering” is most useful at step 4 — these techniques
largely assume that there already exists a well-posed problem and
an understood, implementable solution.  I’ve argued in the past
that it’s not only unnecessary but actually irresponsible to build
“well-engineered” software for tools at stage 1 or 2,
where the answers will often turn out to be “No”.&lt;/p&gt;

&lt;p&gt;It was understood fairly early that the lifecycle for scientific
projects differed a great deal from scientific software development.
Realizing that something correspondingly different training was needed, in the late 90s 
&lt;a href=&quot;https://software-carpentry.org&quot;&gt;Software Carpentry&lt;/a&gt;, and later &lt;a href=&quot;https://carpentries.org&quot;&gt;The Carpentries&lt;/a&gt;,
started teaching more research trainees enough modern programming skills to ask their own 
questions — to navigate the biggest transition from nothing to stage 1, when existing tools
won’t work for their questions; and to get started on the journey of the next transition, to
stage 2, building an entire early prototype.  That training may or may not get students
all the way to the end of stage 2, with issues like speed or advanced functionality remaining,
but those issues will vary from research project to research project, and the goal is to
get the students to the point where they can learn additional material themselves.&lt;/p&gt;

&lt;p&gt;There still isn’t a lot of training for researchers to make the next big jump, from
prototype-for-self to tool-some-others-can-use.  However, authors are beginning to write
resources for students wanting to learn how to proceed&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a class=&quot;footnote&quot; href=&quot;https://www.dursi.ca/feed.xml#fn:1&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;sup&gt;,&lt;/sup&gt;&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a class=&quot;footnote&quot; href=&quot;https://www.dursi.ca/feed.xml#fn:2&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;sup&gt;,&lt;/sup&gt;&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a class=&quot;footnote&quot; href=&quot;https://www.dursi.ca/feed.xml#fn:3&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;sup&gt;,&lt;/sup&gt;&lt;sup id=&quot;fnref:4&quot;&gt;&lt;a class=&quot;footnote&quot; href=&quot;https://www.dursi.ca/feed.xml#fn:4&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;The second-biggest transition in that list, that from 3 to 4, is the one I worry the least
about.  It’s at that stage that existing software engineering teaching, tooling,
and resources become the most helpful.  And while the effort to learn those techniques
and apply them can be significant, at this point the ideas and the tool have proven themselves
useful enough that it is much easier to find the time, people, and resources to complete a 
“research infrastructure”-grade implementation.&lt;/p&gt;

&lt;p&gt;Of course, once the set of ideas is implemented as research infrastructure, it’s
much harder for most practicing researchers to get under the hood and start 
tinkering with by making changes or incorporating additional ideas.  And so the cycle starts again.&lt;/p&gt;

&lt;h3 id=&quot;the-best-scientific-development-frameworks-will-allow-an-incremental-path-towards-maturity&quot;&gt;The Best Scientific Development Frameworks will Allow an Incremental Path Towards Maturity&lt;/h3&gt;

&lt;p&gt;While the research computing community has made great progress in creating development training
specific to their needs, there’s been much less success with programming languages, tools, or
frameworks which reflect the path of research programs.&lt;/p&gt;

&lt;p&gt;Arguably the best programming language for science, and certainly one of the most successful, 
has been a general purpose programming language, Python.  I think the reasons for this include
the relatively smooth path scientific software development can take towards maturity in the
Python ecosystem:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;One can easily and rapidly test out ideas at the REPL and in a notebook. (Stage 1)&lt;/li&gt;
  &lt;li&gt;The large standard library and even larger ecosystem lets you quickly implement a lot of functionality (Stages 1/2)&lt;/li&gt;
  &lt;li&gt;Great tooling exists, including &lt;a href=&quot;https://code.visualstudio.com&quot;&gt;VSCode&lt;/a&gt; which makes much IDE functionality available for free (Stages 2/3)&lt;/li&gt;
  &lt;li&gt;Compared to languages more commonly used earlier like C and FORTRAN, the exception system lets
you implement a number of things and still understand what’s happening before you have to start
implementing boilerplate error handling, making it something that can be added incrementally at later stages. (Stages 2/3/4)&lt;/li&gt;
  &lt;li&gt;Tools like &lt;a href=&quot;http://numba.pydata.org&quot;&gt;Numba&lt;/a&gt;, &lt;a href=&quot;https://www.pypy.org&quot;&gt;PyPy&lt;/a&gt;, or &lt;a href=&quot;http://cython.org&quot;&gt;Cython&lt;/a&gt; allow 
substantial but incremental performance improvement for many kinds of computation (Stages 2/3/4)&lt;/li&gt;
  &lt;li&gt;Tools like &lt;a href=&quot;https://www.pypy.org&quot;&gt;Dask&lt;/a&gt; offer an incremental path to scale (Stages 3/4)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It’s useful to consider incrementalism-as-a-feature in the context
of existing programming environments, each of which have some ideas useful to
scientific computing.  &lt;a href=&quot;http://www.ada2012.org&quot;&gt;Ada&lt;/a&gt;, a highish-level programming
language with an emphasis on correctness, has a reputation of being
a somewhat authoritarian programming environment; however, many of its correctness
features are things you can incrementally add on (things like pre- and post-conditions).
On the other hand, &lt;a href=&quot;https://www.rust-lang.org/en-US/&quot;&gt;Rust&lt;/a&gt;, a lower level
language aimed at systems programming where reliability and security in an environment
where memory bugs continue to cause problems, enables very low-level concurrency
features but one very quickly has to wrestle with Rust’s powerful 
&lt;a href=&quot;https://doc.rust-lang.org/1.8.0/book/references-and-borrowing.html&quot;&gt;borrow checker&lt;/a&gt;;
adding non-trivial sharing semantics to code in Rust results in a
dramatically non-incremental development effort, which is arguably
the right choice for a low-level systems programming language.&lt;/p&gt;

&lt;p&gt;While Python and other general programming languages have flourished,
other frameworks, aimed more directly at solving needs particular
to research or branches of research, have struggled.  Much of this,
of course, has to do with the simple math of adoption; but most
have not made much effort to make tools which ease the development
of increasingly mature research software.&lt;/p&gt;

&lt;p&gt;To their credit, the &lt;a href=&quot;https://julialang.org&quot;&gt;Julia&lt;/a&gt; community has
come closest, but they are focussed on a narrow piece of the issue;
the need for a framework for incremental adoption becomes “one
language for everything” with tools like Numba or PyPy as,
essentially, cheating; and the only maturity metric focused on is
performance.  It’s better to have fast code than not, of course, but it is by no means
the primary development problem of most researchers.&lt;/p&gt;

&lt;p&gt;Having said that, most other programming languages aimed for
scientific communities have not made nearly as much progress on key
usability issues for researchers.  I’ll certainly be watching the
progress of their 1.x releases with some interest.&lt;/p&gt;

&lt;h3 id=&quot;the-developing-field-of-research-software-engineering&quot;&gt;The Developing Field of Research Software Engineering&lt;/h3&gt;

&lt;p&gt;It’s been fascinating to watch from the sidelines over the past two decades
as research software engineering and RSE as a profession has gone from
basically nothing to &lt;a href=&quot;https://rse.ac.uk/conf2018/&quot;&gt;conferences&lt;/a&gt;, 
&lt;a href=&quot;https://carpentries.org&quot;&gt;organizations&lt;/a&gt;, and research.  I’m enormously
heartened by the fact that training now exists to tackle the specific 
challenges of developing software that itself is research into methods
development.&lt;/p&gt;

&lt;p&gt;I’m still somewhat pessimistic, however, on the state of development frameworks
for research computing.  My current work with web services development
just drives home the point of how scarce the tooling is for building
research software.&lt;/p&gt;

&lt;p&gt;The history of research computing since Fortran’s dominance has
been that research software engineering has grafted itself on to
a set of existing general purpose programming languages like C++
or Python, each of which has advantages but also gaps for research
computing.  There are exciting experiments here and there with new
languages, but none are yet particularly compelling.&lt;/p&gt;

&lt;p&gt;As Data Science/Data Engineering becomes more and more common in
commercial enterprises and as a computing use case, we may yet end
up finding frameworks which, if not actually designed for science,
are made for similar purposes.  The good news is that people problems
are hard, while technology problems are (comparatively) tractable.
If one or more promising development frameworks appear in the coming
years, ones that allow a path from “basic methods science”
to “methods commercialization”, other people’s hard
work has led to a generation of research software developers who are ready
to take the plunge.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005412&quot;&gt;&lt;em&gt;Ten simple rules for making research software more robust&lt;/em&gt;, Taschuk &amp;amp; Wilson&lt;/a&gt; &lt;a class=&quot;reversefootnote&quot; href=&quot;https://www.dursi.ca/feed.xml#fnref:1&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;

    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005265&quot;&gt;&lt;em&gt;Ten Simple Rules for Developing Usable Software in Computational Biology&lt;/em&gt;, List, Ebert, &amp;amp; Albrecht&lt;/a&gt; &lt;a class=&quot;reversefootnote&quot; href=&quot;https://www.dursi.ca/feed.xml#fnref:2&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;

    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://katyhuff.github.io/python-testing&quot;&gt;&lt;em&gt;Testing and Continuous Integration with Python&lt;/em&gt;, Huff&lt;/a&gt; &lt;a class=&quot;reversefootnote&quot; href=&quot;https://www.dursi.ca/feed.xml#fnref:3&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;

    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1609.00037.pdf&quot;&gt;&lt;em&gt;Good Enough Practices in Scientific Computing&lt;/em&gt;, Wilson et al.&lt;/a&gt; &lt;a class=&quot;reversefootnote&quot; href=&quot;https://www.dursi.ca/feed.xml#fnref:4&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;

    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Are FPGAs the answer to HPC's woes?</title>
   <link href="https://hpc.social/2018/are-fpgas-the-answer-to-hpc-s-woes/"/>
   <updated>2018-02-24T09:21:00-07:00</updated>
   <id>https://hpc.social/2018/are-fpgas-the-answer-to-hpc-s-woes-</id>
   <content type="html">&lt;h2&gt;Executive Summary&lt;/h2&gt;
&lt;p&gt;Not yet.  I’ll demonstrate why no domain scientist would ever want to program in Verilog, then highlight a few promising directions of development that are addressing this fact.&lt;br /&gt;&lt;br /&gt;The usual disclaimer also applies: the opinions and conjectures expressed below are mine alone and not those of my employer.  Also I am not a computer scientist, so I probably don’t know what I’m talking about.  And even if it seems like I do, remember that I am a storage architect who is wholly unqualified to speak on applications and processor performance.&lt;br /&gt;&lt;br /&gt;&amp;lt;h2&amp;gt;Premise&amp;lt;/h2&amp;gt;We’re now in an age where CPU cores aren’t getting any faster, and the difficulties of shrinking processes below 10 nm means we can’t really pack any more CPU cores on a die.  Where’s performance going to come from if we ever want to get to exascale and beyond?&lt;br /&gt;&lt;br /&gt;Some vendors are betting on &lt;b&gt;larger and larger vectors&lt;/b&gt;–&lt;a href=&quot;https://community.arm.com/processors/b/blog/posts/technology-update-the-scalable-vector-extension-sve-for-the-armv8-a-architecture&quot;&gt;ARM (with its Scalable Vector Extensions)&lt;/a&gt; and &lt;a href=&quot;http://www.nec.com/en/global/solutions/hpc/sx/vector_engine.html&quot;&gt;NEC (with its Aurora coprocessors)&lt;/a&gt; are going down this path.  However, algorithms that aren’t predominantly dense linear algebra will need very efficient scatter and gather operations that can pack vector registers quickly enough to make doing a single vector operation worthwhile.  For example, gathering eight 64-bit values from different parts of memory to issue an eight-wide (512-bit) vector multiply requires pulling eight different cache lines–that’s moving 4096 bits of memory for what amounts to 512 bits of computation.  In order to continue scaling vectors out, CPUs will have to rethink how their vector units interact with memory.  This means either (a) getting a lot more memory bandwidth to support these low &lt;a href=&quot;http://www.nersc.gov/users/application-performance/measuring-arithmetic-intensity/&quot;&gt;flops-per-byte ratios&lt;/a&gt;, or (b) pack vectors closer to the memory so that pre-packed vectors can be fetched through the existing memory channels.&lt;br /&gt;&lt;br /&gt;Another option to consider are &lt;b&gt;GPUs&lt;/b&gt;, which work around the vector packing issue by implementing a massive numbers of registers and giant crossbars to plumb those bytes into arithmetic units.  Even then, though, relying on a crossbar to connect compute and data is difficult to continue scaling; the interconnect industry gave up on this long ago, which is why today’s clusters now connect hundreds or thousands of crossbars into larger fat trees, hypercubes, and dragonflies.  GPUs are still using larger and larger crossbars–NVIDIA’s V100 GPU is one of the &lt;a href=&quot;https://arstechnica.com/gadgets/2017/05/nvidia-tesla-v100-gpu-details/&quot;&gt;physically largest single-die chips ever made&lt;/a&gt;–but there’s an economic limit to how large a die can be.&lt;br /&gt;&lt;br /&gt;This bleak outlook has begun to drive HPC designers towards thinking about smarter ways to use silicon.  Rather than build a general-purpose processor that can do all multiplication and addition operations at a constant rate, the notion is to bring hardware design closer to the algorithms being implemented.  This isn’t a new idea (for example, &lt;a href=&quot;http://dx.doi.org/10.1098/rsta.2013.0387&quot;&gt;RIKEN’s MDGRAPE&lt;/a&gt; and &lt;a href=&quot;http://dx.doi.org/10.1109/SC.2014.9&quot;&gt;DESRES’s Anton&lt;/a&gt; are famous examples of purpose-built chips for specific scientific application areas), but this approach historically has been very expensive relative to just using general-purpose processor parts.  Only now are we at a place where special-purpose hardware may be the only way to sustain HPC’s performance trajectory.&lt;br /&gt;&lt;br /&gt;Given the diversity of applications that run on the modern supercomputer though, expensive and custom chips that only solve one problem aren’t very appetizing.  A close compromise are FPGAs though, and there has been a growing buzz surrounding the viability of relying on FPGAs in mainstream HPC workloads.&lt;br /&gt;&lt;br /&gt;Many of us non-computer scientists in the HPC business only have a vague and qualitative notion of how FPGAs can realistically be used to carry out computations, though.  Since there is growing excitement around FPGAs for HPC as exascale approaches though, I set out to get my hands dirty and figure out how they might fit in the larger HPC ecosystem.&lt;br /&gt;&lt;br /&gt;&amp;lt;h2&amp;gt;Crash course in Verilog&amp;lt;/h2&amp;gt;Verilog can be very difficult to grasp for people who already know how to program languages like C or Fortran (like me!).  On the one hand, it looks a bit like C in that has variables to which values can be assigned, if/then/else controls, for loops, and so on.  However these similarities are deceptive because Verilog does &lt;i&gt;not&lt;/i&gt; execute like C; whereas a C program executes code line by line, one statement after the other, Verilog sort of execute all of the lines at the same time, all the time.&lt;br /&gt;&lt;br /&gt;A C program to turn an LED on and off repeatedly might look like:&lt;br /&gt;&lt;br /&gt;&amp;lt;div&amp;gt;&amp;lt;/div&amp;gt;
where the LED is turned on, &lt;i&gt;then&lt;/i&gt; the LED is turned off, &lt;i&gt;then&lt;/i&gt; we repeat.&lt;br /&gt;&lt;br /&gt;In Verilog, you really have to describe &lt;i&gt;what&lt;/i&gt; components your program will have and &lt;i&gt;how&lt;/i&gt; they are connected.  In the most basic way, the code to blink an LED in Verilog would look more like&lt;br /&gt;&lt;br /&gt;&amp;lt;div&amp;gt;&amp;lt;/div&amp;gt;
&lt;br /&gt;Whereas C is a &lt;i&gt;procedural&lt;/i&gt; language in that you describe a procedure for solving a problem, Verilog is more like a &lt;i&gt;declarative&lt;/i&gt; language in that you describe how widgets can be arranged to solve the problem.&lt;br /&gt;&lt;br /&gt;This can make tasks that are simple to accomplish in C comparatively awkward in Verilog.  Take our LED blinker C code above as an example; if you want to slow down the blinking frequency, you can do something like&lt;br /&gt;&lt;br /&gt;&amp;lt;div&amp;gt;&amp;lt;/div&amp;gt;
&lt;br /&gt;Because Verilog is not procedural, there is no simple way to say “wait a second &lt;i&gt;after&lt;/i&gt; you turn on the LED before doing something else.”  Instead, you have to rely on knowing how much time passes between consecutive clock signals (&lt;code&gt;clk&lt;/code&gt; incrementing).&lt;br /&gt;&lt;br /&gt;For example, the DE10-Nano has a 50 MHz clock generator, so every 1/(50 MHz) (20 nanoseconds), and everything time-based has to be derived from this fundamental clock timer.  The following Verilog statement:&lt;br /&gt;&lt;br /&gt;&amp;lt;div&amp;gt;&amp;lt;/div&amp;gt;
&lt;br /&gt;indicates that every 20 ns, increment the &lt;code&gt;cnt&lt;/code&gt; register (variable) by one.  To make the LED wait for one second after the LED is turned on, we need to figure out a way to do nothing for 50,000,000 clock cycles (1 second / 20 nanoseconds).  The canonical way to do this is to&lt;br /&gt;&amp;lt;ol&amp;gt;&amp;lt;li&amp;gt;create a big register that can store a number up to 50 million&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;express that this register should be incremented by 1 on every clock cycle&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;create a logic block that turns on the LED when our register is larger than 50 million&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;rely on the register eventually overflowing to go back to zero&amp;lt;/li&amp;gt;&amp;lt;/ol&amp;gt;If we make &lt;code&gt;cnt&lt;/code&gt; a 26-bit register, it can count up to 67,108,864 different numbers and our Verilog can look something like&lt;br /&gt;&lt;br /&gt;&amp;lt;div&amp;gt;&amp;lt;/div&amp;gt;
&lt;br /&gt;However, we are still left with two problems:&lt;br /&gt;&amp;lt;ol&amp;gt;&amp;lt;li&amp;gt;&lt;code&gt;cnt&lt;/code&gt; will overflow back to zero once &lt;code&gt;cnt&lt;/code&gt; surpasses 2&lt;sup&gt;26&lt;/sup&gt; - 1&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;We don’t yet know how to express how the LED is connected to our FPGA and should be controlled by our circuit&amp;lt;/li&amp;gt;&amp;lt;/ol&amp;gt;Problem #1 (&lt;code&gt;cnt&lt;/code&gt; overflows) means that the LED will stay &lt;i&gt;on&lt;/i&gt; for exactly 50,000,000 clock cycles (1 second), but it’ll turn &lt;i&gt;off&lt;/i&gt; for only 2&lt;sup&gt;26&lt;/sup&gt; - 1 - 50,000,000 cycles (17,108,860 cycles, or 0.34 seconds).  Not exactly the one second on, one second off that our C code does.&lt;br /&gt;&lt;br /&gt;Problem #2 is solved by understanding the following:&lt;br /&gt;&lt;br /&gt;&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;our LED is external to the FPGA, so it will be at the end of an &lt;i&gt;output wire&lt;/i&gt;&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;the other end of that &lt;i&gt;output wire&lt;/i&gt; must be connected to something inside our circuit–a register, another wire, or something else&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;br /&gt;The conceptually simplest solution to this problem is to create another register (variable), this time only one bit wide, in which our LED state will be stored.  We can then change the state of this register in our &lt;code&gt;if (cnt &amp;gt; 5000000)&lt;/code&gt; block and wire that register to our external LED:&lt;br /&gt;&lt;br /&gt;&amp;lt;div&amp;gt;&amp;lt;/div&amp;gt;
&lt;br /&gt;Note that our &lt;code&gt;assign&lt;/code&gt; statement is outside of our &lt;code&gt;always @(posedge clk)&lt;/code&gt; block because this assignment–connecting our &lt;code&gt;led&lt;/code&gt; output wire to our &lt;code&gt;led_state&lt;/code&gt; register–is a persistent declaration, &lt;i&gt;not&lt;/i&gt; the assignment of a particular value.  We are saying “whatever value is stored in &lt;code&gt;led_state&lt;/code&gt; should always be carried to whatever is on the other end of the &lt;code&gt;led&lt;/code&gt; wire.”  Whenever &lt;code&gt;led_state&lt;/code&gt; changes, &lt;code&gt;led&lt;/code&gt; will simultaneously change as a result.&lt;br /&gt;&lt;br /&gt;With this knowledge, we can actually solve Problem #1 now by&lt;br /&gt;&amp;lt;ol&amp;gt;&amp;lt;li&amp;gt;only counting up to 50 million and not relying on overflow of &lt;code&gt;cnt&lt;/code&gt; to turn the LED on or off, and&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;overflowing the 1-bit &lt;code&gt;led_state&lt;/code&gt; register every 50 million clock cycles&amp;lt;/li&amp;gt;&amp;lt;/ol&amp;gt;Our Verilog module would look like&lt;br /&gt;&lt;br /&gt;&amp;lt;div&amp;gt;&amp;lt;/div&amp;gt;
&lt;br /&gt;and we accomplish the “hello world” of circuit design:&lt;br /&gt;&lt;br /&gt;&amp;lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&amp;gt;&lt;a href=&quot;https://4.bp.blogspot.com/-HwPyRg8Kc6U/Wm0vBaXxf5I/AAAAAAAA0Ho/QKNf3Kn4EqcqdPSl3uUxX8h_fAB9oxSeACLcBGAs/s1600/fpga-blink-1sec.gif&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;239&quot; src=&quot;https://4.bp.blogspot.com/-HwPyRg8Kc6U/Wm0vBaXxf5I/AAAAAAAA0Ho/QKNf3Kn4EqcqdPSl3uUxX8h_fAB9oxSeACLcBGAs/s320/fpga-blink-1sec.gif&quot; width=&quot;320&quot; /&gt;&lt;/a&gt;&amp;lt;/div&amp;gt;
&lt;br /&gt;This Verilog is actually still missing a number of additional pieces and makes very inefficient use of the FPGA’s hardware resources.  However, it shows how awkward it can be to express a simple, four-line procedural program using a hardware description language like Verilog.&lt;br /&gt;&lt;br /&gt;&amp;lt;h2&amp;gt;So why bother with FPGAs at all?&amp;lt;/h2&amp;gt;It should be clear that solving a scientific problem using a procedural language like C is generally more straightforward than with a declarative language like Verilog.  That ease of programming is made possible by a ton of hardware logic that isn’t always used, though.&lt;br /&gt;&lt;br /&gt;Consider our blinking LED example; because the C program is procedural, it takes one CPU thread to walk through the code in our program.  Assuming we’re using a 64-core computer, that means we can only blink up to 64 LEDs at once.  On the other hand, our Verilog module consumes a tiny number of the programmable logic blocks on an FPGA.  When compiled for a $100 hobbyist-grade DE10-Nano FPGA system, it uses only 21 of 41,910 programmable blocks, meaning it can control almost 2,000 LEDs concurrently**.  A high-end FPGA would easily support tens of thousands. &lt;br /&gt;&lt;br /&gt;&amp;lt;table cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;display: block; float: right; margin-left: 1em; text-align: right;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;https://4.bp.blogspot.com/-dv03oqFBdTs/WpEAAg9THGI/AAAAAAAA0Tc/LV6L-sK7S4k3jK4bY8SHja0NynW518QqwCLcBGAs/s1600/cm200-6.jpg&quot; style=&quot;clear: right; margin-bottom: 1em; margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;261&quot; src=&quot;https://4.bp.blogspot.com/-dv03oqFBdTs/WpEAAg9THGI/AAAAAAAA0Tc/LV6L-sK7S4k3jK4bY8SHja0NynW518QqwCLcBGAs/s320/cm200-6.jpg&quot; width=&quot;320&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;The CM2 illuminated an LED whenever an operation was in flight. Blinking the LED in Verilog is easy.  Reproducing the CM2 microarchitecture is a different story.  Image credit to &lt;a href=&quot;http://www.corestore.org/cm200.htm&quot;&gt;Corestore&lt;/a&gt;.&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;Of course, blinking LEDs haven’t been relevant to HPC since the days of Connection Machines, but if you were to replace LED-blinking logic with floating point arithmetic units, the same conclusions apply.  In principle, a single FPGA can process a huge number of FLOPS every cycle by giving up its ability to perform many of the tasks that a more general-purpose CPU would be able to do.  And because FPGAs are reprogrammable, they can be quickly configured to have an optimal mix of special-purpose parallel ALUs and general purpose capabilities to suit different application requirements.&lt;br /&gt;&lt;br /&gt;However, the fact that the fantastic potential of FPGAs hasn’t materialized into widespread adoption is a testament to how difficult it is to bridge the wide chasm between understanding how to solve a physics problem and understanding how to design a microarchitecture.&lt;br /&gt;&lt;br /&gt;&amp;lt;h2&amp;gt;Where FPGAs fit in HPC today&amp;lt;/h2&amp;gt;To date, a few scientific domains have had success in using FPGAs.  For example,&lt;br /&gt;&lt;br /&gt;&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;Experimental instruments that generate data commonly deploy FPGAs close to their detectors to perform very repetitive, relatively simple data filtering or manipulation at extremely high rates.  For example, &lt;a href=&quot;https://blogs.swarthmore.edu/Illumina+GAIIx+Teardown/?p=125#div-comment-191&quot;&gt;Illumina HiSeq DNA sequencers incorporate both Altera and Xilinx FPGAs&lt;/a&gt; to assist with the high-throughput image processing, and &lt;a href=&quot;https://www.nextplatform.com/2016/01/05/an-expanding-role-for-fpgas-in-cerns-future/&quot;&gt;high-energy physics experiments routinely use FPGAs&lt;/a&gt; for signal processing.&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Closer to the HPC side, &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3186629/&quot;&gt;Convey implemented loadable FPGA blocks to perform many algorithms common to bioinformatics&lt;/a&gt;.  For example, they provided an FPGA-accelerated Smith-Waterman algorithm; this algorithm is used to align short DNA sequences along a reference genome and must be executed thousands of times per genome before actual genomic analysis can start.&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;More recently, &lt;a href=&quot;http://edicogenome.com/dragen-bioit-platform/&quot;&gt;Edico Genome&lt;/a&gt; has been very successful in implementing a wide range of common bioinformatics algorithms on FPGA and providing end-to-end analysis processing pipelines that act as drop-in replacements for standard genomic analysis pipelines.&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;div&amp;gt;The success of these FPGA products is due in large part to the fact that the end-user scientists don’t ever have to directly interact with the FPGAs.  In the case of experimental detectors, FPGAs are sufficiently close to the detector that the “raw” data that is delivered to the researcher has already been processed by the FPGAs.  Convey and Edico products incorporate their FPGAs into an appliance, and the process of offloading certain tasks to the FPGA in proprietary applications that, to the research scientist, look like any other command-line analysis program.&amp;lt;/div&amp;gt;&lt;/p&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;With all this said, the fact remains that these use cases are all on the fringe of HPC. &amp;nbsp;They present a black-and-white decision to researchers; to benefit from FPGAs, scientists must completely buy into the applications, algorithms, and software stacks. &amp;nbsp;Seeing as how these FPGA HPC stacks are often closed-source and proprietary, the benefit of being able to see, modify, and innovate on open-source scientific code often outweighs the speedup benefits of the fast-but-rigid FPGA software ecosystem.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h2&gt;Where FPGAs will fit in HPC tomorrow&lt;/h2&gt;
&lt;div&gt;The way I see it, there are two things that must happen before FPGAs can become a viable general-purpose technology for accelerating HPC:&lt;/div&gt;
&lt;div&gt;&lt;ol&gt;&lt;li&gt;Users must be able to integrate FPGA acceleration into their existing applications rather than replace their applications wholesale with proprietary FPGA analogues.&lt;/li&gt;&lt;li&gt;It has to be as easy as&amp;nbsp;&lt;span&gt;f90 -fopenacc&lt;/span&gt;&amp;nbsp;or&amp;nbsp;&lt;span&gt;nvcc&lt;/span&gt;&amp;nbsp;to build an FPGA-accelerated application, and running the resulting accelerated binary has to be as easy as running an unaccelerated binary.&lt;/li&gt;&lt;/ol&gt;&lt;div&gt;The first steps towards realizing this have already been made; both &lt;a href=&quot;https://www.xilinx.com/products/design-tools/software-zone/sdaccel.html&quot;&gt;Xilinx&lt;/a&gt; and &lt;a href=&quot;https://www.altera.com/products/design-software/embedded-software-developers/opencl/overview.html&quot;&gt;Intel/Altera&lt;/a&gt; now offer OpenCL runtime environments that allow scientific applications to offload computational kernels to the FPGA. &amp;nbsp;The Xilinx environment operates much like an OpenCL accelerator, where specific kernels are compiled for the FPGA and loaded as application-specific logic; the Altera environment installs a special OpenCL runtime environment on the FPGA. &amp;nbsp;However, there are a couple of challenges:&lt;/div&gt;
&lt;/div&gt;
&lt;div&gt;&lt;ul&gt;&lt;li&gt;OpenCL tends to be very messy to code in compared to simpler APIs such as OpenACC, OpenMP, CUDA, or HIP. &amp;nbsp;As a result, not many HPC application developers are investing in OpenCL anymore.&lt;/li&gt;&lt;li&gt;Compiling an application for OpenCL on an FPGA still requires going through the entire Xilinx or Altera toolchain. &amp;nbsp;At present, this is &lt;i&gt;&lt;u&gt;not&lt;/u&gt;&lt;/i&gt; as simple as &lt;span&gt;f90 -fopenacc&lt;/span&gt; or &lt;span&gt;nvcc&lt;/span&gt;, and the process of compiling code that targets an FPGA can take orders of magnitude longer than it would for a CPU due to the NP-hard nature of placing and routing across all the programmable blocks.&lt;/li&gt;&lt;li&gt;The FPGA OpenCL stacks are not as polished and scientist-friendly right now; performance analysis and debugging generally still has to be done at the circuit level, which is untenable for domain scientists.&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;Fortunately, these issues are under very active development, and the story surrounding FPGAs for HPC application improves on a month by month basis. &amp;nbsp;We're still years from FPGAs becoming a viable option for accelerating scientific applications in a general sense, but when that day comes, I predict that programming in Verilog for FPGAs will seem as exotic as programming in assembly is for CPUs.&lt;/div&gt;
&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;Rather, applications will likely rely on large collections of pre-compiled FPGA IP blocks (often called&amp;nbsp;&lt;i&gt;FPGA overlays&lt;/i&gt;) that map to common compute kernels. &amp;nbsp;It will then be the responsibility of compilers to identify places in the application source code where these logic blocks should be used to offload certain loops. &amp;nbsp;Since it's unlikely that a magic compiler will be able to identify these loops on their own, users will still have to rely on OpenMP, OpenACC, or some other API to provide hints at compile time. &amp;nbsp;Common high-level functions, such as those provided by LAPACK, will probably also be provided by FPGA vendors as pre-compiled overlays that are hand-tuned.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h2&gt;Concluding Thoughts&lt;/h2&gt;
&lt;div&gt;We're still years away from FPGAs being a viable option for mainstream HPC, and as such, I don't anticipate them as being the key technology that will underpin the world's first exascale systems. &amp;nbsp;Until the FPGA software ecosystem and toolchain mature to a point where domain scientists never have to look at a line of Verilog, FPGAs will remain an accelerator technology at the fringes of HPC.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;However, there is definitely a path for FPGAs to become mainstream, and forward progress is being made. &amp;nbsp;Today's clunky OpenCL implementations are already being followed up by &lt;a href=&quot;https://www.nextplatform.com/2016/10/19/turning-openmp-programs-parallel-hardware/&quot;&gt;research into providing OpenMP-based FPGA acceleration&lt;/a&gt;, and proofs of concept demonstrating &lt;a href=&quot;https://ft.ornl.gov/sites/default/files/IPDPS16_OpenACC2FPGA_PPT.pdf&quot;&gt;OpenACC-based FPGA acceleration&lt;/a&gt; have shown promising levels of performance portability. &amp;nbsp;On the hardware side, FPGAs are also approaching first-class citizenship with &lt;a href=&quot;https://www.nextplatform.com/2017/10/02/intel-gears-fpga-push/&quot;&gt;Intel planning to ship Xeons with integrated FPGAs in 2H2018&lt;/a&gt; and &lt;a href=&quot;https://www.alpha-data.com/dcp/products.php?product=adm-pcie-9v3&quot;&gt;OpenPOWER beginning to ship Xilinx FPGAs with OpenCAPI-based coherence links for POWER9&lt;/a&gt;.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;The momentum is growing, and the growing urgency surrounding post-Moore computing technology is driving investments and demand from both public and private sectors. &amp;nbsp;FPGAs won't be the end-all solution that gets us to exascale, nor will it be the silver bullet that gets us beyond Moore's Law computing, but they will definitely play an increasingly important role in HPC over the next five to ten years.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;If you've gotten this far and are interested in more information, I strongly encourage you to check out &lt;a href=&quot;https://science.energy.gov/~/media/ascr/ascac/pdf/meetings/201612/Finkel_FPGA_ascac.pdf&quot;&gt;FPGAs for Supercomputing: The Why and How&lt;/a&gt;, presented by Hal Finkel, Kazutomo Yoshii, and Franck Cappello at ASCAC. &amp;nbsp;It provides more insight into the application motifs that FPGAs can accelerate, and a deeper architectural treatment of FPGAs as understood by real computer scientists.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;p&gt;&lt;span style=&quot;font-size: xx-small;&quot;&gt;** This is not really true.  Such a design would be limited by the number of physical pins coming out of the FPGA; in reality, output pins would have to be multiplexed, and additional logic to drive this multiplexing would take up FPGA real estate.  But you get the point.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;Save&lt;/span&gt;&lt;span&gt;Save&lt;/span&gt;&lt;br /&gt;&lt;span&gt;Save&lt;/span&gt;&lt;span&gt;Save&lt;/span&gt;&lt;span&gt;Save&lt;/span&gt;&lt;span&gt;Save&lt;/span&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Chapel's Home in the Landscape of New Scientific Computing Languages</title>
   <link href="https://hpc.social/2017/chapel-s-home-in-the-landscape-of-new-scientific-computing-languages/"/>
   <updated>2017-06-04T01:00:00-06:00</updated>
   <id>https://hpc.social/2017/chapel-s-home-in-the-landscape-of-new-scientific-computing-languages</id>
   <content type="html">&lt;p&gt;I was invited to speak at this past weekend’s fourth annual Chapel Implementers and Users Workshop (&lt;a href=&quot;http://chapel.cray.com/CHIUW2017.html&quot;&gt;CHIUW 2017&lt;/a&gt;).
It was a great meeting, with lots of extremely high-quality talks on work being done with and on Chapel.  The slides from the presentations
will be up shortly, and I recommend them - the &lt;a href=&quot;http://chapel.cray.com/CHIUW/2017/choi-slides.pdf&quot;&gt;libfabric&lt;/a&gt;, 
&lt;a href=&quot;http://chapel.cray.com/CHIUW/2017/kayraklioglu-slides.pdf&quot;&gt;KNL&lt;/a&gt;, &lt;a href=&quot;http://chapel.cray.com/CHIUW/2017/krishna-slides.pdf&quot;&gt;use-after-free tracking&lt;/a&gt;, and &lt;a href=&quot;http://chapel.cray.com/CHIUW/2017/azad-slides.pdf&quot;&gt;GraphBLAS&lt;/a&gt; works
were of particular interest to me.  The Code Camp on the next day, working with members the Chapel team on individual particular projects, was also a lot of fun.&lt;/p&gt;

&lt;p&gt;The topic of my own talk was “Chapel’s Home in the Landscape
of New Scientific Computing Languages (and what it can learn from
the neighbours)”; the materials from the talk can be found
&lt;a href=&quot;http://github.com/ljdursi/CHIUW2017&quot;&gt;on github&lt;/a&gt;.  I described
the sorts of problems I’m particularly interested in, surveyed
some of the languages/frameworks in there, and tried to identify
what I saw as Chapel’s role in the environment.&lt;/p&gt;

&lt;p&gt;My slides can be seen below or on &lt;a href=&quot;http://ljdursi.github.io/CHIUW2017/#1&quot;&gt;github&lt;/a&gt;, where &lt;a href=&quot;http://github.com/ljdursi/CHIUW2017&quot;&gt;the complete materials can be found&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Compute Canadian- Building a successful and federated computational research enterprise, together</title>
   <link href="https://hpc.social/2017/compute-canadian-building-a-successful-and-federated-computational-research-enterprise-together/"/>
   <updated>2017-06-01T01:00:00-06:00</updated>
   <id>https://hpc.social/2017/compute-canadian-building-a-successful-and-federated-computational-research-enterprise-together</id>
   <content type="html">&lt;p&gt;Canada is a federated nation, and this is particularly visible in
areas of research funding, where both the federal and provincial
orders of government play a role.  In building a successful digital
research infrastructure to support Canadian science and scholarship,
we must recognize that reality, and rely on the successful examples
of many organizations in Canada and around the world that embrace
such a federated approach.&lt;/p&gt;

&lt;p&gt;In &lt;a href=&quot;https://www.dursi.ca/assets/pdfs/ComputeCanadianDiscussionPaper.pdf&quot;&gt;this discussion paper&lt;/a&gt;,
my colleague Jill Kowalchuck and I lay out what we hope to be the beginnings
of a discussion of what a renewed federation for supporting Canadian 
science with advanced research computing and data could look like.&lt;/p&gt;

&lt;h3 id=&quot;executive-summary&quot;&gt;Executive Summary&lt;/h3&gt;

&lt;p&gt;Computing and data, and the expertise and tools to make use of both, is
now central to all fields of study. Ten years after the creation of
Compute Canada in response to the National Platforms Fund call, and
after the Naylor Report on science funding, it is an apt time for the
Canadian community built around this national research platform to take
stock. Is it doing what we need it to do for Canadian researchers? Is it
working the way we want it to? What should a Canadian computation and
data platform for supporting research look like in the coming years?
This document aims to begin that discussion within the community.&lt;/p&gt;

&lt;p&gt;Here we propose seven principles to guide us in this discussion — that
our project should serve Canadian research in a researcher-centred,
service-oriented, and truly national way; and that it should operate as
a true federation of equal partners, interoperable but not identical,
collaborative and up-to-date. We suggest in particular that it is vital
that our national platform is adaptive and responsive to researchers,
making choices driven by research needs and not technical choices, and
should make full use of the diversity and specialization that a Canadian
federation and its partners offer.&lt;/p&gt;

&lt;p&gt;From those principles, we make evidence-based proposals for a renewed
Canadian organization. Comparisons with successful examples of federated
organizations within Canada and abroad suggest that while the basic
architecture of our federation is sound, important roles and
relationships need to be clarified. While a central office must be
responsible for the processes of defining priorities, strategies, and
standards of interoperability, a successful federation requires those
processes to have buy-in from partners committed to the goals of the
federation. The Board of Directors of the central office in a federation
must have experience and training to handle the delicate task of
governing a central office but being responsible to a national
community. The Members need adequate visibility into the operations of
the central office and the federation as a whole so that they can
support their vital role to the organization. And that engagement needs
to extend to all who are invested in the success of research in Canada:
regional staff and Boards, institutional staff, researchers and funders,
and other organizations that provide digital infrastructure for research
in Canada. This document focusses on Compute Canada in particular, but
the principles and proposals apply to any digital research
infrastructure providers, or the system as a whole.&lt;/p&gt;

&lt;p&gt;Success for this document will mean starting conversations, inspiring
other documents and differing points of view, and the emerging of a
consensus within the community of what a renewed national platform for
the next ten years looks like. That does not mean this document is a
straw-man. The authors have played roles in the national platform
starting at its inception, from researcher to consortium and regional
(east and west) staff and management, and within the Compute Canada
central office, and hope that experience plus the benefit of some
distance have produced a coherent and compelling vision of what the
Compute Canada national project could be. But what matters is not this
proposal; it is what the community as a whole decides it wants its
national platform to be.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Should I use Chapel or Julia for my next project?</title>
   <link href="https://hpc.social/2017/should-i-use-chapel-or-julia-for-my-next-project/"/>
   <updated>2017-05-28T01:00:00-06:00</updated>
   <id>https://hpc.social/2017/should-i-use-chapel-or-julia-for-my-next-project-</id>
   <content type="html">&lt;p&gt;&lt;a href=&quot;https://julialang.org&quot;&gt;Julia&lt;/a&gt; and &lt;a href=&quot;http://chapel.cray.com&quot;&gt;Chapel&lt;/a&gt;
are both newish languages aimed at productitive scientific computing,
with parallel computing capabilities baked in from the start.
There’s lots of information about both online, but not much comparing
the two.  If you are starting a new scientific computing project
and are willing to try something new, which should you choose?  What
are their strengths and weaknesses, and how do they compare?&lt;/p&gt;

&lt;p&gt;Here we walk through a comparison, focusing on distributed-memory
parallelism of the sort one would want for HPC-style simulation.
Both have strengths in largely disjoint areas.  If you want matlib-like
interactivity and plotting, and need only coodinator-worker parallelism,
Julia is the clear winner; if you want MPI+OpenMPI type scability
on rectangular distributed arrays (dense or sparse), Chapel wins
handily.  Both languages and environments have clear untapped
potential and room to grow; we’ll talk about future prospects of
the two languages at the end.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: I’ve updated the timings - I hadn’t been using &lt;code&gt;@inbounds&lt;/code&gt;
in the Julia code, and I had misconfigured my Chapel install so
that the compiles weren’t optimized; this makes a huge difference on
the 2d advection problem.  All timings now are on an AWS c4.8x instance.&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.dursi.ca/feed.xml#a-quick-overview-of-the-two-languages&quot; id=&quot;markdown-toc-a-quick-overview-of-the-two-languages&quot;&gt;A quick overview of the two languages&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.dursi.ca/feed.xml#julia&quot; id=&quot;markdown-toc-julia&quot;&gt;Julia&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.dursi.ca/feed.xml#chapel&quot; id=&quot;markdown-toc-chapel&quot;&gt;Chapel&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.dursi.ca/feed.xml#similarities-and-differences&quot; id=&quot;markdown-toc-similarities-and-differences&quot;&gt;Similarities and differences&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.dursi.ca/feed.xml#standard-library&quot; id=&quot;markdown-toc-standard-library&quot;&gt;Standard library&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.dursi.ca/feed.xml#other-packages&quot; id=&quot;markdown-toc-other-packages&quot;&gt;Other packages&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.dursi.ca/feed.xml#language-features&quot; id=&quot;markdown-toc-language-features&quot;&gt;Language features&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.dursi.ca/feed.xml#simple-computational-tasks&quot; id=&quot;markdown-toc-simple-computational-tasks&quot;&gt;Simple computational tasks&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.dursi.ca/feed.xml#linear-algebra&quot; id=&quot;markdown-toc-linear-algebra&quot;&gt;Linear algebra&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.dursi.ca/feed.xml#stencil-calculation&quot; id=&quot;markdown-toc-stencil-calculation&quot;&gt;Stencil calculation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.dursi.ca/feed.xml#kmer-counting&quot; id=&quot;markdown-toc-kmer-counting&quot;&gt;Kmer counting&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.dursi.ca/feed.xml#parallel-primitives&quot; id=&quot;markdown-toc-parallel-primitives&quot;&gt;Parallel primitives&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.dursi.ca/feed.xml#remote-function-execution&quot; id=&quot;markdown-toc-remote-function-execution&quot;&gt;Remote function execution&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.dursi.ca/feed.xml#futures-atomics-and-synchronization&quot; id=&quot;markdown-toc-futures-atomics-and-synchronization&quot;&gt;Futures, atomics and synchronization&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.dursi.ca/feed.xml#parallel-loops-reductions-and-maps&quot; id=&quot;markdown-toc-parallel-loops-reductions-and-maps&quot;&gt;Parallel loops, reductions, and maps&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.dursi.ca/feed.xml#threading&quot; id=&quot;markdown-toc-threading&quot;&gt;Threading&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.dursi.ca/feed.xml#distributed-data&quot; id=&quot;markdown-toc-distributed-data&quot;&gt;Distributed data&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.dursi.ca/feed.xml#communications&quot; id=&quot;markdown-toc-communications&quot;&gt;Communications&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.dursi.ca/feed.xml#a-2d-advection-problem&quot; id=&quot;markdown-toc-a-2d-advection-problem&quot;&gt;A 2d advection problem&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.dursi.ca/feed.xml#strengths-weaknesses-and-future-prospects&quot; id=&quot;markdown-toc-strengths-weaknesses-and-future-prospects&quot;&gt;Strengths, Weaknesses, and Future Prospects&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.dursi.ca/feed.xml#julia-1&quot; id=&quot;markdown-toc-julia-1&quot;&gt;Julia&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.dursi.ca/feed.xml#chapel-1&quot; id=&quot;markdown-toc-chapel-1&quot;&gt;Chapel&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.dursi.ca/feed.xml#my-conclusions&quot; id=&quot;markdown-toc-my-conclusions&quot;&gt;My conclusions&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.dursi.ca/feed.xml#both-projects-are-strong-and-useable-right-now-at-different-things&quot; id=&quot;markdown-toc-both-projects-are-strong-and-useable-right-now-at-different-things&quot;&gt;Both projects are strong and useable, right now, at different things&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.dursi.ca/feed.xml#both-projects-have-as-yet-untapped-potential&quot; id=&quot;markdown-toc-both-projects-have-as-yet-untapped-potential&quot;&gt;Both projects have as-yet untapped potential&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;a-quick-overview-of-the-two-languages&quot;&gt;A quick overview of the two languages&lt;/h2&gt;

&lt;h3 id=&quot;julia&quot;&gt;Julia&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&quot;https://julialang.org&quot;&gt;Julia project&lt;/a&gt; describes Julia as “a
high-level, high-performance dynamic programming language for
numerical computing.”  It exploits type inference of rich types,
just-in-time compilation, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Multiple_dispatch&quot;&gt;multiple
dispatch&lt;/a&gt; (think
of R, with say &lt;code&gt;print()&lt;/code&gt; defined to operate differently on scalars,
data frames, or linear regression fits) to provide a dynamic,
interactive, “scripting language”-type high level numerical programming
language that gives performance less than but competitive with
C or Fortran.&lt;/p&gt;

&lt;p&gt;The project sees the language as more or less a matlab-killer, and
so focusses on that sort of interface; interactive, through a REPL
or Jupyter notebook (both available to try &lt;a href=&quot;https://juliabox.com&quot;&gt;online&lt;/a&gt;),
with integrated plotting; also, indexing begins at one, as God
intended.&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a class=&quot;footnote&quot; href=&quot;https://www.dursi.ca/feed.xml#fn:1&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;table style=&quot;border: 1px solid black;&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Example from &lt;a href=&quot;https://github.com/dpsanders/scipy_2014_julia&quot;&gt;David Sanders’ SciPy 2014 tutorial&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;&lt;span class=&quot;k&quot;&gt;using&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PyPlot&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# julia set&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt; julia&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;maxiter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maxiter&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;abs2&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;maxiter&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;jset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;UInt8&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;julia&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;complex&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;complex&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;06&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;67&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)))&lt;/span&gt;
             &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:-.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;002&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;002&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;];&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;get_cmap&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;RdGy&quot;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jset&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cmap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;RdGy&quot;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;extent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;])&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;/td&gt;
&lt;td&gt;
&lt;img alt=&quot;Julia set plot&quot; src=&quot;https://www.dursi.ca/assets/julia_v_chapel/juliaset_in_julia.png&quot; /&gt;
&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Julia blurs the distinction between scientific users of Julia and
developers in two quite powerful ways.  The first is lisp-like
&lt;a href=&quot;https://docs.julialang.org/en/stable/manual/metaprogramming/&quot;&gt;metaprogramming&lt;/a&gt;,
where julia code can be generated or modified from within Julia,
making it possible to build domain-specific langauges (DSLs) inside Julia
for problems; this allows simple APIs for broad problem sets which
nonetheless take full advantage of the structure of the particular
problems being solved; &lt;a href=&quot;https://github.com/JuliaStats&quot;&gt;JuliaStats&lt;/a&gt;,
&lt;a href=&quot;https://github.com/JuliaDiffEq/DifferentialEquations.jl&quot;&gt;DifferentialEquations.jl&lt;/a&gt;,
&lt;a href=&quot;https://github.com/JuliaFEM/JuliaFEM.jl&quot;&gt;JuliaFEM&lt;/a&gt;, and
&lt;a href=&quot;https://github.com/JuliaOpt/JuMP.jl&quot;&gt;JuMP&lt;/a&gt; offer hints of what
that could look like.  Another sort of functionality this enables
is &lt;a href=&quot;https://julialang.org/blog/2016/03/parallelaccelerator&quot;&gt;Parallel Accellerator&lt;/a&gt;, an
intel package that can rewrite some regular array operations into
fast, vectorized native code.  This code-is-data aspect of Julia,
combined with the fact that much of Julia itself is written in Julia,
puts user-written code on an equal footing with much “official”
julia code.&lt;/p&gt;

&lt;p&gt;The second way Julia blurs the line between user and developer is
the &lt;a href=&quot;https://docs.julialang.org/en/stable/manual/packages/&quot;&gt;package system&lt;/a&gt;
which uses git and GitHub; this means that once you’ve installed
someone’s package, you’re very close to being able to file a pull
request if you find a bug, or to fork the package to specialize
it to your own needs; and it’s similarly very easy to
contribute a package if you’re already using GitHub to develop the
package.&lt;/p&gt;

&lt;p&gt;Julia has support for remote function execution (“out of the box”
using SSH + TCP/IP, but other transports are available through
packages), and distributed rectangular arrays; thread support
is still experimental, as is shared-memory on-node arrays.&lt;/p&gt;

&lt;h3 id=&quot;chapel&quot;&gt;Chapel&lt;/h3&gt;

&lt;p&gt;While Julia is a scientific programming language with parallel
computing support, Chapel is a programming language for parallel
scientific computing. It is a &lt;a href=&quot;https://en.wikipedia.org/wiki/Partitioned_global_address_space&quot;&gt;PGAS&lt;/a&gt;
language, with partitioned but globally-accessible variables, using
&lt;a href=&quot;https://gasnet.lbl.gov&quot;&gt;GASNet&lt;/a&gt; for communications.  It takes PGAS
two steps further however than languages like &lt;a href=&quot;https://www.dursi.ca/post/coarray-fortran-goes-mainstream-gcc-5-1.html&quot;&gt;Coarray
Fortran&lt;/a&gt;,
&lt;a href=&quot;http://upc.lbl.gov&quot;&gt;UPC&lt;/a&gt;, or &lt;a href=&quot;http://x10-lang.org&quot;&gt;X10&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The first extension is to define all large data structures (arrays,
associative arrays, graphs) as being defined over &lt;em&gt;domains&lt;/em&gt;, and
then definining a library of &lt;em&gt;domain maps&lt;/em&gt; for distributing these
domains over different locality regions (“locales”) (nodes, or NUMA
nodes, or KNL accellerators) and &lt;em&gt;layouts&lt;/em&gt; for describing their layout
within a locale.  By far the best tested and optimized domain maps
are for the cases of dense (and to a lesser extent, CSR-layout
sparse) rectangular arrays, as below, although there support for
associative arrays (dictionaries) and unstructured meshes/graphs
as well.&lt;/p&gt;

&lt;p&gt;The second is to couple those domain maps with parallel iterators
over the domains, meaning that one can loop over the data in parallel
in one loop (think OpenMP) with a “global view” rather than expressing
the parallelism explicitly as a SIMD-type program.  This decouples
the expression of the layout of the data from the expression of the
calculation over the data, which is essential for productive parallel 
computing; it means that tweaking the layouts (or the dimensionality of
the program, or…) doesn’t require rewriting the internals of the
computation.&lt;/p&gt;

&lt;p&gt;The distributions and layouts are written in Chapel, so that users can
contribute new domain maps to the project.&lt;/p&gt;

&lt;table style=&quot;border: 1px solid black;&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt; &lt;td&gt;
Example from &lt;a href=&quot;http://chapel.cray.com/tutorials/ACCU2017/06-DomainMaps.pdf&quot;&gt;Chapel tutorial at ACCU 2017&lt;/a&gt;
&lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-chapel&quot;&gt;var Dom: {1..4, 1..8} dmapped Block({1..4, 1..8});&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;
&lt;img alt=&quot;Block Distribution&quot; src=&quot;https://www.dursi.ca/assets/julia_v_chapel/block-dist.png&quot; /&gt;
&lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-chapel&quot;&gt;var Dom: {1..4, 1..8} dmapped Cyclic(startIdx=(1,1));&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;
&lt;img alt=&quot;Block Distribution&quot; src=&quot;https://www.dursi.ca/assets/julia_v_chapel/cyclic-dist.png&quot; /&gt;
&lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-chapel&quot;&gt;// either case:

var Inner : subdomain(Dom) = {2..3, 2..7};
const north = (-1,0), south = (1,0), east = (0,1), west = (0,-1);

var data, data_new : [Dom] real;
var delta : real;

forall ij in Inner {
    data_new(ij) = (data(ij+north) + data(ij+south)
                    + data(ij+east) + data(ij+west)) / 4.0;
}
delta = max reduce abs(data_new[Dom] - data[Dom]);&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;/td&gt; &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Chapel also exposes its lower-level parallel computing functionality —
such as remote function execution, fork/join task parallelism — so
that one can write a MPI-like SIMD program by explicity launching 
a function on each core:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-chapel&quot;&gt;coforall loc in Locales do 
    on loc do
        coforall tid in 0..#here.maxTaskPar do
            do_simd_program(loc, tid);&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;At roughly eight years old as a publically available project, Chapel
is a slightly older and more mature language than Julia. However,
the language continues to evolve and there are breaking changes
between versions; these are much smaller and more localized breaking
changes than with Julia, so that most recent example code online
works readily.  As its focus has always been on large-scale parallelism
rather than desktop computing, its potential market is smaller
so has attracted less interest and fewer users than Julia
— however, if you read this blog, Chapel’s niche is one you are
almost certainly very interested in.  The relative paucity of users
is reflected in the smaller number of contributed packages, although
an upcoming package manager will likely lower the bar to future
contributions.&lt;/p&gt;

&lt;p&gt;Chapel also lacks a REPL, which makes experimentation and testing
somewhat harder — there’s no equivalent of &lt;a href=&quot;https://juliabox.com&quot;&gt;JuliaBox&lt;/a&gt;
where one can play with the language at a console or in a notebook.
There is an effort in that direction now which may be made easier
by ongoing work on the underlying compiler architecture.&lt;/p&gt;

&lt;h2 id=&quot;similarities-and-differences&quot;&gt;Similarities and differences&lt;/h2&gt;

&lt;h3 id=&quot;standard-library&quot;&gt;Standard library&lt;/h3&gt;

&lt;p&gt;Both &lt;a href=&quot;https://docs.julialang.org/en/stable&quot;&gt;Julia&lt;/a&gt; and &lt;a href=&quot;http://chapel.cray.com/docs/latest/&quot;&gt;Chapel&lt;/a&gt;
have good documentation, and the basic modules or capabilities one would expect from languages 
aimed at technical computing:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Complex numbers&lt;/li&gt;
  &lt;li&gt;Mathematical function libraries&lt;/li&gt;
  &lt;li&gt;Random numbers&lt;/li&gt;
  &lt;li&gt;Linear algebra&lt;/li&gt;
  &lt;li&gt;FFTs&lt;/li&gt;
  &lt;li&gt;C, Python interoperability&lt;/li&gt;
  &lt;li&gt;Multi-precision floats / BigInts&lt;/li&gt;
  &lt;li&gt;MPI interoperability&lt;/li&gt;
  &lt;li&gt;Profiling&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;although there are differences - in Julia, Python interoperability
is much more complete (the Julia set example above used matplotlib
plotting, while &lt;a href=&quot;https://pychapel.readthedocs.io&quot;&gt;pychapel&lt;/a&gt; focuses
on calling Chapel from within python).  Also, Julia’s linear algebra
support is much slicker, styled after Matlab syntax and with a rich
set of matrix types (symmetric, tridiagonal, &lt;em&gt;etc.&lt;/em&gt;), so that for
linear solves, say, a sensible method is chosen automatically; the
consise syntax and “do the right thing” approach are particularly
helpful for interactive use&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a class=&quot;footnote&quot; href=&quot;https://www.dursi.ca/feed.xml#fn:2&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, which is a primary use-case of Julia.&lt;/p&gt;

&lt;p&gt;On profiling, the Julia support is primariy for serial profiling
and text based; Chapel has a very nice tool called
&lt;a href=&quot;http://chapel.cray.com/docs/1.14/tools/chplvis/chplvis.html&quot;&gt;chplvis&lt;/a&gt; 
for visualizing parallel performance.&lt;/p&gt;

&lt;h3 id=&quot;other-packages&quot;&gt;Other packages&lt;/h3&gt;

&lt;p&gt;Julia’s early adoption of a package management framework and very
large initial userbase has lead to a &lt;a href=&quot;http://pkg.julialang.org&quot;&gt;very large ecosystem&lt;/a&gt;
of contributed packages.  As with all such package ecosystems, 
the packages themselves are a bit of a mixed bag – lots are broken or
abandoned, many are simply wrappers to other tools – but there
are also excellent, substantial packages taking full advantage of
Julia’s capabalities that are of immediate interest
to those doing scientific computing, such as 
&lt;a href=&quot;https://github.com/JuliaDiffEq/DifferentialEquations.jl&quot;&gt;DifferentialEquations.jl&lt;/a&gt;
for ODEs, SDEs, and and FEM for some PDEs,
&lt;a href=&quot;https://github.com/BioJulia&quot;&gt;BioJulia&lt;/a&gt; for bioinformatics,
&lt;a href=&quot;http://www.juliadiff.org&quot;&gt;JuliaDiff&lt;/a&gt; for automatic differentiation,
and &lt;a href=&quot;http://juliastats.github.io&quot;&gt;JuliaStats&lt;/a&gt; for R-like
statistical computing.  The julia project would benefit from
having a more curated view of the package listings easily available
so that these high-quality tools were more readily visible to
new users.&lt;/p&gt;

&lt;p&gt;On the other hand, there are almost no packages available for Chapel
outside of the main project.  There are efforts to develop a package
manager inspired by cargo (Rust) and glide (Go); this would be an
important and needed development, almost certainly necessary
to grow the Chapel community.&lt;/p&gt;

&lt;h3 id=&quot;language-features&quot;&gt;Language features&lt;/h3&gt;

&lt;p&gt;The biggest language feature difference is undoubtedly Julia’s
JIT-powered lisp-metaprogramming capabilities; Chapel is a more
statically-compiled language, with generics and reflection but not
full lisp-like code-is-data.  A small downside of Julia’s JIT
approach is that functions are often slow the first time they are
called, as they must be compiled.  Relatedly, Julia is garbage-collected,
which can lead to pauses and memory pressure at unexpected times.
On the other hand, Chapel’s compile time, which is still quite long
even compared to other compilers, makes the development cycle much
slower than it would be with Julia or Python.&lt;/p&gt;

&lt;p&gt;Beyond that, Julia and Chapel are both quite new and have functionality
one might expect in a modern language: first class functions, lambda
functions, comprehensions, keyword/optional parameters, type
inference, generics, reflection, iterators, ranges, coroutines and
green threads, futures, and JuliaDoc/chpldoc python packages for
generating online documentation from source code and embedded
comments.&lt;/p&gt;

&lt;p&gt;More minor but something that quickly comes up: there’s difference
in command-line argument handling which reflects the use
cases each team finds important.  Both give access to an argv-like array of
strings passed to the command line; in base Julia with its interactive
nature, that’s it (although there’s a nice python-argparse inspired
&lt;a href=&quot;http://carlobaldassi.github.io/ArgParse.jl/latest/&quot;&gt;contributed package&lt;/a&gt;),
while in Chapel, intended to make compiled long-running executables
one can define a constant (&lt;code&gt;const n = 10;&lt;/code&gt;) and make it settable
on the command line by prefixing the &lt;code&gt;const&lt;/code&gt; with &lt;code&gt;config&lt;/code&gt; and running
the program with &lt;code&gt;--n 20&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;simple-computational-tasks&quot;&gt;Simple computational tasks&lt;/h2&gt;

&lt;p&gt;Here we take a look at a couple common single-node scientific
computation primitives in each framework (with Python for comparison)
to compare the language features.  Full code for the examples are
available &lt;a href=&quot;http://www.github.com/ljdursi/julia_v_chapel&quot;&gt;on GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;linear-algebra&quot;&gt;Linear algebra&lt;/h3&gt;

&lt;p&gt;For linear algebra operations, Julia’s matlab lineage and
interactive really shine:&lt;/p&gt;

&lt;table style=&quot;border: 1px solid black;&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Julia&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;&lt;span class=&quot;c&quot;&gt;# ...&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eye&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# ...&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Chapel&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c&quot;&gt;&lt;span class=&quot;n&quot;&gt;use&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearAlgebra&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;use&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LAPACK&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;use&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Vector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;fillRandom&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fillRandom&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;writeln&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;..&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;..&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eye&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ipiv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;..&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c_int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;info&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gesv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lapack_memory_order&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row_major&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ipiv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reduce&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;writeln&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Python&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;__future__&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;print_function&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eye&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;solve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The new Chapel &lt;code&gt;LinearAlgebra&lt;/code&gt; and &lt;code&gt;LAPACK&lt;/code&gt; modules don’t really 
work well together yet, so one has to awkwardly switch between
the two idioms, but that’s readily easily fixed.  Julia’s nice
matrix type system allows “do the right-thing” type linear solves,
which is incredibly handy for interactive work, although for a 
compiled program that will be used repeatedly, the clarity of
specifying a specific solver (which Julia also allows) is probably
advantageous.&lt;/p&gt;

&lt;h3 id=&quot;stencil-calculation&quot;&gt;Stencil calculation&lt;/h3&gt;

&lt;p&gt;Below we take a look at a simple 1-d explicit heat diffusion equation,
requiring a small stencil, and see how it compares across the languges.&lt;/p&gt;

&lt;table style=&quot;border: 1px solid black;&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Julia&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;&lt;span class=&quot;c&quot;&gt;# ...&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ngrid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
  &lt;span class=&quot;nd&quot;&gt;@inbounds&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tleft&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ngrid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tright&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iteration&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ntimesteps&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ngrid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
      &lt;span class=&quot;nd&quot;&gt;@inbounds&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;temp_new&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kappa&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;
                      &lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;])&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ngrid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
      &lt;span class=&quot;nd&quot;&gt;@inbounds&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;temp_new&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# ...&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Chapel&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c&quot;&gt;&lt;span class=&quot;c1&quot;&gt;// ...&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ProblemSpace&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;..&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ngrid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;BigDomain&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;..&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ngrid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TNew&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BigDomain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;real&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iteration&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tleft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ngrid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tright&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;left&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;right&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iteration&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;..&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ntimesteps&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ProblemSpace&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;TNew&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kappa&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;right&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ProblemSpace&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TNew&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;// ...&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Python&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# ...
&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'f8[:](i4, i4, f8, f8, f8, f8, f8)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nopython&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nogil&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;onedheat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ngrid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ntimesteps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kappa&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xleft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xright&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tleft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tright&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xright&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xleft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ngrid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.25&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kappa&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ngrid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;double&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;temp_new&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ngrid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;double&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ngrid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tleft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tright&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iteration&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ntimesteps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;temp_new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ngrid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ngrid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kappa&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; \
            &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ngrid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ngrid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ngrid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ngrid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;temp_new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ngrid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ngrid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# ...&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The main difference above is that the easiest way to get fast array
operations out of Julia is to explicitly write out the loops as vs.
numpy, and of explicitly using domains in Chapel.  Timings are
below, for 10,000 timesteps of a domain of size 1,001.  The Julia
script included a “dummy” call to the main program to “warm up” the
JIT, and then called on the routine.  In Julia, for performance we
have to include the &lt;code&gt;@inbounds&lt;/code&gt; macro; Julia’s JIT doesn’t recognize
that the stencil calculation over fixed bounds is in bounds of the
array defined with those same fixed bounds a couple of lines before.
Compile times are included for the Julia and Python JITs (naively
calculated as total run time minus the final time spent running the
calculation)&lt;/p&gt;

&lt;table style=&quot;border: 1px solid black; margin: 0 auto; border-collapse: collapse;&quot;&gt;
&lt;thead&gt;
&lt;th&gt;time&lt;/th&gt; &lt;th&gt;Julia&lt;/th&gt; &lt;th&gt;Chapel&lt;/th&gt; &lt;th&gt;Python + Numpy + Numba&lt;/th&gt;&lt;th&gt;Python + Numpy&lt;/th&gt;
&lt;/thead&gt;
&lt;tbody style=&quot;border: 1px solid black;&quot;&gt;
&lt;tr&gt;&lt;td style=&quot;border: 1px solid black;&quot;&gt;run&lt;/td&gt;&lt;td style=&quot;border: 1px solid black;&quot;&gt;0.0084&lt;/td&gt;&lt;td style=&quot;border: 1px solid black;&quot;&gt;0.0098 s&lt;/td&gt;&lt;td style=&quot;border: 1px solid black;&quot;&gt;0.017 s&lt;/td&gt;&lt;td style=&quot;border: 1px solid black;&quot;&gt;0.069 s&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style=&quot;border: 1px solid black;&quot;&gt;compile&lt;/td&gt;&lt;td style=&quot;border: 1px solid black;&quot;&gt;0.57 s&lt;/td&gt;&lt;td style=&quot;border: 1px solid black;&quot;&gt;4.8s&lt;/td&gt;&lt;td style=&quot;border: 1px solid black;&quot;&gt;0.73 s&lt;/td&gt;&lt;td style=&quot;border: 1px solid black;&quot;&gt; - &lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Julia wins this test, edging out Chapel by 16%; Python with numba is 
surprisingly (to me) fast, coming within a factor of two.&lt;/p&gt;

&lt;h3 id=&quot;kmer-counting&quot;&gt;Kmer counting&lt;/h3&gt;

&lt;p&gt;Fields like bioinformatics or digital humanities push research
computing beyond matrix-slinging and array manipulations into the
more difficult areas of text handling, string manipulation, and
indexing.  Here we mock up a trivial kmer-counter, reading in 
genomic sequence data and counting the distribution of k-length
substrings.  A real implementation (such as in BioJulia or BioPython)
would optimize for the special case we’re in – a small fixed known
alphabet, and a hash function which takes advantage of the fact that
two neighbouring kmers overlap in k-1 characters – but
but here we’re just interested in the dictionary/associative array
handling and simple string slicing.  Here we’re using pure Python for
the Python implementation:&lt;/p&gt;

&lt;table style=&quot;border: 1px solid black;&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Julia&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;&lt;span class=&quot;c&quot;&gt;# ...&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sequences&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;read_sequences&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;infile&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;counts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DefaultDict&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Int8&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;}(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequences&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;kmer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;counts&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kmer&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt; 
&lt;span class=&quot;c&quot;&gt;# ...&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Chapel&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c&quot;&gt;&lt;span class=&quot;c1&quot;&gt;// ...&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequences&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;readfasta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_filename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kmers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;domain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kmer_counts&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kmers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequences&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;..(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kmer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;string&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;..&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kmers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;member&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kmer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;kmer_counts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kmer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;kmer_counts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kmer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;// ...&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Python&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# ...
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;kmer_counts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sequences&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;readfasta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;counts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;collections&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;defaultdict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;kmer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;counts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kmer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;counts&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# ...&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Other than the syntax differences, the main difference here is
Python and Chapel have convenience functions in their &lt;code&gt;defaultdict&lt;/code&gt;s
which mean you don’t have to handle the key-not-yet-found case
separately, and Chapel has the user explicitly declare the domain
of keys.  All perform quite well, particularly Julia; on a 4.5Mb
FASTA file for the reference genome of a strain of E. coli,
we get timings as below&lt;/p&gt;

&lt;table style=&quot;border: 1px solid black; margin: 0 auto; border-collapse: collapse;&quot;&gt;
&lt;thead&gt;
&lt;th&gt;time&lt;/th&gt; &lt;th&gt;Julia&lt;/th&gt; &lt;th&gt;Chapel&lt;/th&gt; &lt;th&gt;Python&lt;/th&gt;
&lt;/thead&gt;
&lt;tbody style=&quot;border: 1px solid black;&quot;&gt;
&lt;tr&gt;&lt;td style=&quot;border: 1px solid black;&quot;&gt;run&lt;/td&gt;&lt;td style=&quot;border: 1px solid black;&quot;&gt;5.3 s&lt;/td&gt;&lt;td style=&quot;border: 1px solid black;&quot;&gt;6.6s&lt;/td&gt;&lt;td style=&quot;border: 1px solid black;&quot;&gt;7.7s&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style=&quot;border: 1px solid black;&quot;&gt;compile&lt;/td&gt;&lt;td style=&quot;border: 1px solid black;&quot;&gt;-&lt;/td&gt;&lt;td style=&quot;border: 1px solid black;&quot;&gt;6.2s&lt;/td&gt;&lt;td style=&quot;border: 1px solid black;&quot;&gt;-&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Beating pure Python on dictionary and string operations isn’t
actually a given, even for a compiled language, as those features
are heavily optimized in Python implementations.&lt;/p&gt;

&lt;p&gt;(One caveat about the timings; pairwise string concatenation in Julia is &lt;em&gt;slow&lt;/em&gt;; 
in reading in the file, concatenating the sequence data in Julia
as it was done in the other languages resulted in a runtime of 54 seconds!
Instead, all sequence fragments were read in and the result put together
at once with &lt;code&gt;join()&lt;/code&gt;.)&lt;/p&gt;

&lt;h2 id=&quot;parallel-primitives&quot;&gt;Parallel primitives&lt;/h2&gt;

&lt;p&gt;Since we’re interested in large-scale computation, parallel features are of
particular interest to us; here we walk through the parallel primitives 
available to the languages and compare them.&lt;/p&gt;

&lt;h3 id=&quot;remote-function-execution&quot;&gt;Remote function execution&lt;/h3&gt;

&lt;p&gt;Both Julia and Chapel make it easy to explicitly launch tasks on other 
processors:&lt;/p&gt;

&lt;table style=&quot;border: 1px solid black;&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Julia&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;&lt;span class=&quot;nd&quot;&gt;@everywhere&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt; whoami&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;myid&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gethostname&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;remotecall_fetch&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;whoami&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;remotecall_fetch&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;whoami&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Chapel&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c&quot;&gt;&lt;span class=&quot;n&quot;&gt;proc&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numTasks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;here&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numPUs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;taskid&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;..&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numTasks&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;begin&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;writeln&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;here&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;here&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;taskid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;coforall&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Locales&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;writeln&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;In Julia, starting julia with &lt;code&gt;juila -p 4&lt;/code&gt; will launch julia with
4 worker tasks (and one coordinator task) on the local host; a &lt;code&gt;--machinefile&lt;/code&gt;
option can be set to launch the tasks on remote hosts (over ssh,
by default, although other “ClusterManager”s are available, for
instance launching tasks on SGE clusters).  In Chapel, launching a
chapel program with &lt;code&gt;-nl 4&lt;/code&gt; will run a program distributed over 4
locales, with options for those hosts set by environment variables.
Within each locale, Chapel will by default run across as many threads as
sensible (as determined by the extremely useful
&lt;a href=&quot;https://www.open-mpi.org/projects/hwloc/&quot;&gt;hwloc&lt;/a&gt; library).&lt;/p&gt;

&lt;p&gt;As seen above, Chapel distinuishes between starting up local and 
remote tasks; this is intrinsic to its “multiresolution” approach
to parallelism, so that it can take advantage of within-NUMA-node,
across-NUMA-node, and across-the-network parallism in different
ways.&lt;/p&gt;

&lt;h3 id=&quot;futures-atomics-and-synchronization&quot;&gt;Futures, atomics and synchronization&lt;/h3&gt;

&lt;p&gt;Once one can have tasks running asynchronously, synchronization
becomes an issue.  Julia and Chapel both have “futures” for 
asynchronous (non-blocking) function calls; futures can be
tested on, waited on or fetched from, with a fetch generally
blocking until the future has been “filled”.  Futures can only
be filled once.&lt;/p&gt;

&lt;p&gt;In fact, in the above, Julia’s &lt;code&gt;remotecall_fetch&lt;/code&gt; performs
the remote call and then fetches, mimicing a blocking call; the
&lt;code&gt;begin&lt;/code&gt; blocks in Chapel do not block.&lt;/p&gt;

&lt;p&gt;Futures work the following way in Julia and Chapel:&lt;/p&gt;

&lt;table style=&quot;border: 1px solid black;&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Julia&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nd&quot;&gt;@async&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;42&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fetch&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Chapel&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c&quot;&gt;&lt;span class=&quot;n&quot;&gt;use&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Futures&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;42&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;async&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lambda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;writeln&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;());&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Both Julia and Chapel have thread-safe atomic primitive
variables, and &lt;code&gt;sync&lt;/code&gt; blocks for joining tasks launched
within them before proceeding.&lt;/p&gt;

&lt;h3 id=&quot;parallel-loops-reductions-and-maps&quot;&gt;Parallel loops, reductions, and maps&lt;/h3&gt;

&lt;p&gt;Both languages make parallel looping, and reduction
over those parallel loops straightforward:&lt;/p&gt;

&lt;table style=&quot;border: 1px solid black;&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Julia&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;&lt;span class=&quot;c&quot;&gt;# parallel loop&lt;/span&gt;
&lt;span class=&quot;nd&quot;&gt;@parallel&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# parallel reduction&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;asum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nd&quot;&gt;@parallel&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt; twox&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;pmap&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;twox&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Chapel&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c&quot;&gt;&lt;span class=&quot;n&quot;&gt;forall&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;..&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;asum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reduce&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;threading&quot;&gt;Threading&lt;/h3&gt;

&lt;p&gt;In Chapel, parallel for loops are automatically assigned hierarchically
according to what the runtime knows about the architecture; threading is
used on-node if multiple cores are available.  Threading is an
&lt;a href=&quot;https://docs.julialang.org/en/stable/manual/parallel-computing/#multi-threading-experimental&quot;&gt;experimental feature&lt;/a&gt; 
in Julia, not quite ready to use for production work yet.&lt;/p&gt;

&lt;h3 id=&quot;distributed-data&quot;&gt;Distributed data&lt;/h3&gt;

&lt;p&gt;Julia has a
&lt;a href=&quot;https://github.com/JuliaParallel/DistributedArrays.jl&quot;&gt;DistributedArrays&lt;/a&gt;
package which are sort of half-PGAS arrays: they can be read from
at any index, but only the local part can be written to.  Chapel
is built around its PGAS distributions and iterators atop them.&lt;/p&gt;

&lt;p&gt;Julia’s DistributedArrays are known not to perform particularly well,
and have been taken out of the base language since 0.4.  They have
been worked on since in preparation for the 0.6 release; however,
the main branch does not appear to be working with 0.6-rc2, or
at least I couldn’t get it working.  This section then mostly covers the
previous version of DistributedArrays.&lt;/p&gt;

&lt;p&gt;Accessing remote values over DistributedArrays is quite slow.  As
such, DistributedArrays performs quite badly for the sort of thing
one might want to use Chapel distributed arrays for; they’re really
more for Monte-Carlo or other mostly-embarrasingly-parallel
calculations, where read access is only needed at the end of the
comptuation or a small number of other times.  Programming for a
stencil-type case or other iterative non-local computations is also
a little awkard; currently one has to remotely spawn tasks where
on the remote array fragments repeatedly to usher along each element
of the computation.  The new version of the arrays will have a
&lt;code&gt;simd()&lt;/code&gt; function which makes doing that nicer;  it also allows for
MPI-style communications, which seems like it is faster than accessing
the data through the distributed array, but for use cases where
that is handy, it’s not clear what one would use the distributed
array for rather than just having each task have its own local
array.&lt;/p&gt;

&lt;p&gt;However, for largely local computation (such as coordinator-worker type
operations), the distributed arrays work well.  Here
we have a STREAM calculation:&lt;/p&gt;

&lt;table style=&quot;border: 1px solid black;&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Julia&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;&lt;span class=&quot;k&quot;&gt;using&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DistributedArrays&lt;/span&gt;
&lt;span class=&quot;nd&quot;&gt;@everywhere&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;importall&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DistributedArrays&lt;/span&gt;

&lt;span class=&quot;nd&quot;&gt;@everywhere&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt; dostreamcalc&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bval&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cval&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;localindexes&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;localpart&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bval&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;localindexes&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;localpart&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cval&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;localindexes&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;localpart&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;localpart&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;localpart&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#...&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dzeros&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;problem_size&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;procs&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;refs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;nd&quot;&gt;@spawnat&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dostreamcalc&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bval&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cval&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ps&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pmap&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fetch&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;refs&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# ...&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Chapel&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c&quot;&gt;&lt;span class=&quot;c1&quot;&gt;// ...&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ProblemSpace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;domain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dmapped&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;boundingBox&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;..&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;problem_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;..&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;problem_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ProblemSpace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;real&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;forall&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt;
     &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// ...&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;communications&quot;&gt;Communications&lt;/h3&gt;

&lt;p&gt;Julia has explicit support for &lt;a href=&quot;https://en.wikipedia.org/wiki/Communicating_sequential_processes&quot;&gt;CSP-style&lt;/a&gt;
channels, like &lt;code&gt;go&lt;/code&gt;, which are something like a cross between queues and futures; they can keep being written to from multiple
tasks:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;&lt;span class=&quot;nd&quot;&gt;@everywhere&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt; putmsg&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pid&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;mypid&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;myid&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;msg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Hi from &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;mypid&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;RemoteChannel&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pid&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;put!&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rr&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;msg&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;myid&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; sent &quot;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;msg&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; to &quot;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pid&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rr&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;

&lt;span class=&quot;nd&quot;&gt;@everywhere&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt; getmsg&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rr&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;msg&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fetch&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rr&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;myid&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; got: &quot;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;msg&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;rr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;remotecall_fetch&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;putmsg&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;remotecall_wait&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getmsg&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rr&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Chapel, by contrast, doesn’t expose these methods; communications
is done implicitly through remote data access or remote code
invocation.&lt;/p&gt;

&lt;h2 id=&quot;a-2d-advection-problem&quot;&gt;A 2d advection problem&lt;/h2&gt;

&lt;p&gt;Having seen the parallel computing tools available in each language,
we try here a simple distributed computation.  Here we try Julia,
Chapel, and Python using &lt;a href=&quot;http://dask.pydata.org/en/latest/&quot;&gt;Dask&lt;/a&gt;
on a simple distributed-memory stencil problem, two dimensional
upwinded advection.  A Gaussian blob is advected by a constant
velocity field; shown below is the initial condition, the blob moved
slightly after a few timesteps, and the difference.&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;2D Advection Plot&quot; src=&quot;https://www.dursi.ca/assets/julia_v_chapel/twod_advection.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We do this in Julia using DistributedArrays, in Chapel using Stencil-distributed
arrays, and in Python using Dask arrays.  The relevant code snippets follow below.&lt;/p&gt;

&lt;table style=&quot;border: 1px solid black;&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Julia&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;&lt;span class=&quot;nd&quot;&gt;@everywhere&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt; get_data_plus_gc&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;domain&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nguard&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ngrid&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;myid&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;procs&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;domain&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;li&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;localindexes&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;domain&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;lp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;localpart&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;domain&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lp&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;data_plus_gc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nguard&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nguard&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;data_plus_gc&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nguard&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nguard&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lp&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;xstart&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;li&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;xend&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;li&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ystart&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;li&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;yend&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;li&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nguard&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;xsg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xstart&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ngrid&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ngrid&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;xeg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xend&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ngrid&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nguard&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nguard&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;data_plus_gc&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nguard&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;domain&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xsg&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nguard&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ystart&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;data_plus_gc&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nguard&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;domain&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xeg&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nguard&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ystart&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;

            &lt;span class=&quot;c&quot;&gt;#...&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data_plus_gc&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;

&lt;span class=&quot;nd&quot;&gt;@everywhere&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt; advect_data&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dens&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nguard&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ngrid&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;velx&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vely&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;locdens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_data_plus_gc&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dens&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nguard&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ngrid&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;#...calculate gradients on locdens&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nguard&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ny&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nguard&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nguard&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nguard&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;localpart&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dens&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nguard&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nguard&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;velx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gradx&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vely&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grady&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#...&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt; timestep&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dens&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nguard&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ngrid&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;velx&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vely&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;procs&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dens&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;refs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;nd&quot;&gt;@spawnat&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;advect_data&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dens&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nguard&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ngrid&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;velx&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vely&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ps&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pmap&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fetch&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;refs&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#...&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Chapel&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c&quot;&gt;&lt;span class=&quot;c1&quot;&gt;//...&lt;/span&gt;

  &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ProblemSpace&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;..&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ngrid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;..&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ngrid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ProblemDomain&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;domain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dmapped&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Stencil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;boundingBox&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ProblemSpace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fluff&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nguard&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nguard&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;periodic&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ProblemSpace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;//...&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dens&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ProblemDomain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;real&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;// density a gaussian of width sigma centred on (initialposx, initialposy)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;forall&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ij&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ProblemSpace&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ij&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ngrid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ij&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ngrid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ij&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;initialposx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;initialposy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

  &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iteration&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;..&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ntimesteps&lt;/span&gt;  &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// update the boundary conditions - periodic&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;updateFluff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;// calculate the upwinded gradient&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// ...&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;dens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;velx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gradx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vely&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grady&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;//...&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Python + Dask&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;c1&quot;&gt;#...
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;dask_step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subdomain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nguard&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
    map_overlap applies a function to a subdomain of a dask array,
    filling the guardcells in first
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subdomain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;map_overlap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;advect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;depth&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nguard&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;boundary&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'periodic'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                 &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;initial_conditions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;initial_posx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;initial_posy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;xx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;meshgrid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;density&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;initial_posx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;initial_posy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;density&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;__main__&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;#...
&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;initial_conditions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;subdomain_init&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;da&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;chunks&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;npts&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;npts&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# These create the steps, but they don't actually perform the execution...
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;subdomain&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dask_step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subdomain_init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nsteps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;subdomain&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dask_step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subdomain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# _this_ performs the execution
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;As with the stream benchmark, we see that the Julia DistributedArrays
require a lot of bookkeeping to use; both Chapel and Dask are much
more straightforward.&lt;/p&gt;

&lt;p&gt;The one-node timings here aren’t even close.  By forcing Chapel to run
on each core separately, the performance isn’t that different than Julia.
But when informed that there is one “locale” and letting
it sort out the details, Chapel benefits dramatically from being
able to use multiple levels of parallelism, and with no extra work;
on a single 8-processor node, running a 1000x1000 grid with all cores
takes the following amount of time:&lt;/p&gt;

&lt;table style=&quot;border: 1px solid black; margin: 0 auto; border-collapse: collapse;&quot;&gt;
&lt;thead&gt;
&lt;th&gt;Julia -p=1&lt;/th&gt;&lt;th&gt;Julia -p=8&lt;/th&gt;&lt;th&gt;Chapel -nl=1 ParTasksPerLocale=8&lt;/th&gt;&lt;th&gt;Chapel -nl=8 ParTasksPerLocale=1&lt;/th&gt;&lt;th&gt;Python&lt;/th&gt;
&lt;/thead&gt;
&lt;tbody style=&quot;border: 1px solid black;&quot;&gt;
&lt;tr&gt;
&lt;td style=&quot;border: 1px solid black;&quot;&gt;177s s&lt;/td&gt;
&lt;td style=&quot;border: 1px solid black;&quot;&gt;264 s&lt;/td&gt;
&lt;td style=&quot;border: 1px solid black;&quot;&gt;&lt;b&gt;0.4 s&lt;/b&gt;&lt;/td&gt;
&lt;td style=&quot;border: 1px solid black;&quot;&gt;145 s&lt;/td&gt;
&lt;td style=&quot;border: 1px solid black;&quot;&gt;193 s&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The 0.4s is not a typo. Threading matters.  Admittedly,
this is a bit of an extreme case, 1000x1000 isn’t a big
grid to distribute over 8 processes, so communications
overhead dominates; Julia seems to suffer that overhead even
with just one process.&lt;/p&gt;

&lt;p&gt;Another interesting thing here is that Python+Numpy+Dask (numba didn’t
help here) is competitive even with Chapel &lt;em&gt;if&lt;/em&gt; you force Chapel
to not use threading on-node, and either made it much easier to
write the program than Julia.&lt;/p&gt;

&lt;h2 id=&quot;strengths-weaknesses-and-future-prospects&quot;&gt;Strengths, Weaknesses, and Future Prospects&lt;/h2&gt;

&lt;p&gt;Both Julia and Chapel are perfectly useable today for problems that
fall within their current bailiwicks, at least for advanced users.
They are strong projects and interesting technologies.  In addition,
both have significant potential and “room to grow” beyond their
current capabilities; but both face challenges as well.&lt;/p&gt;

&lt;h3 id=&quot;julia-1&quot;&gt;Julia&lt;/h3&gt;

&lt;p&gt;Julia’s great flexibility - the metaprogramming and the type system
in particular - gives it a very real opportunity to become a platform
on which many domanin-specific language are written for particular scientific problems.
We see some of that potential in tools like &lt;a href=&quot;https://github.com/JuliaDiffEq/DifferentialEquations.jl&quot;&gt;DifferentialEquations.jl&lt;/a&gt;,
where a simple, general API can nonetheless be used to provide efficient
solutions to problems that span a wide range of regimes and structures;
the &lt;code&gt;solve()&lt;/code&gt; function and the problem definition language essentially
becomes a DSL for a wide range of differential equation problems.
And Julia’s interactive and dynamic nature makes it a natural for 
scientists noodling around on problems, performing numerical
experiments and looking at the results.  While large-scale computing
— in an HPC or Spark-style Big-data sense — is not a forte of
Julia’s right now, the basic pieces are there and it certainly could
be in the future.&lt;/p&gt;

&lt;p&gt;Many of Julia’s disadvantages are inevitable flip sides of some of
those advantages.  Because of the dynamic nature of
the language and its reliance on JIT and type inference, it is
&lt;a href=&quot;https://discourse.julialang.org/t/julia-static-compilation/296/27&quot;&gt;still not
possible&lt;/a&gt;
to fully compile a Julia script into a static executable, meaning
that there will be JIT pauses in initial iterations of running code;
too, the dynamic nature of the language relies on garbage collection,
which can cause either GC pauses (and thus jitter at scale) or
unexpected memory pressure throughout execution.  Similarly, the
fact that it’s so easy to contribute a package to the Julia package
ecosystem means that the package listing is littered with abandoned
and broken packages.&lt;/p&gt;

&lt;p&gt;But some of the disadvantages seem more self-inflicted.  While the
language has been public and actively developed for &lt;a href=&quot;https://julialang.org/blog/2012/02/why-we-created-julia&quot;&gt;over five
years&lt;/a&gt;,
the language is still at v0.6.  While any language will evolve over
time, the Julia community has spent the past five years contininually
re-litigating minor but foundational decisions of syntax and behaviour
in the interests of conceptual purity – v0.4 in late 2015 changed
the capitalization of unsigned integer types and radically changed
the dictionary syntax, while 0.5 in late 2016 dramatically (although
less dramatically than originally proposed after community pushback)
changed the behaviour of arrays (!!) in an event termed the
Arraypocolypse.  Discussions on the correct choice for string
concatenation operator span enormous and non-stop github issue
discussions from late 2012 to mid 2015.  At least one more round
of significant breaking changes are planned before a 1.0 release.
As a result, most non-trivial example code online simply doesn’t
work; thus also the accelerated bitrot of software in the Julia
package listing.  It has been difficult to implement new functionality
on top of base Julia; it’s hard to build powerful parallel computing
tools when one can’t even depend on the behavour of arrays.
I would have liked to use Intel’s ParallelAccelerator for Julia to
see how it worked on the advection problem above, for instance, but Julia 0.6
breaks the ParallelAccelerator, and Julia 0.6 is needed for the &lt;code&gt;@simd&lt;/code&gt;
feature with DistributedArrays.&lt;/p&gt;

&lt;p&gt;So Julia living up to its potential is not a given.  If I were on
Julia’s project team, things that would concern me would include:&lt;/p&gt;

&lt;dl&gt;
  &lt;dt&gt;&lt;strong&gt;Peak Julia?&lt;/strong&gt;&lt;/dt&gt;
  &lt;dd&gt;Julia grew very quickly early on, but since then seems to have topped out;
for example, &lt;a href=&quot;https://g.co/trends/qzmA9&quot;&gt;flat google trends interest&lt;/a&gt;,
and falling off the radar of “languages to watch” lists such as the
&lt;a href=&quot;http://redmonk.com/sogrady/2017/03/17/language-rankings-1-17/&quot;&gt;Redmonk language rankings&lt;/a&gt;,
This may be unfair; these trends may say more about the large initial
surge of interest than stagnation or decline.  “A hugely popular
scientific programing language” almost seems like an oxymoron, after all.
&lt;a href=&quot;https://insights.stackoverflow.com/trends?tags=julia-lang&quot;&gt;Declining Stack Overflow&lt;/a&gt; 
interest may simply reflect that the community has successfully moved discussion
to its &lt;a href=&quot;https://discourse.julialang.org&quot;&gt;discourse&lt;/a&gt; site.
A five-year old language for numerical computing that still hasn’t
reached 1.0 but has popularity comparable to Rust (which started
at the same time but is a more general systems-programming language)
or Fortran (which has an enormous installed base) is pretty remarkable;
further growth may inevitably be more modest simply because of the
small number of scientific programmers out there.  Still, I think
one would want to see interest growing ahead of a 1.0 release,
rather than flat or declining.&lt;/dd&gt;
  &lt;dt&gt;&lt;strong&gt;Instability driving off users, developers&lt;/strong&gt;&lt;/dt&gt;
  &lt;dd&gt;Very early on, community members who used Julia started building
what became &lt;a href=&quot;http://juliastats.github.io&quot;&gt;JuliaStats&lt;/a&gt;, with R-like
data frames, data tables, random distributions, and a growing number
of statistics and machine-learning tools built atop.  This took
significant developer effort, as fundamental to statistical use
cases is “Not Available” or “NA” values, with semantics different
from the NaNs that we in the simulation computing community are so
frequently (if usually unintentionally) familar with.  Thus dataframes
and tables couldn’t simply be built directly on top of numerical
arrays of basic numerical types, but took some effort to build
efficient “nullable” types atop of.  But partly because of instability
in the underlying language, Julia DataFrames and DataArrays have
themselves been under flux, which is show-stopping to R users
considering Julia, and demoralizing to developers.  Many other similar
examples exist in other domains.  If it is true that there is
declining or stagnant interest in Julia, this would certainly be a
contributing factor.&lt;/dd&gt;
  &lt;dt&gt;&lt;strong&gt;The JIT often needs help, even for basic numerical computing tasks&lt;/strong&gt;&lt;/dt&gt;
  &lt;dd&gt;Julia is designed around its JIT compiler, which enables some
of the language’s very cool features - the metaprogramming, the
dynamic nature of the language, the interactivity.  But the JIT
compiler often needs a lot of help to get reasonable performance,
such as use of the the &lt;code&gt;@inbounds&lt;/code&gt; macro in the stencil calculation.
Writing numerical operations in the more readable
vectorized form (like for the stream example in Chapel, &lt;code&gt;C = A + B&lt;/code&gt;
rather than looping over the indices) &lt;a href=&quot;http://www.johnmyleswhite.com/notebook/2013/12/22/the-relationship-between-vectorized-and-devectorized-code/&quot;&gt;has long been slow in Julia&lt;/a&gt;,
although &lt;a href=&quot;https://julialang.org/blog/2017/01/moredots&quot;&gt;a new feature&lt;/a&gt;
may have fixed that.  &lt;a href=&quot;http://parallelacceleratorjl.readthedocs.io/en/latest/index.html&quot;&gt;A third party package&lt;/a&gt;
exists which helps many of the common cases (speeding up stencil
operations on rectangular arrays), which on one hand indicates the
power of Julia metaprogramming capabilities.  But on the other, one
might naturally think that fast numerical operations on arrays would
be something that the core language came with.  Part of the problem here
is that while the Julia ecosystem broadly has a very large number of
contributors, the core language internals (like the JIT itself) 
has only a handful, and complex issues like performance problems
can take a very long time to get solved.&lt;/dd&gt;
  &lt;dt&gt;&lt;strong&gt;The 800lb pythonic gorilla&lt;/strong&gt;&lt;/dt&gt;
  &lt;dd&gt;Python is enormously popular in scientific and data-science type
applications, has huge installed base and number of packages, and
with &lt;a href=&quot;http://www.numpy.org&quot;&gt;numpy&lt;/a&gt; and &lt;a href=&quot;http://numba.pydata.org&quot;&gt;numba&lt;/a&gt;
can be quite fast.  The scientific computing community is now 
grudgingly starting to move to Python 3, and with Python 3.5+ 
supporting &lt;a href=&quot;https://docs.python.org/3/library/typing.html&quot;&gt;type annotations&lt;/a&gt;,
I think there’d start to be a quite real concern that Python would get
Julia-fast (or close enough) before Julia got Python-big.  The fact
that some of Julia’s nicest features like notebook support and coolest new projects
like &lt;a href=&quot;https://github.com/JuliaParallel/Dagger.jl&quot;&gt;Dagger&lt;/a&gt; rely on
or are ports of work originally done for Python (ipython notebook
and &lt;a href=&quot;http://dask.pydata.org/en/latest/&quot;&gt;Dask&lt;/a&gt;) indicate the danger
if Python gets fast enough.&lt;/dd&gt;
&lt;/dl&gt;

&lt;p&gt;Of those four, only the middle two are completely under the Julia
team’s control; a v1.0 released soon, and with solemn oaths sworn
to have no more significant breaking changes until v2.0 would help
developers and users, and onboarding more people into core internals
development would help the underlying technology.&lt;/p&gt;

&lt;h3 id=&quot;chapel-1&quot;&gt;Chapel&lt;/h3&gt;

&lt;p&gt;If I were on the Chapel team, my concerns would be different:&lt;/p&gt;

&lt;dl&gt;
  &lt;dt&gt;&lt;strong&gt;Adoption&lt;/strong&gt;&lt;/dt&gt;
  &lt;dd&gt;It’s hard to escape the fact that Chapel’s user base is very
small.  The good news is that Chapel’s niche, unlike Julia’s, has
no serious immediate competitor — I’d consider other productive
parallel scientific programming languages to be more research
projects than products — which gives it a bit more runway.  But
the niche itself is small, and Chapel’s modest adoption rate within
that niche needs to be addressed in the near future if the language
is to thrive.  The Chapel team is doing many of the right things
—  the package is easy to install (no small feat for a performant
parallel programming language); the compiler is getting faster and
producing faster code; there’s lots of examples, tutorials and
documentation available; and the community is extremely friendly
and welcoming — but it seems clear that users need to be given
more reason to start trying the language.&lt;/dd&gt;
  &lt;dt&gt;&lt;strong&gt;Small number of external contributors&lt;/strong&gt;&lt;/dt&gt;
  &lt;dd&gt;Admittedly, this is related to the fact that the number of users
is small, but it’s also the case that contributing code is nontrivial
if you want to contribute it to the main project, and there’s no central
place where other people could look for your work if you wanted to have
it as an external package.  A package manager would be a real help, 
and it doesn’t have to be elaborate (especially in the initial version).&lt;/dd&gt;
  &lt;dt&gt;&lt;strong&gt;Not enough packages&lt;/strong&gt;&lt;/dt&gt;
  &lt;dd&gt;In turn, this is caused by the small number of external contributors,
and helps keep the adoption low.  Chapel already has the fundamentals
to start building some really nice higher-level packages and solvers
that would make it easy to start writing some types of scientific
codes.  A distributed-memory n-dimensional FFT over one of its
domains; the beginnings of a Chapel-native set of solvers from
&lt;a href=&quot;http://www.netlib.org/scalapack/&quot;&gt;Scalapack&lt;/a&gt; or
&lt;a href=&quot;http://www.mcs.anl.gov/petsc/index.html&quot;&gt;PETSc&lt;/a&gt; (both of which are
notoriously hard to get started with, and in PETSc’s case, even
install); simple static-sized R-style dataframes with some analysis
routines; these are tools which would make it very easy to get
started writing some non-trivial scientific software in Chapel.&lt;/dd&gt;
  &lt;dt&gt;&lt;strong&gt;Too few domain maps and layouts&lt;/strong&gt;&lt;/dt&gt;
  &lt;dd&gt;Being able to, in a few lines of code, write performant, threaded,
NUMA-aware, and distributed memory operations on statically-decomposed
rectangular multidimensional arrays, and have that code work on a
cluster or your desktop is amazing.  But many scientific problems
do not match neatly onto these domains.  Many require dynamically-sized
domains (block-adaptive meshes) or load balancing (tree codes,
dynamic hash tables); others may be static but not quite look like
CSR-sparse arrays.  Domain maps, layouts, and the parallel iterators
which loop over them are the “secret sauce” of Chapel, and can be
written in user code if the underlying capabilities they need are
supported, so they can be contributed externally, but there is little
documention/examples (compared to that on using existing domain maps) available.&lt;/dd&gt;
&lt;/dl&gt;

&lt;p&gt;The good news is that these items are all under the Chapel community’s
control.  Programs that are natural to write in Chapel currently are
easy to write and can perform quite well; the goal then is to expand
the space of those programs by leveraging early adopters into writing
packages.&lt;/p&gt;

&lt;h2 id=&quot;my-conclusions&quot;&gt;My conclusions&lt;/h2&gt;

&lt;p&gt;This is entitled “&lt;em&gt;My&lt;/em&gt; conclusions” because my takeaways might reasonably be
different than yours.  Here’s my take.&lt;/p&gt;

&lt;h3 id=&quot;both-projects-are-strong-and-useable-right-now-at-different-things&quot;&gt;Both projects are strong and useable, right now, at different things&lt;/h3&gt;

&lt;p&gt;I’d have no qualms about recommending Chapel to someone who wanted
to tackle computations on large distributed rectangular arrays,
dense or sparse, or Julia for someone who had a short-lived project
and wanted something interactive and requiring only single-node or
coordinator-worker computations (or patterns that were more about
concurrency than parallelism).  Julia also seems like a good choice for
prototyping a DSL for specific scientific problems.&lt;/p&gt;

&lt;p&gt;Neither project is really a competitor for the other; for Julia the
nearest competitor is likely the Python ecosystem, and for Chapel
it would be status quo (X + MPI + OpenMP/OpenACC) or that people
might try investigating a research project or start playing with
Spark (which is good at a lot of things, but not really scientific
simulation work.)&lt;/p&gt;

&lt;p&gt;Scientific computing communities are very wary of new technologies
(it took 10+ years for Python to start getting any traction), with
the usual, self-fulfulling, fear being “what if it goes away”.  I
don’t think there’s any concern about dead code here for projects
that are started with either.  Chapel will be actively supported
for another couple of years at least, and the underlying tools (like
GASNet) underpin many projects and aren’t going anywhere.  One’s
code wouldn’t be “locked into” Chapel at any rate, as there are MPI
bindings, so that there’s always a path to incrementally port your
code back to MPI if you chose to.  For Julia, the immediate worry
is less about lack of support and more that the project might be
&lt;em&gt;too&lt;/em&gt; actively maintained; that one would have to continually exert
effort to catch your code up with the current version.  In either
case, there are clear paths to follow (porting or upgrading) to
keep your code working.&lt;/p&gt;

&lt;h3 id=&quot;both-projects-have-as-yet-untapped-potential&quot;&gt;Both projects have as-yet untapped potential&lt;/h3&gt;

&lt;p&gt;What’s exciting about both of these projects is how far they could
go.  Chapel already makes certain class of MPI+OpenMP type programs
extremely simple to write with fairly good performance; if that
class of programs expands (either through packages built atop of
current functionality, or expanded functionality through additional
well-supported domain maps) and as performance continues to improve,
it could make large-scale scientific computation accessible to a
much broader community of scientists (and thus science).&lt;/p&gt;

&lt;p&gt;Julia has the same potential to broaden computational science on
the desktop, and (at least in the near term) for computations
requiring only minimal communication like coordinator-worker computations.
But Python is already doing this, and making suprising inroads on
the distributed-memory computing front, and there will be something of a
race to see which gets there first.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;Yes, I said it.  Offsets into buffers can begin at 0, sure, but indices into mathematical objects begin at 1; anything else is madness.  Also: oxford comma, two spaces after a period, and vi are all the correct answers to their respective questions. &lt;a class=&quot;reversefootnote&quot; href=&quot;https://www.dursi.ca/feed.xml#fnref:1&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;

    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;“Do the right thing” isn’t free, however; as with matlab or numpy, when combining objects of different shapes or sizes, the “right thing” can be a bit suprising unless one is very familiar with the tool’s &lt;a href=&quot;https://docs.julialang.org/en/stable/manual/arrays/?highlight=broadcasting#broadcasting&quot;&gt;broadcasting rules&lt;/a&gt; &lt;a class=&quot;reversefootnote&quot; href=&quot;https://www.dursi.ca/feed.xml#fnref:2&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;

    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Reviewing the state of the art of burst buffers</title>
   <link href="https://hpc.social/2017/reviewing-the-state-of-the-art-of-burst-buffers/"/>
   <updated>2017-03-13T01:07:00-06:00</updated>
   <id>https://hpc.social/2017/reviewing-the-state-of-the-art-of-burst-buffers</id>
   <content type="html">&lt;!-- &lt;div style=&quot;border-left: 3px solid #32aaff; border: 1px solid #32aaff; float: right; font-size: x-small; margin-left: 6px; padding: 6px; width: 250px;&quot;&gt;If you're interested in burst buffers and happen to be a student, please reach out and contact me!  We have an &lt;a href=&quot;https://lbl.taleo.net/careersection/2/jobdetail.ftl?job=83459&quot;&gt;internship opportunity in performance analysis of our 1.8 PB/1.5 TB/sec burst buffer&lt;/a&gt; for students of all levels of experience.&lt;/div&gt;
 --&gt;
&lt;p&gt;Just over two years ago I attended my first DOE workshop as a guest representative of the NSF supercomputing centers, and I wrote a post that summarized my key observations of how the DOE was approaching the increase in data-intensive computing problems.  At the time, the most significant thrusts seemed to be&lt;br /&gt;&amp;lt;ol&amp;gt;&amp;lt;li&amp;gt;understanding scientific workflows to keep pace with the need to process data in complex ways&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;deploying burst buffers to overcome the performance limitations of spinning disk relative to the increasing scale of simulation data&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;developing methods and processes to curate scientific data&amp;lt;/li&amp;gt;&amp;lt;/ol&amp;gt;Here we are now two years later, and these issues still take center stage in the discussion surrounding the future of  data-intensive computing.  The DOE has made significant progress in defining its path forward in these areas though, and in particular, both the roles of burst buffers and scientific workflows have a much clearer focus on DOE’s HPC roadmap.  Burst buffers in particular are becoming a major area of interest since they are now becoming commercially available, so in the interests of updating some of the incorrect or incomplete thoughts I wrote about two years ago, I thought I’d write about the current state of the art in burst buffers in HPC.&lt;br /&gt;&lt;br /&gt;Two years ago I had observed that there were two major camps in burst buffer implementations: one that is more tightly integrated with the compute side of the platform that utilizes explicit allocation and use, and another that is more closely integrated with the storage subsystem and acts as a transparent I/O accelerator.  Shortly after I made that observation though, Oak Ridge and Lawrence Livermore announced their GPU-based leadership systems, Summit and Sierra, which would feature a new type of burst buffer design altogether that featured on-node nonvolatile memory.&lt;br /&gt;&lt;br /&gt;This CORAL announcement, combined with the deployment of production, large-scale burst buffers at &lt;a href=&quot;http://www.nersc.gov/news-publications/nersc-news/nersc-center-news/2015/early-users-to-test-new-burst-buffer-on-cori/&quot;&gt;NERSC&lt;/a&gt;, &lt;a href=&quot;http://permalink.lanl.gov/object/tr?what=info:lanl-repo/lareport/LA-UR-15-27819&quot;&gt;Los Alamos&lt;/a&gt;, and &lt;a href=&quot;https://www.hpc.kaust.edu.sa/content/datawarp-burst-buffer-0&quot;&gt;KAUST&lt;/a&gt;, has led me to re-think my taxonomy of burst buffers.  Specifically, it really is important to divide burst buffers into their hardware architectures and software usage modes; different burst buffer architectures can provide the same usage modalities to users, and different modalities can be supported by the same architecture.&lt;br /&gt;&amp;lt;div&amp;gt;&lt;br /&gt;&amp;lt;/div&amp;gt;
For the sake of laying it all out, let’s walk through the taxonomy of &lt;i&gt;burst buffer hardware architectures&lt;/i&gt; and &lt;i&gt;burst buffer software usage modalities&lt;/i&gt;.&lt;br /&gt;&lt;br /&gt;&amp;lt;h2&amp;gt;Burst Buffer Hardware Architectures&amp;lt;/h2&amp;gt;First, consider your typical medium- or large-scale HPC system architecture &lt;i&gt;without&lt;/i&gt; a burst buffer:&lt;br /&gt;&lt;br /&gt;&amp;lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&amp;gt;&lt;a href=&quot;https://4.bp.blogspot.com/-3ETIppfFZVU/WMWRxNCWvSI/AAAAAAAAo7Y/qXtIJNn2LvQf-oyMSA-t3m2zQ7M7MeAPgCLcB/s1600/architecture-baseline.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;162&quot; src=&quot;https://4.bp.blogspot.com/-3ETIppfFZVU/WMWRxNCWvSI/AAAAAAAAo7Y/qXtIJNn2LvQf-oyMSA-t3m2zQ7M7MeAPgCLcB/s320/architecture-baseline.png&quot; width=&quot;320&quot; /&gt;&lt;/a&gt;&amp;lt;/div&amp;gt;
&lt;br /&gt;In this design, you have&lt;br /&gt;&lt;br /&gt;&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;&lt;b&gt;Compute Nodes (CN)&lt;/b&gt;, which might be commodity whitebox nodes like the &lt;a href=&quot;https://www.nextplatform.com/2015/06/24/hyperscale-systems-make-headway-into-hpc/&quot;&gt;Dell C6320 nodes in SDSC’s Comet system&lt;/a&gt; or Cray XC compute blades&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&lt;b&gt;I/O Nodes (ION)&lt;/b&gt;, which might be commodity Lustre LNET routers (commodity clusters), &lt;a href=&quot;http://docs.cray.com/PDF/XC_Series_DVS_Administration_Guide_CLE60UP01.pdf&quot;&gt;Cray DVS nodes&lt;/a&gt; (Cray XC), or &lt;a href=&quot;http://glennklockwood.com/data-intensive/storage/io-forwarding.html#ciod-blue-gene-s-i-o-forwarder&quot;&gt;CIOD forwarders&lt;/a&gt; (Blue Gene)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&lt;b&gt;Storage Nodes (SN)&lt;/b&gt;, which might be Lustre Object Storage Servers (OSSes) or GPFS Network Shared Disk (NSD) servers&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&lt;b&gt;The compute fabric&lt;/b&gt; (blue lines), which is typically Mellanox InfiniBand, Intel OmniPath, or Cray Aries&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&lt;b&gt;The storage fabric&lt;/b&gt; (red lines), which is typically Mellanox InfiniBand or Intel OmniPath&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;br /&gt;Given all these parts, there are a bunch of different places you can stick flash devices to create a burst buffer.  For example…&lt;br /&gt;&lt;br /&gt;&amp;lt;h3&amp;gt;ION-attached Flash&amp;lt;/h3&amp;gt;You can put SSDs inside IO nodes, resulting in an &lt;b&gt;ION-attached flash architecture&lt;/b&gt; that looks like this:&lt;br /&gt;&lt;br /&gt;&amp;lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&amp;gt;&lt;a href=&quot;https://2.bp.blogspot.com/-jc5J5bDY5RU/WMWU6URyljI/AAAAAAAAo7k/weeYZm3yRR0VFuD1dOsGnHv8DIEWP1aMQCLcB/s1600/architecture-on-ion.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;162&quot; src=&quot;https://2.bp.blogspot.com/-jc5J5bDY5RU/WMWU6URyljI/AAAAAAAAo7k/weeYZm3yRR0VFuD1dOsGnHv8DIEWP1aMQCLcB/s320/architecture-on-ion.png&quot; width=&quot;320&quot; /&gt;&lt;/a&gt;&amp;lt;/div&amp;gt;
&lt;br /&gt;Gordon, which was the &lt;a href=&quot;https://www.slideshare.net/glennklockwood/the-protoburst-buffer-experience-with-the-flashbased-file-system-on-sdscs-gordon&quot;&gt;first large-scale deployment of what one could call a burst buffer&lt;/a&gt;, had this architecture.  The flash was presented to the compute nodes as block devices using iSCSI, and a compute node could have anywhere between zero and &lt;a href=&quot;https://kb.iu.edu/d/bcua&quot;&gt;sixteen SSDs&lt;/a&gt; mounted to it entirely via software.  More recently, the Tianhe-2 system at NUDT also deployed this architecture and exposes the flash to user applications via &lt;a href=&quot;https://link.springer.com/article/10.1007/s11704-014-3499-6&quot;&gt;their H&lt;sup&gt;2&lt;/sup&gt;FS middleware&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;&amp;lt;h3&amp;gt;Fabric-attached Flash&amp;lt;/h3&amp;gt;A very similar architecture is to add specific burst buffer nodes on the compute fabric that &lt;i&gt;don’t&lt;/i&gt; route I/O, resulting in a &lt;b&gt;fabric-attached flash architecture&lt;/b&gt;:&lt;br /&gt;&lt;br /&gt;&amp;lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&amp;gt;&lt;a href=&quot;https://1.bp.blogspot.com/-Q5-lIwe8-UE/WMWZ8xgzkKI/AAAAAAAAo70/9OEOYVKanBY3z8r1nOE1bKbG84d3pu63wCLcB/s1600/architecture-on-edge.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;180&quot; src=&quot;https://1.bp.blogspot.com/-Q5-lIwe8-UE/WMWZ8xgzkKI/AAAAAAAAo70/9OEOYVKanBY3z8r1nOE1bKbG84d3pu63wCLcB/s320/architecture-on-edge.png&quot; width=&quot;320&quot; /&gt;&lt;/a&gt;&amp;lt;/div&amp;gt;
Like the ION-attached flash design of Gordon, the flash is still embedded within the compute fabric and is logically closer to the compute nodes than the storage nodes.  &lt;a href=&quot;https://cug.org/proceedings/cug2016_proceedings/includes/files/pap105-file2.pdf&quot;&gt;Cray’s DataWarp solution uses this architecture&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;Because the flash is still on the compute fabric, this design is very similar to ION-attached flash and the decision to chose it over the ION-attached flash design is mostly non-technical.  It can be more economical to embed flash directly in I/O nodes if those nodes have enough peripheral ports (or physical space!) to support the NICs for the compute fabric, the NICs for the storage fabric, and the flash devices.  However as flash technology moves away from being attached via SAS and towards being directly attached to PCIe, it becomes more difficult to stuff that many high-performance peripherals into a single box without imbalancing something.  As such, it is likely that fabric-attached flash architectures will replace ION-attached flash going forward.&lt;br /&gt;&lt;br /&gt;Fortunately, any burst buffer software designed for ION-attached flash designs will also probably work on fabric-attached flash designs just fine.  The only difference is that the burst buffer software will no longer have to compete against the I/O routing software for on-node resources like memory or PCIe bandwidth.&lt;br /&gt;&lt;br /&gt;&amp;lt;h3&amp;gt;CN-attached Flash&amp;lt;/h3&amp;gt;A very different approach to building burst buffers is to attach a flash device to every single compute node in the system, resulting in a &lt;b&gt;CN-attached flash architecture&lt;/b&gt;:&lt;br /&gt;&lt;br /&gt;&amp;lt;div&amp;gt;&amp;lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&amp;gt;&lt;a href=&quot;https://2.bp.blogspot.com/-lL1iGUOJOg4/WMWjk_pBKqI/AAAAAAAAo8I/Xd_3yi3-I0Usm_wnMswE8N18ciqMBmvZgCLcB/s1600/architecture-on-cn.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;183&quot; src=&quot;https://2.bp.blogspot.com/-lL1iGUOJOg4/WMWjk_pBKqI/AAAAAAAAo8I/Xd_3yi3-I0Usm_wnMswE8N18ciqMBmvZgCLcB/s320/architecture-on-cn.png&quot; width=&quot;320&quot; /&gt;&lt;/a&gt;&amp;lt;/div&amp;gt;
&lt;br /&gt;This design is neither superior nor inferior to the ION/fabric-attached flash design.  The advantages it has over ION/fabric-attached flash include&lt;br /&gt;&lt;br /&gt;&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;&lt;b&gt;Extremely high peak I/O performance&lt;/b&gt; -The peak performance scales linearly with the number of compute nodes, so the larger your job, the more performance your job can have.&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&lt;b&gt;Very low variation in I/O performance&lt;/b&gt; - Because each compute node has direct access to its locally attached SSD, contention on the compute fabric doesn’t affect I/O performance.&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;div&amp;gt;However, these advantages come at a cost:&amp;lt;/div&amp;gt;&lt;/p&gt;
&lt;div&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Limited support for shared-file I/O&lt;/b&gt; - &amp;nbsp;Because each compute node doesn't share its SSD with other compute nodes, having many compute nodes write to a single shared file is not a straightforward process. &amp;nbsp;The solution to this issue include from such N-1 style I/O being simply impossible (the default case), relying on &lt;a href=&quot;http://computation.llnl.gov/projects/scalable-checkpoint-restart-for-mpi&quot;&gt;I/O middleware like the SCR library&lt;/a&gt; to manage data distribution, or relying on &lt;a href=&quot;http://sc16.supercomputing.org/sc-archive/tech_poster/poster_files/post255s2-file2.pdf&quot;&gt;sophisticated I/O services like Intel CPPR&lt;/a&gt; to essentially journal all I/O to the node-local flash and flush it to the parallel file system asynchronously.&lt;/li&gt;&lt;li&gt;&lt;b&gt;Data movement outside of jobs becomes difficult&lt;/b&gt; - Burst buffers allow users to stage data into the flash &lt;i&gt;before&lt;/i&gt; their job starts and stage data back to the parallel file system &lt;i&gt;after&lt;/i&gt; their job ends. &amp;nbsp;However in CN-attached flash, this staging will occur while someone else's job might be using the node. &amp;nbsp;This can cause interference, capacity contention, or bandwidth contention. &amp;nbsp;Furthermore, it becomes very difficult to persist data on a burst buffer allocation across multiple jobs without flushing and re-staging it.&lt;/li&gt;&lt;li&gt;&lt;b&gt;Node failures become more problematic&lt;/b&gt; - The point of writing out a checkpoint file is to allow you to restart a job in case one of its nodes fails. &amp;nbsp;If your checkpoint file is actually stored on one of the nodes that failed, though, the whole checkpoint gets lost when a node fails. &amp;nbsp;Thus, it becomes critical to flush checkpoint files to the parallel file system as quickly as possible so that your checkpoint file is safe if a node fails. &amp;nbsp;Realistically though, most application failures are not caused by node failures; a study by LLNL found that &lt;a href=&quot;http://ieeexplore.ieee.org/document/5645453/&quot;&gt;85% of job interrupts do not take out the whole node&lt;/a&gt;.&lt;/li&gt;&lt;li&gt;&lt;b&gt;Performance cannot be decoupled from job size&lt;/b&gt; - Since you get more SSDs by requesting more compute nodes, there is no way to request only a few nodes and a lot of SSDs. &amp;nbsp;While this is less an issue for extremely large HPC jobs whose I/O volumes typically scale linearly with the number of compute nodes, data-intensive applications often have to read and write large volumes of data but cannot effectively use a huge number of compute nodes.&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;If you take a step back and look at what these strengths and weaknesses play to, you might be able to envision what sort of supercomputer design might be best suited for this type of architecture:&lt;/div&gt;
&lt;/div&gt;
&lt;div&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Relatively low node count&lt;/b&gt;, so that you aren't buying way more SSD capacity or performance than you can realistically use given the bandwidth of the parallel file system to which the SSDs must eventually flush&lt;/li&gt;&lt;li&gt;&lt;b&gt;Relatively beefy compute nodes&lt;/b&gt;, so that the low node count doesn't hurt you and so that you can tolerate running I/O services to facilitate the asynchronous staging of data and middleware to support shared-file I/O&lt;/li&gt;&lt;li&gt;&lt;b&gt;Relatively beefy network injection bandwidth&lt;/b&gt;, so that asynchronous stage in/out doesn't severely impact the MPI performance of the jobs that run before/after yours&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;There are also specific application workloads that are better suited to this CN-attached flash design:&lt;/div&gt;
&lt;ul&gt;&lt;li&gt;&lt;b&gt;Relatively large job sizes on average&lt;/b&gt;, so that applications routinely use enough compute nodes to get enough I/O bandwidth. &amp;nbsp;Small jobs may be better off using the parallel file system directly, since parallel file systems can usually deliver more I/O bandwidth to smaller compute node counts.&lt;/li&gt;&lt;li&gt;&lt;b&gt;Relatively low diversity of applications&lt;/b&gt;, so that any applications that rely on shared-file I/O (which is not well supported by CN-attached flash, as we'll discuss later) can either be converted into using the necessary I/O middleware like SCR, or can be restructured to use only file-per-process or not rely on any strong consistency semantics.&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;
&lt;div&gt;And indeed, if you look at the systems that are planning on deploying this type of CN-attached flash burst buffer in the near future, they all fit this mold. &amp;nbsp;In particular, the CORAL Summit and Sierra systems will be deploying these burst buffers at extreme scale, and before them, &lt;a href=&quot;https://twitter.com/ProfMatsuoka/status/837438733133754376&quot;&gt;Tokyo Tech's Tsubame 3.0&lt;/a&gt; will as well. &amp;nbsp;All of these systems derive the majority of their performance from GPUs, leaving the CPUs with the capacity to implement more functionality of their burst buffers in software on the CNs.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h3&gt;Storage Fabric-attached Flash&lt;/h3&gt;
&lt;div&gt;The last notable burst buffer architecture involves attaching the flash on the storage fabric rather than the compute fabric, resulting in SF-attached flash:&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-Eu5ZEFFdU4Q/WMWw2M_rJqI/AAAAAAAAo8c/y8twoMUx0h4cGUCTy0LPH9rkonVlW9gMwCLcB/s1600/architecture-backend.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;155&quot; src=&quot;https://3.bp.blogspot.com/-Eu5ZEFFdU4Q/WMWw2M_rJqI/AAAAAAAAo8c/y8twoMUx0h4cGUCTy0LPH9rkonVlW9gMwCLcB/s320/architecture-backend.png&quot; width=&quot;320&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;This is not a terribly popular design because&lt;/div&gt;
&lt;div&gt;&lt;ol&gt;&lt;li&gt;it moves the flash far away from the compute node, which is counterproductive to low latency&lt;/li&gt;&lt;li&gt;it requires that the I/O forwarding layer (the IONs) support enough bandwidth to saturate the burst buffer, which can get expensive&lt;/li&gt;&lt;/ol&gt;&lt;div&gt;However, for those HPC systems with custom compute fabrics that are not amenable to adding third-party burst buffers, this may be the only possible architecture. &amp;nbsp;For example, the Argonne Leadership Computing Facility has deployed a &lt;a href=&quot;http://files.gpfsug.org/presentations/2016/anl-june/ESS_GPFSUG.pdf&quot;&gt;high-performance GPFS file system as a burst buffer&lt;/a&gt; alongside their high-capacity GPFS file system in this fashion because it is impractical to integrate flash into their Blue Gene/Q's proprietary compute fabric. &amp;nbsp;Similarly, sites that deploy DDN's Infinite Memory Engine burst buffer solution on systems with proprietary compute fabrics (e.g., Cray Aries on Cray XC) will have to deploy their burst buffer nodes on the storage fabric.&lt;/div&gt;
&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h2&gt;Burst Buffer Software&lt;/h2&gt;
&lt;div&gt;Ultimately, all of the different burst buffer architectures still amount to sticking a bunch of SSDs into a supercomputing system, and if this was all it took to make a burst buffer though, burst buffers wouldn't be very interesting. &amp;nbsp;Thus, there is another half of the burst buffer ecosystem: the software and middleware that transform a pile of flash into an I/O layer that applications can actually use productively.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;In the absolute simplest case, this software layer can just be an XFS file system atop RAIDed SSDs that is presented to user applications as node-local storage. &amp;nbsp;And indeed, this is what SDSC's Gordon system did; for many workloads such as file-per-process I/O, it is a suitable way to get great performance. &amp;nbsp;However, as commercial vendors have gotten into the burst buffer game, they have all started using this software layer to differentiate their burst buffer solutions from their competitors'. &amp;nbsp;This has resulted in modern burst buffers now having a lot of functionality that allow users to do interesting new things with their I/O.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;Because this burst buffer differentiation happens entirely in software, it should be no surprise that these burst buffer software solutions look a lot like the software-defined storage products being sold in the enterprise cloud space. &amp;nbsp;The difference is that burst buffer software can be optimized specifically for HPC workloads and technologies, resulting in much nicer and accessible ways in which they can be used by HPC applications.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h3&gt;Common Software Features&lt;/h3&gt;
&lt;div&gt;Before getting too far, it may be helpful to enumerate the features common to many burst buffer software solutions:&lt;/div&gt;
&lt;div&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Stage-in and stage-out&lt;/b&gt; - Burst buffers are designed to make a job's input data already be available on the burst buffer immediately when the job starts, and to allow the flushing of output data to the parallel file system after the job ends. &amp;nbsp;To make this happen, the burst buffer service must give users a way to indicate what files they want to be available on the burst buffer when they submit their job, and they must also have a way to indicate what files they want to flush back to the file system after the job ends.&lt;/li&gt;&lt;li&gt;&lt;b&gt;Background data movement&lt;/b&gt; - Burst buffers are also not designed to be long-term storage, so their reliability can be lower than the underlying parallel file system. &amp;nbsp;As such, users must also have a way to tell the burst buffer to flush intermediate data back to the parallel file system while the job is still running. &amp;nbsp;This should happen using server-to-server copying that doesn't involve the compute node at all.&lt;/li&gt;&lt;li&gt;&lt;b&gt;POSIX I/O API compatibility&lt;/b&gt; - The vast majority of HPC applications rely on the POSIX I/O API (open/close/read/write) to perform I/O, and most job scripts rely on tools developed for the POSIX I/O API (cd, ls, cp, mkdir). &amp;nbsp;As such, all burst buffers provide the ability to interact with data through the POSIX I/O API so that they look like regular old file systems to user applications. &amp;nbsp;That said, the POSIX I/O &lt;i&gt;semantics&lt;/i&gt; might not be fully supported; as will be described below, you may get an I/O error if you try to perform I/O in a fashion that is not supported by the burst buffer.&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;With all this being said, there are still a variety of ways in which these core features can be implemented into a complete burst buffer software solution. &amp;nbsp;Specifically, burst buffers can be accessed through one of several different modes, and each mode provides a different balance of peak performance and usability.&lt;/div&gt;
&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h3&gt;Transparent Caching Mode&lt;/h3&gt;
&lt;div&gt;The most user-friendly burst buffer mode uses flash to simply act as a giant cache for the parallel file system which I call &lt;b&gt;transparent caching mode&lt;/b&gt;. &amp;nbsp;Applications see the burst buffer as a mount point on their compute nodes, and this mount point mirrors the contents of the parallel file system, and any changes I make to one will appear on the other. &amp;nbsp;For example,&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;&lt;pre&gt;$ ls /mnt/lustre/glock&lt;br /&gt;bin  project1  project2  public_html  src&lt;br /&gt;&lt;br /&gt;### Burst buffer mount point contains the same stuff as Lustre&lt;br /&gt;$ ls /mnt/burstbuffer/glock&lt;br /&gt;bin  project1  project2  public_html  src&lt;br /&gt;&lt;br /&gt;### Create a file on Lustre...&lt;br /&gt;$ touch /mnt/lustre/glock/hello.txt&lt;br /&gt;&lt;br /&gt;$ ls /mnt/lustre/glock&lt;br /&gt;bin  hello.txt  project1  project2  public_html  src&lt;br /&gt;&lt;br /&gt;### ...and it automatically appears on the burst buffer.&lt;br /&gt;$ ls /mnt/burstbuffer/glock&lt;br /&gt;bin  hello.txt  project1  project2  public_html  src&lt;br /&gt;&lt;br /&gt;### However its contents are probably not on the burst buffer's flash&lt;br /&gt;### yet since we haven't read its contents through the burst buffer&lt;br /&gt;### mount point, which is what would cause it to be cached&lt;br /&gt;&lt;/pre&gt;&lt;div&gt;&lt;br /&gt;However, if I access a file through the burst buffer mount (&lt;code&gt;/mnt/burstbuffer/glock&lt;/code&gt;) rather than the parallel file system mount (&lt;code&gt;/mnt/lustre/glock&lt;/code&gt;),&lt;br /&gt;&lt;ol&gt;&lt;li&gt;if hello.txt is already cached on the burst buffer's SSDs, it will be read directly from flash&lt;/li&gt;&lt;li&gt;if hello.txt is not already cached on the SSDs, the burst buffer will read it from the parallel file system, cache its contents on the SSDs, and return its contents to me&lt;/li&gt;&lt;/ol&gt;&lt;div&gt;Similarly, if I write to hello.txt via the burst buffer mount, my data will be cached to the SSDs and &lt;i&gt;will not&lt;/i&gt; immediately appear on the parallel file system. &amp;nbsp;It will eventually flush out to the parallel file system, or I could tell the burst buffer service to explicitly flush it myself.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;This transparent caching mode is by far the easiest, since it looks exactly like the parallel file system for all intents and purposes. &amp;nbsp;However if you know that your application will never read any data more than once, it's far less useful in this fully transparent mode. &amp;nbsp;As such, burst buffers that implement this mode provide proprietary APIs that allow you to stage-in data, control the caching heuristics, and explicitly flush data from the flash to the parallel file system. &amp;nbsp;&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;DDN's Infinite Memory Engine and Cray's DataWarp both implement this transparent caching mode, and, in principle, it can be implemented on any of the burst buffer architectures outlined above.&lt;/div&gt;
&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;&lt;h3&gt;Private PFS Mode&lt;/h3&gt;Although the transparent caching mode is the easiest to use, it doesn't give users a lot of control over what data does or doesn't need to be staged into the burst buffer. &amp;nbsp;Another access mode involves creating a private parallel file system on-demand for jobs, which I will call &lt;b&gt;private PFS mode&lt;/b&gt;. &amp;nbsp;It provides a new parallel file system that is only mounted on your job's compute nodes, and this mount point contains only the data you explicitly copy to it:&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;pre&gt;### Burst buffer mount point is empty; we haven't put anything there,&lt;br /&gt;### and this file system is private to my job&lt;br /&gt;$ ls /mnt/burstbuffer&lt;br /&gt;&lt;br /&gt;### Create a file on the burst buffer file system...&lt;br /&gt;$ dd if=/dev/urandom of=/mnt/burstbuffer/mydata.bin bs=1M count=10&lt;br /&gt;10+0 records in&lt;br /&gt;10+0 records out&lt;br /&gt;10485760 bytes (10 MB) copied, 0.776115 s, 13.5 MB/s&lt;br /&gt;&lt;br /&gt;### ...it appears on the burst buffer file system...&lt;br /&gt;$ ls -l /mnt/burstbuffer&lt;br /&gt;-rw-r----- 1 glock glock 10485760 Jan  1 00:00 mydata.bin&lt;br /&gt;&lt;br /&gt;### ...and Lustre remains entirely unaffected&lt;br /&gt;$ ls /mnt/lustre/glock&lt;br /&gt;bin  project1  project2  public_html  src&lt;/pre&gt;&lt;br /&gt;&lt;/div&gt;
This is a little more complicated than transparent caching mode because you must now manage two file system namespaces: the parallel file system and your private burst buffer file system. &amp;nbsp;However this gives you the option to target your I/O to one or the other, so that a tiny input deck can stay on Lustre while your checkpoints are written out to the burst buffer file system.&lt;br /&gt;&lt;br /&gt;In addition, the burst buffer private file system is strongly consistent; as soon as you write data out to it, you can read that data back from any other node in your compute job. &amp;nbsp;While this is true of transparent caching mode &lt;i&gt;if you always access your data through the burst buffer mount point&lt;/i&gt;, you can run into trouble if you accidentally try to read a file from the original parallel file system mount point after writing out to the burst buffer mount. &amp;nbsp;Since private PFS mode provides a completely different file system and namespace, it's a bit harder to make this mistake.&lt;br /&gt;&lt;br /&gt;Cray's DataWarp implements private PFS mode, and the &lt;a href=&quot;https://twitter.com/ProfMatsuoka/status/837440717836414976&quot;&gt;Tsubame 3.0 burst buffer will be implementing private PFS mode using on-demand BeeGFS&lt;/a&gt;. &amp;nbsp;This mode is most easily implemented on fabric/ION-attached flash architectures, but Tsubame 3.0 is demonstrating that it can also be done on CN-attached flash.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Log-structured/Journaling Mode&lt;/h3&gt;As probably the least user-friendly but highest-performing use mode, &lt;b&gt;log-structured (or journaling) mode&lt;/b&gt; burst buffers present themselves to users like a file system, but they do not support the full extent of file system features. &amp;nbsp;Under the hood, writes are saved to the flash not as files, but as records that contain a timestamp, the data to be written, and the location in the file to which the data should be written. &amp;nbsp;These logs are continually appended as the application performs its writes, and when it comes time to flush the data to the parallel file system, the logs are replayed to effectively reconstruct the file that the application was trying to write.&lt;br /&gt;&lt;br /&gt;This can perform extremely well since even random I/O winds up being restructured as sequentially appended I/O. &amp;nbsp;Furthermore, there can be as many logs as there are writers; this allows writes to happen with zero lock contention, since contended writes are resolved out when the data is re-played and flushed.&lt;br /&gt;&lt;br /&gt;Unfortunately, log-structured writes make reading very difficult, since the read can no longer seek directly to a file offset to find the data it needs. &amp;nbsp;Instead, the log needs to be replayed to some degree, effectively forcing a flush to occur. &amp;nbsp;Furthermore, if the logs are spread out across different logical flash domains (as would happen in CN-attached flash architectures), read-back may require the logs to be centrally collected before the replay can happen, or it may require inter-node communication to coordinate who owns the different bytes that the application needs to read.&lt;br /&gt;&lt;br /&gt;What this amounts to is functionality that may present itself like a private parallel file system burst buffer, but behaves very differently on reads and writes. &amp;nbsp;For example, attempting to read the data that exists in a log that doesn't belong to the writer might generate an I/O error, so applications (or I/O middleware) probably need to have very well-behaved I/O to get the full performance benefits of this mode. &amp;nbsp;Most extreme-scale HPC applications already do this, so log-structured/journaling mode is a very attractive approach for very large applications that rely on extreme write performance to checkpoint their progress.&lt;br /&gt;&lt;br /&gt;Log-structured/journaling mode is well suited for CN-attached flash since logs do not need to live on a file system that presents a single shared namespace across all compute nodes. &amp;nbsp;In practice, the IBM CORAL systems will probably provide log-structured/journaling mode through IBM's burst buffer software. &amp;nbsp;Oak Ridge National Laboratory has also demonstrated &lt;a href=&quot;http://ieeexplore.ieee.org/document/7004215/&quot;&gt;a log-structured burst buffer system called BurstMem&lt;/a&gt; on a fabric-attached flash architecture. &amp;nbsp;Intel's CPPR library, to be deployed with the Argonne Aurora system, &lt;a href=&quot;file://C:/Users/Glenn/Downloads/MS19_CORAL_NRE_CPPR_HLD_v1.1_Final.pdf&quot;&gt;may also implement this functionality&lt;/a&gt;&amp;nbsp;atop the 3D XPoint to be embedded in each compute node.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Other Modes &lt;/h3&gt;The above three modes are not the only ones that burst buffers may implement, and some burst buffers support more than one of the above modes. &amp;nbsp;For example, Cray's DataWarp, in addition to supporting private PFS and transparent caching modes, also has a swap mode that allows compute nodes to use the flash as swap space to prevent hard failures for data analysis applications that consume non-deterministic amounts of memory. &amp;nbsp;In addition, &lt;a href=&quot;file://C:/Users/Glenn/Downloads/MS18_CORAL_NRE_CPPR_ScopeStatement_V1.1_final%20(3).pdf&quot;&gt;Intel's CPPR library is targeting byte-addressable nonvolatile memory&lt;/a&gt; which would expose a load/store interface, rather than the typical POSIX open/write/read/close interface, to applications.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Outlook&lt;/h2&gt;&lt;/div&gt;
&lt;p&gt;&amp;lt;/div&amp;gt;&lt;/p&gt;
&lt;div&gt;Burst buffers, practically speaking, remain in their infancy, and there is a lot of room for the landscape I've outlined here to change. &amp;nbsp;For example, the common software features I highlighted (staging, background data movement, and POSIX API support) are still largely implemented via proprietary, non-standard APIs at present. &amp;nbsp;There is effort to get burst buffer vendors to agree to a common API, and as this process proceeds, features may appear or disappear as customers define what is and isn't a worthwhile differentiating feature.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;On the hardware front, the burst buffer ecosystem is also in flux. &amp;nbsp;ION-attached flash is where burst buffers began, but as discussed above, they are likely to be replaced by dedicated fabric-attached flash servers. &amp;nbsp;In addition, the emergence of storage-class memory (that is, byte-addressable nonvolatile memory) will also add a new dimension to burst buffers that may make one architecture the clear winner over the others. &amp;nbsp;At present though, both fabric-attached and CN-attached burst buffers have their strengths and weaknesses, and neither is at risk of disappearing in the next five years.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;As more extreme-scale systems begin to hit the floor and users figure out what does and doesn't work across the diversity of burst buffer hardware and software features, the picture is certain to become clearer. &amp;nbsp;Once that happens, I'll be sure to post another update.&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Beyond Single Core R- Parallel Data Analysis</title>
   <link href="https://hpc.social/2017/beyond-single-core-r-parallel-data-analysis/"/>
   <updated>2017-02-15T00:00:00-07:00</updated>
   <id>https://hpc.social/2017/beyond-single-core-r-parallel-data-analysis</id>
   <content type="html">&lt;p&gt;I was asked recently to do short presentation for the &lt;a href=&quot;https://www.meetup.com/Greater-Toronto-Area-GTA-R-Users-Group&quot;&gt;Greater Toronto R Users Group&lt;/a&gt;
on parallel computing in R; My slides can be seen below or on &lt;a href=&quot;https://ljdursi.github.io/beyond-single-core-R&quot;&gt;github&lt;/a&gt;, where &lt;a href=&quot;https://github.com/ljdursi/beyond-single-core-R&quot;&gt;the complete materials can be found&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I covered some similar things I had covered in a half-day workshop
a couple of years earlier (though, obviously, without the hands-on
component):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;How to think about parallelism and scalability in data analysis&lt;/li&gt;
  &lt;li&gt;The standard parallel package, including what was the snow and multicore facilities, using airline data as an example&lt;/li&gt;
  &lt;li&gt;The foreach package, using airline data and simple stock data;&lt;/li&gt;
  &lt;li&gt;A summary of best practices,&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;with some bonus material tacked on the end touching on a couple advanced topics.&lt;/p&gt;

&lt;p&gt;I was quite surprised at how little had changed since late 2014, other than 
further development of &lt;a href=&quot;http://spark.apache.org/docs/latest/sparkr.html&quot;&gt;SparkR&lt;/a&gt; (which
I didn’t cover), and the interesting but seemingly not very much used &lt;a href=&quot;https://cran.r-project.org/web/packages/future/index.html&quot;&gt;future&lt;/a&gt;
package.   I was also struck by how hard it is to find similar materials
online, covering a range of parallel computing topics in R - it’s rare enough
that even this simple effort made it to the &lt;a href=&quot;https://cran.r-project.org/web/views/HighPerformanceComputing.html&quot;&gt;HPC project view on CRAN&lt;/a&gt; 
(under “related links”).  R &lt;a href=&quot;http://spectrum.ieee.org/computing/software/the-2016-top-programming-languages&quot;&gt;continues to grow in popularity&lt;/a&gt; for data analysis; 
is this all desktop computing?  Is Spark siphoning off the clustered-dataframe
usage?&lt;/p&gt;

&lt;p&gt;(This was also my first time with &lt;a href=&quot;https://support.rstudio.com/hc/en-us/articles/200486468-Authoring-R-Presentations&quot;&gt;RPres&lt;/a&gt; in RStudio;
wow, not a fan, RPres was &lt;em&gt;not&lt;/em&gt; ready for general release.  And I’m a big fan of RMarkdown.)&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>MPI's Place in Big Computing</title>
   <link href="https://hpc.social/2016/mpi-s-place-in-big-computing/"/>
   <updated>2016-10-14T01:00:00-06:00</updated>
   <id>https://hpc.social/2016/mpi-s-place-in-big-computing</id>
   <content type="html">&lt;p&gt;The organizers of &lt;a href=&quot;http://www.eurompi2016.ed.ac.uk&quot;&gt;EuroMPI 2016&lt;/a&gt; were kind enough to invite me to give a keynote and participate in a panel at their meeting, which was held at the end of September in beautiful Edinburgh.  The event was terrific, with lots of very interesting work going on in MPI implementations and with MPI.&lt;/p&gt;

&lt;p&gt;The topic of my talk was “MPI’s Place in Big Computing”; the materials from the talk can be found &lt;a href=&quot;http://github.com/ljdursi/EuroMPI2016&quot;&gt;on github&lt;/a&gt;. The talk, as you might expect, included discussion of high-productivity big data frameworks, but also — and missing from the discussion in my “HPC is dying” blog post — the “data layer” frameworks that underpin them.&lt;/p&gt;

&lt;p&gt;I think a lot of people have taken, quite reasonably, my that blog post to suggest that &lt;a href=&quot;http://spark.apache.org&quot;&gt;Spark&lt;/a&gt; for example is a competitor to MPI; the point I wanted to make is a little more nuanced that that.&lt;/p&gt;

&lt;p&gt;I’m actually skeptical of Spark’s utility for (&lt;em&gt;e.g.&lt;/em&gt;) large-scale simulations. However attractive the model is from a variety of points of view, absent some huge breakthrough I don’t think that functional models with immutable data can support the performance, memory requirements, or performance predictability we require.  (But who knows; maybe that’ll be one of the compromises we find we have to make on the road to exascale).&lt;/p&gt;

&lt;p&gt;But whatever you might think of Spark’s efficacy for your particular use case,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A lot of people manifestly find it to be extremely useful for &lt;em&gt;their&lt;/em&gt; use case; and&lt;/li&gt;
  &lt;li&gt;Performance is quite important to those communities.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So given that, why isn’t Spark built atop of MPI for network communications?  And why isn’t &lt;a href=&quot;http://tensorflow.org&quot;&gt;TensorFlow&lt;/a&gt;, or &lt;a href=&quot;http://dask.pydata.org&quot;&gt;Dask&lt;/a&gt;, or &lt;a href=&quot;http://www.seastar-project.org&quot;&gt;SeaStar&lt;/a&gt;?&lt;/p&gt;

&lt;p&gt;The past five years have seen a huge number of high-productivity tools for large-scale number crunching gain extremely rapid adoption.  Even if you don’t like those particular tools for your problems, surely you’d like for there to exist some tools like that for the traditional HPC community; why do other communications frameworks support this flourishing ecosystem of platforms, and MPI doesn’t?&lt;/p&gt;

&lt;p&gt;There’s another argument there, too - simply from a self-preservation point of view, it would be in MPI’s interest to be adopted by a high-profile big data platform to ensure continued success and support.  But none are; why?  It’s not because the developers of Spark or at Google are just too dumb to figure out MPI’s syntax.&lt;/p&gt;

&lt;p&gt;Going through what does get used for these packages and what doesn’t — which is what I do in this talk — I think the issues become fairly clear.  MPI wants to be both a low-level communications framework and a higher-level programming model, and ends up tripping over it’s own feet trying to dance both dances.  As a communications “data plane” it imposes too many high-level decisions on applications — no fault tolerance, restrictive communications semantics (in-order and arrival guarantees), and provides too few services (&lt;em&gt;e.g.&lt;/em&gt; a performant active message/RPC layer).  And as a high-level programming model it is too low level and is missing different services (communications-aware scheduling came up in several guises at the meeting).&lt;/p&gt;

&lt;p&gt;I don’t think that’s insurmountable; I think inside MPI implementations there is a performant, network-agnostic low-level communications layer trying to get out.  Exposing more MPI runtime services is a move in the right direction.  I was surprised at how open the meeting participants were to making judicious changes — even perhaps breaking some backwards compatability — in the right directions.&lt;/p&gt;

&lt;p&gt;Thanks again to the organizers for extending the opportunity to participate; it was great.&lt;/p&gt;

&lt;p&gt;My slides can be seen below or on &lt;a href=&quot;http://ljdursi.github.io/EuroMPI2016/#1&quot;&gt;github&lt;/a&gt;, where &lt;a href=&quot;http://github.com/ljdursi/EuroMPI2016&quot;&gt;the complete materials can be found&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Jupyter Notebooks for Performing and Sharing Bioinformatics Analyses</title>
   <link href="https://hpc.social/2016/jupyter-notebooks-for-performing-and-sharing-bioinformatics-analyses/"/>
   <updated>2016-09-09T01:00:00-06:00</updated>
   <id>https://hpc.social/2016/jupyter-notebooks-for-performing-and-sharing-bioinformatics-analyses</id>
   <content type="html">&lt;p&gt;I was asked to do a half-day tutorial at the &lt;a href=&quot;https://www.iscb.org/glbioccbc2016-program/workshops&quot;&gt;Great Lakes Bioinformatics conference Workshop session&lt;/a&gt;.
The focus was mainly on R, with some python as well.  We covered:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The basics of Jupyter notebooks - what they are and how they work&lt;/li&gt;
  &lt;li&gt;How to install and run Jupyter notebooks on their laptop, in R and Python&lt;/li&gt;
  &lt;li&gt;How to perform interactive analyses in a web browser using Jupyter&lt;/li&gt;
  &lt;li&gt;Using markdown and latex to&lt;/li&gt;
  &lt;li&gt;How to “Port” an R bioinformatics workflow from some scripts into a Jupyter notebook&lt;/li&gt;
  &lt;li&gt;How to share a Jupyter notebook online, using three different approaches
    &lt;ul&gt;
      &lt;li&gt;SageMathCloud&lt;/li&gt;
      &lt;li&gt;GitHub and&lt;/li&gt;
      &lt;li&gt;mybinder.org&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I think it went prety well; the materials are available &lt;a href=&quot;https://github.com/ljdursi/glbio-jupyter-workshop&quot;&gt;On GitHub&lt;/a&gt;.
It was largely hands-on, so apart from some &lt;a href=&quot;https://github.com/ljdursi/glbio-jupyter-workshop/blob/master/Slides/Jupyter.pdf&quot;&gt;introductory slides&lt;/a&gt;,
it was mainly about giving a tour of the notebook and how use Jupyter to share analyses; the “scripts” that I went through
in presenting the material were aimed at having the students produce the notebooks 
&lt;a href=&quot;https://github.com/ljdursi/glbio-jupyter-workshop/tree/master/Notebooks&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Basics of I/O Benchmarking</title>
   <link href="https://hpc.social/2016/basics-of-i-o-benchmarking/"/>
   <updated>2016-07-22T07:07:00-06:00</updated>
   <id>https://hpc.social/2016/basics-of-i-o-benchmarking</id>
   <content type="html">&lt;p&gt;Most people in the supercomputing business are familiar with using FLOPS as a proxy for how fast or capable a supercomputer is.  This measurement, as observed using the &lt;a href=&quot;http://www.netlib.org/benchmark/hpl/&quot;&gt;High-Performance Linpack (HPL)&lt;/a&gt; benchmark, is the basis for the Top500 list.  However, I/O performance is becoming increasingly important as data-intensive computing becomes a driving force in the HPC community, and even though there is no Top500 list for I/O subsystems, the &lt;a href=&quot;http://www.nersc.gov/research-and-development/apex/apex-benchmarks/ior/&quot;&gt;IOR&lt;/a&gt; benchmark has become the &lt;i&gt;de facto&lt;/i&gt; standard way to measure the I/O capability for clusters and supercomputers.&lt;br /&gt;&lt;br /&gt;Unfortunately, I/O performance tends to be trickier to measure using synthetic benchmarks because of the complexity of the I/O stack that lies between where data is generated (the CPU) to where it’ll ultimately be stored (a spinning disk or SSD on a network file system).  In the interests of clarifying some of the confusion that can arise when trying to determine how capable an I/O subsystem really is, let’s take a look at some of the specifics of running IOR.&lt;br /&gt;&lt;br /&gt;&amp;lt;h2&amp;gt;Getting Started with IOR&amp;lt;/h2&amp;gt;IOR writes data sequentially with the following parameters:&lt;br /&gt;&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;&lt;span style=&quot;font-family: monospace;&quot;&gt;blockSize&lt;/span&gt; (&lt;span style=&quot;font-family: monospace;&quot;&gt;-b&lt;/span&gt;)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&lt;span style=&quot;font-family: monospace;&quot;&gt;transferSize&lt;/span&gt; (&lt;span style=&quot;font-family: monospace;&quot;&gt;-t&lt;/span&gt;)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&lt;span style=&quot;font-family: monospace;&quot;&gt;segmentCount&lt;/span&gt; (&lt;span style=&quot;font-family: monospace;&quot;&gt;-s&lt;/span&gt;)&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&lt;span style=&quot;font-family: monospace;&quot;&gt;numTasks&lt;/span&gt; (&lt;span style=&quot;font-family: monospace;&quot;&gt;-n&lt;/span&gt;)&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;div&amp;gt;which are best illustrated with a diagram:&amp;lt;/div&amp;gt;
&lt;br /&gt;&amp;lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&amp;gt;&lt;a href=&quot;https://2.bp.blogspot.com/-fok4ue8yCiw/V2B-5BCjIlI/AAAAAAAASw0/do7YfsfV8I00b35WAWTeZdiPeWOau_oxwCLcB/s1600/ior-io-pattern.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;212&quot; src=&quot;https://2.bp.blogspot.com/-fok4ue8yCiw/V2B-5BCjIlI/AAAAAAAASw0/do7YfsfV8I00b35WAWTeZdiPeWOau_oxwCLcB/s400/ior-io-pattern.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/div&amp;gt;
&lt;br /&gt;These four parameters are all you need to get started with IOR.  However, naively running IOR usually gives disappointing results.  For example, if we run a four-node IOR test that writes a total of 16 GiB:&lt;br /&gt;&lt;br /&gt;&amp;lt;pre&amp;gt;$ mpirun -n 64 ./ior -t 1m -b 16m -s 16&lt;br /&gt;…&lt;br /&gt;access bw(MiB/s) block(KiB) xfer(KiB) open(s)  wr/rd(s) close(s) total(s) iter&lt;br /&gt;—— ——— ———- ——— ——– ——– ——– ——– —-&lt;br /&gt;&lt;span style=&quot;background-color: #ffff7f; border-radius: 4px; color: #c7254e;&quot;&gt;write  427.36  &lt;/span&gt;  16384      1024.00   0.107961 38.34    32.48    38.34    2&lt;br /&gt;&lt;span style=&quot;background-color: #ffff7f; border-radius: 4px; color: #c7254e;&quot;&gt;read   239.08  &lt;/span&gt;  16384      1024.00   0.005789 68.53    65.53    68.53    2&lt;br /&gt;remove -         -          -         -        -        -        0.534400 2&lt;br /&gt;&amp;lt;/pre&amp;gt;&amp;lt;div&amp;gt;&lt;br /&gt;we can only get a couple hundred megabytes per second out of a Lustre file system that should be capable of a lot more.&lt;br /&gt;&lt;br /&gt;Switching from writing to a single-shared file to one file per process using the &lt;code&gt;-F&lt;/code&gt; (&lt;code&gt;filePerProcess=1&lt;/code&gt;) option changes the performance dramatically:&amp;lt;/div&amp;gt;&lt;/p&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;pre&gt;$ mpirun -n 64 ./ior -t 1m -b 16m -s 16 -F&lt;br /&gt;...&lt;br /&gt;access bw(MiB/s) block(KiB) xfer(KiB) open(s)  wr/rd(s) close(s) total(s) iter&lt;br /&gt;------ --------- ---------- --------- -------- -------- -------- -------- ----&lt;br /&gt;&lt;span style=&quot;background-color: #ffff7f; border-radius: 4px; color: #c7254e;&quot;&gt;write  33645   &lt;/span&gt;  16384      1024.00   0.007693 0.486249 0.195494 0.486972 1&lt;br /&gt;&lt;span style=&quot;background-color: #ffff7f; border-radius: 4px; color: #c7254e;&quot;&gt;read   149473  &lt;/span&gt;  16384      1024.00   0.004936 0.108627 0.016479 0.109612 1&lt;br /&gt;remove -         -          -         -        -        -        6.08     1&lt;/pre&gt;
&lt;div&gt;&lt;div&gt;&lt;br /&gt;This is in large part because letting each MPI process work on its own file cuts out any contention that would arise because of file locking. &amp;nbsp;&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;However, the performance difference between our naive test and the file-per-process test is a bit extreme. &amp;nbsp;In fact, the only way that 146 GB/sec read rate could be achievable on Lustre is if each of the four compute nodes had over 45 GB/sec of network bandwidth to Lustre--that is, a 400 Gbit link on every compute and storage node.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;&lt;h2&gt;Effect of Page Cache on Benchmarking&lt;/h2&gt;What's really happening is that the data being read by IOR isn't actually coming from Lustre; rather, files' contents are already cached, and IOR is able to read them directly out of each compute node's DRAM. &amp;nbsp;The data wound up getting cached during the write phase of IOR as a result of Linux (and Lustre) using a write-back cache to buffer I/O, so that instead of IOR writing and reading data directly to Lustre, it's actually mostly talking to the memory on each compute node.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;To be more specific, although each IOR process thinks it is writing to a file on Lustre and then reading back the contents of that file from Lustre, it is actually&lt;/div&gt;
&lt;div&gt;&lt;/div&gt;
&lt;ol&gt;&lt;li&gt;writing data to a copy of the file that is cached in memory. &amp;nbsp;If there is no copy of the file cached in memory before this write, the parts being modified are loaded into memory first.&lt;/li&gt;&lt;li&gt;those parts of the file in memory (called &quot;pages&quot;) that are now different from what's on Lustre are marked as being &quot;dirty&quot;&lt;/li&gt;&lt;li&gt;the write() call completes and IOR continues on, even though the written data still hasn't been committed to Lustre&lt;/li&gt;&lt;li&gt;independent of IOR, the OS kernel continually scans the file cache for files who have been updated in memory but not on Lustre (&quot;dirt pages&quot;), and then commits the cached modifications to Lustre&lt;/li&gt;&lt;li&gt;dirty pages are declared non-dirty since they are now in sync with what's on disk, but they remain in memory&lt;/li&gt;&lt;/ol&gt;Then when the read phase of IOR follows the write phase, IOR is able to just retrieve the file's contents from memory instead of having to communicate with Lustre over the network.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;There are a couple of ways to measure the read performance of the underlying Lustre file system.  The most crude way is to simply write more data than will fit into the total page cache so that by the time the write phase has completed, the beginning of the file has already been evicted from cache.  For example, increasing the number of segments (&lt;span style=&quot;font-family: monospace;&quot;&gt;-s&lt;/span&gt;) to write more data reveals the point at which the nodes' page cache on my test system runs over very clearly:&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;http://3.bp.blogspot.com/-7M2BLomSgNA/VyZ8L-G_HpI/AAAAAAAALyU/SSQXrYOqJ94V4W61S9-g-UMs90EJ4waewCK4B/s1600/ior-overflowing-cache.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;271&quot; src=&quot;https://3.bp.blogspot.com/-7M2BLomSgNA/VyZ8L-G_HpI/AAAAAAAALyU/SSQXrYOqJ94V4W61S9-g-UMs90EJ4waewCK4B/s400/ior-overflowing-cache.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;br /&gt;However, this can make running IOR on systems with a lot of on-node memory take forever.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;A better option would be to get the MPI processes on each node to only read data that they didn't write. &amp;nbsp;For example, on a four-process-per-node test, shifting the mapping of MPI processes to blocks by four makes each node N read the data written by node N-1.&lt;br /&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;http://1.bp.blogspot.com/-AhRMQWdDOxg/VyZ6lH2wl-I/AAAAAAAALyA/nv-EM4OlhX8BHCNX_Bx173Mr7miyBXx-ACK4B/s1600/IOR%2BreorderTasks.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;131&quot; src=&quot;https://1.bp.blogspot.com/-AhRMQWdDOxg/VyZ6lH2wl-I/AAAAAAAALyA/nv-EM4OlhX8BHCNX_Bx173Mr7miyBXx-ACK4B/s400/IOR%2BreorderTasks.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;&lt;/div&gt;
&lt;div&gt;Since page cache is not shared between compute nodes, shifting tasks this way ensures that each MPI process is reading data it did not write.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;IOR provides the &lt;span style=&quot;font-family: monospace;&quot;&gt;-C&lt;/span&gt; option (reorderTasks) to do this, and it forces each MPI process to read the data written by its neighboring node. &amp;nbsp;Running IOR with this option gives much more credible read performance:&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;pre&gt;$ mpirun -n 64 ./ior -t 1m -b 16m -s 16 -F -C&lt;br /&gt;...&lt;br /&gt;access bw(MiB/s) block(KiB) xfer(KiB) open(s)  wr/rd(s) close(s) total(s) iter&lt;br /&gt;------ --------- ---------- --------- -------- -------- -------- -------- ----&lt;br /&gt;&lt;span style=&quot;background-color: #ffff7f; border-radius: 4px; color: #c7254e;&quot;&gt;write  41326   &lt;/span&gt;  16384      1024.00   0.005756 0.395859 0.095360 0.396453 0&lt;br /&gt;&lt;span style=&quot;background-color: #ffff7f; border-radius: 4px; color: #c7254e;&quot;&gt;read   3310.00 &lt;/span&gt;  16384      1024.00   0.011786 4.95     4.20     4.95     1&lt;br /&gt;remove -         -          -         -        -        -        0.237291 1&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;But now it should seem obvious that the write performance is also ridiculously high.  And again, this is due to the page cache, which signals to IOR that writes are complete when they have been committed to memory rather than the underlying Lustre file system.&lt;br /&gt;&lt;br /&gt;To work around the effects of the page cache on write performance, we can issue an &lt;span style=&quot;font-family: monospace;&quot;&gt;fsync()&lt;/span&gt; call immediately after all of the &lt;span style=&quot;font-family: monospace;&quot;&gt;write()&lt;/span&gt;s return to force the dirty pages we just wrote to flush out to Lustre.  Including the time it takes for &lt;span style=&quot;font-family: monospace;&quot;&gt;fsync()&lt;/span&gt; to finish gives us a measure of how long it takes for our data to write to the page cache and for the page cache to write back to Lustre.&lt;br /&gt;&lt;br /&gt;IOR provides another convenient option, &lt;span style=&quot;font-family: monospace;&quot;&gt;-e&lt;/span&gt; (&lt;span style=&quot;font-family: monospace;&quot;&gt;fsync&lt;/span&gt;), to do just this.  And, once again, using this option changes our performance measurement quite a bit:&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;
&lt;pre&gt;$ mpirun -n 64 ./ior -t 1m -b 16m -s 16 -F -C -e&lt;br /&gt;...&lt;br /&gt;access bw(MiB/s) block(KiB) xfer(KiB) open(s)  wr/rd(s) close(s) total(s) iter&lt;br /&gt;------ --------- ---------- --------- -------- -------- -------- -------- ----&lt;br /&gt;&lt;span style=&quot;background-color: #ffff7f; border-radius: 4px; color: #c7254e;&quot;&gt;write  2937.89 &lt;/span&gt;  16384      1024.00   0.011841 5.56     4.93     5.58     0&lt;br /&gt;&lt;span style=&quot;background-color: #ffff7f; border-radius: 4px; color: #c7254e;&quot;&gt;read   2712.55 &lt;/span&gt;  16384      1024.00   0.005214 6.04     5.08     6.04     3&lt;br /&gt;remove -         -          -         -        -        -        0.037706 0&lt;/pre&gt;
&lt;p&gt;&lt;br /&gt;and we finally have a believable bandwidth measurement for our file system. &lt;br /&gt;&lt;br /&gt;&amp;lt;h2&amp;gt;Defeating Page Cache&amp;lt;/h2&amp;gt;Since IOR is specifically designed to benchmark I/O, it provides these options that make it as easy as possible to ensure that you are actually measuring the performance of your file system and not your compute nodes’ memory.  That being said, the I/O patterns it generates are designed to demonstrate peak performance, not reflect what a real application might be trying to do, and as a result, there are plenty of cases where measuring I/O performance with IOR is not always the best choice.  There are several ways in which we can get clever and defeat page cache in a more general sense to get meaningful performance numbers.&lt;br /&gt;&lt;br /&gt;When measuring &lt;b&gt;write performance&lt;/b&gt;, bypassing page cache is actually quite simple; opening a file with the &lt;span style=&quot;font-family: monospace;&quot;&gt;O_DIRECT&lt;/span&gt; flag going directly to disk.  In addition, the &lt;span style=&quot;font-family: monospace;&quot;&gt;fsync()&lt;/span&gt; call can be inserted into applications, as is done with IOR’s &lt;span style=&quot;font-family: monospace;&quot;&gt;-e&lt;/span&gt; option.&lt;br /&gt;&lt;br /&gt;Measuring &lt;b&gt;read performance&lt;/b&gt; is a lot trickier.  If you are fortunate enough to have root access on a test system, you can force the Linux kernel to empty out its page cache by doing&lt;br /&gt;&amp;lt;blockquote class=&quot;tr_bq&quot;&amp;gt;&lt;span style=&quot;font-family: monospace;&quot;&gt;# echo 1 &amp;gt; /proc/sys/vm/drop_caches&lt;/span&gt;&amp;lt;/blockquote&amp;gt;and in fact, this is often good practice before running any benchmark (e.g., Linpack) because it ensures that you aren’t losing performance to the kernel trying to evict pages as your benchmark application starts allocating memory for its own use.&lt;br /&gt;&lt;br /&gt;Unfortunately, many of us do not have root on our systems, so we have to get even more clever.  As it turns out, there is a way to pass a hint to the kernel that a file is no longer needed in page cache:&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;The effect of passing &lt;span style=&quot;font-family: monospace;&quot;&gt;POSIX_FADV_DONTNEED&lt;/span&gt; using &lt;span style=&quot;font-family: monospace;&quot;&gt;posix_fadvise()&lt;/span&gt; is usually that all pages belonging to that file are evicted from page cache in Linux.  However, this is just a hint–not a guarantee–and the kernel evicts these pages asynchronously, so it may take a second or two for pages to actually leave page cache.  Fortunately, Linux also provides a way to &lt;a href=&quot;https://github.com/glennklockwood/atgtools/blob/master/is_file_in_page_cache.c&quot;&gt;probe pages in a file to see if they are resident in memory&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;Finally, it’s often easiest to just limit the amount of memory available for page cache.  Because application memory always takes precedence over cache memory, simply allocating most of the memory on a node will force most of the cached pages to be evicted.  Newer versions of IOR provide the &lt;span style=&quot;font-family: monospace;&quot;&gt;memoryPerNode&lt;/span&gt; option that do just that, and the effects are what one would expect:&lt;br /&gt;&lt;br /&gt;&amp;lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&amp;gt;&lt;a href=&quot;https://1.bp.blogspot.com/-xiC1K4absXU/V5GVEAfe5dI/AAAAAAAAgwY/HyO4J_ORd2gnJLF7aD3JpNu9p9MqjOc-ACLcB/s1600/ior-memPerNode-test.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;271&quot; src=&quot;https://1.bp.blogspot.com/-xiC1K4absXU/V5GVEAfe5dI/AAAAAAAAgwY/HyO4J_ORd2gnJLF7aD3JpNu9p9MqjOc-ACLcB/s400/ior-memPerNode-test.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/div&amp;gt;
&lt;br /&gt;The above diagram shows the measured bandwidth from a single node with 128 GiB of total DRAM.  The first percent on each x-label is the amount of this 128 GiB that was reserved by the benchmark as application memory, and the second percent is the total write volume.  For example, the “50%/150%” data points correspond to 50% of the node memory (64 GiB) being allocated for the application, and a total of 192 GiB of data being read.&lt;br /&gt;&lt;br /&gt;This benchmark was run on a single spinning disk which is not capable of more than 130 MB/sec, so the conditions that showed performance higher than this were benefiting from some pages being served from cache.  And this makes perfect sense given that the anomalously high performance measurements were obtained when there was plenty of memory to cache relative to the amount of data being read.&lt;br /&gt;&lt;br /&gt;&amp;lt;h2&amp;gt;Corollary &amp;lt;/h2&amp;gt;Measuring I/O performance is a bit trickier than CPU performance in large part due to the effects of page caching.  That being said, page cache exists for a reason, and there are many cases where an application’s I/O performance really is best represented by a benchmark that heavily utilizes cache.&lt;br /&gt;&lt;br /&gt;For example, the BLAST bioinformatics application re-reads all of its input data twice; the first time initializes data structures, and the second time fills them up.  Because the first read caches each page and allows the second read to come out of cache rather than the file system, running this I/O pattern with page cache disabled causes it to be about 2x slower:&lt;br /&gt;&lt;br /&gt;&amp;lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&amp;gt;&lt;a href=&quot;https://4.bp.blogspot.com/-KBZ0TDtNz5w/V5Gc8XLAS3I/AAAAAAAAgwo/GWH6i3xp98oSHilPgPAipG75cClgDhkuACLcB/s1600/cache-vs-nocache.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;290&quot; src=&quot;https://4.bp.blogspot.com/-KBZ0TDtNz5w/V5Gc8XLAS3I/AAAAAAAAgwo/GWH6i3xp98oSHilPgPAipG75cClgDhkuACLcB/s400/cache-vs-nocache.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/div&amp;gt;
&lt;br /&gt;Thus, letting the page cache do its thing is often the most realistic way to benchmark with realistic application I/O patterns.  Once you know &lt;i&gt;how &lt;/i&gt;page cache might be affecting your measurements, you stand a good chance of being able to reason about what the most meaningful performance metrics are.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>An uninformed perspective on TaihuLight's design</title>
   <link href="https://hpc.social/2016/an-uninformed-perspective-on-taihulight-s-design/"/>
   <updated>2016-06-21T07:36:00-06:00</updated>
   <id>https://hpc.social/2016/an-uninformed-perspective-on-taihulight-s-design</id>
   <content type="html">&lt;div style=&quot;line-height: 100%; text-align: center;&quot;&gt;&lt;span style=&quot;font-size: xx-small;&quot;&gt;Note: What follows are my own personal thoughts, opinions, and analyses. &amp;nbsp;I am not a computer scientist and I don't really know anything about processor design or application performance, so it is safe to assume I don't know what I'm talking about. &amp;nbsp;None of this represents the views of my employer, the U.S. government, or anyone except me.&lt;/span&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;a href=&quot;http://top500.org/news/china-tops-supercomputer-rankings-with-new-93-petaflop-machine/&quot;&gt;China’s new 93 PF TaihuLight system&lt;/a&gt; is impressive given the indigenous processor design and its substantial increase in its HPL score over the #2 system, Tianhe-2.  The &lt;a href=&quot;http://www.nytimes.com/2016/06/21/technology/china-tops-list-of-fastest-computers-again.html?_r=0&quot;&gt;popular media has started covering this new system and the increasing presence of Chinese systems on Top500&lt;/a&gt;, suggesting that China’s string of #1 systems may be a sign of shifting tides.  And maybe it is.  China is undeniably committed to investing in supercomputing and positioning itself as a leader in extreme-scale computing.&lt;br /&gt;&lt;br /&gt;That being said, the TaihuLight system isn’t quite the technological marvel and threat to the HPC hegemony that it may seem at first glance.  The system features some some critically limiting design choices that make the system smell like a supercomputer that was &lt;a href=&quot;http://www.scmp.com/tech/science-research/article/1773421/chinese-supercomputer-too-slow-compete-race-hypersonic-weapons&quot;&gt;designed to be #1 on Top500&lt;/a&gt;, not solve scientific problems.  This probably sounds like sour grapes at this point, so let’s take a look at some of the details.&lt;br /&gt;&lt;br /&gt;&amp;lt;h2&amp;gt;Back-of-the-envelope math&amp;lt;/h2&amp;gt;Consider the fact that each TaihuLight node turns 3,062 GFLOPS (that’s 3 TFLOPS) and has 136.51 GB/sec of memory bandwidth. This means that in the time it takes for the processor to load two 64-bit floats into the processor from memory, it could theoretically perform over 350 floating point operations. But it won’t, because it can only load the two operands for one single FLOP.&lt;br /&gt;&lt;br /&gt;Of course, this is an oversimplification of how CPUs work.  Caches exist to feed the extremely high operation rate of modern processors, and where there are so many cores that their caches can’t be fed fast enough, we see technologies like GDDR DRAM and &lt;a href=&quot;http://www.extremetech.com/gaming/179159-gtc-2014-nvidia-reveals-dual-gpu-titan-z-new-pascal-gpu-offers-colossal-memory-bandwidth&quot;&gt;HBM&lt;/a&gt; (on accelerators) and on-package &lt;a href=&quot;https://software.intel.com/en-us/blogs/2016/01/20/an-intro-to-mcdram-high-bandwidth-memory-on-knights-landing&quot;&gt;MCDRAM&lt;/a&gt; (on KNL) appearing so that dozens or hundreds of cores can all retrieve enough floating-point operands from memory to sustain high rates of floating point calculations.&lt;br /&gt;&lt;br /&gt;However, the ShenWei SW26010 chips in the TaihuLight machine have neither GDDR nor MCDRAM; they rely on four DDR3 controllers running at 136 GB/sec to keep all 256 compute elements fed with data.  &lt;a href=&quot;http://www.netlib.org/utk/people/JackDongarra/PAPERS/sunway-report-2016.pdf&quot;&gt;Dongarra’s report on the TaihuLight design&lt;/a&gt; briefly mentions this high skew:&lt;br /&gt;&lt;br /&gt;&amp;lt;blockquote class=&quot;tr_bq&quot;&amp;gt;“The ratio of floating point operations per byte of data from memory on the SW26010 is 22.4 Flops(DP)/Byte transfer, which shows an imbalance or an overcapacity of floating point operations per data transfer from memory. By comparison the Intel Knights Landing processor with 7.2 Flops(DP)/Byte transfer.”&amp;lt;/blockquote&amp;gt;&lt;br /&gt;This measure of “Flops(DP)/Byte transfer” is called arithmetic intensity, and it is a critical optimization parameter when writing applications for manycore architectures.  Highly optimized GPU codes can show &lt;a href=&quot;http://people.eecs.berkeley.edu/~kubitron/cs258/lectures/lec12-Merrimac.pdf&quot;&gt;arithmetic intensities of around 10 FLOPS/byte&lt;/a&gt;, but such applications are often the exception; there are classes of problems that simply do not have high arithmetic intensities.  This diagram, which I stole from the &lt;a href=&quot;https://crd.lbl.gov/departments/computer-science/PAR/research/roofline/&quot;&gt;Performance and Algorithms Research group at Berkeley Lab&lt;/a&gt;, illustrates the spectrum:&lt;br /&gt;&lt;br /&gt;&amp;lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&amp;gt;&lt;a href=&quot;https://3.bp.blogspot.com/-E_1Yi-g0qws/V2jCeZo0dUI/AAAAAAAATSA/2WCXZkchvuUclAXdyIUhv2ODQI7bv4AuwCLcB/s1600/ResizedImage600300-rooflineai.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;200&quot; src=&quot;https://3.bp.blogspot.com/-E_1Yi-g0qws/V2jCeZo0dUI/AAAAAAAATSA/2WCXZkchvuUclAXdyIUhv2ODQI7bv4AuwCLcB/s400/ResizedImage600300-rooflineai.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/div&amp;gt;
&lt;br /&gt;To put this into perspective in the context of hardware, let’s look at the #3 supercomputer, &lt;a href=&quot;https://www.olcf.ornl.gov/titan/&quot;&gt;the Titan system at Oak Ridge National Lab&lt;/a&gt;.  The GPUs on which it is built (&lt;a href=&quot;http://www.nvidia.com/content/tesla/pdf/nvidia-tesla-kepler-family-datasheet.pdf&quot;&gt;NVIDIA’s K20X&lt;/a&gt;) each have a GDDR5-based memory subsystem that can feed the 1.3 TFLOP GPUs at 250 GB/sec.  This means that Titan’s FLOPS/byte ratio is around 5.3, or over 4x lower (more balanced) than the 22 FLOPS/byte of TaihuLight’s SW26010 chips.&lt;br /&gt;&lt;br /&gt;This huge gap means that an application that is perfectly balanced to run on a Titan GPU–that is, an application with an arithmetic intensity of 5.3–will run 4x slower on one of TaihuLight’s SW26010 processors than a Titan GPU.  Put simply, despite being theoretically capable of doing 3 TFLOPS of computing, TaihuLight’s processors would only be able to deliver performance to 1/4th of that, or 0.75 TFLOPS, to this application.  Because of the severely limited per-node memory bandwidth, &lt;b&gt;this 93 PFLOP system would perform like a 23 PFLOP system&lt;/b&gt; on an application that, given an arithmetic intensity of 5.3, would be considered highly optimized by most standards.&lt;br /&gt;&lt;br /&gt;Of course, the indigenous architecture also means that application developers will have to rely on indigenous implementations or ports of performance runtimes like OpenMP and OpenACC, libraries like BLAS, and ISA-specific vector intrinsics.  The maturity of this software stack for the ShenWei-64 architecture remains unknown.&lt;br /&gt;&lt;br /&gt;&amp;lt;h2&amp;gt;What &lt;i&gt;is&lt;/i&gt; interesting&amp;lt;/h2&amp;gt;This all isn’t to say that the TaihuLight system isn’t a notable achievement; it is the first massive-scale deployment of a CPU-based manycore processor, it is the first massive-scale deployment of EDR InfiniBand, and its CPU design is extremely interesting in a number of ways.&lt;br /&gt;&lt;br /&gt;The CPU block diagrams included in Dongarra’s report are a bit like a Rorschach test; my esteemed colleagues at &lt;a href=&quot;http://www.nextplatform.com/2016/06/20/look-inside-chinas-chart-topping-new-supercomputer/&quot;&gt;The Next Platform astutely pointed out its similarities to KNL&lt;/a&gt;, but my first reaction was to compare it with IBM’s Cell processor:&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;https://2.bp.blogspot.com/-rCGxhO2fVGw/V2jMRV379wI/AAAAAAAATSQ/l20liolD4jcU8ZxZbkejw5asAeZIOKvZQCLcB/s1600/Cell%2BBE%2Bvs%2BShenWei%2BSW26010.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;181&quot; src=&quot;https://2.bp.blogspot.com/-rCGxhO2fVGw/V2jMRV379wI/AAAAAAAATSQ/l20liolD4jcU8ZxZbkejw5asAeZIOKvZQCLcB/s400/Cell%2BBE%2Bvs%2BShenWei%2BSW26010.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;IBM Cell BE vs. ShenWei SW26010.  &lt;a href=&quot;http://www.hec.nasa.gov/news/features/2008/cell.074208.html&quot;&gt;Cell diagram stolen from NAS&lt;/a&gt;; &lt;a href=&quot;http://www.netlib.org/utk/people/JackDongarra/PAPERS/sunway-report-2016.pdf&quot;&gt;SW26010 diagram stolen from the Dongarra report&lt;/a&gt;.&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&lt;br /&gt;The Cell processor was ahead of its time in many ways and arguably the first manycore chip targeted at HPC.  It had&lt;br /&gt;&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;a single controller core (the PPE) with L1 and L2 caches&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;eight simpler cores (the SPEs) on an on-chip network with no L2 cache, but an embedded SRAM scratchpad&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;div&amp;gt;and by comparison, the SW26010 has&amp;lt;/div&amp;gt;&lt;/p&gt;
&lt;div&gt;&lt;ul&gt;&lt;li&gt;a single controller core (the MPE) with L1 and L2 caches&lt;/li&gt;&lt;li&gt;sixty-four simpler cores (the CPEs) on an on-chip network with no L2 cache, but an embedded SRAM scratchpad&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;
&lt;p&gt;Of course, the similarities are largely superficial and there are vast differences between the two architectures, but the incorporation of heterogeneous (albeit very similar) cores on a single package is quite bold and is a design point that may play a role in exascale processor designs:&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;https://3.bp.blogspot.com/--seFJF-UhLw/V2jPfRKPXoI/AAAAAAAATSc/MXVgxyovM4YF0xo9k4XMlpbWY0TUJi80QCLcB/s1600/CQP3qklUsAAjcNT.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;300&quot; src=&quot;https://3.bp.blogspot.com/--seFJF-UhLw/V2jPfRKPXoI/AAAAAAAATSc/MXVgxyovM4YF0xo9k4XMlpbWY0TUJi80QCLcB/s400/CQP3qklUsAAjcNT.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;What an exascale processor might look like, as &lt;a href=&quot;https://twitter.com/hpc_guru/status/649645068995792896&quot;&gt;stolen from Kathy Yelick&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&lt;br /&gt;which may feature a combination of many lightweight cores (not unlike the CPE arrays on the TaihuLight processor) and are accompanied by a few capable cores (not unlike the MPE cores).&lt;br /&gt;&lt;br /&gt;The scratchpad SRAM present on all of the CPE cores is also quite intriguing, as it is a marked departure from the cache-oriented design of on-package SRAM that has dominated CPU architectures for decades.  The Dongarra report doesn’t detail how the scratchpad SRAM is used by applications, but it may offer a unique new way to perform byte-granular loads and stores that do not necessarily waste a full cache line’s worth of memory bandwidth if the application knows that memory access is to be unaligned.&lt;br /&gt;&lt;br /&gt;This is a rather forward-looking design decision that makes the CPU look a little more like a GPU.  Some experimental processor designs targeting exascale have proposed eschewing deep cache hierarchies in favor of similar scratchpads:&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;https://3.bp.blogspot.com/-swXDcTMnt4Q/V2jTH1YUpBI/AAAAAAAATSo/NDvIZdI53NMNIsP6ATzeIevJX4yPIQCBACLcB/s1600/Traleika%2BGlacier%2Bblock.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;187&quot; src=&quot;https://3.bp.blogspot.com/-swXDcTMnt4Q/V2jTH1YUpBI/AAAAAAAATSo/NDvIZdI53NMNIsP6ATzeIevJX4yPIQCBACLcB/s400/Traleika%2BGlacier%2Bblock.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;The Traleika Glacier processor design, featuring separate control and execution blocks and scratchpad SRAM.  Adapted from the &lt;a href=&quot;https://xstackwiki.modelado.org/Traleika_Glacier#Architecture&quot;&gt;Traleika Glacier wiki page&lt;/a&gt;.&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&lt;br /&gt;Whether or not we ever hear about how successful or unsuccessful these processor features are remains to be seen, but there may be valuable lessons to be learned ahead of the first generation of exascale processors from architectures like those in the TaihuLight system.&lt;br /&gt;&lt;br /&gt;&amp;lt;h2&amp;gt;Outlook&amp;lt;/h2&amp;gt;At a glance, it is easy to call out the irony in the U.S. government’s decision to ban the sale of Intel’s KNL processors to the Chinese now that the TaihuLight system is public.  It is clear that China is in a position to begin building extreme-scale supercomputers without the help of Intel, and it is very likely that the U.S. embargo accelerated this effort.  As pondered by an notable pundit in the HPC community,&lt;br /&gt;&lt;br /&gt;&amp;lt;blockquote class=&quot;twitter-tweet&quot;&amp;gt;&amp;lt;div dir=&quot;ltr&quot; lang=&quot;en&quot;&amp;gt;If US gov hadn’t barred US &lt;a href=&quot;https://twitter.com/hashtag/HPC?src=hash&quot;&gt;#HPC&lt;/a&gt; tech to China, new No.1 &lt;a href=&quot;https://twitter.com/hashtag/supercomputer?src=hash&quot;&gt;#supercomputer&lt;/a&gt; could’ve been &lt;a href=&quot;https://twitter.com/hashtag/KNL?src=hash&quot;&gt;#KNL&lt;/a&gt;-powered instead of Chinese CPUs? &lt;a href=&quot;https://twitter.com/hashtag/ISC16?src=hash&quot;&gt;#ISC16&lt;/a&gt; &lt;a href=&quot;https://twitter.com/hashtag/backfired?src=hash&quot;&gt;#backfired&lt;/a&gt;&amp;lt;/div&amp;gt;
— Andrew Jones (@hpcnotes) &lt;a href=&quot;https://twitter.com/hpcnotes/status/744976851567779841&quot;&gt;June 20, 2016&lt;/a&gt;&amp;lt;/blockquote&amp;gt; &lt;br /&gt;And this may have been the case.  However, despite the TaihuLight system’s #1 position and very noteworthy Linpack performance and efficiency, is not the massive disruptor that puts the U.S. in the back seat.  Underneath TaihuLight’s shiny, 93-petaflop veneer are some cut corners that substantially lower its ability to reliably deliver the performance and scientific impact commensurate to its Linpack score.  As &lt;a href=&quot;https://twitter.com/hpcprogrammer/status/744982095127248901&quot;&gt;pointed out by a colleague wiser than me&lt;/a&gt;, Intel’s impending KNL chip is the product of years of effort, and it is likely that it will be years before ShenWei’s chip designs and fabs are able to be really deliver a fully balanced, competitive, HPC-oriented microarchitecture.&lt;br /&gt;&lt;br /&gt;With that being said, TaihuLight is still a massive system, and even if its peak Linpack score is not representative of its actual achievable performance in solving real scientific problems, it is undeniably a leadership system.  Even if applications can only realize a small fraction of its Linpack performance, there is a lot of discovery to be made in petascale computing.&lt;br /&gt;&lt;br /&gt;Further, the SW201060 processor itself features some bold design points, and being able to test a heterogeneous processor with scratchpad SRAM at extreme scale may give China a leg up in the exascale architecture design space.  Only time will tell if these opportunities are pursued, or if TaihuLight follows its predecessors into an existence of disuse in a &lt;a href=&quot;http://www.marketwatch.com/story/chinas-bevy-of-supercomputers-goes-unused-2014-07-15&quot;&gt;moldy datacenter&lt;/a&gt; caused by a &lt;a href=&quot;http://www.scmp.com/news/china/article/1543226/chinas-world-beating-supercomputer-fails-impress-some-potential-clients&quot;&gt;high electric bill&lt;/a&gt;, &lt;a href=&quot;http://www.scmp.com/tech/science-research/article/1773421/chinese-supercomputer-too-slow-compete-race-hypersonic-weapons&quot;&gt;poor system design, and lack of software&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Spark, Chapel, TensorFlow- Workshop at UMich</title>
   <link href="https://hpc.social/2016/spark-chapel-tensorflow-workshop-at-umich/"/>
   <updated>2016-05-10T01:00:00-06:00</updated>
   <id>https://hpc.social/2016/spark-chapel-tensorflow-workshop-at-umich</id>
   <content type="html">&lt;p&gt;The kind folks at the University of Michigan’s &lt;a href=&quot;http://micde.umich.edu&quot;&gt;Center for Computational Discovery and Engineering (MICDE)&lt;/a&gt;, which is just part of the very impressive &lt;a href=&quot;http://arc.umich.edu&quot;&gt;Advanced Research Computing&lt;/a&gt; division, invited me to give a workshop there a couple of months ago about the rapidly-evolving large-scale numerical computing ecosystem.&lt;/p&gt;

&lt;p&gt;There’s lots that I want to do to extend this to a half-day length, but the workshop materials — including a VM that can be used to play with &lt;a href=&quot;http://spark.apache.org&quot;&gt;Spark&lt;/a&gt;, &lt;a href=&quot;http://chapel.cray.com&quot;&gt;Chapel&lt;/a&gt; and &lt;a href=&quot;https://www.tensorflow.org&quot;&gt;TensorFlow&lt;/a&gt;, along with Jupyter notebooks for each — can be found &lt;a href=&quot;https://github.com/ljdursi/Spark-Chapel-TF-UMich-2016&quot;&gt;on GitHub&lt;/a&gt; and may be of some use to others as they stand.&lt;/p&gt;

&lt;p&gt;The title and abstract follow.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;h4 id=&quot;next-generation-hpc--what-spark-tensorflow-and-chapel-are-teaching-us-about-large-scale-numerical-computing&quot;&gt;Next Generation HPC?  What Spark, TensorFlow, and Chapel are teaching us about large-scale numerical computing&lt;/h4&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;For years, the academic science and engineering community was almost alone in pursuing very large-scale numerical computing, and MPI - the 1990s-era message passing library - was the lingua franca for such work.  But starting in the mid-2000s, others became interesting in large-scale computing on data.  First internet-scale companies like Google and Yahoo! started performing fairly basic analytics tasks at enormous scale, and now many others are tackling increasingly complex and data-heavy machine-learning computations, which involve very familiar scientific computing tasks such as linear algebra, unstructured mesh decomposition, and numerical optimization.  But these new communities have created programming environments which emphasize what we’ve learned about computer science and programmability since 1994 - with greater levels of abstraction and encapsulation, separating high-level computation from the low-level implementation details, and some in HPC are starting to notice.  This talk will give a brief introduction to Apache Spark environment and Google’s Tensor Flow machine-learning package for high-level numerical computation, as well as the HPC-focused Chapel language from Cray, to show where each can be used today and how they might be used in the future.   The slides for this talk, and examples for each package along with a virtual machine which can be used for running them, will be available at https://github.com/ljdursi/Spark-Chapel-TF-UMich-2016 .&lt;/p&gt;

&lt;/blockquote&gt;
</content>
 </entry>
 
 <entry>
   <title>Approximate Mapping of Nanopore Squiggle Data with Spatial Indexing</title>
   <link href="https://hpc.social/2015/approximate-mapping-of-nanopore-squiggle-data-with-spatial-indexing/"/>
   <updated>2015-12-18T00:00:00-07:00</updated>
   <id>https://hpc.social/2015/approximate-mapping-of-nanopore-squiggle-data-with-spatial-indexing</id>
   <content type="html">&lt;p&gt;Over at the &lt;a href=&quot;http://simpsonlab.github.io/blog/&quot;&gt;Simpson Lab blog&lt;/a&gt;,
I have an post describing a novel method for &lt;a href=&quot;http://simpsonlab.github.io/2015/12/18/kdtree-mapping/&quot;&gt;Directly Mapping
Squiggle Data&lt;/a&gt;,
using k-d trees to map segmented kmers; a simple &lt;a href=&quot;https://github.com/ljdursi/simple-squiggle-pseudomapper&quot;&gt;proof of
concept&lt;/a&gt;
is available on github.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>On Random vs. Streaming I/O Performance; Or seek(), and You Shall Find --- Eventually.</title>
   <link href="https://hpc.social/2015/on-random-vs-streaming-i-o-performance-or-seek-and-you-shall-find-eventually/"/>
   <updated>2015-05-19T01:00:00-06:00</updated>
   <id>https://hpc.social/2015/on-random-vs-streaming-i-o-performance-or-seek-and-you-shall-find-eventually-</id>
   <content type="html">&lt;p&gt;At the &lt;a href=&quot;http://simpsonlab.github.io/blog/&quot;&gt;Simpson Lab blog&lt;/a&gt;, I’ve written a post
&lt;a href=&quot;http://simpsonlab.github.io/2015/05/19/io-performance/&quot;&gt;on streaming vs random access I/O performance&lt;/a&gt;,
an important topic in bioinformatics. Using a very simple problem (randomly choosing lines in a 
non-indexed text file) I give a quick overview of the file system stack and what it means for
streaming performance, and reservoir sampling for uniform random online sampling.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Understanding Partial Order Alignment for Multiple Sequence Alignment</title>
   <link href="https://hpc.social/2015/understanding-partial-order-alignment-for-multiple-sequence-alignment/"/>
   <updated>2015-05-01T01:00:00-06:00</updated>
   <id>https://hpc.social/2015/understanding-partial-order-alignment-for-multiple-sequence-alignment</id>
   <content type="html">&lt;p&gt;Over at the &lt;a href=&quot;http://simpsonlab.github.io/blog/&quot;&gt;Simpson Lab blog&lt;/a&gt;, I have an explainer 
on &lt;a href=&quot;http://simpsonlab.github.io/2015/05/01/understanding-poa/&quot;&gt;Understanding Partial Order Alignment&lt;/a&gt;,
an under-appreciated method for multiple sequence alignment; I hope the explanation there
(and &lt;a href=&quot;https://github.com/ljdursi/poapy&quot;&gt;explanatory implementation&lt;/a&gt;) is useful to those
exploring graph-based approaches to alignment.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>HPC+MPI on RCE Podcast</title>
   <link href="https://hpc.social/2015/hpc-mpi-on-rce-podcast/"/>
   <updated>2015-05-01T01:00:00-06:00</updated>
   <id>https://hpc.social/2015/hpc-mpi-on-rce-podcast</id>
   <content type="html">&lt;p&gt;In &lt;a href=&quot;http://www.rce-cast.com/Podcast/rce-97-jonathan-dursi.html&quot;&gt;the latest episode&lt;/a&gt; of the &lt;a href=&quot;http://www.rce-cast.com&quot;&gt;RCE podcast&lt;/a&gt;, Jeff Squyres, Brock Palen, and I spoke about the HPC and MPI series of blogposts and the community reaction.&lt;/p&gt;

&lt;p&gt;It was a really interesting discussion; Brock has worked closely with an enormous variety of researchers and helps run an HPC centre, while Jeff deeply understands HPC networking, from the getting ones and zeros onto the wires at the lowest-level of hardware up to being an extremely active member of the MPI forum.&lt;/p&gt;

&lt;p&gt;I was really pleased that they asked me to join them; I’ve been listening to their show since at least the VisIt episode in 2009 (I had just missed the Hadoop episode, it turns out) and for some years they were the only big computing podcast around.&lt;/p&gt;

&lt;p&gt;If you were interested in the MPI discussion, you might want to listen to this most recent episode; if you’re interested in big computing software projects more broadly, you should definitely consider subscribing to the podcast.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>More Conjecture on KNL's Near Memory</title>
   <link href="https://hpc.social/2015/more-conjecture-on-knl-s-near-memory/"/>
   <updated>2015-04-29T17:33:00-06:00</updated>
   <id>https://hpc.social/2015/more-conjecture-on-knl-s-near-memory</id>
   <content type="html">&lt;p&gt;The Platform ran &lt;a href=&quot;http://www.theplatform.net/2015/04/28/thoughts-and-conjecture-on-knights-landing-near-memory/&quot;&gt;an interesting collection of conjectures on how KNL’s on-package MCDRAM might be used&lt;/a&gt; this morning, and I recommend reading through it if you’re following the race to exascale.  I was originally going to write this commentary as a &lt;a href=&quot;https://plus.google.com/+glennklockwood/posts&quot;&gt;Google+ post&lt;/a&gt;, but it got a little long, so pardon the lack of a proper lead-in here.&lt;br /&gt;&lt;br /&gt;I appreciated Mr. Funk’s detailed description of how processor caches interact with DRAM, and how this might translate into KNL’s caching mode.  However, he underplays exactly why MCDRAM (and the GDDR on KNC) exists on these manycore architectures in his discussion on how MCDRAM may act as an L3 cache.  On-package memory is not simply another way to get better performance out of the manycore processor; rather, it is a hard requirement for keeping all 60+ cores (and their 120+ 512-bit vector registers, 1.8+ MB of L1 data cache, etc) loaded.  Without MCDRAM, it would be physically impossible for these KNL processors to achieve their peak performance due to memory starvation.  By extension, Mr. Funk’s assumption that this MCDRAM will come with substantially lower latency than DRAM might not be true.&lt;br /&gt;&lt;br /&gt;As a matter of fact, the massive parallelism game is not about latency at all; it came about as a result of latencies hitting a physical floor.  So, rather than drive clocks up to lower latency and increase performance, the industry has been throwing more but slower clocks at a given problem to mask the latencies of data access for any given worker.  While one thread may be stalled due to a cache miss on a Xeon Phi core, the other three threads are keeping the FPU busy to achieve the high efficiency required for performance.  This is at the core of the Xeon Phi architecture (as well as every other massively parallel architecture including GPUs and Blue Gene), so it is unlikely that Intel has sacrificed their power envelope to actually give MCDRAM lower latency than the off-package DRAM on KNL nodes.&lt;br /&gt;&lt;br /&gt;At an architectural level, accesses to MCDRAM still needs to go through memory controllers like off-package DRAM.  Intel hasn’t been marketing the MCDRAM controllers as “cache controllers,” so it is likely that the latencies of memory access are on par with the off-package memory controllers.  There are simply more of these parallel MCDRAM controllers (eight) operating relative to off-package DRAM controllers (two), again suggesting that bandwidth is the primary capability.&lt;br /&gt;&lt;br /&gt;Judging by current trends in GPGPU and KNC programming, I think it is far more likely that this caching mode acts at a much higher level, and Intel is providing it as a convenience for (1) algorithmically simple workloads with highly predictable memory access patterns, and (2) problems that will fit entirely within MCDRAM.  Like with OpenACC, I’m sure there will be some problems where explicitly on/off-package memory management (analogous to OpenACC’s copyin, copyout, etc) aren’t necessary and cache mode will be fine.  Intel will also likely provide all of the necessary optimizations in their compiler collection and MKL to make many common operations (BLAS, FFTs, etc) work well in cache mode as they did for KNC’s offload mode.&lt;br /&gt;&lt;br /&gt;However, to answer Mr. Funk’s question of “Can pre-knowledge of our application’s data use–and, perhaps, even reorganization of that data–allow our application to run still faster if we instead use Flat Model mode,” the answer is almost unequivocally “YES!”  Programming massively parallel architectures has never been easy, and magically transparent caches rarely deliver reliable, high performance.  Even the L1 and L2 caches do not work well without very deliberate application design to accommodate wide vectors; cache alignment and access patterns are at the core of why, in practice, it’s difficult to get OpenMP codes working with high efficiency on current KNC processors.  As much as I’d like to believe otherwise, the caching mode on KNL will likely be even harder to effectively utilize, and explicitly managing the MCDRAM will be an absolute requirement for the majority of applications.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Coarray Fortran Goes Mainstream- GCC 5.1</title>
   <link href="https://hpc.social/2015/coarray-fortran-goes-mainstream-gcc-5-1/"/>
   <updated>2015-04-26T01:00:00-06:00</updated>
   <id>https://hpc.social/2015/coarray-fortran-goes-mainstream-gcc-5-1</id>
   <content type="html">&lt;p&gt;This past week’s release of &lt;a href=&quot;https://gcc.gnu.org/gcc-5/&quot;&gt;GCC 5.1&lt;/a&gt; contains at least &lt;a href=&quot;https://gcc.gnu.org/gcc-5/changes.html&quot;&gt;two new features&lt;/a&gt; that are important to the big technical computing community: &lt;a href=&quot;https://gcc.gnu.org/wiki/Offloading&quot;&gt;OpenMP4/OpenACC offloading&lt;/a&gt; to Intel Phi/NVIDIA accellerators, and compiler support for &lt;a href=&quot;https://gcc.gnu.org/wiki/Coarray&quot;&gt;Coarray Fortran&lt;/a&gt;, with the communications layer provided by the &lt;a href=&quot;http://opencoarrays.org&quot;&gt;OpenCoarrays Project&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;While I don’t want to downplay the importance or technical accomplishment of the OpenMP 4 offloading now being available, I think it’s important to highlight the widespread availability for the first time of a tried-and-tested post-MPI programming model for HPC; and one that, since it is now part of the Fortran standard, is largely immune to fears that it might go away due to lack of interest. Here I’ll give a quick history of Coarray Fortran (CAF), some examples, and the pros and cons of CAF versus other approaches.&lt;/p&gt;

&lt;h2 id=&quot;a-quick-history-of-coarray-fortran&quot;&gt;A quick history of Coarray Fortran&lt;/h2&gt;

&lt;p&gt;Coarray Fortran first became widely known as Co-array Fortran, described in a &lt;a href=&quot;https://scholar.google.ca/scholar?cluster=8719640223898917361&amp;amp;hl=en&amp;amp;as_sdt=0,5&quot;&gt;1998 paper&lt;/a&gt; which described an implementation on Cray systems (T3Es and X1s) of a minimal extension to Fortran 95 which included distributed memory computing of enough complexity to allow real applications.&lt;/p&gt;

&lt;p&gt;The basic idea is simple enough from a developer’s point of view.  As with most MPI programs, a single program is launched across many processors.  Each “image” has its own local variables, as usual.  However, variables can also be defined to have a “co-dimension”; that is, a dimension which indexes that variable across all images.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-fortran&quot;&gt;program coarray1
  implicit none
  integer :: me, right, i
  integer, dimension(3), codimension[*] :: a

  me = this_image()

  right = me + 1
  if (right &amp;gt; num_images()) right = 1

  a(:) = [ (me**i, i=1, 3) ]

  sync all

  print *, &quot;Image &quot;, me, &quot; has a(2) = &quot;, a(2)[me], &quot;; neighbour has &quot;, a(2)[right]
end program coarray1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where square brackets refer to the co-index across images; recall that Fortran, somewhat unfortunately, uses parenthesis both for array indexing and for function arguments.  Note also that, in Fortran fashion, image numbers begin at 1.&lt;/p&gt;

&lt;p&gt;Running this on 4 images gives:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ./coarray1
Image            2  has a(2) =            4 ; neighbour has            9
Image            3  has a(2) =            9 ; neighbour has           16
Image            4  has a(2) =           16 ; neighbour has            1
Image            1  has a(2) =            1 ; neighbour has            4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;While it’s often the case that coarrays are also arrays – as is the case here with &lt;code&gt;a&lt;/code&gt; – that needn’t be true.  Scalar variables - variables with out array dimensions - can nonetheless have codimensions and thus be coarrays.&lt;/p&gt;

&lt;p&gt;Co-indexes needn’t be linear; one can also define co-dimensions of co-rank 2 or higher, to impose a grid-like pattern over the ranks.&lt;/p&gt;

&lt;p&gt;Co-array Fortran continued to be used on Cray systems, and was submitted as a proposal for inclusion into Fortran 2008.  A stripped-down version of the original proposal (losing such things as image “teams”, and the hyphen in Co-array) made it through, with some minor syntax changes.  The Cray Fortran compiler quickly adopted the standard, and &lt;a href=&quot;https://software.intel.com/en-us/articles/distributed-memory-coarray-fortran-with-the-intel-fortran-compiler-for-linux-essential&quot;&gt;Intel’s fFortran compiler&lt;/a&gt; has since version 12 supported SMP coarrays, and distributed-memory coarrays as part of the “Cluster suite” that includes Intel MPI.  IBM and PGI are said to be working on Coarray support. In less widely-used compilers, &lt;a href=&quot;http://web.cs.uh.edu/~openuh/&quot;&gt;OpenUH&lt;/a&gt; supported Coarrays quite early on, as did the now-defunct &lt;a href=&quot;http://www.g95.org&quot;&gt;G95&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A &lt;a href=&quot;http://isotc.iso.org/livelink/livelink?func=ll&amp;amp;objId=17064344&amp;amp;objAction=Open&quot;&gt;technical specification&lt;/a&gt; which is expected to make it into a future Fortran standard largely unscathed re-instates support for teams (giving overlapping functionality with MPI communicators for coordinating subsets of processes), and adds some collective operations, some atomic operations, and Events, which are something like &lt;a href=&quot;http://en.wikipedia.org/wiki/Monitor_(synchronization)&quot;&gt;condition variables&lt;/a&gt;.  GCC 5.1 supports many of these features already.&lt;/p&gt;

&lt;h2 id=&quot;examples&quot;&gt;Examples&lt;/h2&gt;

&lt;p&gt;Let’s take a look at a couple of simple examples to see how Coarray Fortran works in some familiar cases, and how the code complexity compares to MPI.&lt;/p&gt;

&lt;p&gt;We’ll see in part that, unlike with (say) Spark or Chapel examples from earlier in the month, in Coarray Fortran the developer is still responsible for explicitly decomposing the problem.  That means a lot that part of the boilerplate of the MPI versions of the code remains.  However, as communication patterns become more complex, the code can still simplify quite a bit.&lt;/p&gt;

&lt;p&gt;However, having the communications built into the language has another completely different advantage, one we’ve gotten used to not thinking about as we’re more used to using external libraries.  Communication being part of the language means that the compiler itself can perform high-level optimization on commuincations, just as it would with memory access.&lt;/p&gt;

&lt;h3 id=&quot;1d-diffusion-equation&quot;&gt;1D diffusion equation&lt;/h3&gt;

&lt;p&gt;Let’s take a look at a simple example I’ve used before, &lt;a href=&quot;https://github.com/ljdursi/coarray-examples/tree/master/diffusion&quot;&gt;1d diffusion&lt;/a&gt;.  Here, we have a 1D domain broken up across images, or MPI ranks, exchanging data just with nearest neighbours.&lt;/p&gt;

&lt;p&gt;Taking a look at the &lt;a href=&quot;https://github.com/ljdursi/coarray-examples/blob/bc356ec1dce3493c59800f1845c93bf18a6e7403/diffusion/diffusion-coarray.f90#L108&quot;&gt;CAF code&lt;/a&gt;, we have the data exchange part:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-fortran&quot;&gt;!
! exchange boundary information
!

   sync images(neighbours(1:nneighbours))
   if (this_image() /= 1) then
       temperature(1,old) = temperature(locnpoints+1,old)[left]
   endif
   if (this_image() /= num_images()) then
      temperature(locnpoints+2,old) = temperature(2,old)[right]
   endif

!
! update solution
!
   forall (i=2:locnpoints+1)
       temperature(i,new) = temperature(i,old) + &amp;amp;
             dt*kappa/(dx**2) * (                &amp;amp;
                  temperature(i+1,old) -         &amp;amp;
                2*temperature(i,  old) +         &amp;amp;
                  temperature(i-1,old)           &amp;amp;                           )
   end forall
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There’s a synchronize statement at the beginning, to make sure we don’t get ahead of any of our neighbours (or vice versa), and then we pluck the necessary data for our guardcells out of the coarray of temperature.&lt;/p&gt;

&lt;p&gt;This seems familiar, and indeed it’s not that different than the obvious &lt;a href=&quot;https://github.com/ljdursi/coarray-examples/blob/bc356ec1dce3493c59800f1845c93bf18a6e7403/diffusion/diffusion-mpi.f90#L107&quot;&gt;MPI implementation&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-fortran&quot;&gt;   !...

   call MPI_Sendrecv(temperature(locnpoints+1,old), 1, MPI_REAL, right, righttag,  &amp;amp;
             temperature(1,old), 1, MPI_REAL, left,  righttag, MPI_COMM_WORLD, rstatus, ierr)

   call MPI_Sendrecv(temperature(2,old), 1, MPI_REAL, left, lefttag,  &amp;amp;
             temperature(locnpoints+2,old), 1, MPI_REAL, right,  lefttag, MPI_COMM_WORLD, rstatus, ierr)

   !...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(and the update is exactly same).&lt;/p&gt;

&lt;p&gt;But having the exchange done in facilities built into the language has another benefit.  Let’s look back to the coarray version.  There’s a synchronization point, communications, computation, and (although we don’t see it here), a loop back to the synchronization point, as part of the iteration.&lt;/p&gt;

&lt;p&gt;The compiler will, as it does, perform reorderings that it can prove to itself don’t change the meaning of the code but will likely improve performance.  With memory increasingly a bottleneck, compilers frequently perform some sort of prefetch optimization to move requests for data from slow main memory forward, perform computations on data already cache for the ~200 cycles that access will take, and only then work on the data that hopefully has loaded.&lt;/p&gt;

&lt;p&gt;This optimization is familiar in the MPI world, of course; it’s overlapping communication with computation, and is performed using non-blocking Sends and Receives.  But because the communication is explicit to the compiler, it’s a difference of degree, not of kind, that the data is coming from over the network rather than from main memory.  Thus, this optimization is straightforwardly performed automatically by the compiler.&lt;/p&gt;

&lt;p&gt;On the other hand, it is much less automatic for a developer to rewrite &lt;a href=&quot;https://github.com/ljdursi/coarray-examples/blob/1acda1378398f3973a0066e09d89498a36769839/diffusion/diffusion-mpi-nonblocking.f90#L105&quot;&gt;the MPI code&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-fortran&quot;&gt;!
! begin exchange of boundary information
!

           call MPI_Isend(temperature(locnpoints+1,old), 1, MPI_REAL, &amp;amp;
                          right, righttag, MPI_COMM_WORLD, requests(1), ierr)
           call MPI_Isend(temperature(2,old), 1, MPI_REAL, &amp;amp;
                          left, lefttag,  MPI_COMM_WORLD, requests(2), ierr)
           call MPI_Irecv(temperature(1,old), 1, MPI_REAL, &amp;amp;
                          left,  righttag, MPI_COMM_WORLD, requests(3), ierr)
           call MPI_Irecv(temperature(locnpoints+2,old), 1, MPI_REAL, &amp;amp;
                          right, lefttag, MPI_COMM_WORLD, requests(4), ierr)

!
! update solution
!
           forall (i=3:locnpoints)
               temperature(i,new) = temperature(i,old) + &amp;amp;
                     dt*kappa/(dx**2) * (                &amp;amp;
                          temperature(i+1,old) -         &amp;amp;
                        2*temperature(i,  old) +         &amp;amp;
                          temperature(i-1,old)           &amp;amp;
                     )
           end forall
           time = time + dt

!
! wait for communications to complete
!
           call MPI_Waitall(4, requests, statuses, ierr)
!
! update solution
!
           temperature(2,new) = temperature(2,old) + dt*kappa/(dx**2) *  &amp;amp;
                        ( temperature(1,old) - 2*temperature(2, old) + temperature(3,old) )
           temperature(locnpoints+1,new) = temperature(locnpoints+1,old) + dt*kappa/(dx**2) *  &amp;amp;
                        ( temperature(locnpoints,old) - 2*temperature(locnpoints+1, old) + &amp;amp;
                          temperature(locnpoints+2,old) )
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;block-matrix-multiplication&quot;&gt;Block matrix multiplication&lt;/h3&gt;

&lt;p&gt;Let’s take a look at another example, a simple &lt;a href=&quot;https://github.com/ljdursi/coarray-examples/tree/master/blockmatrixmult&quot;&gt;block matrix multiplication&lt;/a&gt; where each image/task has one block of the A and B matrices, and we’re calculating \(C = A \times B\).&lt;/p&gt;

&lt;p&gt;In the &lt;a href=&quot;https://github.com/ljdursi/coarray-examples/blob/bc356ec1dce3493c59800f1845c93bf18a6e7403/blockmatrixmult/blockmatrix-coarray.f90#L38&quot;&gt;CAF version&lt;/a&gt;, this is almost embarrasingly easy:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-fortran&quot;&gt;    sync all
    c = 0.
    do k=1,ncols
        c = c + matmul(a[myrow,k],b[k,mycol])
    enddo
    sync all
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and the exchange not that bad in &lt;a href=&quot;https://github.com/ljdursi/coarray-examples/blob/bc356ec1dce3493c59800f1845c93bf18a6e7403/blockmatrixmult/blockmatrix-mpi.f90#L53&quot;&gt;the MPI version, either&lt;/a&gt;, using the SUMMA algorithm (Cannon’s, which can be better for small $P$, would have been messier):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-fortran&quot;&gt;    do k=0,ncols-1
        aremote = a
        bremote = b
        call MPI_Bcast(aremote, blockrows*blockcols, MPI_INTEGER, k, rowcomm, ierr)
        call MPI_Bcast(bremote, blockrows*blockcols, MPI_INTEGER, k, colcomm, ierr)
        c = c + matmul(aremote, bremote)
    enddo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;although it did take us a lot more boilerplate to get there; three communicators, explicit temporary arrays, etc:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-fortran&quot;&gt;    call MPI_Init(ierr)
    call MPI_Comm_size(MPI_COMM_WORLD, comsize, ierr)

	!...

    allocate(aremote(blockrows,blockcols))
    allocate(bremote(blockcols,blockrows))

	!...

    call MPI_Cart_create(MPI_COMM_WORLD, 2, dims, [1,1], 1, cartcomm, ierr)
    call MPI_Comm_rank(cartcomm, rank, ierr)
    call MPI_Cart_coords(cartcomm, rank, 2, coords, ierr)

    ! create row, column communicators
    call MPI_Comm_split( cartcomm, myrow, mycol, rowcomm, ierr )
    call MPI_Comm_split( cartcomm, mycol, myrow, colcomm, ierr )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and this is still a fairly straightforward communications pattern.  As communications become more complex, the advantage of it being performed implicitly becomes more clear.&lt;/p&gt;

&lt;h3 id=&quot;coarray-pros&quot;&gt;Coarray Pros&lt;/h3&gt;

&lt;p&gt;We’ve only looked at two examples, but that’s enough to get some feelings about the strengths and weaknesses of CAF vs other options:&lt;/p&gt;

&lt;h4 id=&quot;part-of-the-language&quot;&gt;Part of the Language&lt;/h4&gt;

&lt;p&gt;Compilers are enormously more sophisticated than they were twenty+ years ago, and using those optimization engines to our advantage in generating fast communications code is an enormous advantage.  Having the communications be explicit in the language enables the compiler to perform entire suites of automatic optimizations (prefetching, batching, memory/time tradeoffs) that can’t easily done with library-based approaches.&lt;/p&gt;

&lt;h4 id=&quot;stable&quot;&gt;Stable&lt;/h4&gt;

&lt;p&gt;One concern in the HPC community about trying new approaches is lingering doubt about whether a given new tool or language will be around five or ten years later; a concern that can become self-fulfilling.&lt;/p&gt;

&lt;p&gt;As part of the Fortran standard, Coarray Fortran is quite definitely here to stay; there are now several competing implementations, and competition will only improve them.&lt;/p&gt;

&lt;h4 id=&quot;incremental&quot;&gt;Incremental&lt;/h4&gt;

&lt;p&gt;Because Coarray Fortran uses a familiar model — Single Program, Multiple Data, with data manually decomposed — and only changes how the communications are expressed, there is very modest learning curve for developers already familiar with MPI, and very modest porting effort required.&lt;/p&gt;

&lt;p&gt;The familiarity extends in another dimension, as well; Coarray fortran is about as “authentically HPC” as it’s possible to get (Cray!  T3Es!  Fortran!) for a community that is sometimes skeptical of ideas from the outside.&lt;/p&gt;

&lt;p&gt;In addition, this incremental approach also makes interoperability with MPI relatively straightforward, for those requiring MPI-based library support.&lt;/p&gt;

&lt;h4 id=&quot;already-quite-fast&quot;&gt;Already Quite Fast&lt;/h4&gt;

&lt;p&gt;OpenCoarrays, which provides the communications support for gfortran’s coarray implementation, is &lt;a href=&quot;http://opencoarrays.org/yahoo_site_admin/assets/docs/pgas14_submission_7.30712505.pdf&quot;&gt;already comparable to and sometimes faster than&lt;/a&gt; typical MPI code and even faster in some cases the very-well tested Cray coarray implementation(!).  While this is still the first major release of gfortran coarrays, and performance improvements and doubtless bug fixes remain to be made, this is already a fairly solid and fast piece of software.&lt;/p&gt;

&lt;h3 id=&quot;coarray-cons&quot;&gt;Coarray Cons&lt;/h3&gt;

&lt;p&gt;On the other side of the ledger are primarily points we’ve already considered as Pros, but viewed from the glass-half-empty side:&lt;/p&gt;

&lt;h4 id=&quot;part-of-a-language&quot;&gt;Part of &lt;em&gt;A&lt;/em&gt; Language&lt;/h4&gt;

&lt;p&gt;Being built into a language means that it necessarily isn’t available to users of other languages.  I think this is largely inevitable for next-gen HPC approaches, to take full advantage of the compilers and runtimes that are now available, but it certainly will affect adoption; I can’t imagine too many C++ programmers will migrate to Fortran for their next project.  (Although it does start looking intriguing for Matlab or Python/Numpy users).&lt;/p&gt;

&lt;h4 id=&quot;stable-1&quot;&gt;Stable&lt;/h4&gt;

&lt;p&gt;As I’ve mentioned in the context of MPI, too much stability can be a bad thing, and the Fortran committee makes the MPI Forum look like a squirrel on cocaine.  I’m less concerned about that here in the short term, since the Coarrays that went into the standard were based on a model that had been used for years successfully, and new features are already in the works; but any additional new features that are seen to be needed may well be a long time coming.&lt;/p&gt;

&lt;h4 id=&quot;incremental-1&quot;&gt;Incremental&lt;/h4&gt;

&lt;p&gt;That Coarrays are incremental certainly makes it easier to port existing code, but it means that many of my concerns about MPI as a development environment remain unaddressed.  A researcher or application developer still has to perform the manual decomposition of a problem.  This requires an enormous amount of eminently automatable boilerplate and zillions of opportunities for meaningless bugs like off-by-one errors.  (That sort of bookkeeping is precisely what computers are better at than developers!)  That burden also means that substantial amounts of code must be rewritten if the decomposition changes.&lt;/p&gt;

&lt;h4 id=&quot;already-quite-fast-1&quot;&gt;Already Quite Fast&lt;/h4&gt;

&lt;p&gt;…Ok, it’s hard to see much of a downside here.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;The release of gcc-5.1 with coarray support is going to be the first time a huge number of HPC developers have ready access to coarrays.  From my point of view, it’s notably less ambitious than a large number of projects out there, but that may well make it easier to adopt for a sizable community.  Certainly anyone planning to start a new project in Fortran should give it very serious consideration.&lt;/p&gt;

&lt;p&gt;My own hope is that Coarray Fortran will have a large number of delighted users, some of whose appetite then becomes whetted for other still more productive languages and environments for large-scale technical computing.  In the next few posts, I’ll take a closer look at some of those.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>In Praise of MPI Collectives and MPI-IO</title>
   <link href="https://hpc.social/2015/in-praise-of-mpi-collectives-and-mpi-io/"/>
   <updated>2015-04-19T01:00:00-06:00</updated>
   <id>https://hpc.social/2015/in-praise-of-mpi-collectives-and-mpi-io</id>
   <content type="html">&lt;p&gt;While I have a number of posts I want to write on other topics and technologies, there is one last followup I want to make to &lt;a href=&quot;http://www.dursi.ca/hpc-is-dying-and-mpi-is-killing-it/&quot;&gt;my MPI post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Having said what I think is wrong about MPI (the standard, not the implementations, which are of very high quality), it’s only fair to say something about what I think is very good about it.  And &lt;em&gt;why&lt;/em&gt; I like these parts gives lie to one of the most common pro-MPI arguments I’ve been hearing for years; that application programmers coding at low levels is somehow essential - or even just a good idea - for performance.&lt;/p&gt;

&lt;h2 id=&quot;two-great-things-about-mpi&quot;&gt;Two great things about MPI&lt;/h2&gt;

&lt;h3 id=&quot;collective-operations&quot;&gt;Collective Operations&lt;/h3&gt;
&lt;p&gt;Since the very beginning, MPI has defined a suite of &lt;a href=&quot;https://computing.llnl.gov/tutorials/mpi/#Collective_Communication_Routines&quot;&gt;collective communications&lt;/a&gt; that include operations like scatter, gather, &lt;a href=&quot;http://en.wikipedia.org/wiki/Prefix_sum&quot;&gt;prefix scan&lt;/a&gt;, and reduce.  While these weren’t invented by MPI – many were already implemented as “global communications” routines in the &lt;a href=&quot;http://en.wikipedia.org/wiki/Connection_Machine&quot;&gt;CM-2’s&lt;/a&gt; &lt;a href=&quot;http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=365582&quot;&gt;Connection Machine Scientific Software Library&lt;/a&gt;, for instance, and there is lots of literature on implementing those operations on other architectures like the iPSC/860-based hypercube systems – it’s certainly fair to say that it was MPI that popularized them to the point that they’ve started getting &lt;a href=&quot;http://www.mellanox.com/page/products_dyn?product_family=104&amp;amp;menu_section=73&quot;&gt;hardware support in network cards&lt;/a&gt;. The popularization stems partly from how widely taught MPI is, but also from useful generalizations that the MPI Forum made, like user-defined reduction operations, or being able to perform these operations on user-defined subsets of tasks.&lt;/p&gt;

&lt;p&gt;A classic use of MPI collective operations would be using a reduce to find a global sum (or max, or min, or a user defined operation) of local values:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from mpi4py import MPI
import random

comm = MPI.COMM_WORLD
rank = comm.Get_rank()
nprocs = comm.Get_size()

local = random.random()

globalsum = comm.reduce(local, op=MPI.SUM, root=0)
globalmin = comm.reduce(local, op=MPI.MIN, root=0)
globalmax = comm.reduce(local, op=MPI.MAX, root=0)

if rank == 0:
    print &quot;Min, mean, max = &quot;, globalmin, globalsum/nprocs, globalmax
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;mpi-io&quot;&gt;MPI-IO&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;http://beige.ucs.indiana.edu/I590/node86.html&quot;&gt;MPI-IO&lt;/a&gt; is the foundational middleware for HPC parallel I/O.  &lt;a href=&quot;https://hdfgroup.org/HDF5/PHDF5/&quot;&gt;Parallel HDF5&lt;/a&gt; (and thus &lt;a href=&quot;http://www.unidata.ucar.edu/software/netcdf/docs_rc/parallel_io.html&quot;&gt;Parallel NetCDF4&lt;/a&gt;), &lt;a href=&quot;https://www.olcf.ornl.gov/center-projects/adios/&quot;&gt;ADIOS&lt;/a&gt;, and others are built on top of it.  As a result, even application software that doesn’t explicitly use MPI sometimes relies on MPI-IO for reading and writing large files in parallel.&lt;/p&gt;

&lt;p&gt;The key concept in MPI-IO is a “file view”, which describes (in terms of MPI data layouts) where in the file a process will be writing.  Once that’s done, writing data to the file just looks like sending a message to the file.  A trivial example follows below; more complex data layouts like (as often happens in scientific computing) non-contiguous slices of large multidimensional arrays being read and written would look exactly the same:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from mpi4py import MPI

comm = MPI.COMM_WORLD
rank = comm.Get_rank()
nprocs = comm.Get_size()

myString = 'Hello ' if rank % 2 == 0 else 'World!'
stringSize = 6

subarray = MPI.CHAR.Create_subarray( (stringSize*nprocs,), (stringSize,), (stringSize*rank,))
subarray.Commit()

filehandle = MPI.File.Open(comm, 'ioexample.txt', MPI.MODE_CREATE | MPI.MODE_WRONLY)
filehandle.Set_view(0, MPI.CHAR, subarray)
filehandle.Write_all(myString)

filehandle.Close()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;why-theyre-great&quot;&gt;Why they’re great&lt;/h2&gt;

&lt;p&gt;These two very different parts of the MPI standard have three important features in common for this discussion.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;They’re at much higher levels of abstraction than most of the API&lt;/li&gt;
  &lt;li&gt;Application programmers would get worse performance, not better, if they tried to implement their own at lower levels.&lt;/li&gt;
  &lt;li&gt;Original implementations of these APIs didn’t perform nearly as well as current implementations.  But algorithmic and implementation work done by software engineers greatly sped the low level implementations up without applications programmers needing to rewrite their code.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;collectives-and-mpi-io-are-higher-levels-of-abstraction&quot;&gt;Collectives and MPI-IO are higher levels of abstraction&lt;/h3&gt;

&lt;p&gt;Calls to MPI collective operations or MPI-IO describe what should be done, not how to do it, and at a much higher level than &lt;code&gt;MPI_Send()/MPI_Put()&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Operations like “All processes sum their results and distribute the result to all processes”, or “Each process writes to their slice of the file” are enormously broader than “Send this message to process X”.  There’s a large number of ways they could be implemented, and in fact there’s a huge literature on both &lt;a href=&quot;https://scholar.google.ca/scholar?q=mpi+collectives&quot;&gt;collectives&lt;/a&gt; and &lt;a href=&quot;https://scholar.google.ca/scholar?q=mpi-io&quot;&gt;MPI-IO&lt;/a&gt; on various approaches to doing so.&lt;/p&gt;

&lt;h3 id=&quot;application-programmers-reimplementing-them-would-be-worse-for-performance&quot;&gt;Application programmers reimplementing them would be worse for performance&lt;/h3&gt;

&lt;p&gt;If the “low-level application programming is essential for high performance” argument was true, then of course we would be actively dissuading researchers from using these high-level tools.  But we don’t, and we’re right not to.&lt;/p&gt;

&lt;p&gt;Most of us who have worked with enough researchers writing their own HPC codes have had the experience of someone coming into our office who was broadcasting data with a loop over &lt;code&gt;MPI_Send()&lt;/code&gt;s, or trying to write to a shared file using &lt;code&gt;fseek()&lt;/code&gt; or the like, and we’ve directed them to collective operations or MPI-IO instead.  We do the same, of course, when someone is trying to type in some Gaussian Elimination code from Numerical Recipes (no link; that book has done enough damage) and we guide them to our local &lt;a href=&quot;http://en.wikipedia.org/wiki/LAPACK&quot;&gt;LAPACK&lt;/a&gt; implementation instead.&lt;/p&gt;

&lt;p&gt;And we do this because even we don’t believe that scientists implementing these things at low level will give better performance.  It’s not about it being “too hard”; it’s something else entirely.  We know that it would be a huge amount of wasted effort for a &lt;em&gt;worse&lt;/em&gt;, &lt;em&gt;slower&lt;/em&gt;, result.&lt;/p&gt;

&lt;p&gt;MPI collective operation implementations make run-time decisions behind the researchers back, based on the size of the data, and the structure of the communicator being used, to decide whether to use k-ary trees, or hyper-cubes, or split-ring approaches, and in one, two, or multiple phases of communications, to perform the operation.  MPI-IO implementations uses approaches like data-sieving or two-phase I/O to trade off network communication for disk I/O, and use close integration with the filesystem to inform that tradeoff.&lt;/p&gt;

&lt;p&gt;Somebody had to do all that challenging low-level work, yes.  But the idea that those optimizations and algorithmic work is properly the job of the researcher/application programmer is absurd.&lt;/p&gt;

&lt;h3 id=&quot;implementations-got-faster-and-faster&quot;&gt;Implementations got faster and faster&lt;/h3&gt;

&lt;p&gt;These highly optimized implementations of these high-level abstractions did not, of course, spring fully formed from somewhere, any more than the &lt;a href=&quot;http://www.netlib.org/lapack/&quot;&gt;reference implementation of LAPACK/BLAS&lt;/a&gt; was blazingly fast.  The abstractions were created with an understanding of both what application programmers needed and what was implementable, and then years and years of work went into developing the algorithms and implementations that we make use of today.&lt;/p&gt;

&lt;p&gt;Initial implementations of MPI-1 collectives were (naturally!) not super optimized, and there were certainly developers who scoffed at the performance and who pointed out they could do better writing low-level network code on their own.  They were, in that snapshot in time, narrowly correct; but more broadly and in the longer term, they were flat-out wrong.  The most useful and productive approach to a researcher finding out that early versions of those collective operations (say) were slow in some situations was not to break down and re-implement it themselves at low level; it was to file an issue with the library provider, and help them fix it so that it would be faster for everyone.&lt;/p&gt;

&lt;h2 id=&quot;these-points-generalize&quot;&gt;These points generalize&lt;/h2&gt;

&lt;p&gt;I don’t think anything I’ve said above is particuarly controversial. Performance, as well as productivity, for researchers and applications programmers has clearly improved as a result of MPI’s collectives and MPI-IO.&lt;/p&gt;

&lt;p&gt;But for some reason, the idea that this generalizes — that performance as well as productivity of scientific software development would improve if applications developers spent their time using other, newer higher-level constructs while more tool-builders implemented those constructs in efficient ways — is anathaema to a section of our HPC community.&lt;/p&gt;

&lt;p&gt;I’ve yet to hear compelling reasons why operations on distributed multidimensional arrays, or hash tables, or trees, are completely different from collectives or IO; why application programmers have to implement them directly or indirectly in a low-level tool like MPI sends and receives or gets and puts rather than having them implemented by experts in higher-level environments like Chapel, or Spark, or Ignite, or any of a zillion other projects from within or outside of the HPC community.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Objections, Continued</title>
   <link href="https://hpc.social/2015/objections-continued/"/>
   <updated>2015-04-09T01:00:00-06:00</updated>
   <id>https://hpc.social/2015/objections-continued</id>
   <content type="html">&lt;p&gt;Thanks for all of the comments about &lt;a href=&quot;http://dursi.ca/hpc-is-dying-and-mpi-is-killing-it/&quot;&gt;my HPC and MPI post&lt;/a&gt;, on the post itself, or on twitter, or via email.  While much of the comments and discussions were positive, it won’t surprise you to learn that there were objections, too; so I thought I’d keep updating the Objections section in a new post.  I’ve also posted &lt;a href=&quot;http://www.dursi.ca/in-praise-of-mpi-collectives-and-mpi-io/&quot;&gt;one (hopefully last) followup&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;But do keep sending in your objections!&lt;/p&gt;

&lt;h2 id=&quot;further-objections&quot;&gt;Further Objections&lt;/h2&gt;
&lt;h3 id=&quot;youre-saying-wed-have-to-rewrite-all-our-code&quot;&gt;You’re saying we’d have to rewrite all our code!&lt;/h3&gt;

&lt;p&gt;If someone had suggested I add this objection to the original list before publishing, I would have rejected it as too straw-man to use; I’d be transparently putting this objection up just to demolish it. Clearly, no one would actually claim that “the HPC community should urgently start engaging with and using new technical computing technologies” means “you have to burn all your old stuff to the ground”.&lt;/p&gt;

&lt;p&gt;But sure enough, it came up &lt;em&gt;frequently&lt;/em&gt;, in private email, and most dramatically, &lt;a href=&quot;https://twitter.com/KpDooty/status/585582597746622464&quot;&gt;on twitter&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Even though this is by far the most common reaction I got, I hope it’s clear to most readers these aren’t the same things.  Learning (say) C++ and using it in development of new codes doesn’t mean your old C and Fortran stuff stops working.  Or that you’re under an obligation to take the working code in other languages and re-write it all in the new language before ever using it again to maintain some kind of computational moral consistency.&lt;/p&gt;

&lt;p&gt;Your MPI code won’t stop working for you in a fit of rage because you’re seeing other frameworks.  MPI will continue to work and be maintained, exactly because there is 20+ years worth of stuff using it.&lt;/p&gt;

&lt;p&gt;But &lt;strong&gt;new&lt;/strong&gt; software projects are being started every day, in every field, in every region.  This argument is about what we should use for those codes. “Because we’ve always done it that way” isn’t a great reason for a community that’s supposed to be on the cutting edge of computing to keep doing things in one particular framework.&lt;/p&gt;

&lt;h3 id=&quot;big-data-and-hpc-are-completely-different-and-its-ridiculous-to-compare-them&quot;&gt;Big data and HPC are completely different, and its ridiculous to compare them&lt;/h3&gt;

&lt;p&gt;This was a close second in popularity. And this one worries me quite a bit, because it means that there’s a lot of people in our community that are disturbingly unaware what’s going on in computing and data analysis outside of the confines of their office.&lt;/p&gt;

&lt;p&gt;It’s absolutely true that there are Big-Data-y things that are mainly just I/O with a little bit of processing.  But by and large people want to &lt;em&gt;analyze&lt;/em&gt; that large amount of data. And then you end up with absolutely classic big numerical computing problems. To take an early example, Page Rank is, after all, &lt;a href=&quot;http://en.wikipedia.org/wiki/PageRank#History&quot;&gt;an eigenvalue problem&lt;/a&gt;.  The drive for next-generation big data platforms like Spark is no small part to make machine learning algorithms that would be very familiar to us run as efficiently as possible.  Let’s take some example machine learning approaches:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Spectral_clustering&quot;&gt;Spectral clustering&lt;/a&gt; solves an &lt;a href=&quot;https://charlesmartin14.wordpress.com/2012/10/09/spectral-clustering/&quot;&gt;equation for the graph Laplacian&lt;/a&gt; - which looks exactly like any other &lt;a href=&quot;https://www.dursi.ca/spectral clustering heat equation&quot;&gt;parabolic PDE on an unstructured mesh&lt;/a&gt;.  (Thanks to Lorena Barba for &lt;a href=&quot;https://twitter.com/LorenaABarba/status/586515529973764096&quot;&gt;pointing out an embarrasing mistake&lt;/a&gt; in an earlier version of that point.)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Support_vector_machine&quot;&gt;Support Vector Machines&lt;/a&gt; are kernel based methods which involve Green’s functions and 1st order integral equations.&lt;/li&gt;
  &lt;li&gt;Much of machine learning involves fitting a model, which means that there are entire &lt;a href=&quot;http://mitpress.mit.edu/books/optimization-machine-learning&quot;&gt;books&lt;/a&gt; written about large-scale efficient optimization solvers for machine learning, including physical science chestnuts like &lt;a href=&quot;http://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;&gt;gradient descent&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;A common first step in data analysis is dimensional reduction involving (say) &lt;a href=&quot;http://en.wikipedia.org/wiki/Principal_component_analysis&quot;&gt;PCA&lt;/a&gt;, requiring the SVD (or similar factorizations) of huge matricies.&lt;/li&gt;
  &lt;li&gt;In fact, Linear Algebra is omnipresent in machine learning (as it has to be with so much, eg, model fitting), to the point that there are entire &lt;a href=&quot;http://stanford.edu/~rezab/nips2013workshop/&quot;&gt;conferences&lt;/a&gt; on large-scale linear algebra for machine learning.&lt;/li&gt;
  &lt;li&gt;A lot of the data analyses involve statstical bayesian inference, requiring &lt;a href=&quot;http://link.springer.com/article/10.1023%2FA%3A1020281327116&quot;&gt;MCMC&lt;/a&gt; calculations.&lt;/li&gt;
  &lt;li&gt;k-Nearest-Neighbour problems in clustering, kernel density methods, and many other techniques relying on something like a distance or similarity metric require classic N-body solutions like &lt;a href=&quot;https://books.google.ca/books?id=6GvSBQAAQBAJ&amp;amp;pg=PA162&amp;amp;lpg=PA162&amp;amp;dq=k-d+trees+machine+learning&amp;amp;source=bl&amp;amp;ots=GdA2RtbSvY&amp;amp;sig=JStlVpNy5CB8cJewtFYPIb53QCI&amp;amp;hl=en&amp;amp;sa=X&amp;amp;ei=cRknVeH9OIG5sAWMkoCgAQ&amp;amp;ved=0CE4Q6AEwCDgK#v=onepage&amp;amp;q=k-d%20trees%20machine%20learning&amp;amp;f=false&quot;&gt;k-D trees&lt;/a&gt;; and if positions are being updated, they essentially become &lt;a href=&quot;http://www.cs.cmu.edu/~agray/nips-final.pdf&quot;&gt;N-body problems&lt;/a&gt;.  And of course, an entire class of high-dimensional optimization problems often used in machine learning are essentially &lt;a href=&quot;http://en.wikipedia.org/wiki/Particle_swarm_optimization&quot;&gt;tracer particle methods&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;As a result of all this high mathematical intensity, machine learning is of course becoming a rapidly growing user of &lt;a href=&quot;https://registration.gputechconf.com/form/session-listing&amp;amp;doSearch=true&amp;amp;additional_parameter_selector=none&amp;amp;queryInput=&amp;amp;topic_selector=Machine+Learning+%26+Deep+Learning&amp;amp;type_selector=none&quot;&gt;GPUs for their numerical algorithms&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So let’s see; PDEs on unstructured meshes, optimization, gradient descent, large-scale linear algebra, particle methods, GPUs. And of course, time series data of any sort means FFTs. So sure, I don’t know what is running on &lt;em&gt;your&lt;/em&gt; HPC cluster, but is it really that different than the above?&lt;/p&gt;

&lt;h3 id=&quot;mpi-is-great-for-physics-even-if-less-great-for-the-other-stuff&quot;&gt;MPI is great for physics, even if less great for the other stuff&lt;/h3&gt;

&lt;p&gt;I got this by email and on twitter several times.&lt;/p&gt;

&lt;p&gt;Great compared to what? And based on what evidence?&lt;/p&gt;

&lt;p&gt;Say a physics grad student walks in to your office who’s going to develop a small bespoke particle code for their dissertation.  Pointing them to MPI, rather than other technologies with unimpeachable HPC bona fides like UPC, Chapel, Co-array Fortran, or, (for a particle simulation especially) Charm++ seems like it’s the lazy, easy way for &lt;em&gt;us&lt;/em&gt;, and less about what’s actually best for &lt;em&gt;them&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In what sense is it “great for physics” to have the student increase the amount of code they have to write, and debug by a factor of 3x?  In what sense is it great for them to have to re-invent all of the low-level communications algorithms which have been implemented better, in other packages?  Maybe you could make an argument about stability or performance against UPC/Chapel (although I’d counter-argue you’d get immediate and helpful support from the developers) - what’s the argument against pointing the student to Charm++?  Or Intel’s CAF?&lt;/p&gt;

&lt;p&gt;And this doesn’t even begin to cover things like Spark, Flink, or &lt;a href=&quot;http://ignite.incubator.apache.org/index.html&quot;&gt;Ignite&lt;/a&gt; - for simulation, or experimental physics work (which is physics too, right?), which is necessarily heavy on data analysis.&lt;/p&gt;

&lt;h3 id=&quot;youre-just-saying-mpi-is-too-hard&quot;&gt;You’re just saying MPI is too hard&lt;/h3&gt;

&lt;p&gt;I’m really not. As a community, we don’t mind hard.  Solving complex equations is hard, that’s just how it is.  We eat hard for breakfast.  (And the genomics and big-data communities are the same way, because they’re also filled with top-notch people with big computational problems).&lt;/p&gt;

&lt;p&gt;I’m saying something different: MPI is needlessly, pointlessly, and uselessly a huge sink of researcher and toolbuilder effort for little if any reward.&lt;/p&gt;

&lt;p&gt;How many grad students have had to tediously decompose a 2d or 3d grid by hand, write halo exchange code, get it debugged and running, run in that crude fasion for a while, then tried moving to move to overlapped communication and computation, and spent days or weeks trying to get that to work efficiently - and then had to re-write chunks as they need a new variable laid out differently (or just implemented a really bad transposition?) and still gotten performance that an expert would consider poor?&lt;/p&gt;

&lt;p&gt;And regular grid codes are the easy stuff; how many scientist-decades worth of efforts have gone into implementing and re-implementing tree codes or unstructured meshes; and by and large resulting in efficiencies ranging from “meh” to “ugh”?&lt;/p&gt;

&lt;p&gt;Wouldn’t it be better to have experts working on the common lower level stuff, tuning it and optimizing it, so that the scientists can actually focus on the numerics and not the communications?&lt;/p&gt;

&lt;p&gt;The stuff about levels of abstraction isn’t some aesthetic philosophical preference. And I’m not complaining MPI because it’s hard; I’m complaining about it because it’s resulted in an enormous waste of researcher time and compute resources.  Let the scientists focus on hard stuff that matters to their research, not the stuff that can be effectively outsourced to builders.&lt;/p&gt;

&lt;p&gt;Now, we at centres could at least improve &lt;em&gt;this&lt;/em&gt; dreadful state of affairs even with MPI just by doing a better job pointing researchers embarking on a code project to libraries and packages like Trillinos or what have you, and stop counseling them to write raw MPI code themselves.  But of course, we normally don’t, because we keep telling ourselves and the incoming grad students “MPI is great for physics”…&lt;/p&gt;

&lt;h3 id=&quot;its-important-for-students-to-know-whats-going-on-under-the-hood-even-if-theyre-using-other-frameworks&quot;&gt;It’s important for students to know what’s going on under the hood, even if they’re using other frameworks&lt;/h3&gt;

&lt;p&gt;I do have some sympathy for this point, I will admit.&lt;/p&gt;

&lt;p&gt;But anyone who thinks teaching generation after generation of grad students how to manually decompose a 2d mesh and do halo exchange on it using &lt;code&gt;MPI_Sendrecv()&lt;/code&gt; is a productive and rewarding use of time, is someone who doesn’t spend enough time doing it.&lt;/p&gt;

&lt;p&gt;As with other pro-low-level arguments: why is MPI automatically the right level to stop at? If we want to teach students how things really work under the covers, why aren’t we going all the way down to Infiniband or TCP/IP, user mode and kernel mode, and the network stack?  Or, why don’t we stop a level or two above, draw some diagrams on a whiteboard, and move on to actually solving equations?  Why is MPI in particular the right “under the hood” thing to teach, as opposed to GASNet, Charm++, or just pseudo-network-code?&lt;/p&gt;

&lt;p&gt;If the answer to the questions above is “because MPI is what we know and have slides for”, then we need to think about what that implies, and how well it’s serving the research community.&lt;/p&gt;

&lt;h3 id=&quot;but-my-new-code-will-need-libraries-based-on-mpi-that-arent-supported-by-chapelupcsparkother-stuff-yet&quot;&gt;But my new code will need libraries based on MPI that aren’t supported by Chapel/UPC/Spark/other stuff yet!&lt;/h3&gt;

&lt;p&gt;Fair enough. When you choose what you are going to use to write a program, library and tool support really matter.  It’s absolutely true that there are great packages that use MPI, and if your project is going to rely on them, then this isn’t an example of a good project to start expermenting with a new platform on.  This is why such a large fraction of numerical code was in FORTRAN77 for so long.&lt;/p&gt;

&lt;p&gt;Co-array Fortran, Chapel, and others do have various degree of MPI interoperability, so do check that out; but yes, you need what you need.&lt;/p&gt;

&lt;h3 id=&quot;but-people-are-starting-to-build-things-based-on-mpi-3-rma&quot;&gt;But people &lt;em&gt;are&lt;/em&gt; starting to build things based on MPI-3 RMA!&lt;/h3&gt;

&lt;p&gt;This &lt;a href=&quot;http://dursi.ca/hpc-is-dying-and-mpi-is-killing-it/#comment-1952126251&quot;&gt;coment by Jeff on the original post&lt;/a&gt;, is by some measure the most interesting objection I’ve heard so far.&lt;/p&gt;

&lt;p&gt;People are legitimately starting to use MPI-3 RMA in the underlying implementations of higher level tools. If that really took off, then my arguments about MPI not being the right level of abstraction for toolbuilders would clearly be wrong, and a huge part of my post would be rendered irrelevant.&lt;/p&gt;

&lt;p&gt;In that case, I would be completely wrong – and it would be awesome!  A higher-level toolset for researchers could finally flourish, the lower level stuff could be handled by a completely separate group of experts, and MPI would have found its place.&lt;/p&gt;

&lt;p&gt;I want to be clear that I think it would be fantastic - really, the best of all possible worlds - to be wrong in this way.&lt;/p&gt;

&lt;p&gt;I’m going to describe why I really don’t think I am, and what the stumbling blocks are.  Then I’ll discuss an alternate future which sidesteps the worst of those problems, and how it really could be a path to a very productive and growing HPC future - but it will of course never, ever, happen.&lt;/p&gt;

&lt;p&gt;So MPI-3 - useful RMA, being used.  Perfect!  To see the problem that concerns me here, consider two questions; (1) What are the benefits of using MPI for this, and (2) what are the downsides?&lt;/p&gt;

&lt;p&gt;On the upside, it’s great that MPI is sufficient to implement these tools.  But is it necessary?  What is the advantage of using something like MPI over something else, and in particular something lower level?  Maybe it would be a little easier or a little harder, but would it make a big difference?  Particularly to the end-user of the tool being built?&lt;/p&gt;

&lt;p&gt;I doubt it makes much difference either way; the reason I ask is the downside.&lt;/p&gt;

&lt;p&gt;MPI-3 RMA doesn’t come on its own; it’s part of MPI.  And in this context, I’m concerned with two real downsides with using even great parts of MPI for low-level toolbuilding.  They’re related: the heavy-weight forum process, and the enormous baggage of backwards compatability.&lt;/p&gt;

&lt;p&gt;Let’s take the forum process first.  Let’s say there’s two competing tools you could use to build your next lower-layer tool; MPI-3 RMA and some other low-level network abstraction layer.  (What I’m picturing is something like &lt;a href=&quot;https://github.com/ofiwg/libfabric&quot;&gt;OFWG Libfabric&lt;/a&gt;, which you can probably tell I’m quite taken with, but that’s not really right here.  But something at roughly that level or a little higher).&lt;/p&gt;

&lt;p&gt;You’re starting to build your new tool, which contains a number of really innovative ideas; but now you’ve discovered you need one additional feature in either package.&lt;/p&gt;

&lt;p&gt;Which will get you there first?&lt;/p&gt;

&lt;p&gt;The MPI forum was really able to innovate with MPI-3 RMA, because they were nearly starting afresh - or at least complementary with what had gone before.  But now that MPI-3 is out, and a number of projects have used it, the spec is essentially encased in carbonite; the API in its every last detail will outlive us all.  None of the existing APIs will change.&lt;/p&gt;

&lt;p&gt;That’s ok, because the Forum has shown its willingness to add new functions to the spec when justified.  Your case sounds interesting; you should get your answer in a couple of years or so.&lt;/p&gt;

&lt;p&gt;And that’s kind of crazy for a low-level network abstraction layer.  The other package - whatever it is - won’t have that sort of friction.&lt;/p&gt;

&lt;p&gt;There’s another issue in terms of new features; that’s the backwards compatability legacy.&lt;/p&gt;

&lt;p&gt;Let’s take something like fault tolerance, which is important at extreme scale - but will eventually get important for more moderate scales, as well.&lt;/p&gt;

&lt;p&gt;For a really low-level network abstraction, dealing withfault tolerance isn’t an enormous difficulty.  For something higher level like MPI-3 RMA, it’s more challenging, but it’s still something where one could imagine how it might go.&lt;/p&gt;

&lt;p&gt;But for MPI-3+ to develop a feature like fault tolerance, it will have to be created in such a way that it integrates seamlessly with every single MPI feature that has ever existed, without altering any of the semantics of a single one of those calls. The backwards compatability requirements are crushing.&lt;/p&gt;

&lt;p&gt;So this is sort of the tragedy of MPI-3 RMA. It’s a great thing that may have just come too late in the lifecycle of a project to be able to have its full impact.&lt;/p&gt;

&lt;p&gt;Let’s imagine a world where we could just shrug this stuff off.  Let’s imagine a new framework – MPING, MPI++, whatever; which is a substantially paired down version of MPI.  It’s an MPI that has decided what it wants to be; a low level layer for toolbuilders, never to be taught to grad students who are planning to write application software.&lt;/p&gt;

&lt;p&gt;It contains only pared-to-the bone versions of MPI3 RMA, which are demonstrably being found useful; MPI collectives, which are fantastic; MPI-IO, which is also fantastic; and auxiliary stuff like the datatype creation routines, etc.  The communications semantics for everything are greatly relaxed, which would confuse the heck out of newbie end users, but toolbuilders can deal with it.  And there’s no decades of backwards compatability to fight with.&lt;/p&gt;

&lt;p&gt;This vision actually discourages me a bit, because it would be terrific; there’d be an active, vendor-supported, high-performance, productive network abstraction layer for toolbuilders; and no confusion about who it was for.  We could build high-productivity tools for scientific application writing atop a stable, high performance foundation.&lt;/p&gt;

&lt;p&gt;And of course, it will never, ever, happen.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>HPC is dying, and MPI is killing it</title>
   <link href="https://hpc.social/2015/hpc-is-dying-and-mpi-is-killing-it/"/>
   <updated>2015-04-03T01:00:00-06:00</updated>
   <id>https://hpc.social/2015/hpc-is-dying-and-mpi-is-killing-it</id>
   <content type="html">&lt;p&gt;&lt;img alt=&quot;King Canute&quot; src=&quot;http://news.bbcimg.co.uk/media/images/53009000/jpg/_53009665_canutewaves.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Pictured: The HPC community bravely holds off the incoming tide of new technologies and applications.  Via &lt;a href=&quot;http://www.bbc.com/news/magazine-13524677&quot;&gt;the BBC&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;This should be a golden age for High Performance Computing.&lt;/p&gt;

&lt;p&gt;For decades, the work of developing algorithms and implementations for tackling  simulation and data analysis problems at the largest possible scales was obscure if important work. Then, suddenly, in the mid-2000s, two problems — analyzing internet-scale data, and interpreting an incoming flood of genomics data — arrived on the scene with data volumes and performance requirements which seemed quite familiar to HPCers, but with a size of audience unlike anything that had come before.&lt;/p&gt;

&lt;p&gt;Suddenly discussions of topics of scalability, accuracy, large-scale data storage, and distributed matrix arithmetic all became mainstream and urgent.  The number of projects and workshops addressing these topics exploded, and new energy went into implementing solutions problems faced in these domains.&lt;/p&gt;

&lt;p&gt;In that environment, one might expect that programmers with HPC experience – who have dealt routinely with terabytes and now petabytes of data, and have years or decades of experience with designing and optimizing distributed memory algorithms – would be in high demand.&lt;/p&gt;

&lt;p&gt;They are not.&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;Job Trends&quot; src=&quot;https://www.dursi.ca/assets/imgs/hpc_jobgraph.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&quot;http://www.indeed.com/jobtrends?q=hadoop%2Cspark%2Chpc%2Cmpi&quot;&gt;Indeed.com job trends data&lt;/a&gt;.  Note that as many MPI jobs plotted above require certifications with “Master Patient Index” or “Meetings Professionals International” as are seeking someone who knows how to call MPI_Send&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;Google Trends&quot; src=&quot;https://www.dursi.ca/assets/imgs/google_trends.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://trends.google.com/trends/explore/TIMESERIES/1564090800?hl=en-US&amp;amp;tz=240&amp;amp;cat=5&amp;amp;date=today+5-y&amp;amp;q=MPI,hadoop,spark&amp;amp;sni=3&quot;&gt;Google trends data for MPI, Hadoop, and Spark&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Instead of relying on those with experience in existing HPC technology stacks or problems, people tackling these internet-scale machine learning problems and genomic data analysis tasks have been creating their own parallel computing stacks.  New and rediscovered old ideas are flourishing in new ecosystems, and demand for scalable and accurate computation with these new tools is exploding — while the HPC community resolutely stays on the sidelines, occasionally cheering itself with hopeful assertions of relevance like &lt;a href=&quot;http://sc14.supercomputing.org&quot;&gt;SC14&lt;/a&gt;’s rather plaintive tagline, &lt;a href=&quot;http://sc14.supercomputing.org&quot;&gt;“HPC Matters”&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Because within the HPC community, the reaction to these new entrants is mostly not excitement at novel technologies and interesting new problems to solve, but scoffing at solutions which were &lt;a href=&quot;http://en.wikipedia.org/wiki/Not_invented_here&quot;&gt;Not Invented Here&lt;/a&gt;, and suggestions that those who use other platforms simply aren’t doing “real” high performance computing – and maybe don’t know what they’re doing at all.  You can see this attitude even in &lt;a href=&quot;http://www.theplatform.net/2015/03/03/dna-sequencing-not-quite-hpc-yet/&quot;&gt;otherwise well-researched and thought-out pieces&lt;/a&gt;, where the suggestion is that it is genomics researchers’ responsibility to alter what they are doing to better fit existing HPC toolsets.  This thinking misses the rather important fact that it is HPC’s job to support researchers’ computing needs, rather than vice versa.&lt;/p&gt;

&lt;p&gt;The idea that the people at Google doing large-scale machine learning problems (which involves huge sparse matrices) are oblivious to scale and numerical performance is just delusional.  The suggestion that the genomics community is a helpless lot who just don’t know any better and need to be guided back to the one true path is no less so.  The reality is simpler; HPC is wedded to a nearly 25-year old technology stack which doesn’t meet the needs of those communities, and if we were being honest with ourselves is meeting fewer and fewer of the needs of even our traditional user base.&lt;/p&gt;

&lt;p&gt;If HPCers don’t start engaging with these other big-computing communities, both exporting our expertise to new platforms and starting to make use of new tools and technologies from within HPC and beyond, we risk serving an ever-narrowing sliver of big research computing.  And  eventually that last niche will vanish once other technologies can serve even their needs better.&lt;/p&gt;

&lt;h2 id=&quot;why-mpi-was-so-successful&quot;&gt;Why MPI was so successful&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Message_Passing_Interface&quot;&gt;MPI&lt;/a&gt;, long the lingua franca of HPC, has nothing to apologize for. It was inarguably one of the “killer apps” which supported the &lt;a href=&quot;http://en.wikipedia.org/wiki/Beowulf_cluster&quot;&gt;initial growth&lt;/a&gt; of cluster computing, helping shape what the computing world has become today.  It supported a substantial majority of all supercomputing work scientists and engineers have relied upon for the past two-plus decades. Heroic work has gone into MPI implementations, and development of algorithms for such MPI features as &lt;a href=&quot;https://www.cac.cornell.edu/VW/MPIcc/default.aspx?id=xup_guest&quot;&gt;collective operations&lt;/a&gt;.  All of this work could be carried over to new platforms by a hypothetical HPC community that actively sought to engage with and help improve these new stacks.&lt;/p&gt;

&lt;p&gt;MPI, the Message Passing Interface, began as a needed standardization above a dizzying array of high-performance network layers and often-proprietary libraries for communicating over these networks.  It started with routines for explicitly sending and receiving messages, very useful collective operations (broadcast, reduce, etc.), and routines for describing layout of data in memory to more efficiently communicate that data.  It eventually added sets of routines for implicit message passing (one-sided communications) and parallel I/O, but remained essentially at the &lt;a href=&quot;http://en.wikipedia.org/wiki/OSI_model#Layer_4:_transport_layer&quot;&gt;transport layer&lt;/a&gt;, with sends and receives and gets and puts operating on strings of data of uniform types.&lt;/p&gt;

&lt;h2 id=&quot;why-mpi-is-the-wrong-tool-for-today&quot;&gt;Why MPI is the wrong tool for today&lt;/h2&gt;

&lt;p&gt;But nothing lasts forever, and at the cutting edge of computing, a quarter-century is coming quite close to an eternity.  Not only has MPI stayed largely the same in those 25 years, the idea that “everyone uses MPI” has made it nearly impossible for even made-in-HPC-land tools like &lt;a href=&quot;http://chapel.cray.com&quot;&gt;Chapel&lt;/a&gt; or &lt;a href=&quot;http://upc.lbl.gov&quot;&gt;UPC&lt;/a&gt; to make any headway, much less quite different systems like &lt;a href=&quot;https://spark.apache.org&quot;&gt;Spark&lt;/a&gt; or &lt;a href=&quot;https://flink.apache.org&quot;&gt;Flink&lt;/a&gt;, meaning that HPC users are largely stuck with using an API which was a big improvement over anything else available 25 years ago, but now clearly shows its age.  Today, MPI’s approach is hardly ever the best choice for anyone.&lt;/p&gt;

&lt;h3 id=&quot;mpi-is-at-the-wrong-level-of-abstraction-for-application-writers&quot;&gt;MPI is at the wrong level of abstraction for application writers&lt;/h3&gt;

&lt;p&gt;Programming at the transport layer, where every exchange of data has to be implemented with lovingly hand-crafted sends and receives or gets and puts, is an incredibly awkward fit for numerical application developers, who want to think in terms of distributed arrays, data frames, trees, or hash tables. Instead, with MPI, the researcher/developer needs to manually decompose these common data structures across processors, and every update of the data structure needs to be recast into a flurry of messages, synchronizations, and data exchange. And heaven forbid the developer thinks of a new, better way of decomposing the data in parallel once the program is already written. Because in that case, since a new decomposition changes which processors have to communicate and what data they have to send, every relevant line of MPI code needs to be completely rewritten.  This does more than simply slow down development; the huge costs of restructuring parallel software puts up a huge barrier to improvement once a code is mostly working.&lt;/p&gt;

&lt;p&gt;How much extra burden does working at this level of abstraction impose?  Let’s take a look at a trivial example that’s pretty much a best-case scenario for MPI, an explicit solver for a 1D &lt;a href=&quot;http://en.wikipedia.org/wiki/Heat_equation&quot;&gt;diffusion equation&lt;/a&gt;.  Regular communications on a regular grid is just the sort of pattern that is most natural for MPI, and so you will find this example in just about &lt;a href=&quot;https://computing.llnl.gov/tutorials/mpi/&quot;&gt;every&lt;/a&gt; &lt;a href=&quot;http://beige.ucs.indiana.edu/I590/node71.html&quot;&gt;MPI&lt;/a&gt; &lt;a href=&quot;https://github.com/ljdursi/mpi-tutorial/blob/master/presentation/presentation.md&quot;&gt;tutorial&lt;/a&gt; &lt;a href=&quot;https://www.hpc.ntnu.no/display/hpc/Diffusion&quot;&gt;out&lt;/a&gt; &lt;a href=&quot;https://www.cs.princeton.edu/picasso/seminarsS04/MPI_Day2.pdf&quot;&gt;there&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;At &lt;a href=&quot;https://www.dursi.ca/feed.xml#appendix&quot;&gt;the end&lt;/a&gt; of this post are sample programs, written as similarly as possible, of solving the problem in MPI, Spark, and Chapel.  I’d encourage you to scroll down and take a look. The lines of code count follows:&lt;/p&gt;

&lt;center&gt;
&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Framework&amp;nbsp;&amp;nbsp;&lt;/th&gt;
&lt;th&gt;Lines&amp;nbsp;&amp;nbsp;&lt;/th&gt;
&lt;th&gt;Lines of Boilerplate&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://www.dursi.ca/feed.xml#mpi&quot;&gt;MPI+Python&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;52&lt;/td&gt;
&lt;td&gt;20+&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://www.dursi.ca/feed.xml#spark&quot;&gt;Spark+Python&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;28&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://www.dursi.ca/feed.xml#chapel&quot;&gt;Chapel&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/center&gt;

&lt;p&gt;Now, this isn’t an entirely fair comparison. It should be mentioned that in addition to the functionality of the MPI program, the Spark version is automatically fault-tolerant, and the Chapel version has features like automatically reading parameters from the command line.   In addition, changing the data layout across processors in the Chapel version would only involve changing the variable declaration for the global arrays, and maybe writing some code to implement the decomposition in the unlikely event that your distributed array layout wasn’t already supported; similarly, in Spark, it would mean just changing the hash function used to assign partitions to items.&lt;/p&gt;

&lt;p&gt;But even lacking those important additional functionalities, the MPI version is over twice as long as the others, with an amount of boilerplate that is itself the entire length of the Chapel program.  The reason is quite simple.  In Chapel, the basic abstraction is of a domain – a dense array, sparse array, graph, or what have you – that is distributed across processors.  In Spark, it is a &lt;a href=&quot;http://spark.apache.org/docs/1.2.1/quick-start.html&quot;&gt;resiliant distributed dataset&lt;/a&gt;, a table distributed in one dimension.  Either of those can map quite nicely onto various sorts of numerical applications. In MPI, the “abstraction“ is of a message.  And thus the huge overhead in lines of code.&lt;/p&gt;

&lt;p&gt;And this is by far the simplest case; introducing asynchronous communications, or multiple variables with differing layouts, or allowing processors to get out of sync, or requiring load balancing, causes levels of complexity explode. Even just moving to 2D, the amount of MPI boilerplate almost exactly doubles, whereas the only lines that change in the Chapel program is the array declaration and the line that actually executes the stencil computation.&lt;/p&gt;

&lt;p&gt;On the one hand, this increase in complexity is perfectly reasonable; those are more challenging cases of networked computation. But on the other hand, of all available models, MPI is the only one where the researcher is required to reinvent from scratch the solutions to these problems inside the heart of their own application software.  This requires them to focus on network programming instead of (say) differential-equation solving; and to completely re-architect the entire thing if their application needs change.&lt;/p&gt;

&lt;p&gt;Now, none of this is necessarily a problem.  Just because MPI is hugely and unnecessarily burdensome for individual scientists to use directly for complex applications, doesn’t mean that it’s bad, any more than (say) sockets or IB verbs programming is; it could be a useful network-hardware agnostic platform for higher-level tools to be built upon.  Except…&lt;/p&gt;

&lt;h3 id=&quot;mpi-is-at-the-wrong-level-of-abstraction-for-tool-builders&quot;&gt;MPI is at the wrong level of abstraction for tool builders&lt;/h3&gt;

&lt;p&gt;The original book on MPI, &lt;a href=&quot;http://www.mcs.anl.gov/research/projects/mpi/usingmpi/&quot;&gt;Using MPI&lt;/a&gt;, dedicated one of its ten chapters (“Parallel libraries”) to  explicitly describing features intended to make it easier for tool builders to build libraries and tools based on MPI, and two others describing implementations and comparing to other models with relevance to tool-builders.&lt;/p&gt;

&lt;p&gt;This was quite prescient; message-passing based frameworks would indeed soon become very important platforms for building complex parallel and distributed software in different communities.  &lt;a href=&quot;http://www.erlang.org&quot;&gt;Erlang&lt;/a&gt;, released to the public just five years later, is a functional language with message-passing built in that has played a very large role in many communications and control environments.  Rather more recently, &lt;a href=&quot;http://akka.io&quot;&gt;Akka&lt;/a&gt; is a Scala-based message passing framework that, for instance, Spark is built on.&lt;/p&gt;

&lt;p&gt;However, all these years later, while there are several specific numerical libraries built on MPI that MPI programs can use, there are no major general-purpose parallel programming frameworks that primarily use MPI as an underlying layer.  Both &lt;a href=&quot;http://gasnet.lbl.gov&quot;&gt;GASNet&lt;/a&gt; (that UPC and Chapel implementations make use of) and &lt;a href=&quot;http://charm.cs.illinois.edu/research/charm&quot;&gt;Charm++&lt;/a&gt; (a parallel computing framework often used for particle simulation methods, amongst other things) &lt;em&gt;have&lt;/em&gt; MPI back ends, grudgingly, but they are specifically not recommended for use unless nothing else works; indeed, they have both chosen to re-architect the network-agnostic layer, at significant effort, themselves.   (Of the two, GASNet is the more diplomatic about this, &lt;a href=&quot;http://gasnet.lbl.gov/dist/README&quot;&gt;“…bypassing the MPI layer in order to provide the best possible performance”&lt;/a&gt;, whereas the Charm++ group finds MPI problematic enough that, if you must use MPI for “legacy” applications, they recommend using &lt;a href=&quot;http://charm.cs.uiuc.edu/research/ampi/&quot;&gt;an MPI-like layer built ontop of Charm++&lt;/a&gt;, rather than building Charm++ on top of MPI).  Similarly, the group implementing Global Arrays – an example come back to time and again in the MPI books – eventually implemented its own low level library, &lt;a href=&quot;http://hpc.pnl.gov/armci/&quot;&gt;ARMCI&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Probably the closest to a truly MPI-based parallel scientific programming framework is &lt;a href=&quot;http://trilinos.org&quot;&gt;Trilinos&lt;/a&gt;, which is a well-integrated set of libraries for meshing and numerics rather than a parallel programming model.&lt;/p&gt;

&lt;p&gt;The reason for this disconnect is fairly straightforward.  MPI was aimed at two sets of users – the researchers writing applications, and the toolmakers building higher-level tools.  But compromises that were made to the semantics of MPI to make it easier to use and reason about for the scientists, such as the &lt;a href=&quot;http://www.mpi-forum.org/docs/mpi-1.1/mpi-11-html/node41.html&quot;&gt;in-order guarantee&lt;/a&gt; and reliability of messages, made it very difficult to write efficient higher-level tools on top of.&lt;/p&gt;

&lt;p&gt;A particularly strong case study of this dynamic is MPI-2’s one-sided communications, which were aimed squarely at tool developers (certainly a very small fraction of applications written directly in MPI ever used these features).  This set of routines had extremely strict semantics, and as a result, they were &lt;a href=&quot;http://www.cs.berkeley.edu/~bonachea/upc/mpi2.html&quot;&gt;soundly&lt;/a&gt; &lt;a href=&quot;http://dl.acm.org/citation.cfm?id=1359705&quot;&gt;panned&lt;/a&gt; as being unfit for purpose, and more or less studiously ignored.  MPI-3’s &lt;a href=&quot;https://www.cac.cornell.edu/VW/MPIoneSided/default.aspx?id=xup_guest&quot;&gt;new one-sided communications&lt;/a&gt; routines, introduced 14 years later, &lt;a href=&quot;http://blogs.cisco.com/performance/the-new-mpi-3-remote-memory-access-one-sided-interface&quot;&gt;largely fixes this&lt;/a&gt;; but by this point, with GASNet and ARMCI amongst others available and supporting multiple transports, and coming complete with attractive optional higher-level programming models, there’s little compelling reason to use MPI for this functionality.&lt;/p&gt;

&lt;h3 id=&quot;mpi-is-more-than-you-need-for-modest-levels-of-parallelism&quot;&gt;MPI is more than you need for modest levels of parallelism&lt;/h3&gt;

&lt;p&gt;At HPC centres around the world, the large majority of HPC use is composed of jobs requiring 128 cores or fewer.  At that point, most of the parallelism heavy lifting is best done by threading libraries.  For the very modest level of inter-node IPC needed for these 2-4 node jobs, the bare-metal performance of MPI simply isn’t worth the bare-metal complexity.  At that level of parallelism, for most applications almost any sensible framework, whether GASNet-based, or Charm++, or Spark, or down to Python multiprocessing or iPython cluster will give decent performance.&lt;/p&gt;

&lt;h3 id=&quot;mpi-is-less-than-you-need-at-extreme-levels-of-parallelism&quot;&gt;MPI is less than you need at extreme levels of parallelism&lt;/h3&gt;

&lt;p&gt;On the other hand, at the emerging extreme high end of supercomputing – the million-core level and up – the bare-metal aspect of MPI causes different sorts of problems.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;http://en.wikipedia.org/wiki/Mean_time_between_failures&quot;&gt;MTBF&lt;/a&gt; of modern motherboards is on the order of a few hundred thousand hours.  If you’re running on a million cores (say 32,000 nodes or so) for a 24-hour day, failure of some node or another during the run becomes all but certain.  At that point, fault-tolerance, and an ability to re-balance the computation on the altered set of resources, becomes essential.&lt;/p&gt;

&lt;p&gt;Today, MPI’s error handling model is what it has always been; you can assign an &lt;a href=&quot;http://www.mpich.org/static/docs/v3.1/www3/MPI_Errhandler_set.html&quot;&gt;errorhandler&lt;/a&gt; to be called when an error occurs in an MPI program, and when that happens you can… well, you can print a nice message before you crash, instead of crashing &lt;em&gt;without&lt;/em&gt; the nice message.&lt;/p&gt;

&lt;p&gt;This isn’t due to anyone’s lack of trying; the &lt;a href=&quot;https://svn.mpi-forum.org/trac/mpi-forum-web/wiki/FaultToleranceWikiPage&quot;&gt;MPI Fault Tolerance Working Group&lt;/a&gt; has been doing yeomanlike work attempting to bring some level of real fault tolerance to MPI.  But it’s a truly  difficult problem, due in large part to the very strict semantics imposed by MPI.  And after building up 25 years of legacy codes that use MPI, there is absolutely no chance that the pull of the future will exceed the drag of the past in the minds of the MPI Forum - none of those semantics will ever change, for backwards compability reasons.&lt;/p&gt;

&lt;p&gt;Balancing and adapting to changing resources are similarly weak spots for the MPI approach; there’s no way that MPI can possibly be of any use in redistributing your computation for you, any more than you could expect TCP or Infiniband Verbs to automatically do that for you.  If the highest-level abstraction a library supports is the message, there is no way that the library can know anything about what your data structures are or how they must be migrated.&lt;/p&gt;

&lt;p&gt;Fault-tolerance and adaptation are of course genuinely challenging problems; but (for instance) Charm++ (and AMPI atop it) can do adaptation, and Spark can do fault tolerance.  But that’s because they were architected differently.&lt;/p&gt;

&lt;h2 id=&quot;our-users-deserve-the-tools-best-for-them&quot;&gt;Our users deserve the tools best for them&lt;/h2&gt;

&lt;p&gt;None of this is to say that MPI is bad.  But after 25 years of successes, it’s become clear what the limitations are of having the communications layer written within the researchers’ application.  And today those limitations are holding us and our users back, especially compared to what can be done with other alternatives that are already out there on the market.&lt;/p&gt;

&lt;p&gt;And none of this is to say that we should uninstall MPI libraries from our clusters.  For the near term, MPI will remain the best choice for codes that have to run on tens of thousands of cores and have relatively simple communications patterns.&lt;/p&gt;

&lt;p&gt;But it’s always been true that different sorts of computational problems have required different sorts of parallel tools, and it’s time to start agressively exploring those that are already out there, and building on what we already have.&lt;/p&gt;

&lt;p&gt;We have to start using these new tools when they make sense for our users; which is, demonstrably, quite often. It’s already gotten to the point where it’s irresponsible to teach grad students MPI without also exposing them to tools that other groups find useful.&lt;/p&gt;

&lt;p&gt;The HPC community can, and should, be much more than just consumers of these external technologies.  Our assertions of relevance don’t have to be purely aspirational.  We have real expertise that can be brought to bear on these new problems and technologies.  Excellent work has been done in MPI implementations on important problems like the network-agnostic layer, job launching, and collective algorithms.  The people who wrote those network-agnostic layers are already looking into refactoring them into &lt;a href=&quot;https://github.com/ofiwg/libfabric&quot;&gt;new&lt;/a&gt; &lt;a href=&quot;https://www.olcf.ornl.gov/center-projects/common-communication-interface/&quot;&gt;projects&lt;/a&gt; that can be widely used in a variety of contexts, at lower levels of the stack.&lt;/p&gt;

&lt;p&gt;But we need to give up the idea that there is a one-sized fits all approach to large-scale technical computing, and that it has always been and will always be MPI.  Other groups are using different approaches for a reason; we can borrow from them to the benefit of our users, and contribute to those approaches to make them better.&lt;/p&gt;

&lt;h2 id=&quot;we-can-build-the-future&quot;&gt;We can build the future&lt;/h2&gt;

&lt;p&gt;There’s new ways of writing scalable code out there, and completely new classes of problems to tackle, many of which were totally inaccessible just years ago.  Isn’t that why we got into this line of work?  Why don’t more HPC centres have people contributing code to the &lt;a href=&quot;https://github.com/chapel-lang/chapel&quot;&gt;Chapel project&lt;/a&gt;, and why isn’t everyone at least playing with Spark, which is &lt;a href=&quot;http://www.dursi.ca/post/spark-in-hpc-clusters/&quot;&gt;trivial to get up and running on an HPC cluster&lt;/a&gt;?  Why are we spending time scoffing at things, when we can instead be making big research computing better, faster, and bigger?&lt;/p&gt;

&lt;p&gt;Are we the big research computing community, or the MPI community?  Because &lt;em&gt;one&lt;/em&gt; of those two has a bright and growing future.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Many thanks to my colleague Mike Nolta for many suggestions and improvements to this piece and the arguments it contains.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;appendix&quot;&gt;Appendix&lt;/h2&gt;

&lt;p&gt;(&lt;strong&gt;&lt;em&gt;Update&lt;/em&gt;&lt;/strong&gt;: see objections that came up after the publication of this post, on twitter and email, &lt;a href=&quot;http://dursi.ca/post/objections-continued/&quot;&gt;on this new post&lt;/a&gt;.  And see what I like about MPI and why it suggests low-level applications programming isn’t the answer &lt;a href=&quot;http://www.dursi.ca/post/in-praise-of-mpi-collectives-and-mpi-io/&quot;&gt;on the third post&lt;/a&gt;.)&lt;/p&gt;

&lt;h3 id=&quot;objections&quot;&gt;Objections&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;But the HPC market is &lt;a href=&quot;http://www.slideshare.net/insideHPC/hpc-market-update-from-idc&quot;&gt;actually growing&lt;/a&gt;, so this is all clearly nonsense!  Everything’s fine!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;It’s completely true that, although much more slowly in relative or absolute terms than the Hadoop or Spark market, the HPC hardware market is still growing.  But that’s not much of a reed to cling to.&lt;/p&gt;

&lt;p&gt;Famously, &lt;a href=&quot;http://www.tumotech.com/wp-content/uploads/2014/11/mainframe-computer-sales.png&quot;&gt;minicomputer sales&lt;/a&gt; (things like System/36 or VAXen) were still growing rapidly a decade or so after personal computers started to be available, well into the mid-80s. They kept being sold, and faster and faster, because they were much better for the problems they were intended for — right up until the point that they weren’t.&lt;/p&gt;

&lt;p&gt;Similarly, photo film sales were &lt;a href=&quot;http://www.businessweek.com/1999/99_31/b3640098.htm&quot;&gt;going up, if slower, until 2003&lt;/a&gt;(!).  Let’s continue the &lt;a href=&quot;http://en.wikipedia.org/wiki/Disruptive_innovation&quot;&gt;disruptive innovation&lt;/a&gt; clichés as analogies for a moment — as we all now know, Kodak invented the digital camera.  The film company’s problem wasn’t that it lacked the expertise that was needed in the new era; it simply flatly refused to use its expertise in these new ways.  And as a result it is a shell of its former self today – a tiny, niche, player.  Bringing the comparison closer to home is  the experience of the once world-striding Blackberry, which ridiculed the iPhone as being, amongst other things, an inefficient user of network communications. (&lt;a href=&quot;http://www.theglobeandmail.com/report-on-business/the-inside-story-of-why-blackberry-is-failing/article14563602/?page=all&quot;&gt;“It’s going to collapse the network!”&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;Take a look at the market for developers.  We’ve clearly passed the market peak for MPI programmers, and if HPC continues to be an MPI-only shop, our community will be shut out of the exciting things that are going on today, while many of our users begin being attracted by the benefits of these other approaches for their problems.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;But MPI is much faster than the others because it’s bare metal!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If this is so important, why don’t HPC programmers save even &lt;em&gt;more&lt;/em&gt; overhead by packing raw Infiniband frames themselves?&lt;/p&gt;

&lt;p&gt;HPC programmers should know better than most that once you have some software that solves a complex problem well, getting it to go fast is comparatively straightforward, given enough developer hours.&lt;/p&gt;

&lt;p&gt;It’s absolutely true that current MPI implementations, having had decades to work on it, have got screamingly fast MPI-1 functionality and, to a lesser extent, decent one-sided communications performance. But we live in an era where even &lt;a href=&quot;http://julialang.org/benchmarks/&quot;&gt;JavaScript can have the same order-of-magnitude performance as C or Fortran&lt;/a&gt; - and JavaScript might as well have been explicitly designed to be un-en-fastable.&lt;/p&gt;

&lt;p&gt;Chapel already can be &lt;a href=&quot;http://chapel.cray.com/hpcc/hpcc09.pdf&quot;&gt;as fast or faster than MPI in many common cases&lt;/a&gt;; indeed, higher level abstractions allow compilers and runtimes to make optimizations that can’t be performed one individual library calls.&lt;/p&gt;

&lt;p&gt;And unless the basic abstractions used by Spark (&lt;a href=&quot;http://www.thecloudavenue.com/2014/01/resilient-distributed-datasets-rdd.html&quot;&gt;RDDs&lt;/a&gt;) or Flink or the myriad of other options are inherently broken in some way to make fast implementations impossible — and there’s no evidence that they are — they too will get faster.  There’s no reason why blazing-fast network communications should have be done at the application layer – in the code that is describing the actual scientific computation. The HPC community can choose to help with implementing that tuning, bringing their expertise and experience to bear.  Or they can choose not to, in which case it will happen anyway, without them.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;But MPI will adopt new feature X which will change everything!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Let me tell you a story.&lt;/p&gt;

&lt;p&gt;MPI-1 and MPI-2 used 32-bit integers for all counts. This means that programs using MPI – the lingua franca of supercomputing, in an era when already outputing terabytes of data being routine – could not (for instance) write out more than 2e9 objects at once without taking some meaningless additional steps.&lt;/p&gt;

&lt;p&gt;This was discussed at length in the process leading up to the 2012 release of MPI-3, the first .0 release in 14 years.  After much discussion it was decided that changing things would be a &lt;a href=&quot;http://blogs.cisco.com/performance/can-i-mpi_send-and-mpi_recv-with-a-count-larger-than-2-billion&quot;&gt;“backwards compatability nightmare”&lt;/a&gt;, so the result was that the existing API… was left exactly as it is.  But!  There was a new larger data type, MPI_Count, which is used in a couple new routines (like &lt;code&gt;MPI_Type_get_extent_X&lt;/code&gt;, in addition to the old &lt;code&gt;MPI_Type_get_extent&lt;/code&gt;) which simplifies some of the pointless steps you have to take.  Yay?&lt;/p&gt;

&lt;p&gt;And that’s the story of how, in 2015, our self-imposed standard of supercomputing has a hardcoded in 32-bit limit throughout almost its entire API, limiting how many objects it can deal with at once without going through pointless but straightforward hoops.  A 32-bit limit: 90’s retro-cool computing, like chiptune music and pixelated graphics with 8-bit color.  This is unfortunate, but inevitable; after a tool has existed for 25 years, maintainers feel more responsibility towards the past than to the future.  Which is perfectly reasonable, and maybe even the correct decision for that tool; but that’s when one need to start looking elsewhere for new projects.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;But these other tools use programming languages I find to be icky.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Yes, well, perhaps the various alternatives involve languages that lack the austere beauty of Fortran and Matlab, but so it goes.  One approach to this would be to help expand these tools reach into the HPC community by writing bindings and APIs for languages more familiar in this space.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;But the Hadoop-y communities are incredibly naive about high performance interconnects, multicore/shared memory, complex scheduling,…&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Yes! This is 100% true.  And on the HPC community’s side, we’re quite innocent when it comes to fault tolerance at scale, building reusable tools, architecting APIs so that normal scientists can use them while hiding communications complexity beneath, and integrating nicely with systems industry cares about.  There’s a window where we can help each other and contribute meaningfully to each groups success.  But other communities can and will eventually figure out, say, multicore with or without our help.&lt;/p&gt;

&lt;h2 id=&quot;sample-code&quot;&gt;Sample Code&lt;/h2&gt;

&lt;p&gt;Below are code samples referred to earlier in the piece.&lt;/p&gt;

&lt;h3 id=&quot;mpi&quot;&gt;MPI&lt;/h3&gt;

&lt;p&gt;Here is the 1D diffusion in MPI, Python:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;#!/usr/bin/env python
import numpy
from mpi4py import MPI

def ranksProcs():                            # boilerplate
    comm = MPI.COMM_WORLD
    rank = comm.Get_rank()
    nprocs = comm.Get_size()
    leftProc  = rank-1 if rank &amp;gt; 0 else MPI.PROC_NULL
    rightProc = rank+1 if rank &amp;lt; nprocs-1 else MPI.PROC_NULL
    return (comm, rank, nprocs, leftProc, rightProc)

def localnitems(procnum, nprocs, nitems):   # boilerplate
    return (nitems + procnum)/nprocs

def myRange(procnum, nprocs, ncells):       # boilerplate
    start = 0
    for p in xrange(procnum):
        start = start + localnitems(p, nprocs, ncells)
    locNcells = localnitems(procnum, nprocs, ncells)
    end = start + locNcells - 1
    return (start, locNcells, end)

def ICs(procnum, nprocs, ncells, leftX, rightX, ao, sigma):
    start, locNcells, end = myRange(procnum, nprocs, ncells)
    dx = (rightX-leftX)/(ncells-1)
    startX = leftX + start*dx
    x = numpy.arange(locNcells*1.0)*dx + startX
    temperature = ao*numpy.exp(-(x*x)/(2.*sigma*sigma))

    return temperature

def guardcellFill(data, comm, leftProc, rightProc, leftGC, rightGC):  # boilerplate
    rightData = numpy.array([-1.])           
    leftData = numpy.array([-1.])

    comm.Sendrecv(data[1],  leftProc, 1,  rightData, rightProc, 1)
    comm.Sendrecv(data[-2], rightProc, 2, leftData,  leftProc, 2)

    data[0]  = leftGC if leftProc == MPI.PROC_NULL else leftData
    data[-1] = rightGC if rightProc == MPI.PROC_NULL else rightData
    return data

def timestep(olddata, coeff):
    newdata = numpy.zeros_like(olddata)
    newdata[1:-1] = olddata[1:-1] +
                     coeff*(olddata[0:-2] - 2.*olddata[1:-1] + olddata[2:])
    return newdata

def simulation(ncells, nsteps, leftX=-10., rightX=+10., sigma=3., ao=1.,
               coeff=.375):
    comm, procnum, nprocs, leftProc, rightProc = ranksProcs()
    T = ICs(procnum, nprocs, ncells, leftX, rightX, ao, sigma)
    leftGC = T[0]   # fixed BCs
    rightGC = T[-1]
    print &quot;IC: &quot;, procnum, T
    for step in xrange(nsteps):
        T = timestep(T, coeff)
        guardcellFill(procnum, nprocs, T, comm, leftProc, rightProc,
                      leftGC, rightGC)    # boilerplate

    print &quot;Final: &quot;, procnum, T

if __name__ == &quot;__main__&quot;:
    simulation(100, 20)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;spark&quot;&gt;Spark&lt;/h3&gt;

&lt;p&gt;1D diffusion in Spark, python (is fault-tolerant)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import numpy
from pyspark import SparkContext

def simulation(sc, ncells, nsteps, nprocs, leftX=-10., rightX=+10.,
               sigma=3., ao=1., coeff=.375):
    dx = (rightX-leftX)/(ncells-1)

    def tempFromIdx(i):
        x = leftX + dx*i + dx/2
        return (i, ao*numpy.exp(-x*x/(2.*sigma*sigma)))

    def interior(ix):                        # boilerplate
        return (ix[0] &amp;gt; 0) and (ix[0] &amp;lt; ncells-1)

    def stencil(item):
        i,t = item
        vals = [ (i,t) ]
        cvals = [ (i, -2*coeff*t), (i-1, coeff*t), (i+1, coeff*t) ]
        return vals + filter(interior, cvals)

    temp = map(tempFromIdx,range(ncells))
    data= sc.parallelize(temp).partitionBy(nprocs, rangePartitioner)
    print &quot;IC: &quot;
    print data.collect()
    for step in xrange(nsteps):
        print step
        stencilParts = data.flatMap(stencil)
        data = stencilParts.reduceByKey(lambda x,y:x+y)
    print &quot;Final: &quot;
    print data.collect()

if __name__ == &quot;__main__&quot;:
    sc = SparkContext(appName=&quot;SparkDiffusion&quot;)
    simulation(sc, 100, 20, 4)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;chapel&quot;&gt;Chapel&lt;/h3&gt;

&lt;p&gt;1D diffusion in Chapel (can read parameters from command line)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-c&quot;&gt;use blockDist;

config var ncells = 100, nsteps = 20,  leftX = -10.0, rightX = +10.0,
           sigma = 3.0, ao = 1.0, coeff = .375;

proc main() {
  const pDomain  = {1..ncells} dmapped Block({1..ncells});
  const interior = pDomain.expand(-1);
  const dx = (rightX - leftX)/(ncells-1);
  var x, temp, tempNew : [pDomain] real = 0.0;

  forall i in pDomain do {
    x[i] = leftX + (i-1)*dx;
    temp[i] = ao*exp(-x[i]*x[i]/(2.0*sigma*sigma));
  }

  writeln(&quot;ICs: &quot;, temp, &quot;\n&quot;);

  for step in [1..nsteps] do {
    forall i in interior do
      tempNew(i) = temp(i) + coeff*(temp(i-1) - 2.0*temp(i) + temp(i+1));
    temp[interior] = tempNew[interior];
  }

  writeln(&quot;Final: &quot;, temp);
}
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>Spark in HPC clusters</title>
   <link href="https://hpc.social/2015/spark-in-hpc-clusters/"/>
   <updated>2015-03-02T00:00:00-07:00</updated>
   <id>https://hpc.social/2015/spark-in-hpc-clusters</id>
   <content type="html">&lt;p&gt;Over the past several years, as research computing centres and others who run HPC clusters tried to accommodate other forms of computing for data analysis, &lt;a href=&quot;http://www.sdsc.edu/~allans/MyHadoop.pdf&quot;&gt;much&lt;/a&gt; &lt;a href=&quot;http://www.hadoopsphere.com/2013/06/options-for-mapreduce-with-hpc.html&quot;&gt;effort&lt;/a&gt; went into trying to incorporate Hadoop jobs into the scheduler along with other more traditional HPC jobs.  It never went especially well, which is a shame, because it seems that those past unsuccessful attempts have &lt;a href=&quot;http://www.hadoopsphere.com/2013/06/options-for-mapreduce-with-hpc.html&quot;&gt;discouraged&lt;/a&gt; experimentation with related next-generation technologies which are a much better fit for large-scale technical computing.&lt;/p&gt;

&lt;p&gt;Hadoop v1 was always going to be a niche player and an awkward fit for big technical computing - and HPCers weren’t the only ones to notice this.  Hadoop MapReduce’s mandatory dumping of output to disk after every Map/Reduce stage rendered it nearly unusable for any sort of approach which required iteration, or interactive use. Machine learning users, who often rely on many of the same iterative linear algebra solvers that physical science simulation users need, equally found Hadoop unhelpful.  Hadoop v1 solved one set of problems – large single-pass data processing – very well, but those weren’t the problems that the technical computing community needed solved.&lt;/p&gt;

&lt;p&gt;The inefficiency of flushing to disk wasn’t necessarily the difficulty that HPC centres had with incorporating Hadoop into their clusters, however.  Dumping to disk could be sped up with caching, or SSDs.  The real issue was with &lt;a href=&quot;http://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html&quot;&gt;HDFS&lt;/a&gt;, the filesystem which Hadoop relies on.  Because every job needed very rapid access to its data – to read the entire set in to the compute nodes, do minimal processing, then flush it back out – the file system was intimately tied to Hadoop cluster scheduling, which worked very hard (reasonably enough) to schedule the compute next to the data.  But with Hadoop “on demand” in a cluster, how is this to work?  One could spin up a new HDFS within each Hadoop job – but now the user has to have the new empty HDFS ingest the data files (probably with replication) initially, and then stage the data out of the doomed-to-be-shut-down HDFS afterwards.  But this staging in and out will certainly take substantially longer than even the rest of the job’s I/O, which already likely dominates runtime.  One can reserve a number of nodes for Hadoop jobs and keep a persistent HDFS store there, but this now defeats the purpose of running Hadoop in the cluster; one might as well just hive off those nodes into a separate system.  Probably the best approach, which worked better than I think anyone had any right to expect, was to run &lt;a href=&quot;http://wiki.lustre.org/index.php/Running_Hadoop_with_Lustre&quot;&gt;Hadoop on Lustre&lt;/a&gt;, but it remained awkward even for those who already were using Lustre for their cluster.&lt;/p&gt;

&lt;p&gt;The HPC community’s reaction to those problems – problems with a technology they were already skeptical of due to &lt;a href=&quot;http://en.wikipedia.org/wiki/Not_invented_here&quot;&gt;Not Invented Here Syndrome&lt;/a&gt; –  was largely to give up on anything that seemed “Hadoopy” as a sensible approach.  The large-scale machine learning community, which didn’t necessarily have that luxury, was instead already looking for in-memory approaches to avoid this problem entirely.&lt;/p&gt;

&lt;p&gt;Two very promising “post-Hadoop” in-memory approaches which are much better suited to large-scale technical computing than Hadoop v1 ever was are also Apache projects - &lt;a href=&quot;https://spark.apache.org&quot;&gt;Spark&lt;/a&gt; and &lt;a href=&quot;https://flink.apache.org&quot;&gt;Flink&lt;/a&gt;.  Flink has some really interesting features - including using a database-like query optimizer for almost all computations - but there’s no real question that currently, Spark is the more mature and capable of the offerings.&lt;/p&gt;

&lt;p&gt;Spark can make use of HDFS, and other related file stores, but those aren’t requirements; since iterative computation can be done in memory given enough RAM, there is much less urgency in having the data local to the computation if the computation is long enough.  Instead, Spark can simply use a POSIX interface to whatever filesystem is already running on your cluster.&lt;/p&gt;

&lt;p&gt;Spark not only lacks hard HDFS-style requirements, but can also run in &lt;a href=&quot;http://spark.apache.org/docs/latest/spark-standalone.html&quot;&gt;standalone mode&lt;/a&gt; without a heavyweight scheduler like &lt;a href=&quot;http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html&quot;&gt;Yarn&lt;/a&gt; or &lt;a href=&quot;http://mesos.apache.org/&quot;&gt;Mesos&lt;/a&gt;.   This standalone mode makes it quite easy to simply spin up a Spark “cluster” within a job, reading from the file system as any other job would.  (Earlier versions of Spark made this unnecessarily difficult, with the standalone startup scripts having hardcoded values that assumed only one such job at a time; this is somewhat easier now.)&lt;/p&gt;

&lt;p&gt;Thus, below is a little job submission script for a Spark job on &lt;a href=&quot;http://www.scinethpc.ca&quot;&gt;SciNet&lt;/a&gt;; it starts up a Spark master on the head node of the job, sets the workers, and runs a simple wordcount example.&lt;/p&gt;

&lt;p&gt;Spark’s well-thought-out python interface, standalone mode, and filesystem-agnostic approach, makes Spark a much better match for traditional HPC systems than Hadoop technologies ever were.&lt;/p&gt;

&lt;p&gt;Spark is covered a little bit in my and Mike Nolta’s &lt;a href=&quot;http://www.dursi.ca/hadoop-for-hpcers/&quot;&gt;Hadoop-for-HPCers&lt;/a&gt; workshop.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
#
#PBS -l nodes=3:ppn=8,walltime=0:20:00
#PBS -N spark-test

nodes=($( cat $PBS_NODEFILE | sort | uniq ))
nnodes=${#nodes[@]}
last=$(( $nnodes - 1 ))

cd $PBS_O_WORKDIR

export SPARK_HOME=/scinet/gpc/Libraries/spark/spark-1.0.2-bin-hadoop2/
ssh ${nodes[0]} &quot;module load java; cd ${SPARK_HOME}; ./sbin/start-master.sh&quot;
sparkmaster=&quot;spark://${nodes[0]}:7077&quot;

for i in $( seq 0 $last )
do
    ssh ${nodes[$i]} &quot;cd ${SPARK_HOME}; module load java; nohup ./bin/spark-class org.apache.spark.deploy.worker.Worker ${sparkmaster} &amp;amp;&amp;gt; ${SCRATCH}/work/nohup-${nodes[$i]}.out&quot; &amp;amp;
done

rm -rf ${SCRATCH}/wordcounts

cat &amp;gt; sparkscript.py &amp;lt;&amp;lt;EOF
from pyspark import SparkContext

sc = SparkContext(appName=&quot;wordCount&quot;)
file = sc.textFile(&quot;${SCRATCH}/moby-dick.txt&quot;)
counts = file.flatMap(lambda line: line.split(&quot; &quot;)).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b)
counts.saveAsTextFile(&quot;${SCRATCH}/wordcounts&quot;)
EOF

module load java
${SPARK_HOME}/bin/spark-submit --master ${sparkmaster} sparkscript.py

ssh ${nnodes[0]} &quot;module load java; cd ${SPARK_HOME}; ./sbin/stop-master&quot;
for i in $( seq 0 $last )
do
    ssh ${nodes[$i]} &quot;killall java&quot;
done
wait

&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>Thoughts on the NSF Future Directions Interim Report</title>
   <link href="https://hpc.social/2015/thoughts-on-the-nsf-future-directions-interim-report/"/>
   <updated>2015-01-29T07:53:00-07:00</updated>
   <id>https://hpc.social/2015/thoughts-on-the-nsf-future-directions-interim-report</id>
   <content type="html">&lt;p&gt;The National Academies recently released an interim report entitled &lt;a href=&quot;http://www.nap.edu/catalog/18972/future-directions-for-nsf-advanced-computing-infrastructure-to-support-us-science-and-engineering-in-2017-2020&quot;&gt;Future Directions for NSF Advanced Computing Infrastructure to Support U.S. Science and Engineering in 2017-2020&lt;/a&gt; as a part of &lt;a href=&quot;http://www.nsf.gov/awardsearch/showAward?AWD_ID=1344417&amp;amp;HistoricalAwards=false&quot;&gt;a $723,000 award&lt;/a&gt; commissioned to take a hard look at where the NSF’s supercomputing program is going.  Since releasing the interim report, the committee has been soliciting feedback and input from the research community to consider as they draft their final report, and I felt compelled to put some of my thoughts into a response.&lt;br /&gt;&lt;br /&gt;NSF’s HPC programs are something I hold near and dear since I got my start in the industry by supporting two NSF-owned supercomputers.  I put a huge amount of myself into Trestles and Gordon, and I still maintain that job encompassed the most engaging and rewarding work I’ve ever done.  However, the NSF’s lack of a future roadmap for its HPC program made my future feel perpetually uncertain, and this factored heavily in my decision to eventually pursue other opportunities.&lt;br /&gt;&lt;br /&gt;Now that I am no longer affiliated with NSF, I wanted to delineate some of the problems I observed during my time on the inside with the hope that someone more important than me really thinks about how they can be addressed.  The report requested feedback in nine principal areas, so I’ve done my best to contextualize my thoughts with the committee’s findings. &lt;br /&gt;&lt;br /&gt;With that being said, I wrote this all up pretty hastily.  Some of it may be worded strongly, and although I don’t mean to offend anybody, I stand by what I say.  That doesn’t mean that my understanding of everything is correct though, so it’s probably best to assume that I have no idea what I’m talking about here.&lt;br /&gt;&lt;br /&gt;Finally, a glossary of terms may make this more understandable:&lt;br /&gt;&lt;br /&gt;&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;XD is the NSF program that funds XSEDE; it finances infrastructure and people, but it does not fund supercomputer procurements or operations&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Track 1 is the program that funded Blue Waters, the NSF’s leadership-class HPC resource&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Track 2 is the program that funds most of the XSEDE supercomputers.  It funded systems like Ranger, Keeneland, Gordon, and Stampede&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;br /&gt;&amp;lt;hr /&amp;gt;&lt;br /&gt;&amp;lt;h2 style=&quot;text-align: left;&quot;&amp;gt;1. How to create advanced computing infrastructure that enables integrated discovery involving experiments, observations, analysis, theory, and simulation.&amp;lt;/h2&amp;gt;Answering this question involves a few key points:&lt;br /&gt;&amp;lt;ol&amp;gt;&amp;lt;li&amp;gt;Stop treating NSF’s cyberinfrastructure as a computer science research project and start treating it like research infrastructure operation.  Office of Cyberinfrastructure (OCI) does not belong in Computer &amp;amp; Information Science &amp;amp; Engineering (CISE).&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Stop funding cyberinfrastructure solely through capital acquisition solicitations and restore reliable core funding to NSF HPC centers.  This will restore a community that is conducive to retaining expert staff.&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Focus OCI/ACI and raise the bar for accountability and transparency.   Stop funding projects and centers that have no proven understanding of operational (rather than theoretical) HPC.&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Either put up or give up.  The present trends in funding lie on a road to death by attrition.  &amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Don’t waste time and funding by presuming that outsourcing responsibility and resources to commercial cloud or other federal agencies will effectively serve the needs of the NSF research community.&amp;lt;/li&amp;gt;&amp;lt;/ol&amp;gt;I elaborate on these points below.&lt;br /&gt;&lt;br /&gt;&amp;lt;h2 style=&quot;text-align: left;&quot;&amp;gt;2. Technical challenges to building future, more capable advanced computing systems and how NSF might best respond to them.&amp;lt;/h2&amp;gt;&amp;lt;blockquote class=&quot;tr_bq&quot;&amp;gt;“Today’s approach of federating distributed compute- and data-intensive resources to meet the increasing demand for combined computing and data capabilities is technically challenging and expensive.”&amp;lt;/blockquote&amp;gt;This is true.&lt;br /&gt;&amp;lt;blockquote class=&quot;tr_bq&quot;&amp;gt;“New approaches that co-locate computational and data resources might reduce costs and improve performance. Recent advances in cloud data center design may provide a viable integrated solution for a significant fraction of (but not all) data- and compute-intensive and combined workloads.”&amp;lt;/blockquote&amp;gt;This strong statement is markedly unqualified and unsubstantiated.  If it is really recommending that the NSF start investing in the cloud, consider the following:&lt;br /&gt;&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;Cloud computing resources are designed for burst capabilities and are only economical when workloads are similarly uneven.  In stark contrast, most well-managed HPCs see constant, high utilization which is where the cloud becomes economically intractable.&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;The suggestion that cloud solutions can “improve performance” is unfounded.  At a purely technological level, the cloud will never perform as well as unvirtualized HPC resources, period.  Data-intensive workloads and calculations that require modest inter-node communication will suffer substantially.&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;br /&gt;In fact, if any cost reduction or performance improvement can be gained by moving to the cloud, I can almost guarantee that incrementally more can be gained by simply addressing the non-technological aspects of the current approach of operating federated HPC.  Namely, the NSF must&lt;br /&gt;&amp;lt;ol&amp;gt;&amp;lt;li&amp;gt;Stop propping up failing NSF centers who have been unable to demonstrate the ability to effectively design and operate supercomputers. &amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Stop spending money on purely experimental systems that domain scientists cannot or will not use.&amp;lt;/li&amp;gt;&amp;lt;/ol&amp;gt;&lt;br /&gt;&lt;b&gt;The NSF needs to re-focus its priorities and stop treating the XD program like a research project and start treating it like a business&lt;/b&gt;.  Its principal function should be to deliver a product (computing resources) to customers (the research community).  Any component that is not helping domain scientists accelerate discovery should be strongly scrutinized.  Who are these investments truly satisfying?&lt;br /&gt;&amp;lt;blockquote class=&quot;tr_bq&quot;&amp;gt;“New knowledge and skills will be needed to effectively use these new advanced computing technologies.”&amp;lt;/blockquote&amp;gt;This is a critical component of XD that is extremely undervalued and underfunded.  Nobody is born with the ability to know how to use HPC resources, and &lt;b&gt;optimization should be performed on users in addition to code&lt;/b&gt;.  There is huge untapped potential in collaborative training between U.S. federal agencies (DOE, DOD) and European organizations (PRACE).  If there is bureaucratic red tape in the way, it needs to be dealt with at an official level or circumvented at the grassroots level.&lt;br /&gt;&lt;br /&gt;&amp;lt;h2 style=&quot;text-align: left;&quot;&amp;gt;3. The computing needs of individual research areas.&amp;lt;/h2&amp;gt;XDMoD shows this.  &lt;b&gt;The principal workloads across XSEDE are from traditional domains like physics and chemistry, and the NSF needs to recognize that this is not going to change substantially&lt;/b&gt; over the lifetime of a program like XD. &lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://1.bp.blogspot.com/-CJtYj8P1tP0/VMnV1LPcKAI/AAAAAAAAK04/jREwPiKa77I/s1600/Screen%2BShot%2B2015-01-27%2Bat%2B10.20.54%2BPM.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;306&quot; src=&quot;http://1.bp.blogspot.com/-CJtYj8P1tP0/VMnV1LPcKAI/AAAAAAAAK04/jREwPiKa77I/s1600/Screen%2BShot%2B2015-01-27%2Bat%2B10.20.54%2BPM.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;Straight from XDMoD for 2014.  MPS = math and physical sciences, BIO = biological sciences, GEO = geosciences.  NSF directorate is not a perfect alignment; for example, I found many projects in BIO were actually chemistry and materials science.&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&lt;br /&gt;&lt;br /&gt;While I wholeheartedly agree that new communities should be engaged by lowering the barriers to entry, these activities cannot be done at a great expense of undercutting the resources required by the majority of XD users.&lt;br /&gt;&lt;br /&gt;The cost per CPU cycle should not be deviating wildly between Track 2 awards because the ROI on very expensive cycles will be extremely poor.  If the NSF wants to fund experimental systems, it needs to do that as an activity that is separate from the production resources.  Alternatively, only a small fraction of each award should be earmarked for new technologies that represent a high risk; the Stampede award was a fantastic model of how a conservative fraction of the award (10%) can fund an innovative and high-risk technology.&lt;br /&gt;&lt;br /&gt;&amp;lt;h2 style=&quot;text-align: left;&quot;&amp;gt;4. How to balance resources and demand for the full spectrum of systems, for both compute- and data-intensive applications, and the impacts on the research community if NSF can no longer provide state-of-the-art computing for its research community.&amp;lt;/h2&amp;gt;&amp;lt;blockquote class=&quot;tr_bq&quot;&amp;gt;“But it is unclear, given their likely cost, whether NSF will be able to invest in future highest-tier systems in the same class as those being pursued by the Department of Energy, Department of Defense, and other federal mission agencies and overseas.”&amp;lt;/blockquote&amp;gt;The NSF does not have the budget to support leadership computing.  This is clear even from a bird’s eye view: &lt;a href=&quot;http://science.energy.gov/~/media/budget/pdf/sc-budget-request-to-congress/fy-2014/Cong_Budget_2014_Advanced_Computing.pdf&quot;&gt;DOE ASCR’s budget for FY2012 was $428 million&lt;/a&gt; and, by comparison, &lt;a href=&quot;http://www.nsf.gov/about/budget/fy2014/pdf/18_fy2014.pdf&quot;&gt;NSF ACI’s budget was only $211 million&lt;/a&gt;.  Worse yet, despite having half the funding of its DOE counterpart, the NSF owned HPC resources at seven universities in FY2012 compared to ASCR’s three centers.&lt;br /&gt;&lt;br /&gt;Even if given the proper funding, the NSF’s practice of spreading Track 2 awards across many universities to operate its HPC assets is not conducive to operating leadership computing.  The unpredictable nature of Track 2 awards has resulted in very uneven funding for NSF centers which, quite frankly, is a terrible way to attract and retain the highly knowledgeable world-class staff that is necessary to operate world-class supercomputers.&lt;br /&gt;&lt;br /&gt;&amp;lt;h2 style=&quot;text-align: left;&quot;&amp;gt;5. The role of private industry and other federal agencies in providing advanced computing infrastructure.&amp;lt;/h2&amp;gt;The report makes some very troubling statements in reference to this question.&lt;br /&gt;&amp;lt;blockquote class=&quot;tr_bq&quot;&amp;gt;“Options for providing highest-tier capabilities that merit further exploration include purchasing computing services from federal agencies…”&amp;lt;/blockquote&amp;gt;This sounds dirty.  Aren’t there are regulations in place that restrict the way in which money can flow between the NSF and DOE?  I’m also a little put off by the fact that this option is being put forth in a report that is crafted by a number of US DOE folks whose DOE affiliations are masked by university affiliations in the introductory material.&lt;br /&gt;&amp;lt;blockquote class=&quot;tr_bq&quot;&amp;gt;“…or by making arrangements with commercial services (rather than more expensive purchases by individual researchers).”&amp;lt;/blockquote&amp;gt;Providing advanced cyberinfrastructure for the open science community is not a profitable venture.  &lt;b&gt;There is no money in HPC operations&lt;/b&gt;.  I do not see any “leadership” commercial cloud providers offering the NSF a deal on spare cycles, and the going rate for commercial cloud time is known to be &lt;a href=&quot;http://www.alcf.anl.gov/magellan&quot;&gt;far more expensive than deploying HPC resources in-house&lt;/a&gt; at the national scale.&lt;br /&gt;&lt;br /&gt;&amp;lt;h2 style=&quot;text-align: left;&quot;&amp;gt;6. The challenges facing researchers in obtaining allocations of advanced computing resources and suggestions for improving the allocation and review processes.&amp;lt;/h2&amp;gt;&amp;lt;blockquote class=&quot;tr_bq&quot;&amp;gt;“Given the “double jeopardy” that arises when researchers must clear two hurdles—first, to obtain funding for their research proposal and, second, to be allocated the necessary computing resources—the chances that a researcher with a good idea can carry out the proposed work under such conditions is diminished.”&amp;lt;/blockquote&amp;gt;XD needs to be more tightly integrated with other award processes to mitigate the double jeopardy issue.  I have a difficult time envisioning the form which this integration would take, but the NSF GRF’s approach of prominently featuring NSF HPC resources as a part of the award might be a good start.  As an adaptive proposal reviewer within XSEDE and a front-line interface with first-time users, I found that having the NSF GRF bundle XSEDE time greatly reduced the entry barrier for new users and made it easier for us reviewers to stratify the proposals.  Another idea may be to invite NSF center staff to NSF contractors’ meetings (if such things exist; I know &lt;a href=&quot;http://science.energy.gov/bes/mse/principal-investigators-meetings/&quot;&gt;they do for DOE BES&lt;/a&gt;) to show a greater amount of integration across NSF divisions.&lt;br /&gt;&lt;br /&gt;In addition, the current XSEDE allocation proposal process is extremely onerous.  The &lt;a href=&quot;https://portal.xsede.org/allocation-policies&quot;&gt;document that describes the process&lt;/a&gt; is ridiculously long and contains of obscure requirements that serve absolutely no purpose.  For example, all XSEDE proposals require a separate document detailing the scaling performance of their scientific software.  Demonstrating an awareness of the true costs of performing certain calculations has its merits, but a detailed analysis of scaling is not even relevant for the majority of users who run modest-scale jobs or use off-the-shelf black-box software like Gaussian.  The only thing these obscure requirements do is prevent new users, who are generally less familiar with all of the scaling requirements nonsense, from getting any time.  If massive scalability is truly required by an application, the PI needs to be moved over to the Track 1 system (Blue Waters) or referred to &lt;a href=&quot;http://www.doeleadershipcomputing.org/&quot;&gt;INCITE&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;As a personal anecdote, many of us center staff found ourselves simply short-circuiting the aforementioned allocations guide and providing potential new users with a guide to the guide.  It was often sufficient to provide a checklist of minutia whose absence would result in an immediate proposal rejection and allow the PIs to do what they do best—write scientific proposals for their work.  Quite frankly, the fact that we had to provide a guide to understanding the guide to the allocations process suggests that the allocations process itself is grossly over-engineered.&lt;br /&gt;&lt;br /&gt;&amp;lt;h2 style=&quot;text-align: left;&quot;&amp;gt;7. Whether wider and more frequent collection of requirements for advanced computing could be used to inform strategic planning and resource allocation; how these requirements might be used; and how they might best be collected and analyzed.&amp;lt;/h2&amp;gt;The XD program has already established a solid foundation for reporting the popularity and usability of NSF HPC resources in &lt;a href=&quot;https://xdmod.ccr.buffalo.edu/&quot;&gt;XDMoD&lt;/a&gt;.  The requirements of the majority are evolving more slowly than computer scientists would have everyone believe.&lt;br /&gt;&lt;br /&gt;Having been personally invested in two Track 2 proposals, I have gotten the impression that the review panels who select the destiny of the NSF’s future HPC portfolio are more impressed by cutting edge, albeit untested and under-demanded, proposals.  Consequentially, taking a “functional rather than a technology-focused or structural approach” to future planning will result in further loss of focus.  Instead of delivering conservatively designed architectures that will enjoy guaranteed high utilization, &lt;b&gt;functional approaches will give way to computer scientists on review panels dictating what resources domain scientists should be using&lt;/b&gt; to solve their problems.  The cart will be before the horse.&lt;br /&gt;&lt;br /&gt;Instead, it would be far more valuable to include more operational staff in strategic planning.  The people on the ground know how users interact with systems and what will and won’t work.  As with the case of leadership computing, the &lt;b&gt;NSF does not have the financial commitment to be leading the design of novel computing architectures at large scales&lt;/b&gt;.  Exotic and high-risk technologies should be simply left out of the NSF’s Track 2 program, incorporated peripherally but funded through other means (e.g., MRIs), or incorporated in the form of a small fraction of a larger, lower-risk resource investment.&lt;br /&gt;&lt;br /&gt;A perspective of the greater context of this has been &lt;a href=&quot;http://www.computer.org/cms/Computer.org/ComputingNow/docs/CISE-17-02-EIC.pdf&quot;&gt;eloquently written by Dr. Steven Gottlieb&lt;/a&gt;.  Given his description of the OCI conversion to ACI, it seems like taking away the Office of Cyberinfrastructure’s (OCI’s) autonomy and placing it under Computer &amp;amp; Information Science &amp;amp; Engineering (CISE) exemplifies an ongoing and significant loss of focus within NSF.  This changed reflected the misconception that architecting and operating HPC resources for domain sciences is a computer science discipline. &lt;br /&gt;&lt;br /&gt;This is wrong. &lt;br /&gt;&lt;br /&gt;Computer scientists have a nasty habit of creating tools that are intellectually interesting but impractical for domain scientists.  These tools get “thrown over the wall,” never to be picked up, and represent an overall waste of effort in the context of operating HPC services for non-computer scientists.  Rather, operating HPC resources for the research community requires experienced technical engineers with a pragmatic approach to HPC.  Such people are most often not computer scientists, but former domain scientists who know what does and doesn’t work for their respective communities.&lt;br /&gt;&lt;br /&gt;&amp;lt;h2 style=&quot;text-align: left;&quot;&amp;gt;8. The tension between the benefits of competition and the need for continuity as well as alternative models that might more clearly delineate the distinction between performance review and accountability and organizational continuity and service capabilities.&amp;lt;/h2&amp;gt;&amp;lt;blockquote class=&quot;tr_bq&quot;&amp;gt;“Although NSF’s use of frequent open competitions has stimulated intellectual competition and increased NSF’s financial leverage, it has also impeded collaboration among frequent competitors, made it more difficult to recruit and retain talented staff, and inhibited longer-term planning.”&amp;lt;/blockquote&amp;gt;Speaking from firsthand experience, I can say that &lt;b&gt;working for an NSF center is a life of a perpetually uncertain future and dicing up FTEs into frustratingly tiny pieces&lt;/b&gt;.  While some people are driven by competition and fundraising (I am one of them), an entire organization built up to support multi-million dollar cyberinfrastructure cannot be sustained this way.&lt;br /&gt;&lt;br /&gt;At the time I left my job at an NSF center, my salary was covered by six different funding sources at levels ranging from 0.05 to 0.30 FTEs.  Although this officially meant that I was only 30% committed to directly supporting the operation of one of our NSF supercomputers, the reality was that I (and many of my colleagues) simply had to put in more than 100% of my time into the job.  This is a very high-risk way to operate because committed individuals get noticed and almost invariably receive offers of stable salaries elsewhere.  Retaining talent is extremely difficult when you have the least to offer, and the current NSF funding structure makes it very difficult for centers to do much more than continually hire entry-level people to replace the rising stars who find greener pastures.&lt;br /&gt;&lt;br /&gt;Restoring reliable, core funding to the NSF centers would allow them to re-establish a strong foundation that can be an anchor point for other sites wishing to participate in XD.  This will effectively cut off some of the current sites operating Track 2 machines, but frankly, &lt;b&gt;the NSF has spread its HPC resources over too many sites at present and is diluting its investments&lt;/b&gt; in people and infrastructure.  The basis for issuing this core funding could follow a pattern similar to that of XD where long-term (10-year) funding is provisioned with a critical 5-year review.&lt;br /&gt;&lt;br /&gt;If the NSF cannot find a way to re-establish reliable funding, it needs to &lt;b&gt;accept defeat and stop trying to provide advanced cyberinfrastructure&lt;/b&gt;.  The current method of only funding centers indirectly through HPC acquisitions and associated operations costs is unsustainable for two reasons:&lt;br /&gt;&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;The length of these Track 2 awards (typically 3 years of operations) makes future planning impossible.  Thus, this current approach forces centers to follow high-risk and inadequately planned roadmaps.&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;All of the costs associated with maintaining world-class expertise and facilities have to come from someone else’s coffers.  Competitive proposals for HPC acquisitions simply cannot afford to request budgets that include strong education, training, and outreach programs, so these efforts wind up suffering.&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;br /&gt;&lt;br /&gt;&amp;lt;h2 style=&quot;text-align: left;&quot;&amp;gt;9. How NSF might best set overall strategy for advanced computing-related activities and investments as well as the relative merits of both formal, top-down coordination and enhanced, bottom-up process.&amp;lt;/h2&amp;gt;Regarding the top-down coordination, the NSF should drop the Track 2 program’s current solicitation model where proposers must have a vendor partner to get in the door.  This is unnecessarily restrictive and fosters an unhealthy ecosystem where vendors and NSF centers are both scrambling to pair up, resulting in high-risk proposals.  Consider the implications:&lt;br /&gt;&amp;lt;ol&amp;gt;&amp;lt;li&amp;gt;Vendors are forced to make promises that they may not be able to fulfill (e.g., Track 2C and Blue Waters).  Given these two (of nine) solicitations resulted in substantial wastes of time and money (over 20% vendor failure rate!), I find it shocking that the NSF continues to operate this way.&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;NSF centers are only capable of choosing the subset of vendors who are willing to play ball with them, resulting in a high risk of sub-optimal pricing and configurations for the end users of the system.&amp;lt;/li&amp;gt;&amp;lt;/ol&amp;gt;&lt;br /&gt;I would recommend a model, similar to many European nations’, where a solicitation is issued for a vendor-neutral proposal to deploy and support a program that is built around a resource.  A winning proposal is selected based on not only the system features, its architecture, and the science it will support, but the plan for training, education, collaboration, and outreach as well.  Following this award, the bidding process for a specific hardware solution begins.&lt;br /&gt;&lt;br /&gt;This addresses the two high-risk processes mentioned above and simultaneously eliminates the current qualification in Track 2 solicitations that no external funding can be included in the proposal.  By leaving the capital expenses out of the selection process, the NSF stands to get the best deal from all vendors and other external entities independent of the winning institution.&lt;br /&gt;&lt;br /&gt;Bottom-up coordination is much more labor-intensive because it requires highly motivated people at the grassroots to participate.  Given the NSF’s current inability to provide stable funding for highly qualified technical staff, I cannot envision how this would actually come together.&lt;br /&gt;&amp;lt;div&amp;gt;&lt;br /&gt;&amp;lt;/div&amp;gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Machine Learning for Scientists</title>
   <link href="https://hpc.social/2014/machine-learning-for-scientists/"/>
   <updated>2014-12-20T00:00:00-07:00</updated>
   <id>https://hpc.social/2014/machine-learning-for-scientists</id>
   <content type="html">&lt;p&gt;I recently taught a 1-day &lt;a href=&quot;http://ljdursi.github.io/ML-for-scientists&quot;&gt;machine learning workshop for scientists&lt;/a&gt;  for the good folks at &lt;a href=&quot;http://www.scinethpc.ca&quot;&gt;SciNetHPC&lt;/a&gt;.  There was enough interest (nearly forty people signed up for a day-long session near the end of term) that we had to book a large-ish classroom.&lt;/p&gt;

&lt;p&gt;There’s a lot of interest in the topic — which might even be surprising, given that a lot of the material is either familiar or pretty easy to digest for those who spend a lot of their time doing scientific data analysis. But for those coming to it for the first time and on their own, the difference in terminology (“features”? “shrinkage”?  Wait, you just mean variables and regularization?) and the huge number of different methods available can be pretty baffling.&lt;/p&gt;

&lt;p&gt;And I think it helps to have someone with a science background to explain the very different approaches taken to modelling than in the sciences (especially the natural sciences) and &lt;em&gt;why&lt;/em&gt; it is that way.  Having that connection means that you can translate – so that the very real expertise and experience they do already have can be a benefit, rather than throwing up barriers.  (“Bias-Variance tradeoff?  You mean you’re willing to introduce error just to get the error bars down a bit – centred on the &lt;em&gt;wrong&lt;/em&gt; &lt;em&gt;answer&lt;/em&gt;?  What kind of monster are you, and what dangerous nonsense is this machine learning stuff?”)&lt;/p&gt;

&lt;p&gt;This was the first time teaching this material, and while there are some things I’d like to improve (especially doing more on PCA and clustering, although I don’t know what I’d take out for a 1-day class), I think that it went fairly well.  The presentation can be seen &lt;a href=&quot;http://ljdursi.github.io/ML-for-scientists&quot;&gt;online&lt;/a&gt;, and everything’s available on &lt;a href=&quot;https://github.com/ljdursi/ML-for-scientists&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Incidentally, this was my first time using &lt;a href=&quot;http://slidify.org&quot;&gt;Slidify&lt;/a&gt; for a presentation, and I really enjoyed it – this may be the first markdown/html5 setup that finally gets me willingly moving away from Keynote for this sort of material.  Obviously, Slidify integrates much more closely with R than with python, particularly for graphics; but still, it was a pleasure to use.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Storage Utilization in the Long Tail of Science</title>
   <link href="https://hpc.social/2014/storage-utilization-in-the-long-tail-of-science/"/>
   <updated>2014-11-05T15:53:00-07:00</updated>
   <id>https://hpc.social/2014/storage-utilization-in-the-long-tail-of-science</id>
   <content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Since changing careers and moving up to the San Francisco Bay Area in July, I haven’t had nearly as much time to post interesting things here on my blog—I guess that’s the startup life.  That isn’t to say that my life in DNA sequencing hasn’t been without interesting observations to explore though; the world of high-throughput sequencing is becoming increasingly dependent on high-performance computing, and many of the problems being solved in genomics and bioinformatics are stressing aspects of system architecture and cyberinfrastructure that haven’t gotten a tremendous amount of exercise from the more traditional scientific domains in computational research. &lt;br /&gt;&lt;br /&gt;Take, for example, &lt;a href=&quot;http://systems.illumina.com/systems/hiseq-x-sequencing-system.ilmn&quot;&gt;the biggest and baddest DNA sequencer on the market&lt;/a&gt;: over the course of a three-day run, it outputs around 670 GB of raw (but compressed) sequence data, and this data is spread out over 1,400,000 files.  This would translate to an average file size of around 500 KB, but the reality is that the file sizes are a lot less uniform:&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://3.bp.blogspot.com/-f1nf0-PQkRA/VCjZ9NZatZI/AAAAAAAAKuQ/cQZfm6HKV28/s1600/hiseqx-filesizedist.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;271&quot; src=&quot;http://3.bp.blogspot.com/-f1nf0-PQkRA/VCjZ9NZatZI/AAAAAAAAKuQ/cQZfm6HKV28/s1600/hiseqx-filesizedist.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;Figure 1. File size distribution of a single flow cell output (~770 gigabases) on Illumina’s highest-end sequencing platform&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&lt;br /&gt;After some basic processing (which involves opening and closing hundreds of these files repeatedly and concurrently), these data files are converted into very large files (tens or hundreds of gigabytes each) which then get reduced down to data that is more digestible over the course of hundreds of CPU hours.  As one might imagine, this entire process is very good at taxing many aspects of file systems, and on the computational side, most of this IO-intensive processing is not distributed and performance benefits most from single-stream, single-client throughput.&lt;br /&gt;&lt;br /&gt;As a result of these data access and processing patterns, the storage landscape in the world of DNA sequencing and bioinformatics is quite different from conventional supercomputing.  Some large sequencing centers do use the file systems we know and love (and hate) like &lt;a href=&quot;http://www.nersc.gov/users/computational-systems/genepool/file-storage-and-io/&quot;&gt;GPFS at JGI&lt;/a&gt; and &lt;a href=&quot;http://insidehpc.com/2013/10/sanger-institute-deploys-22-petabytes-lustre-powered-ddn-storage/&quot;&gt;Lustre at Sanger&lt;/a&gt;, but it appears that most small- and mid-scale sequencing operations are relying heavily on network-attached storage (NAS) for both receiving raw sequencer data and being a storage substrate for all of the downstream data processing.&lt;br /&gt;&lt;br /&gt;I say all of this because these data patterns—accessing large quantities of small files and large files with a high degree of random IO—is a common trait in many scientific applications used in the “long tail of science.”  The fact is, the sorts of IO for which parallel file systems like Lustre and GPFS are designed are tedious (if not difficult) to program, and for the majority of codes that don’t require thousands of cores to make new discoveries, simply reading and writing data files in a naïve way is “good enough.”&lt;br /&gt;&lt;br /&gt;&amp;lt;h3&amp;gt;The Long Tail&amp;lt;/h3&amp;gt;This long tail of science is also using up a huge amount of the supercomputing resources made available to the national open science community; to illustrate, 98% of all jobs submitted to the XSEDE supercomputers in 2013 used 1024 or fewer CPU cores, and these modest-scale jobs represented over 50% of all the CPU time burned up on these machines.&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://3.bp.blogspot.com/-h1Xc98JyrW0/VCjapVMZXQI/AAAAAAAAKuY/aB-B7ZjkOZQ/s1600/Job%2BSize%2BDistribution%2B-%2B2013.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;271&quot; src=&quot;http://3.bp.blogspot.com/-h1Xc98JyrW0/VCjapVMZXQI/AAAAAAAAKuY/aB-B7ZjkOZQ/s1600/Job%2BSize%2BDistribution%2B-%2B2013.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;Figure 2. Cumulative job size distribution (weighted by job count and SUs consumed) for all jobs submitted to XSEDE compute resources in 2013&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&lt;br /&gt;The NSF has responded to this shift in user demand by awarding &lt;a href=&quot;http://www.sdsc.edu/News%20Items/PR100313_comet.html&quot;&gt;Comet, a 2 PF supercomputer designed to run these modest-scale jobs&lt;/a&gt;.  The Comet architecture limits its full-bisection bandwidth interconnectivity to &lt;a href=&quot;http://dx.doi.org/10.1145/2616498.2616540&quot;&gt;groups of 72 nodes&lt;/a&gt;, and these 72-node islands will actually have enough cores to satisfy 99% of all the jobs submitted to XSEDE clusters in 2013 (see above).  By limiting the full-bisection connectivity to smaller islands and using less rich connectivity between islands, the cost savings in not having to buy so many mid-tier and core switches are then turned into additional CPU capacity.&lt;br /&gt;&lt;br /&gt;What the Comet architecture &lt;i&gt;doesn’t&lt;/i&gt; address, however, is the question of data patterns and IO stress being generated by this same long tail of science—the so-called 99%.  If DNA sequencing is any indicator of the 99%, parallel file systems are actually a poor choice for high-capacity, mid-scale jobs because their &lt;a href=&quot;http://dx.doi.org/10.1145/2159352.2159356&quot;&gt;performance degrades significantly when facing many small files&lt;/a&gt;.  Now, the real question is, are the 99% of HPC jobs really generating and manipulating lots of small files in favor of the large striped files that Lustre and GPFS are designed to handle?  That is, might the majority of jobs on today’s HPC clusters actually be better served by file systems that are less scalable but handle small files and random IO more gracefully?&lt;br /&gt;&lt;br /&gt;Some colleagues and I set out to answer this question last spring, and a part of this quest involved looking at every single file on two of SDSC’s Data Oasis file systems.  This represented about 1.7 PB of real user data spread across two Lustre 2.4 file systems—one designed for temporary scratch data and the other for projects storage—and we wanted to know if users’ data really consisted of the large files that Lustre loves or if, like job size, the 99% are really working with small files.  Since SDSC’s two national resources, Gordon and Trestles, restrict the maximum core count for user jobs to modest-scale submissions, these file systems should contain files representative of long-tail users.&lt;br /&gt;&lt;br /&gt;&amp;lt;h2&amp;gt;Scratch File Systems&amp;lt;/h2&amp;gt;At the roughest cut, files can be categorized based on whether their size is on the order of bytes and kilobytes (size &amp;lt; 1024*1024 bytes), megabytes (&amp;lt; 1024 KB), gigabytes (&amp;lt;1024 MB), and terabytes (&amp;lt; 1024 GB).  Although pie charts are generally a terrible way to show relative compositions, this is how the files on the 1.2 PB scratch file system broke down:&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://1.bp.blogspot.com/-e-UpylKZPBw/VCjcZFRbClI/AAAAAAAAKuk/uf38vgGNnNk/s1600/file%2Bcount%2Bpie.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;320&quot; src=&quot;http://1.bp.blogspot.com/-e-UpylKZPBw/VCjcZFRbClI/AAAAAAAAKuk/uf38vgGNnNk/s1600/file%2Bcount%2Bpie.png&quot; width=&quot;296&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;Figure 3. Fraction of file count consumed by files of a given size on Data Oasis’s scratch file system for Gordon&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&lt;br /&gt;&lt;br /&gt;The above figure shows the number of files on the file system classified by their size, and there are clearly a preponderance of small files less than a gigabyte in size.  This is not terribly surprising as the data is biased towards smaller files; that is, you can fit a thousand one-megabyte files in the same space that a single one-gigabyte file would take up.  Another way to show this data is by how much file system capacity is taken up by files of each size:&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://3.bp.blogspot.com/-htbZijzc2MY/VCjcdu-hPrI/AAAAAAAAKus/Y8F4ohme4Yg/s1600/file%2Bsize%2Bpie.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;320&quot; src=&quot;http://3.bp.blogspot.com/-htbZijzc2MY/VCjcdu-hPrI/AAAAAAAAKus/Y8F4ohme4Yg/s1600/file%2Bsize%2Bpie.png&quot; width=&quot;296&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;Figure 4. File system capacity consumed by files of a given size on Data Oasis’s scratch file system for Gordon&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&lt;br /&gt;&lt;br /&gt;This makes it very apparent that the vast majority of the used space on this scratch file system—a total of 1.23 PB of data—are taken up by files on the order of gigabytes and megabytes.  There were only seventeen files that were a terabyte or larger in size.  &lt;br /&gt;&lt;br /&gt;Incidentally, I don’t find it too surprising that there are so few terabyte-sized files; even in the realm of Hadoop, median job dataset sizes are on the order of a dozen gigabytes (e.g., Facebook has reported that &lt;a href=&quot;http://dx.doi.org/10.1145/2169090.2169092&quot;&gt;90% of its jobs read in under 100 GB of data&lt;/a&gt;).  Examining file sizes with much finer granularity reveals that the research data on this file system isn’t even of Facebook scale though:&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://4.bp.blogspot.com/-FrkhmvkuOao/VCjclW_IAqI/AAAAAAAAKu0/7sMuQQlrXas/s1600/file%2Bsize%2Bdistribution.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;272&quot; src=&quot;http://4.bp.blogspot.com/-FrkhmvkuOao/VCjclW_IAqI/AAAAAAAAKu0/7sMuQQlrXas/s1600/file%2Bsize%2Bdistribution.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;Figure 5. Number of files of a given size on Data Oasis’s scratch file system for Gordon.  This data forms the basis for Figure 3 above&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&lt;br /&gt;&lt;br /&gt;While there are a large number of files on the order of a few gigabytes, it seems that files on the order of tens of gigabytes or larger are far more scarce.  Turning this into relative terms,&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://1.bp.blogspot.com/-b1zxYPEkiA4/VCjctOul9LI/AAAAAAAAKu8/I0LHbxoEoTU/s1600/cumul%2Bfile%2Bsize%2Bdistribution.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;276&quot; src=&quot;http://1.bp.blogspot.com/-b1zxYPEkiA4/VCjctOul9LI/AAAAAAAAKu8/I0LHbxoEoTU/s1600/cumul%2Bfile%2Bsize%2Bdistribution.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;Figure 6. Cumulative distribution of files of a given size on Data Oasis’s scratch file system for Gordon&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&lt;br /&gt;&lt;br /&gt;we can make more meaningful statements.  In particular,&lt;br /&gt;&lt;br /&gt;&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;90% of the files on this Lustre file system are 1 megabyte or smaller&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;99% of files are 32 MB or less&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;99.9% of files are 512 MB or less&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;and 99.99% of files are 4 GB or less&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;br /&gt;The first statement is quite powerful when you consider the fact that the default stripe size in Lustre is 1 MB.  The fact that 90% of files on the file system are smaller than this means that &lt;b&gt;90% of users’ files really gain no advantages by living on Lustre&lt;/b&gt;.  Furthermore, since this is a scratch file system that is meant to hold temporary files, it would appear that either user applications are generating a large amount of small files, or users are copying in large quantities of small files and improperly using it for cold storage.  Given the quota policies for Data Oasis, I suspect there is a bit of truth to both.&lt;br /&gt;&lt;br /&gt;Circling back a bit though, I said earlier that comparing just the quantity of files can be a bit misleading since a thousand 1 KB files will take up the same space as a single 1 MB file.   We can also look at how much total space is taken up by files of various sizes.&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://3.bp.blogspot.com/-jSmVlIJTa9E/VCjdrFMZlZI/AAAAAAAAKvI/gVPZm53WnDA/s1600/bin%2Bweight%2Band%2Bcumul%2Bdist.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;271&quot; src=&quot;http://3.bp.blogspot.com/-jSmVlIJTa9E/VCjdrFMZlZI/AAAAAAAAKvI/gVPZm53WnDA/s1600/bin%2Bweight%2Band%2Bcumul%2Bdist.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;Figure 7. File system capacity consumed by files of a given size on Data Oasis’s scratch file system for Gordon.  This is just a more finely diced version of the data presented in Figure 4 above.&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&lt;br /&gt;The above chart is a bit data-dense so it takes some staring at to understand what’s going on.  First looking at the purple line, we can pull out some pretty interesting facts:&lt;br /&gt;&lt;br /&gt;&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;Half of the file system’s used capacity (50%) is consumed by files that are 1 GB or less in size&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Over 20% of the file system’s used capacity is taken up by files smaller than 64 MB&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;About 10% of the capacity is used by files that are 64 GB or larger&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&lt;br /&gt;The blue boxes represent the derivative of that purple line—that is, how much space is taken up by files of only one specific size.  The biggest chunk of the file system (141 TB) is taken up by 4 GB files, but it appears that there is a substantial range of file sizes that take up very similarly sized pieces of the pie.  512 MB files take up a total of 139 TB; 1 GB, 2 GB, and 8 GB files all take up over 100 TB of total space each as well.  In fact, files ranging from 512 MB to 8 GB comprise 50% of the total file system capacity.&lt;br /&gt;&lt;br /&gt;Why the sweet spot for space-consuming files is between 512 MB and 8 GB is unclear, but I suspect it’s more caused by the human element in research.  In my own research, I worked with files in this range simply because it was enough data to be statistically meaningful while still small enough to quickly re-analyze or transfer to a colleague.  For file sizes above this range, the mass of the data made it difficult to manipulate using the “long-tail” cyberinfrastructure available to me. But, perhaps as more national-scale systems comes online to meet the needs of these sorts of workloads, this sweet spot will creep out to larger file sizes.&lt;br /&gt;&lt;br /&gt;&amp;lt;h2&amp;gt;Projects Storage&amp;lt;/h2&amp;gt;The above discussion admittedly comes with a lot of caveats.  In particular, the scratch file system we examined was governed by no hard quotas which did lead some people to leave data resident for longer than they probably should have.  However, the other file system we analyzed was SDSC’s Data Oasis projects storage which was architected for capacity over performance and featured substantially more disks per OSS.  This projects storage also came with 500 GB quotas by default, forcing users to be a little more mindful of what was worth keeping.&lt;br /&gt;&lt;br /&gt;Stepping back to the coarse-grained kilobyte/megabyte/gigabyte/terabyte pie charts, here is how projects storage utilization compared to scratch storage:&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://3.bp.blogspot.com/-wdsd5yB18VE/VCjiRbSA0HI/AAAAAAAAKvU/W52Xv6-Z8-w/s1600/ct%2Bbreakdown%2Bcompare.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;400&quot; src=&quot;http://3.bp.blogspot.com/-wdsd5yB18VE/VCjiRbSA0HI/AAAAAAAAKvU/W52Xv6-Z8-w/s1600/ct%2Bbreakdown%2Bcompare.png&quot; width=&quot;348&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;Figure 8. Fraction of file count consumed by files of a given size on Data Oasis’s projects file system (shared between Gordon and Trestles users)&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&lt;br /&gt;On the basis of file counts, it’s a bit surprising that users seem to store more smaller (kilobyte-sized) files in their projects space than their scratch space.  This may imply that the beginning and end data bookending simulations aren’t as large as the intermediate data generated during the calculation.  Alternately, it may be a reflection of user naïveté; I’ve found that newer users were often afraid to use the scratch space because of the perception that their data may vanish from there without advanced notice.  Either way, gigabyte-sized files comprised a few hundredths of a percent of files, and terabyte-sized files were more scarce still on both file systems.  The trend was uniformly towards smaller sizes on projects space.&lt;br /&gt;&lt;br /&gt;As far as space consumed by these files, the differences remain subtle.&lt;br /&gt;&lt;br /&gt;&amp;lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td style=&quot;text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://1.bp.blogspot.com/-3cs8iwXSnRA/VCjmML1IMMI/AAAAAAAAKvg/HUFZh8BYsLk/s1600/size%2Bbkdown%2Bcompare.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;271&quot; src=&quot;http://1.bp.blogspot.com/-3cs8iwXSnRA/VCjmML1IMMI/AAAAAAAAKvg/HUFZh8BYsLk/s1600/size%2Bbkdown%2Bcompare.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&amp;gt;Figure 9. Fraction of file system capacity consumed by files of a given size on Data Oasis’s projects file system&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&lt;br /&gt;There appears to be a trend towards users keeping larger files in their projects space, and the biggest change is the decrease in megabyte-sized files in favor of gigabyte-sized files.  However, this trend is very small and persists across a finer-grained examination of file size distributions:&lt;br /&gt;&lt;br /&gt;&amp;lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&amp;gt;&amp;lt;/div&amp;gt;&lt;/p&gt;
&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;http://3.bp.blogspot.com/-yYPXPGFN2ck/VCjvIHxoJpI/AAAAAAAAKv8/MiUafRm7yCU/s1600/megaplot.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;271&quot; src=&quot;http://3.bp.blogspot.com/-yYPXPGFN2ck/VCjvIHxoJpI/AAAAAAAAKv8/MiUafRm7yCU/s1600/megaplot.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Figure 10.&amp;nbsp;File system capacity consumed by files of a given size on Data Oasis's projects file system&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;&lt;br /&gt;Half of the above plot is the same data shown above, making this plot twice as busy and confusing.  However there’s a lot of interesting data captured in it, so it’s worth the confusing presentation.  In particular, the overall distribution of mass with respect to the various file sizes is remarkably consistent between scratch and projects storage.  We see the same general peak of file size preference in the 1 GB to 10 GB range, but there is a subtle bimodal divide in projects storage that reveals preference for 128MB-512MB and 4GB-8GB files which manifests in the integrals (red and purple lines) that show a visibly greater slope in these regions.&lt;br /&gt;&lt;br /&gt;The observant reader will also notice that the absolute values of the bars are smaller for projects storage and scratch storage; this is a result of the fact that the projects file system is subject to quotas and, as a result, is not nearly as full of user data.  To complicate things further, the projects storage represents user data from two different machines (each with unique job size policies, to boot), whereas the scratch storage is only accessible from one of those machines.  Despite these differences though, user data follows very similar distributions between both file systems.&lt;br /&gt;&lt;br /&gt;&amp;lt;h2&amp;gt;Corollaries&amp;lt;/h2&amp;gt;It is probably unclear what to take away from these data, and that is with good reason.  There are fundamentally two aspects to quantifying storage utilizations–raw capacity and file count–because they represent two logically separate things.  There is some degree of interchangeability (e.g., storing a whole genome in one file vs. storing each chromosome its own file), and this is likely contributing to the broad peak in file size between 512 MB and 8 GB.  With that being said, it appears that the typical long-tail user stores a substantial amount of decidedly “small” files on Lustre, and this is exemplified by the fact that 90% of the files resident on the file systems analyzed here are 1 MB or less in size.&lt;br /&gt;&amp;lt;div&amp;gt;&lt;br /&gt;&amp;lt;/div&amp;gt;&lt;/p&gt;
&lt;div&gt;This alone suggests that large parallel file systems may not actually be the most appropriate choice for HPC systems that are designed to support a large group of long-tail users. &amp;nbsp;While file systems like Lustre and GPFS certainly provide a unique &lt;i&gt;capability&lt;/i&gt; in that some types of medium-sized jobs absolutely require the IO capabilities of parallel file systems, there are a larger number of long-tail applications that do single-thread IO, and some of these perform IO in such an abusive way (looking at you, quantum chemistry) that they cannot run on file systems like Lustre or GPFS because of the number of small files and random IO they use.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;So if Lustre and GPFS aren't the unequivocal best choice for storage in long-tail HPC, what are the other options?&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;h3&gt;Burst Buffers&lt;/h3&gt;&lt;/div&gt;
&lt;div&gt;I would be remiss if I neglected to mention burst buffers here since they are designed, in part, to address the limitations of parallel file systems. &amp;nbsp;However, their actual usability remains unproven. &amp;nbsp;Anecdotally, long-tail users are generally not quick to alter the way they design their jobs to use cutting-edge technology, and my personal experiences with Gordon (and its 300 TB of flash) were that getting IO-nasty user applications to effectively utilize the flash was often a very manual process that introduced new complexities, pitfalls, and failure modes. &amp;nbsp;Gordon was a very experimental platform though, and &lt;a href=&quot;http://www.cray.com/Products/Computing/XC/DataWarp.aspx&quot;&gt;Cray's new DataWarp&lt;/a&gt; burst buffer seems to be the first large-scale productization of this idea. &amp;nbsp;It will be interesting to see how well it works for real users when the technology starts &lt;a href=&quot;https://www.nersc.gov/users/computational-systems/cori/&quot;&gt;hitting the floor for open science in mid-2016&lt;/a&gt;, if not sooner.&lt;/div&gt;
&lt;div&gt;&lt;h3&gt;High-Performance NAS&lt;/h3&gt;&lt;/div&gt;
&lt;div&gt;An emerging trend in HPC storage is the use of high-performance NAS as a complementary file system technology in HPC platforms. &amp;nbsp;Traditionally, NAS has been a very poor choice for HPC applications because of the limited scalability of the typical NAS architecture--data resides on traditional local file system with network service being provided by an additional software layer like NFS, and the ratio of storage capacity to network bandwidth out of the NAS is very high.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;The emergence of cheap RAM and enterprise SSDs has allowed some sophisticated file systems like ZFS and NetApp's WAFL to demonstrate very high performance, especially in delivering very high random read performance, by using both RAM and flash as a buffer between the network and spinning rust. &amp;nbsp;This allows certain smaller-scale jobs to enjoy substantially better performance when running on flash-backed NAS than a parallel file system. &amp;nbsp;Consider the following IOP/metadata benchmark run on a parallel file system and a NAS head with SSDs for caching:&lt;br /&gt;&lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;http://1.bp.blogspot.com/-fyF1j9G0ouU/VFHNnHOB-YI/AAAAAAAAKxQ/owVhLiILb1E/s1600/mdstat-stats-per-sec.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;271&quot; src=&quot;http://1.bp.blogspot.com/-fyF1j9G0ouU/VFHNnHOB-YI/AAAAAAAAKxQ/owVhLiILb1E/s1600/mdstat-stats-per-sec.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Figure 11. File stat rate on flash-backed NAS vs. a parallel file system as measured by &lt;a href=&quot;http://mdtest.sourceforge.net/&quot;&gt;the mdtest benchmark&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;A four-node job that relies on &lt;a href=&quot;http://pubs.opengroup.org/onlinepubs/009695399/functions/stat.html&quot;&gt;statting&lt;/a&gt; many small files (for example, an application that traverses a large directory structure such as the output of one of the Illumina sequencers I mentioned above) &lt;i&gt;can&lt;/i&gt; achieve a much higher IO rate on a high-performance NAS than on a parallel file system. &amp;nbsp;Granted, there are a lot of qualifications to be made with this statement and benchmarking high-performance NAS is worth a post of its own, but the above data illustrate a case where NAS may be preferable over something like Lustre.&lt;/div&gt;
&lt;h3&gt;Greater Context&lt;/h3&gt;
&lt;div&gt;Parallel file systems like Lustre and GPFS will always play an essential role in HPC, and I don't want to make it sound like they can be universally replaced by high-performance NAS. &amp;nbsp;They are fundamentally architected to scale out so that increasing file system bandwidth does not require adding new partitions or using &lt;a href=&quot;http://www.netapp.com/us/products/platform-os/infinite-volume.aspx&quot;&gt;software to emulate a single namespace&lt;/a&gt;. &amp;nbsp;In fact, the single namespace of parallel file systems makes the management of the storage system, its users, and the underlying resources very flexible and straightforward. &amp;nbsp;No volume partitioning needs to be imposed, so scientific applications' and projects' data consumption do not have to align with physical hardware boundaries.&lt;br /&gt;&lt;br /&gt;&lt;div&gt;However, there are cases where a single namespace is not necessary at all; for example, user home directories are naturally partitioned with fine granularity and can be mounted in a uniform location while physically residing on different NAS heads with a simple autofs map. &amp;nbsp;In this example, leaving user home directories on a pool of NAS filers offers two big benefits:&lt;br /&gt;&lt;br /&gt;&lt;ol&gt;&lt;li&gt;Full independence of the underlying storage mitigates the impact of one bad user. &amp;nbsp;A large job dropping multiple files per MPI process will crush both Lustre and NFS, but in the case of Lustre, the MDS may become unresponsive and block IO across all users' home directories.&lt;/li&gt;&lt;li&gt;Flash caches on NAS can provide higher performance on IOP-intensive workloads at long-tail job sizes. &amp;nbsp;In many ways, high-performance NAS systems have the built-in burst buffers that parallel file systems are only now beginning to incorporate.&lt;/li&gt;&lt;/ol&gt;&lt;div&gt;Of course, these two wins come at a cost:&lt;/div&gt;
&lt;div&gt;&lt;ol&gt;&lt;li&gt;Fully decentralized storage is more difficult to manage. &amp;nbsp;For example, balancing capacity across all NAS systems is tricky when users have very different data generation rates that they do not disclose ahead of time.&lt;/li&gt;&lt;li&gt;Flash caches can only get you so far, and NFS will fall over when enough IO is thrown at it. &amp;nbsp;I mentioned that 98% of all jobs use 1024 cores or fewer (see Figure 1), but 1024 cores all performing heavy IO on a typical capacity-rich, bandwidth-poor NAS head will cause it to grind to a halt.&lt;/li&gt;&lt;/ol&gt;&lt;div&gt;&lt;div&gt;Flash-backed high-performance NAS is not an end-all storage solution for long-tail computational science, but it also isn't something to be overlooked outright. &amp;nbsp;As with any technology in the HPC arena, its utility may or may not match up well with users' workloads, but when it does, it can deliver less pain and better performance than parallel file systems.&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;h2&gt;Acknowledgments&amp;nbsp;&lt;/h2&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div&gt;As I mentioned above, the data I presented here was largely generated as a result of an internal project in which I participated while at SDSC. &amp;nbsp;I couldn't have cobbled this all together without the help of SDSC's HPC Systems group, and I'm really indebted to &lt;a class=&quot;g-profile&quot; href=&quot;https://plus.google.com/115709389472600856394&quot; target=&quot;_blank&quot;&gt;+Rick&lt;/a&gt;,&amp;nbsp;&lt;a class=&quot;g-profile&quot; href=&quot;https://plus.google.com/105132496853043288048&quot; target=&quot;_blank&quot;&gt;+Haisong&lt;/a&gt;, and&amp;nbsp;&lt;a class=&quot;g-profile&quot; href=&quot;https://plus.google.com/113299603442523075439&quot; target=&quot;_blank&quot;&gt;+Trevor&lt;/a&gt;&amp;nbsp;for doing a lot of the heavy lifting in terms of generating the original data, getting systems configured to test, and figuring out what it all meant when the dust settled (even after I had left!). &amp;nbsp;SDSC's really a world-class group of individuals.&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>The Shell For Scientists</title>
   <link href="https://hpc.social/2014/the-shell-for-scientists/"/>
   <updated>2014-10-05T01:00:00-06:00</updated>
   <id>https://hpc.social/2014/the-shell-for-scientists</id>
   <content type="html">&lt;p&gt;I’ve posted a half-day “The Shell for Scientists”
&lt;a href=&quot;https://github.com/ljdursi/shell-for-scientists&quot;&gt;tutorial&lt;/a&gt; that
I’ve given variants on a number of times; the motivating problem,
provided by Greg Wilson for a two-day set of of tutorials at the
University of Toronto, was cleaning up a bunch of auditory lab data
on people’s cochlear implants.&lt;/p&gt;

&lt;p&gt;The focus is on productivity and automation; PDF slides are available
&lt;a href=&quot;https://github.com/ljdursi/shell-for-scientists/raw/master/presentation/presentation.pdf&quot;&gt;here&lt;/a&gt;
(although I really should translate them into a markdown-based format to
make them more re-usable).&lt;/p&gt;

&lt;p&gt;Covered are a number of basic shell commands&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;echo&lt;/li&gt;
  &lt;li&gt;pwd&lt;/li&gt;
  &lt;li&gt;cd&lt;/li&gt;
  &lt;li&gt;ls&lt;/li&gt;
  &lt;li&gt;man&lt;/li&gt;
  &lt;li&gt;file&lt;/li&gt;
  &lt;li&gt;cat&lt;/li&gt;
  &lt;li&gt;more&lt;/li&gt;
  &lt;li&gt;wc&lt;/li&gt;
  &lt;li&gt;mv&lt;/li&gt;
  &lt;li&gt;cp&lt;/li&gt;
  &lt;li&gt;rm&lt;/li&gt;
  &lt;li&gt;head&lt;/li&gt;
  &lt;li&gt;tail&lt;/li&gt;
  &lt;li&gt;sort&lt;/li&gt;
  &lt;li&gt;mkdir&lt;/li&gt;
  &lt;li&gt;rmdir&lt;/li&gt;
  &lt;li&gt;grep&lt;/li&gt;
  &lt;li&gt;for..do..done&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As well as simple script writing.  There is some optional material
on make (again, for automation) and ssh/scp (because that was
frequently necessary for tutorials at SciNet).  There are a number
of hands-on exercises sprinkled throughout.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Floating-Point Data Shouldn't Be Serialized As Text</title>
   <link href="https://hpc.social/2014/floating-point-data-shouldn-t-be-serialized-as-text/"/>
   <updated>2014-10-01T01:00:00-06:00</updated>
   <id>https://hpc.social/2014/floating-point-data-shouldn-t-be-serialized-as-text</id>
   <content type="html">&lt;p&gt;Write data files in a binary format, unless you’re going to actually be reading the output - and you’re not going to be reading a millions of data points.&lt;/p&gt;

&lt;p&gt;The reasons for using binary are threefold, in decreasing importance:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Accuracy&lt;/li&gt;
  &lt;li&gt;Performance&lt;/li&gt;
  &lt;li&gt;Data size&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Accuracy concerns may be the most obvious.  When you are converting a (binary) floating point number to a string representation of the decimal number, you are inevitably going to truncate at some point.  That’s ok if you are sure that when you read the text value back into a floating point value, you are certainly going to get the same value; but that is actually a subtle question and requires choosing your format carefully.  Using default formatting, various compilers perform this task with varying degrees of quality.  &lt;a href=&quot;http://randomascii.wordpress.com/2013/02/07/float-precision-revisited-nine-digit-float-portability/&quot;&gt;This blog post&lt;/a&gt;, written from the point of view of a games programmer, does a good job of covering the issues; but note that for technical computing, we generally must be much more demanding about accuracy.&lt;/p&gt;

&lt;p&gt;Let’s consider a little program which, for a variety of formats, writes a single-precision real number out to a string, and then reads it back in again, keeping track of the maximum error it encounters.  We’ll just go from 0 to 1, in units of machine epsilon.  The code follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-c&quot;&gt;#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;math.h&amp;gt;
#include &amp;lt;float.h&amp;gt;

int main(int argc, char **argv) {

    const int nformats=5;
    char *formats[] = { &quot;%11.4f&quot;, &quot;%13.6f&quot;, &quot;%15.8f&quot;, &quot;%17.10f&quot;, &quot;%f&quot; };
    float maxerrors[nformats];

    for (int fmt=0; fmt&amp;lt;nformats; fmt++)
        maxerrors[fmt] = 0.;

    float input = 0;
    while (input &amp;lt; 1) {
        for (int fmt=0; fmt&amp;lt;nformats; fmt++) {
            char stringrep[128];
            sprintf(stringrep, formats[fmt], input);

            float output;
            sscanf(stringrep, &quot;%f&quot;, &amp;amp;output);

            float err = fabs(output-input);
            if (err &amp;gt; maxerrors[fmt])
                maxerrors[fmt] = err;
        }

        input += FLT_EPSILON;
    }

    printf(&quot;Maximum errors: \n&quot;);
    for (int fmt=0; fmt&amp;lt;nformats; fmt++)
        printf(&quot;%12s\t&quot;, formats[fmt]);
    printf(&quot;\n&quot;);
    for (int fmt=0; fmt&amp;lt;nformats; fmt++)
        printf(&quot;%12.6f\t&quot;, maxerrors[fmt]);
    printf(&quot;\n&quot;);

    return 0;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and when we run it, we get:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ./accuracy
Maximum errors:
      %11.4f	      %13.6f	      %15.8f	     %17.10f	          %f
5.000830e-05	5.066395e-07	7.450581e-09	5.820766e-11	5.066395e-07
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that even using a format with 8 digits after the decimal place - which we might think would be plenty, given that &lt;a href=&quot;http://stackoverflow.com/questions/24377058/decimal-accuracy-of-binary-floating-point-numbers/24387402#24387402&quot;&gt;single precision reals are only accurate to 6-7 decimal places&lt;/a&gt; - when the data makes a round trip through string-formatting  we don’t get exact copies back, off by approximately $10^{-8}$.  And this compiler’s default format does &lt;em&gt;not&lt;/em&gt; give us accurate round-trip floating point values; some error is introduced!  If you’re a video-game programmer, that level of accuracy may well be enough.  For scientific/technical work, however, that might absolutely not be ok, particularly if there’s some bias to where the error is introduced, or if the error occurs in what is supposed to be a conserved quantity.&lt;/p&gt;

&lt;p&gt;Note that if you try running this code, you’ll notice that it takes a surprisingly long time to finish.  That’s because, maybe surprisingly, performance is another real issue with text output of floating point numbers.  Consider a following simple program, which just writes out a 2d array of a 5000 × 5000 floats as text (using &lt;code&gt;fprintf()&lt;/code&gt; and as unformatted binary, using &lt;code&gt;fwrite()&lt;/code&gt;.   The code will follow, but to start here’s the timing outputs:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ./io-performance 5000
Text      : time = 20.229191
Raw Binary: time = 0.042213
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that when writing to disk, the binary output is &lt;strong&gt;479 times&lt;/strong&gt; as fast as text output.  There are two reasons for this - one is that you can write out data all at once, rather than having to loop; the other is that generating the string decimal representation of a floating point number is a surprisingly subtle operation which requires a significant amount of computing for each value.&lt;/p&gt;

&lt;p&gt;Finally, is data size; the text file in the above example comes out (on my system - depends on compilers default floating string representation, etc) to about 4 times the size of the binary file.&lt;/p&gt;

&lt;p&gt;Now, there are real problems with binary output.  In particular, raw binary output is very brittle.  If you change platforms, or your data size changes, your output may no longer be any good.  Adding new variables to the output will break the file format unless you always add new data at the end of the file, and you have no way of knowing ahead of time what variables are in a binary blob you get from your collaborator (who might be you, three months ago).&lt;/p&gt;

&lt;p&gt;Most of the downsides of binary output are avoided by using libraries which use binary output to serialize, but include enough metadata to describe the data.  For output of large scientific arrays, &lt;a href=&quot;http://www.unidata.ucar.edu/software/netcdf/&quot;&gt;NetCDF&lt;/a&gt; – which writes self-describing binary files that are much more “future proof” than raw binary – is a good chioce.  Better still, since it’s a standard, many tools read NetCDF files.  In other contexts, formats like &lt;a href=&quot;http://bsonspec.org&quot;&gt;BSON&lt;/a&gt; make a lot of sense.&lt;/p&gt;

&lt;p&gt;There are many NetCDF tutorials on the internet; one I wrote is is &lt;a href=&quot;http://wiki.scinethpc.ca/wiki/images/a/af/Netcdfhdf5.pdf&quot;&gt;here&lt;/a&gt;.  A simple example using NetCDF gives IO performance results much closer to raw binary than to text:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ./io-performance
Text      : time = 20.504855
Raw Binary: time = 0.049945
NetCDF4   : time = 0.155822
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;but gives you a nice self-describing file:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ncdump -h test.nc
netcdf test {
dimensions:
	X = 5000 ;
	Y = 5000 ;
variables:
	float Array(X, Y) ;
		Array:units = &quot;ergs&quot; ;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and file sizes about the same as raw binary:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ du -sh test.*
96M	test.dat
96M	test.nc
382M	test.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;the code follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-c&quot;&gt;#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;sys/time.h&amp;gt;
#include &amp;lt;netcdf.h&amp;gt;
#include &amp;lt;string.h&amp;gt;

void tick(struct timeval *t);
double tock(struct timeval *t);
void writenetcdffile(const char *filename, int n, float **data);

int main(int argc, char **argv) {

    if (argc &amp;lt; 2) {
        fprintf(stderr,&quot;Usage: %s n -- test write speeds of n x n array\n&quot;, argv[0]);
        exit(1);
    }

    int n = atoi(argv[1]);
    const int defaultn = 5000;
    if (n &amp;lt; 1 || n &amp;gt; 10000) {
        fprintf(stderr, &quot;Invalid n %s: using n = %d\n&quot;, argv[1], defaultn);
        n = defaultn;
    }

    float **data = malloc(n*sizeof(float *));
    float *p     = malloc(n*n*sizeof(float));
    for (int i=0; i&amp;lt;n; i++)
        data[i] = &amp;amp;(p[i*n]);

    for (int i=0; i&amp;lt;n; i++)
        for (int j=0; j&amp;lt;n; j++)
            data[i][j] = i*n+j;

    struct timeval timer;
    tick(&amp;amp;timer);
    FILE *txt = fopen(&quot;test.txt&quot;,&quot;w&quot;);
    for (int i=0; i&amp;lt;n; i++) {
        for (int j=0; j&amp;lt;n; j++)
            fprintf(txt, &quot;%f &quot;, data[i][j]);
        fprintf(txt, &quot;\n&quot;);
    }
    fclose(txt);
    printf(&quot;Text      : time = %lf\n&quot;, tock(&amp;amp;timer));

    tick(&amp;amp;timer);
    FILE *bin = fopen(&quot;test.dat&quot;,&quot;wb&quot;);
    fwrite(data[0], sizeof(float), n*n, bin);
    fclose(bin);
    printf(&quot;Raw Binary: time = %lf\n&quot;, tock(&amp;amp;timer));

    tick(&amp;amp;timer);
    writenetcdffile(&quot;test.nc&quot;, n, data);
    printf(&quot;NetCDF4   : time = %lf\n&quot;, tock(&amp;amp;timer));

    free(data[0]);
    free(data);
}


void tick(struct timeval *t) {
    gettimeofday(t, NULL);
}

/* returns time in seconds from now to time described by t */
double tock(struct timeval *t) {
    struct timeval now;
    gettimeofday(&amp;amp;now, NULL);
    return (double)(now.tv_sec - t-&amp;gt;tv_sec) + ((double)(now.tv_usec - t-&amp;gt;tv_usec)/1000000.);
}

void writenetcdffile(const char *filename, int n, float **data) {
    /* identifiers */
    int file_id, xdim_id, ydim_id;
    int data_id;

    /* sizes */
    int datadims[2];

    /* name of units for data */
    const char *dataunits=&quot;ergs&quot;;

    /* return status */
    int status;

        /* Create a new file - clobber anything existing */
    status = nc_create(filename, NC_CLOBBER, &amp;amp;file_id);
    /* netCDF routines return NC_NOERR on success */
    if (status != NC_NOERR) {
        fprintf(stderr,&quot;Could not open file %s\n&quot;, filename);
        return;
    }

    /* define the dimensions */
    nc_def_dim(file_id, &quot;X&quot;, n, &amp;amp;xdim_id);
    nc_def_dim(file_id, &quot;Y&quot;, n, &amp;amp;ydim_id);

    /* now that we've defined the dimensions, we can define variables on them */
    datadims[0] = xdim_id;  datadims[1] = ydim_id;
    nc_def_var(file_id, &quot;Array&quot;,  NC_FLOAT, 2, datadims, &amp;amp;data_id);

    /* assign units to the variables */
    nc_put_att_text(file_id, data_id, &quot;units&quot;, strlen(dataunits), dataunits);

    /* we are now done defining variables and their attributes */
    nc_enddef(file_id);

    /* Write out the data to the variables we've defined */
    nc_put_var_float(file_id, data_id, &amp;amp;(data[0][0]));

    nc_close(file_id);
    return;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(This post is crosslisted from a &lt;a href=&quot;http://stackoverflow.com/questions/24395686/best-way-to-write-a-large-array-to-file-in-fortran-text-vs-other/24396176#24396176&quot;&gt;StackOverflow Answer&lt;/a&gt;.)&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Hadoop For HPCers</title>
   <link href="https://hpc.social/2014/hadoop-for-hpcers/"/>
   <updated>2014-09-04T01:00:00-06:00</updated>
   <id>https://hpc.social/2014/hadoop-for-hpcers</id>
   <content type="html">&lt;p&gt;I and my colleague Mike Nolta have put together &lt;a href=&quot;https://github.com/ljdursi/hadoop-for-hpcers-tutorial&quot;&gt;a half-day tutorial on Hadoop&lt;/a&gt; - briefly covering HDFS, Map Reduce, &lt;a href=&quot;http://pig.apache.org&quot;&gt;Pig&lt;/a&gt;, and Spark - for an HPC audience, and put the materials on &lt;a href=&quot;https://github.com/ljdursi/hadoop-for-hpcers-tutorial&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://hadoop.apache.org&quot;&gt;Hadoop&lt;/a&gt; ecosystem of tools continues to rapidly grow, and now includes tools like &lt;a href=&quot;https://spark.apache.org&quot;&gt;Spark&lt;/a&gt; and &lt;a href=&quot;http://flink.incubator.apache.org&quot;&gt;Flink&lt;/a&gt; that are very good for iterative numerical computation - either simulation or data analysis.   These tools, and the underlying technologies, are (or should be) of real interest to the HPC community, but most materials are written for audiences with web application or maybe machine-learning backgrounds, which makes it harder for an HPC audience to see how they can be useful to them and how they might be applied.&lt;/p&gt;

&lt;p&gt;Most of the source code is Python.  Included on git hub are all sources for the examples, a vagrantfile for a VM to run the software on your laptop, and the presentation in &lt;a href=&quot;https://github.com/ljdursi/hadoop-for-hpcers-tutorial/blob/master/presentation/presentation.md&quot;&gt;Markdown&lt;/a&gt; and &lt;a href=&quot;https://github.com/ljdursi/hadoop-for-hpcers-tutorial/blob/master/presentation/keynote-presentation.pdf?raw=true&quot;&gt;PDF&lt;/a&gt;.  Feel free to fork, send pull requests, or use the materials as you see fit.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Exascale in perspective- RSC's 1.2 petaflop rack</title>
   <link href="https://hpc.social/2014/exascale-in-perspective-rsc-s-1-2-petaflop-rack/"/>
   <updated>2014-06-29T21:31:00-06:00</updated>
   <id>https://hpc.social/2014/exascale-in-perspective-rsc-s-1-2-petaflop-rack</id>
   <content type="html">&lt;div&gt;Russian supercomputing manufacturer &lt;a href=&quot;http://insidehpc.com/2014/06/29/rsc-announces-record-compute-density-xeon-phi-isc14/&quot;&gt;RSC generated some buzz at ISC'14 last week when they showed their 1.2 PF-per-rack Xeon Phi-based platform&lt;/a&gt;. &amp;nbsp;I was aware of this system from when they &lt;a href=&quot;http://insidehpc.com/2014/04/02/rsc-petastream-delivers-1-2-pflops-per-rack-xeon-phi/&quot;&gt;first announced it a few months prior&lt;/a&gt;, and I referenced it in a piece of a blog post I was writing about the scarier aspects of exascale computing. &amp;nbsp;Given my impending career change though, it is unclear that I will have the time to ever finish that post before it becomes outdated. &amp;nbsp;Since RSC is back in the spotlight though, I thought I'd post the piece I wrote up to illustrate how wacky this 1.2 PF rack really is in terms of power consumption. &amp;nbsp;Power consumption, of course, is the limiting factor standing between today and the era of exascale computing.&lt;br /&gt;&lt;br /&gt;So, to put a 400 kW, 1.2 PF rack into perspective, here is that piece:&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;&lt;hr /&gt;&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;h2&gt;The Importance of Energy Efficiency&lt;/h2&gt;Up through the petascale era in which we currently live, raw performance of high-performance components--processors, RAM, and interconnect--were what limited the ultimate performance of a given high-end machine. &amp;nbsp;The first petaflop machine, Los Alamos' Roadrunner, derived most of its FLOPs from &lt;a href=&quot;http://www.redbooks.ibm.com/redpapers/pdfs/redp4477.pdf&quot;&gt;high-speed PowerXCell 8i processors pushing 3.2 GHz per core&lt;/a&gt;. &amp;nbsp;Similarly, the first 10 PF supercomputer, RIKEN's K computer, derived its performance from &lt;a href=&quot;http://www.fujitsu.com/downloads/TC/isc12/k-computer-isc12.pdf&quot;&gt;its sheer size of 864 cabinets&lt;/a&gt;. &amp;nbsp;Although I don't mean to diminish the work done by the engineers that actually got these systems to deliver this performance, the petascale era really was made possible by making really big systems out of really fast processors.&lt;br /&gt;&lt;br /&gt;By contrast, Exascale represents the first milestone where the limitation does &lt;i&gt;not&lt;/i&gt; lie in making these high-performance components faster; rather, performance is limited by the amount of electricity that can be physically delivered to a processor and the amount of heat that can be extracted from it. &amp;nbsp;This limitation is what has given rise to these massively parallel processors that eschew a few fast cores for a larger number of low-powered ones. &amp;nbsp;By keeping clock speeds low and densely packing many (dozens or hundreds) of compute cores on a single silicon die, these massively parallel processors are now realizing power efficiencies (flops per watt) that are an order of magnitude higher than what traditional CPUs can deliver.&lt;br /&gt;&lt;br /&gt;The closest technology on the market that will probably resemble the future's exaflop machines are based on accelerators--either NVIDIA GPUs or Intel's MICs. &amp;nbsp;The goal will be to jam as many of these massively parallel processors into as small a space and with as tight of an integration as possible. &amp;nbsp;Recognizing this trend, NERSC has opted to build what I would call the first &quot;pre-exascale&quot; machine in its &lt;a href=&quot;https://www.nersc.gov/users/computational-systems/nersc-8-system-cori/&quot;&gt;NERSC-8 procurement&lt;/a&gt;&amp;nbsp;which will feature a homogeneous system of manycore processors.&lt;br /&gt;&lt;br /&gt;However, such pre-exascale hardware doesn't actually exist yet, and NERSC-8 won't appear until 2016. &amp;nbsp;What does exist, though, is a product by Russia's &lt;a href=&quot;http://rscgroup.ru/&quot;&gt;RSC Group&lt;/a&gt; called PetaStream: a rack packed with 1024 current-generation Xeon Phi (Knight's Corner) coprocessors that has a peak performance of 1.2 PF/rack. &amp;nbsp;While this sounds impressive, it also highlights the principal challenge of exascale computing: power consumption. &amp;nbsp;One rack of RSC PetaStream is rated for 400 kW, delivering 3 GFLOPs/watt peak. &amp;nbsp;Let's put this into perspective.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Kilowatts, megawatts, and gigawatts in perspective&lt;/h2&gt;During a recent upgrade to our data center infrastructure, three &lt;a href=&quot;http://multiquip.com/multiquip/DCA220SSCU4i.htm&quot;&gt;MQ DCA220SS-series diesel generators&lt;/a&gt; were brought in for the critical systems. &amp;nbsp;Each is capable of producing 220 kVA according to the spec sheets.&lt;/div&gt;
&lt;div&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;http://2.bp.blogspot.com/-9fyvZaqnAY8/U4YzYjZPpQI/AAAAAAAAKcw/4p4v0RKhoTI/s1600/2014-02-14+17.09.29.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;320&quot; src=&quot;http://2.bp.blogspot.com/-9fyvZaqnAY8/U4YzYjZPpQI/AAAAAAAAKcw/4p4v0RKhoTI/s1600/2014-02-14+17.09.29.jpg&quot; width=&quot;239&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Three 220 kVA diesel generators plugged in during a PM at SDSC&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;It would take three of these diesel generators to power a single rack of RSC's PetaStream. &amp;nbsp;Of course, these backup diesel generators aren't a very efficient way of generating commercial power, so this example is a bit skewed.&lt;br /&gt;&lt;br /&gt;Let's look at something that is used to generate large quantities of commercial power instead. &amp;nbsp;A &lt;a href=&quot;http://www.ge-energy.com/products_and_services/products/wind_turbines/ge_1.5_77_wind_turbine.jsp&quot;&gt;GE 1.5-77 wind turbine&lt;/a&gt;, which is GE's most popular model, is advertised as delivering 1.5 megawatts at wind speeds above 15 miles per hour.&lt;br /&gt;&lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;http://4.bp.blogspot.com/-cQNM7XKbFmM/U4Y3JL8-YAI/AAAAAAAAKdA/LVO_ohhL9bc/s1600/20090821_turbines_blades1.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;320&quot; src=&quot;http://4.bp.blogspot.com/-cQNM7XKbFmM/U4Y3JL8-YAI/AAAAAAAAKdA/LVO_ohhL9bc/s1600/20090821_turbines_blades1.jpg&quot; width=&quot;212&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;GE 1.5 MW wind turbine. &amp;nbsp; Source: &lt;a href=&quot;http://www.nrel.gov/news/features/feature_detail.cfm/feature_id=1717&quot;&gt;NREL&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;Doing the math, this means that the above pictured turbine would be able to power only three racks of RSC PetaStream on a breezy day.&lt;br /&gt;&lt;br /&gt;To create a supercomputer with a peak capability of an exaflop using RSC's platform, you'd need over 800 racks of PetaStream and over 300 MW of power to turn it all on. &amp;nbsp;That's over 200 of the above GE wind turbines and enough electrity to power about 290,000 homes in the U.S. &amp;nbsp;Wind farms of this size do exist; for example,&lt;br /&gt;&lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;http://1.bp.blogspot.com/-1ZFeqhmyzLQ/U7By0J6HxhI/AAAAAAAAKls/v24Y5NpFG7U/s1600/1024px-WindTurbinesWallaWallaRiverWashington.JPG&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;243&quot; src=&quot;http://1.bp.blogspot.com/-1ZFeqhmyzLQ/U7By0J6HxhI/AAAAAAAAKls/v24Y5NpFG7U/s1600/1024px-WindTurbinesWallaWallaRiverWashington.JPG&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;300 MW Stateline Wind Farm. &amp;nbsp;Source: &lt;a href=&quot;http://en.wikipedia.org/wiki/File:WindTurbinesWallaWallaRiverWashington.JPG&quot;&gt;Wikimedia Commons&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;the &lt;a href=&quot;http://en.wikipedia.org/wiki/Stateline_Wind_Farm&quot;&gt;Stateline Wind Farm&lt;/a&gt;, which was built on the border between Oregon and Washington, has a capacity of about 300 MW. &amp;nbsp;Of course, wind farms of this capacity cannot be built in any old place.&lt;br /&gt;&lt;br /&gt;Commercial nuclear power plants can be built in a variety of places though, and they typically generate on the order of 1 gigawatt (GW) of power per reactor. &amp;nbsp;In my home state of New Jersey, the &lt;a href=&quot;http://www.pseg.com/family/power/nuclear/index.jsp&quot;&gt;Hope Creek Nuclear Generating Station&lt;/a&gt; has a single reactor that was built to deliver about 1.2 GW of power:&lt;br /&gt;&lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;http://1.bp.blogspot.com/-x6ewW2IogbY/U7B1B4muwpI/AAAAAAAAKl4/t1qY4Q8irZE/s1600/469px-Hope_creek_NPP.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;400&quot; src=&quot;http://1.bp.blogspot.com/-x6ewW2IogbY/U7B1B4muwpI/AAAAAAAAKl4/t1qY4Q8irZE/s1600/469px-Hope_creek_NPP.jpg&quot; width=&quot;312&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;1.2 GW Hope Creek nuclear power station. &amp;nbsp;The actual reactor is housed in the concrete cylinder to the bottom left. &amp;nbsp;Courtesy of the Nuclear Regulatory Commission.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;This is enough to power almost 4 exaflops of PetaStream. &amp;nbsp;Of course, building a nuclear reactor for every exaflop supercomputer would be extremely costly, given the multi-billion dollar cost of building reactors like this. &amp;nbsp;Clearly, the energy efficiency (flops/watt) of computing technology needs to improve substantially before we can arrive at the exascale era.&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Perspectives on the Current State of Data-Intensive Scientific Computing</title>
   <link href="https://hpc.social/2014/perspectives-on-the-current-state-of-data-intensive-scientific-computing/"/>
   <updated>2014-06-24T19:08:00-06:00</updated>
   <id>https://hpc.social/2014/perspectives-on-the-current-state-of-data-intensive-scientific-computing</id>
   <content type="html">&lt;p&gt;I recently had the benefit of being invited to attend two workshops in Oakland, CA, hosted by the U.S. Department of Energy (DOE), that shared the common theme of emerging trends in data-intensive computing: the &lt;a href=&quot;http://www.nersc.gov/users/training/nersc-training-events/joint-data-intensive/&quot;&gt;Joint User Forum on Data-Intensive Computing&lt;/a&gt; and the &lt;a href=&quot;http://www.nersc.gov/research-and-development/HPCOR/&quot;&gt;High Performance Computing Operational Review&lt;/a&gt;.  My current employment requires that I stay abreast of all topics in data-intensive scientific computing (I wish there was &lt;a href=&quot;https://twitter.com/NicoleHemsoth/status/433484762787741696&quot;&gt;an acronym to abbreviate this&lt;/a&gt;…DISC perhaps?) so I didn’t go in with the expectation of being exposed to a world of new information.  As it turned out though, I did gain a very insightful perspective on how data-intensive scientific computing (DISC), and I daresay &lt;i&gt;Big Data&lt;/i&gt;, is seen from the people who operate some of the &lt;a href=&quot;https://asc.llnl.gov/computing_resources/sequoia/&quot;&gt;world’s&lt;/a&gt; &lt;a href=&quot;https://www.olcf.ornl.gov/titan/&quot;&gt;largest&lt;/a&gt; &lt;a href=&quot;http://www.lanl.gov/asc/trinity-highlight.php&quot;&gt;supercomputers&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;The DOE perspective is surprisingly realistic, application-oriented, and tightly integrated with high-performance computing.  There was the obligatory discussion of Hadoop and how it may be wedged into machines at &lt;a href=&quot;https://github.com/chu11/magpie&quot;&gt;LLNL with Magpie&lt;/a&gt;, &lt;a href=&quot;http://www.nersc.gov/assets/Uploads/T12-Experience-with-Data-Parallel-Frameworks.pdf&quot;&gt;ORNL&lt;/a&gt; with &lt;a href=&quot;https://github.com/jhorey/SpotHadoop&quot;&gt;Spot Hadoop&lt;/a&gt;, and &lt;a href=&quot;http://www.nersc.gov/assets/Uploads/T10-Joint-Facilities-User-Forum.pdf&quot;&gt;SDSC&lt;/a&gt; with &lt;a href=&quot;https://github.com/glennklockwood/myhadoop&quot;&gt;myHadoop&lt;/a&gt;, of course, and there was also some discussion of real production use of Hadoop on &lt;i&gt;bona fide&lt;/i&gt; Hadoop clusters at some of the DOE labs.  However, Hadoop played only a minor role in the grand scheme of the two meetings for &lt;a href=&quot;http://glennklockwood.blogspot.com/2014/05/hadoops-uncomfortable-fit-in-hpc.html&quot;&gt;all of the reasons I’ve outlined previously&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;Rather, these two meetings had three major themes that crept into all aspects of the discussion:&lt;br /&gt;&amp;lt;ol&amp;gt;&amp;lt;li&amp;gt;Scientific workflows&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Burst buffers&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Data curation&amp;lt;/li&amp;gt;&amp;lt;/ol&amp;gt;&amp;lt;div&amp;gt;I found this to be a very interesting trend, as #1 and #2 (workflows and burst buffers) aren’t topics I’d heard come up at any other DISC workshops, forums, or meetings I’ve attended.  The connection between DISC and workflows wasn’t immediately evident to me, and burst buffers are a unique aspect of cyberinfrastructure that have only been thrust into the spotlight with the &lt;a href=&quot;https://www.nersc.gov/users/computational-systems/nersc-8-system-cori/nersc-8-procurement/trinity-nersc-8-rfp/&quot;&gt;NERSC-8/LANL Trinity RFP last fall&lt;/a&gt;.  However, all three of these topics will become central to both data-intensive scientific computing and, by virtue of their ability to &lt;i&gt;produce&lt;/i&gt; data, exascale supercomputers.&amp;lt;/div&amp;gt;&lt;/p&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h2&gt;Scientific workflows&lt;/h2&gt;
&lt;div&gt;Workflows are one of those aspects of scientific computing that have been easy to dismiss as the toys of computer scientists because traditional problems in high-performance computing have typically been quite monolithic in how they are run. &amp;nbsp;&lt;a href=&quot;https://kepler-project.org/&quot;&gt;SDSC's own Kepler&lt;/a&gt; and &lt;a href=&quot;http://pegasus.isi.edu/&quot;&gt;USC's Pegasus&lt;/a&gt; systems are perhaps the most well-known and highly engineered workflow management systems, and I have to confess that when I'd first heard of them a few years ago, I thought they seemed like a very complicated way to do very simple tasks.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;As it turns out though, both data-intensive scientific computing and exascale computing (by virtue of the output size of exaflop calculations) tend to follow patterns that look an awful lot like map/reduce at a very abstract level. &amp;nbsp;This is a result of the fact that most data-intensive problems are not processing giant monoliths of tightly coupled and inter-related data; rather, they are working on large collections of generally independent data. &amp;nbsp;Consider the &lt;a href=&quot;http://www.sdsc.edu/Events/ipp_webinars/large_scale_genomics.pdf&quot;&gt;recent talk I gave about a large-scale genomic study on which I consulted&lt;/a&gt;; the general data processing flow was&lt;/div&gt;
&lt;div&gt;&lt;ol&gt;&lt;li&gt;Receive 2,190 input files, 20 GB each, from a data-generating instrument&lt;/li&gt;&lt;li&gt;Do some processing on each input file&lt;/li&gt;&lt;li&gt;Combine groups of five input files into 438 files, each 100 GB in size&lt;/li&gt;&lt;li&gt;Do more processing&amp;nbsp;&lt;/li&gt;&lt;li&gt;Combine 438 files into 25 overlapping groups to get 100 files, each 2.5 GB in size&lt;/li&gt;&lt;li&gt;Do more processing&lt;/li&gt;&lt;li&gt;Combine 100 files into a single 250 GB file&lt;/li&gt;&lt;li&gt;Perform statistical analysis on this 250 GB file for scientific insight&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;
&lt;div&gt;The natural &lt;i&gt;data-parallelism&lt;/i&gt; inherent from the data-generating instrument means that any collective insight to be gleaned from this data requires some sort of mapping and reduction, and the process of managing this large volume of distributed data is where scientific workflows become a necessary part of data-intensive scientific computing. &amp;nbsp;Managing terabytes or petabytes of data distributed across thousands or millions of logical records (whether they be files on a file system, rows in a database, or whatever else) very rapidly becomes a problem that nobody will want to do by hand. &amp;nbsp;Hadoop/HDFS delivers an automated framework for managing these sorts of workflows if you don't mind rewriting all of your processing steps against the Hadoop API and building out HDFS infrastructure, but if this is not the case, alternate workflow management systems begin to look very appealing.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;The core debate was not whether or not workflow management systems were a necessary component in DISC; rather, I observed two salient, open questions:&lt;/div&gt;
&lt;div&gt;&lt;ol&gt;&lt;li&gt;The systems in use at DOE (notably &lt;a href=&quot;https://pythonhosted.org/FireWorks/&quot;&gt;Fireworks&lt;/a&gt; and &lt;a href=&quot;https://bitbucket.org/berkeleylab/qdo&quot;&gt;qdo&lt;/a&gt;) are primarily used to work around deficiencies in current HPC schedulers (e.g., Moab and SLURM) in that they cannot handle scheduling hundreds of thousands of tiny jobs concurrently. &amp;nbsp;Thus, &lt;b&gt;should these workflow managers be integrated into the scheduler&lt;/b&gt; to address these shortcomings at their source?&lt;/li&gt;&lt;li&gt;&lt;b&gt;How do we stop every user from creating his or her own workflow manager scripts&lt;/b&gt; and adopt an existing solution instead? &amp;nbsp;Should one workflow manager rule them all, or should a Darwinian approach be taken towards the current diverse landscape of existing software?&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;
&lt;div&gt;Question #1 is a highly technical question that has several dimensions; ultimately though, it's not clear to me that there is enough incentive for resource manager and scheduler developers to really dig into this problem. &amp;nbsp;They haven't done this yet, and I can only assume that this is a result of the perceived domain-specificity and complexity of each workflow. &amp;nbsp;In reality, a large number of workflows can be accommodated by two simple features: support for directed acyclic graphs (DAGs) of tasks and support for lightweight, fault-tolerant task scheduling within a pool of reserved resources. &amp;nbsp;Whether or not anyone will rise to the challenge of incorporating this &lt;i&gt;in a usable way&lt;/i&gt; is an open question, but there certainly is a need for this in the emerging realm of DISC.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;Question #2 is more interesting to me since this problem of multiple people cooking up different but equivalent solutions to the same problems is pervasive throughout computational and computer science. This is in large part due to the fatal assumption held by many computer scientists that good software can be simply &quot;thrown over the fence&quot; to scientists and it will be adopted. &amp;nbsp;This has never worked; rather, the majority of widely adopted software technologies in HPC have been a result of the standardization of a landscape of similar but non-standard tools. &amp;nbsp;This is something I touched on &lt;a href=&quot;http://glennklockwood.blogspot.com/2014/05/hadoops-uncomfortable-fit-in-hpc.html&quot;&gt;in a previous post&lt;/a&gt;&amp;nbsp;when outlining the history of MPI and OpenMP's successes.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;I don't think the menagerie of workflows' developers are ready to settle on a standard, as the field is not mature enough to have a holistic understanding of all of the issues that workflows need to solve. &amp;nbsp;Despite the numerous presentations and discussions of various workflow solutions being used across DOE's user facilities, my presentation was the only one that considered optimizing workflow execution for the underlying hardware. &amp;nbsp;Given that the target audience of these talks were users of high-performance computing, the lack of consideration given to the performance aspects of workflow optimization is a testament to this immaturity.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h2 id=&quot;bb&quot;&gt;Burst buffers&lt;/h2&gt;
&lt;div&gt;For those who haven't been following the details of one of DOE's more recent procurement rounds, the &lt;a href=&quot;https://www.nersc.gov/users/computational-systems/nersc-8-system-cori/nersc-8-procurement/trinity-nersc-8-rfp/&quot;&gt;NERSC-8 and Trinity request for proposals&lt;/a&gt; (RFP) explicitly required that all vendor proposals include a &lt;i&gt;burst buffer&lt;/i&gt; to address the capability of multi-petaflop simulations to dump tremendous amounts of data in very short order. &amp;nbsp;The target use case is for petascale checkpoint-restart, where the memory of thousands of nodes (hundreds of terabytes of data) needs to be flushed to disk in an amount of time that doesn't dominate the overall execution time of the calculation.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;The concept of what a &lt;i&gt;burst buffer&lt;/i&gt; is remains poorly defined. &amp;nbsp;I got the sense that there are two outstanding definitions:&lt;/div&gt;
&lt;div&gt;&lt;ul&gt;&lt;li&gt;The &lt;i&gt;NERSC burst buffer&lt;/i&gt; is something more tightly integrated on the compute side of the system and may be a resource that can be allocated on a per-job basis&lt;/li&gt;&lt;li&gt;The &lt;i&gt;Argonne burst buffer&lt;/i&gt; is something more tightly integrated on the storage side of the system and acts in a fashion that is largely transparent to the user. &amp;nbsp;This sounded a lot like &lt;a href=&quot;http://insidehpc.com/2014/05/08/video-efficient-distributed-burst-buffer-system-lustre/&quot;&gt;the burst buffer support being explored for Lustre&lt;/a&gt;.&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;In addition, &lt;a href=&quot;http://www.hpcwire.com/2014/05/01/burst-buffers-flash-exascale-potential/&quot;&gt;Los Alamos National Labs (LANL) is exploring burst buffers for the Trinity procurement&lt;/a&gt;, and it wasn't clear to me if they had chosen a definition or if they are exploring all angles. &amp;nbsp;One commonality is that DOE is going full-steam ahead on providing this burst buffer capability in some form or another, and solid-state storage is going to be a central enabling component.&lt;/div&gt;
&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;Personally, I find the NERSC burst buffer concept a lot more interesting since it provides a more general purpose flash-based resource that can be used in novel ways. &amp;nbsp;For example, emerging software-defined storage platforms like &lt;a href=&quot;http://www.emc.com/cloud/vipr/index.htm&quot;&gt;EMC's Vipr&lt;/a&gt; can potentially provide very fine-grained access to flash as-needed to make better overall use of the underlying SSDs in HPC environments serving a broad user base (e.g., NERSC and the NSF centers). &amp;nbsp;Complementing these software technologies are emerging hardware technologies like DSSD's D5 product which will be exposing flash to compute systems in innovative ways at hardware, interconnect, and software levels.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;Of course, the fact that &lt;a href=&quot;http://www.sdsc.edu/supercomputing/gordon/&quot;&gt;my favorite supercomputer&lt;/a&gt; provides dynamically allocatable SSDs in a fashion not far removed from these NERSC burst buffers probably biases me, but we've demonstrated unique DISC successes enabled by our ability to pile tons of flash on to single compute nodes. &amp;nbsp;This isn't to say that the Argonne burst buffer isn't without merit; given that the Argonne Leadership Computing Facility (ALCF) caters to &lt;i&gt;capability&lt;/i&gt; jobs rather than &lt;i&gt;capacity&lt;/i&gt; jobs, their user base is better served by providing a uniform, transparent burst I/O capability across all nodes. &amp;nbsp;The NERSC burst buffer, by comparison, is a lot less transparent and will probably be much more susceptible to user disuse or misuse. &amp;nbsp;I suspect that when the dust settles, both takes on the burst buffer concept will make their way into production use.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;A lot of the talk and technologies surrounding burst buffers are shrouded in NNSA secrecy or vendor non-disclosures, so I'm not sure what more there is to be said. &amp;nbsp;However, the good folks at&amp;nbsp;&lt;a href=&quot;http://www.hpcwire.com/2014/05/01/burst-buffers-flash-exascale-potential/&quot;&gt;HPCwire ran an insightful article on burst buffers&lt;/a&gt; after the NERSC-8 announcement for those who are interested in more detail.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h2&gt;Data curation&lt;/h2&gt;
&lt;div&gt;The final theme that bubbled just beneath the surface of the DOE workshops was the idea that we are coming upon an era where scientists can no longer save all their data from all their calculations in perpetuity. &amp;nbsp;Rather, someone will have to become the curator of the scientific data being generated by computations and figure out what is and is not worth keeping, and how or where that data should be stored and managed. &amp;nbsp;This concept of selectively retaining user data manifested in a variety of discussions ranging from in-place data sharing and publication with &lt;a href=&quot;https://www.globus.org/researchers/plus-plans&quot;&gt;Globus Plus&lt;/a&gt; and &lt;a href=&quot;https://fasterdata.es.net/science-dmz/&quot;&gt;science DMZs&lt;/a&gt; to &lt;a href=&quot;https://www.alcf.anl.gov/user-guides/using-hpss&quot;&gt;transparently managing online data volumes with hierarchical storage management&lt;/a&gt;&amp;nbsp;(HSM). &amp;nbsp;However, the common idea was that scientists are going to have to start coming to grips with data management themselves, as facilities will soon be unable to cope with the entirety of their users' data.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;This was a particularly interesting problem to me because it very closely echoed the sentiments that came about from &lt;a href=&quot;http://leveragebigdata.com/&quot;&gt;Datanami's recent LeverageBIGDATA&lt;/a&gt; event which had a much more industry-minded audience. &amp;nbsp;The general consensus is that several fields are far ahead of the pack in terms of addressing this issue; the high-energy physics community has been filtering data at its genesis (e.g., &lt;a href=&quot;http://lhcb-public.web.cern.ch/lhcb-public/en/Data%20Collection/Triggers-en.html&quot;&gt;ignoring the data from uninteresting collision events&lt;/a&gt;) for years now, and enterprises seem comfortable with retaining marketing data for only as long as it is useful. &amp;nbsp;By comparison, NERSC's tape archive has not discarded user data since its inception several decades ago; each new tape system simply repacks the previous generation's tape to roll all old data forward.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;All of the proposed solutions for this problem revolve around metadata. &amp;nbsp;The reality is that not all user data has equal importance, and there is a need to provide a mechanism for users (or their applications) to describe this fact. &amp;nbsp;For example, the principal use case for the aforementioned burst buffers is to store massive checkpoint-restart files; while these checkpoints are important to retain &lt;i&gt;while&lt;/i&gt; a calculation is running, they have limited value &lt;i&gt;after&lt;/i&gt; the calculation has completed. &amp;nbsp;Rather than rely on a user to manually recognize that these checkpoints can be deleted, the hope is that metadata attributes can be attached to these checkpoint files to indicate that they are not critical data that must be retained forever for automated curation systems to understand.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;The exact way this metadata would be used to manage space on a file system remains poorly defined. &amp;nbsp;A few examples of exactly how metadata can be used to manage data volume in data-intensive scientific computing environments include&lt;/div&gt;
&lt;div&gt;&lt;ul&gt;&lt;li&gt;tagging certain files or directories as permanent or ephemeral, signaling that the file system can purge certain files whenever a cleanup is initiated;&lt;/li&gt;&lt;li&gt;tagging certain files with a set expiration date, either as an option or by default. &amp;nbsp;When a file ages beyond a certain point, it would be deleted;&lt;/li&gt;&lt;li&gt;attributing a sliding scale of &quot;importance&quot; to each file, so that files of low importance can be transparently migrated to tape via HSM&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;Some of these concepts are already implemented, but the ability for users &lt;i&gt;and&lt;/i&gt; applications to attach extensible metadata to files in a file system-agnostic way does not yet exist. &amp;nbsp;I think this is a significant gap in technology that will need to be filled in very short order as pre-exascale machines begin to demonstrate the ability to generate tremendous I/O loads. &amp;nbsp;Frankly, I'm surprised this issue hasn't been solved in a broadly deployable way yet.&lt;/div&gt;
&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;The good news here is that the problem of curating digital data is not new; it is simply new to high-performance computing. &amp;nbsp;In the spirit of doing things the right way, DOE invited &lt;a href=&quot;https://twitter.com/deemagnoni&quot;&gt;the director of LANL's Research Library&lt;/a&gt; to attend the workshops, and she provided valuable insights into how methods of digital data curation may be applied to these emerging challenges in data-intensive scientific computing.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h2&gt;Final Thoughts&lt;/h2&gt;
&lt;div&gt;The products of the working groups' conventions at the HPC Operational Review are being assembled into a report to be delivered to DOE's Office of Science, and it should be available online at the &lt;a href=&quot;http://www.nersc.gov/research-and-development/HPCOR/&quot;&gt;HPCOR 2014 website&lt;/a&gt; as well as the &lt;a href=&quot;http://www.osti.gov/home/&quot;&gt;usual DOE document repository&lt;/a&gt;&amp;nbsp;in a few months. &amp;nbsp;Hopefully it will reflect what I feel was the essence of the workshop, but at any rate, it should contain a nice perspective on how we can expect the HPC community to address the new demands emerging from data-intensive scientific computing (DISC) community.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;In the context of high-performance computing,&amp;nbsp;&lt;/div&gt;
&lt;div&gt;&lt;ul&gt;&lt;li&gt;Workflow management systems will continue to gain importance as data sets become larger, more parallel, and more unwieldy.&lt;/li&gt;&lt;li&gt;Burst buffers, in one form or another, will become the hardware solution to the fact that all exascale simulations will become data-intensive problems.&lt;/li&gt;&lt;li&gt;Data curation frameworks are the final piece of the puzzle and will provide the manageability of data at rest.&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;
&lt;div&gt;None of these three legs are fully developed, and this is simply an indication of data-intensive scientific computing's immaturity relative to more traditional high-performance computing: &amp;nbsp;&lt;/div&gt;
&lt;div&gt;&lt;ul&gt;&lt;li&gt;Workflows need to converge on some sort of standardized API or feature set in order to provide the incentive to users to abandon their one-off solutions.&lt;/li&gt;&lt;li&gt;Burst buffer technology has diverged into two solutions centered at either the compute or storage side of a DISC platform; both serve different workloads, and the underlying hardware and software configurations remain unfinished.&lt;/li&gt;&lt;li&gt;Effective data curation requires a metadata management system that will allow both users and their applications to identify the importance of data to automate sensible data retention policy enforcement and HSM.&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;Of course, I could be way off in terms of what I took away from these meetings seeing as how I don't really know what I'm talking about. &amp;nbsp;Either way, it was a real treat to be invited out to hang out with the DOE folks for a week; I got to meet some of my personal supercomputing heroes, share war stories, and make some new pals. &lt;br /&gt;&lt;br /&gt;I also got to spend eight days getting to know the Bay Area. &amp;nbsp;So as not to leave this post entirely without a picture,&lt;/div&gt;
&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;http://4.bp.blogspot.com/-FIgACJFuqu8/U6kbTBKwNbI/AAAAAAAAKlM/L1wZAaZrBn0/s1600/10455579_10152060455017282_6816950694141486686_n.jpg&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;295&quot; src=&quot;http://4.bp.blogspot.com/-FIgACJFuqu8/U6kbTBKwNbI/AAAAAAAAKlM/L1wZAaZrBn0/s1600/10455579_10152060455017282_6816950694141486686_n.jpg&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;I also learned that I have a weird fascination with streetcars. &amp;nbsp;I'm glad I was introduced to supercomputers first.&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Spark on Supercomputers- A Few Notes</title>
   <link href="https://hpc.social/2014/spark-on-supercomputers-a-few-notes/"/>
   <updated>2014-06-08T03:34:00-06:00</updated>
   <id>https://hpc.social/2014/spark-on-supercomputers-a-few-notes</id>
   <content type="html">&lt;p&gt;I’ve been working with Apache Spark quite a bit lately in an effort to bring it into the fold as a viable tool for solving some of the data-intensive problems encountered in supercomputing.  I’ve already added support for &lt;a href=&quot;https://github.com/glennklockwood/myhadoop/tree/spark&quot;&gt;provisioning Spark clusters to a branch of the myHadoop framework&lt;/a&gt; I maintain so that Slurm, Torque, and SGE users can begin playing with it, and as a result of these efforts, I’ve discovering a number of interesting issues with Spark running on traditional supercomputers.&lt;br /&gt;&lt;br /&gt;At this point in time, Spark is very rough around the edges.  The core implementation of resilient distributed datasets are all there and work wonderfully, but I’ve found that it doesn’t take long to start discovering bugs and half-implemented features that can get very confusing very quickly.  Perhaps half of the problems I’ve faced are the result of the fact that I have been trying to run Spark in non-traditional ways (for example, over hosts’ TCP over InfiniBand interfaces and with non-default config directories), and although the documentation claims to support all of the features necessary to make this possible, the reality is a bit different.&lt;br /&gt;&lt;br /&gt;What follows are just some incoherent notes I’ve taken while porting Spark to the myHadoop framework.  Spark is rapidly developing and it is constantly improving, so I hope this post becomes outdated as the Spark developers make the framework more robust.&lt;br /&gt;&lt;br /&gt;&amp;lt;h2&amp;gt;Control Script Problems&amp;lt;/h2&amp;gt;Hadoop and Spark both ship with “control scripts” or “&lt;a href=&quot;http://spark.apache.org/docs/0.9.1/spark-standalone.html#cluster-launch-scripts&quot;&gt;cluster launch scripts&lt;/a&gt;” that facilitate the starting and stopping of the entire cluster of daemons.  At the highest level, this includes start-all.sh and stop-all.sh, which make calls to start-dfs.sh and start-yarn.sh (in Hadoop) and start-master.sh and start-slaves.sh.  In Hadoop, these scripts work wonderfully, but Spark’s implementation of these control scripts is still quite immature because they carry implicit assumptions about users’ Spark configurations.&lt;br /&gt;&lt;br /&gt;Like Hadoop, Spark supports a spark-env.sh file (located in $SPARK_CONF_DIR) which defines environment variables for all of the remote Spark workers that are spawned across the cluster.  This file is an ideal place to put the following environment variable definitions:&lt;br /&gt;&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;SPARK_MASTER_IP - the default value for this is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hostname&lt;/code&gt; which is generally not a great default on most clusters.  On Rocks, we append “.ibnet” to the hostname to get Spark to operate over the InfiniBand fabric.&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;SPARK_LOCAL_IP - again, ensure that this is set up to use the correct interface on the cluster.  We append .ibnet on Rocks.&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;SPARK_HOME, SPARK_PREFIX, and SPARK_CONF_DIR should also be defined here since spark-env.sh will usually override the variables defined by spark-config.sh (see below)&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;$SPARK_HOME/sbin/spark-config.sh is where much of the Spark control scripts’ “intelligence” comes from as far as defining the environment variables that Spark needs to launch.  In particular, spark-config.sh defines the following variables &lt;i&gt;before&lt;/i&gt; reading spark-env.sh:&lt;br /&gt;&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;SPARK_PREFIX&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;SPARK_HOME&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;SPARK_CONF_DIR&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;The problem is that &lt;b&gt;spark-config.sh will stomp all over anything the user defines&lt;/b&gt; for the above variables, and since spark-config.sh is called from within all of the Spark control scripts (both evoked by the user and evoked by sub-processes on remote hosts during the daemon spawning process), trying to get Spark to use non-default values for SPARK_CONF_DIR (e.g., exactly what myHadoop does) gets to be tedious. &lt;br /&gt;&lt;br /&gt;The Spark developers tried to work around this by having the control scripts call spark-env.sh after spark-config.sh, meaning you should be able to define your own SPARK_CONF_DIR in spark-env.sh.  Unfortunately, this mechanism of calling spark-env.sh after spark-config.sh appears as&lt;br /&gt;&lt;br /&gt;&amp;lt;pre&amp;gt;. “$sbin/spark-config.sh”&lt;br /&gt;&lt;br /&gt;if [ -f “${SPARK_CONF_DIR}/spark-env.sh” ]; then&lt;br /&gt;  . “${SPARK_CONF_DIR}/spark-env.sh”&lt;br /&gt;fi&lt;br /&gt;&amp;lt;/pre&amp;gt;&lt;br /&gt;That is, spark-config.sh will stomp all over any user-specified SPARK_CONF_DIR, and then use the SPARK_CONF_DIR from spark-config.sh to look for spark-env.sh.  Thus, there is no actual way to get the Spark control scripts (as of version 0.9) to honor the user-specified SPARK_CONF_DIR.  It looks like the latest commits to Spark have started to address this, but a cursory glance over the newest control scripts suggests that this remains broken.&lt;br /&gt;&lt;br /&gt;Anyway, as a result of this, myHadoop’s Spark integration eschews the Spark control scripts and handles spawning the daemons more directly using the &lt;a href=&quot;http://spark.apache.org/docs/0.9.1/spark-standalone.html#starting-a-cluster-manually&quot;&gt;manual method of spawning slaves&lt;/a&gt;.  Doing this averts the following issues:&lt;br /&gt;&amp;lt;ol&amp;gt;&amp;lt;li&amp;gt;start-slaves.sh can’t find any slaves because it always looks for $SPARK_HOME/etc/slaves.  This can be worked around by passing SPARK_SLAVES=$SPARK_CONF_DIR/slaves to start-slaves.sh for a non-default SPARK_CONF_DIR.&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;stop-master.sh doesn’t do anything useful because you still need to kill -9 the master process by hand.  Not sure why this is the case.&amp;lt;/li&amp;gt;&amp;lt;/ol&amp;gt;&amp;lt;div&amp;gt;&lt;br /&gt;&amp;lt;/div&amp;gt;&lt;/p&gt;
&lt;h2&gt;Deciphering Spark Errors&lt;/h2&gt;
&lt;p&gt;Here are various cryptic stack traces I’ve encountered while working on Spark.  I kept these mostly for myself, but I’ve started meeting people that hit the same problems and thought it might be worthwhile to share the diagnoses I’ve found.&lt;br /&gt;&lt;br /&gt;In general, Spark seems to work best when used conservatively, but when you start doing things that do not strictly fall within the anticipated use case, things break in strange ways.  For example, if you try to write an RDD with an empty element (e.g., a text file with empty lines), you would get this really crazy error that does not actually say anything meaningful:&lt;br /&gt;&lt;br /&gt;&amp;lt;pre style=&quot;font-size: smaller;&quot;&amp;gt;14/04/30 16:23:07 ERROR Executor: Exception in task ID 19&lt;br /&gt;scala.MatchError: 0 (of class java.lang.Integer)&lt;br /&gt;     at org.apache.spark.api.python.PythonRDD\(anon$1.read(PythonRDD.scala:110)&amp;lt;br /&amp;gt;&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;at org.apache.spark.api.python.PythonRDD\)anon$1.(PythonRDD.scala:153)&lt;br /&gt;     at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:96)&lt;br /&gt;     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)&lt;br /&gt;     at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)&lt;br /&gt;     at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)&lt;br /&gt;     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)&lt;br /&gt;     at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)&lt;br /&gt;     at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)&lt;br /&gt;     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)&lt;br /&gt;     at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)&lt;br /&gt;     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)&lt;br /&gt;     at org.apache.spark.scheduler.Task.run(Task.scala:53)&lt;br /&gt;     at org.apache.spark.executor.Executor$TaskRunner\(anonfun$run$1.apply$mcV$sp(Executor.scala:213)&amp;lt;br /&amp;gt;&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)&amp;lt;br /&amp;gt;&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)&amp;lt;br /&amp;gt;&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&amp;lt;br /&amp;gt;&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&amp;lt;br /&amp;gt;&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;at java.lang.Thread.run(Thread.java:722)&amp;lt;/pre&amp;gt;&amp;lt;br /&amp;gt;I filed a bug report about this particular problem and the&amp;amp;nbsp;&amp;lt;a href=&quot;https://github.com/apache/spark/pull/644&quot;&amp;gt;issue has been fixed&amp;lt;/a&amp;gt;, but it's just one of those edge cases where Spark will fail catastrophically (I had to look at the source code to figure out what &quot;scala.MatchError&quot; meant). &amp;amp;nbsp;Usually you wouldn't be operating on empty data sets, but I discovered this error when I was trying to quickly determine if my Spark slaves were communicating with my master correctly by issuing&amp;lt;br /&amp;gt;&amp;lt;br /&amp;gt;&amp;lt;pre&amp;gt;file = sc.textFile('hdfs://master.ibnet0/user/glock/input.txt')&amp;lt;br /&amp;gt;file.saveAsTextFile('hdfs://master.ibnet0/user/glock/output')&amp;lt;/pre&amp;gt;&amp;lt;br /&amp;gt;That is, simply reading in a file and writing it back out with pyspark would cause catastrophic failure. &amp;amp;nbsp;This is what I meant when I say Spark's still rough around the edges.&amp;lt;br /&amp;gt;&amp;lt;br /&amp;gt;Here are a few more errors I've encountered. &amp;amp;nbsp;They're not problems with Spark, but the stack traces and exceptions thrown can be a little mysterious. &amp;amp;nbsp;I'm pasting it all here for the sake of googlers who may run into these same problems.&amp;lt;br /&amp;gt;&amp;lt;br /&amp;gt;If you try to use Spark built against Hadoop 2 with a Hadoop 1 HDFS, you'll get this IPC error:&amp;lt;br /&amp;gt;&amp;lt;br /&amp;gt;&amp;lt;pre style=&quot;font-size: smaller;&quot;&amp;gt;&amp;amp;gt;&amp;amp;gt;&amp;amp;gt; file.saveAsTextFile('hdfs://s12ib:54310/user/glock/gutenberg.out')&amp;lt;br /&amp;gt;Traceback (most recent call last):&amp;lt;br /&amp;gt;&amp;amp;nbsp; File &quot;&quot;, line 1, in &amp;lt;br /&amp;gt;&amp;amp;nbsp; File &quot;/home/glock/apps/spark-0.9.0/python/pyspark/rdd.py&quot;, line 682, in saveAsTextFile&amp;lt;br /&amp;gt;&amp;amp;nbsp; &amp;amp;nbsp; keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path)&amp;lt;br /&amp;gt;&amp;amp;nbsp; File &quot;/home/glock/apps/spark-0.9.0/python/lib/py4j-0.8.1-src.zip/py4j/java_gateway.py&quot;, line 537, in __call__&amp;lt;br /&amp;gt;&amp;amp;nbsp; File &quot;/home/glock/apps/spark-0.9.0/python/lib/py4j-0.8.1-src.zip/py4j/protocol.py&quot;, line 300, in get_return_value&amp;lt;br /&amp;gt;py4j.protocol.Py4JJavaError: An error occurred while calling o23.saveAsTextFile.&amp;lt;br /&amp;gt;: org.apache.hadoop.ipc.RemoteException: &amp;lt;b&amp;gt;Server IPC version 9 cannot communicate with client version 4&amp;lt;/b&amp;gt;&amp;lt;br /&amp;gt;&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;at org.apache.hadoop.ipc.Client.call(Client.java:1070)&amp;lt;br /&amp;gt;&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:225)&amp;lt;br /&amp;gt;&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;at $Proxy7.getProtocolVersion(Unknown Source)&amp;lt;br /&amp;gt;&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:396)&amp;lt;br /&amp;gt;&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:379)&amp;lt;br /&amp;gt;&amp;lt;br /&amp;gt;&amp;lt;/pre&amp;gt;&amp;lt;br /&amp;gt;If your Pythons aren't all the same version across the nodes when Spark workers are instantiated, you might get a cryptic error like this when trying to call the count() method on an RDD:&amp;lt;br /&amp;gt;&amp;lt;br /&amp;gt;&amp;lt;pre style=&quot;font-size: smaller;&quot;&amp;gt;14/04/30 16:15:11 ERROR Executor: Exception in task ID 12&amp;lt;br /&amp;gt;org.apache.spark.api.python.PythonException: Traceback (most recent call last):&amp;lt;br /&amp;gt;&amp;amp;nbsp; File &quot;/home/glock/apps/spark-0.9.0-incubating-bin-hadoop1/python/pyspark/worker.py&quot;, line 77, in main&amp;lt;br /&amp;gt;&amp;amp;nbsp; &amp;amp;nbsp; serializer.dump_stream(func(split_index, iterator), outfile)&amp;lt;br /&amp;gt;&amp;amp;nbsp; File &quot;/home/glock/apps/spark-0.9.0-incubating-bin-hadoop1/python/pyspark/serializers.py&quot;, line 182, in dump_stream&amp;lt;br /&amp;gt;&amp;amp;nbsp; &amp;amp;nbsp; self.serializer.dump_stream(self._batched(iterator), stream)&amp;lt;br /&amp;gt;&amp;amp;nbsp; File &quot;/home/glock/apps/spark-0.9.0-incubating-bin-hadoop1/python/pyspark/serializers.py&quot;, line 117, in dump_stream&amp;lt;br /&amp;gt;&amp;amp;nbsp; &amp;amp;nbsp; for obj in iterator:&amp;lt;br /&amp;gt;&amp;amp;nbsp; File &quot;/home/glock/apps/spark-0.9.0-incubating-bin-hadoop1/python/pyspark/serializers.py&quot;, line 171, in _batched&amp;lt;br /&amp;gt;&amp;amp;nbsp; &amp;amp;nbsp; for item in iterator:&amp;lt;br /&amp;gt;&amp;amp;nbsp; File &quot;/home/glock/apps/spark-0.9.0-incubating-bin-hadoop1/python/pyspark/rdd.py&quot;, line 493, in func&amp;lt;br /&amp;gt;&amp;amp;nbsp; &amp;amp;nbsp; if acc is None:&amp;lt;br /&amp;gt;&amp;lt;b&amp;gt;TypeError: an integer is required&amp;lt;/b&amp;gt;&amp;lt;br /&amp;gt;&amp;lt;br /&amp;gt;&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;at org.apache.spark.api.python.PythonRDD\)anon$1.read(PythonRDD.scala:131)&lt;br /&gt;     at org.apache.spark.api.python.PythonRDD\(anon$1.(PythonRDD.scala:153)&amp;lt;br /&amp;gt;&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:96)&amp;lt;br /&amp;gt;&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)&amp;lt;br /&amp;gt;&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)&amp;lt;br /&amp;gt;&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)&amp;lt;br /&amp;gt;&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;at org.apache.spark.scheduler.Task.run(Task.scala:53)&amp;lt;br /&amp;gt;&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;at org.apache.spark.executor.Executor$TaskRunner\)anonfun$run$1.apply$mcV$sp(Executor.scala:213)&lt;br /&gt;     at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)&lt;br /&gt;     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)&lt;br /&gt;     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&lt;br /&gt;     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&lt;br /&gt;     at java.lang.Thread.run(Thread.java:722)&amp;lt;/pre&amp;gt;&lt;br /&gt;&lt;br /&gt;If you try to write an RDD to a file with mismatched Python versions, or if you were using anything earlier than Python 2.7 (e.g., 2.6) with any Spark version earlier than 1.0.0, you’d see this:&lt;br /&gt;&lt;br /&gt;&amp;lt;pre style=&quot;font-size: smaller;&quot;&amp;gt;14/04/30 17:53:20 WARN scheduler.TaskSetManager: Loss was due to org.apache.spark.api.python.PythonException&lt;br /&gt;org.apache.spark.api.python.PythonException: Traceback (most recent call last):&lt;br /&gt;  File “/home/glock/apps/spark-0.9.0-incubating-bin-hadoop2/python/pyspark/worker.py”, line 77, in main&lt;br /&gt;    serializer.dump_stream(func(split_index, iterator), outfile)&lt;br /&gt;  File “/home/glock/apps/spark-0.9.0-incubating-bin-hadoop2/python/pyspark/serializers.py”, line 117, in dump_stream&lt;br /&gt;    for obj in iterator:&lt;br /&gt;  File “/home/glock/apps/spark-0.9.0-incubating-bin-hadoop2/python/pyspark/rdd.py”, line 677, in func&lt;br /&gt;    if not isinstance(x, basestring):&lt;br /&gt;&lt;b&gt;SystemError: unknown opcode&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;     at org.apache.spark.api.python.PythonRDD\(anon$1.read(PythonRDD.scala:131)&amp;lt;br /&amp;gt;&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;at org.apache.spark.api.python.PythonRDD\)anon$1.(PythonRDD.scala:153)&lt;br /&gt;     at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:96)&lt;br /&gt;     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)&lt;br /&gt;     at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)&lt;br /&gt;     at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)&lt;br /&gt;     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)&lt;br /&gt;     at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)&lt;br /&gt;     at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)&lt;br /&gt;     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)&lt;br /&gt;     at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)&lt;br /&gt;     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)&lt;br /&gt;     at org.apache.spark.scheduler.Task.run(Task.scala:53)&lt;br /&gt;     at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213)&lt;br /&gt;     at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)&lt;br /&gt;     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)&lt;br /&gt;     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&lt;br /&gt;     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&lt;br /&gt;     at java.lang.Thread.run(Thread.java:722)&amp;lt;/pre&amp;gt;&lt;br /&gt;&lt;br /&gt;If your HDFS URI is wrong, the error message actually makes sense.  It is buried quite deeply though.&lt;br /&gt;&lt;br /&gt;&amp;lt;pre style=&quot;font-size: smaller;&quot;&amp;gt;Traceback (most recent call last):&lt;br /&gt;  File “”, line 1, in &lt;br /&gt;  File “/home/glock/apps/spark-0.9.0-incubating-bin-hadoop2/python/pyspark/rdd.py”, line 682, in saveAsTextFile&lt;br /&gt;    keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path)&lt;br /&gt;  File “/home/glock/apps/spark-0.9.0-incubating-bin-hadoop2/python/lib/py4j-0.8.1-src.zip/py4j/java_gateway.py”, line 537, in &lt;strong&gt;call&lt;/strong&gt;&lt;br /&gt;  File “/home/glock/apps/spark-0.9.0-incubating-bin-hadoop2/python/lib/py4j-0.8.1-src.zip/py4j/protocol.py”, line 300, in get_return_value&lt;br /&gt;py4j.protocol.Py4JJavaError: An error occurred while calling o23.saveAsTextFile.&lt;br /&gt;: java.lang.IllegalArgumentException: &lt;b&gt;java.net.UnknownHostException: s12ib.ibnet0&lt;/b&gt;&lt;br /&gt;     at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:418)&lt;br /&gt;     at org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.java:231)&lt;br /&gt;     at org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:139)&lt;br /&gt;     at org.apache.hadoop.hdfs.DFSClient.(DFSClient.java:510)&lt;br /&gt;     at org.apache.hadoop.hdfs.DFSClient.(DFSClient.java:453)&lt;br /&gt;     at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:136)&lt;br /&gt;     at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2433)&lt;br /&gt;     at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:88)&lt;br /&gt;     at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2467)&lt;br /&gt;     at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2449)&lt;br /&gt;     at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:367)&lt;br /&gt;     at org.apache.hadoop.fs.Path.getFileSystem(Path.java:287)&lt;br /&gt;     at org.apache.hadoop.mapred.SparkHadoopWriter$.createPathFromString(SparkHadoopWriter.scala:193)&lt;br /&gt;     at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:685)&lt;br /&gt;     at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:572)&lt;br /&gt;     at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:894)&lt;br /&gt;     at org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:355)&lt;br /&gt;     at org.apache.spark.api.java.JavaRDD.saveAsTextFile(JavaRDD.scala:27)&lt;br /&gt;     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br /&gt;     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)&lt;br /&gt;     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)&lt;br /&gt;     at java.lang.reflect.Method.invoke(Method.java:597)&lt;br /&gt;     at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)&lt;br /&gt;     at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)&lt;br /&gt;     at py4j.Gateway.invoke(Gateway.java:259)&lt;br /&gt;     at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)&lt;br /&gt;     at py4j.commands.CallCommand.execute(CallCommand.java:79)&lt;br /&gt;     at py4j.GatewayConnection.run(GatewayConnection.java:207)&lt;br /&gt;     at java.lang.Thread.run(Thread.java:619)&lt;br /&gt;Caused by: java.net.UnknownHostException: s12ib.ibnet0&lt;br /&gt;     … 29 more&amp;lt;/pre&amp;gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Hadoop's Uncomfortable Fit in HPC</title>
   <link href="https://hpc.social/2014/hadoop-s-uncomfortable-fit-in-hpc/"/>
   <updated>2014-05-17T06:28:00-06:00</updated>
   <id>https://hpc.social/2014/hadoop-s-uncomfortable-fit-in-hpc</id>
   <content type="html">&lt;p&gt;Hadoop has come up in a few conversations I’ve had in the last few days, and it’s occurred to me that the supercomputing community continues having a difficult time fully understanding how Hadoop currently fits (and should fit) into scientific computing.  HPCwire was kind enough &lt;a href=&quot;http://www.hpcwire.com/2014/02/11/hpc-hacking-hadoop/&quot;&gt;to run a piece that let me voice my perspective&lt;/a&gt; of the realities of Hadoop use in HPC a few months ago–that is, scientists are still getting a feel for Hadoop and what it can do, and it just isn’t seeing widespread adoption in scientific computing yet.  This contrasts with the tremendous buzz surrounding the “Hadoop” brand and ultimately gives way to strange dialogue, originating from the HPC side of the fence, like this:&lt;br /&gt;&lt;br /&gt;&amp;lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&amp;gt;&lt;a href=&quot;http://4.bp.blogspot.com/-_fXs6Nh1BWw/U3UNE3S96pI/AAAAAAAAKbQ/GE-h2Aphdlw/s1600/Screen+Shot+2014-05-15+at+11.24.37+AM.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;188&quot; src=&quot;http://4.bp.blogspot.com/-_fXs6Nh1BWw/U3UNE3S96pI/AAAAAAAAKbQ/GE-h2Aphdlw/s1600/Screen+Shot+2014-05-15+at+11.24.37+AM.png&quot; width=&quot;320&quot; /&gt;&lt;/a&gt;&amp;lt;/div&amp;gt;
&lt;br /&gt;&amp;lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&amp;gt;&amp;lt;/div&amp;gt;
I’m not sure if this original comment was facetious and dismissive of the Hadoop buzz or if it was a genuinely interested observation.  Regardless of the intent, both interpretations reveal an important fact: Hadoop is being taken seriously only at a subset of supercomputing facilities in the US, and at a finer granularity, only by a subset of professionals within the HPC community.  Hadoop is in a very weird place within HPC as a result, and I thought it might benefit the greater discussion of its ultimate role in research computing if I laid out some of the factors contributing to Hadoop’s current awkward fit.  The rest of this post will strive to answer two questions: &lt;b&gt;Why does Hadoop remain at the fringe of high-performance computing, and what will it take for it to be a serious solution in HPC?&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;&amp;lt;h2&amp;gt;#1. Hadoop is an invader&amp;lt;/h2&amp;gt;I think what makes Hadoop uncomfortable to the HPC community is that, unlike virtually every other technology that has found successful adoption within research computing, &lt;b&gt;Hadoop was not designed by HPC people&lt;/b&gt;.  Compare this to a few other technologies that are core to modern supercomputing:&lt;br /&gt;&lt;br /&gt;&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;&lt;a href=&quot;http://beige.ucs.indiana.edu/I590/node54.html&quot;&gt;MPI was literally born at the world’s largest supercomputing conference&lt;/a&gt;, and the reference was developed by computer scientists major universities and national labs.  It was developed by scientists for scientists.&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&lt;a href=&quot;http://openmp.org/wp/about-openmp/&quot;&gt;OpenMP was developed by an industrial consortium comprised of vendors of high-performance computing hardware and software&lt;/a&gt;.  Like MPI, this standard emerged as a result of vendor-specific threading APIs causing compatibility nightmares across different high-end computing platforms.&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&lt;a href=&quot;http://www.nvidia.com/object/cuda_home_new.html&quot;&gt;CUDA&lt;/a&gt; was developed out of &lt;a href=&quot;http://graphics.stanford.edu/projects/brookgpu/&quot;&gt;Brook&lt;/a&gt; which was developed by &lt;a href=&quot;http://graphics.stanford.edu/people.html&quot;&gt;computer scientists at Stanford&lt;/a&gt;.  Again, CUDA now is largely targeted at high-performance computing (although &lt;a href=&quot;http://venturebeat.com/2014/03/27/big-data-visualization-firm-map-d-takes-home-100k-in-nvidias-emerging-companies-summit-contest/&quot;&gt;this is changing&lt;/a&gt;–and it’ll be interesting to see if adoption outside of HPC really happens)&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;div&amp;gt;By contrast, &lt;a href=&quot;http://hortonworks.com/big-data-insights/spotlight-on-the-early-history-of-hadoop/&quot;&gt;Hadoop was developed by Yahoo&lt;/a&gt;, and the original &lt;a href=&quot;http://research.google.com/archive/mapreduce.html&quot;&gt;MapReduce was developed by Google&lt;/a&gt;.  They were not created to solve problems in fundamental science or national defense; they were created to provide a service for the masses.  They weren’t meant to interface with traditional supercomputers or domain scientists; Hadoop is very much an interloper in the world of supercomputing.&amp;lt;/div&amp;gt;&lt;/p&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;The notion that Hadoop's commercial origins make it contentious for stodgy people in the traditional supercomputing arena may sound silly without context, but the fact is, developing a framework for a commercial application rather than a scientific application leaves it with an interesting amount of baggage.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h2&gt;#2. Hadoop looks funny&lt;/h2&gt;
&lt;div&gt;The most obvious baggage that Hadoop brings with it to HPC is the fact that it is written in Java. &amp;nbsp;One of the &lt;a href=&quot;http://www.oracle.com/technetwork/java/intro-141325.html&quot;&gt;core design features of the Java language was to allow its programmers to write code once and be able to run it on any hardware platform&lt;/a&gt;--a concept that is diametrically opposite to the foundations of high-performance computing, where code should be compiled and optimized for the specific hardware on which it will run. &amp;nbsp;Java made sense for Hadoop due to its origins in the world of web services, but Java maintains a perception of being slow and inefficient. &amp;nbsp;Slow and inefficient codes are, frankly, offensive to most HPC professionals, and I'd wager than a majority of researchers in traditional HPC scientific domains simply don't know the Java language at all. &amp;nbsp;I sure don't.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;The idea of &lt;a href=&quot;http://www.cnet.com/news/supercomputer-beagle-can-analyze-240-whole-genomes-in-two-days/&quot;&gt;running Java applications on supercomputers is beginning to look less funny&lt;/a&gt; nowadays with the explosion of cheap genome sequencing. &amp;nbsp;Some of the most popular foundational applications in bioinformatics (e.g., &lt;a href=&quot;http://www.broadinstitute.org/gatk/&quot;&gt;GATK&lt;/a&gt; and &lt;a href=&quot;http://picard.sourceforge.net/&quot;&gt;Picard&lt;/a&gt;) are written in Java, and although considered an &quot;emerging community&quot; within the field of supercomputing, bioinformatics is rapidly outgrowing the capabilities of lab-scale computing. &amp;nbsp;Perhaps most telling is &lt;a href=&quot;http://www.bio-itworld.com/2014/3/20/broad-intel-announce-speed-improvements-gatk-powered-by-intel-optimizations.html&quot;&gt;Intel's recent contributions to the Java-based GATK&lt;/a&gt; which facilitate much richer use of AVX operations for variant calling.&lt;br /&gt;&lt;br /&gt;With that being said though, Java is still a very strange way to interact with a supercomputer. &amp;nbsp;Java applications don't compile, look, or feel like normal applications in UNIX as a result of their cross-platform compatibility. &amp;nbsp;Its runtime environment exposes a lot of very strange things to the user for no particularly good reason (-Xmx1g? &amp;nbsp;I'm still not sure why I need to specify this to see the version of Java I'm running, much less do anything else**) and it doesn't support shared-memory parallelism in an HPC-oriented way (manual thread management, thread pools...yuck). &amp;nbsp;For the vast majority of HPC users coming from traditional domain sciences and the professionals who support their infrastructure, Java applications remain unconventional and foreign.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div style=&quot;font-size: xx-small; line-height: 120%;&quot;&gt;** A few readers have pointed out that this isn't necessary, and on regular desktops or servers, they would be correct. &amp;nbsp;However, this remark &lt;i&gt;is&lt;/i&gt; true on multi-user, shared resources like supercomputer login nodes where ulimits exist to prevent one user from rendering the node unusable for everyone else. &amp;nbsp;For example, we only allow up to 4 GB of RAM per user on our larger machine's login nodes, and this is not sufficient to run java -version. &amp;nbsp;Yes, there are ways to work around this, but that's the whole point I was trying to make--this is an aspect of Java that is weird when compared to plain old C and Fortran applications.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h2&gt;#3. Hadoop reinvents HPC technologies poorly&lt;/h2&gt;
&lt;div&gt;For those who have taken a serious look at the performance characteristics of Hadoop, the honest truth is that it re-invents a lot of functionality that has existed in HPC for decades, and it does so very poorly. &amp;nbsp;Consider the following examples:&lt;/div&gt;
&lt;div&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;Hadoop uses TCP with a combination of REST and RPC for inter-process communication&lt;/b&gt;. &amp;nbsp;HPC has been using lossless DMA-based communication, which provides better performance in all respects, for years now.&lt;/li&gt;&lt;li&gt;&lt;b&gt;Hadoop doesn't really handle multi-tenancy and its schedulers are terrible&lt;/b&gt;. &amp;nbsp;The architecture of Hadoop is such that, with a 3x replication factor, a single cluster can only support three concurrent jobs at a time with optimal performance. &amp;nbsp;Its current scheduler options have very little in the way of intelligent, locality-aware job placement.&lt;/li&gt;&lt;li&gt;&lt;b&gt;Hadoop doesn't support scalable interconnect topologies&lt;/b&gt;. &amp;nbsp;The rack-aware capabilities of Hadoop, while powerful for their intended purpose, do not support scalable network topologies like multidimensional meshes and toruses. &amp;nbsp;They handle Clos-style network topologies, period.&lt;/li&gt;&lt;li&gt;&lt;b&gt;HDFS is very slow and very obtuse&lt;/b&gt;. &amp;nbsp;Parallel file systems like Lustre and GPFS have been an integral part of HPC for years, and HDFS is just very slow and difficult to use by comparison. &amp;nbsp;The lack of a POSIX interface means getting data in and out is tedious, and its vertical integration of everything from replication and striping to centralized metadata in Java makes it rather unresponsive.&lt;/li&gt;&lt;/ol&gt;&lt;div&gt;However, these poor reinventions are not the result of ignorance; rather, Hadoop's reinvention of a lot of HPC technologies arises from reason #1 above: Hadoop was not designed to run on supercomputers and it was not designed to fit into the existing matrix of technologies available to traditional HPC. &amp;nbsp;Rather, it was created to interoperate with web-oriented infrastructure. &amp;nbsp;Specifically addressing the above four points,&lt;/div&gt;
&lt;/div&gt;
&lt;div&gt;&lt;ol&gt;&lt;li&gt;Hadoop uses TCP/IP and Ethernet because virtually all data center infrastructure is centered around these technologies, not high-speed RDMA. &amp;nbsp;Similarly, REST and RPC are used across enterprise-oriented services because they are simple protocols.&lt;/li&gt;&lt;li&gt;Multi-tenancy arises when many people want to use a scarce resource such as a supercomputer; in the corporate world, resources should never be a limiting factor because waiting in line is what makes consumers look elsewhere. &amp;nbsp;This principle and the need for elasticity is what has made the cloud so attractive to service providers. &amp;nbsp;It follows that Hadoop is designed to provide a service for a single client such as a single search service or data warehouse.&lt;/li&gt;&lt;li&gt;Hadoop's support for Clos-style (leaf/spine) topologies models most data center networks. &amp;nbsp;Meshes, toruses, and more exotic topologies are exclusive to supercomputing and had no relevance to Hadoop's intended infrastructure.&lt;/li&gt;&lt;li&gt;HDFS implements everything in software to allow it to run on the cheapest and simplest hardware possible--JBODs full of spinning disk. &amp;nbsp;The lack of a POSIX interface is a direct result of Hadoop's optimization for large block reads and data warehousing. &amp;nbsp;By making HDFS write-once, a lot of complex distributed locking can go out the window because MapReduce doesn't need it.&lt;/li&gt;&lt;/ol&gt;&lt;div&gt;This loops back around to item #1 above: Hadoop came from outside of HPC, and it carries this baggage with it.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h2&gt;#4. Hadoop evolution is backwards&lt;/h2&gt;&lt;div style=&quot;border: 1px solid black; display: block; float: right; font-size: xx-small; margin-left: 1em; padding: 0.5em; width: 40%;&quot;&gt;&lt;div style=&quot;background-color: black; color: white; text-align: center; width: 100%;&quot;&gt;A tiny anecdote&lt;/div&gt;
&lt;br /&gt;I gave two MapReduce-related consultations this past month which really highlighted how this evolutionary path of Hadoop (and MapReduce in general) is not serving HPC very well.&lt;br /&gt;&lt;br /&gt;My first meeting was with a few folks from a large clinical testing lab that was beginning to to incorporate genetic testing into their service lineup.  They were having a difficult time keeping up with the volume of genetic data being brought in by their customers and were exploring &lt;a href=&quot;https://portal.futuregrid.org/manual/hadoop-blast&quot;&gt;Hadoop BLAST&lt;/a&gt; as an alternative to their current BLAST-centric workflow.  The problem, though, is that Hadoop BLAST was developed as an academic project when Hadoop 0.20 (which has evolved into Hadoop 1.x) was the latest and greatest technology.  Industry has largely moved beyond Hadoop version 1 onto Hadoop 2 and YARN, and this lab was having significant difficulties in getting Hadoop BLAST to run on their brand new Hadoop cluster because its documentation hasn't been updated in three years&lt;br /&gt;&lt;br /&gt;The other meeting was with a colleague who works for a multinational credit scoring company. &amp;nbsp;They were deploying Spark on their Cloudera cluster with the aforementioned clinical testing company: their data collection processes were outgrowing their computational capabilities and they were exploring better alternatives for data exploration. &amp;nbsp;The problem they encountered was not one caused by their applications being frozen in time after someone finished their Ph.D.; rather, their IT department had botched the Spark installation.&lt;br /&gt;&lt;br /&gt;This disparity is pervasive throughout the Hadoop application ecosystem. &amp;nbsp;Tools created for scientific research seem to be abandoned just as quickly as they were created, so looking for existing Hadoop-based tools for research can be a frustrating game of chasing 404s.&lt;/div&gt;
&lt;/div&gt;
&lt;div&gt;Generally speaking, the development of all technologies at the core of HPC have followed a similar evolutionary path into broad adoption. &amp;nbsp;Both software and hardware technologies arise as disparities between available and necessary solutions widen. &amp;nbsp;Researchers often hack together non-standard solutions to these problems until a critical mass is achieved, and a standard technology emerges to unify these varying solutions and fill the gap. &amp;nbsp;OpenMP is a great example--before it became standard, there were a number of vendor-specific pragma-based multithreading APIs; &amp;nbsp;&lt;a href=&quot;http://docs.cray.com/books/S-2179-52/html-S-2179-52/z1075740829oswald.html&quot;&gt;Cray&lt;/a&gt;, &lt;a href=&quot;http://docs.oracle.com/cd/E19059-01/stud.9/817-6694/10_parallel.html#74354&quot;&gt;Sun&lt;/a&gt;, and &lt;a href=&quot;http://techpubs.sgi.com/library/tpl/cgi-bin/getdoc.cgi?coll=0650&amp;amp;db=bks&amp;amp;srch=&amp;amp;fname=/SGI_Developer/Pragmas/sgi_html/ch09.html&quot;&gt;SGI&lt;/a&gt;&amp;nbsp;all had their own implementations that did the same thing but made porting codes between systems very unpleasant. &amp;nbsp;These vendors ultimately all adopted a standard interface which became OpenMP, and that technology has been embraced because it provided a portal way of solving the original motivating problem.&lt;br /&gt;&lt;br /&gt;The evolution of Hadoop has very much been a backwards one; it entered HPC as a solution to a problem which, by and large, did not yet exist. &amp;nbsp;As a result, it followed a common, but backwards, pattern by which computer scientists, not domain scientists, get excited by a new toy and invest a lot of effort into creating proof-of-concept codes and use cases. &amp;nbsp;Unfortunately, this sort of development is fundamentally unsustainable because of its nucleation in a vacuum, and in the case of Hadoop, researchers moved on to the next big thing and largely abandoned their model applications as the shine of Hadoop faded (see sidebar). &amp;nbsp;This has left a graveyard of software, documentation, and ideas that are frozen in time and rapidly losing relevance as Hadoop moves on.&lt;br /&gt;&lt;br /&gt;Consider this evolutionary path of Hadoop compared to OpenMP: there were no OpenMP proofs-of-concept. &amp;nbsp;There didn't need to be any; the problems had already been defined by the people who needed OpenMP, so by the time OpenMP was standardized and implemented in compilers, application developers already knew where it would be needed.&lt;br /&gt;&lt;br /&gt;Not surprisingly, innovation in the Hadoop software ecosystem remains in the sphere for which it was developed: data warehousing and data analytics. &amp;nbsp;Applications and libraries like &lt;a href=&quot;http://www.cloudera.com/content/cloudera/en/products-and-services/cdh/impala.html&quot;&gt;Impala&lt;/a&gt;, &lt;a href=&quot;http://parquet.io/&quot;&gt;Parquet&lt;/a&gt;, and &lt;a href=&quot;http://spark.apache.org/&quot;&gt;Spark&lt;/a&gt; are at the cutting edge of applied analytics in the Hadoop/MapReduce ecosystem and represent useful, usable implementations of some really novel ideas.&lt;br /&gt;&lt;br /&gt;&lt;h2 style=&quot;clear: both;&quot;&gt;How &lt;i&gt;can&lt;/i&gt; Hadoop fit into HPC?&lt;/h2&gt;So this all is why Hadoop is in this awkward position, but does this mean Hadoop (and MapReduce) will never be welcome in the world of HPC? &amp;nbsp;Alternatively, what would it take for Hadoop to become a universally recognized core technology in HPC?&lt;br /&gt;&lt;br /&gt;I'll say up front that there are no easy answers--if there were, I wouldn't be delivering this monologue. &amp;nbsp;However, solutions are being developed and attempted to address a few of the four major barriers I outlined above.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Reimplement MapReduce in an HPC-oriented way&lt;/h3&gt;This idea has been tried in a number of different ways (see &lt;a href=&quot;http://mapreduce.sandia.gov/&quot;&gt;MPI MapReduce&lt;/a&gt; and &lt;a href=&quot;http://mapreduce.stanford.edu/&quot;&gt;Phoenix&lt;/a&gt;), but none have really gained traction. &amp;nbsp;I suspect this is largely the result of one particular roadblock: there just aren't that many problems which are so onerous in the traditional HPC space that reimplementing a solution in a relatively obscure implementation of MapReduce becomes worth the effort. &amp;nbsp;As I mentioned in point #4 above, HPC vendors haven't been creating their own MapReduce APIs to address the demands of their customers as they did for OpenMP and MPI's predecessors, so Hadoop's role in HPC is not clearly addressing a problem that needs an immediate solution.&lt;br /&gt;&lt;br /&gt;&lt;i&gt;This is not to say that the data-oriented problems at which Hadoop excels do not exist within the domain sciences&lt;/i&gt;. &amp;nbsp;Rather, there are two key roles that Hadoop/MapReduce will play in scientific computations:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Solving existing problems&lt;/b&gt;: &amp;nbsp;The most activity I've seen involving Hadoop in domain sciences comes out of bioinformatics and observational sciences. &amp;nbsp;Bioinformatics, as a consumer of HPC cycles, is still in its infancy, but &lt;a href=&quot;http://glennklockwood.blogspot.com/2014/01/the-1000-genome-computational.html&quot;&gt;the data sets being generated by next-generation sequencers are enormous&lt;/a&gt;--the data to describe a single human genome, even when compressed, takes up about 120 GB. &amp;nbsp;Similarly, advances in imaging and storage technology have allowed &lt;a href=&quot;http://cas.sdss.org/astro/en/skyserver/paper/&quot;&gt;astronomy&lt;/a&gt; and &lt;a href=&quot;http://blog.cloudera.com/blog/2012/07/processing-rat-brain-neuronal-signals-using-a-hadoop-computing-cluster-part-i/&quot;&gt;radiology&lt;/a&gt; to generate extremely large collections of data.&lt;/li&gt;&lt;li&gt;&lt;b&gt;Enabling new problems&lt;/b&gt;: One of Hadoop's more long-term promises is not solving the problems of today, but giving us a solution to problems we previously thought to be intractable. &amp;nbsp;Although I can't disclose too much detail, an example of this lies in statistical mechanics: many problems involving large ensembles of particles have relied on data sampling or averaging to reduce the sheer volume of numerical information into a usable state. &amp;nbsp;Hadoop and MapReduce allow us to start considering what deeper, more subtle patterns may emerge if a massive trajectory through phase space could be dumped and analyzed with, say, machine learning methods.&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;Unfortunately, reimplementing MapReduce inside the context of existing HPC paradigms represents a large amount of work for a relatively small subset of problems. &amp;nbsp;Some sort of catalyzing scientific problem will need to emerge to give vendors and application developers a strong reason to start re-thinking their problems in terms of MapReduce.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Incorporate HPC technologies in Hadoop&lt;/h3&gt;Rather than reimplementing Hadoop/MapReduce as an HPC technology, I think a more viable approach forward is to build upon the Hadoop framework and correct some of its poorly reinvented features I described in item #3 above. &amp;nbsp;This will allow HPC to continuously fold in new innovations being developed in Hadoop's traditional competencies--data warehousing and analytics--as they become relevant to scientific problems. &amp;nbsp;Some serious effort is being made to this end:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;The &lt;a href=&quot;http://hadoop-rdma.cse.ohio-state.edu/&quot;&gt;RDMA for Apache Hadoop project&lt;/a&gt;, headed by the esteemed D.K. Panda and his colleagues at OSU, has replaced Hadoop's TCP/RPC communication modes with native RDMA &lt;a href=&quot;http://hadoop-rdma.cse.ohio-state.edu/performance/terasort/&quot;&gt;with really impressive initial results&lt;/a&gt;.&lt;/li&gt;&lt;li&gt;Some larger players in the HPC arena have begun to provide rich support for high-performance parallel file systems as a complete alternative to HDFS. &amp;nbsp;IBM's GPFS file system has a &lt;a href=&quot;http://public.dhe.ibm.com/common/ssi/ecm/en/dcs03038usen/DCS03038USEN.PDF&quot;&gt;file placement optimization (FPO)&lt;/a&gt; capability that allows GPFS to act as a drop-in replacement for HDFS, and &lt;a href=&quot;http://www.intel.com/content/www/us/en/software/intel-hpc-distribution-for-apache-hadoop-software.html&quot;&gt;Intel was selling native Lustre&lt;/a&gt; support before they &lt;a href=&quot;http://newsroom.intel.com/community/intel_newsroom/blog/2014/03/27/cloudera-intel-commit-to-accelerate-and-transform-how-enterprises-use-big-data-intel-makes-significant-equity-investment-in-cloudera&quot;&gt;sold IDH to Cloudera&lt;/a&gt;.&lt;/li&gt;&lt;li&gt;I would be remiss if I did not mention my own efforts in making Hadoop provisioning as seamless as possible on batch-based systems with &lt;a href=&quot;https://github.com/glennklockwood/myhadoop/&quot;&gt;myHadoop&lt;/a&gt;.&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;In addition to incorporating these&amp;nbsp;&lt;i&gt;software&lt;/i&gt; technologies from HPC into Hadoop, there are some really clever things you can do with &lt;i&gt;hardware&lt;/i&gt; technologies that make Hadoop much more appealing to traditional HPC. &amp;nbsp;I am working on some exciting and innovative (if I may say so) architecture designs that will further lower the barrier between Hadoop and HPC at my day job, and with any luck, we'll get to see some of these ideas go into production in the next few years.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Make MapReduce Less Weird&lt;/h3&gt;The very nature of MapReduce is a very strange one to supercomputing--it solves a class of problems that the world's fastest supercomputers just weren't designed to solve. &amp;nbsp;Rather than make raw compute performance the most important capability, MapReduce treats I/O scalability as the most important capability and CPU performance is secondary. &amp;nbsp;As such, it will always be weird until such a day comes when science faces an equal balance of compute-limited and data-limited problems. &amp;nbsp;Fundamentally, I'm not sure that such a day will ever come. &amp;nbsp;Throwing data against a wall to see what sticks is good, but deriving analytical insight is better.&lt;br /&gt;&lt;br /&gt;With that all being said, there's room for improvement in making Hadoop less weird. &amp;nbsp;&lt;a href=&quot;http://spark.apache.org/&quot;&gt;Spark&lt;/a&gt; is an exciting project because it sits at a nice point between academia and industry; &lt;a href=&quot;https://amplab.cs.berkeley.edu/projects/spark-lightning-fast-cluster-computing/&quot;&gt;developed at Berkeley&lt;/a&gt; but targeted directly at Hadoop, it feels like it was developed for scientists, and it treats high-performance as a first-class citizen by providing the ability to utilize memory a lot more efficiently than Hadoop does. &amp;nbsp;It also doesn't have such a heavy-handed Java-ness to it and provides a reasonably rich interface for Python (and&amp;nbsp;&lt;a href=&quot;https://amplab.cs.berkeley.edu/2014/01/26/large-scale-data-analysis-made-easier-with-sparkr/&quot;&gt;R support is on the way&lt;/a&gt;!). &amp;nbsp;There still are a lot of rough edges (this is where the academic origins shine through, I think) but I'm hopeful that it cleans up under the Apache project.&lt;br /&gt;&lt;br /&gt;Perhaps more than (or inclusive of) the first two paths forward in increasing MapReduce adoption in research science, Spark holds the most promise in that it feels less like Hadoop and more normal from the HPC perspective. &amp;nbsp;It doesn't force you to cast your problem in terms of a map and a reduce step; the way in which you interact with your data (your &lt;i&gt;resilient distributed dataset&lt;/i&gt;, or RDD, in Spark parlance) is &lt;a href=&quot;http://spark.apache.org/docs/0.9.0/api/pyspark/pyspark.rdd.RDD-class.html&quot;&gt;much more versatile&lt;/a&gt; and is more likely to directly translate to the logical operation you want to perform. &amp;nbsp;It also supports the basic things Hadoop lacks such as iterative operations.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Moving Forward&lt;/h2&gt;I think I have a pretty good idea about why Hadoop has received a lukewarm, and sometimes cold, reception in HPC circles, and much of these underlying reasons are wholly justified. &amp;nbsp;Hadoop's from the wrong side of the tracks from the purists' perspective, and it's not really changing the way the world will do its high-performance computing. &amp;nbsp;There is a disproportionate amount of hype surrounding it as a result of its revolutionary successes in the commercial data sector.&lt;br /&gt;&lt;br /&gt;However, Hadoop and MapReduce aren't to be dismissed outright either. &amp;nbsp;There is a growing subset of scientific problems that are growing against a scalability limit in terms of data movement, and at some point, solving these problems using conventional, CPU-oriented parallelism will reduce to using the wrong tool for the job. &amp;nbsp;The key, as is always the case in this business, is to understand the job and realize that there are more tools in the toolbox than just a hammer.&lt;br /&gt;&lt;br /&gt;As these data-intensive and data-bound problems gain a growing presence in traditional HPC domains, I hope the progress being made on making Hadoop and MapReduce more relevant to research science continues. &amp;nbsp;I mentioned above that great strides forward are being made to truly bridge the gap of utility and making MapReduce a serious go-to solution to scientific problems, and although Hadoop remains on the fringe of HPC today, it won't pay to dismiss it for too much longer.&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Parallelizing R on Supercomputers</title>
   <link href="https://hpc.social/2014/parallelizing-r-on-supercomputers/"/>
   <updated>2014-04-25T00:29:00-06:00</updated>
   <id>https://hpc.social/2014/parallelizing-r-on-supercomputers</id>
   <content type="html">&lt;div&gt;&lt;b&gt;Executive summary&lt;/b&gt;: &amp;nbsp;I've posted a tutorial on &lt;a href=&quot;http://www.glennklockwood.com/di/R-para.php&quot;&gt;how to parallelize R codes&lt;/a&gt; on my website. &amp;nbsp;This post is a more personal reflection on how I got there.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;&lt;hr /&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;“Parallel Options for R” was the title of the first talk I ever presented on behalf of my employer, and despite the fact that I &lt;a href=&quot;http://www.theguardian.com/lifeandstyle/2013/nov/09/impostor-syndrome-oliver-burkeman&quot;&gt;didn’t (and still don’t) know anything&lt;/a&gt; about the R language, statistics, or how to parallelize any of it, the shoe seemed to fit at the time.  The talk went over well, and I’ve been &lt;a href=&quot;http://pace.sdsc.edu//sites/pace.sdsc.edu/bootcamp2/201405/schedule.html&quot;&gt;asked&lt;/a&gt; &lt;a href=&quot;http://www.meetup.com/San-Diego-Data-Science-R-Users-Group/events/135782742/&quot;&gt;to give&lt;/a&gt; &lt;a href=&quot;http://extension.ucsd.edu/studyarea/index.cfm?vAction=singleCourse&amp;amp;vCourse=CSE-41185&quot;&gt;the talk&lt;/a&gt; in my capacity as the resident “parallel R guy” plenty of times since.&lt;br /&gt;&amp;lt;div&amp;gt;&lt;br /&gt;&amp;lt;/div&amp;gt;&lt;/p&gt;
&lt;div&gt;Every once in a while I get asked how I came to become so involved in some of the weird topics about which I write and speak--after all, I really have no formal training in things like &lt;a href=&quot;http://glennklockwood.blogspot.com/2013/12/high-performance-virtualization-sr-iov_14.html&quot;&gt;SR-IOV&lt;/a&gt;, &lt;a href=&quot;http://glennklockwood.blogspot.com/2014/02/deploying-hadoop-on-traditional.html&quot;&gt;Hadoop&lt;/a&gt;, and &lt;a href=&quot;http://glennklockwood.blogspot.com/2014/01/the-1000-genome-computational.html&quot;&gt;next-generation gene sequencing&lt;/a&gt;. &amp;nbsp;As much as I'd like to claim I just have some infinite sage-like knowledge, the reality is that I have to learn about these various technologies as a result of my day job--answering helpdesk tickets. &amp;nbsp;In the case of parallel R, I simply got a ticket in January 2013 that read,&lt;/div&gt;
&lt;div&gt;&lt;blockquote class=&quot;tr_bq&quot;&gt;&quot;I just ran an intensive R script through [the supercomputer]. &amp;nbsp;Its not much faster than my own machine. &amp;nbsp;Could you point me to a tutorial for how I can make the process run in different processors in parallel?&quot;&lt;/blockquote&gt;&lt;/div&gt;
&lt;div&gt;I couldn't very well say &quot;lol no idea&quot; (which was the truth), but the fact is that there are only about three whole people in my group** who are tasked with solving every problem that comes in from the thousand unique users who run jobs on our system every year. &amp;nbsp;If I didn't know the answer, there was a good chance that nobody else knew either. &amp;nbsp;That doesn't change the fact that someone needs to answer the user's question though, and that fact is what got me into the parallel R business.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;In my quest for an answer to this user's helpdesk request, I further discovered that there were no good tutorials online that explain the process of parallelizing R codes. &amp;nbsp;Thus, I wound up having to &lt;a href=&quot;http://shop.oreilly.com/product/0636920021421.do&quot;&gt;buy a book&lt;/a&gt; to learn what I need to know to answer the user's question. &amp;nbsp;So I did, and I learned the rough basics of how someone might go about parallelizing their R codes. &amp;nbsp;I gave the user a few starting pointers, some of the libraries that he might want to check out on CRAN, and tried to provide some boilerplate code that might help him parallelize his particular script. &amp;nbsp;We then went our separate ways.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;With all this reflection aside though, I never lost sight of the reality that I never did answer the user's question: what is a good tutorial on how to parallelize R codes?&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;This question has actually come up a number of times from a number of users over the last year. &amp;nbsp;Rather than take the easy route and tell everyone to attend my next talk on the subject, I decided to turn my presentation on parallelizing R into a series of tutorials which I've put on my website:&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;http://www.glennklockwood.com/di/R-para.php&quot;&gt;&lt;b&gt;Parallel Options for R&lt;/b&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;It's not comprehensive by any means; notably, I did not cover either the &lt;a href=&quot;http://r-pbd.org/&quot;&gt;pbdr library out of UTK/Oak Ridge&lt;/a&gt; (an omission with no particularly good justification) or &lt;a href=&quot;http://r-sprint.org/&quot;&gt;SPRINT from Edinburgh&lt;/a&gt; (it's a bit specialized in functionality). &amp;nbsp;I also haven't had the opportunity to convert my presentation on using R with Hadoop and Spark into the final component of this tutorial. &amp;nbsp;Those topics will come as time permits. &amp;nbsp;Regardless, I hope someone finds the write-up useful.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;&lt;span style=&quot;font-size: xx-small;&quot;&gt;** I say &quot;whole people&quot; to reflect that our funding provides somewhere in the neighborhood of three full-time equivalent employees providing front-line user support. &amp;nbsp;That funding winds up getting distributed across more physical staff.&lt;/span&gt;&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Quantum ESPRESSO- Performance Benefits of Vendor-Optimized Libraries</title>
   <link href="https://hpc.social/2014/quantum-espresso-performance-benefits-of-vendor-optimized-libraries/"/>
   <updated>2014-02-25T16:42:00-07:00</updated>
   <id>https://hpc.social/2014/quantum-espresso-performance-benefits-of-vendor-optimized-libraries</id>
   <content type="html">&lt;div class=&quot;p1&quot;&gt;In my previous post, I presented a lot of different options you can use to build Quantum ESPRESSO which are (admittedly) very confusing. &amp;nbsp;At the end of the day, the set of options that produce the fastest-running executable matters the most, so I went through and benchmarked many of the permutations of compiler/MPI/library options.&lt;/div&gt;
&lt;div class=&quot;p1&quot;&gt;&lt;br /&gt;What this post ultimately illustrates is that&amp;nbsp;&lt;i&gt;you should never use the Netlib reference implementations of BLAS and LAPACK&lt;/i&gt;; even &lt;a href=&quot;http://www.netlib.org/blas/faq.html#5&quot;&gt;Netlib says as much&lt;/a&gt;. &amp;nbsp;ScaLAPACK is much less broadly supported by hardware vendors (e.g., the ACML library that shipped with the PGI compiler I used did not include it), but most of the hardware-dependent optimizations are done below the BLACS level and within the MPI library and associated hardware drivers. &amp;nbsp;As such, I was able to use Intel's MKL ScaLAPACK when building with the Intel compiler in the data below, but I had to use Netlib's ScaLAPACK with ACML-optimized BLAS and LAPACK when compiling with PGI.&lt;br /&gt;&lt;br /&gt;The actual benchmark I used was the &lt;a href=&quot;http://www.deisa.eu/science/benchmarking/codes/quantumespresso&quot;&gt;DEISA AUSURF112 benchmark&lt;/a&gt;&amp;nbsp;problem with only one pool using 64 MPI processes. &amp;nbsp;The two testing platforms were&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;SDSC's Gordon supercomputer (four nodes)&lt;/li&gt;&lt;ul&gt;&lt;li&gt;16× 2.6 GHz Intel Xeon E5-2670 (Sandy Bridge) cores&lt;/li&gt;&lt;li&gt;64 GB DDR3 SDRAM&lt;/li&gt;&lt;li&gt;Mellanox ConnectX-3 QDR HCAs on PCIe 3.0&lt;/li&gt;&lt;li&gt;Mellanox Infiniscale IV switch&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;SDSC's Trestles supercomputer (two nodes)&lt;/li&gt;&lt;ul&gt;&lt;li&gt;32×&amp;nbsp;2.4 GHz AMD Opteron 6136 (Magny Cours) nodes&lt;/li&gt;&lt;li&gt;64 GB DDR3 SDRAM&lt;/li&gt;&lt;li&gt;Mellanox ConnectX QDR HCAs on PCIe 2.0&lt;/li&gt;&lt;li&gt;Voltaire Grid Director 4700 switch&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;br /&gt;I don't know the port-to-port latency for the Trestles runs, but the application is bandwidth-bound due to the problem geometry (one pool) and the large amount of &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;MPI_Allreduce&lt;/span&gt;s and &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;MPI_Alltoallv&lt;/span&gt;s render the latency largely irrelevant. &amp;nbsp;More information about the communication patterns of this benchmark are available from the &lt;a href=&quot;http://www.hpcadvisorycouncil.com/pdf/QuantumEspresso_Performance_Analysis.pdf&quot;&gt;HPC Advisory Council&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;On both testing systems, the software versions were the same:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Compilers&lt;/b&gt;: Intel 2013.1.117 and PGI 13.2&lt;/li&gt;&lt;li&gt;&lt;b&gt;MPI libraries&lt;/b&gt;: MVAPICH2 1.9 and OpenMPI 1.6.5&lt;/li&gt;&lt;li&gt;&lt;b&gt;Vendor FFTs&lt;/b&gt;: MKL 11.0.1 and ACML 5.3.0&lt;/li&gt;&lt;li&gt;&lt;b&gt;Vendor BLAS/LAPACK&lt;/b&gt;: MKL 11.0.1 and ACML 5.3.0&lt;/li&gt;&lt;li&gt;&lt;b&gt;Vendor ScaLAPACK&lt;/b&gt;: MKL 11.0.1 (used Netlib ScaLAPACK 2.0.2 with PGI)&lt;/li&gt;&lt;li&gt;&lt;b&gt;Reference FFTs&lt;/b&gt;: FFTW 3.3.3&lt;/li&gt;&lt;li&gt;&lt;b&gt;Reference BLAS/LAPACK&lt;/b&gt;: Netlib 3.4.2&lt;/li&gt;&lt;li&gt;&lt;b&gt;Reference ScaLAPACK&lt;/b&gt;: Netlib 2.0.2&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;&lt;h2&gt;Vendor-optimized Libraries&lt;/h2&gt;On Gordon, MKL shows extremely good performance compared to ACML, and this is to be expected given the fact that Intel's MKL is optimized for Gordon's ability to do AVX operations.&lt;br /&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;http://3.bp.blogspot.com/-9W-NxWoL_-c/UwASBxQ_ZBI/AAAAAAAAKRc/wN5-WgL4y7Q/s1600/Gordon+Comparison.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;271&quot; src=&quot;http://3.bp.blogspot.com/-9W-NxWoL_-c/UwASBxQ_ZBI/AAAAAAAAKRc/wN5-WgL4y7Q/s1600/Gordon+Comparison.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Performance with vendor libraries on Gordon&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;
&lt;div class=&quot;p1&quot;&gt;&lt;br /&gt;In addition, the difference in MPI libraries is also quite consistent. &amp;nbsp;Although the point-to-point performance of MVAPICH2 and OpenMPI over the same fabric should be comparable, the two libraries have different implementations of MPI collective operations. &amp;nbsp;Quantum ESPRESSO is dominated by costly &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;MPI_Allreduce&lt;/span&gt; and &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;MPI_Alltoallv&lt;/span&gt;, so the level of optimization within the MPI implementations is very apparent.&lt;br /&gt;&lt;br /&gt;In fact, the PGI and OpenMPI build (which uses the Netlib ScaLAPACK, as opposed to a vendor-supplied ScaLAPACK which MKL provides) would hang on collectives unless the following environment variable was passed to the OpenMPI runtime:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;OMPI_MCA_coll_sync_barrier_after=100&lt;/pre&gt;&lt;br /&gt;This switch forces the OpenMPI runtime to sync all processes after every 100 collective operations to prevent certain MPI ranks from racing so far ahead of the rest that a deadlock occurs. &amp;nbsp;OpenMPI does this after every 1,000 collectives by default. &amp;nbsp;Alternatively, HPCAC suggests the following tunings for OpenMPI:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;OMPI_MCA_mpi_affinity_alone=1&lt;br /&gt;OMPI_MCA_coll_tuned_use_dynamic_rules=1&lt;br /&gt;OMPI_MCA_coll_tuned_barrier_algorithm=6&lt;br /&gt;OMPI_MCA_coll_tuned_allreduce_algorithm=0&lt;/pre&gt;&lt;br /&gt;These collective tunings also prevented deadlocking of the benchmark, but the performance was no better than simply increasing the implicit barrier frequency with &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;OMPI_MCA_coll_sync_barrier_&lt;/span&gt;*.&lt;br /&gt;&lt;br /&gt;Trestles, with its AMD processors, does not realize as large a benefit from using MKL:&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div class=&quot;p1&quot;&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;http://3.bp.blogspot.com/-ijpV8iYr6uw/UwASP0zFqQI/AAAAAAAAKRk/peFbho9NXQ0/s1600/Trestles+Comparison.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;271&quot; src=&quot;http://3.bp.blogspot.com/-ijpV8iYr6uw/UwASP0zFqQI/AAAAAAAAKRk/peFbho9NXQ0/s1600/Trestles+Comparison.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Performance with vendor libraries on Trestles&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;
&lt;div class=&quot;p1&quot;&gt;&lt;br /&gt;MKL still outperforms ACML even on AMD processors, but the margin is almost negligible. &amp;nbsp;As with the Gordon case though, the difference in MPI implementations is start because of OpenMPI's poor collective performance.&lt;br /&gt;&lt;br /&gt;It is worth noting that PGI with OpenMPI did not work unless&amp;nbsp;&lt;i&gt;both&lt;/i&gt; of the following OpenMPI parameters were specified:&lt;/div&gt;
&lt;div class=&quot;p1&quot;&gt;&lt;br /&gt;&lt;/div&gt;
&lt;pre&gt;OMPI_MCA_coll_sync_barrier_after=100&lt;br /&gt;OMPI_MCA_coll_sync_barrier_before=100&lt;/pre&gt;
&lt;div class=&quot;p1&quot;&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div class=&quot;p1&quot;&gt;At smaller processor counts, ScaLAPACK compiled with OpenMPI (both Netlib's and MKL's implementations) performed horrendously. &amp;nbsp;I don't know exactly what the conflict is, but OpenMPI and ScaLAPACK do not seem to play nicely.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h2&gt;Netlib reference implementations&lt;/h2&gt;
&lt;div class=&quot;p1&quot;&gt;As a fun afterthought, I thought it also might be useful to compare the vendor libraries to Netlib's reference implementations of BLAS and LAPACK. &amp;nbsp;I rebuilt the four compiler+MPI combinations on both systems using Netlib's BLAS, LAPACK, and ScaLAPACK (as well as the stock FFTW library instead of MKL or ACML's versions) to see how badly Netlib's reference really performs, and here are the results:&lt;/div&gt;
&lt;div class=&quot;p1&quot;&gt;&lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;http://3.bp.blogspot.com/-XrD1PtrJ_iI/UwASdyK5f_I/AAAAAAAAKR4/f13bCkKCte8/s1600/Gordon+Reference.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;271&quot; src=&quot;http://3.bp.blogspot.com/-XrD1PtrJ_iI/UwASdyK5f_I/AAAAAAAAKR4/f13bCkKCte8/s1600/Gordon+Reference.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Performance with Netlib reference libraries on Gordon. &amp;nbsp;The build with Intel and MVAPICH2 was not able to run.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;On SDSC's Gordon resource, the OpenMPI builds were between 3× and 4× slower, but the PGI build with MVAPICH2 was only(!) 64% slower. &amp;nbsp;This is a curious result, as I would have expected performance to be dramatically worse across all combinations of compiler and MPI library since BLAS and LAPACK should really show no performance difference when it comes to the choice of MPI library. &lt;br /&gt;&lt;br /&gt;The above results suggest that Quantum ESPRESSO makes its heavy use of BLAS and LAPACK through the ScaLAPACK library, and as such, the ScaLAPACK implementation and its performance with each of the MPI libraries is critically important. &amp;nbsp;Of course, even with a good combination of ScaLAPACK and MPI stack, having a vendor-optimized BLAS and LAPACK goes a long way in increasing overall performance by more than 50%.&lt;br /&gt;&lt;br /&gt;It should also be obvious that the Intel and MVAPICH2 build's performance data is absent. &amp;nbsp;This is because the build with Intel and MVAPICH2 repeatedly failed with this error:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;** On entry to DLASCL parameter number 4 had an illegal value&lt;/pre&gt;&lt;br /&gt;This error is the result of DGELSD within LAPACK not converging within the hard-coded criteria. &amp;nbsp;&lt;a href=&quot;https://icl.cs.utk.edu/lapack-forum/viewtopic.php?t=529&quot;&gt;This problem has been detailed at the LAPACK developers' forums&lt;/a&gt;, and the limits were actually dramatically increased since the postings in the aforementioned forum. &lt;br /&gt;&lt;br /&gt;Despite that patch though, the problem still manifests in the newest versions of Netlib's reference BLACS/ScaLAPACK implementation, and I suspect that this is really &lt;a href=&quot;http://mailman.cse.ohio-state.edu/pipermail/mvapich-discuss/2013-May/004434.html&quot;&gt;a fundamental limitation of the BLACS library relying on platform-dependent behavior to produce its results&lt;/a&gt;. &amp;nbsp;Recall from above that the vendor-supplied implementations of LAPACK do not trigger this error.&lt;br /&gt;&lt;br /&gt;On Trestles, the results are even worse:&lt;br /&gt;&lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;http://4.bp.blogspot.com/-Gu-zuZeCanw/UwASr7WVTAI/AAAAAAAAKR8/FxMmVFKNWoo/s1600/Trestles+Reference.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;271&quot; src=&quot;http://4.bp.blogspot.com/-Gu-zuZeCanw/UwASr7WVTAI/AAAAAAAAKR8/FxMmVFKNWoo/s1600/Trestles+Reference.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Performance with Netlib reference libraries on Trestles. &amp;nbsp;Only the build with PGI and MVAPICH2 was able to run.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;When built with the Intel compiler, both MVAPICH2- and OpenMPI-linked builds trigger the DLASCL error. &amp;nbsp;The PGI and OpenMPI build do not trigger this error, but instead hang on collectives even with the OpenMPI tunings I reported for the vendor-optimized Trestles PGI+OpenMPI build.&lt;br /&gt;&lt;br /&gt;Cranking up the implicit barrier frequency beyond 100 might have gotten the test to run, but quite frankly, having to put a barrier before &lt;i&gt;and&lt;/i&gt; after every 100th collective is already an extremely aggressive modification to runtime behavior. &amp;nbsp;Ultimately, this data all suggests that you should, in fact, never use the Netlib reference implementations of BLAS and LAPACK.&lt;/div&gt;
&lt;div class=&quot;p1&quot;&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h2&gt;Summary of Data&lt;/h2&gt;
&lt;div class=&quot;p1&quot;&gt;Here is an overall summary of the test matrix:&lt;/div&gt;
&lt;div class=&quot;p1&quot;&gt;&lt;br /&gt;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;http://1.bp.blogspot.com/-a9IQN7AJdeU/UwATBXIp5bI/AAAAAAAAKSE/FFJ8mvqUc3s/s1600/Comparison+All.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;271&quot; src=&quot;http://1.bp.blogspot.com/-a9IQN7AJdeU/UwATBXIp5bI/AAAAAAAAKSE/FFJ8mvqUc3s/s1600/Comparison+All.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;&gt;Overall performance comparison for AUSURF112 benchmark&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;This benchmark is very sensitive to the performance of collectives, and exactly how collectives are performed is specific to the MPI implementation being used. &amp;nbsp;OpenMPI shows weaker collective performance across the board, and as a result, significantly worse performance.&lt;br /&gt;&lt;br /&gt;These collective calls are largely made via the ScaLAPACK library though, and since ScaLAPACK is built upon BLAS and LAPACK, it is critical to have all components (BLAS, LAPACK, ScaLAPACK, and the MPI implementation) working together. &amp;nbsp;In all cases tested, &lt;b&gt;Intel's MKL library along with MVAPICH2 provides the best performance&lt;/b&gt;. &amp;nbsp;As one may guess, ACML also performs well on AMD Opteron processors, but its lack of optimization for AVX instructions prevented it from realizing the full performance possible on Sandy Bridge processors.&lt;br /&gt;&lt;br /&gt;In addition to performance, there are conclusions to be drawn about &lt;i&gt;application resiliency&lt;/i&gt;, or Quantum ESPRESSO's ability to run calculations without hanging or throwing strange errors:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;PGI with MVAPICH2 was the most resilient combination&lt;/b&gt;; it worked out of the box with all combinations of BLAS/LAPACK/ScaLAPACK tested&lt;/li&gt;&lt;li&gt;&lt;b&gt;PGI with OpenMPI was the least resilient combination&lt;/b&gt;, perhaps because ACML's lack of ScaLAPACK bindings forces the use of Netlib ScaLAPACK. &amp;nbsp;Combining Netlib BLAS/LAPACK/ScaLAPACK with PGI/OpenMPI simply failed on Trestles, and getting Netlib ScaLAPACK to play nicely with either MKL or ACML's BLAS/LAPACK libraries when compiled against PGI and OpenMPI required tuning of the OpenMPI collectives.&lt;/li&gt;&lt;li&gt;In both test systems, &lt;b&gt;using vendor libraries wherever possible make Quantum ESPRESSO run more reliably&lt;/b&gt;. &amp;nbsp;The only roadblocks encountered when using MKL or ACML arose when they were combined with PGI and OpenMPI, where special collective tunings had to be done.&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;At the end of the day, there aren't many big surprises here. &amp;nbsp;There are three take-away lessons:&lt;br /&gt;&lt;ol&gt;&lt;li&gt;MKL provides very strong optimizations for the Intel x86 architecture, and ACML isn't so bad either. &amp;nbsp;You run into trouble when you start linking against Netlib libraries.&lt;/li&gt;&lt;li&gt;MVAPICH2 has better collectives than OpenMPI, and this translates into better ScaLAPACK performance. &amp;nbsp;Again, this becomes less true when you start linking against Netlib libraries.&lt;/li&gt;&lt;li&gt;&lt;b&gt;Don't use the Netlib reference implementations of BLAS, LAPACK, or ScaLAPACK&lt;/b&gt; because they aren't designed for performance or resiliency. &amp;nbsp;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Using Netlib caused performance to drop by between 60% and 400%&lt;/b&gt;, and&amp;nbsp;&lt;/li&gt;&lt;li&gt;&lt;b&gt;only half of the builds that linked against the Netlib reference trials would even run&lt;/b&gt;.&lt;/li&gt;&lt;/ul&gt;&lt;/ol&gt;&lt;div&gt;Friends don't let friends link against Netlib!&lt;/div&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Quantum ESPRESSO- Compiling and Choice of Libraries</title>
   <link href="https://hpc.social/2014/quantum-espresso-compiling-and-choice-of-libraries/"/>
   <updated>2014-02-25T00:59:00-07:00</updated>
   <id>https://hpc.social/2014/quantum-espresso-compiling-and-choice-of-libraries</id>
   <content type="html">&lt;div class=&quot;p1&quot;&gt;We recently upgraded our two big machines at work, and as a result of that upgrade, a number of our users had to rebuild their installation of Quantum ESPRESSO. &amp;nbsp;As it turns out, little quirks in our system conflicted with little quirks in Quantum ESPRESSO after the upgrade and resulted in the regular process of just doing &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;./configure&lt;/span&gt; and make not working out of the box.&lt;/div&gt;
&lt;div class=&quot;p1&quot;&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div class=&quot;p1&quot;&gt;Since I had been playing with Quantum ESPRESSO for the purpose of &lt;a href=&quot;http://glennklockwood.blogspot.com/2013/12/high-performance-virtualization-sr-iov_14.html&quot;&gt;benchmarking QDR InfiniBand virtualized with SR-IOV&lt;/a&gt;, I also took it upon myself to iron out exactly how to squeeze the best performance out of QE with respect to compilers, MPI stacks, and choice of linear algebra libraries. &amp;nbsp;For the sake of posterity (or at least until a new version of QE comes out that makes this all irrelevant), here are my notes.&lt;br /&gt;&lt;br /&gt;I also wrapped all of these build options into &lt;a href=&quot;https://github.com/sdsc/sdsc-user/blob/master/makefiles/espresso/build-espresso.sh&quot;&gt;a script that will configure and build optimized versions of Quantum ESPRESSO&lt;/a&gt; for various compiler and MPI combinations on the two machines I support at work.&lt;/div&gt;
&lt;div class=&quot;p1&quot;&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h2&gt;BLAS, LAPACK, and ScaLAPACK&lt;/h2&gt;
&lt;div class=&quot;p1&quot;&gt;Quantum ESPRESSO, like a multitude of other scientific codes, does a lot of linear algebra and uses the BLAS, LAPACK, and ScaLAPACK libraries to this end. &amp;nbsp;I have to shamefully admit that I never fully understood the relationship between these libraries before[1], but figuring out how to build Quantum ESPRESSO to deliver the best performance was a great excuse to sit down and get it straightened out.&lt;/div&gt;
&lt;div class=&quot;p1&quot;&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div class=&quot;p1&quot;&gt;BLAS, LAPACK, and ScaLAPACK are all libraries (and &lt;i&gt;de facto&lt;/i&gt; standard APIs) that provide increasing levels of abstraction to glue applications to underlying hardware. &amp;nbsp;This is the way I see this layering taking place:&lt;/div&gt;
&lt;div class=&quot;p1&quot;&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;http://4.bp.blogspot.com/-AMFxB0saoqI/UsfIweiCCPI/AAAAAAAAKLg/qWV-_P_d-V8/s1600/LAPACK+Stack.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;218&quot; src=&quot;http://4.bp.blogspot.com/-AMFxB0saoqI/UsfIweiCCPI/AAAAAAAAKLg/qWV-_P_d-V8/s400/LAPACK+Stack.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;p1&quot;&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div class=&quot;p1&quot;&gt;&lt;b&gt;BLAS&lt;/b&gt; is the lowest-level library and provides subroutines that do basic vector operations. &amp;nbsp;Netlib provides a &lt;a href=&quot;http://www.netlib.org/blas/&quot;&gt;reference implementation of BLAS written in Fortran&lt;/a&gt;, but the big idea behind BLAS is to allow hardware vendors to provide &lt;a href=&quot;http://www.netlib.org/blas/faq.html#5&quot;&gt;highly tuned versions of the BLAS subroutines&lt;/a&gt; that obviate the need for application developers to worry about optimizing their linear algebra for every possible computer architecture on which the application might run. &amp;nbsp;This motivation is also what gave rise to the MPI standard, but unlike MPI, BLAS is not an actual standard.&lt;/div&gt;
&lt;div class=&quot;p1&quot;&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div class=&quot;p1&quot;&gt;&lt;b&gt;LAPACK&lt;/b&gt; builds upon BLAS and provides higher-level matrix operations such as diagonalization (i.e., solving for eigenvectors and eigenvalues) and inversion. &amp;nbsp;BLAS and LAPACK seem to be bundled together when actually implemented (e.g., IBM ESSL and Intel MKL both provide both optimized BLAS and LAPACK), but they provide two distinct layers of abstracting the mathematical complexity away from application developers.&lt;/div&gt;
&lt;div class=&quot;p1&quot;&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div class=&quot;p1&quot;&gt;&lt;b&gt;ScaLAPACK&lt;/b&gt; builds upon LAPACK and provides a set of subroutines (prefixed with the letter P) that are analogous to the subroutines provided by LAPACK. &amp;nbsp;The big difference is that ScaLAPACK uses MPI to parallelize these LAPACK routines, whereas LAPACK itself (and the underlying BLAS) are completely serial (e.g., Netlib's reference distribution) or rely on shared memory for parallelization (e.g., multithreaded).&lt;/div&gt;
&lt;div class=&quot;p1&quot;&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div class=&quot;p1&quot;&gt;ScaLAPACK is where things get a little hairy because it not only relies on BLAS as an abstraction layer for doing computations, but it relies on the &lt;b&gt;BLACS&lt;/b&gt; library to abstract away the inter-node communications. &amp;nbsp;The MPI standard is supposed to do much of the same thing though, and in fact BLACS now only supports MPI, making it somewhat of an antiquated layer of abstraction. &amp;nbsp;It follows that most vendors seem to optimize their MPI libraries and leave BLACS unchanged relative to the reference distribution.&lt;/div&gt;
&lt;div class=&quot;p1&quot;&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div class=&quot;p1&quot;&gt;As I'll mention below, BLACS is a growing source of problems with ScaLAPACK. &amp;nbsp;BLACS is &lt;a href=&quot;http://mailman.cse.ohio-state.edu/pipermail/mvapich-discuss/2013-May/004434.html&quot;&gt;known to have non-deterministic behavior&lt;/a&gt; which renders it sensitive to the MPI implementation upon which is layered, causing ScaLAPACK to not work under similarly non-deterministic conditions.&lt;/div&gt;
&lt;div class=&quot;p1&quot;&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div class=&quot;p1&quot;&gt;&lt;span style=&quot;font-size: xx-small;&quot;&gt;[1] I have a compelling excuse though! &amp;nbsp;I got my start in scientific computing doing molecular dynamics simulations, and there just isn't a great deal of linear algebra required to calculate most models. &amp;nbsp;I did work on &lt;a href=&quot;http://dx.doi.org/10.1021/jp207181s&quot;&gt;an electronegativity-based model that required solving big systems of equations&lt;/a&gt;, but we found that there were more efficient ways to tackle the underlying physical problem like &lt;a href=&quot;http://dx.doi.org/10.1063/1.2206578&quot;&gt;using a clever extended Lagrangian methods&lt;/a&gt;.&lt;/span&gt;&lt;/div&gt;
&lt;div class=&quot;p1&quot;&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h2&gt;Building Quantum ESPRESSO&lt;/h2&gt;
&lt;div class=&quot;p1&quot;&gt;Customizing a build of Quantum ESPRESSO isn't completely standard compared to most non-scientific Linux packages, but it's miles ahead of most scientific packages in that it uses autoconf instead of a home-cooked build process.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Choice of Libraries&lt;/h3&gt;There are a few key factors to define when building Quantum ESPRESSO. &amp;nbsp;As you may have guessed from the previous section, they are (in no particular order):&lt;br /&gt;&lt;ul&gt;&lt;li&gt;choice of compiler&lt;/li&gt;&lt;li&gt;choice of MPI implementation&lt;/li&gt;&lt;li&gt;choice of BLAS library&lt;/li&gt;&lt;li&gt;choice of LAPACK library&lt;/li&gt;&lt;li&gt;choice of ScaLAPACK library&lt;/li&gt;&lt;li&gt;choice of FFT library&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;
&lt;div class=&quot;p1&quot;&gt;On most academic systems like SDSC's Gordon and Trestles, there are several options available for each one of these parameters, and figuring out (1) how to actually define your choice for each, and (2) determine which provides the best performance can be a bear. &amp;nbsp;What's worse is that these choices are often tied together; for example, the best ScaLAPACK implementation might not be compatible with the best FFT library.&lt;br /&gt;&lt;br /&gt;Gordon and Trestles provide the following options:&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;table style=&quot;margin: 0 auto;&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Compiler&lt;/th&gt;&lt;th&gt;Options&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;MPI&lt;/td&gt;&lt;td&gt;Intel and PGI&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;BLAS&lt;/td&gt;&lt;td&gt;MVAPICH2 and OpenMPI&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;LAPACK&lt;/td&gt;&lt;td&gt;MKL, ACML, and Netlib Reference&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;ScaLAPACK&lt;/td&gt;&lt;td&gt;MKL and Netlib Reference&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;FFTs&lt;/td&gt;&lt;td&gt;MKL, ACML, or FFTW3&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;There are actually more than this (e.g., GNU compilers and the MPICH implementation), but I did not test them.&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h3&gt;Passing Library Choices to the Build Process&lt;/h3&gt;&lt;div&gt;As of Quantum ESPRESSO 5.0.3, which is what I used here, you can't specify libraries in the autoconf-standard way (e.g., &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;--with-lapack=/opt/lapack/...&lt;/span&gt;). &amp;nbsp;I suspect this is because the actual implementations these libraries don't follow a standard convention (e.g., LAPACK calls aren't necessarily in a shared object called&amp;nbsp;&lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;liblapack.so&lt;/span&gt;), but the QE build process &lt;i&gt;does&lt;/i&gt; honor certain environment variables.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;&lt;b&gt;To specify compiler&lt;/b&gt;, you can simply set the &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;CC&lt;/span&gt;, &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;FC&lt;/span&gt;, and &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;F77&lt;/span&gt; environment variables as with any other application that uses autoconf, e.g.,&lt;/div&gt;
&lt;blockquote style=&quot;font-family: Courier New, Courier, monospace; font-size: smaller; text-align: left;&quot;&gt;export CC=icc&lt;br /&gt;export FC=ifort&lt;br /&gt;export F77=ifort&lt;/blockquote&gt;&lt;div&gt;QE will actually pick up any proprietary compiler in your &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;$PATH&lt;/span&gt; before it reverts to the GNU compilers, which is a surprisingly sensible approach. &amp;nbsp;On SDSC's machines, as long as you have the intel or pgi modules loaded, just plain old &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;./configure&lt;/span&gt; will pick it up.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;The MPI stack&lt;/b&gt; will be automatically detected based on whatever &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;mpif90&lt;/span&gt; is in your path. &amp;nbsp;Again, as long as you have a valid MPI module loaded (&lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;openmpi_ib&lt;/span&gt; or &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;mvapich2_ib&lt;/span&gt; on Gordon/Trestles), you don't have to do anything special.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;The BLAS implementation&lt;/b&gt; is selected by setting the &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;BLAS_LIBS&lt;/span&gt; environment variable to the appropriate link-time options. &amp;nbsp;For example, the Netlib reference BLAS compiled with the Intel compiler is installed in /opt/lapack/intel/lib on SDSC's machines; thus, your &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;BLAS_LIBS&lt;/span&gt; should be passed to configure as&lt;br /&gt;&lt;blockquote style=&quot;font-family: Courier New, Courier, monospace; font-size: smaller; text-align: left;&quot;&gt;export BLAS_LIBS=&quot;-L/opt/lapack/intel/lib -lblas&quot;&lt;/blockquote&gt;Similarly, &lt;b&gt;the LAPACK implementation&lt;/b&gt; can be specified using the &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;LAPACK_LIBS&lt;/span&gt; environment variable. &amp;nbsp;At SDSC, we install the Netlib BLAS and LAPACK in the same directory, so your &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;LAPACK_LIBS&lt;/span&gt; should actually contain the same library path as &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;BLAS_LIBS&lt;/span&gt;:&lt;br /&gt;&lt;blockquote style=&quot;font-family: Courier New, Courier, monospace; font-size: smaller; text-align: left;&quot;&gt;export LAPACK_LIBS=&quot;-L/opt/lapack/intel/lib -llapack&quot;&lt;/blockquote&gt;We (and many other supercomputing sites) provide a handy dandy environment variable when you load this &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;lapack&lt;/span&gt; module called &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;$LAPACKHOME&lt;/span&gt;. &amp;nbsp;With this environment variable, you can specify the generic (non-compiler-specific) line to configure:&lt;br /&gt;&lt;blockquote style=&quot;font-family: Courier New, Courier, monospace; font-size: smaller; text-align: left;&quot;&gt;export BLAS_LIBS=&quot;-L$LAPACKHOME/lib -lblas&quot; &lt;br /&gt;export LAPACK_LIBS=&quot;-L$LAPACKHOME/lib -llapack&quot;&lt;/blockquote&gt;for convenience.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;The ScaLAPACK libraries&lt;/b&gt; are much the same and are passed to autoconf via the SCALAPACK_LIBS environment variable. &amp;nbsp;To use the Netlib reference on Gordon/Trestles, you can load the &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;scalapack&lt;/span&gt; module and to configure:&lt;br /&gt;&lt;blockquote style=&quot;font-family: Courier New, Courier, monospace; font-size: smaller; text-align: left;&quot;&gt;export SCALAPACK_LIBS=&quot;-L$SCALAPACKHOME/lib -lscalapack&quot;&lt;/blockquote&gt;Finally, &lt;b&gt;the&amp;nbsp;FFT libraries&lt;/b&gt; are defined via the &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;FFT_LIBS&lt;/span&gt; environment variable. &amp;nbsp;To use our fftw installation, &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;module load fftw&lt;/span&gt; and configure:&lt;br /&gt;&lt;blockquote style=&quot;font-family: Courier New, Courier, monospace; font-size: smaller; text-align: left;&quot;&gt;export FFT_LIBS=&quot;-L$FFTWHOME/lib -lfftw3&quot;&lt;/blockquote&gt;This is all well and good, but using the reference implementations for BLAS and LAPACK, as I will show, will result in very poor performance.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Using Vendor-Optimized Libraries&lt;/h3&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h4&gt;Intel&lt;/h4&gt;Since none of these libraries are really standardized, vendors are free to bury their API wrappers in whatever libraries they want and support them to whatever extent they want. &amp;nbsp;Intel's compilers come bundled with their Math Kernel Library (MKL) which provides bindings for&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;BLAS:&lt;/b&gt;&lt;br /&gt;&lt;span style=&quot;font-family: Courier New, Courier, monospace; font-size: smaller;&quot;&gt;BLAS_LIBS=&quot;-lmkl_intel_lp64 -lmkl_sequential -lmkl_core&quot;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;LAPACK:&lt;/b&gt;&lt;br /&gt;&lt;span style=&quot;font-family: Courier New, Courier, monospace; font-size: smaller;&quot;&gt;LAPACK_LIBS&lt;/span&gt; can be left as the default since BLAS and LAPACK are buried in the same libraries&lt;/li&gt;&lt;li&gt;&lt;b&gt;ScaLAPACK/BLACS:&lt;/b&gt;&lt;br /&gt;&lt;span style=&quot;font-family: Courier New, Courier, monospace; font-size: smaller;&quot;&gt;SCALAPACK_LIBS=&quot;-lmkl_scalapack_lp64 -lmkl_blacs_openmpi_lp64&quot;&lt;/span&gt; for OpenMPI &lt;b&gt;OR&lt;/b&gt;&lt;br /&gt;&lt;span style=&quot;font-family: Courier New, Courier, monospace; font-size: smaller;&quot;&gt;SCALAPACK_LIBS=&quot;-lmkl_scalapack_lp64 -lmkl_blacs_intelmpi_lp64&quot;&lt;/span&gt; for MVAPICH2&lt;/li&gt;&lt;li&gt;&lt;b&gt;FFTW&lt;/b&gt;:&lt;br /&gt;&lt;span style=&quot;font-family: Courier New, Courier, monospace; font-size: smaller;&quot;&gt;FFT_LIBS=&quot;-lmkl_intel_lp64 -lmkl_sequential -lmkl_core&quot;&lt;/span&gt; for modern versions of MKL; older versions had the FFTW3 bindings in a separate library&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;so your final configure command should look something like&lt;br /&gt;&lt;blockquote style=&quot;font-family: 'Courier New', Courier, monospace; font-size: smaller;&quot;&gt;./configure \&lt;br /&gt;&amp;nbsp; CC=icc \&lt;br /&gt;&amp;nbsp; CXX=icpc \&lt;br /&gt;&amp;nbsp; FC=ifort \&lt;br /&gt;&amp;nbsp; F77=ifort \&lt;br /&gt;&amp;nbsp; BLAS_LIBS=&quot;-lmkl_intel_lp64 -lmkl_sequential -lmkl_core&quot; \&lt;br /&gt;&amp;nbsp; SCALAPACK_LIBS=&quot;-lmkl_scalapack_lp64 -lmkl_blacs_openmpi_lp64&quot; \&lt;br /&gt;&amp;nbsp; FFT_LIBS=&quot;-lmkl_intel_lp64 -lmkl_sequential -lmkl_core&quot;&lt;/blockquote&gt;when compiling with OpenMPI, or with a slightly modified &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;SCALAPACK_LIBS&lt;/span&gt; line (&lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;-lmkl_blacs_intelmpi_lp64&lt;/span&gt;) when compiling with MVAPICH2.&lt;br /&gt;&lt;br /&gt;&lt;h4&gt;PGI/AMD&lt;/h4&gt;PGI's compilers come bundled with the AMD Core Math Library (ACML), which provides bindings for BLAS, LAPACK, and FFTW, but its lack of ScaLAPACK means we still must use Netlib's ScaLAPACK and BLACS libraries. &amp;nbsp;Be sure to load the pgi module, your preferred MPI module, and the scalapack module first!&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;BLAS:&lt;/b&gt;&lt;br /&gt;&lt;span style=&quot;font-family: Courier New, Courier, monospace; font-size: smaller;&quot;&gt;BLAS_LIBS=&quot;-L$PGIHOME/libso -lacml&quot;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;LAPACK:&lt;/b&gt;&lt;br /&gt;&lt;span style=&quot;font-family: Courier New, Courier, monospace; font-size: smaller;&quot;&gt;LAPACK_LIBS&lt;/span&gt;&amp;nbsp;can be left as the default since BLAS and LAPACK are buried in the same ACML library&lt;/li&gt;&lt;li&gt;&lt;b&gt;ScaLAPACK/BLACS:&lt;/b&gt;&lt;br /&gt;&lt;span style=&quot;font-family: Courier New, Courier, monospace; font-size: smaller;&quot;&gt;SCALAPACK_LIBS=&quot;-L$SCALAPACKHOME/lib -lscalapack&quot;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;FFTW&lt;/b&gt;:&lt;br /&gt;&lt;span style=&quot;font-family: Courier New, Courier, monospace; font-size: smaller;&quot;&gt;FFT_LIBS=&quot;-L$PGIHOME/libso -lacml&quot; &lt;/span&gt;even though ACML is included in the &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;$BLAS_LIBS&lt;/span&gt; variable--this is because autoconf may pick up a system fftw library which needs to be superceded by the FFTW bindings in ACML.&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;so your final configure command should look something like&lt;br /&gt;&lt;blockquote style=&quot;font-family: 'Courier New', Courier, monospace; font-size: smaller;&quot;&gt;./configure \&lt;br /&gt;&amp;nbsp; CC=pgcc \&lt;br /&gt;&amp;nbsp; CXX=pgCC \&lt;br /&gt;&amp;nbsp; FC=pgf90 \&lt;br /&gt;&amp;nbsp; F77=pgf77 \&lt;br /&gt;&amp;nbsp; BLAS_LIBS=&quot;-L$PGIHOME/libso -lacml&quot; \&lt;br /&gt;&amp;nbsp; SCALAPACK_LIBS=&quot;-L$SCALAPACKHOME/lib -lscalapack&quot; \&lt;br /&gt;&amp;nbsp; FFT_LIBS=&quot;-L$PGIHOME/libso -lacml&quot;&lt;/blockquote&gt;After doing this, there is one additional bit of manual hacking that must be done! &amp;nbsp;PGI is known to trigger problems in Quantum ESPRESSO's IO library, IOTK, and you will need to compile with the &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;-D__IOTK_WORKAROUND1&lt;/span&gt; switch enabled. &amp;nbsp;This command will hack the necessary line in &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;make.sys&lt;/span&gt;:&lt;br /&gt;&lt;blockquote&gt;&lt;span style=&quot;font-family: Courier New, Courier, monospace; font-size: x-small;&quot;&gt;sed -i 's/^DFLAGS\(.*\)$/DFLAGS\1 -D__IOTK_WORKAROUND1/' make.sys&lt;/span&gt;&lt;/blockquote&gt;I owe a lot of gratitude to &lt;a href=&quot;http://filippospiga.me/&quot;&gt;Filippo Spiga&lt;/a&gt; of Cambridge/the Quantum ESPRESSO Foundation for helping me quickly work through some of the issues I encountered in getting all of these builds to work correctly.&lt;br /&gt;&lt;br /&gt;In my next post, I will show what effect all of these options has on actual application performance.&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Linux perf, libquadmath, and GFortran's Insane Behavior</title>
   <link href="https://hpc.social/2014/linux-perf-libquadmath-and-gfortran-s-insane-behavior/"/>
   <updated>2014-02-12T20:20:00-07:00</updated>
   <id>https://hpc.social/2014/linux-perf-libquadmath-and-gfortran-s-insane-behavior</id>
   <content type="html">&lt;p&gt;&lt;b&gt;Executive Summary&lt;/b&gt;: &lt;a href=&quot;http://gcc.gnu.org/onlinedocs/libquadmath/&quot;&gt;libquadmath&lt;/a&gt; was introduced in GFortran 4.6 which fundamentally changed what the &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;-fdefault-real-8&lt;/span&gt; switch does.  Rather than promoting all floating point arithmetic to double precision, it doubles the width of all floating point types, so explicitly typed double precision is converted to quad precision.  This quad precision is orders of magnitude slower since it must be done in software, causing binaries built with &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;-fdefault-real-8&lt;/span&gt; to grind to a halt when built with GFortran 4.6 and newer.  The solution is to add &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;-fdefault-double-8&lt;/span&gt; to undo this implicit doubling of explicit &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;real*8&lt;/span&gt;.&lt;br /&gt;&lt;br /&gt;What follows is a case study of sorts in how I discovered this.  Maybe my methodology will be useful for others who are tasked with debugging performance problems.&lt;br /&gt;&lt;br /&gt;&amp;lt;h2 id=&quot;prob&quot;&amp;gt;The Problem&amp;lt;/h2&amp;gt;A colleague from my past in research science sent me an e-mail this morning with a very typical problem that people run into whenever they try transferring their applications from one machine to another.  He wrote,&lt;br /&gt;&amp;lt;blockquote class=&quot;tr_bq&quot;&amp;gt;“I’ve been having a problem with compiling on a new workstation that is an HP with the newer gcc/gfortran 4.6.3.  The executable for the code runs very slow.  If I compile the exact same on the cluster or one of the Dell workstations (both have gfortran 4.4.3) it runs very fast on both.  Also, if I transfer the compiled binary from the cluster to the new HP workstation, it runs fast.”&amp;lt;/blockquote&amp;gt;That is to say,&lt;br /&gt;&lt;br /&gt;&amp;lt;table style=&quot;border-collapse: collapse; border: 1px solid black; margin: 0 auto; text-align: center;&quot;&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr style=&quot;border: 1px solid black;&quot;&amp;gt;&amp;lt;th&amp;gt;&amp;lt;/th&amp;gt;&amp;lt;th style=&quot;border: 1px solid black;&quot;&amp;gt;Run on New &lt;br /&gt;Workstation&amp;lt;/th&amp;gt;&amp;lt;th style=&quot;border: 1px solid black;&quot;&amp;gt;Run on Old&lt;br /&gt;Workstation&amp;lt;/th&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr style=&quot;border: 1px solid black;&quot;&amp;gt;&amp;lt;th style=&quot;border: 1px solid black; text-align: right;&quot;&amp;gt;Compiled on &lt;br /&gt;New Workstation&amp;lt;/th&amp;gt;&amp;lt;td style=&quot;background: #ff7777; border: 1px solid black;&quot;&amp;gt;SLOW&amp;lt;/td&amp;gt;&amp;lt;td style=&quot;border: 1px solid black;&quot;&amp;gt;?&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;th style=&quot;text-align: right;&quot;&amp;gt;Compiled on &lt;br /&gt;Old Workstation&amp;lt;/th&amp;gt;&amp;lt;td style=&quot;background: #77ff77; border: 1px solid black;&quot;&amp;gt;FAST&amp;lt;/td&amp;gt;&amp;lt;td style=&quot;background: #77ff77;&quot;&amp;gt;FAST&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&lt;br /&gt;&amp;lt;div&amp;gt;The fact that the old binary ran fast on the new machine ruled out some odd hardware or kernel problem and suggested that the issue was somewhere in userland.  Userland issues are always fixable issues, so this alone suggests that there will be a solution to this issue if we dig deep enough.&amp;lt;/div&amp;gt;&lt;/p&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h2 id=&quot;arcane&quot;&gt;A Little Logic, A Little Arcane Knowledge&lt;/h2&gt;
&lt;div&gt;The difference in performance was probably related to the upgrade from GFortran 4.4 to GFortran 4.6, and just to make sure this was a well-defined problem, I re-built the application and ran the test case on a local machine to ensure that the problem was reproducible on hardware and an OS with which I was familiar. &amp;nbsp;I built with&lt;/div&gt;
&lt;div&gt;&lt;ul&gt;&lt;li&gt;The GFortran 4.4 that ships with Red Hat 6. &amp;nbsp;My colleague said that his build with GFortran 4.4 ran fine, and I was able to confirm that &lt;b&gt;GFortran 4.4 produced a reliably fast executable&lt;/b&gt;.&lt;/li&gt;&lt;li&gt;The GFortran 4.6 that ships with Ubuntu 12.04 (my colleague's machine). &amp;nbsp;He said that this one ran very slowly, and I could confirm that &lt;b&gt;GFortran 4.6 did, indeed, produce an unusably slow binary&lt;/b&gt;.&amp;nbsp;&lt;/li&gt;&lt;li&gt;The GFortran 4.8 that I built as a &quot;latest-and-greatest&quot; version on my test system. &amp;nbsp;I wanted to verify that there wasn't some bug in 4.6 that was patched out of subsequent releases. &amp;nbsp;Unfortunately this was not the case, as &lt;b&gt;GFortran 4.8 also produced a very slow binary&lt;/b&gt;.&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;The good news was that the problem is reproducible and we have a baseline case where the application &lt;i&gt;does&lt;/i&gt; behave as intended. &amp;nbsp;This meant that, in the worst-case scenario, we can do line-by-line comparisons of the assembly code for the working and non-working binaries to see where the problem lies. &amp;nbsp;Thus, we know the problem has a solution.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;Of course, the bad news was that some change made between GFortran 4.4 and GFortran 4.6 broke this code, and we have to figure out exactly what this change was.&lt;/div&gt;
&lt;div&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div&gt;This is where arcane knowledge comes in: I know two facts about GCC that suggest this may be, in fact, a problem with GFortran:&lt;/div&gt;
&lt;/div&gt;
&lt;div&gt;&lt;ol&gt;&lt;li&gt;GFortran has been known to throw backwards compatibility to the wind and make wild changes default behavior. &amp;nbsp;For example, g77 and GFortran 4.1 used 8-byte record marker lengths by default, but then &lt;a href=&quot;http://gcc.gnu.org/wiki/GFortran/News#gfortran_4.2&quot;&gt;switched over to 4-byte markers in GFortran 4.2 to be in line with what every other Fortran compiler does&lt;/a&gt;. &amp;nbsp;This meant that data generated by GFortran 4.1 was not compatible with anything else. &amp;nbsp;It wouldn't have surprised me if they did this sort of thing again.&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;http://gcc.gnu.org/wiki/GFortran/News#gfortran_4.6&quot;&gt;GCC introduced libquadmath in version 4.6&lt;/a&gt; which made all GFortran objects built with 4.6 or later pull in libquadmath. &amp;nbsp;This used to cause me problems because Red Hat 5 did not ship with libquadmath, making all binaries dynamically linked against GFortran 4.6 not portable* to RHEL5. &amp;nbsp;Thus, this issue might have something to do with the addition of libquadmath.&lt;/li&gt;&lt;/ol&gt;&lt;div&gt;&lt;span style=&quot;font-size: xx-small;&quot;&gt;* I acknowledge that trying to move binaries between machines is pretty crazy in its own right. &amp;nbsp;Explaining why this was an actual issue for me is both uninteresting and beyond the scope of this post.&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h2 id=&quot;perfuse&quot;&gt;Examining Baseline Performance&lt;/h2&gt;&lt;/div&gt;
&lt;div&gt;All modern Linux kernels ship with the &lt;a href=&quot;https://perf.wiki.kernel.org/&quot;&gt;perf subsystem&lt;/a&gt; which makes diagnosing performance problems significantly easier than it has been in the past. &amp;nbsp;If you haven't familiarized yourself with them yet, you really need to--all it took for me was a 2-minute demo by&amp;nbsp;&lt;a class=&quot;g-profile&quot; href=&quot;https://plus.google.com/109775321689856324025&quot; target=&quot;_blank&quot;&gt;+Peter Kjellström&lt;/a&gt;&amp;nbsp;at SC'13 last year to realize Linux perf is serious business. &amp;nbsp;We will simply use it as an alternative to gprof in this case so that we don't have to re-build all this code with instrumentation, but &lt;a href=&quot;https://perf.wiki.kernel.org/index.php/Tutorial&quot;&gt;perf can also do a lot of things&lt;/a&gt; that used to be the exclusive domain of special-purpose libraries like &lt;a href=&quot;http://icl.cs.utk.edu/papi/&quot;&gt;PAPI&lt;/a&gt; and &lt;a href=&quot;http://ipm-hpc.org/&quot;&gt;IPM&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;Running the &quot;good&quot; build of this application through&lt;sup&gt;†&lt;/sup&gt; perf establishes our baseline expected behavior:&lt;br /&gt;&lt;br /&gt;&lt;pre style=&quot;font-family: monospace; font-size: smaller; margin-left: 2em;&quot;&gt;$ &lt;span style=&quot;color: #0b5394;&quot;&gt;&lt;b&gt;perf record -o fast.report -g ./mdvgg.x&lt;/b&gt;&lt;/span&gt;&lt;br /&gt;WARNING: Kernel address maps (/proc/{kallsyms,modules}) are restricted,&lt;br /&gt;check /proc/sys/kernel/kptr_restrict.&lt;br /&gt;&lt;br /&gt;Samples in kernel functions may not be resolved if a suitable vmlinux&lt;br /&gt;file is not found in the buildid cache or in the vmlinux path.&lt;br /&gt;&lt;br /&gt;Samples in kernel modules won't be resolved at all.&lt;br /&gt;&lt;br /&gt;If some relocation was applied (e.g. kexec) symbols may be misresolved&lt;br /&gt;even with a suitable vmlinux or kallsyms file.&lt;br /&gt;&lt;br /&gt;&lt;span style=&quot;color: #999999;&quot;&gt; Pressure list found&lt;br /&gt; taux,y,z:         0.000000       0.000000       0.000000&lt;br /&gt; txx0,tyy0,tzz0:   1013250.       1013250.       1013250.&lt;br /&gt; Wolf beta value:  4.4600E-008&lt;br /&gt;&lt;br /&gt;***Lennard-Jones parameters, epsilons in ergs ***                               &lt;br /&gt; 0.00000D+00    0.00000D+00    0.00000D+00    0.00000D+00    0.00000D+00&lt;br /&gt;...&lt;br /&gt;  average energy per atom:        -0.39932E-11    -57.4739kcal/mole&lt;br /&gt;  average energy with selfterm:   -0.51864E-11    -74.6481kcal/mole&lt;/span&gt;&lt;br /&gt;[ perf record: Woken up 3 times to write data ]&lt;br /&gt;[ perf record: Captured and wrote 0.895 MB fast.report (~39121 samples) ]&lt;br /&gt;&lt;/pre&gt;where&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;-o fast.report&lt;/span&gt; dumps the recorded data to a file called &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;fast.report&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;-g&lt;/span&gt; generates call graphs in addition to the flat profile (this isn't always necessary)&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;./mdvgg.x&lt;/span&gt; is the application binary we are profiling&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;The scary warnings about kernel functions are harmless and a result of this entire debugging process being run as an unprivileged user. &amp;nbsp;Once the job finishes running, viewing the report reveals (with some extraneous data removed for brevity):&lt;br /&gt;&lt;br /&gt;&lt;pre style=&quot;font-family: monospace; font-size: smaller; margin-left: 2em;&quot;&gt;$ &lt;span style=&quot;color: #0b5394;&quot;&gt;perf report -i fast.report --stdio --sort dso -g flat&lt;/span&gt;&lt;br /&gt;...&lt;/pre&gt;&lt;pre style=&quot;font-family: monospace; font-size: smaller; margin-left: 2em;&quot;&gt;# Overhead  Command         Shared Object&lt;br /&gt;# ........  .......  ....................&lt;br /&gt;#&lt;br /&gt;    72.13%  mdvgg.x  mdvgg.x             &lt;br /&gt;            61.12%&lt;br /&gt;                &lt;b&gt;&lt;span style=&quot;color: red;&quot;&gt;pairs_&lt;/span&gt;&lt;/b&gt;&lt;br /&gt;                &lt;b&gt;&lt;span style=&quot;color: blue;&quot;&gt;move1_&lt;/span&gt;&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;             7.00%&lt;br /&gt;                listwater_&lt;br /&gt;                bulk_&lt;br /&gt;                MAIN__&lt;br /&gt;                0x400efd&lt;br /&gt;...&lt;br /&gt;    20.99%  mdvgg.x  libc-2.12.so        &lt;br /&gt;            14.09%&lt;br /&gt;                &lt;b&gt;&lt;span style=&quot;color: magenta;&quot;&gt;__memset_sse2&lt;/span&gt;&lt;/b&gt;&lt;br /&gt;                bulk_&lt;br /&gt;                MAIN__&lt;br /&gt;                0x400efd&lt;br /&gt;&lt;br /&gt;             0.97%&lt;br /&gt;                &lt;b&gt;&lt;span style=&quot;color: magenta;&quot;&gt;__memset_sse2&lt;/span&gt;&lt;/b&gt;&lt;br /&gt;                MAIN__&lt;br /&gt;                0x400efd&lt;br /&gt;&lt;/pre&gt;where&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;-i fast.report&lt;/span&gt; is the file containing our recorded data&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;--stdio&lt;/span&gt; prevents perf from using the interactive text user interface (I only added this because I can't paste interactions into a blog)&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;--sort dso&lt;/span&gt; presents the output in a relatively compact way sorted by the shared object in which time was being spent&lt;/li&gt;&lt;li&gt;&lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;-g flat&lt;/span&gt; presents a relatively flat profile (we don't need the full call graph)&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;Thus, the majority of our runtime is taken up in a subroutine called &lt;span style=&quot;color: red; font-family: Courier New, Courier, monospace;&quot;&gt;&lt;b&gt;pairs&lt;/b&gt;&lt;/span&gt;, called from &lt;span style=&quot;color: blue; font-family: Courier New, Courier, monospace;&quot;&gt;&lt;b&gt;move1&lt;/b&gt;&lt;/span&gt;&amp;nbsp;when this application is working normally. &amp;nbsp;A surprising fraction of runtime was also consumed by &lt;span style=&quot;color: magenta; font-family: Courier New, Courier, monospace;&quot;&gt;&lt;b&gt;memset(3)&lt;/b&gt;&lt;/span&gt;&amp;nbsp;in this case, but this was the result of my test input being so small that most of the actual runtime was spent doing initialization. &amp;nbsp;Even though this is generally not a great way to test application performance, it is acceptable in this case because even initialization takes 20x longer with the &quot;bad&quot; binary built against GFortran 4.6 &lt;span style=&quot;font-size: xx-small;&quot;&gt;(which in itself is a very insightful behavior that suggests that there is something systematically wrong with the bad binary)&lt;/span&gt;. &amp;nbsp;The simplest and shortest possible run required to reproduce the issue should elucidate where the problem lies.&lt;br /&gt;&lt;br /&gt;Now, profiling the &quot;bad&quot; binary built with GFortran 4.6 should give us a definite place to start looking:&lt;br /&gt;&lt;br /&gt;&lt;pre style=&quot;font-family: monospace; font-size: smaller; margin-left: 2em;&quot;&gt;$ &lt;span style=&quot;color: #0b5394;&quot;&gt;&lt;b&gt;perf record -o slow.report -g ./mdvgg.x&lt;/b&gt;&lt;/span&gt;&lt;br /&gt;WARNING: Kernel address maps (/proc/{kallsyms,modules}) are restricted,&lt;br /&gt;check /proc/sys/kernel/kptr_restrict.&lt;br /&gt;...&lt;br /&gt;&lt;br /&gt;$ &lt;span style=&quot;color: #0b5394;&quot;&gt;&lt;b&gt;perf report -i slow.report --stdio --sort dso -g flat&lt;/b&gt;&lt;/span&gt;&lt;br /&gt;...&lt;br /&gt;# Overhead         Shared Object&lt;br /&gt;# ........  ....................&lt;br /&gt;#&lt;br /&gt;    93.59%  libgcc_s.so.1       &lt;br /&gt;            48.69%&lt;br /&gt;                &lt;b&gt;&lt;span style=&quot;color: blue;&quot;&gt;__sfp_handle_exceptions&lt;/span&gt;&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;            13.54%&lt;br /&gt;                &lt;b&gt;&lt;span style=&quot;color: magenta;&quot;&gt;__multf3&lt;/span&gt;&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;             6.89%&lt;br /&gt;                &lt;b&gt;&lt;span style=&quot;color: magenta;&quot;&gt;__addtf3&lt;/span&gt;&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;             6.16%&lt;br /&gt;                &lt;b&gt;&lt;span style=&quot;color: magenta;&quot;&gt;__subtf3&lt;/span&gt;&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;...&lt;br /&gt;     3.02%  libquadmath.so.0.0.0&lt;br /&gt;             1.62%&lt;br /&gt;                __sfp_handle_exceptions&lt;br /&gt;&lt;br /&gt;     &lt;b&gt;&lt;span style=&quot;color: red;&quot;&gt;2.67%  mdvgg.x&lt;/span&gt;&lt;/b&gt;             &lt;br /&gt;             1.91%&lt;br /&gt;                hcristo_&lt;br /&gt;                fcc100_&lt;br /&gt;&lt;br /&gt;...&lt;/pre&gt;&lt;br /&gt;Well there's our problem! &amp;nbsp;Only &lt;b&gt;&lt;span style=&quot;color: red;&quot;&gt;2.67%&lt;/span&gt;&lt;/b&gt; of the application runtime is actually being spent running the &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;mdvgg.x&lt;/span&gt; application, and a huge amount of time is being spent in some &lt;span style=&quot;color: blue; font-family: Courier New, Courier, monospace;&quot;&gt;&lt;b&gt;__sfp_handle_exceptions&lt;/b&gt;&lt;/span&gt; call. &amp;nbsp;What gives?&lt;br /&gt;&lt;br /&gt;Now I'm not ashamed to say that I routinely turn to Google to figure out what most of this sort of computer nonsense means. &amp;nbsp;Unfortunately, searching for &quot;&lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;__sfp_handle_exceptions&lt;/span&gt;&quot; doesn't turn up anything useful, so the only hint we have is that the name of the call suggests that this &quot;bad&quot; build is generating a lot of floating point exceptions (FPEs). &lt;br /&gt;&lt;br /&gt;The logical next step is to rebuild the application with a lot of FPE trapping (&lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;FCFLAGS+=-ffpe-trap=invalid,zero,overflow,underflow,denormal&lt;/span&gt;). &amp;nbsp;This will determine if the code had been generating a ton of floating point exceptions all along but GFortran had just gotten stricter in 4.6. &amp;nbsp;Unfortunately, doing this just leads to more disappointment--the application does not generate any of the common floating point exceptions, meaning that this mysterious &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;__sfp_handle_exceptions&lt;/span&gt; is, in fact, not handling serious floating point exceptions. &amp;nbsp;What else could it be doing?&lt;br /&gt;&lt;br /&gt;&lt;span style=&quot;font-size: xx-small;&quot;&gt;&lt;sup&gt;†&lt;/sup&gt; Although this particular application was both quick enough to run entirely through perf and serial enough to not require any special considerations with MPI, getting these perf profiles from long-running and highly parallel codes is similarly easy. &amp;nbsp;Instead of running the application through perf (&lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;perf record -o fast.report -g ./mdvgg.x&lt;/span&gt;) you can attach perf to an already-running process for a fixed period of time to generate a sample of the overall performance profile. &amp;nbsp;This is achieved by doing &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;perf record -o fast.report -g -p &lt;i&gt;&lt;/i&gt; sleep 10&lt;/span&gt;. &amp;nbsp;Perf attaches to the specified pid and gathers data from it, and just sleeps for ten seconds before detaching.&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;h2 id=&quot;diagnosis&quot;&gt;Quad-Precision: Back to the Intel 80286&lt;/h2&gt;Giving up on &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;__sftp_handle_exceptions&lt;/span&gt; and moving on down the performance profile, it appears that suddenly libquadmath (which, as I mentioned above, appeared after our &quot;working&quot; compiler version was released) is soaking up cycles. &amp;nbsp;Furthermore, a quick googling of some of those big offenders like &lt;span style=&quot;color: magenta; font-family: Courier New, Courier, monospace;&quot;&gt;&lt;b&gt;__multf3&lt;/b&gt;&lt;/span&gt;, &lt;span style=&quot;color: magenta; font-family: Courier New, Courier, monospace;&quot;&gt;&lt;b&gt;__addtf3&lt;/b&gt;&lt;/span&gt;, and &lt;span style=&quot;color: magenta; font-family: Courier New, Courier, monospace;&quot;&gt;&lt;b&gt;__subtf3&lt;/b&gt;&lt;/span&gt; reveals that &lt;b&gt;they are software implementations of long-double arithmetic&lt;/b&gt;--the application is now doing quad precision arithmetic in this &quot;bad&quot; build whereas it is definitely not doing this in our &quot;good&quot; build.&lt;br /&gt;&lt;br /&gt;Suddenly everything becomes a little clearer: long-double floating point arithmetic involves numbers stored in 128-bit precision, but 64-bit CPUs &lt;span style=&quot;font-size: xx-small;&quot;&gt;(or more properly, FPUs)&lt;/span&gt; are only capable of handling (you guessed it) 64-bit precision floating point calculations. &amp;nbsp;Thus, to get an application to do calculations in 128-bit precision, a software layer (libquadmath) must emulate 128-bit floating point hardware and actually translate the binary logic into something the 64-bit CPU can understand. &amp;nbsp;This is analogous to getting a 3rd grader to do a large calculation (e.g., 6×8) by breaking into pieces they know how to solve (e.g., 8+8, 8+8, 8+8), and it is a &lt;i&gt;very&lt;/i&gt; slow process. &amp;nbsp;This massive performance loss is why Intel has had a hardware floating point unit in every processor it's designed since the 20386 (ca. 1985).&lt;br /&gt;&lt;br /&gt;The obvious question is then why GFortran 4.6 has decided to start carrying out all of the calculations in this code in quad precision by default. &amp;nbsp;Surely the GFortran developers didn't think forcing all arithmetic to be done in software was a good idea, right?&lt;br /&gt;&lt;br /&gt;&lt;h2 id=&quot;gf&quot;&gt;Redefining Default Behavior&lt;/h2&gt;Of course not. &lt;br /&gt;&lt;br /&gt;The next challenge, then, is to dig through the GFortran 4.6 manual to figure out what the libquadmath integration did to default behavior, or alternatively, what compiler flags started changing the precision of variables and calculations automatically.&lt;br /&gt;&lt;br /&gt;This is where knowledge of Fortran becomes important, because an unfortunate aspect of F77 (which has carried forward in F90) is its implicit typing. &amp;nbsp;A novice Fortran programmer (like a new graduate student) may think that doing something like&lt;br /&gt;&lt;br /&gt;&lt;pre style=&quot;margin-left: 2em;&quot;&gt;implicit real*8(a-h,o-z)&lt;br /&gt;value1 = 0.31415926535e+1&lt;/pre&gt;&lt;br /&gt;will store a double-precision (&lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;real*8&lt;/span&gt;) value in &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;value1&lt;/span&gt;. &amp;nbsp;This isn't the case, as the &quot;&lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;e+1&lt;/span&gt;&quot; instead of &quot;&lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;d+1&lt;/span&gt;&quot; tends to render this a single-precision value. &amp;nbsp;This isn't &lt;i&gt;always&lt;/i&gt; the case, but let it suffice to say that the details get messy and I've seen different compilers handle this in different ways by default.&lt;br /&gt;&lt;br /&gt;Anyway, every Fortran compiler has options to override this implicit typing and force all floating point values into double precision. &amp;nbsp;In GFortran, this has traditionally been &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;-fdefault-real-8&lt;/span&gt;; that is, the default data type for real (i.e., single-precision) values is &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;real*8&lt;/span&gt;, or double precision. &amp;nbsp;In this particular code's makefile, this flag was enabled to override the sloppy coding practices of decades of graduate students and ensure precision wasn't going down the drain because someone used E's instead of D's in 1997.&lt;br /&gt;&lt;br /&gt;When a simple search for &quot;quadmath&quot; in the GFortran 4.6 manual turns up nothing, searching for &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;-fdefault-real-8&lt;/span&gt; is the next step. &amp;nbsp;Lo and behold, &lt;a href=&quot;http://gcc.gnu.org/onlinedocs/gcc-4.6.3/gfortran/Fortran-Dialect-Options.html&quot;&gt;this gem appears&lt;/a&gt;:&lt;br /&gt;&lt;blockquote class=&quot;tr_bq&quot;&gt;&lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;-fdefault-real-8&lt;/span&gt;&lt;br /&gt;Set the default real type to an 8 byte wide type. Do nothing if this is already the default. This option also affects the kind of non-double real constants like 1.0, and &lt;b&gt;does promote the default width of DOUBLE PRECISION to 16 bytes if possible&lt;/b&gt;, unless &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;-fdefault-double-8&lt;/span&gt; is given, too.&amp;nbsp;&lt;/blockquote&gt;Bingo. &amp;nbsp;Any code that previously used &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;-fdefault-real-8&lt;/span&gt; to ensure that all floating point arithmetic was being done in double precision now does &lt;b&gt;all explicitly typed double precision arithmetic as 128-bit quad precision in software as its effective default behavior&lt;/b&gt;. &amp;nbsp;What's worse is that this change in behavior &lt;a href=&quot;http://gcc.gnu.org/gcc-4.6/changes.html#fortran&quot;&gt;isn't even mentioned in the release notes for GFortran 4.6&lt;/a&gt;&amp;nbsp;because &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;-fdefault-real-8&lt;/span&gt; has always &lt;i&gt;tried&lt;/i&gt; to promote &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;real*8&lt;/span&gt; to &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;real*16&lt;/span&gt; as its intended behavior; it simply never succeeded because GFortran didn't support software quad-precision before libquadmath appeared in 4.6.&lt;br /&gt;&lt;br /&gt;Quite frankly, defining the behavior of something as straightforward-sounding as &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;-fdefault-real-8&lt;/span&gt; to be so environment-specific is insane. &amp;nbsp;The only use case where this new behavior would even make sense is if a programmer intentionally mixes &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;real*4&lt;/span&gt; and &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;real*8&lt;/span&gt; datatypes within code and wants to see what will happen if all variable widths are doubled uniformly. &amp;nbsp;On the other hand if &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;-fdefault-real-8&lt;/span&gt; was being used to ensure all calculations were done in double-precision (as was the case in this application and at least a few other unrelated scientific codes with which I have worked), performance takes a catastrophic hit simply because a new quad-precision math library is bundled with GCC.&lt;br /&gt;&lt;br /&gt;It would make more sense if GFortran added a &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;-fdefault-real-16&lt;/span&gt; (a la Intel Fortran's &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;-real-size 128&lt;/span&gt; switch) to promote all floating point to quad precision. &amp;nbsp;In fact, I find it difficult to make sense of GFortran's choice to make &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;-fdefault-real-8&lt;/span&gt; preserve mixed precision codes as it does; the only case where I can envision this sort of behavior being useful is in codes that implement their own reduced-precision FFTs. &amp;nbsp;I have literally never encountered such a code, though.&lt;br /&gt;&lt;br /&gt;Ultimately the solution to this problem, for those who are fortunate enough to get to the bottom of it, is to simply add &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;-fdefault-double-8&lt;/span&gt; in addition to &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;-fdefault-real-8&lt;/span&gt;. &amp;nbsp;This was enough to fix the issue my colleague was having, and now his lab is back to crunching away with molecular dynamics at normal speed.&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Scalable Data Analysis in R</title>
   <link href="https://hpc.social/2014/scalable-data-analysis-in-r/"/>
   <updated>2014-01-16T00:00:00-07:00</updated>
   <id>https://hpc.social/2014/scalable-data-analysis-in-r</id>
   <content type="html">&lt;p&gt;R is a great environment for interactive analysis on your desktop, but when your data needs outgrow your personal computer, it’s not clear what to do next.&lt;/p&gt;

&lt;p&gt;I’ve put together material for a day-long tutorial on scalable data analysis in R.  It covers:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A brief introduction to R for those coming from a Python background;&lt;/li&gt;
  &lt;li&gt;The &lt;a href=&quot;http://cran.r-project.org/web/packages/bigmemory/index.html&quot;&gt;bigmemory&lt;/a&gt; package for out-of-core computation on large data matrices, with a simple physical sciences example;&lt;/li&gt;
  &lt;li&gt;The standard parallel package, including what was the snow and multicore facilities, using &lt;a href=&quot;http://stat-computing.org/dataexpo/2009/the-data.html&quot;&gt;airline data&lt;/a&gt; as an example&lt;/li&gt;
  &lt;li&gt;The &lt;a href=&quot;http://cran.r-project.org/web/packages/foreach/index.html&quot;&gt;foreach&lt;/a&gt; package, using airline data and simple stock data;&lt;/li&gt;
  &lt;li&gt;The &lt;a href=&quot;http://cran.r-project.org/web/packages/Rdsm/index.html&quot;&gt;Rdsm&lt;/a&gt; package for shared memory; and&lt;/li&gt;
  &lt;li&gt;a brief introduction to the powerful &lt;a href=&quot;http://r-pbd.org&quot;&gt;pbdR&lt;/a&gt; pacakges for extremely large-scale computation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The presentation for the material, in R markdown (so including the sourcecode) is in the presentation directory; you can read the resulting presentation &lt;a href=&quot;https://github.com/ljdursi/scalable-analysis-R/blob/master/presentation/ScalableDataAnalysis-R.md&quot;&gt;as markdown there&lt;/a&gt;, or &lt;a href=&quot;https://github.com/ljdursi/scalable-analysis-R/blob/master/presentation/ScalableDataAnalysisInR.pdf?raw=true&quot;&gt;as a PDF&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The R code from the slides can be found in the R directory.&lt;/p&gt;

&lt;p&gt;Some data can be found in the data directory; but as you might expect in a workshop on scalable data analysis, the files are quite large!  Mostly you can just find scripts for downloading the data; running make in the main directory will pull almost everything down, but a little more work needs go to into automating some of the production of the data products used.&lt;/p&gt;

&lt;p&gt;Suggestions, as always, greatly welcomed.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Present and Future Computing, Data, and Networks Committee of the Canadian Astronomical Society (CASCA)</title>
   <link href="https://hpc.social/2012/present-and-future-computing-data-and-networks-committee-of-the-canadian-astronomical-society-casca/"/>
   <updated>2012-01-13T00:00:00-07:00</updated>
   <id>https://hpc.social/2012/present-and-future-computing-data-and-networks-committee-of-the-canadian-astronomical-society-casca-</id>
   <content type="html">&lt;p&gt;&lt;a href=&quot;https://www.dursi.ca/assets/pdfs/CCI_WhitePaper_2012.pdf&quot;&gt;This document&lt;/a&gt; is a whitepaper I wrote for the &lt;a href=&quot;http://casca.ca/?page_id=273&quot;&gt;CASCA Computing and Data committee&lt;/a&gt; outlining the computing needs for the Canadian astronomy community for the coming several years.  It does a fairly decent job of laying out the diverse range of large-scale R&amp;amp;D computing needs for the national community.&lt;/p&gt;

&lt;h2 id=&quot;executive-summary&quot;&gt;Executive Summary&lt;/h2&gt;

&lt;p&gt;Advanced research computing resources have never been so essential to the Canadian Astronomy and Astrophysics research community. In the past few years, astronomical researchers have benefited greatly from modern large-scale computing systems; a diverse range of resources, which are a good match to the diverse computing needs of our scientists; and good working relationships with existing providers, allowing flexibility and collaboration between these centres and research groups.&lt;/p&gt;

&lt;p&gt;However, CASCA has concerns about the near future of advanced research computing available to its researchers. Here the Computers, Data, and Networks Committee of CASCA present, on behalf of the Society, a summary of the current state of the computing needs, successes, and concerns of our researchers taken from previous consultative summaries and their updates. This is the first step of a process that will continue through the first half of 2013, which will include a comprehensive survey of research computing needs of the Canadian Astronomy and Astrophysics community, and will investigate a variety of strategies for meeting those needs.&lt;/p&gt;

&lt;p&gt;Early systems funded by the CFI NPF are already showing their age; in many cases they are out of their maintenance contract and are already starting to fail. The lack of any clear signs of new investment on the horizon means that even if existing systems were to continue operating perfectly, as other nations continue to invest in new research computing platforms, our researchers, using stagnant computing hardware, will not only fall behind our international competitors as data volumes continue to increase, but also be unable to make full use of prior investments.&lt;/p&gt;

&lt;p&gt;When new funding does become available, the Canadian astronomy community would like to see changes in emphasis taken as lessons learned from the CFI NPF procurement. Previous investment focused largely on computing hardware. While this addressed a real and pressing need resulting from years of underinvestment, the research endeavor requires a more holistic approach. Computing hardware investments must be balanced with similar investments in storage, highly qualified personnel, software development, and networking to maximize results.&lt;/p&gt;

&lt;p&gt;In this report, we recommend an urgent search for new and sustainable sources of funding for advanced research computing funding; an increased focus on personnel, software development, and storage; maintaining a diverse range of systems; enabling major longer-term projects by committing resources for longer than the one-year allocation window currently of the RAC process; continuing to enable close working relationships with research groups and computing providers, preferably as close to the researchers as possible. In addition, we recommend that CCI’s board, through the proposed Researcher Advisory Committee or otherwise, establish a direct relationship with CASCA (and similar professional groups), with via persons charged with representing the needs of these research communities in planning for  Compute Canada.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Stopping your program at the first NaN</title>
   <link href="https://hpc.social/2012/stopping-your-program-at-the-first-nan/"/>
   <updated>2012-01-12T00:00:00-07:00</updated>
   <id>https://hpc.social/2012/stopping-your-program-at-the-first-nan</id>
   <content type="html">&lt;p&gt;If you know that somewhere in your program, there lurks a catastrophic numerical bug that puts NaNs or Infs into your results and you want to know where it first happens, the search can be a little frustrating.   However, as before, the IEEE standard can help you; these illegal events (divide by zero, underflow or overflow, or invalid operations which cause NaNs) can be made to trigger exceptions, which will stop your code right at the point where it happens; then if you run your code through a debugger, you can find the very line where it happens.&lt;/p&gt;

&lt;p&gt;We’ll discuss using the gnu compilers here; other compiler suites have similar options.&lt;/p&gt;

&lt;p&gt;Let’s take a look at the following Fortran code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;program nantest
    real :: a, b, c

    a = 1.
    b = 2.

    c = a/b
    print *, c,a,b

    a = 0.
    b = 0.

    c = a/b
    print *, c,a,b

    a = 2.
    b = 1.

    c = a/b
    print *,c,a,b
end program nantest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If we compile this code with &lt;code&gt;-ffpe-trap=invalid&lt;/code&gt; (I usually add &lt;code&gt;,zero,overflow&lt;/code&gt; , and even &lt;code&gt;underflow&lt;/code&gt; if I think that’s causing me a problem in intermediate results), then the debugger can tell us the line where it all goes wrong:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ gfortran -o nantest nantest.f90 -ffpe-trap=invalid,zero,overflow -g -static
$ gdb nantest
[...]
(gdb) run
Starting program: /scratch/ljdursi/Testing/fortran/nantest
  0.50000000       1.0000000       2.0000000    

Program received signal SIGFPE, Arithmetic exception.
0x0000000000400384 in nantest () at nantest.f90:13
13          c = a/b
Current language:  auto; currently fortran
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With the intel fortran compiler (ifort), using the option &lt;code&gt;-fpe0&lt;/code&gt; will do the same thing.&lt;/p&gt;

&lt;p&gt;It’s a little tricker with C code; we have to actually insert a call to &lt;code&gt;feenableexcept()&lt;/code&gt;, which enables floating point exceptions, and is defined in fenv.h;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-c&quot;&gt;#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;fenv.h&amp;gt;

int main(int argc, char **argv) {
    float a, b, c;
    feenableexcept(FE_DIVBYZERO | FE_INVALID | FE_OVERFLOW);

    a = 1.;
    b = 2.;

    c = a/b;
    printf(&quot;%f %f %f\n&quot;, a, b, c);

    a = 0.;
    b = 0.;

    c = a/b;
    printf(&quot;%f %f %f\n&quot;, a, b, c);

    a = 2.;
    b = 1.;

    c = a/b;
    printf(&quot;%f %f %f\n&quot;, a, b, c);

    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;but the effect is the same:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ gcc -o nantest nantest.c -lm -g
$ gdb ./nantest
[...]
(gdb) run
Starting program: /scratch/s/scinet/ljdursi/Testing/exception/nantest
1.000000 2.000000 0.500000

Program received signal SIGFPE, Arithmetic exception.
0x00000000004005d0 in main (argc=1, argv=0x7fffffffe4b8) at nantest.c:17
17	    c = a/b;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;either way, you have a much better handle on where the errors are occuring.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Testing Roundoff</title>
   <link href="https://hpc.social/2011/testing-roundoff/"/>
   <updated>2011-11-23T00:00:00-07:00</updated>
   <id>https://hpc.social/2011/testing-roundoff</id>
   <content type="html">&lt;p&gt;A &lt;a href=&quot;http://www.cs.berkeley.edu/~wkahan/Stnfrd50.pdf&quot;&gt;talk&lt;/a&gt; has been circulating (HT: Hacker News) from a conference celebrating &lt;a href=&quot;http://compmath50.stanford.edu/&quot;&gt;50 years of scientific computing at Stanford&lt;/a&gt; where the author, William Kahan, discusses an old and sadly disused trick for testing the numerical stability of the implementation of an algorithm that should work with any C99 or Fortran 2003 compiler without changing the underlying code.  It’s definitely a tool that’s worth having in your toolbox, so it’s worth mentioning here.&lt;/p&gt;

&lt;p&gt;We’ll consider a simple numerical problem; imagine a projectile launched from height $h = 0$ with velocity $v_0=5000 \mathrm{m s}^{-1}$, and subject to the Earth’s gravitational accelleration, $g = 9.81 \mathrm{m} \mathrm{s}^{-2}$. We’re going to ask when the (first) time is that the projectile hits a height h.&lt;/p&gt;

&lt;p&gt;This is going to be an application of our friend the quadratic equation:&lt;/p&gt;

&lt;p&gt;[r = \frac{-b \pm \sqrt{b^2 - 4 a c}}{2 a}]&lt;/p&gt;

&lt;p&gt;Now, because of the repeated subtraction, a naive implementation of this equation is known to undergo catastrophic cancellation near $b^2=4 a c$, or for where the discriminant is much less than \(b\) — in our case, near the ends and the peak of the projectile’s trajectory.   We’re going to demonstrate that below.&lt;/p&gt;

&lt;p&gt;Now, before we show that such sensitivity can happen, we should ask — why would we care? If we test our code and know it gives “good enough” answers under the conditions that matter to us, does it really matter what could happen in other circumstances? The answer, of course, is yes. There are a lot of things we could want to do — increase the agressiveness of compiler optimizations when compiling our code, for instance — which will have the effect of numerically perturbing our computation; and we need to know if those small perturbations will have small, or large, effects on our answers.&lt;/p&gt;

&lt;p&gt;It turns out that IEEE 754, the standard for floating point numbers, can give us some help with this. (Everyone who does numerical work should know at least a little bit about the floating point standard, or at least the issues involved with floating point numbers. &lt;a href=&quot;http://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html&quot;&gt;What every computer scientist should know about floating point&lt;/a&gt;, particularly the first few sections, is an essential guide). The floating point standard - which almost all widely-used computing hardware should support - allows you to set certain properties of the mathematics “on the fly”. One particularly useful feature is the ability to set how the last digit of all floating point operations are rounded - to nearest (the default), to zero (eg, always truncate), to positive infinity (eg, always round up) or to negative infinity (always round down). In the C99 standard, this is implemented in the “fenv.h” header and the math library; in Fortran2003, this is part of the intrinsic IEEE_ARITHMETIC module, where you can call IEEE_SET_ROUNDING_MODE.&lt;/p&gt;

&lt;p&gt;By changing the rounding, you are perturbing every floating point operation in your calculation. If this perturbation results in significant changes in your result, then your calculation is very fragile, and you may have to look into re-writing the calculation, using another algorithm, or resorting to using higher precision for that calculation (which will push the perturbations to less significant decimal places). If not, then you have some evidence that your calculation is robust to perturbations, at least in the last bit.&lt;/p&gt;

&lt;p&gt;Below we have an example of how you’d do this in C. We have a simple routine which uses the obvious implementation of the quadratic equation to calculate the time when the projectile is at one meter, and we perform this calculation with all available rounding modes:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-c&quot;&gt;#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;math.h&amp;gt;
#include &amp;lt;fenv.h&amp;gt;

const int NOSOLN=-1;
const int SOLN  = 0;

int time(const float vo, const float g, const float ho, float *time) {
    float disc  = (vo*vo - 2.*g*ho);

    if (disc &amp;lt; 0) return NOSOLN;

    disc = sqrt(disc);
    float root1 = (vo + disc)/g;
    float root2 = (vo - disc)/g;

    if ((root2 &amp;gt;= 0.) &amp;amp;&amp;amp; root2 &amp;lt; root1)
        *time = root2;
    else
        *time = root1;

    return SOLN;
}


int main(int argc, char **argv) {

    const float g =9.81;
    const float vo=5000.;
    const int   ho=1.;

    int nroundings=4;
    int roundings[]={FE_TONEAREST, FE_UPWARD, FE_DOWNWARD, FE_TOWARDZERO};
    char *names[]  ={&quot;To nearest&quot;, &quot;To +inf&quot;, &quot;To -inf&quot;, &quot;To zero&quot;};

    for (int r=0; r&amp;lt;nroundings; r++) {
        int status = fesetround(roundings[r]);
        if (status) {
            fprintf(stderr,&quot;Could not set rounding to '%s'.\n&quot;, names[r]);
        } else {
            float soln;
            time(vo, g, ho, &amp;amp;soln);
            printf(&quot;%s: %f\n&quot;, names[r], soln);
        }
    }

    return 0;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We compile the code with gcc (any C99 compiler should work):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ gcc -O0 -Wall -std=c99 quadratic.c -o quadratic -lm
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we need to explicitly link in the math library, and to turn off optimization (so that the compiler doesn’t replace the repeated calls to time() with a single call). Running this, we find:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./quadratic
To nearest: 0.000199
To +inf: 0.000149
To -inf: 0.000249
To zero: 0.000249
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Changing the rounding modes changes the result by 50%! This shows that our current implementation - which is not giving obviously wrong answers - is extremely fragile in the presence of numerical noise, and we should exercise extreme caution with compiler flags, etc. (How to re-write the expression to be more robust to small changes is a topic for another day.)&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Codes as Instruments- Community Applications and Simulation Software for the Hardware Architectures of the Next Decade</title>
   <link href="https://hpc.social/2010/codes-as-instruments-community-applications-and-simulation-software-for-the-hardware-architectures-of-the-next-decade/"/>
   <updated>2010-06-17T01:00:00-06:00</updated>
   <id>https://hpc.social/2010/codes-as-instruments-community-applications-and-simulation-software-for-the-hardware-architectures-of-the-next-decade</id>
   <content type="html">&lt;p&gt;It is becoming increasingly problematic that, even as computing and data becomes more and more fundamental to research, and the complexity and diversity of computing technologies out there grows, getting stable funding for developing high-quality research software remains so difficult.&lt;/p&gt;

&lt;p&gt;In &lt;a href=&quot;https://www.dursi.ca/assets/pdfs/CAI.pdf&quot;&gt;this whitepaper&lt;/a&gt; for the &lt;a href=&quot;http://www.casca.ca/lrp2010/&quot;&gt;CASCA 2010 Long Range Plan&lt;/a&gt;, my colleague &lt;a href=&quot;http://www.astro.uvic.ca/~fherwig/&quot;&gt;Falk Herwig&lt;/a&gt; and I lay out the case for increased funding of R&amp;amp;D software development by professional research software developers.  We make a couple points which I genuinely believe to be strong:&lt;/p&gt;

&lt;p&gt;First, increased benefits. A successful community code can support an enoormous body of research.  By the (admittedly somewhat crude) count we use in this paper, the top six reseach codes in Astronomy accounted for approximately 50% of the computational astronomy publications over the period of study, and the top three - &lt;a href=&quot;http://www.nublado.org/&quot;&gt;Cloudy&lt;/a&gt;, &lt;a href=&quot;http://www.mpa-garching.mpg.de/galform/gadget/&quot;&gt;Gadget&lt;/a&gt;, and &lt;a href=&quot;http://www.flash.uchicago.edu/site/&quot;&gt;FLASH&lt;/a&gt;, which I was part of - accounted for nearly 40%.  That is an enormous about of R&amp;amp;D effort enabled by those projects.&lt;/p&gt;

&lt;p&gt;Second, reduced costs. We cite from the growing research software development literature to demonstrate the high (and growing) challenges of engineering these codes in a scientists’ spare time, and the high cost of software defects.  By having a small cadre of professional research software development personnel, better quality software can be developed more efficiently.&lt;/p&gt;

&lt;p&gt;Finally, a word about the title - this is an analogy due to Falk, and while it’s been controversial, I think there’s a lot of truth to it.  Astronomy has always relied heavily on, for instance, telescopes - but a telescope is only part of an observational facility.  A big photon-gathering dish is only as useful as the scientific instrument that’s placed at its focus to make sense of those photons.  Similarly, a huge computer by itself has no scientific value without software to run on it.  Unless our community invests in computational instruments with the same level of seriousness as observational instruments, our ability to make use of these facilities is going to be needlessly limited.&lt;/p&gt;

&lt;h3 id=&quot;abstract&quot;&gt;Abstract&lt;/h3&gt;

&lt;p&gt;Modern astronomical research requires increasingly sophisticated computing facilities and software tools. Computational tools have become the fundamental tools to turn observational raw data into scientific insight. Complex multi-physics simulation codes have developed into tools for numerical experiments that provide scientific insight beyond classical theory. Canadian researchers need an environment for developement and maintenance of these critical tools. In particular, the drastically enhanced complexity of deeply heterogeneous hardware architectures poses a real challenge to using present and future HPC facilties.&lt;/p&gt;

&lt;p&gt;Without a national program in astrophysical simulation science and astronomy application code developement we are becoming vulnerable with respect to our ability to maximise the scientific return from existing and planned investments into atronomy. In addition, there are significant industrial/commercial HQP needs that simulation and application code program could start to address, if it is properly aligned with academic training opportunities.&lt;/p&gt;

&lt;p&gt;We outline the framework and requirements for such a framework for developing Canadian astronomical application and simulation codes — and code builders. In the US decadal plan process, voices are calling for similar emphasis on developing infrastructure and incentives for open community codes (Weiner et al. 2009). We propose funding several small interdisciplinary teams of postdocs, graduate students, and staff, housed in departments at Universities that have or are about to make a commitment in a relevant area (e.g. applied math, computational physics, modeling science). These teams can, while training astronomical and computational HQP, focus on building tools that have been deemed to be high priorities by the astronomical and astrophysical communities in order to make the best scientific use of our new computational faciliites.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Canadian Astronomical Computing, Data And Network Facilities- A White Paper for the 2010 Long Range Plan</title>
   <link href="https://hpc.social/2010/canadian-astronomical-computing-data-and-network-facilities-a-white-paper-for-the-2010-long-range-plan/"/>
   <updated>2010-05-01T01:00:00-06:00</updated>
   <id>https://hpc.social/2010/canadian-astronomical-computing-data-and-network-facilities-a-white-paper-for-the-2010-long-range-plan</id>
   <content type="html">&lt;p&gt;In &lt;a href=&quot;https://www.dursi.ca/assets/pdfs/CDandN_WP.pdf&quot;&gt;this whitepaper&lt;/a&gt; for the &lt;a href=&quot;http://www.casca.ca/lrp2010/&quot;&gt;CASCA 2010 Long Range Plan&lt;/a&gt;, I and the rest of the Computing, Data, and Network committee of CASCA lay out the state of ecosystem for computation in support of Canadian astronomy, and suggests a path forward for the time period of the 2010-2020 long range plan.&lt;/p&gt;

&lt;h3 id=&quot;abstract&quot;&gt;Abstract&lt;/h3&gt;

&lt;p&gt;Significant investment in new large, expensive astronomical observing facilities spanning a substantial portion of the electronic spectrum was a dominant theme of LRP2000 and continues to be necessary for Canadian astronomy to maintain its world position. These developments are generating increasingly large volumes of data. Such investments only makes sense if they are balanced by strong infrastructure support to ensure that data acquired with these facilities can be readily accessed and analyzed by observers, and that theoreticians have the tools available to simulate and understand their context. This will require continuing investment in computational facilities to store and analyze the data, networks to ensure useful access to the data and products by Canadian researchers, and personnel to help Canadian researchers make use of these tools.&lt;/p&gt;

&lt;p&gt;In addition, large parallel simulations have become an essential tool for astrophysical theory, and Canadian Astronomy has world-leading simulators and developers who rely on world-class High Performance Computing facilities being maintained in Canada to do their research effectively.&lt;/p&gt;

&lt;p&gt;We recommend that Compute Canada be funded at $72M/yr to bring HPC funding per capita in line with G8 norms; that part of every Compute Canada technology renewal include a Top-20 class computing facility; NSERC and other funding agencies begin supporting software development as an integral component of scientific research; that the staff funding for consortia be tripled, including local access to technical analyst staff; and that the last mile bottleneck of campus networking less than 10 Gb/s be addressed where it is impacting researchers, with particular urgency for the current 1 Gb/s connection at the CADC.&lt;/p&gt;
</content>
 </entry>
 

</feed>
