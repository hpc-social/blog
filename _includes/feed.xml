<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en-US"><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="https://hpc.social/personal-blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://hpc.social/personal-blog/" rel="alternate" type="text/html" hreflang="en-US" /><updated>2023-01-13T19:46:16-07:00</updated><id>https://hpc.social/personal-blog/feed.xml</id><title type="html">hpc.social - Aggregated Personal Blog</title><subtitle>Shared personal experiences and stories</subtitle><author><name>hpc.social</name><email>info@hpc.social</email></author><entry><title type="html">Adam’s weekly (-ish) update, 2022-12-20</title><link href="https://hpc.social/personal-blog/2022/adam-s-weekly-ish-update-2022-12-20/" rel="alternate" type="text/html" title="Adam’s weekly (-ish) update, 2022-12-20" /><published>2022-12-20T18:14:52-07:00</published><updated>2022-12-20T18:14:52-07:00</updated><id>https://hpc.social/personal-blog/2022/adam-s-weekly-ish-update-2022-12-20</id><content type="html" xml:base="https://hpc.social/personal-blog/2022/adam-s-weekly-ish-update-2022-12-20/">&lt;h2&gt;What&amp;#8217;s new&lt;/h2&gt;

&lt;p&gt;The past few weeks have been on the intense side at work, so I completely lost track of the blog and haven&amp;#8217;t had a chance to write much in that time. However, I&amp;#8217;m now on a holiday break, and finally have time to sit down at a keyboard to write more than code and Slack messages.&lt;/p&gt;

&lt;p&gt;&lt;span id=&quot;more-289&quot;&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;One of the highlights of the past few weeks was a trip to San Jose, and the NVIDIA headquarters. I changed teams at work back in July, transferring from a group that was closely integrated with product management, to a more straightforward engineering team which &lt;a href=&quot;https://blogs.nvidia.com/blog/2020/08/14/making-selene-pandemic-ai/&quot;&gt;designs and builds new high-performance computing systems&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;This was the first chance I&amp;#8217;ve had to meet up with other members of my new team in person, and it was a really wonderful experience to be in the same physical space as folks who were previously just images on my screen. I love working remotely, but it&amp;#8217;s also great to be able to stand in front of a white board with someone and brainstorm, or get coffee and just have a chat with a coworker outside of a video call with an agenda.&lt;/p&gt;

&lt;p&gt;(Plus, we were all careful and managed to avoid catching COVID from each other! Which was a win on its own.)&lt;/p&gt;

&lt;p&gt;Now, for the next two weeks I&amp;#8217;m off work, and planning to take some time to relax and spend time on projects that are harder to focus on during busy work weeks. Expect (maybe) less about computers in my blog and social feeds, and more about D&amp;amp;D, baking, and tasty cocktails.&lt;/p&gt;

&lt;h2&gt;What I&amp;#8217;m reading, watching, and listening to&lt;/h2&gt;

&lt;p&gt;I&amp;#8217;ve been a bit too scattered to focus on actual books the past few weeks, but I did find time for a few interesting articles and podcasts. In particular,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://acoup.blog/2022/12/02/collections-why-roman-egypt-was-such-a-strange-province/&quot;&gt;&amp;#8220;Why Roman Egypt was such a strange province&amp;#8221;&lt;/a&gt;, from Bret Devereaux: As usual from Devereaux, an accessible but extremely detailed discussion of why so much of what we know about the Roman empire is from Egyptian records, but why that also might not be representative of the broader empire.&lt;/li&gt;



&lt;li&gt;&lt;a href=&quot;https://willgallego.com/2022/12/18/emoji-as-incident-resolution-tools/&quot;&gt;&amp;#8220;Emoji as incident resolution tools&amp;#8221;&lt;/a&gt;, from Will Gallego: A fun discussion of how using emoji as part of a team&amp;#8217;s communication can add nuance and shared understanding during incident management, along with a discussion of the disadvantages and costs associated with the practice.&lt;/li&gt;



&lt;li&gt;&lt;a href=&quot;https://www.mikulskibartosz.name/modern-software-architecture-in-2022/&quot;&gt;&amp;#8220;What does modern software architecture look like in 2022?&amp;#8221;&lt;/a&gt;, from Bartosz Mikulski: A nice  article which discusses how service-oriented software architecture can often include an explicit expectation of change. For example, the architecture might include notes on an ongoing deprecation of a library, or might signpost the need to factor a new microservice out when overall system load gets high enough.&lt;/li&gt;



&lt;li&gt;&lt;a href=&quot;https://www.bradyheywood.com.au/podcasts/&quot;&gt;The Brady Heywood podcast&lt;/a&gt;: Found via the &lt;a href=&quot;https://oxide.computer/podcasts/oxide-and-friends/1137359&quot;&gt;Oxide and Friends podcast&lt;/a&gt;, the Brady Heywood podcast is a series on engineering disasters and their consequences from a forensic engineering firm. It&amp;#8217;s mostly not being updated any more (with the podcasters moving on to a separate series on complexity science), but it has a deep back catalog of good episodes, and includes thoughtful discussions of human factors, safety engineering, and how organizational pressures become manifest in engineering artifacts.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Recent recipes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://smittenkitchen.com/2016/12/homemade-irish-cream/&quot;&gt;Smitten Kitchen&amp;#8217;s Homemade Irish Cream&lt;/a&gt;: This is a recipe I make every year, and I often give away small bottles of it as holiday gifts. It&amp;#8217;s really ridiculously tasty, much better than Baileys  or similar, and good either on its own or in hot chocolate.&lt;/li&gt;



&lt;li&gt;&lt;a href=&quot;https://smittenkitchen.com/2014/12/fairytale-of-new-york/&quot;&gt;Smitten Kitchen&amp;#8217;s Fairytale of New York&lt;/a&gt;: This is a really tasty whiskey cocktail, and the star of the show is a &amp;#8220;winter warmth syrup&amp;#8221; that substitutes in for simple syrup. The syrup is simply very tasty, and turns what&amp;#8217;s effectively an OId Fashioned variant into a lovely holiday cocktail.&lt;/li&gt;



&lt;li&gt;Sparkling gingerbread from &lt;a href=&quot;http://www.apt2bbakingco.com/snacking-cakes&quot;&gt;Yossy Arefi&amp;#8217;s Snaking Cakes&lt;/a&gt;: This recipe takes a little more prep than most of Arefi&amp;#8217;s &amp;#8220;snacking cakes&amp;#8221;, as it includes ginger three ways (ground, fresh, and crystallized), but it&amp;#8217;s worth the few minutes of extra work.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Pet photos&lt;/h2&gt;

&lt;figure class=&quot;wp-block-image size-large is-resized&quot;&gt;&lt;img alt=&quot;A white calico cat and a gray tabby cat lounging on a large brown pet bed in front of a gas fireplace.&quot; class=&quot;wp-image-295&quot; height=&quot;512&quot; src=&quot;https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_7207-768x1024.jpeg&quot; width=&quot;384&quot; /&gt;&lt;figcaption class=&quot;wp-element-caption&quot;&gt;I&amp;#8217;m pretty sure these two want me to turn the fireplace on.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;figure class=&quot;wp-block-image size-large is-resized&quot;&gt;&lt;img alt=&quot;A gray tabby cat lounges on a dog bed, while a golden doodle lays on the floor nearby and looks forlornly at the bed.&quot; class=&quot;wp-image-294&quot; height=&quot;512&quot; src=&quot;https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_1725-1024x1024.jpeg&quot; width=&quot;512&quot; /&gt;&lt;figcaption class=&quot;wp-element-caption&quot;&gt;Just Percy bullying the dog by stealing his bed.&lt;/figcaption&gt;&lt;/figure&gt;</content><author><name>Thinking Out Loud</name></author><category term="ajdecon" /><summary type="html">What&amp;#8217;s new</summary></entry><entry><title type="html">Visualizing Spectrum LSF data with Grafana</title><link href="https://hpc.social/personal-blog/2022/visualizing-spectrum-lsf-data-with-grafana/" rel="alternate" type="text/html" title="Visualizing Spectrum LSF data with Grafana" /><published>2022-12-13T00:06:51-07:00</published><updated>2022-12-13T00:06:51-07:00</updated><id>https://hpc.social/personal-blog/2022/visualizing-spectrum-lsf-data-with-grafana</id><content type="html" xml:base="https://hpc.social/personal-blog/2022/visualizing-spectrum-lsf-data-with-grafana/">&lt;p&gt;&lt;strong&gt;Overview&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;System monitoring is a fundamental part of IT best practices. High performance computing (HPC) environments are no exception to this. At the high-end, HPC clusters can consist of
thousands of servers, processing millions of jobs per day. HPC admins need ways to monitor the overall cluster to determine system status and availability through to the efficiency
of workloads. Servers today produce a wide array of metrics which can be monitored for example to check for various conditions. Additionally, workload schedulers also produce a wealth
of data about jobs. Having a single dashboard to show this type of detail can be of great benefit.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.ibm.com/products/hpc-workload-management&quot;&gt;IBM Spectrum LSF Suites&lt;/a&gt; provide a complete solution for HPC workload management. This includes reporting capabilities out of the box. Spectrum LSF Suite features an integrated web
interface for job management and reporting. The reporting capabilities include a number of reports out of the box, with the ability to customize and add new reports. The reporting
capability in Spectrum LSF Suite and IBM Spectrum LSF Explorer is underpinned by Elasticsearch, which is used to store, index and query data. With LSF data in Elasticsearch, it’s
also possible to configure LSF command-line interface (CLI) tools to query information from Elasticsearch rather than flat files – for greater performance. This is controlled via
the &lt;strong&gt;LSF_QUERY_ES_FUNCTIONS&lt;/strong&gt; parameter of Spectrum LSF. More details about the &lt;strong&gt;LSF_QUERY_ES_FUNCTIONS&lt;/strong&gt; can be found in the LSF documentation &lt;a href=&quot;https://www.ibm.com/docs/en/spectrum-lsf/10.1.0?topic=lsfconf-lsf-query-es-functions&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;(1) Here is a look at the indices that are created by LSF in Elasticsearch. Note that the status shows as yellow because I only have a single Elasticsearch node.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-plaintext&quot;&gt;# curl -XGET localhost:9200/_cat/indices
yellow open lsf_events-202205             tejh7jsMSwSeQUJzYM7cww 5 1    1137     0 808.1kb 808.1kb
yellow open lsf_jobs_pendingreason-202204 4wi7Ta8uQPSXlFBqPh4kOQ 5 1   90531     0   8.6mb   8.6mb
yellow open lsf_events-202204             tWYvW_w8TVyU1deRFOEoZg 5 1  116957 32691  59.1mb  59.1mb
yellow open lsf_jobs_active-202212        Q0pStQxvTgaeL7R-f02XWA 5 1  210052     0  50.6mb  50.6mb
yellow open lsf_jobs_pendingreason-202206 ENWIwfGrSqCHvi53aUQXJQ 5 1   44991     0   4.5mb   4.5mb
yellow open host_booleanres_latest        RE8thZCgTGeMBGodeMfXEQ 5 1       5     0  23.3kb  23.3kb
yellow open lsf_jobs_pendingreason-202205 yo0iZH_4TvOqq6kQgBluvA 5 1     111     0 181.4kb 181.4kb
yellow open lsf_jobs_pend-202212          9ViIS3nDRFewrqtILEbKTQ 5 1     707     0 446.9kb 446.9kb
yellow open lsf_hostconf_latest           9N1Y8ML4TiyaamCPEDRQog 5 1       2     0  10.6kb  10.6kb
yellow open lsf_events-202209             rtKQ8F4bSleHl8EbAQez8A 5 1    8200   955   4.4mb   4.4mb
yellow open lsf_events-202206             UUKPWfN7SZ-dzVs5NAkjUg 5 1   79503 23452  36.8mb  36.8mb
yellow open lsf_hostmetrics-202209        7FUNFCWPQtuGyx5jTJLb1A 5 1    4701     0   2.2mb   2.2mb
yellow open lsf_hostmetrics-202208        52xef_3hQWK-jVuJqyUpHA 5 1    3823     0   1.9mb   1.9mb
yellow open lsf_hostmetrics-202207        IqZYhU0RQNGIFWSRH-Ym8Q 5 1    6316     0   2.9mb   2.9mb
yellow open lsf_job_acct-202209           h1ZgCSB8RwCBxwIUUzDHEQ 5 1    2050   438   1.9mb   1.9mb
yellow open lsf_jobs_active-202209        iBfnf07CTcS7Gb6TxwomRA 5 1    2658     0     1mb     1mb
yellow open lsf_hostmetrics-202206        0PXSYBOgTA2Qa_zzaafUPg 5 1    4301     0   2.1mb   2.1mb
yellow open model                         xSqB_T_VSByOzYavEcEVyQ 1 1      55     0   257kb   257kb
yellow open lsf_job_acct-202206           C639GnzBSjCEVczfh5u23g 5 1   16719   353   8.9mb   8.9mb
yellow open lsf_jobs_active-202204        8gN_ENkQRTSfnmxrtMcOlA 5 1   33286     0   9.8mb   9.8mb
yellow open lsf_job_acct-202205           LOxmhm_8RxaCuTd7YWYbLw 5 1     274     0 439.4kb 439.4kb
yellow open lsf_jobs_active-202205        61u2RlXgR_SXagmZfrmttQ 5 1    1880     0   1.1mb   1.1mb
yellow open lsf_jobs_pend-202209          eTgqPp9nQOScNiwyUWXmHA 5 1       9     0 106.2kb 106.2kb
yellow open lsf_job_acct-202204           dDDegS6RQSWtWN99eklexg 5 1   28902  2177  17.4mb  17.4mb
yellow open lsf_jobs_active-202206        8ivkjWSNR1Sh_BxWACP0ZA 5 1   16921     0   4.6mb   4.6mb
yellow open lsf_current_status            92KE3V4YSJ-RtRp_kepxYg 5 1  115450     0     9mb     9mb
yellow open lsf_hostmetrics-202210        vbuK2wW3RRmXuY07tDPUNQ 5 1     785     0 942.1kb 942.1kb
yellow open lsf_jobs_pend-202206          OhSwn-b0SiSj8mCW5tcNIA 5 1      22     0 244.6kb 244.6kb
yellow open lsf_jobs_pend-202205          OfBtWklETYK9cRx000aNPw 5 1       1     0  12.7kb  12.7kb
yellow open lsf_events-202212             WUC5KJWmS-2WIN8XCQpSuw 5 1  712399 74728   337mb   337mb
yellow open lsf_jobs_pend-202204          OhUsXqohSciZTPZlTryMyA 5 1      50     0 275.3kb 275.3kb
yellow open resource_attributes_latest    R9bk_WIPTU62dVg3O1LDBA 5 1       5     0  24.4kb  24.4kb
yellow open lsf_jobs_pendingreason-202212 55iwDC5mRI-eRbzQLwWP6Q 5 1 3314828     0 288.7mb 288.7mb
yellow open pa-lite-log                   o8-jaNoGTsSVcjJW5Ufs0w 5 1    1549     0 547.2kb 547.2kb
yellow open lsf_job_acct-202212           4HXvAD02Sxq0tgp2fS2cfQ 5 1  161502     0  73.6mb  73.6mb
yellow open lsf_hostmetrics-202212        Tki6OJ41R363u9Tx02N4zw 5 1    2548     0   1.7mb   1.7mb
yellow open lsf_jobs_pendingreason-202209 D3TOZY2ORiK9PppGVt10Fg 5 1    2511     0 381.4kb 381.4kb&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;(2) With the LSF data stored in Elasticsearch, the next step is to connect to the Grafana server. Here we point our browser to the Grafana server on the default port: &lt;em&gt;http://lsf_manager:3000&lt;/em&gt; and login to Grafana. This step assumes an account has already been setup on Grafana. Here we are using the default admin account.&lt;/p&gt;

&lt;p&gt;(3) In Grafana, navigate to &lt;strong&gt;Configuration&lt;/strong&gt; -&amp;gt; &lt;strong&gt;Data sources&lt;/strong&gt;. It’s here that it will be possible to add an Elasticsearch data source&lt;/p&gt;

&lt;figure&gt;&lt;img src=&quot;https://www.gaborsamu.com/images/grafana_3.png&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;(4) Next, click the &lt;strong&gt;Add data source&lt;/strong&gt; button.&lt;/p&gt;

&lt;figure&gt;&lt;img src=&quot;https://www.gaborsamu.com/images/grafana_4.png&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;(5 In the list of data sources, filter by name for &lt;em&gt;Elasticsearch&lt;/em&gt; and click the Select button on the Elasticsearch entry.&lt;/p&gt;

&lt;figure&gt;&lt;img src=&quot;https://www.gaborsamu.com/images/grafana_5.png&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;(6) When configuring the data source, it’s necessary to specify an index name. This is where the list of indices in Elasticsearch that we generated earlier will come in handy. For this example, we wish to display the total number of pending jobs in the Spectrum LSF cluster over time. This data is stored in the &lt;em&gt;lsf_jobs_pend*&lt;/em&gt; indices in Elasticsearch. To configure the data source appropriately, we specify the following values:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Name:	“LSF pending jobs”&lt;/li&gt;
&lt;li&gt;URL: http://localhost:9200&lt;/li&gt;
&lt;li&gt;Index name: “lsf_jobs_pend*”&lt;/li&gt;
&lt;li&gt;Time field name: “time_stamp”&lt;/li&gt;
&lt;li&gt;Version: 7.0+
Note that the URL needs to point to the Elasticsearch server. In this case, both the Elasticsearch server and Grafana server are running on the same host.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Next click on the &lt;strong&gt;Save &amp;amp; Test button&lt;/strong&gt;. It should return the message &lt;em&gt;Index OK. Time field name OK.&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Assuming that no errors were found, click on the &lt;strong&gt;Back&lt;/strong&gt; button.&lt;/p&gt;

&lt;figure&gt;&lt;img src=&quot;https://www.gaborsamu.com/images/grafana_6.png&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;(7) Now you should see &lt;em&gt;LSF pending jobs&lt;/em&gt; listed as a Data Source.&lt;/p&gt;

&lt;figure&gt;&lt;img src=&quot;https://www.gaborsamu.com/images/grafana_7.png&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;(8) With the data source configured, we’re now ready to configure a dashboard to display the LSF pending job information. Navigate to &lt;strong&gt;Create&lt;/strong&gt; -&amp;gt; &lt;strong&gt;Dashboard&lt;/strong&gt;.&lt;/p&gt;

&lt;figure&gt;&lt;img src=&quot;https://www.gaborsamu.com/images/grafana_8.png&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;(9) Click on &lt;strong&gt;Add an empty panel&lt;/strong&gt;. This is used to create a new panel where the LSF pending job information will be plotted.&lt;/p&gt;

&lt;figure&gt;&lt;img src=&quot;https://www.gaborsamu.com/images/grafana_9.png&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;(10) In the panel editor, specify the following options:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Panel title: “LSF pending jobs”&lt;/li&gt;
&lt;li&gt;Specify the data source “LSF pending jobs” which was created previously&lt;/li&gt;
&lt;li&gt;Specify a suitable time range (2 days)&lt;/li&gt;
&lt;li&gt;Line width (5 points)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You should immediately see in the panel editor the plot of the hourly pending jobs.  Click on the &lt;strong&gt;Apply&lt;/strong&gt; button to save the changes.&lt;/p&gt;

&lt;figure&gt;&lt;img src=&quot;https://www.gaborsamu.com/images/grafana_10.png&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;(11) After clicking Apply, you will be returned to the Dashboard screen. The Dashboard should now display the new LSF pending jobs panel that was created above. This Dashboard could also include panels for system metrics collected by Prometheus for example.&lt;/p&gt;

&lt;figure&gt;&lt;img src=&quot;https://www.gaborsamu.com/images/grafana_11.png&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;(12) Next, click on the diskette icon in the upper right to save the Dashboard with the LSF pending jobs panel.  We’ll name it &lt;em&gt;Spectrum LSF cluster status&lt;/em&gt;.&lt;/p&gt;

&lt;figure&gt;&lt;img src=&quot;https://www.gaborsamu.com/images/grafana_12.png&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;Additional panels can be added to the &lt;em&gt;Spectrum LSF cluster status&lt;/em&gt; based on the data logged by Spectrum LSF to Elasticsearch.&lt;/p&gt;

&lt;p&gt;That concludes the simple example of plotting Spectrum LSF cluster data from Elasticsearch in Grafana.  As mentioned, the IBM Spectrum LSF Suites integrated web interface also provides reporting capabilities, with several built-in reports provided out of the box. Below, we’ve included a screenshot of the &lt;em&gt;pending job analysis&lt;/em&gt; report included with Spectrum LSF Suites.&lt;/p&gt;

&lt;figure&gt;&lt;img src=&quot;https://www.gaborsamu.com/images/lsf_pending.png&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Summary&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Spectrum LSF provides many hooks and integration points enabling administrators to change things ranging from scheduling behavior and the output of query commands through to job information being logged to Elasticsearch. Spectrum LSF is highly customizable by organizations to suit specific needs and requirements. We’ve demonstrated this using Grafana to visualize data from the LSF scheduler in a simple example. Following the above example, administrators can combine existing HPC cluster system level reporting in Grafana with job information from Spectrum LSF for a better overall view and understanding of the infrastructure.&lt;/p&gt;</content><author><name>Ramblings of a supercomputing enthusiast.</name></author><category term="gaborsamu" /><summary type="html">Overview</summary></entry><entry><title type="html">Adam’s weekly update, 2022-12-04</title><link href="https://hpc.social/personal-blog/2022/adam-s-weekly-update-2022-12-04/" rel="alternate" type="text/html" title="Adam’s weekly update, 2022-12-04" /><published>2022-12-05T05:49:35-07:00</published><updated>2022-12-05T05:49:35-07:00</updated><id>https://hpc.social/personal-blog/2022/adam-s-weekly-update-2022-12-04</id><content type="html" xml:base="https://hpc.social/personal-blog/2022/adam-s-weekly-update-2022-12-04/">&lt;h2&gt;What&amp;#8217;s new&lt;/h2&gt;

&lt;p&gt;This week was really intense from a work perspective. Not &amp;#8220;bad intense&amp;#8221;, but the kind of week where every day was spent with such a level of focus, that at 5 PM or so I found myself staring off into space and forgetting words. I think I got some good things accomplished, but my brain also felt like mush by the time the weekend came.&lt;/p&gt;

&lt;p&gt;&lt;span id=&quot;more-268&quot;&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;This week I&amp;#8217;m traveling to San Jose for work (I just checked into my hotel a little while ago!), so I fully expect this week to also be eaten by work. So I don&amp;#8217;t promise anything terribly interesting for next week&amp;#8217;s post&amp;#8230;&lt;/p&gt;

&lt;p&gt;However, I did take advantage of a Sunday in San Jose to visit the &lt;a href=&quot;https://computerhistory.org/&quot;&gt;Computer History Museum&lt;/a&gt; in Mountain View! I try to visit the museum every few years, and while a lot of the exhibits are the same, enough things change that I always get something new from the visit. Also, I&amp;#8217;ve been doing a lot of reading about hardware development and the history thereof lately, so it was interesting to examine the museum through that new lens.&lt;/p&gt;

&lt;p&gt;I may write more about my visit later this week &amp;#8212; it definitely sparked some thoughts &amp;#8212; but in the mean time, here are a few photos I took while wandering around the museum.&lt;/p&gt;

&lt;figure class=&quot;wp-block-image size-large is-resized&quot;&gt;&lt;img alt=&quot;A mechanical computer built mostly of brass, with various numerical dials. A small placard labels this as a replica of the Babbage Difference Engine No. 1 Demonstration Piece.&quot; class=&quot;wp-image-282&quot; height=&quot;800&quot; src=&quot;https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_6894-768x1024.jpg&quot; width=&quot;600&quot; /&gt;&lt;figcaption class=&quot;wp-element-caption&quot;&gt;The Babbage Difference Engine, and other mechanical computers, have always fascinated me.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;figure class=&quot;wp-block-image size-large is-resized&quot;&gt;&lt;img alt=&quot;The Cray-1, a round computer with its own built-in seating attached.&quot; class=&quot;wp-image-283&quot; height=&quot;446&quot; src=&quot;https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_6965-1024x768.jpg&quot; width=&quot;595&quot; /&gt;&lt;figcaption class=&quot;wp-element-caption&quot;&gt;Can&amp;#8217;t visit the museum without visiting the Cray-1.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;figure class=&quot;wp-block-image size-large is-resized&quot;&gt;&lt;img alt=&quot;The Connection Machine 1, a large black cube divided in eight sections.&quot; class=&quot;wp-image-284&quot; height=&quot;768&quot; src=&quot;https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_6973-768x1024.jpg&quot; width=&quot;576&quot; /&gt;&lt;figcaption class=&quot;wp-element-caption&quot;&gt;I would have loved to have seen a CM-1 in operation, with its red LEDs showing the operation of its many single-bit CPUs.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;figure class=&quot;wp-block-image size-large is-resized&quot;&gt;&lt;img alt=&quot;The front panel of an Altair 8800 computer, with an array of LEDs and switches controlling the state of individual bits.&quot; class=&quot;wp-image-285&quot; height=&quot;449&quot; src=&quot;https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_7037-1024x768.jpg&quot; width=&quot;598&quot; /&gt;&lt;figcaption class=&quot;wp-element-caption&quot;&gt;Having recently read Charles Petzold&amp;#8217;s &amp;#8220;Code&amp;#8221;, I was struck by how closely the front panel of the Altair 8800 resembles the fictional front panel of the computer that Petzold constructs from logic gates up.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;figure class=&quot;wp-block-image size-large is-resized&quot;&gt;&lt;img alt=&quot;A Dell PowerEdge R710 lays on a white plastic table, top cover off, surrounded by instructions on how to disassemble it.&quot; class=&quot;wp-image-286&quot; height=&quot;467&quot; src=&quot;https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_7073-1024x768.jpg&quot; width=&quot;623&quot; /&gt;&lt;figcaption class=&quot;wp-element-caption&quot;&gt;The CHM Learning Lab now includes a back room with a couple of Dell PowerEdge R710 servers, complete with instructions for how to disassemble and reassemble them. Anyone who wants can wander in and take them apart. It was great fun watching a 5-year-old kid pulling components out of one of these&amp;#8230; As well as feeling a little weird, as I think I&amp;#8217;ve run these in production!&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2&gt;What I&amp;#8217;m reading&lt;/h2&gt;

&lt;p&gt;I don&amp;#8217;t have a ton to share this week &amp;#8212; honestly, the whole week feels like a blur &amp;#8212; but here are two books that I recommend.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.aliettedebodard.com/bibliography/novels/the-universe-of-xuya/the-red-scholars-wake/&quot;&gt;The Red Scholar&amp;#8217;s Wake, by Aliette de Bodard&lt;/a&gt;: As the blurb says, &amp;#8220;Lesbian space pirates!&amp;#8221; Also, a really wonderful novella about building a new relationship amidst grief, power differentials, politics, and space battles. I think I basically recommend everything that de Bodard writes, but especially this. And it basically stands alone! So you can read this first, without going back to the other stories in the same world.&lt;/li&gt;



&lt;li&gt;&lt;a href=&quot;https://www.harpercollins.com/products/dealers-of-lightning-michael-a-hiltzik?variant=40824247779362&quot;&gt;Dealers of Lightning: XEROX PARC and the Dawn of the Computer Age, by Michael Hiltzik&lt;/a&gt;: I&amp;#8217;ve just started this, but it&amp;#8217;s already a really interesting snapshot of a key period in the development of the personal computer.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Recent recipes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://smittenkitchen.com/2019/12/unfussy-sugar-cookies/&quot;&gt;Smitten Kitchen&amp;#8217;s Unfussy Sugar Cookies&lt;/a&gt;: These cookies did, indeed, prove to be both tasty and easy to make. If you just want some easy cookies to snack on, I absolutely recommend this recipe.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Pet photos&lt;/h2&gt;

&lt;figure class=&quot;wp-block-image size-large is-resized&quot;&gt;&lt;img alt=&quot;Phyrne the calico cat stares down into the camera from a stairway&quot; class=&quot;wp-image-279&quot; height=&quot;414&quot; src=&quot;https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_6881-768x1024.jpg&quot; width=&quot;310&quot; /&gt;&lt;/figure&gt;

&lt;figure class=&quot;wp-block-image size-large is-resized&quot;&gt;&lt;img alt=&quot;Close-up on the face of Percy the gray tabby cat&quot; class=&quot;wp-image-280&quot; height=&quot;420&quot; src=&quot;https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_6879-768x1024.jpg&quot; width=&quot;314&quot; /&gt;&lt;/figure&gt;

&lt;figure class=&quot;wp-block-image size-large is-resized&quot;&gt;&lt;img alt=&quot;Benny the golden doodle curled up on a dog bed&quot; class=&quot;wp-image-281&quot; height=&quot;238&quot; src=&quot;https://thinking.ajdecon.org/wp-content/uploads/2022/12/IMG_6876-1024x768.jpg&quot; width=&quot;317&quot; /&gt;&lt;/figure&gt;</content><author><name>Thinking Out Loud</name></author><category term="ajdecon" /><summary type="html">What&amp;#8217;s new</summary></entry><entry><title type="html">An Initial Look at Deep Learning IO Performance</title><link href="https://hpc.social/personal-blog/2022/an-initial-look-at-deep-learning-io-performance/" rel="alternate" type="text/html" title="An Initial Look at Deep Learning IO Performance" /><published>2022-11-28T00:00:00-07:00</published><updated>2022-11-28T00:00:00-07:00</updated><id>https://hpc.social/personal-blog/2022/an-initial-look-at-deep-learning-io-performance</id><content type="html" xml:base="https://hpc.social/personal-blog/2022/an-initial-look-at-deep-learning-io-performance/">&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;

&lt;p&gt;This blog post describes an investigation of IO behavior of TensorFlow and PyTorch during resnet50 training running on Lambda Lab’s 8x V100 GPU instances.  Both ephemeral local NVMe storage and network attached persistent storage was tested.  The local NVMe storage was fast enough to achieve a throughput rate required to hit synthetic test targets.  The network attached persistent storage may not be able to fully saturate 8 V100 GPUs during training, though can achieve nearly the same level of performance as the local storage so long as TFRecords are utilized.  Further, there are specific behaviors and bottlenecks in TensorFlow and PyTorch that can reduce training performance when using real data from ImageNet.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;Thank you to Michael Balaban at Lambda Labs for providing access to their GPU cloud for this testing.  Thank you to Chuan Li for the creation of his TensorFlow benchmarking tools.  Thank you also to Andrej Karpathy, Toby Boyd, Yanan Cao, Sanjoy Das, Thomas Joerg, and Justin Lebar for their excellent blog posts on deep learning and XLA performance that helped inform this article.  I hope that this post will be useful for others as your work and writing was useful for me.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;…just because you can formulate your problem as RL doesn’t mean you should. If you insist on using the technology without understanding how it works you are likely to fail.&lt;/em&gt;&lt;/p&gt;


  &lt;p&gt;        Andrej Karpathy, &lt;a href=&quot;https://karpathy.github.io/2019/04/25/recipe/&quot;&gt;A Recipe for Training Neural Networks&lt;/a&gt;, 2019&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;That was the phrase that stuck in my head when I first started this project.   What project you may ask?  I want to understand how deep learning experiments utilize fast storage devices.  Not just any experiments either: &lt;em&gt;real&lt;/em&gt; ones, preferably big.  That’s how I happened upon Andrej Karpathy’s blog.  He is the former Sr. Director of AI at Tesla and knows a thing or two about training big neural networks.  I’ve spent the last decade working on Ceph and have worked on distributed systems and distributed storage for nearly 2 decades at this point.  But training neural nets?  The closest I’ve come was back in the early 2000s when I tried to build a tool to predict video game framerates.  I scraped benchmark numbers from review websites and built M5 decision trees based on hardware and video card settings.  It sort of worked, but was terribly overtrained on a small (~4000 sample) dataset.  Training with petabytes of data to teach an AI how to responsibly drive a car?  I can already feel a bit of imposter syndrome setting in.&lt;/p&gt;

&lt;p&gt;Thankfully my goal is comparatively modest.  I don’t need to build a cutting edge classifier or explore the intricacies of manually implementing back-propagation.  I simply want to understand the IO patterns that are involved when training big datasets with fast GPUs so I can help researchers speed up their work.  Up until now, my ability to do this was fairly limited.  At the day job I’ve had access to a small group of nodes with extremely modest GPUs.  I set up runs with MLPerf but the datasets (WMT G-E and CoCo) easily fit into memory. Other than a short burst of read traffic at the very beginning of training there was very little IO.  Recently I had the opportunity to meet Michael Balaban, Co-Founder of &lt;a href=&quot;https://lambdalabs.com/&quot;&gt;Lambda Labs&lt;/a&gt;.  I told him what I wanted to do and he gave me access to Lambda’s GPU cloud and beta persistent storage to give it a try.  I was able to grab one of Lambda’s 8x Tesla V100 instances (These things are incredibly popular so it’s best to grab one early in the morning!).  Not all of Lambda’s instance types currently have access to the persistent storage but the V100 instances in the Texas zone do.  Once secured, I got to work.&lt;/p&gt;

&lt;h2 id=&quot;tensorflow---synthetic&quot;&gt;TensorFlow - Synthetic&lt;/h2&gt;

&lt;p&gt;Before even attempting to run tests with real data, I realized I needed a baseline to start with.  Luckily, Chuan Li, Lambda’s Chief Scientific Officer, wrote a tool for running TensorFlow benchmarks and made it available on github &lt;a href=&quot;https://github.com/lambdal/lambda-tensorflow-benchmark&quot;&gt;here&lt;/a&gt;. One of the advantages of Lambda’s cloud is that they’ve already bundled up many popular tools for running deep-learning workloads into one package called &lt;a href=&quot;https://lambdalabs.com/lambda-stack-deep-learning-software&quot;&gt;Lambda Stack&lt;/a&gt; which comes pre-installed when you start an instance.  This made it fast to get started, though I did run into one issue.  Lambda Stack comes standard with TensorFlow 2, but Chuan Li’s tool relies on a TensorFlow benchmark submodule that is designed to work with TensorFlow 1.  Luckily, the parent repository was unofficially updated to work with Tensorflow 2 (with a warning that it is no longer being maintained).  A quick “git checkout master” in the “benchmarks” submodule directory got everything working.  Chuan Li’s tool makes it simple to run tests with several preconfigured templates already included.  I chose the fp16 resnet50 configuration as it should be fast at processing images and is fairly standard.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;TF_XLA_FLAGS=--tf_xla_auto_jit=2 ./batch_benchmark.sh X X 1 100 2 config/config_resnet50_replicated_fp16_train_syn
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Using the invocation provided in the benchmark README.md file, I was able to quickly run benchmarks with synthetic data on up to 8 V100 GPUs in the node.  At one point I got stuck, hitting what appeared at first to be an unexplainable 25% performance loss. I reran the tests multiple times and even monitored GPU clockspeeds/temperatures in nvidia-smi with no luck.  Ultimately I discovered my error.  In the slow cases, I had inadvertently left out the “TF_XLA_FLAGS=–tf_xla_auto_jit=2” environment variable.  It turns out that setting this allows Tensorflow compile and execute functions with XLA (Accelerated Linear Algebra) support which is a pretty big win for these tests.&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://markhpc.github.io/images/2022-11-28-Lambda/Tensorflow_-_ResNet50_Synthetic_Training_fp16.svg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;At this point I decided that I needed to understand how Chuan Li’s tool works.  It turns out that he is using the same base tf_cnn_benchmarks.py benchmark code that companies like Nvidia and Dell also use for benchmarking their GPU solutions.  I spent some time running it directly with Dell’s settings from their deep learning overview &lt;a href=&quot;https://infohub.delltechnologies.com/l/high-speed-object-storage-for-deep-learning/overview-3284&quot;&gt;here&lt;/a&gt;.  Unfortunately those tests had mixed results, even after various tweaks.  While researching the XLA issues I mentioned earlier however, I made an even better &lt;a href=&quot;https://blog.tensorflow.org/2018/11/pushing-limits-of-gpu-performance-with-xla.html&quot;&gt;discovery&lt;/a&gt; on the TensorFlow website.  I found an excellent blog post with performance data written by some of the core Tensorflow developers.  It’s now 4 years old, but still appears to be quite valid.  The tuning options used were both simpler and resulted in higher performance versus other configurations that I’ve come across.&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://markhpc.github.io/images/2022-11-28-Lambda/Tensorflow_-_ResNet50_Synthetic_Training_fp16_blog_compare.svg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Training with synthetic data in Lambda’s cloud resulted in similar performance to what the Tensorflow developer’s reported.  In fact, using their own settings yielded slightly faster results when running on Lambda’s 8xV100 instance!  It was incredibly encouraging to me that even in Lambda’s cloud environment with virtual machine instances I could achieve performance that was as fast or faster than what the Tensorflow developers were reporting.&lt;/p&gt;

&lt;h1 id=&quot;choosing-a-real-data-set&quot;&gt;Choosing a Real Data Set&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;The first step to training a neural net is to not touch any neural net code at all and instead begin by thoroughly inspecting your data.&lt;/em&gt;&lt;/p&gt;


  &lt;p&gt;        Andrej Karpathy, &lt;a href=&quot;https://karpathy.github.io/2019/04/25/recipe/&quot;&gt;A Recipe for Training Neural Networks&lt;/a&gt;, 2019&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;Having convinced myself that I had Tensorflow operating reasonably efficiently in synthetic tests, it was time to start thinking about what dataset to use for “real” training.  The largest and most obvious choice is ImageNet.  ImageNet is composed of over 1.2 million categorized images that form a roughly 160GB training dataset.  It is also the largest dataset I could find that was publicly accessible. Downloading it isn’t so easy however. The only version that I could access is the ImageNet Object Localization Challenge dataset hosted on &lt;a href=&quot;https://www.kaggle.com/c/imagenet-object-localization-challenge&quot;&gt;kaggle&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;After finally figuring out how to download the data, it was time to follow Andrej’s advice and try to learn something about it.  While ImageNet is curated and annotated, it has many images of different sizes, dimensions, and pixel counts.  Images also come from many sources with different levels of quality.  Through the power of stack-exchange I was able to find a bash one-liner script to generate a histogram of image sizes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;find . -type f -print0 | xargs -0 ls -l | awk '{size[int(log($5)/log(2))]++}END{for (i in size) printf(&quot;%10d %3d\n&quot;, 2^i, size[i])}' | sort -n
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://markhpc.github.io/images/2022-11-28-Lambda/ImageNet_-_Image_Distribution_by_Approximate_Size.svg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Roughly 80% of the images are in the 64KB or 128KB size bins. Almost all of the remaining images are smaller.  That gives us a pretty good idea of what kind of IOs to expect during classification.  Or at least…it does for frameworks that read those images directly.  In Tensorflow’s case, there’s an alternative format called TFRecord.  TFRecords are basically collections of image data sequentially laid out in much larger files.  Instead of iterating over thousands or millions of individual image files, TFRecords allow Tensorflow to instead stream fewer, larger files that each house multiple images.  It’s a one time cost to pre-process the data so Tensorflow has less work to do during training.  After I downloaded the ImageNet data I took a shot at converting the ImageNet LOC data into TensorFlow records.  Luckily, the TensorFlow tpu github repository already has a &lt;a href=&quot;https://github.com/tensorflow/tpu/blob/master/tools/datasets/README.md&quot;&gt;tool&lt;/a&gt; that can do this.  I had to manipulate the dataset slightly, but ultimately this process worked (at least for the training data):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pip install gcloud google-cloud-storage
pip install protobuf==3.20.1

mkdir ~/data/ImageNetFoo
ln -s ~/data/ImageNet/ILSVRC/Data/CLS-LOC/train ~/data/ImageNetFoo/train
ln -s ~/data/ImageNet/ILSVRC/Data/CLS-LOC/val ~/data/ImageNetFoo/val
ln -s ~/data/ImageNet/ILSVRC/Data/CLS-LOC/test ~/data/ImageNetFoo/test
ln -s ~/data/ImageNet/LOC_synset_mapping.txt ~/data/ImageNetFoo/synset_labels.txt
python imagenet_to_gcs.py --raw_data_dir=/home/ubuntu/data/ImageNetFoo --local_scratch_dir=/home/ubuntu/ExaltedOrbs/ImageNet/tf_records --nogcs_upload
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Perhaps I should say that this worked so long as the original dataset was located on the local NVMe drive.  The persistent storage didn’t fare as well.  Attempting to decompress ImageNet on the persistent storage resulted in blowing past the max number of open files allowed with errors like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;OSError: [Errno 24] Too many open files.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Unfortunately this couldn’t be fixed on the instance.  It appeared to be passed through from the host and the persistent storage was completely unusable until the instance was rebooted.  Recently I spoke to one of Lambda’s engineers and they are working on a fix. (It may already be implemented by the time you read this!)  I also want to note that the persistent storage is still in beta so issues like this are not entirely unexpected.  Having said that, before hitting the error it was significantly slower extracting ImageNet on the persistent storage vs on the local NVMe storage.  It’s probably best to extract ImageNet locally and then write the large TFRecords to the persistent storage during the conversion process.  Luckily extracting ImageNet to local storage was fine, and storing the original archive and the resulting TFRecords on the persistent storage worked perfectly fine as well.&lt;/p&gt;

&lt;h2 id=&quot;fio---baseline-io-results&quot;&gt;FIO - Baseline IO Results&lt;/h2&gt;

&lt;p&gt;Next, I turned my attention to running baseline tests on Lambda’s local and persistent storage using fio.  Fio is a highly configurable and well respected benchmark in the storage community and perfect for generating baseline results.  I decided to use a dataset size that is roughly similar to ImageNet (200GB), the libaio engine in fio with direct IO, and an appropriately high IO depth to let the NVMe drives stretch their legs a bit.&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://markhpc.github.io/images/2022-11-28-Lambda/Lambda_Labs_8xv100_Storage.svg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Throughput with the local NVMe drive(s) is surprisingly good.  The persistent storage is slower, but still might be fast enough at a little over 1GB/s for large reads.  16K IOPS was somewhat slower in both cases.  I chose 16K so that I could quickly compare to tests I ran in my Ceph QEMU/KVM performance blog post &lt;a href=&quot;https://ceph.io/en/news/blog/2022/qemu-kvm-tuning/&quot;&gt;here&lt;/a&gt;.  Without getting into the details, I suspect there’s still some room for improved IOPS with Lambda’s setup.  Luckily though, converting into TFRecords should make Tensorflow throughput bound instead of latency bound.  What about PyTorch or other tools that want to read images directly though?  Fio gives us the ability to simulate it by using its ‘bssplit’ feature.  We can take the size ranges and percentiles generated when examining ImageNet and give fio a similar distribution:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fio --ioengine=libaio --direct=1 --bssplit=2K/1:4K/2:8K/4:16K/8:32K/13:64K/38:128K/33:256K/1 --iodepth=128 --rw=randread --norandommap --size=200G --numjobs=1 --runtime=300 --time_based --name=foo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://markhpc.github.io/images/2022-11-28-Lambda/Lambda_Labs_8xV100_Storage_Reads_Second_Bssplit.svg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This isn’t exactly right as we are not reading data spread across millions of files, but it should provide something of an upper bound on what to expect.  It looks like the persistent storage can do approximately 10K reads/second at a throughput rate of around 750MB/s.  The local storage is about 3-4 times faster.  Local storage should be fast enough to support the kind of images/second throughput rates we want to hit in Tensorflow on 8 V100 GPUs, but the jury is still out for the persistent storage.&lt;/p&gt;

&lt;h2 id=&quot;tensorflow---imagenet&quot;&gt;Tensorflow - ImageNet&lt;/h2&gt;

&lt;p&gt;Running benchmarks with real data rather than synthetic data is fairly straightforward in Tensorflow.  You simply append data_dir and data_name flags to the CLI invocation to let it know where the TFRecords are located:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sync; echo 3 | sudo tee /proc/sys/vm/drop_caches
python ./tf_cnn_benchmarks.py --batch_size=256 --num_batches=100 --model=resnet50 --optimizer=momentum --variable_update=replicated --all_reduce_spec=nccl --use_fp16=True --nodistortions --gradient_repacking=2 --compute_lr_on_cpu=True --single_l2_loss_op=True --xla_compile=True --num_gpus=8 --loss_type_to_report=base_loss --data_dir=/home/ubuntu/ImageNet-TF/train --data_name=imagenet
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://markhpc.github.io/images/2022-11-28-Lambda/Tensorflow_-_ResNet50_Real_Training_First_Attempt_fp16.svg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ouch.  Much lower performance with the ImageNet data vs synthetic!  This is especially unfortunate given that 4 years ago the Tensorflow developers reported much better results.  I spent some time reading and experimenting with different settings.  Ultimately the one setting that made a substantial difference was “datasets_num_private_threads”.  In the Tensorflow benchmark source code, this setting is described as: “[The] number of threads for a private threadpool created for all datasets computation.”  I’ll go into more detail what these threads are doing in a bit. For now, let’s see how increasing the number of threads affects the results:&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://markhpc.github.io/images/2022-11-28-Lambda/Tensorflow_-_ResNet50_ImageNet_Training_fp16_private_threads.svg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Increasing the number of private threads has a dramatic effect on performance, though I was unable to fully match the performance achieved in the synthetic tests on either the local or persistent storage.  The local storage fared better at high thread counts gradually topping out at around 8600 images/second.  At high private thread counts the persistent storage topped out between 7000-8000 images/second with a higher degree of variability between runs.  I suspect that in this case the persistent storage has likely hit its (per instance) limit.&lt;/p&gt;

&lt;p&gt;In addition to having a dramatic effect on performance, changing the private thread count also had a large effect on the CPU consumption of the TensorFlow process.  CPU usage increases almost linearly with additional private threads up to around 30 cores.  What exactly are these private threads doing?  To answer that question, I utilized two tools that I often deploy when diagnosing CPU usage in Ceph.  When testing with a lower number of private threads, I used linux’s perf tool to look at where cycles are being consumed when the private threads are fully saturated.  At higher levels of private threads, I used my wallclock profiler &lt;a href=&quot;https://github.com/markhpc/uwpmp&quot;&gt;uwpmp&lt;/a&gt; to look at how private threads spend their time when increasing the thread count no longer improves performance.&lt;/p&gt;

&lt;p&gt;In the first case with perf, we can get a good view of the work that these private threads are doing:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;--77.31%--tensorflow::ThreadPoolDevice::Compute
          |          
          |--51.19%--0x7f511a00c7d8
          |          |          
          |           --51.18%--tensorflow::jpeg::Uncompress
          |--14.48%--tensorflow::ResizeBilinearOp&amp;lt;Eigen::ThreadPoolDevice, unsigned char&amp;gt;::Compute
          |--5.47%--tensorflow::CastOpBase::Compute
          |--2.66%--tensorflow::ReverseV2Op&amp;lt;Eigen::ThreadPoolDevice, unsigned char, int&amp;gt;::Compute
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The majority of the cycles consumed is in jpeg decompression and resize operations, along with a smattering of other stuff.  What happens if we look at a case with a higher private thread count but now look at wallclock time instead of cycles?  I ended up having some trouble getting the profiler to work properly and consistently get clean callgraphs, but I was able to get at least one run in that revealed some interesting information.  First, I saw time spent in the same functions that perf told us we were spending cycles in:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;+ 100.00% Eigen::ThreadPoolTempl&amp;lt;tensorflow::thread::EigenEnvironment&amp;gt;::WorkerLoop(int)
 + 99.90% ???
 |+ 97.30% ???
 ||+ 92.40% ???
 |||+ 77.10% _PyEval_EvalFrameDefault
 ||||+ 47.20% ???
 |||||+ 38.10% tensorflow::jpeg::Uncompress(void const*, int, tensorflow::jpeg::UncompressFlags const&amp;amp;, long*, std::function&amp;lt;unsigned char* (int, int, int)&amp;gt;)
 ||||+ 12.20% tensorflow::ResizeBilinearOp&amp;lt;Eigen::ThreadPoolDevice, unsigned char&amp;gt;::Compute(tensorflow::OpKernelContext*)
 ||||+ 4.40% tensorflow::CastOpBase::Compute(tensorflow::OpKernelContext*)
 ||||+ 1.70% tensorflow::ReverseV2Op&amp;lt;Eigen::ThreadPoolDevice, unsigned char, int&amp;gt;::Compute(tensorflow::OpKernelContext*)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But the wallclock profile also exposed that there may be contention in multiple areas in the private threads around some of the nsync synchronization primitives being used:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; |||||||    |  + 4.50% nsync::nsync_mu_semaphore_p(nsync::nsync_semaphore_s_*)
 |||||||    |   + 4.50% syscall

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This almost always appeared nested deep inside:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tensorflow::BFCAllocator::AllocateRaw(unsigned long, unsigned long, tensorflow::AllocationAttributes const&amp;amp;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Sadly I was missing a number of debug symbols and don’t 100% trust the wallclock trace.  For now I’ll just say that the private threads are doing a significant amount of work decompressing and manipulating the image data to keep the GPUs fed.  I suspect that with newer and faster GPUs the image retrieval pipeline could become an even bigger issue when training with real image data.  The mystery for me is how The TensorFlow developers achieved such good results 4 years ago without using dedicated private threads at all.  Perhaps they had a significantly faster jpeg decompression mechanism that I am unaware of?&lt;/p&gt;

&lt;h2 id=&quot;pytorch---imagenet&quot;&gt;PyTorch - ImageNet&lt;/h2&gt;

&lt;p&gt;After running Tensorflow, I also ran some benchmarks in PyTorch using Nvidia’s “DeepLearningExamples” github &lt;a href=&quot;https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Classification/ConvNets/resnet50v1.5&quot;&gt;repo&lt;/a&gt;.  First, I installed the prereqs and setup the repository:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pip install 'git+https://github.com/NVIDIA/dllogger'
pip install --extra-index-url https://developer.download.nvidia.com/compute/redist --upgrade nvidia-dali-cuda110
git clone https://github.com/NVIDIA/DeepLearningExamples
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, prepared ImageNet for usage in PyTorch:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd ~/data/ImageNet/ILSVRC/Data/CLS-LOC/val
wget -qO- https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh | bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And finally ran a test:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd DeepLearningExamples/PyTorch/Classification/ConvNets
sync; echo 3 | sudo tee /proc/sys/vm/drop_caches
python ./multiproc.py --nproc_per_node 1 ./main.py --arch resnet50 --label-smoothing 0.1 --run-epoch 1 --amp --static-loss-scale 256 --workspace /home/ubuntu/data/ImageNet-Scratch /home/ubuntu/data/ImageNet-Orig/ILSVRC/Data/CLS-LOC/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are a couple of differences here versus the TensorFlow tests.  First, I’m using the raw ImageNet archive instead of a preprocessed TFRecord dataset, so the read behavior is different.  Because I was unable to extract or copy the raw ImageNet archive onto the persistent storage, I’m also only testing the local NVMe drive.  Finally, I didn’t see any specific examples for running with fp16 in nVidia’s documentation, so I’m using amp (automatic mixed precision) which may be slightly slower.&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://markhpc.github.io/images/2022-11-28-Lambda/Pytorch_-_ResNet50v15_ImageNet_Training_AMP.svg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Given the number of differences it’s tough to draw direct comparisons with Tensorflow.  Amp is one difference, but it’s quite possible that there are tuning options that could improve performance here that I don’t know about.  I did notice that PyTorch, like Tensorflow, is using quite a bit of CPU to keep the GPUs working.  I suspect that there are ways to tweak the IO pipeline that could improve performance.  For now though, let’s compare the IO patterns on the local NVMe drive during the Tensorflow and PyTorch runs.  I was hoping to be able to use blktrace to do this, but unfortunately was unable to get any data from the virtual devices in the instance.  I was able to collect more general statistics using collectl however.&lt;/p&gt;

&lt;h5 id=&quot;disk-read-statistics-during-pytorch-8-gpu-run&quot;&gt;Disk Read Statistics During PyTorch 8 GPU run:&lt;/h5&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Time&lt;/th&gt;
      &lt;th&gt;Name&lt;/th&gt;
      &lt;th&gt;KBytes&lt;/th&gt;
      &lt;th&gt;Merged&lt;/th&gt;
      &lt;th&gt;IOs&lt;/th&gt;
      &lt;th&gt;Size&lt;/th&gt;
      &lt;th&gt;Wait&lt;/th&gt;
      &lt;th&gt;QLen&lt;/th&gt;
      &lt;th&gt;SvcTim&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;00:29:18&lt;/td&gt;
      &lt;td&gt;vda&lt;/td&gt;
      &lt;td&gt;761136&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;6746&lt;/td&gt;
      &lt;td&gt;113&lt;/td&gt;
      &lt;td&gt;58&lt;/td&gt;
      &lt;td&gt;431&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;00:29:19&lt;/td&gt;
      &lt;td&gt;vda&lt;/td&gt;
      &lt;td&gt;752172&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;6648&lt;/td&gt;
      &lt;td&gt;113&lt;/td&gt;
      &lt;td&gt;112&lt;/td&gt;
      &lt;td&gt;810&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;00:29:20&lt;/td&gt;
      &lt;td&gt;vda&lt;/td&gt;
      &lt;td&gt;747824&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;6595&lt;/td&gt;
      &lt;td&gt;113&lt;/td&gt;
      &lt;td&gt;84&lt;/td&gt;
      &lt;td&gt;604&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;00:29:21&lt;/td&gt;
      &lt;td&gt;vda&lt;/td&gt;
      &lt;td&gt;735964&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;6583&lt;/td&gt;
      &lt;td&gt;112&lt;/td&gt;
      &lt;td&gt;73&lt;/td&gt;
      &lt;td&gt;551&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;00:29:22&lt;/td&gt;
      &lt;td&gt;vda&lt;/td&gt;
      &lt;td&gt;695636&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;6237&lt;/td&gt;
      &lt;td&gt;112&lt;/td&gt;
      &lt;td&gt;102&lt;/td&gt;
      &lt;td&gt;760&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h5 id=&quot;disk-read-statistics-during-tensorflow-8-gpu-run&quot;&gt;Disk Read Statistics During TensorFlow 8 GPU run:&lt;/h5&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Time&lt;/th&gt;
      &lt;th&gt;Name&lt;/th&gt;
      &lt;th&gt;KBytes&lt;/th&gt;
      &lt;th&gt;Merged&lt;/th&gt;
      &lt;th&gt;IOs&lt;/th&gt;
      &lt;th&gt;Size&lt;/th&gt;
      &lt;th&gt;Wait&lt;/th&gt;
      &lt;th&gt;QLen&lt;/th&gt;
      &lt;th&gt;SvcTim&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;00:38:45&lt;/td&gt;
      &lt;td&gt;vda&lt;/td&gt;
      &lt;td&gt;1081324&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;8440&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;00:38:46&lt;/td&gt;
      &lt;td&gt;vda&lt;/td&gt;
      &lt;td&gt;927512&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;7241&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;00:38:47&lt;/td&gt;
      &lt;td&gt;vda&lt;/td&gt;
      &lt;td&gt;913512&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;7130&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;00:38:48&lt;/td&gt;
      &lt;td&gt;vda&lt;/td&gt;
      &lt;td&gt;1047444&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;8186&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;00:38:49&lt;/td&gt;
      &lt;td&gt;vda&lt;/td&gt;
      &lt;td&gt;968776&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;7560&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;
When just looking at the IO sizes, both runs appear similar, but that doesn’t tell the whole story.  It is likely that Tensorflow is doing much larger reads that are broken up into contiguous 128KB chunks by the block layer based on the underlying device’s max_sectors_kb setting.  The tells here are the very low queue length and wait times for the TensorFlow run versus the PyTorch run.  In both case the device service times are low (0), but in the TensorFlow case IOs are still backing up in the device queue.&lt;/p&gt;

&lt;p&gt;Interestingly, it appears that it may be possible to use nVidia’s DALI (Data Loading Library) package to &lt;a href=&quot;https://docs.nvidia.com/deeplearning/dali/archives/dali_170/user-guide/docs/examples/frameworks/pytorch/pytorch-various-readers.html&quot;&gt;read TFRecords into PyTorch&lt;/a&gt;.  I didn’t have time to attempt it, but potentially that could have a big effect on IO behavior and performance as well.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;As I’ve been writing this post, I realize just how complicated it is to understand the performance characteristics of training of neural networks.  Even as we talk about metrics like images/second, the options that are used (batch size for instance) can also affect convergence.  It’s very difficult to come up with a common methodology that is always better than others.  I wonder if another metric, like reaching a desired level of convergence, would be better in the end.  Having said that, I am glad for having done this exercise as I learned some valuable things:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Pre-processing data into a format like TFRecords on fast local storage is a big win from an IO perspective.  It lets storage systems that have slow metadata performance succeed so long as they have enough sequential read throughput to keep the machine learning framework busy.  This is a big win for many distributed file systems that may have substandard metadata performance (and even the good ones may still benefit).&lt;/p&gt;

  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To train on a dataset like ImageNet, you need somewhere around 1-1.3GB/s of raw disk throughput to keep 8 V100 GPUs busy when training in fp16.  For amp or fp32 the requirements are likely lower since the GPUs can’t work quite as fast.  With modern GPUs that are faster than the V100, the disk throughput requirements could be significantly higher.&lt;/p&gt;

  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Lambda’s local NVMe storage is likely fast enough to saturate 8 GPUs, even newer ones, so long as the rest of the IO path can keep up.  The persistent storage appears to become a bottleneck with sufficient GPUs and TensorFlow private threads, though can still function fairly well so long as TFRecords are used.  A concern going forward is how to ensure that the data pipeline in TensorFlow and PyTorch are fast enough to keep the GPUs fed.  The Tensorflow benchmark required a large number of private threads and showed potential evidence of contention at high thread counts.  PyTorch did not appear to natively support TFRecords, but NVidia DALI or other 3rd party code might help improve the IO path.&lt;/p&gt;

  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If it’s necessary to train directly with images rather than TFRecords, it may not make sense to host them on shared file systems.  It appears that Tensorflow and possibly PyTorch give users the ability to specify a separate training data and work directory.  If all operations against the training data are reads, it may be better to host datasets on read-only block device snapshots. For instance with Ceph, perhaps you could create a read/write RBD volume where you put a certain dataset, take a snapshot, and then map that snapshot as read only on multiple instances that all need access to the same image set.&lt;/p&gt;

  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Even with a training set as large as ImageNet, Lambda’s instances have so much memory that eventually the entire dataset becomes cached.  It was necessary to sync and drop caches before each test and keep tests short enough that they didn’t re-read the same data from buffer cache.  I was able to watch as long running tests eventually stopped performing reads and got faster as time went on.  This could make apples-to-apples comparison between different storage vendors difficult if not carefully controlled.&lt;/p&gt;

  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;I’m almost certainly missing additional tweaks that can help speed up both Tensorflow and PyTorch.  This post shouldn’t be seen as the be-all/end-all for how to achieve high performance with these frameworks, but I hope it may at least help showcase some of the areas that are valuable to investigate when trying to train with real data and achieve high performance.&lt;/p&gt;

  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This wraps up my initial work looking at Deep Learning IO behavior.  I hope that next time I can come armed with a bit more knowledge about the internals of how PyTorch and Tensorflow work, focus a bit more on the quality of the training, find even larger datasets to work with, and maybe actually accomplish something useful rather than just play with ImageNet.&lt;/p&gt;

&lt;p&gt;Thanks for reading!&lt;/p&gt;</content><author><name>Mark Nelson's Blog</name></author><category term="markhpc" /><summary type="html">Abstract</summary></entry><entry><title type="html">Adam’s weekly update, 2022-11-27</title><link href="https://hpc.social/personal-blog/2022/adam-s-weekly-update-2022-11-27/" rel="alternate" type="text/html" title="Adam’s weekly update, 2022-11-27" /><published>2022-11-27T15:28:16-07:00</published><updated>2022-11-27T15:28:16-07:00</updated><id>https://hpc.social/personal-blog/2022/adam-s-weekly-update-2022-11-27</id><content type="html" xml:base="https://hpc.social/personal-blog/2022/adam-s-weekly-update-2022-11-27/">&lt;h2&gt;What&amp;#8217;s new&lt;/h2&gt;

&lt;p&gt;The first thing that&amp;#8217;s new is&amp;#8230; this post! I&amp;#8217;m going to try to do at least a weekly post on the blog now, just a general update and some links. This will &lt;em&gt;hopefully&lt;/em&gt; help me get back into the habit of writing on the blog regularly, and maybe inspire me to write a bit more in general.&lt;/p&gt;

&lt;p&gt;&lt;span id=&quot;more-264&quot;&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;I was off work this week for the Thanksgiving holiday, and traveled Michigan to visit my parents and my brother&amp;#8217;s family. My mom has been struggling with some pretty major health issues this year, so it was really wonderful and reassuring to get to spend some time with her and my dad. I also finally got to meet my brother&amp;#8217;s three-year-old son, who was born &lt;em&gt;right&lt;/em&gt; before the pandemic started, and who I hadn&amp;#8217;t managed to meet up until now.&lt;/p&gt;

&lt;p&gt;On the tech-related front, I used this week to take a break from Twitter (mostly), and to be honest&amp;#8230; it was kinda refreshing! I had developed a pretty bad Twitter habit this year, doomscrolling for more time than I like to admit. While I really like Twitter and I&amp;#8217;ve had some nice career boosts from it, it was also a time sink that was not entirely healthy.&lt;/p&gt;

&lt;p&gt;Admittedly, that time was somewhat replaced by playing around on the &lt;a href=&quot;https://calico.social/ajdecon&quot;&gt;Fediverse / Mastodon&lt;/a&gt;. But with the lack of algorithmic suggestions, quote tweets, and other means of virality, that network so far feels a lot quieter and less time-consuming than Twitter. Tim Bray has a &lt;a href=&quot;https://www.tbray.org/ongoing/When/202x/2022/11/26/Bye-Twitter&quot;&gt;good post&lt;/a&gt; up which discusses some of the advantages and pitfalls of federated social media, and I can highly recommend reading that. I&amp;#8217;m still a bit skeptical that it will be a practical &amp;#8220;Twitter replacement&amp;#8221; for most people, but so far I&amp;#8217;m finding it pleasant.&lt;/p&gt;

&lt;h2&gt;What I&amp;#8217;m reading&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Nonfiction book: &lt;/strong&gt;&lt;a href=&quot;https://bookshop.org/p/books/code-the-hidden-language-of-computer-hardware-and-software-charles-petzold/18465738&quot;&gt;Code, Second Edition, by Charles Petzold&lt;/a&gt;. This book walks through the process of building a working computer, starting with ideas like Morse code, then working up from logic gates on up. This is technically a re-read, as I read the first edition&amp;#8230; 10+ years ago? But I&amp;#8217;m getting a lot more out of it this time around, and really enjoying it.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Fiction book: &lt;/strong&gt;&lt;a href=&quot;https://bookshop.org/p/books/the-spare-man-mary-robinette-kowal/18834426&quot;&gt;The Spare Man, by Mary Robinette Kowal&lt;/a&gt;. A cozy murder mystery on a luxury cruise to Mars. I&amp;#8217;m only a few chapters in, but already greatly enjoying myself.&lt;/li&gt;



&lt;li&gt;&lt;a href=&quot;https://ferd.ca/hiding-theory-in-practice.html&quot;&gt;&amp;#8220;Hiding theory in practice&amp;#8221;, by Fred Hebert&lt;/a&gt;. I&amp;#8217;ve been reading a lot about safety engineering and its application to computing lately, but that community can sometimes get off into the weeds about points of theory that don&amp;#8217;t have consensus in the broader computing community. This post has a good discussion of how to use the theory of safety engineering to guide decisions, without requiring that everyone working with you be handed a reading list.&lt;/li&gt;



&lt;li&gt;&lt;a href=&quot;https://cohost.org/mononcqc/post/385225-paper-repentance-as&quot;&gt;&amp;#8220;Paper: Repentance as Rebuke: Betrayal and Moral Injury in Safety Engineering&amp;#8221;, also by Fred Hebert&lt;/a&gt;. A discussion of &lt;a href=&quot;https://link.springer.com/article/10.1007/s11948-022-00412-2&quot;&gt;a paper by Dekker &lt;em&gt;et al&lt;/em&gt;&lt;/a&gt; which looks at the aftermath of the 737 MAX air disasters, and the public repentance of some of the engineers who were involved. Go read the post, it&amp;#8217;s great. And I&amp;#8217;m planning to read the original paper this week.&lt;/li&gt;



&lt;li&gt;&lt;a href=&quot;https://chipsandcheese.com/2022/11/15/cannon-lake-intels-forgotten-generation/&quot;&gt;&amp;#8220;Cannon Lake: Intel&amp;#8217;s Forgotten Generation&amp;#8221;, from &lt;em&gt;Chips and Cheese&lt;/em&gt;&lt;/a&gt;. Really I&amp;#8217;ve been reading a bunch of the technical posts from &lt;em&gt;Chips and Cheese&lt;/em&gt; lately, and they&amp;#8217;re doing pretty good analyses of recent hardware. They&amp;#8217;ve definitely earned that spot in my RSS reader.&lt;/li&gt;



&lt;li&gt;&lt;a href=&quot;https://glennklockwood.blogspot.com/2022/11/sc22-recap.html&quot;&gt;Glenn K Lockwood&amp;#8217;s &amp;#8220;SC&amp;#8217;22 Recap&amp;#8221;&lt;/a&gt;. I was sad to miss Supercomputing this year, though enough folks have come down with COVID that I don&amp;#8217;t really regret the decision. But Glenn wrote up a really interesting recap post, with an interesting new viewpoint now that he&amp;#8217;s working at Microsoft Azure. Among other things, he included a whole section titled &lt;em&gt;The underwhelming&lt;/em&gt;, with the opening line &amp;#8220;The biggest deal appears to be that exascale is here, and it turns out that it&amp;#8217;s not that big of a deal.&amp;#8221;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Recent recipes&lt;/h2&gt;

&lt;p&gt;Because it was Thanksgiving, I did a lot of cooking this week! I&amp;#8217;m not going to list everything I made, but a few of my favorites were:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.delish.com/cooking/recipe-ideas/a23340027/cheesy-garlic-butter-rolls-recipe/&quot;&gt;Cheesy Garlic Butter Rolls from Delish&lt;/a&gt;: Nothing special, but really tasty.&lt;/li&gt;



&lt;li&gt;&lt;a href=&quot;https://smittenkitchen.com/2019/11/challah-stuffing/&quot;&gt;Challah Stuffing from Smitten Kitchen&lt;/a&gt;: This recipe was a huge winner, with most of the family coming back for seconds, and then having more the next day for leftovers. It was really good, and is probably what I&amp;#8217;ll make if I ever do stuffing again.&lt;/li&gt;



&lt;li&gt;&lt;a href=&quot;https://smittenkitchen.com/2008/09/best-challah-egg-bread/&quot;&gt;Best Challah from Smitten Kitchen&lt;/a&gt;: I baked the bread that went into the stuffing, and it was really tasty on its own! This recipe makes two loaves, and I only needed one for the stuffing. So I also made french toast with it, which worked really nicely.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Pet photos&lt;/h2&gt;

&lt;p&gt;Gotta have those pet photos.&lt;/p&gt;

&lt;figure class=&quot;wp-block-image size-large is-resized&quot;&gt;&lt;img alt=&quot;A blond golden doodle in a red harness and a blue bandanna lays on sandy dirt and looks into the camera&quot; class=&quot;wp-image-271&quot; height=&quot;233&quot; src=&quot;https://thinking.ajdecon.org/wp-content/uploads/2022/11/IMG_6863-1024x768.jpeg&quot; width=&quot;311&quot; /&gt;&lt;/figure&gt;

&lt;figure class=&quot;wp-block-image size-large is-resized&quot;&gt;&lt;img alt=&quot;A white calico cat sits on a blanket and washes her front paw&quot; class=&quot;wp-image-272&quot; height=&quot;410&quot; src=&quot;https://thinking.ajdecon.org/wp-content/uploads/2022/11/69075713241__19379770-6B0C-4780-8DD0-30C62A033C88-768x1024.jpeg&quot; width=&quot;308&quot; /&gt;&lt;/figure&gt;

&lt;figure class=&quot;wp-block-image size-large is-resized&quot;&gt;&lt;img alt=&quot;A gray-brown tabby cat wearing a green collar sitting on a wall, looking vaguely toward the camera&quot; class=&quot;wp-image-273&quot; height=&quot;405&quot; src=&quot;https://thinking.ajdecon.org/wp-content/uploads/2022/11/69073206299__DB9CA33B-0EB5-4681-96DA-8368554B6B8A-768x1024.jpeg&quot; width=&quot;304&quot; /&gt;&lt;/figure&gt;</content><author><name>Thinking Out Loud</name></author><category term="ajdecon" /><summary type="html">What&amp;#8217;s new</summary></entry><entry><title type="html">SC’22 Recap</title><link href="https://hpc.social/personal-blog/2022/sc-22-recap/" rel="alternate" type="text/html" title="SC’22 Recap" /><published>2022-11-24T02:00:00-07:00</published><updated>2022-11-24T02:00:00-07:00</updated><id>https://hpc.social/personal-blog/2022/sc-22-recap</id><content type="html" xml:base="https://hpc.social/personal-blog/2022/sc-22-recap/">&lt;p&gt;The biggest annual conference in HPC, the &lt;a href=&quot;https://sc22.supercomputing.org&quot;&gt;SC conference&lt;/a&gt;, was recently held in Dallas, Texas in its second hybrid incarnation since being all-remote for the pandemic. This year attracted over 11,000 attendees which is much closer to the pre-pandemic high of 14,000 than last year's 7,000, and judging from the crushed conference rooms and busy expo floor, it looks like SC is not that much worse for wear.&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;This year's conference quite different for me since I attended for my first time as a vendor, not a researcher or practitioner, and I spent most of my days behind closed doors talking to customers. I didn't get to attend any of the keynotes, BOFs, or panels to which I wasn't invited as a result, so I'm not really qualified to give an erudite summary of the conference or expo this year.&lt;/p&gt;
&lt;p&gt;So instead, I'm just writing down what I remember in order that I remember it and not necessarily in a coherent narrative form. I'm sure I missed a lot (for example, mixed precision seemed big this year, and I heard Jack Dongarra gave a fantastic Turing Award talk) so I encourage others to write their own recaps and share with the community!&lt;span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;h2 style=&quot;text-align: left;&quot;&gt;High-level themes&lt;/h2&gt;
&lt;p&gt;I actually started writing an SC'21 recap last year which I never posted, and re-reading the intro was funny--you'd think nothing has changed in the last year.&lt;/p&gt;
&lt;h3 style=&quot;text-align: left;&quot;&gt;The underwhelming&lt;/h3&gt;
&lt;p&gt;The biggest deal appears to be that exascale is here, and it turns out that it's not that big of a deal. China let the air out of the tires by debuting their exascale systems at SC'21, and not only did they thumb their nose at Top500 by not submitting, they debuted by winning a Gordon Bell prize instead. The first US exascale system, Frontier, was debuted at ISC this year leaving its showing at SC a bit deflated too. &lt;a href=&quot;https://www.hpcwire.com/2022/11/17/2022-gordon-bell-prize-goes-to-plasma-accelerator-research/&quot;&gt;Frontier was featured in the Gordon Bell prize-winning paper&lt;/a&gt; this year, but that work required the use of four Top-10 systems, not just Frontier, painting the reality that one giant computer rarely stands on its own when it comes to advancing science.&lt;/p&gt;
&lt;p&gt;This isn't to say that deploying exascale systems isn't a noteworthy feat and worth commendation, but I felt like the hype over the last five years treated the achievement like an end state instead of a milestone. And now that we've passed the milestone, the community is grasping to figure out what comes next. So what &lt;i&gt;is&lt;/i&gt; next?&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Quantum&lt;/b&gt; had a strong and growing presence at SC, as it has for the last few years. But the conclusion of the panel &quot;&lt;a href=&quot;https://www.hpcwire.com/2022/11/19/quantum-are-we-there-or-close-yet-no-says-the-panel/&quot;&gt;Quantum Computing: A Future for HPC Acceleration&lt;/a&gt;&quot; was that no, it's not close to being ready.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Disaggregation and composability&lt;/b&gt; was another theme with growing momentum. And like quantum, there was a panel asking the same question: &quot;&lt;a href=&quot;https://www.hpcwire.com/off-the-wire/informal-poll-of-sc22-attendees-suggests-a-bright-future-for-composability/&quot;&gt;Does HPC need composability now?&lt;/a&gt;&quot; The answer, again, was no, not yet. More on that below.&lt;/p&gt;
&lt;p&gt;What about &lt;b&gt;RISC-V&lt;/b&gt;? Surely that will revolutionize the field. As it turns out, the answer there is also that &lt;a href=&quot;https://www.hpcwire.com/2022/11/18/risc-v-is-far-from-being-an-alternative-to-x86-and-arm-in-hpc/&quot;&gt;RISC-V is not ready to do anything useful for HPC yet&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The list goes on of technologies and trends that people are trying to boost now that exascale is &quot;solved.&quot; The reality, I think, is that &quot;exascale&quot; will take years to actually mature since it appears to have a ton of technical debt that accumulated during the race to be first. US Exascale rests on the shoulders of AMD and Intel, two companies whose software stacks have not caught up to the market leader, so there will be a lot of thrashing around as development practices and optimization settle out around these systems.&lt;/p&gt;
&lt;p&gt;Struggling with code porting is not very exciting to computer science Ph.D.s, so I expect future SCs to mirror this one and bifurcate into two distinct tracks: those struggling to identify the next big thing in the research space, and those struggling to use the systems that were rushed to deployment.&lt;/p&gt;
&lt;h3 style=&quot;text-align: left;&quot;&gt;The unexpected&lt;/h3&gt;
&lt;p&gt;My SC experience was very biased since I didn't get out much, but two related themes kept popping up across different meetings and the sessions I did attend.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Power efficiency is serious business now&lt;/b&gt;. It used to seem like people talked about the need for energy-efficient HPC in an abstract sense while continuing to jam more power into every rack without changing their approach to system design, facilities, and deployment models. That has hit a hard wall with energy prices soaring in Europe, though. The financial impacts of power-inefficient supercomputing have gone from a one-time capex cost to an ongoing opex cost that is putting many HPC facilities on an unsustainable cost trajectory. Even sites that aren't doing new deployments are facing sudden, sharp increases in their costs, and nobody has good answers about how they will keep the lights on.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Cloud HPC is confusing&lt;/b&gt;. With only &lt;a href=&quot;https://www.nextplatform.com/2022/11/08/hpc-follows-the-enterprise-into-the-cloud/&quot;&gt;15% of total HPC dollars winding up in the cloud&lt;/a&gt;, it's little surprise that most HPC folks are only peripherally aware of what HPC in the cloud really means. Worse yet, a subset of those folks are actively hostile towards the idea of running HPC workloads in the cloud. I spoke with my colleagues from all three major cloud service providers as well as my colleagues in DOE, NSF, and education throughout the week, and everyone painted this same general picture.&lt;/p&gt;
&lt;p&gt;There seems to be a mismatch between the expectations of on-prem HPC folks and cloud HPC folks. For example, I was asked why Windows doesn't support OpenMP very well, and after a bit of digging, I realized that the question really wasn't about using OpenMP on Windows as much as it was about using OpenMP in the cloud. There was a latent assumption that &quot;HPC in Microsoft's cloud&quot; must mean &quot;HPC on Windows&quot; which, for the record, is false--I don't even know how to use Windows anymore. Similarly, people decried the performance impacts of sharing HPC nodes with others in the cloud (they are not shared), overheads of virtualizing InfiniBand or GPUs (everyone uses PCIe passthrough or SR-IOV for HPC nodes), and other misconceptions.&lt;/p&gt;
&lt;p&gt;This isn't to say that cloud people aren't confused too; I heard stories about conversations that went sideways because a cloud folks (not from my employer, thankfully!) didn’t realize that the requirements of a traditional gov/edu HPC facility couldn’t be neatly wrapped up into a single workload with a single solution, contrary to the case across many commercial AI shops. And both sides are struggling to find models for partnership and engagement that mirror the traditional relationship between places like a DOE or NSF facility and a company like Cray. HPC departments are used to buying supercomputers and parallel file systems, while cloud providers sell computing and storage as a &lt;i&gt;service&lt;/i&gt;. The distinction may seem trivial at the surface, but there's a large divide that becomes evident once both sides start trying to drill into the details of what a partnership would look like.&lt;/p&gt;
&lt;h2 style=&quot;text-align: left;&quot;&gt;Parallel I/O in Practice Tutorial&lt;/h2&gt;
&lt;p&gt;This was my fifth year contributing to the Parallel I/O in Practice Tutorial with my colleagues at Argonne and Google, and it was our first time doing it in-person since 2019. It felt really good to be back in front of people to opine about the perils of POSIX and the greatness of the &lt;a href=&quot;https://www.mcs.anl.gov/research/projects/darshan/&quot;&gt;Darshan I/O profiling tool&lt;/a&gt;, and this year I retired out the material I used to present on burst buffers (since DataWarp and Infinite Memory Engine have lost relevance in HPC) and the &lt;a href=&quot;https://www.nersc.gov/tokio/&quot;&gt;TOKIO holistic I/O analysis framework&lt;/a&gt; (since it is no longer funded/maintained). In their stead, I presented material on &lt;a href=&quot;https://wiki.lustre.org/Lustre_User_Group_2022&quot;&gt;benchmarking with IOR and mdtest I debuted at LUG 2022 this year&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I haven't gotten feedback yet on whether this change was a net positive one, but I think it went over well. Benchmarking I/O is really challenging if you don't understand how things like page cache really work in distributed systems, and walking through some benchmark examples concretizes a lot of abstract parallel file system concepts like locking and striping. And since benchmarking is a rabbit hole of arbitrary complexity, ending the tutorial with advanced benchmarking topics turned out to be a nice way to add buffer to the end of an eight-hour stretch of carefully timed presentations. It's very easy to skip over the nuances of analyzing mdtest outputs if attendees have a lot of questions about more important things at the end of the day.&lt;/p&gt;
&lt;p&gt;The most surprising observation of the tutorial is how many attendees aren't using MPI anymore. We got a lot of questions last year about task-oriented I/O, and this year had some great questions about trying to understand or tune the I/O performed by Python-based analytics frameworks. We decided to add support for &lt;a href=&quot;https://www.mcs.anl.gov/research/projects/darshan/2019/12/11/new-experimental-version-of-darshan-available-for-instrumenting-non-mpi-applications/&quot;&gt;Darshan to profile non-MPI applications back in 2019&lt;/a&gt; which is now paying dividends by ensuring it is a relevant tool for these new analytics and AI workloads, and we'll probably have to give more attention to optimizing these workloads' I/O in the future.&lt;/p&gt;
&lt;h2 style=&quot;text-align: left;&quot;&gt;DAOS User Group&lt;/h2&gt;
&lt;p&gt;Monday morning was cold and rainy--a perfect day to attend the &lt;a href=&quot;https://daosio.atlassian.net/wiki/spaces/DC/pages/11248861216/DUG22&quot;&gt;2022 DAOS User Group&lt;/a&gt; which was held off-site at the Fairmont Hotel.&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;Whether you particularly care about DAOS or not, the cross-community HPC I/O brain trust is guaranteed to be in attendance, and this year did not disappoint. In addition to the expected stakeholders from Intel and DOE, representatives from all three big CSPs were in attendance. Google Cloud, Seagate, and HPE/Cray were all on the agenda, painting a diversifying landscape of large HPC companies investing time into DAOS and the strength and willingness of the DAOS team to partner with all comers.&lt;/p&gt;
&lt;h3 style=&quot;text-align: left;&quot;&gt;Life after Optane&lt;/h3&gt;
&lt;p&gt;The question that opened up the meeting, of course, was &quot;what is the future of DAOS since Intel cancelled Optane?&quot; Kelsey Prantis had the official statement (I'll replace the grainy photo once the DUG slides are online...):&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;The high-level project answer is that DAOS isn't going anywhere. Aurora, by virtue of still having Optane DIMMs, will not be affected, and DAOS will maintain support for Optane until Intel drops its last Optane DIMMs (Crow Pass for Sapphire Rapids) from support life sometime towards the end of this decade.&lt;/p&gt;
&lt;p&gt;For new customers who aren't going to use Optane, the answer is &quot;&lt;a href=&quot;https://daosio.atlassian.net/issues/?jql=labels%20%3D%20%22md_on_ssd%22&quot;&gt;Metadata on NVMe&lt;/a&gt;,&quot; a development being codeveloped by Intel, HPE, and Google to implement a write-ahead log (WAL) and allow DAOS to use volatile DRAM instead of Optane. It will work like a file system journal in that a compact representation of writes will be committed to NVMe immediately after landing in DRAM, and then DAOS will asynchronously write back the properly serialized representation of that transaction after it is acknowledged. Johann Lombardi had a helpful cartoon that showed how this WAL will fit into DAOS:&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;A key benefit of DAOS's implementation of this WAL is that it will be able to still service incoming writes while flushing old writes; although I don't fully grasp how this works, it is something enabled by the sophisticated I/O scheduler already implemented in DAOS.&lt;/p&gt;
&lt;p&gt;The complete implementation isn't expected to be released until Spring 2024, but it appears to touch only a few components of DAOS and doesn't affect anything above the VOS layer of the DAOS server.&lt;/p&gt;
&lt;p&gt;There was also mention of developing operability with new &lt;a href=&quot;https://news.samsung.com/global/samsung-electronics-unveils-far-reaching-next-generation-memory-solutions-at-flash-memory-summit-2022&quot;&gt;CXL-attached memory-semantic SSDs&lt;/a&gt; to keep the persistent memory capability of DAOS alive beyond Optane. I'm not sure if this would offer a performance benefit over the metadata-on-NVMe feature; early results show that metadata-on-NVMe actually delivers higher IOPS than Optane since the synchronous write path is much simpler without having to account for memory persistence. That said, I didn't really follow the full extent of options on the table for how DAOS metadata may work across different types of memory though.&lt;/p&gt;
&lt;h3 style=&quot;text-align: left;&quot;&gt;DAOS in the flesh at Argonne&lt;/h3&gt;
&lt;p&gt;Kevin Harms presented an update on Aurora's massive 220 PB DAOS installation and laid out its configuration. There are 1,024 DAOS servers based on the Intel Coyote Pass server design, each sporting&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul style=&quot;text-align: left;&quot;&gt;&lt;li&gt;2x Intel Xeon 5320 (Ice Lake) sockets&lt;/li&gt;&lt;li&gt;2x DAOS engines (one per socket)&lt;/li&gt;&lt;li&gt;16x 32GB DDR4 DIMMs&lt;/li&gt;&lt;li&gt;16x 512GB Optane DIMMs (Persistent Memory 200)&lt;/li&gt;&lt;li&gt;16x 15.36 TB Samsung PM1733 NVMe SSDs&lt;/li&gt;&lt;li&gt;2x 200 Gb/s Slingshot NICs&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;The total configuration is quoted at 220 PB usable, but Kevin pointed out that this assumes that every object is erasure coded at 16+2. Unlike virtually every other storage system out there, though, users can choose the data protection for their individual objects when they create them, meaning this 220 PB capacity is an upper limit to what users can do. Users with very hot, read-only objects may choose to replicate instead of erasure code, while others who are capacity-constrained may choose to erasure code everything at 16+2 at the cost of latency and IOPS. This flexibility is really powerful for users since they can tailor their object layout (&quot;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/articles/technical/understanding-data-redundancy-and-sharding-in-daos.html&quot;&gt;object class&lt;/a&gt;&quot; in DAOS parlance) to match the needs of their workload.&lt;/p&gt;
&lt;p&gt;Argonne will be slicing up this DAOS system by giving each scientific project its own DAOS pool, and each pool will be assigned to only 80% of the available DAOS servers by default. This seems like a nice way of providing most of the storage system performance to every user, but offering more freedom to work around bad hardware, bad users, and other performance problems that plague file systems like Lustre that distribute everything across every single server equally.&lt;/p&gt;
&lt;p&gt;Finally, I noticed that Aurora will be using Samsung SSDs, not the Intel (now Solidigm) QLC NAND that appeared in all the DAOS slides floating around two years ago. I'm not sure what happened there, but the move from Solidigm QLC to Samsung TLC couldn't have been cheap.&lt;/p&gt;
&lt;h3 style=&quot;text-align: left;&quot;&gt;New features and contributions&lt;/h3&gt;
&lt;p&gt;DAOS is starting to pick up some truly valuable features that are being developed and contributed by third parties. Of note, croit has contributed a feature which allows DAOS to serve up NVMe over Fabrics targets, and Seagate contributed an S3 gateway for DAOS. Along with the DFS file system interface, DAOS now offers the trifecta of standard object, block, and file services just like Ceph. Unlike Ceph though, performance on DAOS is a first-class citizen. While croit made it clear that the NVMeoF support still has a ways to go to improve the way it does thread pooling and provides resilience, they showed 1.4 million IOPS from a single storage client using TCP over Ethernet with minimal client-side overhead.&lt;/p&gt;
&lt;p&gt;Intel is also developing multitenant support for DFUSE, allowing a single compute node to share a DAOS mount and let permissions be enforced through UID/GID just like a regular file system. Before this update, the FUSE-based nature of DAOS allowed any unprivileged user to mount their container (good), but only one FUSE agent could be alive on a single node at a time (not good) which prevented multiple users sharing a node from both mounting their own containers.&lt;/p&gt;
&lt;p&gt;DAOS also has some longer-term enhancements that I thought were interesting:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul style=&quot;text-align: left;&quot;&gt;&lt;li&gt;expanding the range of POSIX calls supported by DAOS's intercept library to include metadata calls and memory-mapped I/O using &lt;a href=&quot;https://docs.kernel.org/admin-guide/mm/userfaultfd.html&quot;&gt;userfaultfd&lt;/a&gt;&lt;/li&gt;&lt;li&gt;implementing collaborative caching - essentially reimplementing the Linux kernel page cache in userspace so that multiple processes can share cached DAOS pages&lt;/li&gt;&lt;li&gt;supporting a computational storage paradigm by enabling offload of &lt;a href=&quot;https://github.com/rlane/ubpf&quot;&gt;userspace eBPF scripts&lt;/a&gt; to DAOS servers&lt;/li&gt;&lt;/ul&gt;
&lt;h3 style=&quot;text-align: left;&quot;&gt;DAOS in a larger data center ecosystem&lt;/h3&gt;
&lt;p&gt;Dean Hildebrand from Google Cloud then gave an overview of Google's efforts in bringing DAOS into the cloud. He had some nice performance graphs and I'll link the full presentation here once it's uploaded (it's worth a watch), but the part I found the most insightful was how they are trying to decide where a technology like DAOS fits in the larger cloud storage ecosystem. He outlined two different ways DAOS could work in GCP:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ol style=&quot;text-align: left;&quot;&gt;&lt;li&gt;&lt;b&gt;Caching&lt;/b&gt;: Google Cloud Storage (GCS) is the point of truth and DAOS is a cache&lt;/li&gt;&lt;li&gt;&lt;b&gt;Tiering&lt;/b&gt;: DAOS is a point of truth, and GCS is an archive&lt;/li&gt;&lt;/ol&gt;
&lt;p&gt;&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;He said they were leaning towards the caching model where data only lives ephemerally in DAOS, and personally, I think this is the right move since DAOS in the cloud is not resilient without Optane. However, this choice reflects a much larger tension in cloud storage for HPC:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ol style=&quot;text-align: left;&quot;&gt;&lt;li&gt;The centerpiece of every cloud's data story is a scalable, low-cost, low-performance object store which is analogous to what on-prem HPC would call campaign, community, or project storage.&lt;/li&gt;&lt;li&gt;HPC demands higher performance than what these object stores can generally deliver though.&lt;/li&gt;&lt;/ol&gt;
&lt;div&gt;To bridge the gap between these two truths, auxiliary services must bolt on to the object layer and provide higher performance, at a higher cost, for the duration of I/O-intensive HPC jobs. Some choose to provide true tiering from object into a resilient layer of flash (like &lt;a href=&quot;https://aws.amazon.com/fsx/lustre/&quot;&gt;FSx Lustre&lt;/a&gt; and &lt;a href=&quot;https://docs.weka.io/overview/data-storage&quot;&gt;Weka&lt;/a&gt; do), while others project the contents of the object through a high-performance caching layer (like &lt;a href=&quot;https://azure.microsoft.com/en-us/products/hpc-cache/#overview&quot;&gt;HPC Cache&lt;/a&gt; and &lt;a href=&quot;https://aws.amazon.com/blogs/aws/amazon-file-cache-a-high-performance-cache-on-aws-for-your-on-premises-file-systems/&quot;&gt;File Cache&lt;/a&gt;) and are never meant to persistently hold data.&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;This isn't rocket science, but I never thought deeply about the two models since campaign/community/project storage in on-prem HPC is usually fast enough to avoid needing caches or fine-grained tiering capabilities.&lt;/p&gt;
&lt;p&gt;John Bent also had a thought-provoking presentation about how Seagate's now-&quot;deprioritized&quot; CORTX object store, which once &lt;a href=&quot;https://blog.seagate.com/enterprises/seagate-and-sage-project-innovate-to-boost-hpc-and-big-data-community/&quot;&gt;competed with DAOS as Mero&lt;/a&gt;, contains ideas that can complement DAOS:&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;Whereas DAOS delivers high performance using NVMe, CORTX delivers great economics using HDDs, and their strengths are complementary to each other. While I don't fully grasp how a tiered (or caching!) system comprised of DAOS and CORTX could be implemented, John rightly pointed out that the same level of space efficiency can deliver higher data protection if multi-level erasure coding is used to stripe across durable block storage. His specific example was erasure coding at 8+1 across servers and 10+1 within servers to deliver both high efficiency and high durability. This could map to something like running DAOS atop something like CORVAULT, but I don't think all the necessary pieces are in place to realize such a harmonious coexistence yet.&lt;/p&gt;
&lt;p&gt;Of course, completely tossing Reed-Solomon for something more sophisticated (like VAST does with its locally decodable 150+4 scheme) obviates the need for multilevel erasure entirely. But DAOS has not gone down that route yet.&lt;/p&gt;
&lt;p&gt;And as with every talk John gives, there were lots of other interesting nuggets scattered throughout his presentation. Two of my favorites were:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul style=&quot;text-align: left;&quot;&gt;&lt;li&gt;A slide that pointed out that, when you buy something like Ceph as an appliance, you may be spending only 25% of the total cost on storage media and the rest is infrastructure, service, and support. This struck me as a bit on the low end, but some enterprisey NAS and midrange parallel file system appliances can go this low. Spending 60% to 90% on media is a lot nicer for the buyer (and companies like Seagate) if you can buy at scale or eschew the white-glove support, and John suggested that it's up to companies like Seagate to fix the software issues that require customers to pay for white-glove support in the first place.  After all, the less someone spends on support and licenses, the more they can spend on Seagate hard drives.&lt;/li&gt;&lt;li&gt;John's final slide pointed out that object stores were originally designed to get around the limitations of POSIX file systems, but as they've evolved over the last decade, they're starting to look a lot like file systems anyway since they require strong consistency, hierarchical namespaces, and familiar file semantics. Has all the work put into developing super-fast object stores like DAOS over the last ten years really just brought us back full circle to parallel file systems?  Companies like VAST and Weka have shown that &lt;a href=&quot;https://www.nextplatform.com/2017/09/11/whats-bad-posix-io/&quot;&gt;maybe POSIX isn't as bad as the research community (myself included!) have claimed it to be&lt;/a&gt;; it was really just low-performance implementations that nobody wanted.&lt;/li&gt;&lt;/ul&gt;
&lt;div&gt;Once John's talk is uploaded to the DUG 2022 website, I'll link it here.  Like Dean Hildebrand's talk, it is well worth watching (but for wildly different reasons!)&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;h2 style=&quot;text-align: left;&quot;&gt;PDSW 2022&lt;/h2&gt;
&lt;p&gt;I had to duck out of the DAOS User Group early to run (through the rain) to the 7th International Parallel Data Systems Workshop (PDSW 2022) on Monday afternoon.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;Much to everyone’s surprise, PDSW was only given a half day this year and everything felt a little compressed as a result. The organizers kept the work-in-progress (WIP) sessions which can often be an interesting peek into what students are pursuing, but little A/V problems and the unforgiving schedule probably did a disservice to the up-and-comers who use the WIP track to lay the groundwork for future full-length papers. Hopefully SC’23 restores PDSW to its original full-day status.&amp;lt;p&amp;gt;&amp;lt;/p&amp;gt;&lt;/p&gt;
&lt;h3 style=&quot;text-align: left;&quot;&gt;Splinters keynote from Arif Merchant at Google&lt;/h3&gt;
&lt;p&gt;The keynote presentation was given by Arif Merchant from Google about Splinters, the framework that Google Cloud uses to sample I/Os in a scalable way. The challenge they face is that it's impossible to trace and store every single I/O that hits Google's storage servers (D servers), but having an understanding of I/O patterns is essential for characterizing workload I/O behavior and planning for future infrastructure. In fact, this problem is so important that Google isn't the only cloud that's solved it!&lt;/p&gt;
&lt;p&gt;A lot of what Arif talked about is very similar to how Azure does its I/O tracing under the hood. I suppose it should not be surprise that there are only so many ways to solve the challenge of sampling individual IOPS in a way that fairly represents the aggregate workload of a huge distributed storage system. One really smart thing Splinters does that I liked was sample along two different dimensions: not only do they evenly sample across all IOPS at a fixed rate (the obvious thing), but they also sample across files at a fixed rate. In this latter case of per-file sampling, they take a tiny fraction of files and capture every I/O for that file to get a complete picture of how individual files are being accessed.&lt;/p&gt;
&lt;p&gt;This file sampling fills the huge gap that exists when randomly sampling IOPS alone. Because different I/Os have different &quot;costs&quot; (for example, reading a 1 MiB file using a single 1 MiB read op or 256x 4 KiB read ops are functionally equivalent to an application), randomly sampling ops introduces systematic biases that can be difficult to back out after the data has been sampled, subsampled, aggregated, and reduced. Splinters' approach lets you see the workload from two different angles (and biases) and answer a much larger range of questions about what's really happening across thousands of storage servers.&lt;/p&gt;
&lt;p&gt;That said, it was interesting to hear Arif describe how Splinters evolved out of a different internal Google project but wound up outliving it. Splinters is also similar to, but slightly different from, their &lt;a href=&quot;https://research.google/pubs/pub36356/&quot;&gt;Dapper&lt;/a&gt; infrastructure which also does scalable distributed system tracing. And he made overtures to &lt;a href=&quot;https://research.google/pubs/pub41344/&quot;&gt;F1&lt;/a&gt;, a scalable SQL database that is similar to (but not the same as) the SQL-like query interface that Splinters uses. I got the impression that new technologies come and go pretty quickly at Google, and there's a large appetite for creating new software systems outright rather than shoehorning an existing system into solving a new problem. I can't say one way is better than the other; I was just surprised at the contrast with my own experiences.&lt;/p&gt;
&lt;h3 style=&quot;text-align: left;&quot;&gt;Practical papers&lt;/h3&gt;
&lt;p&gt;PDSW had a healthy combination of both very-researchy papers and applied research papers this year. I could only stick around for the applied papers, and two left an impression.&lt;/p&gt;
&lt;p&gt;In the first, &lt;a href=&quot;https://jeanlucabez.io&quot;&gt;Jean Luca Bez&lt;/a&gt; presented &lt;a href=&quot;https://github.com/hpc-io/drishti&quot;&gt;Drishti&lt;/a&gt;, a tool that lives downstream of the Darshan I/O profiling library and finally does what the Darshan community has danced around for years--turning a Darshan log into an actionable set of recommendations on how to improve I/O performance. It does this by cataloguing a bunch of heuristics and using Darshan's new Python integrations to pore through a log and identify known-problematic I/O patterns. Like Jean Luca's &lt;a href=&quot;https://dxt-explorer.readthedocs.io/en/latest/&quot;&gt;DXT Explorer tool&lt;/a&gt;, Drishti has a slick user interface and greatly extends the usability and insights that can be pulled out of a Darshan log file. It probably won't win a Turing Award, but this sort of work is probably going to benefit scores of HPC end-users by making Darshan (and troubleshooting I/O problems) much more accessible to mere mortals for years to come.&lt;/p&gt;
&lt;p&gt;Adrian Jackson also presented a very tidy &lt;a href=&quot;https://arxiv.org/abs/2211.09162&quot;&gt;apples-to-apples comparison of DAOS and Lustre on the same hardware&lt;/a&gt; using both a systems-level benchmark and an application-inspired, object-oriented data model benchmark. The specific bake-off of a new curiosity (DAOS) and the decades-old incumbent (Lustre) is probably interesting to storage nerds, but I think the real novelty of the work is in its exploration of some uncomfortable realities that the HPC I/O community will have to face in the coming years:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul style=&quot;text-align: left;&quot;&gt;&lt;li&gt;Does &quot;slow memory&quot; (nonvolatile Optane or CXL-attached memory SSDs) give actual benefit to existing file systems (like Lustre), or is rethinking the entire storage stack (like DAOS did) really necessary to unlock the performance of new hardware?&lt;/li&gt;&lt;li&gt;Do applications need to rethink their approach to I/O to make use of post-POSIX storage systems like DAOS, or is performing I/O as you would on a file system (Lustre) on a post-POSIX storage system (DAOS) good enough?&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;My take from the work is that, for simple I/O patterns like checkpoint/restart, you can get pretty far by just treating something like DAOS the same as you would a parallel file system:&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;b&gt;&lt;span style=&quot;font-size: x-small;&quot;&gt;Figure from Manubens et al, &quot;&lt;a href=&quot;https://arxiv.org/abs/2211.09162&quot;&gt;Performance Comparison of DAOS and Lustre for Object Data Storage Approaches&lt;/a&gt;.&quot;&lt;/span&gt;&lt;/b&gt;&lt;/div&gt;
&lt;p&gt;But if you want your data at rest to have the same data model as how it's handled within the application, you really ought to use a storage system that supports data models that are more expressive than a stream of bytes (which is what POSIX files are).&lt;/p&gt;
&lt;p&gt;The authors didn't do a perfect job of giving Lustre its fair shake since they chose to use (abuse) directories and files to represent their application's data model on-disk instead of developing an object-file model that file systems like Lustre handle a little better. But let's be real--HPC is full of applications that do the exact same thing and represent datasets on-disk using complex hierarchies of directories and files simply because that's the easiest way to map the application's representation of data into the standard file system model. In that sense, storage systems that represent rich data models in a high-performance way should be really valuable to naive applications that map in-memory data structures directly to files and directories.&lt;/p&gt;
&lt;p&gt;Going back to John Bent's closing slide from his DAOS User Group talk, though, does any of this even matter since all answers lead back to parallel file systems? Maybe there's something to be learned about adding better back-door APIs that support more diverse data models than what POSIX file interfaces give us.&lt;/p&gt;
&lt;h2 style=&quot;text-align: left;&quot;&gt;The SC22 Expo&lt;/h2&gt;
&lt;p&gt;The expo is my favorite part of SC because it's when I get to talk to people one-on-one and learn about corners of the HPC industry that I would've never otherwise sought out. Much to my dismay, though, I had very little time to walk the floor this year--so little that I didn't get any swag. If you want to read up on what interesting technology was being showcased, I strongly recommend reading &lt;a href=&quot;https://www.servethehome.com/?s=sc22&quot;&gt;all the great content that Patrick Kennedy and his team at STH created covering the expo&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;That said, I did notice some curious trends about the show floor overall.&lt;/p&gt;
&lt;p&gt;The NVIDIA booth was notably absent this year (though they shared booth space with partners), and many of the usual top vendors had significantly smaller presence on the expo floor. Just for fun, I compiled the top ten(ish) vendors by booth size:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ol style=&quot;text-align: left;&quot;&gt;&lt;li&gt;Weka.io (3,200 sqft)&lt;/li&gt;&lt;li&gt;VAST Data, Department of Energy, Penguin Computing, HPE, and Microsoft (2,500 sqft)&lt;/li&gt;&lt;li&gt;AWS (2,000 sqft)&lt;/li&gt;&lt;li&gt;Google and TACC (1,600 sqft)&lt;/li&gt;&lt;li&gt;Supermicro, AMD, Intel, Dell, NASA, and Indiana University (1,500 sqft)&lt;/li&gt;&lt;/ol&gt;
&lt;p&gt;I think it's amazing to see all-flash storage companies at the top of the list alongside all of the Big 3 cloud service providers. I may be reading too much into this, but this may mean that the money behind SC is shifting towards companies playing in the cloud-based AI space instead of traditional big iron for simulation. Or perhaps it's a sign that most of the traditional HPC players are taking a hard look at the return they get on a big booth given the current economic climate and pulled back this year.&lt;/p&gt;
&lt;p&gt;I did chat with a couple colleagues who completely opted out of a booth this year (for reference, &lt;a href=&quot;https://hallerickson.ungerboeck.com/prod/app85.cshtml?AppCode=VFP&amp;amp;OrgCode=34&amp;amp;EvtID=5025&amp;amp;CC=SC22SM&quot;&gt;SC'21&lt;/a&gt; had 10% fewer exhibitor booths than &lt;a href=&quot;https://hallerickson.ungerboeck.com/prod/app85.cshtml?AppCode=VFP&amp;amp;OrgCode=34&amp;amp;EvtID=5020&amp;amp;CC=SC19&quot;&gt;SC'19&lt;/a&gt;), and the reasoning was consistent: they found more value in having staff meet with customers privately or attend the technical sessions and engage with people organically. Combined with a bit of bad taste left over from SC's &lt;a href=&quot;https://sc21.supercomputing.org/exhibits/exhibit-at-sc/&quot;&gt;high cost of hosting pandemic-era &quot;digital booths&quot;&lt;/a&gt; despite low return (did anyone visit digital booths at SC'20 or SC'21?), I can see why some vendors may have chosen to skip the expo this year.&lt;/p&gt;
&lt;p&gt;Whatever the reasons may be, I was a bit sad to see such a small presence from some of my favorites like IBM, Fujitsu, Atos, and NEC. Hopefully the SC Exhibits Committee (and the economy!) can find ways to bring back the pre-pandemic glory of the show floor.&lt;/p&gt;
&lt;p&gt;The expo wasn't all doom and gloom though! Even though I couldn't make my complete rounds this year, there were a couple of highlights for me.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;h3 style=&quot;text-align: left;&quot;&gt;VAST's masterful marketing&lt;/h3&gt;
&lt;p&gt;Perhaps the splashiest vendor at SC was VAST Data who had a brilliant marketing presence. First was the giant Vastronaut mascot that was the centerpiece of their booth:&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;A &lt;a href=&quot;https://twitter.com/search?q=sc22%20vast&amp;amp;f=live&quot;&gt;quick search of Twitter&lt;/a&gt; shows just how many people seized the opportunity to take a selfie at their booth. I would love to know how they transported that thing to and from the conference, but whatever the cost, I'll bet it was worth it.&lt;/p&gt;
&lt;p&gt;At the Grand Opening Gala on Monday, they also gave out delightfully tacky light-up cowboy hats that everyone seemed to be wearing:&lt;/p&gt;
&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p dir=&quot;ltr&quot; lang=&quot;en&quot;&gt;We were there! &lt;a href=&quot;https://twitter.com/hashtag/sc22?src=hash&amp;amp;ref_src=twsrc%5Etfw&quot;&gt;#sc22&lt;/a&gt; &lt;a href=&quot;https://twitter.com/hashtag/sc2022?src=hash&amp;amp;ref_src=twsrc%5Etfw&quot;&gt;#sc2022&lt;/a&gt; &lt;a href=&quot;https://twitter.com/VAST_Data?ref_src=twsrc%5Etfw&quot;&gt;@VAST_Data&lt;/a&gt; &lt;a href=&quot;https://t.co/fWhuSgBfpL&quot;&gt;pic.twitter.com/fWhuSgBfpL&lt;/a&gt;&lt;/p&gt;
— ntnu-hpc (@ntnuhpc) &lt;a href=&quot;https://twitter.com/ntnuhpc/status/1592330266932301829?ref_src=twsrc%5Etfw&quot;&gt;November 15, 2022&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;The subtle genius of this was that not only did people wear them during the gala and the &lt;a href=&quot;https://beowulfbash.com&quot;&gt;Flop Gun-themed Beowulf Bash 2022 party&lt;/a&gt; later that night, but they had to wear them on their plane rides home since they were so inconveniently bulky. Proof in point, my wife (who doesn't work in tech) sent me this text message to confirm that she was waiting for me at the right luggage carousel at San Francisco Airport:&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;I wonder how many innocent bystanders, traveling home for Thanksgiving on Thursday or Friday, saw the shiny cowboy hats at airports around the country and wondered what VAST was.&lt;/p&gt;
&lt;p&gt;The icing on the cake was VAST's CEO, Renen Hallak, parading around in an unmissable Chuck McGill-style space suit all week, clearly not taking himself too seriously and painting VAST as a work hard/play hard kind of company. Now, do flashy space suits and blinking cowboy hats alone mean VAST has a great product? I can't say&lt;sup&gt;**&lt;/sup&gt;. But marketing is an art that I appreciate, and VAST hit some great notes this year.&lt;/p&gt;
&lt;p style=&quot;font-size: xx-small;&quot;&gt;&lt;sup&gt;**&lt;/sup&gt; (Seriously, I'm not sure I wouldn't get in trouble for opining about another company here.)&lt;/p&gt;
&lt;h3 style=&quot;text-align: left;&quot;&gt;The Microsoft hardware bar&lt;/h3&gt;
&lt;p&gt;The only booth where I spent any appreciable time this year was my own employer's. I personally love booth duty and accosting strangers on the show floor, especially if there's something interesting at the booth to jumpstart a conversation. When I worked at SDSC it was a &lt;a href=&quot;https://www.sdsc.edu/News%20Items/PR111213_meteor.html&quot;&gt;Raspberry Pi cluster&lt;/a&gt;, and at the Microsoft booth this year it was the &quot;hardware bar.&quot;&lt;/p&gt;
&lt;p&gt;In addition to the customary booth presentations with giveaways, swag desk, seating area, and a fun caricature artist, the physical servers that underpin the HPC nodes in Azure were on display. &lt;a href=&quot;https://www.opencompute.org/wiki/Server/ProjectOlympus&quot;&gt;Microsoft contributes its hardware platform designs to the Open Compute Project&lt;/a&gt; so the physical hardware that runs in Azure data centers isn't entirely mysterious. Still, every cloud has its hardware secrets, so I was surprised to see these servers laid bare.&lt;/p&gt;
&lt;p&gt;The newest HPC node type (dubbed &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/virtual-machines/hbv4-series&quot;&gt;HBv4&lt;/a&gt;) on display was a node powered by AMD's Genoa processors just announced a few days earlier:&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;This wasn't a display model, either; it had real DDR5 DRAM, a real NDR InfiniBand HCA, real PCIe Gen5, and real big OCP mezzanine card with real big aluminum heat sinks and a big Microsoft sticker on top. A couple visitors commented on the way the heat piping for those Genoa CPUs was done which I guess is unusual; rather than have a giant copper block on top of each socket, heat pipes connect the socket to massive aluminum heat sinks that are closer to the chassis inlets. In retrospect it makes sense; Genoa has a whopping twelve DDR5 DIMMs per socket which leaves little extra room for heat sinks, and these 88+ core sockets have a staggering thermal design power.&lt;/p&gt;
&lt;p&gt;Another exotic piece of hardware on display was an &quot;ND MI200 v4&quot; server:&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;It's logically similar to Azure's &quot;&lt;a href=&quot;https://learn.microsoft.com/en-us/azure/virtual-machines/nda100-v4-series&quot;&gt;ND A100 v4&lt;/a&gt;&quot; server platform with two CPU sockets, eight SXM4 GPU sockets, eight 200G HDR InfiniBand HCAs, and a bunch of M.2 NVMes. But this specific server has eight MI200 GPUs on a common OAM baseboard and uses Infinity Fabric for GPU-to-GPU communication. I've never seen an OAM-socketed anything in real life before, much less eight of them on a baseboard, so I thought this was pretty great to see in the flesh.&lt;/p&gt;
&lt;p&gt;The ND A100 v4 platform was also on display and looked very similar-but-different with its eight A100 GPUs and HGX baseboard:&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;And unlike the MI200 variant, the general public can run on these nodes.&lt;/p&gt;
&lt;p&gt;I'm not sure what more I'm allowed to say, but my colleague Karl made a nice, &lt;a href=&quot;https://twitter.com/KarlPodesta/status/1593627537330126851?s=20&amp;amp;t=uthjeb7YYmTZWRVWaF4XUA&quot;&gt;quick video that runs through the entire Microsoft booth&lt;/a&gt; that's worth a watch, and more details can be had by contacting me or your favorite Microsoft account team privately.&lt;/p&gt;
&lt;p&gt;Of course, the hardware bar was just a way to lure people into the booth so I could achieve my real goal: meeting new folks. As I wrote before, one of my biggest realizations at SC this year is how generally confused people are about what HPC in the cloud really means--both people who come from traditional on-prem HPC and people who come from traditional enterprisey cloud. I found myself surprising many of the people with whom I spoke on the show floor with factoids that I have taken for granted. For example,&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul style=&quot;text-align: left;&quot;&gt;&lt;li&gt;Linux is the most common OS on these HPC node types. While you probably(?) can run Windows if you want on this stuff, I think only a few niche markets do this.&lt;/li&gt;&lt;li&gt;The usage model for an HPC cluster in the cloud can be the same as on-prem. You can have login nodes, Slurm, home directories, parallel file systems, and all that. Jobs don't have to be containerized or turned into a VM image.&lt;/li&gt;&lt;li&gt;The InfiniBand coming out of these nodes is real InfiniBand with real OFED that supports real mpich/mvapich/OpenMPI. It's the same stuff as in on-prem supercomputers. And nodes are assembled into &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/virtual-machines/sizes-hpc&quot;&gt;full-bisection fat tree InfiniBand&lt;/a&gt; clusters just like normal.&lt;/li&gt;&lt;li&gt;There's no noisy neighbor problem on compute nodes because HPC node types aren't shared between users. When you run a VM on an HPC node, you get the whole thing. Just like on large supercomputers.&lt;/li&gt;&lt;li&gt;There's no horrible loss of performance due to running in a VM. Virtualization extensions, PCIe passthrough, and SR-IOV bypass the hypervisor for most things. Inside your VM, you see real Zen cores and real Mellanox HCAs, not virtualized devices.&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;My takeaway impression is that a lot of traditional HPC folks looked at the cloud five or ten years ago, had a sour experience, and haven't paid attention since. In those last five years, though, AI has changed the game. Massive demand for the latest CPUs and accelerators, funded by live-fast-die-young venture capital, has given cloud vendors tremendous financial incentive to catch up to on-prem levels of performance efficiency for AI workloads. And it just so happens that infrastructure that's good for AI is also good for traditional modeling and simulation.&lt;/p&gt;
&lt;h2 style=&quot;text-align: left;&quot;&gt;SCinet!&lt;/h2&gt;
&lt;p&gt;One of the unexpected highlights of my SC this year arose from a chance encounter with a former coworker from NERSC, &lt;a href=&quot;https://www.nersc.gov/about/nersc-staff/networking-security/ronal-kumar/&quot;&gt;Ron Kumar&lt;/a&gt;, who gave me a whirlwind tour of SCinet.&lt;/p&gt;
&lt;p&gt;I have to confess great ignorance around SCinet in general; I always saw it was a weird technological proof of concept that the strange networking people at work would go off and do in the weeks leading up to the actual conference. I knew they did some impressive wide-area transfer demos (like the &lt;a href=&quot;https://scinet.supercomputing.org/community/documents/43/sc17-Kettimuthu-transferring_1petabyte_per_day.pdf&quot;&gt;petabyte-in-a-day demo at SC'16&lt;/a&gt;), but I didn't really get the significance.&lt;/p&gt;
&lt;p&gt;So what is SCinet? It's this yellow bundle of cables dangling from the ceiling.&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&amp;lt;p&amp;gt;The yellow cables are 144-core fiber trunks that bring over a terabit per second of bandwidth into the convention center from the Internet via the national research backbones like ESnet and Internet2 and distribute many terabits per second of capacity throughout the SC conference venue. For comparison, most HPC centers in the US only have a tenth of SCinet’s wide-area bandwidth at best since 400G infrastructure is still rolling out.&amp;lt;/p&amp;gt;&lt;/p&gt;
&lt;p&gt;Most attendees may be familiar with the row of expensive-looking networking racks behind a glass wall towards the back of the expo which is where those yellow cables dangling from the ceiling end. Here's a photo from inside that glass wall:&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;What I didn't realize is that if you go around to the back of the giant walled area behind this glass display, there's a security checkpoint that gates entry into a massive network operations center (NOC) full of laptops, spools of fiber, meeting rooms, and busily working teams in charge of all the lower layers of the networking stack.&lt;/p&gt;
&lt;p&gt;The process to get into the NOC involves an escort and being tagged in with a tamper-proof wristband, and I learned on the tour that there's millions upon millions of dollars worth of high-end networking equipment in the racks shown above. If you look closely, you can see a security camera at the end of the aisle that speaks to this; that camera was one of many.&lt;/p&gt;
&lt;p&gt;Behind the pretty public-facing side of the SCinet racks is a mess of fiber and cables:&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;I guess if you have to tear all this down after just a few weeks, there's no point in investing days in dressing it all up nicely! I particularly enjoyed the fiber panels in the third rack that appear to be affixed to the rack post with shoe laces.&lt;/p&gt;
&lt;p&gt;This year, SCinet did do a neat proof-of-concept where they demonstrated three 400G routers from three vendors (Juniper, Arista, and Cisco?) all talking the same protocol to handle what I assume is the core routing for everything in the convention center:&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;I wish I remembered exactly what was going on here, but I know enough about networking to know that, despite there being standard protocols for coordinating between networking gear, each vendor does their own implementation that is rarely easy to get interoperability from. If anyone out there knows the details of this achievement, please let me know so I can explain this a little better!&lt;/p&gt;
&lt;p&gt;In addition to networking nerd-level demonstrations, SCinet also serves up all the wifi across the convention center. That is why there were tripods with access points scattered around, and why astute attendees may have noticed janky networking equipment scattered around that looked like this:&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;Again, I get it: for a network infrastructure that's only going to last a week, I don't think it's a good use of anyone's time or money to nicely dress all the networking.&lt;/p&gt;
&lt;p&gt;One last factoid I didn't know until this year was that exhibitors can request 100 Gb/s network drops into their individual booths for demos (or downloading the latest version of a PowerPoint presentation &lt;i&gt;really fast&lt;/i&gt;). The end result of supporting both a vast wifi network and 100G fiber across the show floor is that there was a &lt;u&gt;lot&lt;/u&gt; of fiber going into the single row of SCinet equipment:&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;Finally, when I &lt;a href=&quot;https://twitter.com/glennklockwood/status/1592725187015114752?s=61&amp;amp;t=1c4Kbx75SpTJhCruzuy0Ng&quot;&gt;posted some of these photos online&lt;/a&gt; during the conference, my colleague Bilel was kind enough to post a slide from the SC22 opening presentation that had the speeds and feeds of what I had toured:&lt;/p&gt;
&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p dir=&quot;ltr&quot; lang=&quot;en&quot;&gt;Candy Culhane shared Scinet facts &lt;a href=&quot;https://twitter.com/hashtag/SC22?src=hash&amp;amp;ref_src=twsrc%5Etfw&quot;&gt;#SC22&lt;/a&gt; &lt;a href=&quot;https://twitter.com/hashtag/HPC?src=hash&amp;amp;ref_src=twsrc%5Etfw&quot;&gt;#HPC&lt;/a&gt;&lt;br /&gt;&lt;br /&gt;5.01 Tb/s of WAN capacity&lt;br /&gt;$70M in HW &amp;amp; SW, &amp;amp; services provided by 29 SCinet contrib.&lt;br /&gt;175 volunteers from 80 vol. organiz.&lt;br /&gt;&amp;gt; 450 wireless deployed&lt;br /&gt;29 network research exhibition proposals&lt;br /&gt;11.7 miles of fiber &lt;br /&gt;2384 fiber patch &lt;a href=&quot;https://t.co/JtPhjVHZJd&quot;&gt;https://t.co/JtPhjVHZJd&lt;/a&gt; &lt;a href=&quot;https://t.co/kwGl5Ydqp5&quot;&gt;pic.twitter.com/kwGl5Ydqp5&lt;/a&gt;&lt;/p&gt;
— Bilel Hadri (@mnoukhiya) &lt;a href=&quot;https://twitter.com/mnoukhiya/status/1592737463617089536?ref_src=twsrc%5Etfw&quot;&gt;November 16, 2022&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;If you know anyone involved with SCinet, I highly recommend seeing if you can get a tour at the next SC. Even as a relative networking novice, I walked away with a much greater appreciation for the annual achievement of building SCinet. And who knows? Once I get bored of this whole storage thing, maybe I'll try getting into high-performance networking.&lt;/p&gt;
&lt;h2 style=&quot;text-align: left;&quot;&gt;Composability panel&lt;/h2&gt;
&lt;p&gt;This year I was invited to participate in a panel titled &quot;Smackdown! Does HPC Need Composability Now?&quot; moderated by Addison Snell and Dan Olds from &lt;a href=&quot;https://www.intersect360.com&quot;&gt;Intersect360 Research&lt;/a&gt;. This panel was...different. Unlike the traditional SC panel where panelists take turns presenting slides and saying erudite things, this panel had two teams of panelists. And my team only had one slide to present:&lt;/p&gt;
&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;/div&gt;
&lt;p&gt;The ground rules included &quot;personal attacks are allowed,&quot; and needless to say, the panel was about equal parts entertainment and technical discourse. That's not a bad thing, though.&lt;/p&gt;
&lt;p&gt;Addison and Dan did a phenomenal job of pulling their respective teams together and leading discussion in a format that both brought forward the key pros and cons of composability in HPC while poking fun at the thinly veiled, ego-driven personalities that often make up these sorts of panels. Rather than politely dancing around issues like sacrificing memory bandwidth by putting accelerators at the far end of a PCIe bus or gaining higher utilization by allowing users to mix and match CPU, NICs, and GPUs, us panelists were free to shoot straight (or perhaps a bit hyperbolically) and call each other out on our hidden agendas.&lt;/p&gt;
&lt;p&gt;I hope it goes without saying that all us panelists were in on the format and don't actually think people on the other side are dumb. By wrapping technical arguments in snarky comments, we could keep the level of discussion accessible to a wide audience, drive home the key points from both sides, and ensure that we weren't losing audience members who don't care about the PhD-level details as much as they want to hear what their peers are thinking about this exciting new space. I got some feedback afterwards that I didn't seem to hold back, so if anyone did take anything I said seriously, I am very sorry!&lt;/p&gt;
&lt;p&gt;On a technical level, what was the outcome?&lt;/p&gt;
&lt;p&gt;It turns out that &lt;a href=&quot;https://www.hpcwire.com/off-the-wire/informal-poll-of-sc22-attendees-suggests-a-bright-future-for-composability/&quot;&gt;there was about a 60/40 split between people who felt composability wasn't required yet and those who felt it was&lt;/a&gt; after both sides argued their case. Even among panelists, many of us were a lot less convinced about our respective positions than we let on during the panel itself. I got a chuckle when I realized that I wasn't the only one who, when invited to be on the panel, asked &quot;what side do you want me to argue?&quot; I honestly could have gone either way because the dust has not yet settled. &lt;a href=&quot;https://www.tacc.utexas.edu/about/directory/dan-stanzione&quot;&gt;Dan Stanzione, director of TACC&lt;/a&gt;, gave the truest answer to the question of &quot;will composability help HPC&quot; up front--&quot;&lt;a href=&quot;https://twitter.com/HPC_Guru/status/1592604467698241537?s=20&amp;amp;t=tn3WQBUY9M0MWSfqx1XLKA&quot;&gt;it depends&lt;/a&gt;.&quot; Maybe this is a growth opportunity, or maybe it's a lukewarm reception.&lt;/p&gt;
&lt;p&gt;Either way, composable technologies are hitting the market regardless of whether you think they'll be useful or not.  &lt;a href=&quot;https://www.nextplatform.com/2022/11/10/amd-genoa-epyc-server-cpus-take-the-heavyweight-title/&quot;&gt;AMD Genoa supports CXL 1.1 with extensions for memory pooling&lt;/a&gt;, &lt;a href=&quot;https://news.samsung.com/global/samsung-electronics-unveils-far-reaching-next-generation-memory-solutions-at-flash-memory-summit-2022&quot;&gt;Samsung has memory-semantic SSDs&lt;/a&gt;, and everyone and their mother is working on photonics to get higher bandwidths and lower latencies over longer distances. This makes it easier for people to dip their toes in the water to see if composability makes sense, and I think that's what a lot of people will wind up doing in the coming years.&lt;/p&gt;
&lt;h2 style=&quot;text-align: left;&quot;&gt;Customer meetings&lt;/h2&gt;
&lt;p&gt;Unlike in years past, my SC experience this year was dominated by customer meetings. I've been on the customer side of the table plenty of times, but I was surprised to find that it was actually more fun to be on the vendor side for a change. I'm part salesman at heart, so I found it personally gratifying to end a meeting with people nodding along rather than scratching their heads. I learned as a customer that it's very easy for vendors to go way off the rails and waste everyone's time, so I was grateful to have avoided the awkward confusion that punctuates those kinds of meetings. &lt;/p&gt;
&lt;p&gt;I also went into the week worrying that I'd be sitting in the same room, hearing the same pitch and the same jokes, and answering the same questions all week. Thankfully, I work with some great field, business, and product teams who set up interesting conversations rather than rote recitations of boring roadmap slides. Approaching the same topics from different angles helped me figure out how all the pieces of what I'm working on fit together to make a complete picture too; there weren't nearly as many opportunities to do this in the DOE world since the end-users of the HPC systems on which I worked aren't told anything until all the design decisions have already been made.&lt;/p&gt;
&lt;h2 style=&quot;text-align: left;&quot;&gt;A few personal notes&lt;/h2&gt;
&lt;p&gt;This SC was significant to me at a variety of levels; it was the first time I'd gotten on an airplane since February 2020, the first time I'd traveled since starting a new job at a new company, and the first time I'd met any of my new coworkers outside of the structure of a Teams call. During the pandemic I realized that getting out into the world and talking to people from all corners of HPC were my favorite part of my job. Not being able to go to events like SC and maintain that a sense of community involvement dramatically impacted my level of professional satisfaction for the last two years, so I'm glad I was able to finally go this year.&lt;/p&gt;
&lt;p&gt;Though customer meetings were a lot more fun than I expected them to be, I still felt bummed that I could spend so little time walking the expo, talking to folks, and attending all the BOFs normally on my &lt;a href=&quot;https://sc22.supercomputing.org/presentation/?id=bof124&amp;amp;sess=sess331&quot;&gt;must&lt;/a&gt;-&lt;a href=&quot;https://sc22.supercomputing.org/presentation/?id=bof112&amp;amp;sess=sess307&quot;&gt;attend&lt;/a&gt; &lt;a href=&quot;https://sc22.supercomputing.org/presentation/?id=bof110&amp;amp;sess=sess369&quot;&gt;list&lt;/a&gt;. Compounding this was my personal choice to not dine indoors and consequently miss out on almost all other chances to catch up with old friends and colleagues. I also decided to leave SC a day earlier than I usually do to reduce my risk of getting sick which didn't help either. There's never enough time at SC, but this year was particularly pressed.&lt;/p&gt;
&lt;p&gt;I say all this not to complain, but to say how much I appreciated the people who went out of their way to come accost me during the precious few hours I actually had on the exhibit floor. Some I'd not seen since SC'19, and some I'd never actually met since we only started working together mid-pandemic. The conference is busy for everyone, so giving me a slice of your time was very meaningful. That sense of community membership is why I go to SC, it's why I still work in this business, and it's why I try to contribute whatever I can to whomever wants it whether it be a student, engineer, salesperson, or marketer.&lt;/p&gt;</content><author><name>Glenn K. Lockwood's Blog</name></author><category term="glennklockwood" /><summary type="html">The biggest annual conference in HPC, the SC conference, was recently held in Dallas, Texas in its second hybrid incarnation since being all-remote for the pandemic. This year attracted over 11,000 attendees which is much closer to the pre-pandemic high of 14,000 than last year's 7,000, and judging from the crushed conference rooms and busy expo floor, it looks like SC is not that much worse for wear. This year's conference quite different for me since I attended for my first time as a vendor, not a researcher or practitioner, and I spent most of my days behind closed doors talking to customers. I didn't get to attend any of the keynotes, BOFs, or panels to which I wasn't invited as a result, so I'm not really qualified to give an erudite summary of the conference or expo this year. So instead, I'm just writing down what I remember in order that I remember it and not necessarily in a coherent narrative form. I'm sure I missed a lot (for example, mixed precision seemed big this year, and I heard Jack Dongarra gave a fantastic Turing Award talk) so I encourage others to write their own recaps and share with the community! High-level themes I actually started writing an SC'21 recap last year which I never posted, and re-reading the intro was funny--you'd think nothing has changed in the last year. The underwhelming The biggest deal appears to be that exascale is here, and it turns out that it's not that big of a deal. China let the air out of the tires by debuting their exascale systems at SC'21, and not only did they thumb their nose at Top500 by not submitting, they debuted by winning a Gordon Bell prize instead. The first US exascale system, Frontier, was debuted at ISC this year leaving its showing at SC a bit deflated too. Frontier was featured in the Gordon Bell prize-winning paper this year, but that work required the use of four Top-10 systems, not just Frontier, painting the reality that one giant computer rarely stands on its own when it comes to advancing science. This isn't to say that deploying exascale systems isn't a noteworthy feat and worth commendation, but I felt like the hype over the last five years treated the achievement like an end state instead of a milestone. And now that we've passed the milestone, the community is grasping to figure out what comes next. So what is next? Quantum had a strong and growing presence at SC, as it has for the last few years. But the conclusion of the panel &quot;Quantum Computing: A Future for HPC Acceleration&quot; was that no, it's not close to being ready. Disaggregation and composability was another theme with growing momentum. And like quantum, there was a panel asking the same question: &quot;Does HPC need composability now?&quot; The answer, again, was no, not yet. More on that below. What about RISC-V? Surely that will revolutionize the field. As it turns out, the answer there is also that RISC-V is not ready to do anything useful for HPC yet. The list goes on of technologies and trends that people are trying to boost now that exascale is &quot;solved.&quot; The reality, I think, is that &quot;exascale&quot; will take years to actually mature since it appears to have a ton of technical debt that accumulated during the race to be first. US Exascale rests on the shoulders of AMD and Intel, two companies whose software stacks have not caught up to the market leader, so there will be a lot of thrashing around as development practices and optimization settle out around these systems. Struggling with code porting is not very exciting to computer science Ph.D.s, so I expect future SCs to mirror this one and bifurcate into two distinct tracks: those struggling to identify the next big thing in the research space, and those struggling to use the systems that were rushed to deployment. The unexpected My SC experience was very biased since I didn't get out much, but two related themes kept popping up across different meetings and the sessions I did attend. Power efficiency is serious business now. It used to seem like people talked about the need for energy-efficient HPC in an abstract sense while continuing to jam more power into every rack without changing their approach to system design, facilities, and deployment models. That has hit a hard wall with energy prices soaring in Europe, though. The financial impacts of power-inefficient supercomputing have gone from a one-time capex cost to an ongoing opex cost that is putting many HPC facilities on an unsustainable cost trajectory. Even sites that aren't doing new deployments are facing sudden, sharp increases in their costs, and nobody has good answers about how they will keep the lights on. Cloud HPC is confusing. With only 15% of total HPC dollars winding up in the cloud, it's little surprise that most HPC folks are only peripherally aware of what HPC in the cloud really means. Worse yet, a subset of those folks are actively hostile towards the idea of running HPC workloads in the cloud. I spoke with my colleagues from all three major cloud service providers as well as my colleagues in DOE, NSF, and education throughout the week, and everyone painted this same general picture. There seems to be a mismatch between the expectations of on-prem HPC folks and cloud HPC folks. For example, I was asked why Windows doesn't support OpenMP very well, and after a bit of digging, I realized that the question really wasn't about using OpenMP on Windows as much as it was about using OpenMP in the cloud. There was a latent assumption that &quot;HPC in Microsoft's cloud&quot; must mean &quot;HPC on Windows&quot; which, for the record, is false--I don't even know how to use Windows anymore. Similarly, people decried the performance impacts of sharing HPC nodes with others in the cloud (they are not shared), overheads of virtualizing InfiniBand or GPUs (everyone uses PCIe passthrough or SR-IOV for HPC nodes), and other misconceptions. This isn't to say that cloud people aren't confused too; I heard stories about conversations that went sideways because a cloud folks (not from my employer, thankfully!) didn’t realize that the requirements of a traditional gov/edu HPC facility couldn’t be neatly wrapped up into a single workload with a single solution, contrary to the case across many commercial AI shops. And both sides are struggling to find models for partnership and engagement that mirror the traditional relationship between places like a DOE or NSF facility and a company like Cray. HPC departments are used to buying supercomputers and parallel file systems, while cloud providers sell computing and storage as a service. The distinction may seem trivial at the surface, but there's a large divide that becomes evident once both sides start trying to drill into the details of what a partnership would look like. Parallel I/O in Practice Tutorial This was my fifth year contributing to the Parallel I/O in Practice Tutorial with my colleagues at Argonne and Google, and it was our first time doing it in-person since 2019. It felt really good to be back in front of people to opine about the perils of POSIX and the greatness of the Darshan I/O profiling tool, and this year I retired out the material I used to present on burst buffers (since DataWarp and Infinite Memory Engine have lost relevance in HPC) and the TOKIO holistic I/O analysis framework (since it is no longer funded/maintained). In their stead, I presented material on benchmarking with IOR and mdtest I debuted at LUG 2022 this year. I haven't gotten feedback yet on whether this change was a net positive one, but I think it went over well. Benchmarking I/O is really challenging if you don't understand how things like page cache really work in distributed systems, and walking through some benchmark examples concretizes a lot of abstract parallel file system concepts like locking and striping. And since benchmarking is a rabbit hole of arbitrary complexity, ending the tutorial with advanced benchmarking topics turned out to be a nice way to add buffer to the end of an eight-hour stretch of carefully timed presentations. It's very easy to skip over the nuances of analyzing mdtest outputs if attendees have a lot of questions about more important things at the end of the day. The most surprising observation of the tutorial is how many attendees aren't using MPI anymore. We got a lot of questions last year about task-oriented I/O, and this year had some great questions about trying to understand or tune the I/O performed by Python-based analytics frameworks. We decided to add support for Darshan to profile non-MPI applications back in 2019 which is now paying dividends by ensuring it is a relevant tool for these new analytics and AI workloads, and we'll probably have to give more attention to optimizing these workloads' I/O in the future. DAOS User Group Monday morning was cold and rainy--a perfect day to attend the 2022 DAOS User Group which was held off-site at the Fairmont Hotel. Whether you particularly care about DAOS or not, the cross-community HPC I/O brain trust is guaranteed to be in attendance, and this year did not disappoint. In addition to the expected stakeholders from Intel and DOE, representatives from all three big CSPs were in attendance. Google Cloud, Seagate, and HPE/Cray were all on the agenda, painting a diversifying landscape of large HPC companies investing time into DAOS and the strength and willingness of the DAOS team to partner with all comers. Life after Optane The question that opened up the meeting, of course, was &quot;what is the future of DAOS since Intel cancelled Optane?&quot; Kelsey Prantis had the official statement (I'll replace the grainy photo once the DUG slides are online...): The high-level project answer is that DAOS isn't going anywhere. Aurora, by virtue of still having Optane DIMMs, will not be affected, and DAOS will maintain support for Optane until Intel drops its last Optane DIMMs (Crow Pass for Sapphire Rapids) from support life sometime towards the end of this decade. For new customers who aren't going to use Optane, the answer is &quot;Metadata on NVMe,&quot; a development being codeveloped by Intel, HPE, and Google to implement a write-ahead log (WAL) and allow DAOS to use volatile DRAM instead of Optane. It will work like a file system journal in that a compact representation of writes will be committed to NVMe immediately after landing in DRAM, and then DAOS will asynchronously write back the properly serialized representation of that transaction after it is acknowledged. Johann Lombardi had a helpful cartoon that showed how this WAL will fit into DAOS: A key benefit of DAOS's implementation of this WAL is that it will be able to still service incoming writes while flushing old writes; although I don't fully grasp how this works, it is something enabled by the sophisticated I/O scheduler already implemented in DAOS. The complete implementation isn't expected to be released until Spring 2024, but it appears to touch only a few components of DAOS and doesn't affect anything above the VOS layer of the DAOS server. There was also mention of developing operability with new CXL-attached memory-semantic SSDs to keep the persistent memory capability of DAOS alive beyond Optane. I'm not sure if this would offer a performance benefit over the metadata-on-NVMe feature; early results show that metadata-on-NVMe actually delivers higher IOPS than Optane since the synchronous write path is much simpler without having to account for memory persistence. That said, I didn't really follow the full extent of options on the table for how DAOS metadata may work across different types of memory though. DAOS in the flesh at Argonne Kevin Harms presented an update on Aurora's massive 220 PB DAOS installation and laid out its configuration. There are 1,024 DAOS servers based on the Intel Coyote Pass server design, each sporting 2x Intel Xeon 5320 (Ice Lake) sockets2x DAOS engines (one per socket)16x 32GB DDR4 DIMMs16x 512GB Optane DIMMs (Persistent Memory 200)16x 15.36 TB Samsung PM1733 NVMe SSDs2x 200 Gb/s Slingshot NICs The total configuration is quoted at 220 PB usable, but Kevin pointed out that this assumes that every object is erasure coded at 16+2. Unlike virtually every other storage system out there, though, users can choose the data protection for their individual objects when they create them, meaning this 220 PB capacity is an upper limit to what users can do. Users with very hot, read-only objects may choose to replicate instead of erasure code, while others who are capacity-constrained may choose to erasure code everything at 16+2 at the cost of latency and IOPS. This flexibility is really powerful for users since they can tailor their object layout (&quot;object class&quot; in DAOS parlance) to match the needs of their workload. Argonne will be slicing up this DAOS system by giving each scientific project its own DAOS pool, and each pool will be assigned to only 80% of the available DAOS servers by default. This seems like a nice way of providing most of the storage system performance to every user, but offering more freedom to work around bad hardware, bad users, and other performance problems that plague file systems like Lustre that distribute everything across every single server equally. Finally, I noticed that Aurora will be using Samsung SSDs, not the Intel (now Solidigm) QLC NAND that appeared in all the DAOS slides floating around two years ago. I'm not sure what happened there, but the move from Solidigm QLC to Samsung TLC couldn't have been cheap. New features and contributions DAOS is starting to pick up some truly valuable features that are being developed and contributed by third parties. Of note, croit has contributed a feature which allows DAOS to serve up NVMe over Fabrics targets, and Seagate contributed an S3 gateway for DAOS. Along with the DFS file system interface, DAOS now offers the trifecta of standard object, block, and file services just like Ceph. Unlike Ceph though, performance on DAOS is a first-class citizen. While croit made it clear that the NVMeoF support still has a ways to go to improve the way it does thread pooling and provides resilience, they showed 1.4 million IOPS from a single storage client using TCP over Ethernet with minimal client-side overhead. Intel is also developing multitenant support for DFUSE, allowing a single compute node to share a DAOS mount and let permissions be enforced through UID/GID just like a regular file system. Before this update, the FUSE-based nature of DAOS allowed any unprivileged user to mount their container (good), but only one FUSE agent could be alive on a single node at a time (not good) which prevented multiple users sharing a node from both mounting their own containers. DAOS also has some longer-term enhancements that I thought were interesting: expanding the range of POSIX calls supported by DAOS's intercept library to include metadata calls and memory-mapped I/O using userfaultfdimplementing collaborative caching - essentially reimplementing the Linux kernel page cache in userspace so that multiple processes can share cached DAOS pagessupporting a computational storage paradigm by enabling offload of userspace eBPF scripts to DAOS servers DAOS in a larger data center ecosystem Dean Hildebrand from Google Cloud then gave an overview of Google's efforts in bringing DAOS into the cloud. He had some nice performance graphs and I'll link the full presentation here once it's uploaded (it's worth a watch), but the part I found the most insightful was how they are trying to decide where a technology like DAOS fits in the larger cloud storage ecosystem. He outlined two different ways DAOS could work in GCP: Caching: Google Cloud Storage (GCS) is the point of truth and DAOS is a cacheTiering: DAOS is a point of truth, and GCS is an archive He said they were leaning towards the caching model where data only lives ephemerally in DAOS, and personally, I think this is the right move since DAOS in the cloud is not resilient without Optane. However, this choice reflects a much larger tension in cloud storage for HPC: The centerpiece of every cloud's data story is a scalable, low-cost, low-performance object store which is analogous to what on-prem HPC would call campaign, community, or project storage.HPC demands higher performance than what these object stores can generally deliver though. To bridge the gap between these two truths, auxiliary services must bolt on to the object layer and provide higher performance, at a higher cost, for the duration of I/O-intensive HPC jobs. Some choose to provide true tiering from object into a resilient layer of flash (like FSx Lustre and Weka do), while others project the contents of the object through a high-performance caching layer (like HPC Cache and File Cache) and are never meant to persistently hold data. This isn't rocket science, but I never thought deeply about the two models since campaign/community/project storage in on-prem HPC is usually fast enough to avoid needing caches or fine-grained tiering capabilities. John Bent also had a thought-provoking presentation about how Seagate's now-&quot;deprioritized&quot; CORTX object store, which once competed with DAOS as Mero, contains ideas that can complement DAOS: Whereas DAOS delivers high performance using NVMe, CORTX delivers great economics using HDDs, and their strengths are complementary to each other. While I don't fully grasp how a tiered (or caching!) system comprised of DAOS and CORTX could be implemented, John rightly pointed out that the same level of space efficiency can deliver higher data protection if multi-level erasure coding is used to stripe across durable block storage. His specific example was erasure coding at 8+1 across servers and 10+1 within servers to deliver both high efficiency and high durability. This could map to something like running DAOS atop something like CORVAULT, but I don't think all the necessary pieces are in place to realize such a harmonious coexistence yet. Of course, completely tossing Reed-Solomon for something more sophisticated (like VAST does with its locally decodable 150+4 scheme) obviates the need for multilevel erasure entirely. But DAOS has not gone down that route yet. And as with every talk John gives, there were lots of other interesting nuggets scattered throughout his presentation. Two of my favorites were: A slide that pointed out that, when you buy something like Ceph as an appliance, you may be spending only 25% of the total cost on storage media and the rest is infrastructure, service, and support. This struck me as a bit on the low end, but some enterprisey NAS and midrange parallel file system appliances can go this low. Spending 60% to 90% on media is a lot nicer for the buyer (and companies like Seagate) if you can buy at scale or eschew the white-glove support, and John suggested that it's up to companies like Seagate to fix the software issues that require customers to pay for white-glove support in the first place.  After all, the less someone spends on support and licenses, the more they can spend on Seagate hard drives.John's final slide pointed out that object stores were originally designed to get around the limitations of POSIX file systems, but as they've evolved over the last decade, they're starting to look a lot like file systems anyway since they require strong consistency, hierarchical namespaces, and familiar file semantics. Has all the work put into developing super-fast object stores like DAOS over the last ten years really just brought us back full circle to parallel file systems?  Companies like VAST and Weka have shown that maybe POSIX isn't as bad as the research community (myself included!) have claimed it to be; it was really just low-performance implementations that nobody wanted. Once John's talk is uploaded to the DUG 2022 website, I'll link it here.  Like Dean Hildebrand's talk, it is well worth watching (but for wildly different reasons!) PDSW 2022 I had to duck out of the DAOS User Group early to run (through the rain) to the 7th International Parallel Data Systems Workshop (PDSW 2022) on Monday afternoon. Much to everyone’s surprise, PDSW was only given a half day this year and everything felt a little compressed as a result. The organizers kept the work-in-progress (WIP) sessions which can often be an interesting peek into what students are pursuing, but little A/V problems and the unforgiving schedule probably did a disservice to the up-and-comers who use the WIP track to lay the groundwork for future full-length papers. Hopefully SC’23 restores PDSW to its original full-day status.&amp;lt;p&amp;gt;&amp;lt;/p&amp;gt; Splinters keynote from Arif Merchant at Google The keynote presentation was given by Arif Merchant from Google about Splinters, the framework that Google Cloud uses to sample I/Os in a scalable way. The challenge they face is that it's impossible to trace and store every single I/O that hits Google's storage servers (D servers), but having an understanding of I/O patterns is essential for characterizing workload I/O behavior and planning for future infrastructure. In fact, this problem is so important that Google isn't the only cloud that's solved it! A lot of what Arif talked about is very similar to how Azure does its I/O tracing under the hood. I suppose it should not be surprise that there are only so many ways to solve the challenge of sampling individual IOPS in a way that fairly represents the aggregate workload of a huge distributed storage system. One really smart thing Splinters does that I liked was sample along two different dimensions: not only do they evenly sample across all IOPS at a fixed rate (the obvious thing), but they also sample across files at a fixed rate. In this latter case of per-file sampling, they take a tiny fraction of files and capture every I/O for that file to get a complete picture of how individual files are being accessed. This file sampling fills the huge gap that exists when randomly sampling IOPS alone. Because different I/Os have different &quot;costs&quot; (for example, reading a 1 MiB file using a single 1 MiB read op or 256x 4 KiB read ops are functionally equivalent to an application), randomly sampling ops introduces systematic biases that can be difficult to back out after the data has been sampled, subsampled, aggregated, and reduced. Splinters' approach lets you see the workload from two different angles (and biases) and answer a much larger range of questions about what's really happening across thousands of storage servers. That said, it was interesting to hear Arif describe how Splinters evolved out of a different internal Google project but wound up outliving it. Splinters is also similar to, but slightly different from, their Dapper infrastructure which also does scalable distributed system tracing. And he made overtures to F1, a scalable SQL database that is similar to (but not the same as) the SQL-like query interface that Splinters uses. I got the impression that new technologies come and go pretty quickly at Google, and there's a large appetite for creating new software systems outright rather than shoehorning an existing system into solving a new problem. I can't say one way is better than the other; I was just surprised at the contrast with my own experiences. Practical papers PDSW had a healthy combination of both very-researchy papers and applied research papers this year. I could only stick around for the applied papers, and two left an impression. In the first, Jean Luca Bez presented Drishti, a tool that lives downstream of the Darshan I/O profiling library and finally does what the Darshan community has danced around for years--turning a Darshan log into an actionable set of recommendations on how to improve I/O performance. It does this by cataloguing a bunch of heuristics and using Darshan's new Python integrations to pore through a log and identify known-problematic I/O patterns. Like Jean Luca's DXT Explorer tool, Drishti has a slick user interface and greatly extends the usability and insights that can be pulled out of a Darshan log file. It probably won't win a Turing Award, but this sort of work is probably going to benefit scores of HPC end-users by making Darshan (and troubleshooting I/O problems) much more accessible to mere mortals for years to come. Adrian Jackson also presented a very tidy apples-to-apples comparison of DAOS and Lustre on the same hardware using both a systems-level benchmark and an application-inspired, object-oriented data model benchmark. The specific bake-off of a new curiosity (DAOS) and the decades-old incumbent (Lustre) is probably interesting to storage nerds, but I think the real novelty of the work is in its exploration of some uncomfortable realities that the HPC I/O community will have to face in the coming years: Does &quot;slow memory&quot; (nonvolatile Optane or CXL-attached memory SSDs) give actual benefit to existing file systems (like Lustre), or is rethinking the entire storage stack (like DAOS did) really necessary to unlock the performance of new hardware?Do applications need to rethink their approach to I/O to make use of post-POSIX storage systems like DAOS, or is performing I/O as you would on a file system (Lustre) on a post-POSIX storage system (DAOS) good enough? My take from the work is that, for simple I/O patterns like checkpoint/restart, you can get pretty far by just treating something like DAOS the same as you would a parallel file system: Figure from Manubens et al, &quot;Performance Comparison of DAOS and Lustre for Object Data Storage Approaches.&quot; But if you want your data at rest to have the same data model as how it's handled within the application, you really ought to use a storage system that supports data models that are more expressive than a stream of bytes (which is what POSIX files are). The authors didn't do a perfect job of giving Lustre its fair shake since they chose to use (abuse) directories and files to represent their application's data model on-disk instead of developing an object-file model that file systems like Lustre handle a little better. But let's be real--HPC is full of applications that do the exact same thing and represent datasets on-disk using complex hierarchies of directories and files simply because that's the easiest way to map the application's representation of data into the standard file system model. In that sense, storage systems that represent rich data models in a high-performance way should be really valuable to naive applications that map in-memory data structures directly to files and directories. Going back to John Bent's closing slide from his DAOS User Group talk, though, does any of this even matter since all answers lead back to parallel file systems? Maybe there's something to be learned about adding better back-door APIs that support more diverse data models than what POSIX file interfaces give us. The SC22 Expo The expo is my favorite part of SC because it's when I get to talk to people one-on-one and learn about corners of the HPC industry that I would've never otherwise sought out. Much to my dismay, though, I had very little time to walk the floor this year--so little that I didn't get any swag. If you want to read up on what interesting technology was being showcased, I strongly recommend reading all the great content that Patrick Kennedy and his team at STH created covering the expo. That said, I did notice some curious trends about the show floor overall. The NVIDIA booth was notably absent this year (though they shared booth space with partners), and many of the usual top vendors had significantly smaller presence on the expo floor. Just for fun, I compiled the top ten(ish) vendors by booth size: Weka.io (3,200 sqft)VAST Data, Department of Energy, Penguin Computing, HPE, and Microsoft (2,500 sqft)AWS (2,000 sqft)Google and TACC (1,600 sqft)Supermicro, AMD, Intel, Dell, NASA, and Indiana University (1,500 sqft) I think it's amazing to see all-flash storage companies at the top of the list alongside all of the Big 3 cloud service providers. I may be reading too much into this, but this may mean that the money behind SC is shifting towards companies playing in the cloud-based AI space instead of traditional big iron for simulation. Or perhaps it's a sign that most of the traditional HPC players are taking a hard look at the return they get on a big booth given the current economic climate and pulled back this year. I did chat with a couple colleagues who completely opted out of a booth this year (for reference, SC'21 had 10% fewer exhibitor booths than SC'19), and the reasoning was consistent: they found more value in having staff meet with customers privately or attend the technical sessions and engage with people organically. Combined with a bit of bad taste left over from SC's high cost of hosting pandemic-era &quot;digital booths&quot; despite low return (did anyone visit digital booths at SC'20 or SC'21?), I can see why some vendors may have chosen to skip the expo this year. Whatever the reasons may be, I was a bit sad to see such a small presence from some of my favorites like IBM, Fujitsu, Atos, and NEC. Hopefully the SC Exhibits Committee (and the economy!) can find ways to bring back the pre-pandemic glory of the show floor. The expo wasn't all doom and gloom though! Even though I couldn't make my complete rounds this year, there were a couple of highlights for me. VAST's masterful marketing Perhaps the splashiest vendor at SC was VAST Data who had a brilliant marketing presence. First was the giant Vastronaut mascot that was the centerpiece of their booth: A quick search of Twitter shows just how many people seized the opportunity to take a selfie at their booth. I would love to know how they transported that thing to and from the conference, but whatever the cost, I'll bet it was worth it. At the Grand Opening Gala on Monday, they also gave out delightfully tacky light-up cowboy hats that everyone seemed to be wearing: We were there! #sc22 #sc2022 @VAST_Data pic.twitter.com/fWhuSgBfpL — ntnu-hpc (@ntnuhpc) November 15, 2022 The subtle genius of this was that not only did people wear them during the gala and the Flop Gun-themed Beowulf Bash 2022 party later that night, but they had to wear them on their plane rides home since they were so inconveniently bulky. Proof in point, my wife (who doesn't work in tech) sent me this text message to confirm that she was waiting for me at the right luggage carousel at San Francisco Airport: I wonder how many innocent bystanders, traveling home for Thanksgiving on Thursday or Friday, saw the shiny cowboy hats at airports around the country and wondered what VAST was. The icing on the cake was VAST's CEO, Renen Hallak, parading around in an unmissable Chuck McGill-style space suit all week, clearly not taking himself too seriously and painting VAST as a work hard/play hard kind of company. Now, do flashy space suits and blinking cowboy hats alone mean VAST has a great product? I can't say**. But marketing is an art that I appreciate, and VAST hit some great notes this year. ** (Seriously, I'm not sure I wouldn't get in trouble for opining about another company here.) The Microsoft hardware bar The only booth where I spent any appreciable time this year was my own employer's. I personally love booth duty and accosting strangers on the show floor, especially if there's something interesting at the booth to jumpstart a conversation. When I worked at SDSC it was a Raspberry Pi cluster, and at the Microsoft booth this year it was the &quot;hardware bar.&quot; In addition to the customary booth presentations with giveaways, swag desk, seating area, and a fun caricature artist, the physical servers that underpin the HPC nodes in Azure were on display. Microsoft contributes its hardware platform designs to the Open Compute Project so the physical hardware that runs in Azure data centers isn't entirely mysterious. Still, every cloud has its hardware secrets, so I was surprised to see these servers laid bare. The newest HPC node type (dubbed HBv4) on display was a node powered by AMD's Genoa processors just announced a few days earlier: This wasn't a display model, either; it had real DDR5 DRAM, a real NDR InfiniBand HCA, real PCIe Gen5, and real big OCP mezzanine card with real big aluminum heat sinks and a big Microsoft sticker on top. A couple visitors commented on the way the heat piping for those Genoa CPUs was done which I guess is unusual; rather than have a giant copper block on top of each socket, heat pipes connect the socket to massive aluminum heat sinks that are closer to the chassis inlets. In retrospect it makes sense; Genoa has a whopping twelve DDR5 DIMMs per socket which leaves little extra room for heat sinks, and these 88+ core sockets have a staggering thermal design power. Another exotic piece of hardware on display was an &quot;ND MI200 v4&quot; server: It's logically similar to Azure's &quot;ND A100 v4&quot; server platform with two CPU sockets, eight SXM4 GPU sockets, eight 200G HDR InfiniBand HCAs, and a bunch of M.2 NVMes. But this specific server has eight MI200 GPUs on a common OAM baseboard and uses Infinity Fabric for GPU-to-GPU communication. I've never seen an OAM-socketed anything in real life before, much less eight of them on a baseboard, so I thought this was pretty great to see in the flesh. The ND A100 v4 platform was also on display and looked very similar-but-different with its eight A100 GPUs and HGX baseboard: And unlike the MI200 variant, the general public can run on these nodes. I'm not sure what more I'm allowed to say, but my colleague Karl made a nice, quick video that runs through the entire Microsoft booth that's worth a watch, and more details can be had by contacting me or your favorite Microsoft account team privately. Of course, the hardware bar was just a way to lure people into the booth so I could achieve my real goal: meeting new folks. As I wrote before, one of my biggest realizations at SC this year is how generally confused people are about what HPC in the cloud really means--both people who come from traditional on-prem HPC and people who come from traditional enterprisey cloud. I found myself surprising many of the people with whom I spoke on the show floor with factoids that I have taken for granted. For example, Linux is the most common OS on these HPC node types. While you probably(?) can run Windows if you want on this stuff, I think only a few niche markets do this.The usage model for an HPC cluster in the cloud can be the same as on-prem. You can have login nodes, Slurm, home directories, parallel file systems, and all that. Jobs don't have to be containerized or turned into a VM image.The InfiniBand coming out of these nodes is real InfiniBand with real OFED that supports real mpich/mvapich/OpenMPI. It's the same stuff as in on-prem supercomputers. And nodes are assembled into full-bisection fat tree InfiniBand clusters just like normal.There's no noisy neighbor problem on compute nodes because HPC node types aren't shared between users. When you run a VM on an HPC node, you get the whole thing. Just like on large supercomputers.There's no horrible loss of performance due to running in a VM. Virtualization extensions, PCIe passthrough, and SR-IOV bypass the hypervisor for most things. Inside your VM, you see real Zen cores and real Mellanox HCAs, not virtualized devices. My takeaway impression is that a lot of traditional HPC folks looked at the cloud five or ten years ago, had a sour experience, and haven't paid attention since. In those last five years, though, AI has changed the game. Massive demand for the latest CPUs and accelerators, funded by live-fast-die-young venture capital, has given cloud vendors tremendous financial incentive to catch up to on-prem levels of performance efficiency for AI workloads. And it just so happens that infrastructure that's good for AI is also good for traditional modeling and simulation. SCinet! One of the unexpected highlights of my SC this year arose from a chance encounter with a former coworker from NERSC, Ron Kumar, who gave me a whirlwind tour of SCinet. I have to confess great ignorance around SCinet in general; I always saw it was a weird technological proof of concept that the strange networking people at work would go off and do in the weeks leading up to the actual conference. I knew they did some impressive wide-area transfer demos (like the petabyte-in-a-day demo at SC'16), but I didn't really get the significance. So what is SCinet? It's this yellow bundle of cables dangling from the ceiling. &amp;lt;p&amp;gt;The yellow cables are 144-core fiber trunks that bring over a terabit per second of bandwidth into the convention center from the Internet via the national research backbones like ESnet and Internet2 and distribute many terabits per second of capacity throughout the SC conference venue. For comparison, most HPC centers in the US only have a tenth of SCinet’s wide-area bandwidth at best since 400G infrastructure is still rolling out.&amp;lt;/p&amp;gt; Most attendees may be familiar with the row of expensive-looking networking racks behind a glass wall towards the back of the expo which is where those yellow cables dangling from the ceiling end. Here's a photo from inside that glass wall: What I didn't realize is that if you go around to the back of the giant walled area behind this glass display, there's a security checkpoint that gates entry into a massive network operations center (NOC) full of laptops, spools of fiber, meeting rooms, and busily working teams in charge of all the lower layers of the networking stack. The process to get into the NOC involves an escort and being tagged in with a tamper-proof wristband, and I learned on the tour that there's millions upon millions of dollars worth of high-end networking equipment in the racks shown above. If you look closely, you can see a security camera at the end of the aisle that speaks to this; that camera was one of many. Behind the pretty public-facing side of the SCinet racks is a mess of fiber and cables: I guess if you have to tear all this down after just a few weeks, there's no point in investing days in dressing it all up nicely! I particularly enjoyed the fiber panels in the third rack that appear to be affixed to the rack post with shoe laces. This year, SCinet did do a neat proof-of-concept where they demonstrated three 400G routers from three vendors (Juniper, Arista, and Cisco?) all talking the same protocol to handle what I assume is the core routing for everything in the convention center: I wish I remembered exactly what was going on here, but I know enough about networking to know that, despite there being standard protocols for coordinating between networking gear, each vendor does their own implementation that is rarely easy to get interoperability from. If anyone out there knows the details of this achievement, please let me know so I can explain this a little better! In addition to networking nerd-level demonstrations, SCinet also serves up all the wifi across the convention center. That is why there were tripods with access points scattered around, and why astute attendees may have noticed janky networking equipment scattered around that looked like this: Again, I get it: for a network infrastructure that's only going to last a week, I don't think it's a good use of anyone's time or money to nicely dress all the networking. One last factoid I didn't know until this year was that exhibitors can request 100 Gb/s network drops into their individual booths for demos (or downloading the latest version of a PowerPoint presentation really fast). The end result of supporting both a vast wifi network and 100G fiber across the show floor is that there was a lot of fiber going into the single row of SCinet equipment: Finally, when I posted some of these photos online during the conference, my colleague Bilel was kind enough to post a slide from the SC22 opening presentation that had the speeds and feeds of what I had toured: Candy Culhane shared Scinet facts #SC22 #HPC5.01 Tb/s of WAN capacity$70M in HW &amp;amp; SW, &amp;amp; services provided by 29 SCinet contrib.175 volunteers from 80 vol. organiz.&amp;gt; 450 wireless deployed29 network research exhibition proposals11.7 miles of fiber 2384 fiber patch https://t.co/JtPhjVHZJd pic.twitter.com/kwGl5Ydqp5 — Bilel Hadri (@mnoukhiya) November 16, 2022 If you know anyone involved with SCinet, I highly recommend seeing if you can get a tour at the next SC. Even as a relative networking novice, I walked away with a much greater appreciation for the annual achievement of building SCinet. And who knows? Once I get bored of this whole storage thing, maybe I'll try getting into high-performance networking. Composability panel This year I was invited to participate in a panel titled &quot;Smackdown! Does HPC Need Composability Now?&quot; moderated by Addison Snell and Dan Olds from Intersect360 Research. This panel was...different. Unlike the traditional SC panel where panelists take turns presenting slides and saying erudite things, this panel had two teams of panelists. And my team only had one slide to present: The ground rules included &quot;personal attacks are allowed,&quot; and needless to say, the panel was about equal parts entertainment and technical discourse. That's not a bad thing, though. Addison and Dan did a phenomenal job of pulling their respective teams together and leading discussion in a format that both brought forward the key pros and cons of composability in HPC while poking fun at the thinly veiled, ego-driven personalities that often make up these sorts of panels. Rather than politely dancing around issues like sacrificing memory bandwidth by putting accelerators at the far end of a PCIe bus or gaining higher utilization by allowing users to mix and match CPU, NICs, and GPUs, us panelists were free to shoot straight (or perhaps a bit hyperbolically) and call each other out on our hidden agendas. I hope it goes without saying that all us panelists were in on the format and don't actually think people on the other side are dumb. By wrapping technical arguments in snarky comments, we could keep the level of discussion accessible to a wide audience, drive home the key points from both sides, and ensure that we weren't losing audience members who don't care about the PhD-level details as much as they want to hear what their peers are thinking about this exciting new space. I got some feedback afterwards that I didn't seem to hold back, so if anyone did take anything I said seriously, I am very sorry! On a technical level, what was the outcome? It turns out that there was about a 60/40 split between people who felt composability wasn't required yet and those who felt it was after both sides argued their case. Even among panelists, many of us were a lot less convinced about our respective positions than we let on during the panel itself. I got a chuckle when I realized that I wasn't the only one who, when invited to be on the panel, asked &quot;what side do you want me to argue?&quot; I honestly could have gone either way because the dust has not yet settled. Dan Stanzione, director of TACC, gave the truest answer to the question of &quot;will composability help HPC&quot; up front--&quot;it depends.&quot; Maybe this is a growth opportunity, or maybe it's a lukewarm reception. Either way, composable technologies are hitting the market regardless of whether you think they'll be useful or not.  AMD Genoa supports CXL 1.1 with extensions for memory pooling, Samsung has memory-semantic SSDs, and everyone and their mother is working on photonics to get higher bandwidths and lower latencies over longer distances. This makes it easier for people to dip their toes in the water to see if composability makes sense, and I think that's what a lot of people will wind up doing in the coming years. Customer meetings Unlike in years past, my SC experience this year was dominated by customer meetings. I've been on the customer side of the table plenty of times, but I was surprised to find that it was actually more fun to be on the vendor side for a change. I'm part salesman at heart, so I found it personally gratifying to end a meeting with people nodding along rather than scratching their heads. I learned as a customer that it's very easy for vendors to go way off the rails and waste everyone's time, so I was grateful to have avoided the awkward confusion that punctuates those kinds of meetings. I also went into the week worrying that I'd be sitting in the same room, hearing the same pitch and the same jokes, and answering the same questions all week. Thankfully, I work with some great field, business, and product teams who set up interesting conversations rather than rote recitations of boring roadmap slides. Approaching the same topics from different angles helped me figure out how all the pieces of what I'm working on fit together to make a complete picture too; there weren't nearly as many opportunities to do this in the DOE world since the end-users of the HPC systems on which I worked aren't told anything until all the design decisions have already been made. A few personal notes This SC was significant to me at a variety of levels; it was the first time I'd gotten on an airplane since February 2020, the first time I'd traveled since starting a new job at a new company, and the first time I'd met any of my new coworkers outside of the structure of a Teams call. During the pandemic I realized that getting out into the world and talking to people from all corners of HPC were my favorite part of my job. Not being able to go to events like SC and maintain that a sense of community involvement dramatically impacted my level of professional satisfaction for the last two years, so I'm glad I was able to finally go this year. Though customer meetings were a lot more fun than I expected them to be, I still felt bummed that I could spend so little time walking the expo, talking to folks, and attending all the BOFs normally on my must-attend list. Compounding this was my personal choice to not dine indoors and consequently miss out on almost all other chances to catch up with old friends and colleagues. I also decided to leave SC a day earlier than I usually do to reduce my risk of getting sick which didn't help either. There's never enough time at SC, but this year was particularly pressed. I say all this not to complain, but to say how much I appreciated the people who went out of their way to come accost me during the precious few hours I actually had on the exhibit floor. Some I'd not seen since SC'19, and some I'd never actually met since we only started working together mid-pandemic. The conference is busy for everyone, so giving me a slice of your time was very meaningful. That sense of community membership is why I go to SC, it's why I still work in this business, and it's why I try to contribute whatever I can to whomever wants it whether it be a student, engineer, salesperson, or marketer.</summary></entry><entry><title type="html">Converged Computing</title><link href="https://hpc.social/personal-blog/2022/converged-computing/" rel="alternate" type="text/html" title="Converged Computing" /><published>2022-11-18T08:30:00-07:00</published><updated>2022-11-18T08:30:00-07:00</updated><id>https://hpc.social/personal-blog/2022/converged-computing</id><content type="html" xml:base="https://hpc.social/personal-blog/2022/converged-computing/">&lt;p&gt;For many years, there has been a battle between cloud and HPC. The cloud side of the equation says “micro services, cloud native!”
and the HPC side says “too expensive!” Conversations often don’t progress because both sides are up-in-arms and 
focused on why they cannot work together. At best, we might get access to cloud from an HPC center,
or an company might present a product as branded for “HPC.” But it’s not truly collaborative in the way that I’d like.&lt;/p&gt;

&lt;p&gt;I’ll also step back and comment that (I do not believe) folks (myself included) on the HPC side have done enough
to sit at the table. For example, we haven’t been a voice in the Open Containers Initiative (&lt;a href=&quot;https://supercontainers.github.io/containers-wg/&quot; target=&quot;_blank&quot;&gt;although I’ve tried&lt;/a&gt;), nor have we been present (historically) for conferences that are more focused around cloud native technologies.
There is no pointing fingers or fault here - it’s just a matter of two different cultures, and it’s been challenging figuring out how to talk to one another, and how to work together. I’ve tried my best to be involved, to the best of my ability, in small ways on both sides. But I’m only one person. This isn’t to say there haven’t been small collaborations, but I believe we can do more.&lt;/p&gt;

&lt;h2 id=&quot;change-is-coming&quot;&gt;Change is Coming&lt;/h2&gt;

&lt;p&gt;I think this is going to change. The reason is because both sides of the equation have started to realize we have similar goals,
and it’s not about creating hybrid environments – having both pancakes and waffles for breakfast – but rather convergence – recognizing that pancakes and waffles are both kinds of breakfast cakes, and we can take features that we like of each to create a breakfast cake that will make everyone happy.
The idea of “Converged Computing” comes from my amazing team (see &lt;a href=&quot;https://www.youtube.com/watch?v=9VwAcSOtph0&quot; target=&quot;_blank&quot;&gt;Dan’s talk at KubeCon here&lt;/a&gt;) and is the idea that technologies from HPC can be integrated into more traditionally cloud approaches to produce a solution that
solves problems on both sides. Explicitly for these projects, it means testing the Flux Framework scheduler alongside Kubernetes. Do we still want portable workflows that can move from an HPC environment to cloud? Of course.
However, the niche or gradient that I’m interested in is the space that lives &lt;em&gt;between&lt;/em&gt; these two worlds.&lt;/p&gt;

&lt;p&gt;While I won’t go into huge detail (this would be more appropriate for a talk) the lab openly works on 
&lt;a href=&quot;https://github.com/flux-framework&quot; target=&quot;_blank&quot;&gt;Flux Framework&lt;/a&gt;, a resource manager that (in my opinion) is one of the coolest projects coming out of our space. I started working with these teams a few months ago, and am bringing my excitement and vision for (what I hope to be) a future where we are actively developing alongside other Kubernetes projects, and our work is well-known and established in this space.
What does that mean? Let me share some cool work under development. This is all being done publicly on GitHub, so there is
no issue to talk about it! My first year or so at the lab I was hired under a research project, and although I learned a lot, I haven’t felt inspired and driven until starting this work. Let’s talk about some of it! 🎉️&lt;/p&gt;

&lt;h3 id=&quot;the-flux-operator&quot;&gt;The Flux Operator&lt;/h3&gt;

&lt;div style=&quot;padding: 20px;&quot;&gt;
&lt;img src=&quot;https://flux-framework.org/flux-operator/_images/the-operator.jpg&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;If you aren’t familiar with Kubernetes Operators, let’s step back and talk about a human operator. If you are a syadmin managing apps
with associated services and databases on a cluster, you often had to do maintenance or update tasks like increasing a storage volume,
or modifying a service to a new user need. As this pattern has emerged as a common thing, they have come up with the concept of a Kubernetes Operator - an actual controller you install to your cluster that can automate this. In simple terms, after you install an operator to your cluster,
you can hand it a desired state (represented in a yaml configuration file) and the operator will do whatever it takes to reach that state. What does that means in the context of Flux? The Flux Operator is interested in creating
what we are calling a “Mini Cluster,” illustrated below.&lt;/p&gt;

&lt;div style=&quot;padding: 20px;&quot;&gt;
&lt;img src=&quot;https://flux-framework.org/flux-operator/_images/design-three-team1.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;In Kubernetes object terms this is an &lt;a href=&quot;https://kubernetes.io/docs/tasks/job/indexed-parallel-processing-static/&quot; target=&quot;_blank&quot;&gt;Indexed Job&lt;/a&gt;, a few config maps, secrets, and a &lt;a href=&quot;https://flux-framework.org/flux-restful-api/&quot; target=&quot;_blank&quot;&gt;RESTFul API&lt;/a&gt; and user interface that I designed exposed as a service.  You can read more about our current design &lt;a href=&quot;https://flux-framework.org/flux-operator/development/designs.html&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This Mini Cluster is generated from a “custom resource definition” or CRD (the yaml you provide), and it can take &lt;a href=&quot;https://flux-framework.org/flux-operator/getting_started/custom-resource-definition.html&quot; target=&quot;_blank&quot;&gt;these parameters&lt;/a&gt;. Concetually, you as the user own the Mini Cluster and can submit jobs to it (either via the web interface or the API) until you are done. When you are done, you can bring down the cluster.&lt;/p&gt;

&lt;p&gt;We are excited for this work because in the next months (to a bit longer) we are going to be testing different kinds of workloads 
running using Flux alongside this Mini Cluster, but on Kubernetes! I’ve started a small repository of dummy examples that I’m extending quickly at
&lt;a href=&quot;https://github.com/rse-ops/flux-hpc&quot; target=&quot;_blank&quot;&gt;rse-ops/flux-hpc&lt;/a&gt; and please open an issue there if you have a suggestion.&lt;/p&gt;

&lt;h3 id=&quot;stay-tuned&quot;&gt;Stay Tuned!&lt;/h3&gt;

&lt;p&gt;Stay tuned for more work in this space! I’ve been doing a ton of programming in Go, Python, and working
on a wide range of technologies, and fairly quickly, and I am very much in my happy place. Please come and join us! ❤️&lt;/p&gt;</content><author><name>Vanessasaurus</name></author><category term="vsoch" /><summary type="html">For many years, there has been a battle between cloud and HPC. The cloud side of the equation says “micro services, cloud native!” and the HPC side says “too expensive!” Conversations often don’t progress because both sides are up-in-arms and focused on why they cannot work together. At best, we might get access to cloud from an HPC center, or an company might present a product as branded for “HPC.” But it’s not truly collaborative in the way that I’d like.</summary></entry><entry><title type="html">Ceph OSD CPU Scaling - Part 1</title><link href="https://hpc.social/personal-blog/2022/ceph-osd-cpu-scaling-part-1/" rel="alternate" type="text/html" title="Ceph OSD CPU Scaling - Part 1" /><published>2022-11-08T00:00:00-07:00</published><updated>2022-11-08T00:00:00-07:00</updated><id>https://hpc.social/personal-blog/2022/ceph-osd-cpu-scaling-part-1</id><content type="html" xml:base="https://hpc.social/personal-blog/2022/ceph-osd-cpu-scaling-part-1/">&lt;p&gt;Last summer we had a user that hit some performance issues based on a recommendation to use 2 cores per OSD in their systems.  I wanted to provide some data for the community and wrote up a blog &lt;a href=&quot;https://ceph.io/en/news/blog/2022/ceph-osd-cpu-scaling/&quot;&gt;post&lt;/a&gt; on the ceph.io website.  Please take a look!&lt;/p&gt;</content><author><name>Mark Nelson's Blog</name></author><category term="markhpc" /><summary type="html">Last summer we had a user that hit some performance issues based on a recommendation to use 2 cores per OSD in their systems. I wanted to provide some data for the community and wrote up a blog post on the ceph.io website. Please take a look!</summary></entry><entry><title type="html">Containerize It, Baby!</title><link href="https://hpc.social/personal-blog/2022/containerize-it-baby/" rel="alternate" type="text/html" title="Containerize It, Baby!" /><published>2022-11-03T09:30:00-06:00</published><updated>2022-11-03T09:30:00-06:00</updated><id>https://hpc.social/personal-blog/2022/containerize-it-baby-</id><content type="html" xml:base="https://hpc.social/personal-blog/2022/containerize-it-baby/">&lt;p&gt;I’ve just submit my &lt;a href=&quot;https://twitter.com/vsoch/status/1588215058009464832&quot; target=&quot;_blank&quot;&gt;entry&lt;/a&gt; to the HPC Guru Elevator Pitch Contest for the Supercomputing 2022 conference!&lt;/p&gt;

&lt;p&gt;I’m fairly sure (like many of these contests) it will be a politically correct winner - someone that is best appealing
to the conference, but I’ll take a stand right now that I think my submission is tops in terms of creativity
and excited energy! I mean, there is just no alternative when it comes to technologies I’m excited about.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Containerize it, baby!&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;Mic Drop!&lt;/em&gt; 🎙️&lt;/p&gt;

&lt;p&gt;Regardless of the outcome of this contest, I feel like I’ve already won - I’ve had so much fun making this and sharing with the community! 🎉️&lt;/p&gt;</content><author><name>Vanessasaurus</name></author><category term="vsoch" /><summary type="html">I’ve just submit my entry to the HPC Guru Elevator Pitch Contest for the Supercomputing 2022 conference!</summary></entry><entry><title type="html">happy living close (-ish) to the metal</title><link href="https://hpc.social/personal-blog/2022/happy-living-close-ish-to-the-metal/" rel="alternate" type="text/html" title="happy living close (-ish) to the metal" /><published>2022-11-02T00:18:17-06:00</published><updated>2022-11-02T00:18:17-06:00</updated><id>https://hpc.social/personal-blog/2022/happy-living-close-ish-to-the-metal</id><content type="html" xml:base="https://hpc.social/personal-blog/2022/happy-living-close-ish-to-the-metal/">&lt;p&gt;For various reasons, I’ve been doing a little bit of career introspection lately. One of the interesting realizations to come out of this is that, despite in practice doing mostly software work, I’ve been happiest when my work involved a strong awareness of the hardware I was running on.&lt;/p&gt;

&lt;p&gt;&lt;span id=&quot;more-247&quot;&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;I suppose it shouldn’t be a surprise, exactly, but I hadn’t exactly thought about it in those terms before! Before I got into computing, I got a bachelors degree in physics, and got through much of a PhD in materials science. While I wasn’t building computers directly, I was definitely working regularly on hardware, building experimental apparatus involving various combinations of vacuum chambers, lasers, exotic microscopes, custom electronics, and microfluidics.&lt;/p&gt;

&lt;p&gt;In terms of my computing career, I’ve generally worked in the area of “high-performance computing”, a buzzword that means I’ve focused on building fast parallel systems aimed at researchers. &lt;/p&gt;

&lt;p&gt;It’s a sub-field that lends itself to awareness of hardware: even as a new baby sysadmin, I was staring at motherboard block diagrams and thinking about the performance differences between different PCIe topologies. &lt;/p&gt;

&lt;p&gt;And because HPC is one of the areas that took the longest to embrace cloud computing, I spent a lot of years doing work in datacenters. Most of my work would usually involve writing code, doing configuration management, and managing Linux systems… but on a regular basis I’d head into a big loud room full of air conditioners and server racks, carrying a screwdriver.&lt;/p&gt;

&lt;p&gt;Amusingly, my relatively recent stint at a hyperscaler was the first time I had worked on computers, but didn’t have my office in the same building as the computers I was running! Even there I was at least somewhat cognizant of hardware specifics, and one of my early projects was performance testing on the B&lt;a href=&quot;https://www.opencompute.org/documents/facebook-bryce-canyon-storage-system-specification&quot;&gt;ryce Canyon &lt;/a&gt;storage node, to see if it was ready for use in a large-scale distributed filesystem.&lt;/p&gt;

&lt;p&gt;And these days, at NVIDIA, I’m enjoying being even closer to the metal. (At least conceptually; I still work remote…) I spend my days thinking about datacenter requirements, cable lengths, firmware upgrades, hardware health checks, and application performance tests on large clusters. And I love getting to play with these shiny toys.&lt;/p&gt;

&lt;p&gt;Anyway, this is just a ramble. But a useful one. While I’d be the first to admit that cloud has its place, and I use it for some personal projects, I really enjoy understanding the hardware I run on. I have trouble thinking of computers as remote abstractions with no underlying detail. They are pleasingly physical in my mind, even if they’re thousands of miles away.&lt;/p&gt;</content><author><name>Thinking Out Loud</name></author><category term="ajdecon" /><summary type="html">For various reasons, I’ve been doing a little bit of career introspection lately. One of the interesting realizations to come out of this is that, despite in practice doing mostly software work, I’ve been happiest when my work involved a strong awareness of the hardware I was running on.</summary></entry></feed>