<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en-US"><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="https://hpc.social/personal-blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://hpc.social/personal-blog/" rel="alternate" type="text/html" hreflang="en-US" /><updated>2026-01-06T20:49:35-07:00</updated><id>https://hpc.social/personal-blog/feed.xml</id><title type="html">hpc.social - Aggregated Personal Blog</title><subtitle>Shared personal experiences and stories</subtitle><author><name>hpc.social</name><email>info@hpc.social</email></author><entry><title type="html">‚ÄúMy Cousin Vinny‚Äù as an LLM benchmark</title><link href="https://hpc.social/personal-blog/2026/my-cousin-vinny-as-an-llm-benchmark/" rel="alternate" type="text/html" title="‚ÄúMy Cousin Vinny‚Äù as an LLM benchmark" /><published>2026-01-05T04:04:31-07:00</published><updated>2026-01-05T04:04:31-07:00</updated><id>https://hpc.social/personal-blog/2026/-my-cousin-vinny-as-an-llm-benchmark</id><content type="html" xml:base="https://hpc.social/personal-blog/2026/my-cousin-vinny-as-an-llm-benchmark/"><![CDATA[<p>Mike Caulfield wrote a <a href="https://mikecaulfield.substack.com/p/notes-towards-a-narrative-llm-benchmark">very thorough and quite entertaining article</a> about posing the following question to ChatGPT:</p>

<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p>What were Marisa Tomei‚Äôs most famous quotes from My Cousin Vinny and what was the context?</p>

</blockquote>

<p>Depending on the model selected, the answers to this varied from hilariously wrong, to plausible-but-flawed, to accurate. </p>

<p>Interestingly, substantial test-time compute (‚Äúthinking time‚Äù) seems to be necessary to do a good job here, despite the easy availability online of famous quotes, plot summaries, and even the script. While the fast-response models available for free were prone to hallucinate. </p>

<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p>At the same time I was struck just how&nbsp;<em>much</em> reasoning time needed to be expended to get this task right. It‚Äôs possible that&nbsp;<em>My Cousin Vinny</em>&nbsp;is uniquely hard to parse, but I don‚Äôt think that is the case. I‚Äôve tried this with a half dozen other films and the pattern seems to hold. If it‚Äôs true that a significant amount of similar film contextualization tasks are solvable with test-time compute but require extensive compute to get it right, it seems to me this could be the basis of a number of useful benchmarks.</p>

</blockquote>

<p>The <a href="https://mikecaulfield.substack.com/p/notes-towards-a-narrative-llm-benchmark">full article</a> is well-worth reading, and not only because it discusses <em>My Cousin Vinny</em> in substantial detail (great movie).</p>]]></content><author><name>Thinking Out Loud</name></author><category term="ajdecon" /><summary type="html"><![CDATA[Mike Caulfield wrote a very thorough and quite entertaining article about posing the following question to ChatGPT:]]></summary></entry><entry><title type="html">Podcasts and blogs I‚Äôm following in early 2026</title><link href="https://hpc.social/personal-blog/2026/podcasts-and-blogs-i-m-following-in-early-2026/" rel="alternate" type="text/html" title="Podcasts and blogs I‚Äôm following in early 2026" /><published>2026-01-02T19:13:35-07:00</published><updated>2026-01-02T19:13:35-07:00</updated><id>https://hpc.social/personal-blog/2026/podcasts-and-blogs-i-m-following-in-early-2026</id><content type="html" xml:base="https://hpc.social/personal-blog/2026/podcasts-and-blogs-i-m-following-in-early-2026/"><![CDATA[<p>As part of the new year, I&#8217;m going through my feed readers for podcasts and blogs. This is mostly a cleanup exercise to remove sources that I regularly skip, but I&#8217;m also adding in a few feeds for sites that I find myself regularly clicking on in social media. As part of this, I figured I&#8217;d share the sources that made the cut to stick around.</p>

<p><span id="more-377"></span></p>

<p>You&#8217;ll notice that there are a lot of podcasts in this post! With two golden doodles in the family, I spend a lot of time on dog walks, not to mention doing chores around the house. Because of this, it&#8217;s often a lot easier for me to listen to content than read it, and indeed I often find myself feeding long-form text articles into <a href="https://elevenreader.io/">ElevenReader</a> so that I can listen to those items too.</p>

<p><strong>Nerd notes:</strong></p>

<ul class="wp-block-list">
<li>I self-host <a href="https://freshrss.org/index.html">FreshRSS</a> to aggregate written blog feeds and read them using <a href="https://reederapp.com/">Reeder</a>. FreshRSS is hosted on a private VPS that I access via <a href="https://tailscale.com/">Tailscale</a> on my various devices, because I&#8217;m really the only person that needs to access it.</li>



<li>I listen to podcasts via <a href="https://overcast.fm/">Overcast</a>, which I prefer for its audio features to the default Apple Podcasts app.</li>



<li>This is not 100% complete as there are some blogs I follow purely through the Patreon site, and I haven‚Äôt (yet) taken the time to go through that and add them to this list.</li>



<li>There are a few NSFW items left out intentionally, as my voice on this blog is at least semi-professional <img alt="üòâ" class="wp-smiley" src="https://s.w.org/images/core/emoji/17.0.2/72x72/1f609.png" style="height: 1em;" /> </li>
</ul>

<h2 class="wp-block-heading">Computing-related</h2>

<p><strong>Podcasts:</strong></p>

<ul class="wp-block-list">
<li><em><a href="https://oxide-and-friends.transistor.fm/">Oxide and Friends</a></em> is a weekly live show recorded by Bryan Cantrill, Adam Leventhal, and friends from the <a href="https://oxide.computer">Oxide Computer Company</a>. Despite being a &#8220;corporate&#8221; podcast, it generally has the vibe of &#8220;Car Talk for Computers&#8221; and can often dig into really interesting computer systems topics, and even computer and industry history. Some of the episodes get into specifics of the Oxide product, which may be interesting or skip-able depending on your interests.</li>



<li><em><a href="https://www.fafo.fm/">Fork Around and Find Out</a></em> is a resurrection of the <em>Ship It!</em> podcast that used to be part of the Changelog network, focused on production systems, on-call, and large-scale engineering. The updates are a bit irregular but Justin and Autumn are enjoyable hosts to listen to.</li>



<li><em><a href="https://www.thisisfinepod.com/">This is Fine!</a></em> is a podcast on resilience engineering from Colette Alexander and Clint Byrum. It sometimes picks up topics from discussions in the <a href="https://resilienceinsoftware.org/">Resilience in Software Foundation Slack</a> instance and is almost always a fun listen.</li>



<li><em><a href="https://randsinrepose.com/the-important-thing/">The Important Thing</a> </em>is an occasional discussion podcast between <a href="https://randsinrepose.com/">Michael Lopp</a> and <a href="https://troxell.com/">Lyle Troxell</a>. Often very random, with widely varying episode lengths, it&#8217;s nicely chatty and a great listen during dog walks.</li>



<li><em><a href="https://signalsandthreads.com/">Signals and Threads</a></em>, from Jane Street, is another occasional podcast but often gets very deep into interesting software topics such as performance analysis, state machine replication, and memory management &#8212; with a focus on low-latency trading systems that have interesting constraints. </li>



<li><a href="https://changelog.com/podcast"><em>The Changelog</em></a> is a long-standing general software news podcast. I dip in and out of this one based on the topic being covered, and often like their &#8220;Changelog and Friends&#8221; chatty episodes more than the news or interview episodes.</li>



<li><em><a href="https://comparchpodcast.podbean.com/">The Compute Architecture Podcast</a></em> updates very infrequently but often has interesting, hardware- or systems-focused interviews.</li>
</ul>

<p><strong>Personal blogs:</strong></p>

<ul class="wp-block-list">
<li><a href="https://charity.wtf/">Charity Majors</a> is a long-time follow of mine for her technical work on observability, her practical SRE/sysadmin mentality, and her useful perspectives on engineering management.</li>



<li><a href="https://www.brendangregg.com/blog/">Brendan Gregg</a> posts infrequently, but will publish fascinating deep dives on performance engineering that are always worth reading.</li>



<li><a href="https://utcc.utoronto.ca/~cks/space/blog/">Chris Siebenmann</a> is a sysadmin at the University of Toronto and a prolific blogger about nuts-and-bolts Linux admin topics. He publishes a <em>ton</em> so I dip in and out of his feed, but always keep it in my feed reader.</li>



<li><a href="https://www.drcathicks.com/blog">Cat Hicks</a> does psychological research on software teams and I always learn a ton from her writing.</li>



<li><a href="https://soatok.blog/b/">Soatok</a> writes excellent, interesting, and opinionated articles on security and cryptography topics.</li>



<li><a href="https://ferd.ca/">Fred Hebert</a> is an SRE with a strong interest in resilience engineering who frequently discusses interesting academic research on resilience and human factors.</li>



<li><a href="https://blog.glennklockwood.com/">Glenn Lockwood</a> is an HPC engineer who I&#8217;ve known online for a long time, and who came into the field from materials science in a similar manner to me. He&#8217;s worked at SDSC, NERSC, Microsoft, and VAST, and his annual recap of the SuperComputing conference is worth reading every year. (So is the rest of his blog!)</li>



<li><a href="https://www.seangoedecke.com/">Sean Goedecke</a> is a software engineer at Github who writes interesting work on AI and on the dynamics of large companies.</li>



<li><a href="https://simonwillison.net/">Simon Willison</a> is one of the most essential bloggers on AI and LLMs today, not to mention incredibly prolific. His style of writing short posts on whatever he&#8217;s thinking about is one I hope to emulate more often here!</li>



<li>Rachel Kroll, aka <a href="https://rachelbythebay.com/w/">Rachel By the Bay</a>, is a long-time sysadmin/SRE who writes on detailed sysadmin and software engineering topics in an often-ironic fashion.</li>



<li><a href="https://xeiaso.net/blog/">Xe Iaso</a> is a software engineer and author of the <a href="https://anubis.techaro.lol/">Anubis</a> Web AI Firewall tool. Xer blog covers a wide variety of software, systems, and AI work.<br /></li>
</ul>

<p><strong>Technical blogs and industry news:</strong></p>

<ul class="wp-block-list">
<li><em><a href="https://lwn.net/">LWN</a></em> is the definitive source for Linux and free software news, and is supported by the community via subscriptions. You should subscribe!</li>



<li><em><a href="https://chipsandcheese.com/">Chips and Cheese</a></em> does really interesting deep dives into chip architecture and performance, often focused on newer products but occasionally digging into older hardware.</li>



<li><a href="https://semianalysis.com/"><em>SemiAnalysis</em></a> is at this point one of the most essential news sources for the semiconducting industry, and one of the few paid sources I follow.</li>



<li><em><a href="https://jepsen.io/blog">Jepsen</a></em> performs detailed analyses of distributed systems reliability and consistency by <a href="https://aphyr.com/">Kyle Kingsbury</a>, both as consulting engagements and for the community. Read all of these, they&#8217;re excellent!</li>



<li><em><a href="https://semiengineering.com/">Semiconductor Engineering</a></em> is one of the long-standing industry news sites. I don&#8217;t read a ton of this but I do keep an eye on the feed for interesting headlines.</li>



<li>Similarly, <em><a href="https://www.datacenterdynamics.com/en/">Data Center Dynamics</a></em> is one of the standard industry news sites for data centers.</li>
</ul>

<h2 class="wp-block-heading">News and Politics</h2>

<p><strong>Podcasts:</strong></p>

<ul class="wp-block-list">
<li><em><a href="https://www.lawfaremedia.org/podcasts-multimedia/podcast">The Lawfare Podcast</a></em> covers a really wide variety of national security law topics. My only regular listen is their <em><a href="https://www.lawfaremedia.org/podcasts-multimedia/podcast/rational-security">Rational Security</a></em> episodes which provide a weekly roundup of relevant news in an informal discussion format, but I dip in and out of the others.</li>



<li><em><a href="https://www.bloomberg.com/podcasts/series/money-stuff">Money Stuff</a></em> is a fun weekly podcast from Matt Levine and Katie Greifeld of Bloomberg News, who discuss weekly financial news from a very nerdy perspective. I&#8217;m not generally a huge finance person, but I like that this podcast allows me to listen in to people geeking out about the topic.</li>



<li><em><a href="https://www.economist.com/the-world-in-brief">The World in Brief</a></em> from the Economist is their daily quick summary of the news. I have very mixed feelings about the Economist in general &#8212; as with many British sources, they platform far too much transphobia &#8212; but I have yet to find a better substitute for &#8220;quick morning summary of the news&#8221;. At least, nothing else that doesn&#8217;t make me want to throw my phone at a wall.</li>
</ul>

<p><strong>Blogs and News:</strong></p>

<ul class="wp-block-list">
<li><em><a href="https://restofworld.org/">Rest of World</a></em> covers tech industry news with a focus on impacts outside the West, and often has really interesting coverage from a different angle.</li>



<li><em><a href="https://www.liberalcurrents.com/">Liberal Currents</a></em> is a political blog focused on liberalism, both in current events and as a political philosophy, and has published a lot of excellent pieces since I started reading it in 2025.</li>
</ul>

<h2 class="wp-block-heading">Miscellaneous</h2>

<p><strong>Podcasts:</strong></p>

<ul class="wp-block-list">
<li><a href="https://www.armscontrolwonk.com/archive/author/podcast/"><em>Arms Control Wonk</em></a> continues to be a good listen, though it&#8217;s updated something sporadically the past few years. The coverage of nuclear weapons, missiles and other delivery systems, and current events around arms control (or lack thereof) is very good. If you sponsor them via Patreon, their Slack instance is also a fascinating discussion forum, though I only dip in and out of it occasionally.</li>



<li><em><a href="https://culturestudypod.substack.com/">The Culture Study Podcast</a></em> by Anne Helen Petersen features conversations between Anne and a guest and focuses on listener Q&amp;A. It often covers culture topics that I otherwise don&#8217;t get much of through other feeds. For example, recent episodes have talked about anything from birding to K-pop to the anatomy of cultural panics. I don&#8217;t listen to every episode, but they&#8217;re often quite fun.</li>



<li><em><a href="https://www.liberalcurrents.com/neonliberalism/">Neon Liberalism</a></em> is a regular podcast from <a href="https://www.liberalcurrents.com/">Liberal Currents</a>. I might put this in the News category except that it often digs into political topics from a historical or theoretical perspective rather than just focusing on current events.</li>



<li>Similarly, <em><a href="https://open.spotify.com/show/29wW6zsYyYuelcFJcyHOmv">Reimagining Liberty</a></em> from Aaron Ross Powell digs into political theory and current events from the perspective of Powell&#8217;s particular strain of libertarian-ism, which is much more in conversation with modern liberalism and anarchism vs more right-wing strains.</li>
</ul>

<p><strong>Personal blogs:</strong></p>

<ul class="wp-block-list">
<li><a href="https://www.funraniumlabs.com/">Phil Broughton</a> is a health physicist at UC Berkeley who has worked in classified nuclear work at LLNL as well as spending a year in Alaska, and has a wealth of fascinating and hilarious stories.</li>



<li><a href="https://www.goodreads.com/author/show/16094.Lois_McMaster_Bujold/blog">Lois McMaster Bujold</a> is one of my favorite science fiction and fantasy authors. While she&#8217;s semi-retired, she still writes occasional novellas following Penric, a sorcerer in her World of the Five Gods, which I really love. Her blog helpfully announces new stories!</li>



<li><a href="https://whatever.scalzi.com/">John Scalzi</a> is another favorite author, and also has an excellent blog called <em>Whatever</em>.</li>



<li>Bret Devereaux writes <em><a href="https://acoup.blog/">A Collection of Unmitigated Pedantry</a></em> about history, the military, and pop culture. If you&#8217;re interested in an extensive deep dive into the military missteps made Saruman in <em>The Two Towers</em>, this is the blog for you!</li>



<li><em><a href="https://www.bitsaboutmoney.com/">Bits About Money</a></em> is Patrick McKenzie&#8217;s blog about finance, and each entry tends to be a highly-nerdy deep dive about how some interesting corner of the financial system works.</li>
</ul>

<p><strong>Webcomics:</strong></p>

<ul class="wp-block-list">
<li><em><a href="https://www.giantitp.com/comics/oots.html">The Order of the Stick</a></em> is a long-running D&amp;D stick-figure comic that I have been reading for longer than I can really say. I highly recommend it, though I&#8217;ll warn you that with an archive of &gt;1,300 comics (and growing!) you are likely to lose a lot of time this way.</li>



<li><em><a href="https://questionablecontent.net/">Questionable Content</a></em> is a slice-of-life comic about a coffee shop&#8230; with robots, super-intelligent AIs, stupid dick jokes, and more. Also has a long archive to dig through.</li>



<li><em><a href="https://www.girlgeniusonline.com/">Girl Genius</a></em> is a long-running online comic book about &#8220;Adventure, Romance, and MAD SCIENCE!&#8221; and thoroughly excellent.</li>
</ul>]]></content><author><name>Thinking Out Loud</name></author><category term="ajdecon" /><summary type="html"><![CDATA[As part of the new year, I&#8217;m going through my feed readers for podcasts and blogs. This is mostly a cleanup exercise to remove sources that I regularly skip, but I&#8217;m also adding in a few feeds for sites that I find myself regularly clicking on in social media. As part of this, I figured I&#8217;d share the sources that made the cut to stick around.]]></summary></entry><entry><title type="html">On Friday deploys</title><link href="https://hpc.social/personal-blog/2025/on-friday-deploys/" rel="alternate" type="text/html" title="On Friday deploys" /><published>2025-12-30T20:58:00-07:00</published><updated>2025-12-30T20:58:00-07:00</updated><id>https://hpc.social/personal-blog/2025/on-friday-deploys</id><content type="html" xml:base="https://hpc.social/personal-blog/2025/on-friday-deploys/"><![CDATA[<p><a href="https://charity.wtf/2025/12/24/on-friday-deploys-sometimes-that-puppy-needs-murdering-xpost/">This post</a> from Charity Majors on Friday deploys is well worth reading. </p>

<p>In the past I‚Äôve seen her comment on how deployments should be carried out fearlessly regardless of when, and I‚Äôve often felt like saying ‚Äúyeah, well, ‚Ä¶‚Äù. Because of course I agree with that as a goal, but many real-world orgs and conditions make it challenging.</p>

<p>This most recent post talks about the situations when those freezes <em>can</em> make sense, even if they‚Äôre not ideal. And in particular I like the discussion about what really needs to be frozen is not deploys, but merges:</p>

<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p>To a developer, ideally, the act of merging their changes back to main and those changes being deployed to production should feel like one singular atomic action, the faster the better, the less variance the better. You merge, it goes right out. You don‚Äôt want it to go out, you better not merge.</p>




<p>The worst of both worlds is when you let devs keep merging diffs, checking items off their todo lists, closing out tasks, for days or weeks. All these changes build up like a snowdrift over a pile of grenades. You aren‚Äôt going to find the grenades til you plow into the snowdrift on January 5th, and then you‚Äôll find them with your face. Congrats!</p>

</blockquote>]]></content><author><name>Thinking Out Loud</name></author><category term="ajdecon" /><summary type="html"><![CDATA[This post from Charity Majors on Friday deploys is well worth reading.]]></summary></entry><entry><title type="html">Why generic software design advice is often useless</title><link href="https://hpc.social/personal-blog/2025/why-generic-software-design-advice-is-often-useless/" rel="alternate" type="text/html" title="Why generic software design advice is often useless" /><published>2025-12-30T03:33:15-07:00</published><updated>2025-12-30T03:33:15-07:00</updated><id>https://hpc.social/personal-blog/2025/why-generic-software-design-advice-is-often-useless</id><content type="html" xml:base="https://hpc.social/personal-blog/2025/why-generic-software-design-advice-is-often-useless/"><![CDATA[<p>In <em><a href="https://www.seangoedecke.com/you-cant-design-software-you-dont-work-on/">You can&#8217;t design software you don&#8217;t work on</a>, </em>Sean Goedecke discusses why generic advice on the design of software systems is often unhelpful.</p>

<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p><strong>When you‚Äôre doing real work, concrete factors dominate generic factors</strong>. Having a clear understanding of what the code looks like right now is far, far more important than having a good grasp on general design patterns or principles.</p>

</blockquote>

<p>This tracks with my experience not just of software systems, but also systems with a hardware component (eg ML training clusters) or a facility component (eg datacenters). The specifics of your system absolutely dominate any general design guidance.</p>

<p>As the manager of a team that publishes reference architectures, I do think that it‚Äôs helpful to clearly understand where your specific design differs from generic advice. If you‚Äôre going off the beaten path, you should k<em>now </em>you‚Äôre doing that! And be able to plan for any additional validation involved in doing that.</p>

<p>But relatedly, this is part of why I think that any generic advice should be based on some actually existing system. If you are telling someone they should follow a given principle, you should be able to point to an implementation that <em>does</em> follow that principle. </p>

<p>Or else you‚Äôre just speculating into the void. Which admittedly can be <em>fun</em> but is not nearly as valuable as speaking from experience.</p>]]></content><author><name>Thinking Out Loud</name></author><category term="ajdecon" /><summary type="html"><![CDATA[In You can&#8217;t design software you don&#8217;t work on, Sean Goedecke discusses why generic advice on the design of software systems is often unhelpful.]]></summary></entry><entry><title type="html">Large software systems</title><link href="https://hpc.social/personal-blog/2025/large-software-systems/" rel="alternate" type="text/html" title="Large software systems" /><published>2025-12-28T19:50:27-07:00</published><updated>2025-12-28T19:50:27-07:00</updated><id>https://hpc.social/personal-blog/2025/large-software-systems</id><content type="html" xml:base="https://hpc.social/personal-blog/2025/large-software-systems/"><![CDATA[<p>In <em><a href="https://seangoedecke.com/nobody-knows-how-software-products-work/">Nobody understands how large software products work</a></em>, Sean Goedecke makes a number of good points about how difficult it is to really grasp large software systems.</p>

<p>In particular, some features impact every part of the system in unforeseen ways:</p>

<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p>Why are these features complicated? Because&nbsp;<strong>they affect every single other feature you build</strong>. If you add organizations and policy controls, you must build a policy control for every new feature you add. If you localize your product, you must include translations for every new feature. And so on. Eventually you‚Äôre in a position where you‚Äôre trying to figure out whether a self-hosted enterprise customer in the EU is entitled to access a particular feature, and&nbsp;<em>nobody knows</em>&nbsp;&#8211; you have to go and read through the code or do some experimenting to figure it out.</p>

</blockquote>

<p>Sean also points out that eventually the code itself has to be the source of truth, and debugging requires deep investigation of the continually-changing system.</p>

<p>I‚Äôve seen this happen in a bunch of different orgs, and it does seem to be true, especially for products with a large number of collaborating teams. I would add that in addition to the code itself, you often need to have conversations with the relevant teams to discern intent and history. Documentation only goes so far, eventually you need talk to people.</p>]]></content><author><name>Thinking Out Loud</name></author><category term="ajdecon" /><summary type="html"><![CDATA[In Nobody understands how large software products work, Sean Goedecke makes a number of good points about how difficult it is to really grasp large software systems.]]></summary></entry><entry><title type="html">SC‚Äô25 recap</title><link href="https://hpc.social/personal-blog/2025/sc-25-recap/" rel="alternate" type="text/html" title="SC‚Äô25 recap" /><published>2025-12-01T14:34:00-07:00</published><updated>2025-12-01T14:34:00-07:00</updated><id>https://hpc.social/personal-blog/2025/sc-25-recap</id><content type="html" xml:base="https://hpc.social/personal-blog/2025/sc-25-recap/"><![CDATA[<p>
  The annual SC conference was held last week, drawing over
  <a href="https://www.hpcwire.com/2025/11/19/sc25-observations-more-pumps-than-processors/">16,000 registrants and 560 exhibitors</a>
  to in St. Louis, Missouri to talk about high-performance computing, artificial
  intelligence, infrastructure, and science. It was my tenth time attending
  in-person (12th overall), and as is always the case, it was a great week to
  reconnect with colleagues, hear what people are worrying about, and get a
  finger on the pulse of the now-rapidly changing HPC industry.
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">Outside the SC'25 convention center on the only clear day of the week.</figcaption>
</figure>
</div>
<p>Although every SC I've attended always felt a little different from the
  previous year, this one felt quite different. Part of that results from my own
  personal circumstances: this is the first year I attended as an employee of
  VAST Data, and so the people with whom I met and the technical problems to
  which I paid attention were certainly biased towards those most relevant to my
  work. But the backdrop of the whole conference has also shifted. It's been
  three SC conferences since ChatGPT came out, and it's now undeniable that AI
  isn't simply on the horizon; it's shaping the field of HPC and scientific
  computing. What used to be an argument of "<a href="https://blog.glennklockwood.com/2024/05/isc24-recap.html#section11">us vs. them</a>" is now more like "them (and us?)"<span></span></p>
<p></p>
<p>
  As has become tradition, I'm sharing some of my thoughts from the week with
  the world in the hopes that someone finds this interesting and insightful.
  I've roughly organized them into two areas big themes and the exhibition hall.
</p>
<ul style="text-align: left;">
<li>
<a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#big-themes">Big themes</a>
<ul>
<li>
<a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#theme-1-the-big-number-is-losing-its-shine">Theme 1: The big number is losing its shine</a>
<ul>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#top500">Top500</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#the-gordon-bell-prize">The Gordon Bell Prize</a></li>
<li>
<a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#fixing-problems-caused-by-the-big-number">Fixing problems caused by the big number</a>
</li>
</ul>
</li>
<li>
<a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#theme-2-hpc-policy-is-becoming-ai-policy">Theme 2: HPC policy is becoming AI policy</a>
</li>
<li>
<a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#theme-3-ai-discourse-is-growing-up">Theme 3: AI discourse is growing up</a>
<ul>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#agentic-workflows">Agentic workflows</a></li>
<li>
<a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#data-and-agentcentric-service-infrastructure">Data and agent-centric service infrastructure</a>
</li>
</ul>
</li>
</ul>
</li>
<li>
<a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#the-exhibit-hall">The exhibit hall</a>
<ul>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#by-the-numbers">By the numbers</a></li>
<li>
<a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#interesting-new-technology">Interesting new technology</a>
<ul>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#dell-ir700">Dell IR700</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#hpe-cray-gx5000">HPE Cray GX5000</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="big-themes">Big themes</h2>
<p>
  HPC has always been at the center of a tension between keeping things the same
  (supercomputers are the most stable the day they are turned off) and pushing
  the technological envelope (which is the fastest way to unlock new discovery).
  The desire to push the envelope has always been a "pull" towards the future;
  researchers first led with kooky ideas (like DAOS and Kokkos), and as those
  ideas turn from research into production, they make new technologies (like
  all-flash and AMD GPUs) accessible to scientists.
</p>
<p>
  What hasn't historically happened, though, is a strong "push" towards the
  future. Scientific HPC centers push themselves to justify building the next
  big supercomputer, but it's been a given that there will always be another big
  machine, so this push has been internal and gentle. Combined with the
  not-so-urgent pull of HPC researchers, every center has gotten a new machine
  every five years or so.
</p>
<p>
  This is the year where it became clear to me that AI is now exerting a strong
  push on the HPC industry--a shove even, forcing HPC centers around the world
  to align themselves on an AI mission if they want to survive. All the
  big-money HPC systems being announced this year are clearly being positioned
  as AI-first and AI-motivated, and these announcements are going well beyond
  simply peppering "AI" throughout the press release and otherwise acting as if
  it was business-as-usual. This is the first SC where I saw scientists,
  architects, and decision-makers being being forced to confront real tradeoffs
  favor either HPC or AI, and they are beginning to choose AI.
</p>
<p>
  This push-and-pull on HPC towards the future manifested in three big themes.
</p>
<h3 id="theme-1-the-big-number-is-losing-its-shine">
  Theme 1: The big number is losing its shine
</h3>
<p>
  HPC has long organized itself around treating the big machine and the big
  number as its top priority, and this is why the two largest HPC conferences of
  the year honor the semiannual release of the Top500 list on their main stage.
  However, this year felt like the first time that one number (that somehow
  reflects "performance") dominated the conversation. Instead, the discourse was
  more diffuse and discussed "performance and x" or "the supercomputer and x."
</p>
<h4 id="top500">Top500</h4>
<p>
  The place where this was most evident to me was at the
  <a href="https://sc25.conference-program.com/presentation/?id=bof117&amp;sess=sess409">Top500 BOF</a>, where the latest list was unveiled.
</p>
<p>
  The biggest announcement was that Europe now has its first benchmark-confirmed
  exascale system in JUPITER, which ran a full-system HPL at
  <a href="https://mastodon.social/@andih/115566907716591104">1,000,184 TFLOPS</a>
  for two hours and seven minutes. However, JUPITER didn't get any stage time at
  the BOF since, like Aurora, it actually debuted on a previous list with a
  sub-exascale run. This run pushed it over the finish exascale finish line, but
  if the Top500 list metadata is to be believed, the run used 100% of JUPITER's
  5,884 nodes to break the barrier--a feat that is unlikely to be reproduced on
  any production applications, since it is rare to have zero failed nodes in any
  large-scale production environment.
</p>
<p>
  So, while there was little fanfare for Europe in breaking the exaflops barrier
  with its new big machine and big number, there were some big
  announcements--one overt, and others more muted.
</p>
<p>
  The biggest news was that <strong>the Top500 list is changing hands</strong>.
  Whereas it has historically been controlled by three people--Jack Dongarra,
  Horst Simon, and Erich Strohmaier--it will be transitioning to be
  community-controlled under the stewardship of ACM SIGHPC. Dongarra, Simon, and
  Strohmaier will still be on the steering committee under the ACM stewardship,
  but this new governance structure opens the doors for new ideas to breathe new
  life into the way systems are ranked and, more broadly, how "performance" is
  meant to be interpreted from Rmax.
</p>
<p>
  At present, the list (and related lists) are bound by rules that, in the
  present day of reduced-precision accelerators, make little sense. For example,
  using the Ozaki scheme within the LU decomposition is not allowed by Top500
  despite the fact that it can produce the same answer with the same numerical
  accuracy much faster than hardware FP64. And while the HPL-MxP benchmark does
  allow solving the same problem using more creative methods, Strohmaier
  highlighted a problem there too: it never dictated how to deal with multiple
  levels of mixed precision until AIST broke the rankings. AIST ran HPL-MxP at
  both 16-bit and 8-bit precisions, resulting in their ABCI 3.0 system
  simultaneously ranking at #6 and #10.
</p>
<p>
  These sorts of issues make it easy to question the value of leaderboards like
  Top500 or HPL-MxP, as their definition of "performance" becomes increasingly
  further divorced from how large supercomputers are really used. The past few
  years have shown that there hasn't been the time or energy to get ahead of
  these ambiguities amongst the three men maintaining the list, so transitioning
  it to ACM will hopefully be a positive move that will give the list a chance
  to be revitalized.
</p>
<p>
  To their credit,
  <strong>the incipient stagnation of the Top500 list</strong> was called out by
  Strohmaier during his analysis of the list, acknowledging that "growth has
  tremendously slowed down compared to what it used to be" and "we don't have
  proof of what is actually the reason for that:"
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">All the key highlights of this SC's Top500 list.</figcaption>
</figure>
</div>
<p>China has stopped submitting, the AI and hyperscale providers really never
  started submitting, and retired systems are being thrown off the list long
  before they fall off the bottom. To me, this was a tacit acknowledgment that
  the list does not have a bright future out to 2030 unless it is modernized to
  be relevant to the way in which today's largest systems are actually being
  used--which is not DGEMM.
</p>
<p>
  The final surprising acknowledgment during Strohmaier's talk was that
  <strong>the list is trailing the state of the art in hardware</strong> by
  quite a bit. He pointed out that Blackwell systems are only now starting to
  appear even though they've been shipping in volume for the better part of a
  year. While he hypothesized that there is "uneasiness" about Blackwell in an
  HPC context, the reality is that there are no Blackwells for HPC until the
  Blackwell orders for hyperscale AI have been fulfilled. HPC is second in line,
  and even then, the only Blackwells I could find on this year's Top500 list
  were NVL8 configurations--not the NVL72 configurations that have been filling
  up hyperscale datacenters like
  <a href="https://glennklockwood.com/garden/systems/Fairwater">Fairwater</a>.
</p>
<p>
  Strohmaier pointed out that Blackwell, by virtue of its HBM3e (vs. Hopper's
  HBM3), is showing up higher on the HPCG list (which is a memory bandwidth
  test) than on Top500 (which is an FP64 FLOPS test). He phrased this as
  evidence that "not everything is bad for the HPC community," but I would have
  phrased my conclusion a little differently:
</p>
<ol type="1">
<li>
    Blackwell is actually great for HPC, because most real workloads are
    memory-bandwidth bound, not FLOPS bound. The fact that B200 offers similar
    FP64 FLOPS at higher memory bandwidth means that real applications will get
    higher effective use of those FP64 FLOPS.
  </li>
<li>
    Despite the above, Blackwell doesn't perform well on Top500 because HPL
    doesn't reflect the reality that memory bandwidth is important. It follows
    that HPL doesn't reflect the reality of real HPC applications. A Blackwell
    system can be significantly better for real HPC applications than a
    comparably sized Hopper system even though it may rank lower than Hopper on
    Top500.
  </li>
<li>
    Blackwell isn't showing up in volume now because the HPC community is second
    in line. The HPC community isn't uneasy as much as it is completely locked
    out. The first NVIDIA-based exascale system debuted in November 2025 despite
    its GPU being three years old, suggesting that if big Blackwell systems ever
    appear on Top500, it'll happen in 2026-2027.
  </li>
</ol>
<p>
  All of this is a roundabout way of showing that the big number--in this case,
  the HPL score--no longer leads meaningful conversation around how useful a
  system is for science.
</p>
<h4 id="the-gordon-bell-prize">The Gordon Bell Prize</h4>
<p>
  Another major indicator of the changing tide away from the big number was the
  work that won this year's
  <a href="https://awards.acm.org/bell">Gordon Bell Prize</a>. The winning
  paper, titled "<a href="https://arxiv.org/html/2504.16344v2">Real-time Bayesian inference at extreme scale: A digital twin for tsunami
    early warning applied to the Cascadia subduction zone</a>," wasn't the typical case of running a huge simulation for a few hours and
  reporting some result. Rather, it described a four-step workflow that
  culminates in the desired insight popping out of a computation that runs
  across only 128 nodes and completes in less than 0.2 seconds. Furthermore, the
  hero run part could be decomposed into trivially parallel components, allowing
  the bulk of the computation to be geographically distributed across HPC
  centers or GPUs spread across on-prem and cloud providers.
</p>
<p>
  My understanding of the work is that there was a massive "offline" computation
  to precompute a few key matrices (Phase 1) followed by two shorter offline
  steps that turn those matrices into the core of the digital twin. The last
  step, which was "online" and designed to be computed in real-time, could then
  take this core and solve the input problem with extremely low latency. This
  workflow front-loads a hero run in such a way that, if an earthquake were to
  occur, the risk of tsunami could be calculated in less than a second using
  only modest compute resources and the precomputed core.
</p>
<p>
  The authors eschewed methods that generated tons of FLOPS in favor of methods
  that were less FLOPS-efficient but got to the answer faster. In the authors'
  own words:
</p>
<blockquote>
<p>
    As shown in Fig.¬†<a href="https://arxiv.org/html/2504.16344v2#S7.F7">7</a>, higher FLOP/s does not necessarily lead to faster time-to-solution. On
    MI300A nodes of¬†<em>El Capitan</em>, the best-performing
    implementation, Fused PA, achieves a lower percentage (5.2%) of theoretical
    peak FLOP/s than Fused MF (5.5%) but is faster.
  </p>
</blockquote>
<p>
  Interestingly, the hero computation here was embarrassingly parallel(ish) as
  well; in their demonstration run, the hero run (Phase 1) was broken into 621
  independent calculations each requiring 128 nodes (512 A100 GPUs) for about an
  hour. Because they are independent, these tasks could be parallelized across
  multiple HPC centers as well, and my understanding of the data volumes
  involved are modest; Phase 1 would require a single shared copy of the input
  mesh (a hundred GiB?) per HPC center, and each of the 621 tasks would output
  around 8 GiB which would have to be copied back.
</p>
<p>
  While I don't understand the mathematics behind this work, the paper took what
  would've been a huge exascale-class mathematical problem ("10 years on a
  sustained 1 EFLOP/s machine") and reformulated it into a workflow that solves
  the problem faster and more usefully. Instead of brute-forcing the problem
  with a big supercomputer, they split it into separate offline and online
  parts, and this naturally allowed the most computationally expensive part to
  be geographically distributable.
</p>
<p>
  This work surrendered the need for a single big machine, and it didn't produce
  a big-number result. But it did win the Gordon Bell Prize, again signaling
  that the HPC community is beginning to look beyond performance-only and think
  about awarding innovation according to outcomes, not just FLOPS.
</p>
<p>
  The talk for this paper can be viewed
  <a href="https://sc25.conference-program.com/presentation/?id=gb106&amp;sess=sess577">here in the SC25 Digital Experience</a>.
</p>
<h4 id="fixing-problems-caused-by-the-big-number">
  Fixing problems caused by the big number
</h4>
<p>
  Most of my perception around the HPC community beginning to de-emphasize the
  singular big machine or big number arose from organic interactions I had with
  colleagues and customers though. It's hard to summarize how these
  conversations went, but the
  <a href="https://sc25.conference-program.com/presentation/?id=bof197&amp;sess=sess439">Lustre Community BoF</a>
  is a good example of what I saw elsewhere.
</p>
<p>
  Lustre has long been the gold standard in high-performance parallel I/O in the
  HPC community because it was designed from day one to deliver high bandwidth
  above all else. As a result, Lustre already has the big number solved in many
  ways, and events like the Lustre BOF are a great case study in what it looks
  like for a performance-first technology to be pushed into adapting to deliver
  more than just a big number.
</p>
<p>
  First, the ever-innovative St√©phane Thiell from Stanford discussed the process
  and tooling he developed to enable online capacity expansion of a Lustre file
  system. The basis for it was a distributed, fault-tolerant tool he developed
  that uses redis, lfs find, and lfs migrate to manage the state of file
  migrations across Lustre servers as the file system is rebalanced. While a
  part of me thought this was a great tool that would be super helpful for many
  others, another part of me was kind of horrified.
</p>
<p>
  Maybe I've been spoiled by working in hyperscale and AI these past three
  years, but online capacity expansion and rebalancing is a built-in capability
  of all distributed storage systems these days. All the major cloud object
  stores do this, as do all modern parallel file systems including Quobyte,
  VAST, and WEKA. Of course, none of these modern systems are as efficient (on a
  per-CPU core or per-SSD basis) as Lustre at delivering peak performance. But
  St√©phane's talk made me realize the price that's paid for this great
  performance.
</p>
<p>
  Andreas Dilger and others went on to talk about Lustre futures, and as they
  were speaking, I noticed that nobody was talking about performance
  improvements to Lustre. Rather, feature development was focused on catching up
  in every other dimension--data governance, reliability, manageability, and
  others. For example, Andreas talked a bit about the upcoming "multi-tenancy"
  features coming to Lustre:
</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">It's a lot of work to retrofit multitenancy into a performance-first file system.</figcaption>
</figure>
</div>

<p>I put ‚Äúmulti-tenancy‚Äù in quotes because these changes really represent
trying to back into a security posture that is fundamentally different from the
one that Lustre was designed around. In the pursuit of performance, Lustre (as
with most other HPC technologies) was designed assuming that security was
someone else‚Äôs problem. By the time someone could log into a system that could
mount a Lustre file system, they had already been authenticated, and it was up
to the OS on each compute node to authorize any interactions with Lustre itself.
This is the ‚Äúimplicit trust‚Äù model.</p>
<p></p>
<p>
  The problem, of course, is that the rest of the world has adopted a "zero
  trust" model which makes many things (except performance!) generally easier.
  Compliance is easier when the system assumes that everything is encrypted as a
  default and key management can be delegated to a third party. Because Lustre
  didn't do this from the outset, it is going through this process of
  retrofitting encryption in various places and using a mixture of nodemaps,
  UID/GID maps, and shared secrets to patch over all the places where trust was
  fundamentally implicit.
</p>
<p>
  Later on in the BOF, panelists acknowledged (some half-heartedly) that
  manageability of Lustre was a barrier. One panelist admitted that it took five
  years of work to almost get to the point where a Lustre update can be done
  without crashing applications. Another panelist said that multitenancy in
  Lustre is easy <em>if you follow a million steps</em>, and that his company
  was developing script-based ways to simplify this. While the idea of using
  scripts to simplify operations is not bad, from a secure supply chain
  standpoint, relying on third-party bash scripts to enable features required
  for legal compliance is horrifying.
</p>
<p>
  I don't mean to pick on Lustre alone here; other HPC technologies such as
  InfiniBand, Slurm, and DAOS are facing the same reality: retrofitting modern
  requirements like security and manageability into architectures that
  prioritized performance and scalability over everything else are now going
  through similar contortions to meet modern requirements around data
  governance. For those HPC centers who do not have to worry about compliance
  (which is most of open-science computing), these technologies will continue to
  be just fine.
</p>
<p>
  However, the
  <a href="https://blocksandfiles.com/2025/11/18/vast-data-dell-versity-and-spectra-logic-are-shining-storage-stars-on-taccs-horizon/">successes of these modern file systems</a>
<a href="https://blocksandfiles.com/2025/07/04/vast-doudna-supercomputer-storage/">across leading HPC centers</a>
  and the proliferation of alternative technologies such as
  <a href="https://nrp.ai">Kubernetes-based HPC</a> and
  <a href="https://blogs.microsoft.com/blog/2025/11/12/infinite-scale-the-architecture-behind-the-azure-ai-superfactory/?utm_source=chatgpt.com">MRC over Ethernet</a>
  tells me that HPC coming around to the idea that marginal increases in
  performance are no longer worth missing out on factors that weigh heavily on
  day-to-day operations like manageability, reliability, and flexibility.
</p>
<h3 id="theme-2-hpc-policy-is-becoming-ai-policy">
  Theme 2: HPC policy is becoming AI policy
</h3>
<p>
  Some of the biggest news at SC was not actually showcased at the conference
  despite being what many people wanted to talk about in side conversations: HPC
  policy is rapidly becoming AI policy, resulting in a slew of huge (but poorly
  defined) "public-private partnerships."
</p>
<p>
  As a bit of background, the Oak Ridge Leadership Computing facility announced
  its next system, Discovery, in late October--this was the result of a
  "typical" supercomputer procurement process that
  <a href="https://www.nextplatform.com/2023/10/02/the-first-peeks-at-the-doe-post-exascale-supercomputers/">first came into the public eye in 2023</a>. However, the Discovery announcement also included mention of a smaller
  system, Lux, which will "<a href="https://www.olcf.ornl.gov/2025/10/27/ornl-amd-and-hpe-to-deliver-does-newest-ai-supercomputers-discovery-and-lux/">leverage the Oracle Cloud Infrastructure (OCI)</a>" (whatever that means) to provide earlier access to AMD MI355X GPUs ahead of
  Discovery's full-scale deployment.
</p>
<p>
  Then, two days later, Argonne National Laboratory announced a
  <a href="https://www.energy.gov/articles/energy-department-announces-new-partnership-nvidia-and-oracle-build-largest-doe-ai">similar arrangement with Oracle Cloud and NVIDIA</a>
  to deliver a small (Lux-sized) GPU supercomputer named Equinox, followed by a
  much-larger 100,000-GPU supercomputer named Solstice. Neither Equinox nor
  Solstice are attached to a "typical" supercomputer procurement; the follow-on
  to Aurora, to be named
  <a href="https://intro-hpc-bootcamp.alcf.anl.gov/sites/hpc/files/2025-09/WelcomeToHPC_Papka.pdf">Helios</a>, is
  <a href="https://www.alcf.anl.gov/draft-technical-requirements-alcf-4-system">still in planning</a>
  and will be deployed in 2028. This strongly suggests that, whatever
  "public-private partnership" means to the DOE, it is not the same as the
  typical leadership computing systems; it is its own AI-centric program.
</p>
<p>
  At SC itself, Evangelos Floros (EuroHPC's head of infrastructure) also
  mentioned the "need for public-private partnerships" to realize EuroHPC's goal
  of building "AI Gigafactories" with "100,000 advanced AI processors" across
  Europe.
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">"Need for public-private partnerships" to fund AI factories is recognized by EuroHPC too.</figcaption>
</figure>
</div>
<p>Again, what exactly this "public-private partnership" model entails in Europe
  was never really defined.
</p>
<p>
  What was clear is that both American and European efforts are declaring the
  need to build massive (100K+ GPU) supercomputers for AI, the traditional HPC
  centers will be the public stewards of them, and "public-private partnerships"
  are the only way to realize them since governments alone cannot foot the bill.
</p>
<p>
  The Top500 BOF also included a short, awkward talk by Rick Stevens titled "The
  DOE AI Initiatives" that amounted to Stevens saying he had nothing to say.
  What really happened, I suspect, is that DOE's new "<a href="https://genesis.energy.gov">Genesis Mission</a>," which was announced the week <em>after</em> the SC conference, was a week
  late and therefore couldn't be discussed as originally planned. If Stevens had
  been able to describe the Genesis Mission, though, I'm sure he would've also
  described "public-private partnership" as a key aspect, since the same
  language is used in the
  <a href="https://www.whitehouse.gov/presidential-actions/2025/11/launching-the-genesis-mission/">Executive Order that established Genesis</a>. And I'm sure his description would've been no clearer about what this
  really means than what EuroHPC or the OCI/DOE descriptions have stated.
</p>
<p>
  Most revealing was my observation that, even outside of the proper conference
  program, nobody really knew what any of this meant. I talked to plenty of my
  colleagues from both government HPC and hyperscale cloud organizations, and
  the only consistent message was that there aren't many concrete facts backing
  up the the press releases right now. It appears that these partnerships were
  brokered far outside the usual channels that large supercomputer procurements
  are normally done, and the people in charge of actually delivering on the
  promises of the press releases are still figuring out what is possible.
</p>
<p>
  Connecting the dots between Lux/Equinox/Solstice, Genesis, and a recent
  <a href="https://www.energy.gov/sites/default/files/2025-04/RFI%20to%20Inform%20Public%20Bids%20to%20Construct%20AI%20Infrastructure%20%28website%20copy%29.pdf">RFI</a>
  and
  <a href="https://sam.gov/workspace/contract/opp/7864e8f4d61f42dc811ba095a41c8368/view">RFP</a>
  from DOE to allow
  <a href="https://www.energy.gov/articles/doe-announces-site-selection-ai-data-center-and-energy-infrastructure-development-federal">hyperscalers to build AI factories on federal land</a>, it appears that what is happening is...
</p>
<ul>
<li>
    The DOE has a bunch of land that is adjacent to the National Labs that is
    undeveloped but has the infrastructure to support massive AI factories.
    Specifically named is a 110-acre parcel at Argonne that can accommodate up
    to 1 GW "AI data park," and a 100-acre parcel at Oak Ridge with up to 800
    MW. These details were disclosed in
    <a href="https://www.energy.gov/sites/default/files/2025-04/RFI%20to%20Inform%20Public%20Bids%20to%20Construct%20AI%20Infrastructure%20%28website%20copy%29.pdf">an RFI they issued earlier in the spring</a>.
  </li>
<li>
    The
    <a href="https://www.energy.gov/articles/energy-department-announces-new-partnership-nvidia-and-oracle-build-largest-doe-ai">Solstice press release</a>
    specifically said that DOE envisions "shared investments and shared
    computing power between government and industry." Given the RFI/RFP were
    about land leases, these public-private partnerships may involve splitting
    the costs of space/power/cooling (the land and infrastructure being leased)
    and the capital/operations (the supercomputer cloud services being built)
    between the Labs and Oracle.
  </li>
</ul>
<p>
  A potential model for operations is that cloud providers are allowed to build
  and operate commercial AI cloud services adjacent to the DOE HPC facilities in
  exchange for the DOE Genesis Mission being entitled to some of those AI cloud
  capabilities. Exactly how much supercomputing resources hyperscalers like OCI
  would give to DOE, and exactly how much it would cost the DOE Labs to serve as
  landlords, is probably still undefined. But seeing as how power is the single
  biggest limiter in AI these days, I expect this model will only spread costs
  around, not actually lower them.
</p>
<p>
  If this is indeed how Genesis plays out, this would establish a bizarre new
  way for the government to acquire HPC (or AI) capabilities that completely
  sidesteps the standard procurement model. Instead of plunking down a hundred
  million dollars a year to finance a new leadership supercomputer, we might be
  moving into a world where the Labs plunk down a hundred million dollars a year
  to cover the costs of power, space, and cooling for a cloud provider. And
  instead of owning a leadership supercomputer, these national HPC facilities
  wind up consuming HPC (well, AI) resources from cloud providers--hopefully at
  a cost that reflects the fact that the cloud providers are also profiting from
  cycles being sold off of these machines to commercial AI customers.
</p>
<p>
  But again, this is all speculation based on the consistencies I heard
  throughout the conference and the experience I had trying to build these sorts
  of partnership with the HPC community while I worked at Microsoft. I may be
  right, or I may be wildly wrong. There are probably only a handful of people
  in the world with a clear idea of what these partnerships are meant to look
  like right now, and they are all way above the heads of the people at the HPC
  centers who will be tasked with executing on the vision.
</p>
<p>
  Selfishly, I am also left with a bit of heartburn over all of this news. I put
  a lot of personal time and energy into giving the HPC community the
  information it needed to feel comfortable about partnering with hyperscale AI
  infrastructure providers while I was at Microsoft, and it often felt like a
  Sisyphean task. Within months of me giving up and moving on from my career at
  a cloud provider, seeing a complete reversal of policy from the leadership HPC
  folks--and to see the "other guy" in pole position--is a bit of a slap in the
  face.
</p>
<p>
  I also couldn't help but notice that the cloud provider in all the headlines
  in the US didn't seem to demonstrate a very strong and unified presence at SC
  this year. Comically, they didn't even use their own brand's colors for their
  booth on the exhibit floor. And the color scheme they did use left no room for
  Oak Ridge's Lux system, which will be AMD-based, to be showcased.
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">Oracle's booth at SC25. Their brand color is red, not green. Or so I thought.</figcaption>
</figure>
</div>
<p>Though I may have read too much into this, it feels like these public-private
  partnerships are not necessarily composed of equal partners with equal levels
  of commitment.
</p>
<p>
  More broadly, I left the conference concerned that the discourse happening
  around these cloud-HPC/AI integrations--at least in the US--appears to have
  regressed compared to where it was when I worked at Microsoft. Many of the
  things we had to figure out years ago (cybersecurity models, impacts on jobs
  at the HPC centers) seem to have reset to zero. And sidestepping the
  procurement processes for leadership computing to enable these public-private
  partnerships will either require significant new funding (of which Genesis
  provides none; the executive order as-written appears to recolor existing
  money) or robbing Peter (the budget funding the next generation of leadership
  HPCs) to pay Paul (the cloud providers serving up compute resources for AI).
  As a result, I can envision a future where all of the money that used to fund
  leadership computing for science becomes money to fund commercial AI
  factories, resulting in a slow evaporation of the LCFs as their HPC
  capabilities shrink in size and relevance.
</p>
<p>
  Though there's lots more to be said on this topic, it's all based on
  conjecture. So, maybe the best thing to do is quietly wait and see.
</p>
<h3 id="theme-3-ai-discourse-is-growing-up">
  Theme 3: AI discourse is growing up
</h3>
<p>
  This was the first SC where it felt like the discourse around AI's role in the
  future of scientific computing actually carried some substance. Whereas
  previous years saw talk that mostly revolved around basic ideas like "do LLMs
  hallucinate too much?" or "can ChatGPT write MPI code?," I sat in on a number
  of interesting talks and conversations that skipped the question of "is AI
  useful?" and went straight to "this is how AI is proving useful to us."
</p>
<p>
  Maybe it's related to the previous theme: HPC money is becoming AI money, so
  AI research is becoming required to stay afloat. Or maybe it's because 2025
  has been the year of agentic AI, and agents allow LLMs to be integrated much
  more surgically into complex workflows. Or maybe confirmation bias led me to
  sit in sessions and talk with people who are at the frontier of applying AI to
  scientific discovery. Whatever the case may be, I was glad to hear so much
  discussion from researchers around the importance of all the connective tissue
  required to operationalize AI in scientific computing.
</p>
<h4 id="agentic-workflows">Agentic workflows</h4>
<p>
  A great example of this was the
  <a href="https://aiexscale.github.io">1st International Symposium on Artificial Intelligence and Extreme-Scale
    Workflows</a>, which happened on Friday. One of the invited speakers, Dr. Katrin Heitmann,
  connected a lot of dots in my head with a talk she gave on how massive-scale,
  physics-based simulation workflows can benefit from agentic AI.
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">Heitmann's vision on how agentic approaches can augment (but not replace) humans in complex scientific workflows.</figcaption>
</figure>
</div>
<p>The crux of the challenge faced by most massive-scale simulation (like
  <a href="https://cpac.hep.anl.gov/projects/hacc/">HACC</a>, the cosmology code
  for which she is famous) is that they generate massive amounts of data. The
  <a href="https://www.anl.gov/cels/article/simulating-the-cosmos-frontiere-sets-new-record-with-trillionparticle-universe-model">most recent HACC run</a>
  generated hundreds of terabytes of compressed data per checkpoint and over a
  hundred petabytes of data in the end; this cosmological simulation serves as a
  reference dataset from which downstream cosmological research can draw when
  exploring targeted questions. The challenge, of course, is finding relevant
  pieces of the simulated universe from amidst a hundred petabytes of raw data.
</p>
<p>
  Dr. Heitmann's premise is that agents and tools have very specific scopes and
  capabilities, and researchers have control over which of these tools they wish
  to use. However, they can hand off these tools to an agentic workflow to let
  it autonomously sift through all of the data, looking for specific features
  within the simulated universe that are relevant. A specific example she gave
  was the process of examining 500 million galaxy clusters; with an agentic,
  AI-driven approach, a postdoc was able to interactively sift through these
  objects without examining each one individually. For truly interesting
  objects, a separate agent could go search the literature and provide an
  explanation as to why it may be interesting, absolving the postdoc from having
  to make round trips between the dataset and external literature.
</p>
<p>
  That all said, it was clear from this talk (and others) that integrating
  agentic AI into scientific inquiry is still in its early days. But what I
  appreciated about this talk (and the entire workshop) is that it sidestepped
  pedestrian questions about trustworthiness by acknowledging that the goal
  isn't full autonomy, but rather, enabling researchers to do things faster.
  There is still a human at the start and the end of the workflow just as there
  always has been, but agents can reduce the number of times a human must be in
  the loop.
</p>
<h4 id="data-and-agentcentric-service-infrastructure">
  Data and agent-centric service infrastructure
</h4>
<p>
  Even when AI wasn't the main topic of discussion, it was clear to me at this
  SC that AI is influencing the way researchers are thinking about the
  infrastructure surrounding supercomputers. A great example of this was the
  keynote at the <a href="https://www.pdsw.org/index.shtml">PDSW workshop</a>,
  given by the ever-insightful
  <a href="https://sc25.conference-program.com/presentation/?id=misc185&amp;sess=sess202">Dr. Rob Ross, where he offered a retrospective on the work his team has
    done over the last two decades</a>, what he felt they got right, what they missed, and what's ahead.
</p>
<p>
  Towards the end of his presentation, he made the case that "science is
  increasingly multi-modal." But rather than talk about multimodality in the AI
  sense, he was emphasizing that there's more to scientific computing than
  performance:
</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">Domain science, provenance, search, and resilience are equal partners to performance in scientific computing.</figcaption>
</figure>
</div>

<p>Taken at face value, this slide positions performance on equal footing
with domain science, provenance, findability, and his argument was that we‚Äôve
moved beyond the world where the only storage problem that HPC faces is
checkpointing. Just as Dr. Heitmann would say on Friday, Dr. Ross‚Äô argument was
that the increasing volume of scientific data coming out of both exascale
simulation and scientific instruments is driving the field towards more
automation. And with automation comes a greater need to understand data
provenance‚Äìafter all, if automation produces a surprising result, a human
ultimately has to go back and understand exactly how the automation generated
that result.</p>
<p></p>
<p>
  He also point out that in this coming world of automation-by-necessity,
  infrastructure itself might have to be rethought. After all, traditional
  technologies like parallel file systems were designed to make the lives of
  human researchers easier; when the primary consumer of data becomes AI agents,
  not humans, there may be better ways to organize and expose data than through
  files and directories. A human might repeatedly cd and ls to find a specific
  dataset on a file system, whereas an agent use a query a flat index to find
  the same data in a single step.
</p>
<p>
  At the end of the same PDSW workshop, I was fortunate enough to contribute to
  <a href="https://sc25.conference-program.com/presentation/?id=miscp112&amp;sess=sess202">a panel</a>
  where many of these same themes--how will data systems change as AI plays a
  greater role in scientific discovery--were discussed. Although we touched on a
  lot of topics, what stuck with me was a general acknowledgment that, while HPC
  has always talked about data management and provenance as being important,
  they were always treated as a "nice to have" rather than a "must have."
  However, as was echoed across many presentations (including the two I
  described above), governance and provenance are now becoming non-negotiable as
  larger datasets drive us towards AI-driven automation.
</p>
<p>
  Regardless of what you think about AI's ability to accelerate scientific
  discovery, I left SC with the feeling that AI is forcing the HPC community to
  grow up with regards to how seriously it takes data management:
</p>
<ul>
<li>
    The size and velocity of datasets generated by simulation or experiment is
    growing beyond any single person's ability to analyze it by hand. The
    complexity of these data are also making it harder to develop
    herustics-based or analytical approaches to combing through all of it.
  </li>
<li>
    The best path forward to understanding these data is through AI (via
    purpose-built models for analysis) or AI-driven data exploration (via
    autonomous, agentic workflows).
  </li>
<li>
    Automation or autonomous workflows will always act under authority delegated
    to them by human researchers, meaning there is a growing need to be able to
    inspect how these workflows arrived at the conclusions they generate.
  </li>
<li>
    Understanding how an answer was achieved requires significantly better data
    management features such as governance, provenance, and auditability. A
    result is ultimately only useful if a human can trust it, and that trust
    comes from understanding which data informed that conclusion, how that data
    was created, and how it was modified over time.
  </li>
</ul>
<p>
  Put differently, checkpointing was the main concern of I/O research because
  I/O performance was the first scalability issue that scientific computing ran
  into as supercomputers and scientific instruments got bigger. However, we're
  now at a point where issues ancillary to performance have reached the limits
  of scalability. Dr. Ross's multi-modal slide indicate that provenance,
  indices/search, and resilience are some examples of these new barriers, but
  there are plenty more as well.
</p>
<p>
  In a sense, this theme is the opposite side of the same coin as the first
  theme I discussed--that the big number is losing its shine. The hardest
  questions going forward aren't the obvious ones about scaling performance;
  they are about scaling everything else to keep up. AI seems to be the
  technology that has cleared a path to these data management hurdles, but the
  benefits of adopting strong data management practices and systems will extend
  far beyond the reach of just enabling AI-based automation.
</p>
<h2 id="the-exhibit-hall">The exhibit hall</h2>
<p>
  The exhibit hall has long been one of my favorite parts of attending SC
  because it's a great way to get a feeling for what technologies and vendors
  are hot, where the innovation is trending, and what sorts of commercial
  problems are worth solving. Every year I feel like I have less and less time
  to walk the exhibit hall though, and the layout and composition of this year's
  exhibition meant I only saw a small fraction of what I wanted to see in the
  few days it was up.
</p>
<p>
  The most common comment I heard about the exhibit this year is captured in
  Doug Eadline's article,
  <a href="https://www.hpcwire.com/2025/11/26/sc25-observations-more-pumps-than-processors/">SC25 Observations: More Pumps than Processors</a>
  (which is well worth the read!). The same commentary was repeated throughout
  the OCP conference in October as well, suggesting that there is a lot of money
  to be made (or at least the prospect of money) in helping datacenters get
  outfitted for the liquid cooling demanded by the next generation of
  large-scale GPU infrastructure. However, I found the overwhelming amount of
  space devoted to liquid cooling companies acutely problematic at SC25 this
  year for two reasons:
</p>
<ol type="1">
<li>
<strong>Most SC attendees have nothing to do with liquid cooling</strong>. A
    colleague of mine who operates supercomputers for the energy sector asked
    one of these big liquid cooling vendors what he could do to actually engage
    with them. After all, he doesn't buy liquid cooling infrastructure; he buys
    whole supercomputers that come with heat exchangers and CDUs that are
    integrated into the solution. The vendor had no good answer, because the
    reality is that the typical supercomputer user or buyer has no say over what
    piping, coolant, or exchangers are used inside the machine itself. The whole
    point of buying an integrated supercomputer is to not have to deal with that
    level of details.
  </li>
<li>
<strong>These liquid cooling vendors soaked up a ton of floor space</strong>. A few of these physical infrastructure providers had massive (50x50)
    booths sprinkled across the exhibit hall. Combined with the fact that the
    average SC attendee has nothing to do with liquid cooling meant that the
    booths that were more likely to be relevant to a typical attendee were much
    further apart than they had to be.
  </li>
</ol>
<p>
  The end result was that the exhibit hall was absolutely gargantuan and yet
  information-sparse. In fact, this year saw a secondary exhibit hall in the old
  football stadium serve as overflow space, because the entire primary exhibit
  hall was full. What's worse is that this overflow space was (as best as I
  could tell) completely disconnected from the main hall, and the only time I
  ever saw it was from the dining area used to serve lunch for the tutorials.
</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">The exhibit hall's overflow space being set up in the former football stadium.</figcaption>
</figure>
</div>

<p>I would‚Äôve been furious if if I had been stuck with a booth in this
overflow space, because I can‚Äôt imagine the foot traffic in there was very high.
I personally couldn‚Äôt even find the entrance to this second exhibition area in
the few hours I had to look for it.</p>
<p></p>
<p>
  I can't help but think the SC organizers leaned far too much into booking up
  as much space (and therefore exhibitor dollars) as possible without thinking
  about the dilutive effects of having such a massive vendor count. Some vendors
  definitely benefitted from having a good location near one of the hall
  entrances, but I also heard a nontrivial amount of grumbling around how little
  traffic there was at some of the big booths. It wouldn't surprise me if there
  was a contraction of the HPC mainstays at SC26.
</p>
<h3 id="by-the-numbers">By the numbers</h3>
<p>
  Rather than rely solely on anecdotes though, it's also fun to take a
  quantitative look at the changes in exhibitors relative to last year. Since I
  spent the time figuring out how to generate tree maps for my SC24 recap last
  year, I figured I should re-run the same analysis to compare SC25 to SC24.
</p>
<p>
  Of the biggest booths who were exhibiting for the first time this year, it
  should be no surprise that the two biggest new entrants were Danfoss (liquid
  cooling infrastructure) and Mitsubishi Heavy Industries (gas turbines and
  other large-scale infrastructure):
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>
 <figcaption class="image-caption">New exhibitors with the largest booths.</figcaption></figure>
</div>
<p>Of the other top new exhibitors, some (Solidigm, Sandisk, C-DAC, MinIO, and
  University of Missouri Quantum Innovation Center) were quite relevant to the
  typical SC attendee. Arm was also back after having skipped SC24. But there
  were scores of new exhibitors whose services and products seem much more
  relevant to very niche aspects of physical datacenter infrastructure.
</p>
<p>
  Of the exhibitors who didn't show up to SC25 but had big booths at SC24, there
  was a diverse mix of markets:
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">Vendors who didn't show up to SC'25 but had big booths at SC'24.</figcaption>
</figure>
</div>
<p>Sadly, higher ed and government popped up on this list (see
  <a href="https://www.hpcwire.com/2025/11/26/sc25-observations-more-pumps-than-processors/">Doug Eadline's take on this for more</a>). A bunch of datacenter infrastructure providers also vanished, including
  Valvoline and Boundary Electric; this suggests that some of the top new
  vendors of this year (Danfoss, Mitsubishi) may similarly vanish entirely next
  year after realizing that SC isn't really their crowd. But I was also
  surprised to see some big names in AI vanish; Iris Energy (IREN) is a GPU
  cloud provider that just inked a multi-billion dollar deal with Microsoft;
  Ingrasys manufactures much of the world's GB200 NVL72 infrastructure; Groq,
  Sambanova, and SambaNova also inexplicably vanished.
</p>
<p>
  Perhaps more interesting are the top growers; these vendors exhibited both
  last year and this year, but went significantly larger on their booth sizes:
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">Biggest increases in booth size at SC'25 vs. SC'24.</figcaption>
</figure>
</div>
<p>Legrand, which provides datacenter infrastructure bits, likely grew as a
  result of it acquiring USystems and merging USystems' booth with Legrand's
  booth this year. The other big booth expansions are mostly household names
  though; Gates, EBARA, and GRC are cooling vendors that the typical SC attendee
  can't do much with, but the others are organizations with whom a researcher or
  HPC datacenter operator might actually talk to.
</p>
<p>
  Finally, the top contractions in booth space are a mix of service providers,
  HPC facilities or research centers, and component suppliers:
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">Biggest decreases in booth size at SC'25 vs. SC'24.</figcaption>
</figure>
</div>
<p>Of the biggest vendors who downsized, Carahsoft is a component reseller and
  service provider, Stulz is a liquid cooling company, HLRS is a German
  supercomputer center, and Viridien is an HPC services company that primarily
  serves the energy sector. It is surprising to see AWS shrink while Microsoft
  grew, and it is doubly surprising to see Oracle shrink when it's at the center
  of the biggest HPC deployment news of the season. Given that these booth sizes
  are chosen a year in advance, this may speak to how unexpected the turn of
  events were that resulted in Oracle carrying the cloud services end of DOE's
  big public-private partnerships.
</p>
<h3 id="interesting-new-technology">Interesting new technology</h3>
<p>
  For reasons I'll discuss later, I didn't have much time to walk the exhibit
  hall floor. Combined with the fact that everything was so spread out and
  diffuse, I just didn't get a great sense of what interesting new technology
  was being introduced this year beyond what tended to stick out. And amidst all
  the giant CDUs and liquid cooling infrastructure, it was hard for anything to
  stick out except really big compute cabinets.
</p>
<h4 id="dell-ir700">Dell IR700</h4>
<p>
  Dell's booth had a fully loaded IR7000 rack on display (as they did at
  <a href="https://blog.glennklockwood.com/2025/03/gtc-2025-recap.html#dells-480-kw-ir7000">GTC earlier in the year</a>) with 36 GB200 NVL4 sleds. At 50OU high (almost eight feet tall), this thing
  is physically huge:
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">Dell's 50OU IR7000 rack, fully loaded. This is what TACC Horizon will be built from.</figcaption>
</figure>
</div>
<p>Unlike the version they had on display at GTC though, this one had both the
  front door and a full rear-door heat exchanger installed:
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">HUGE rear-door heat exchanger on the back of the Dell IR7000 rack.</figcaption>
</figure>
</div>
<p>What's notable about this platform is that we now know that it is the basis
  for both
  <a href="https://tacc.utexas.edu/systems/horizon/">TACC's upcoming Horizon system</a>
  (which will have
  <a href="https://glennklockwood.com/garden/systems/Horizon">28 of these fully loaded racks</a>) and
  <a href="https://www.nersc.gov/what-we-do/computing-for-science/doudna-system">NERSC's upcoming Doudna system</a>
  (which will have Vera Rubin rather than Blackwell). This rack was nominally
  designed for hyperscale AI and is the basis for Dell's GB200 NVL72 (XE9712)
  deployments at places like CoreWeave and xAI, which means that it'll be
  thoroughly tested at scale long before TACC or NERSC have it up and running.
  This is the opposite of what has historically happened: before AI, it was
  usually government HPC that had to debug new rack-scale architectures before
  industry would touch it.
</p>
<h4 id="hpe-cray-gx5000">HPE Cray GX5000</h4>
<p>
  However, government HPC will still have a chance to debug a new supercomputing
  platform in the recently announced
  <a href="https://glennklockwood.com/garden/Cray-GX">Cray GX</a> (formally
  called "the HPE Cray Supercomputing GX platform"), which is the successor to
  the current Cray EX platform. This is the platform that the
  <a href="https://glennklockwood.com/garden/systems/Discovery">Discovery supercomputer</a>
  at OLCF will use, and HPE had a CPU-only blade (<a href="https://glennklockwood.com/garden/nodes/Cray-GX250">Cray GX250</a>) and a rack mockup on display at SC:
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">HPE's new GX blade form factor. This one appears to be the GX250, the 8-socket CPU-only blade.</figcaption>
</figure>
</div>
<p>It's hard to tell the size of this blade from the photo, but if you look at
  the relative size of the CPU socket and the DIMM slots, you can get a sense of
  how physically massive it is--it's like a coffee table. It also isn't
  perfectly rectangular; Cray decided to put this unusual protrusion on the
  front of the blades which is where the four NICs and eight E1.S SSDs are
  housed:
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">A look at the side of the Cray GX blade's "nose" showing the side-mounted NIC ports.</figcaption>
</figure>
</div>
<p>This nose(?) adds more surface area to the front of the rack, and it makes
  more sense when you see a rack full of these nodes. HPE had a full GX5000 rack
  with mocked-up cardboard nodes in their booth as well:
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">Fully loaded GX5000 rack. The nodes were cardboard, but pretty nice cardboard.</figcaption>
</figure>
</div>
<p>By having the NIC ports (which are Slingshot 400) face the sides of the rack
  rather than stick out the front, the bend radius of all that copper doesn't
  have to be quite as dramatic to route it along the sides of these node noses.
  And unlike previous Cray designs, there's also no midplane or backplane that
  connect the nodes in a rack to the rack-local switches; everything connects
  through discrete copper or optical cables.
</p>
<p>
  At the center of the rack is a liquid-cooled switch chassis, and each rack can
  support either 8-, 16-, or 32-switch configurations. Each switch is a 64-port
  Slingshot 400 switch, and I think the premise is that a single GX5000 rack is
  always exactly one dragonfly group. If you want a smaller group, you use a
  switch chassis with fewer switches.
</p>
<p>
  Interestingly, this GX will also support non-Slingshot Ethernet and XDR
  InfiniBand switches. Given that both XDR InfiniBand and 800G Ethernet are
  shipping today and have twice the bandwidth that Slingshot 400 will have when
  it starts shipping in a year, perhaps the Slingshot 400 option is just a
  stopgap until HPE's investments in Ultra Ethernet result in a product. The
  lack of a network backplane in the rack also makes it easier for the rack to
  accommodate the non-dragonfly topologies that would be required for InfiniBand
  or Ethernet.
</p>
<p>
  The rear of the rack is remarkably unremarkable in that it simply contains a
  rear bus bar and the liquid cooling manifolds and mates. In this sense, the
  rack looks very
  <a href="https://www.opencompute.org/documents/open-rack-base-specification-version-3-pdf">OCP-like</a>; the boring stuff is in the back, everything exciting is serviced from the
  front, and the rack itself is passive plumbing. Like any OCP ORv3 rack, power
  shelves slot in just as server blades do, and they use the same liquid cooling
  infrastructure as the rest of the rack. They power the bus bar, and the blades
  and switches draw from the same bus bar.
</p>
<p>
  Compared to an ORv3 rack though, these GX racks are wider and shorter. The
  width probably offers more flexibility for future NVIDIA or AMD GPU boards,
  but I was surprised that Cray didn't go ultra tall like Dell's 50OU IR7000. I
  was also surprised to hear that Cray is launching GX with a 400 kW cabinet
  design; power appears to already be a limiting factor in the nodes launching
  with GX. A single 400 kW GX rack can support
</p>
<ul>
<li>40 CPU-only blades (81,920 cores of Venice)</li>
<li>28 AMD Venice+MI430X blades (112 GPUs)</li>
<li>24 NVIDIA Vera+Rubin blades (192 GPUs)</li>
</ul>
<p>
  For reference, the demo GX5000 rack pictured above had only 29 blades and 16
  switches. I assume that fitting 40 blades into the rack requires using the
  smallest dragonfly group possible.
</p>
<p>
  On the cooling front, the GX5000 rack will launch with support for the same
  1.6 MW CDUs as the current Cray EX platform. I heard talk of a neat sidecar
  CDU option as well, but the person with whom I spoke at the HPE booth said
  that would come a little later.
</p>
<p>
  Overall, I was surprised by how un-exotic the new Cray GX platform is compared
  to what the AI world has been doing with ORv3 racks. The fact that Cray and
  Dell's designs are more similar than different suggests that the HPC/AI world
  is converging on a place where the future is uncertain, and flexibility is
  more important that highly engineered racks that optimize for very specific
  nodes and networks. It also suggests that the real value of buying Cray is
  higher up the stack; liquid cooling, power delivery, and rack integration is
  becoming commoditized thanks to AI.
</p>
<p>
  I was also surprised that Cray's next-generation design is not obviously
  superior to what the hyperscale community is designing. Whereas the GX rack
  caps out at 400 kW, Dell's will allegedly scale up to 480 kW. That said,
  today's IR7000 racks shipping for Horizon are only 215 kW (for GPU racks) and
  100 kW (for CPU-only racks) according to a talk given by Dan Stanzione:
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">The physical configuration of TACC's upcoming Horizon supercomputer.</figcaption>
</figure>
</div>
<p>So until the final specifications for the Rubin GPU are released, I suspect we
  won't know whether Cray still leads the pack in terms of compute density, or
  if Dell made the better bet by aligning its supercomputing platform on a
  standard OCP rack design.
</p>]]></content><author><name>Glenn K. Lockwood&apos;s Blog</name></author><category term="glennklockwood" /><summary type="html"><![CDATA[The annual SC conference was held last week, drawing over 16,000 registrants and 560 exhibitors to in St. Louis, Missouri to talk about high-performance computing, artificial intelligence, infrastructure, and science. It was my tenth time attending in-person (12th overall), and as is always the case, it was a great week to reconnect with colleagues, hear what people are worrying about, and get a finger on the pulse of the now-rapidly changing HPC industry.]]></summary></entry><entry><title type="html">The trap of prioritizing impact</title><link href="https://hpc.social/personal-blog/2025/the-trap-of-prioritizing-impact/" rel="alternate" type="text/html" title="The trap of prioritizing impact" /><published>2025-09-20T14:46:41-06:00</published><updated>2025-09-20T14:46:41-06:00</updated><id>https://hpc.social/personal-blog/2025/the-trap-of-prioritizing-impact</id><content type="html" xml:base="https://hpc.social/personal-blog/2025/the-trap-of-prioritizing-impact/"><![CDATA[<p>(I wrote this originally as a comment in <a href="https://randsinrepose.com/welcome-to-rands-leadership-slack/">RLS </a>in response to a staff-level engineer who was frustrated at how little they got to code anymore, and it resonated with enough folks that maybe it‚Äôs worth sharing here!)</p>

<p>There‚Äôs a trap I‚Äôve seen a lot of staff+ folks fall into where they over-prioritize the idea that they should always be doing ‚Äúthe right, most effective thing for the company‚Äù. When I see engineers complain that they don‚Äôt get to code enough, I often suspect they‚Äôve fallen prey to this.</p>

<p>I say <strong><em>that‚Äôs a trap</em></strong>! because I see people do this at the expense of their own job satisfaction and growth, which is bad for both them and (eventually) for the company which is likely to lose them.</p>

<p>I don‚Äôt blame people for falling into this trap, it‚Äôs what we‚Äôre rewarded for. I‚Äôve fallen into it! I have stopped doing technical work I cared about, prioritized #impact, and fought fires wherever they arose. I have spent all my time mentoring and teaching and none coding. The result was often grateful colleagues, but also burnout and leaving jobs I otherwise liked.</p>

<p>Whereas when I‚Äôve allowed myself to be like 30% selfish ‚Äî picking some of my work because it was fun and technical, even when doing so was not the ‚Äúmost impactful‚Äù thing I could do ‚Äî I was happier, learned more, and stayed in roles longer.</p>

<p>An example: I worked on a team that was doing capacity planning poorly and was buying too much hardware. (On-prem, physical hardware.) I could have solved the problem with a spreadsheet, but that was boring and made my soul hurt.</p>

<p>What I did instead was dig into how our container scheduling platform worked, and wrote a nifty little CLI tool that would look at the team‚Äôs configured workloads and spit out a capacity requirement calculation. It took about three times as long as the spreadsheet would have, but it was fun and accomplished the same goal and gave me some experience in the container platform. And it wasn‚Äôt that much of a time sink.</p>

<p>Was that better for the company? No idea. I hope it was ‚Äî I hear the tool is still maintained and no one has replaced it with a spreadsheet yet! But that‚Äôs a happy accident.</p>

<p>Was it better for me? Absolutely! It was a bit selfish, but it made an otherwise tedious task more fun and I learned some useful tricks.</p>

<p>So ‚Äî if you wish you had more time to code‚Ä¶ go code a bit more. Don‚Äôt let the idea of being more effective guilt you into giving it up. Your career is your career and you should enjoy it.</p>]]></content><author><name>Thinking Out Loud</name></author><category term="ajdecon" /><summary type="html"><![CDATA[(I wrote this originally as a comment in RLS in response to a staff-level engineer who was frustrated at how little they got to code anymore, and it resonated with enough folks that maybe it‚Äôs worth sharing here!)]]></summary></entry><entry><title type="html">Lessons learned from three years in cloud supercomputing</title><link href="https://hpc.social/personal-blog/2025/lessons-learned-from-three-years-in-cloud-supercomputing/" rel="alternate" type="text/html" title="Lessons learned from three years in cloud supercomputing" /><published>2025-07-11T05:26:00-06:00</published><updated>2025-07-11T05:26:00-06:00</updated><id>https://hpc.social/personal-blog/2025/lessons-learned-from-three-years-in-cloud-supercomputing</id><content type="html" xml:base="https://hpc.social/personal-blog/2025/lessons-learned-from-three-years-in-cloud-supercomputing/"><![CDATA[<p>I recently decided to leave Microsoft after having spent just over three years there, first as a storage product manager, then as a compute engineer. Although I touched many parts of Azure's infrastructure during that time, everything I did was at the intersection of large-scale supercomputing and hyperscale cloud. There was no shortage of interesting systems to figure out and problems to solve, but as I began to wrap my arms around the totality of hyperscale AI training in the cloud, I also began to see the grand challenges that lay ahead.</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">Outside Microsoft's Silicon Valley Campus minutes after I was escorted off the premises.</figcaption>
</figure>
</div>
<p>Although many of those challenges would probably be fun and exciting to tackle, the more I learned, the more I found myself asking the same two questions: what did I want to do with the rest of my career, and was the path I was following going in the right direction? I spent a lot of time thinking about this, and my decision to leave Microsoft ultimately reflects the answer at which I arrived. But rather than indulge myself by recounting my introspection, I thought I would share some of the things that I learned while at Microsoft in the hopes that others find value in my experience.</p>
<p>To that end, I've split this post into two sections:</p>
<ol type="1">
<li>Things I've observed about <a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#hpc"><strong>HPC and technology trends</strong></a> from the perspective of a cloud/hyperscale/AI practitioner and provider, and</li>
<li>Things I've realized about <a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#career"><strong>jobs and careers</strong></a> from the perspective of someone who's now worked in <a href="https://www.sdsc.edu/">academia</a>, a <a href="https://www.cnbc.com/2019/09/12/10x-genomics-txg-biotech-start-up-surges-in-ipo-debut.html">successful startup</a>, <a href="https://www.nersc.gov/">government</a>, and now <a href="https://www.microsoft.com/">Big Tech</a> and is about halfway through his career</li>
</ol>
<p>I consider this to be the concluding chapter of a three-part series that began with¬†<a href="https://blog.glennklockwood.com/2022/05/life-and-leaving-nersc.html">Life and leaving NERSC</a>¬†and continued with¬†<a href="https://blog.glennklockwood.com/2024/08/how-has-life-after-leaving-labs-been.html">How has life after leaving the Labs been going</a>.</p>
<p>Also, please note that I authored this the day after my employment at Microsoft ended, and I was not beholden to any company or organization at the time of writing. <i>The views expressed below are mine alone</i>.</p>
<!--<ul><ul><li><a href="#hpc">HPC</a><ul><li><a href="#hpc-wants-to-be-like-the-cloud-not-in-it">HPC wants to be like the cloud, not in it</a></li><li><a href="#cloud-is-expensive-but-not-for-the-reasons-most-think">Cloud is expensive, but not for the reasons most think</a></li><li><a href="#although-sometimes-it-is">...Although sometimes it is</a></li><li><a href="#influencing-the-cloud-is-hard">Influencing the cloud is hard</a></li></ul></li><li><a href="#career">Career</a><ul><li><a href="#people-matter">People matter</a></li><li><a href="#company-culture-matters-too">Company culture matters, too</a></li><li><a href="#being-good-at-things-isnt-always-a-job">Being good at things isn't always a job</a></li><li><a href="#you-dont-have-to-be-your-employer">You don't have to be your employer</a></li><li><a href="#happiness-sometimes-costs-money">Happiness sometimes costs money</a></li></ul></li><li><a href="#final-thoughts">Final thoughts</a></li></ul></ul>-->
<h2 id="hpc">HPC</h2>
<p>Everything I did at Microsoft touched supercomputers in one way or another, and my day job was exclusively supporting Microsoft's largest AI training supercomputers. Despite that, I did a lot of moonlighting in support of Azure's Federal business, and this is how I justified giving talks at events like like <a href="https://sites.google.com/lbl.gov/nersc50-nug/home">NERSC@50</a>, <a href="https://sc24.supercomputing.org">SC</a>, and <a href="https://www.glennklockwood.com/garden/Salishan">Salishan</a> in my last year. It's also what let me straddle both worlds: I had a rare, first-hand knowledge of how the <a href="https://www.glennklockwood.com/garden/systems/Eagle">de facto largest supercomputers in the world</a> were built and used, and I had a front-row seat for how leaders in the traditional supercomputing world perceived (and sometimes misunderstood) what we were doing in the cloud.</p>
<p>Before I get into specific observations though, I should clarify some nomenclature that I will use throughout:</p>
<ul>
<li><strong>Supercomputers</strong> are the piles of compute nodes with a high-speed interconnect that are designed to solve one big problem in parallel. This is a generic term to describe the instrument, not its workload.</li>
<li><strong>HPC</strong>, <strong>traditional HPC</strong>, <strong>modsim</strong>, and <strong>scientific computing</strong> all refer to the ecosystem built around using something like MPI to solve a problem rooted in some type of science. Every big supercomputer run by DOE, procured through EuroHPC, and sited at the world-famous, government-funded supercomputer centers falls into this category.</li>
<li><strong>Cloud</strong>, <strong>hyperscale</strong>, and <strong>AI training</strong> all refer to the ecosystem built to train large language models. The supercomputers are run by hyperscale companies like Microsoft, Amazon, or Meta whose backgrounds have not historically been in the world of supercomputing.</li>
</ul>
<p>I realize that these are not very precise, but they're the easiest way to contrast what I learned inside Microsoft (a hyperscale cloud) with the world I came from prior (traditional HPC).</p>
<h3 id="hpc-wants-to-be-like-the-cloud-not-in-it">HPC wants to be like the cloud, not in it</h3>
<p>When I left NERSC in May 2022, <a href="https://blog.glennklockwood.com/2022/05/life-and-leaving-nersc.html">I speculated that the future of large-scale supercomputer centers</a> would be follow one of two paths:</p>
<ol type="1">
<li>They develop and squish cloud technologies into their supercomputers to make them more cloud-like, or</li>
<li>They abandon the idea of buying individual systems and instead enter into long-term relationships where flagship HPC systems are colocated inside cloud datacenters sited in places with low-cost, low-carbon power.</li>
</ol>
<p>I was hoping that the desire to continue building systems after passing the exascale milestone would make the next click-stop follow path #2, but early indications (across the global HPC landscape) are that the community has chosen path #1.</p>
<p>HPC centers around the world are embracing the idea of cloudifying on-prem supercomputers by adding virtualization, containerization, and integration with other services to enable complex workflows. And as a part of that, they're reinventing many of the technology integrations that have always been first-class citizens in cloud: CSCS added capabilities to create <a href="https://www.cscs.ch/publications/news/2024/new-research-infrastructure-alps-supercomputer-inaugurated">"versatile software-defined clusters" on their latest Cray system, Alps</a>. NERSC's next system, Doudna, is envisioned to allow its users to "<a href="https://www.vastdata.com/sharedeverything/how-nersc-is-rewriting-the-role-of-the-supercomputer">move from programming the supercomputer to programming the datacenter</a>." But none of these systems are actually using commercial cloud services in non-trivial ways.</p>
<p>In the year or two that followed ChatGPT, the notion of large-scale supercomputers in the cloud was a green field, and cloud providers were open to chasing all sorts of silly ideas. This made it the ideal time for the leadership HPC computing community to get a seat at the hyperscale table. Although their budgets couldn't compete with AI, HPC centers could've drafted on the investments of AI buildout and offered the societal impacts of using GPUs for science as a nice complement to the societal impacts of using GPUs for AI training.</p>
<p>Much to my dismay, though, that window of opportunity was spent decrying the investment in hyperscale and AI rather than trying to exploit it; that window was the year of "<a href="https://blog.glennklockwood.com/2024/05/isc24-recap.html#section11">us versus them</a>." And unfortunately, that window has essentially closed as accountants and CFOs have now sharpened their pencils and are searching for returns on the investments made in GPU infrastructure. The intrinsic value of supercomputing infrastructure in the cloud has been reduced to the point where <a href="https://www.theregister.com/2024/10/31/microsoft_q1_fy_2025/">Microsoft's CEO outright said they were turning away customers who just wanted to pay for GPU clusters</a>, because higher-quality revenue could be made from inferencing services that use those same GPUs.</p>
<p>So even if the HPC community woke up tomorrow and realized the long-term benefits of partnering with commercial clouds (instead of trying to copy them), I don't think cloud providers would respond with the same enthusiasm to meet in the middle now as they would have a year or two ago. I don't think this was a deliberate decision on behalf of the cloud providers, and they may not even fully realize this change. But the future of hyperscale supercomputing is rapidly crystallizing, and because HPC wasn't present in the solution, there's no room for it in the final structure.</p>
<h3 id="cloud-is-expensive-but-not-for-the-reasons-most-think">Cloud is expensive, but not for the reasons most think</h3>
<p>It's been easy to write off the cloud as too expensive for HPC, and most people do silly math based on public list prices for VMs to justify their position. The narrative usually goes something like, "<a href="https://info.ornl.gov/sites/publications/Files/Pub202373.pdf">if a single GPU VM costs $40/hr, then running 10,000 of them for five years will cost 17X more than our on-prem supercomputer!</a>" That's not how it works, and nobody pays that price. That $40/hr is the maximum possible price, and it includes the cost to the cloud provider of keeping nodes idle in the event that someone shows up and suddenly wants to use one on-demand.</p>
<p>But even if you cut out all the profit for the cloud provider and just look at the cost of the physical infrastructure, building a supercomputer in the cloud is just more expensive than putting a bunch of whitebox nodes into a traditional HPC datacenter. There's a couple reasons for this, and here are a couple in no particular order:</p>
<p><strong>High availability</strong>: Every cloud datacenter has redundant power, and most of them have <em>very</em> redundant power. This is provisioned independently of whatever goes inside of that datacenter, so when you deploy a 10 MW supercomputer inside a 10 MW cloud datacenter, that comes with at least 10 MW of backup diesel generators, UPSes, and the electrical infrastructure. HPC workloads don't really need this, but it's hard to deploy HPC in the cloud without a ton of generators and UPSes coming along for the ride. This is changing with AI-specific cloud datacenters now being built, but these AI datacenters still have way more redundant power than a typical on-prem HPC datacenter. Building a cloud datacenter with the minimal redundancy that a traditional HPC datacenter has would mean that facility couldn't ever be used for anything but HPC, and that would undercut the overall flexibility upon which cloud economics are built.</p>
<p><strong>Cloud-side infrastructure</strong>: Every compute node has to be attached to the frontend cloud network in addition to a backend high-speed network like InfiniBand, unlike a traditional supercomputer where nodes are only attached to one high-speed network. While the cost of the smart NIC in each node is just a couple hundred dollars, every cloud supercomputer has to have a complete frontend network built out to support every single compute node--that's a ton of switches, routers, and fiber that must be properly provisioned all the way up to the cloud region in which those nodes are deployed. This frontend network is what enables all the cool cloud features on every node (full SDN, integration with other cloud services, etc), but these features aren't generally worth their cost when running meat-and-potatoes HPC workloads like MPI jobs by themselves. Their value only really shines through when executing complex workflows that, for example, couple an MPI job with stateful services and globally accessible data sharing with fine-grained access controls, all fully automated through programmable APIs and full RBAC.</p>
<p><strong>AI-optimized system architecture</strong>: AI-optimized GPU supercomputers contain a bunch of components that your typical Cray or Eviden simply wouldn't have. I wrote about the <a href="https://www.glennklockwood.com/garden/differences-between-AI-and-HPC">differences between AI and HPC supercomputers elsewhere</a>, but in brief, AI workloads specifically benefit from having tens of terabytes of local SSDs and all-optical (no copper) RDMA fabrics. These add to the COGS (cost of goods sold) of an AI-optimized supercomputer, meaning that that a supercomputer with a thousand GPUs designed for AI is going to be more expensive than one designed for scientific computing no matter where it's deployed. And cloud providers are all optimizing their supercomputers for AI.</p>
<p>There's a bunch of other cloud "stuff" that is required as well; every cloud region has a first footprint which is a LOT of general-purpose servers and storage that is required to support the basic cloud control plane. Before any user-facing cloud resources (including supercomputers) can be deployed, there has to be tens or hundreds of racks of this cloud "stuff" that is up and running. And although the cost of that first footprint is amortized over many customers in larger or older cloud regions, larger single-use infrastructures (like supercomputers) carry a proportionally larger fraction of the cost to deploy the first footprint.</p>
<p>So when you look at the cost of running a single compute node in a cloud supercomputer, there are a bunch of extra ingredients baked in that you wouldn't get by just signing a check over to an OEM:</p>
<ul>
<li>a high availability SLA, afforded in part by all those generators and UPSes</li>
<li>slick cloud service integrations, privacy features, virtual networking, afforded by that frontend cloud network</li>
<li>better performance for AI training or inferencing workloads, afforded by extra SSDs and all-optical interconnects</li>
<li>a bunch of other typical TCO stuff--the power consumed by the node, the opportunity cost of free floor tiles in your datacenter, and the engineers and technicians that keep it all running</li>
</ul>
<p>Ultimately, someone needs to pay for all of these extra ingredients. Cloud providers <em>could</em> just eat the costs themselves and sell the supercomputing service at a price comparable to what a customer would pay for an on-prem supercomputer--and sometimes they do. But this dilutes the profitability of the deal, and it increases the risks of the cloud provider losing money if unexpected issues arise during execution. Losing money is objectively bad business, so it's usually cloud customers who are paying for all these extra capabilities regardless of if they use them or not.</p>
<p>So if all you want to do is run big MPI jobs, and you have no use for the extra availability, cloud integrations, privacy and security, and programmable infrastructure, sure--the price per-node is going to be higher in the cloud than on-prem. You're paying for a bunch of features that you don't need.</p>
<h3 id="although-sometimes-it-is">...Although sometimes it is</h3>
<p>Sometimes buying a supercomputer in the cloud is straight up more expensive because of the value it provides though. For example, I remember a case where a large AI company needed to train a big LLM on many thousands of GPUs, so they signed an agreement which gave them exclusive access to a cloud supercomputer that strongly resembled a specific GPU system in the DOE complex. Because I used to work in the DOE, I knew how much DOE paid to buy their GPU cluster, and I also knew that three years of maintenance was included in that cost.</p>
<p>What amazed me is what this AI company was willing to pay (roughly) the same price that DOE paid for their on-prem supercomputer, but in exchange, get exclusive access to a comparably capable cloud supercomputer (same GPUs model, similar GPU count, similar interconnect) for <em>one year only</em>. Put differently, being able to use a big, cutting-edge GPU cluster was worth up to 3x more to this AI company than it was to the DOE.</p>
<p>While it may sound like I'm spilling secrets here, the reality is that anyone working for a cloud provider wouldn't be able to tell which AI deal I was describing here--they all look like this, and they're all willing to spend significantly more than the HPC community for the same compute capability. This gives you a sense of the real value that AI companies place on all the benefits that cloud-based supercomputers can provide.</p>
<p>This isn't all bad for HPC, though. Every fat deal with an AI company means that there can be another deal with an HPC center that has slim margins. For example, let's say an AI company is willing to pay a billion dollars for a supercomputer whose TCO is only $330M--that means the cloud provider gets 67% margin. If the cloud provider's overall margin target is 50%, that means it can sell an identical supercomputer to an HPC customer at zero profit (for $330M) and still walk away happy. Thus, it is possible for the price of a supercomputer for HPC to be subsidized by all the money that the AI industry is throwing into supercomputing. Whether or not a cloud provider ever cuts deals like this is a business decision though--and as I said earlier, I don't think they're as open to silly ideas now as they used to be.</p>
<p>The real hurdle that I was never able to overcome out, though, is a result of the fact that there is finite expertise in HPC and AI in the world. HPC-AI is ultimately a zero-sum game, and every hour spent working with an HPC customer is usually an hour that isn't being spent working with a much more profitable AI customer. I constantly ran into this problem working in hyperscale AI; my full-time job was to deal with AI customers, but I enjoyed interacting with HPC customers too. As a result, I had to do a lot of my the HPC-specific work (preparing conference presentations, for example) on nights, weekends, and vacations. It was just hard to tell people that I couldn't help improve job uptime on a massive training run because I was preparing a talk for a workshop that, frankly, might be openly hostile to my message.</p>
<h3 id="influencing-the-cloud-is-hard">Influencing the cloud is hard</h3>
<p>Because the difference in investment is so big between HPC and AI, many of the carrots that the HPC community has traditionally dangled in front of HPC vendors aren't very enticing to the hyperscale AI community. For example, both US and European HPC programs have relied heavily on non-recurring engineering (NRE) contracts with industry partners to incentivize the creation of products that are well-suited for scientific computing; <a href="https://www.energy.gov/articles/department-energy-awards-six-research-contracts-totaling-258-million-accelerate-us">PathFoward</a> and <a href="https://research-and-innovation.ec.europa.eu/funding/funding-opportunities/funding-programmes-and-open-calls/horizon-2020_en">Horizon 2020</a> both come to mind as well-funded, successful efforts on this front.</p>
<p>However, HPC is the only customer community that really tries to do this, and it echoes a time when the HPC community was at the forefront of scale and innovation. Nowadays, the prospect of accepting $1M/year NRE contract to implement XYZ is completely unappetizing to a hyperscaler; it would probably cost more than $1M/year just to figure out how a company with <a href="https://www.microsoft.com/investor/reports/ar24/">$250 billion in annual revenue</a> can handle such an unusual type of contract and payment. Add to to this the weird intellectual property rules (like disentangling a <a href="https://www.energy.gov/gc/articles/advance-patent-waiver-wa2017-007?utm_source=chatgpt.com">40% cost sharing advance waiver</a> for a tiny project within a multi-billion-dollar business), and it can become a corporate quagmire to go anywhere near NRE projects. Companies with well-insulated HPC silos can probably manage this better, but part of hyperscale economics is that everything overlaps with everything else as much as possible across supercomputing, general-purpose computing, hardware, and software.</p>
<p>As a result of this, I really struggled to understand how a $20M/year service contract and a $1M/year NRE contract is materially different from a $21M/year service contract in the cloud world. For most (non-HPC) cloud customers, the RFP comes in saying "we need XYZ" and some product manager notes customer demand for XYZ. If the demand is large enough, the feature winds up on roadmap, and the cloud provider develops it as a part of regular business. If there is no other demand, then an NRE contract isn't really going to change that; maintaining feature XYZ long-term will cost far more than a couple million dollars, so implementing it would be a bad decision. This isn't unique to cloud, for what it's worth; while there are some successful HPC NRE stories, there are far more NRE-originated products that had no product-market fit and were <a href="https://cug.org/proceedings/cug2016_proceedings/includes/files/pap105s2-file1.pdf">simply abandoned</a> after the associated supercomputer was retired.</p>
<p>As best as I can tell, NRE has become a way for big HPC customers to maintain the illusion that they are influencing hyperscalers. A hyperscaler could propose some NRE, and an HPC buyer could fund it, and there could be weekly meetings where the two get together and pretend like they're collaborating and codesigning. The hyperscaler could write milestone reports, and they could attend quarterly business reviews with the customer. But this feels like an act. You simply can't move a $250B/year company that isn't solely organized around supercomputing with the lure of a couple million a year.</p>
<p>This is not to say that NRE and codesign have no purpose in HPC! I'm sure component vendors (GPUs, networking, and the like) can make minor tweaks that offer big upside for the HPC community. But I learned that, as in several other dimensions, the HPC community is being pushed towards buying whatever is already on the truck, and NRE isn't going to have the impact that it once did.</p>
<h2 id="career">Career</h2>
<p>In addition to learning about how the hyperscale supercomputer world works in practice, my time at Microsoft exposed me to a segment of the supercomputing community that I didn't know existed: junior software engineers who were unwittingly thrown into the deep end of HPC straight out of college and were desperate to find their footing in both the technology and their careers overall. Maybe the most impactful work I did in the past three years was not technical at all, but instead came through some internal talks I gave on my professional journey in HPC and the one-on-one conversations that followed.</p>
<p>Since I've gotten such positive feedback when I talk and write about this aspect of HPC, I'll also share some things I've learned about choosing the right employer and job during my time at Microsoft.</p>
<h3 id="people-matter">People matter</h3>
<p>I learned that the right team matters more than the right job. It is profoundly important to me that I get to work with people with the same level of passion and curiosity, even if we are working on different problems.</p>
<p>In retrospect, I realize that I have been very lucky that my career has progressed through organizations that were packed to the gills with people with whom I shared values. They wanted to go to conferences to share their work, they wanted to hear about how others are solving similar challenges, and they weren't afraid to present (and challenge) new ideas. As I learned over the last three years though, I think these traits are acutely concentrated in the HPC world since HPC itself originated from academia and a culture of independence and self-direction. They certainly aren't universal to all workplaces.</p>
<p>To be clear, I am not saying that my coworkers at Microsoft weren't passionate or curious. But I did learn that, at big tech companies, you can have a perfectly successful career by keeping your head down and cranking away at the tasks given to you. If the work changes one day, it's actually a virtue to be able to walk away from the old project and turn your complete attention to a new one. Did the company just <a href="https://www.theverge.com/2024/10/1/24259369/microsoft-hololens-2-discontinuation-support">cancel the product you've been working on</a>? No problem. If you were good at writing code for Windows update, you'll probably be just fine at coordinating planned maintenances for supercomputers. A colleague of mine called these people "survivors," because they will do the best they can with whatever they're given.</p>
<p>While this agility is great if you love programming, it can also engender numbness and dispassion for any specific application area. If a "survivor" can just as easily program for HoloLens as they can for GPU telemetry, they also likely don't really <em>care</em> about either HoloLens or GPUs. This isn't a bad thing, and I am certainly not passing judgment on people who don't care about GPUs. But it does mean that it's harder for someone who really cares about GPUs to connect with a teammate who really doesn't. And this has many knock-on effects in day-to-day work; it's only natural for people who share common values to help each other out, while relative strangers are less likely to go that extra mile. Finding that common ground to promote "some person on team X" to "my trusted colleague on team X" is that much harder.</p>
<p>This difficulty in finding my community amidst all the survivors is what led me to look outside of my company to find my people. I went to events like the <a href="https://www.olcf.ornl.gov/tag/smoky-mountain-conference/">Smoky Mountains Conference</a> and <a href="https://sites.google.com/lbl.gov/nersc50-nug/home">NERSC@50</a> and took the stage to literally beg the HPC community to give me a reason to work with them. By the letter of my job description, I was never supposed to be on stage; I was supposed to spending all my time behind my desk, thinking about the reliability of our biggest supercomputers. But I liked working with the people in the HPC community, and I liked working with our HPC sales organization, because we all shared common values; we were passionate about HPC and the mission of advancing scientific computing. So, I wound up spending a lot of time working on simple things with HPC folks and not enough time doing my day job.</p>
<h3 id="company-culture-matters-too">Company culture matters, too</h3>
<p>In an organization where individuals don't often share a lot of common ground, I learned that it's incumbent upon everyone to make a deliberate effort to maintain a culture of working together and helping each other out. A positive workplace culture won't happen by itself across a massive organization. To this end, Satya has a bunch of corporate culture mantras that are often repeated to keep reminding people of the way employees should treat each other.</p>
<p>For example, he has a mantra of "<a href="https://www.msn.com/en-us/money/other/how-satya-nadella-created-a-learn-it-all-culture-at-microsoft-to-help-it-become-a-3-trillion-powerhouse/ar-BB1qWoRY">be a learn-it-all, not a know-it-all</a>." But I found that many people struggled to really understand how to do this in practice; when confronted with a tough problem ("your database keeps timing out when we point a thousand nodes at it at once"), it's often too easy to just be a know-it-all ("nobody else does that, so you are doing it wrong") rather than a learn-it-all ("why are you doing it that way?"). And the older a company is, the harder it is for decades-long veterans to maintain openness to new challenges in the silo they've built around themselves.</p>
<p>I've worked with HPC users for long enough to know that this attitude is pervasive anywhere you put a bunch of smart people with different perspectives into a room. However, it wasn't until I came to Microsoft that I learned that there's something to be gained by explicitly and repeatedly reminding people that they should strive to understand at least as much as they try to explain. Should I ever find myself in a leadership position, this is definitely a mantra I will carry with me and repeat to others, and I will credit my time at Microsoft with appreciating how to really live this mentality, not just parrot it.</p>
<h3 id="being-good-at-things-isnt-always-a-job">Being good at things isn't always a job</h3>
<p>People tell me that I'm pretty good at a bunch of stuff: figuring out how technologies work, explaining complex concepts in understandable ways, and taking a critical look at data and figuring out what's missing. And I enjoy doing these things; this is why I post to <a href="https://blog.glennklockwood.com/">my blog</a>, maintain <a href="https://www.glennklockwood.com/garden/">my digital garden</a>, and love <a href="https://www.youtube.com/playlist?list=PLtPey-3r1oZS0S5pPcWq-L4yrT9-R0gIm">getting on stage and giving presentations</a>. But people also say that, because I'm good at these things, there'd be no shortage of opportunities for me in the HPC industry should I ever go looking.</p>
<p>However, I've learned that a <em>job</em> has to be an amalgamation of <em>responsibilities</em> that create value, and connecting "things I'm good at" with "things that need to be done" is not always straightforward. For example, if I am <em>good at</em> learning things and share what I learned with others, what kind of jobs actually turn that into a <em>responsibility</em>?</p>
<ul>
<li><strong>Developers</strong> don't really do this at all. Their job is really to keep those git commits coming. Sometimes this requires learning new things, but writing blog posts or giving talks is not in the job description, so they don't count for much on performance reviews.</li>
<li><strong>Product managers</strong> do a little of this. I had to learn a few things and then repeat them a lot when I was a PM. Over and over. To customers, to executives, to partner teams. It was 5% learning and 95% sharing.</li>
<li><strong>Salespeople</strong> also do a little of this. They have to stay current on customer needs and product features, then repeat them a lot.</li>
<li><strong>System architects</strong> do a fair amount of this. I had to learn about what technologies are on the horizon, figure out how to piece them into an idea that could be implemented, then explain why it'd all be a good idea to others.</li>
<li><strong>Educators</strong> do a lot of this. The technology industry is always moving, so learning is required to stay up to date. They also get to be selective about the ideas worth sharing and downplay the rest.</li>
</ul>
<p>Each one of these roles has its own downsides too; for example, product managers and salespeople often have to nag people a lot, which I don't think anyone likes. And many of these roles require sharing knowledge with people who really don't want to hear it. After all, what customer is eager to talk to every salesperson who comes in the door?</p>
<p>Trying to find the ideal job is not just a matter of being good at many things; it's a matter of finding specific jobs that contain a maximal number of things you're good at and a minimal number of things you don't want to do. It's an NP-hard problem, and I've come to realize that the only way to solve it is through trial-and-error. I'm sure some people get lucky and figure out the optimal path on their first try, but for the rest of us, the only way to approach the optimal path is to continuously reflect and not longer on a known-suboptimal path for any longer than is necessary.</p>
<p>I've given up on trying to find the perfect job, because I've learned that it probably doesn't exist. I'm good at some things, I'm bad at some things; I enjoy some responsibilities, and I dislike some responsibilities. As with every other job I've had, I learned a lot about all four of these categories during my time at Microsoft, and my choice of next step has been informed by that. I don't expect it to be perfect, but I have high hopes that it will be a step in the right direction.</p>
<h3 id="you-dont-have-to-be-your-employer">You don't have to be your employer</h3>
<p>When I left the government for a corporate job, one of my biggest worries was losing credibility with peers whose opinions I respected. It's easy to dismiss the viewpoint of someone at a large vendor with a rationalization like, "of course they'd say that; it's their job," but I learned that the HPC community isn't so reductive. People are smart, and most were willing to engage with the quality of my ideas before checking the affiliation on my conference badge.</p>
<p>The trick, of course, was finding ways to share ideas in a way that didn't upset my corporate overlords but had substantive value to my audience. I think I figured this out, and in short, I found that leading with honesty and precision works best. The HPC community was built on sharing experiences and learnings about what does and doesn't work, so embracing that--rather than name-dropping products and making hyperbolic claims--seemed to keep me getting invited back to the HPC conferences and workshops that I wanted to attend.</p>
<p>I wasn't completely intentional in building whatever credibility I've gained over the last three years, but I was intentional in avoiding work that would clearly compromise it. I never want to be accused of misrepresenting the limits of my understanding, so I will never present a slide containing statements or plots that I can't substantiate. I also never want to be accused of misrepresenting the truth, so I am as forthright as possible in disclosing when I do (or don't) have an incentive to say something.</p>
<p>Because I stayed true to myself, I think I was the same person at Microsoft as I was at NERSC or SDSC. That continuity helped my peers quickly recalibrate after I became a vendor, and I think this helped me do more than if I had gone all-in on the role of a cloud spokesperson. Of course, there were times when I had to take on an employer-specific persona, but that's just business, and I've found that peers recognize that this is just a part of the game that we all must play.</p>
<p>The result of all this wasn't clear to me until after I started telling people I was leaving Microsoft. There are a bunch of HPC-specific projects I undertook on the side (e.g., reviewing and advising on research, serving on panels), and I started notifying people that I would have to find other Microsoft engineers to take over these obligations since I was leaving. Much to my surprise though, everyone responded the same way: the request to have me help was specifically to me, not my employer. Short of any conflicts of interest, they didn't care who employed me and valued my contributions regardless of who was signing my paychecks.</p>
<p>So, after three years working for an HPC vendor, I have learned that most people won't define you by your employer as long as you don't define yourself by your employer. It is possible to work for a company that sells HPC and still maintain your own identity as a person, but it requires thoughtful effort and a supportive (or indifferent!) employer. If you act like a company shill, you will be regarded as one, but not many jobs in industry actually <em>require</em> that to fulfill your responsibilities.</p>
<h3 id="happiness-sometimes-costs-money">Happiness sometimes costs money</h3>
<p>I think most people would agree that, while money can't buy happiness, it certainly helps. What I didn't realize until recently, though, is a reciprocal truth: sometimes happiness costs money.</p>
<p>A year ago, I wrote about <a href="https://blog.glennklockwood.com/2024/08/how-has-life-after-leaving-labs-been.html#pay-good">how the pay in industry compares to working at the national labs</a>, and I described how my golden handcuffs were structured. An optimist might say that these vesting schedules are a way to keep a happy employee from being lured away, but I think it's equally common that these are truly handcuffs. They are a constant reminder that, even in the darkest of days, there is a six-figure reason to grit one's teeth and persevere.</p>
<p>I've come to realize that there is an adverse correlation between a few factors:</p>
<ul>
<li>Smaller organizations offer more flexibility to mold a job around your preferences, because there is more work scope spread across fewer people.</li>
<li>Larger organizations can afford to offer larger total compensation, but flexibility is limited to the scope of any single team.</li>
</ul>
<p>I kind of thought about it like this:</p>
<div class="separator" style="clear: both; text-align: center;"></div>
<p>When I realized that I should explore other paths, I had to determine where in this continuum I wanted to wind up: do I care more about a fat paycheck, or do I care more about enjoying my day-to-day responsibilities? And once offers started coming in, exactly how much of a pay cut was I willing to take in exchange for the flexibility that I would receive?</p>
<p>By the time I handed in my resignation at Microsoft, I knew exactly how much this happiness was worth to me. Alternatively, I found out how much opportunity cost I was willing to pay for the ability (hopefully!) to reconnect with my day-to-day work. The calculus was an interesting exercise involving a bunch of Monte Carlo simulation which I won't detail here, but as it turns out, I was willing to pay a lot of money for the chance to do something that aligned more completely with what I wanted to do with the rest of my career. In the end, I gave up hundreds of thousands in unvested stock, and I am taking a six-figure pay cut in annual base pay when I start my next job. For me, though, this was a fair price to pay.</p>
<h2 id="final-thoughts">Final thoughts</h2>
<p>After three years in the world of hyperscale supercomputing, I have come away with two major learnings that now shape how I think about the future.</p>
<p>On the technical front, I think the HPC community has chosen to keep going its own way and reinvent the cloud rather than work meaningfully with hyperscale cloud providers. There was a brief window of opportunity where <a href="https://idiomorigins.org/origin/if-the-mountain-wont-come-to-muhammad-then-muhammed-must-go-to-the-mountain">the mountain may have actually come to Muhammed</a>, and the trajectory of scientific computing could have fundamentally changed to align with the growth trajectory of hyperscale AI. However, I don't think the HPC community was ready to take a big swing during those early days post-ChatGPT or do an earnest assessment of what that future could've looked like. I also worry that the window has closed, and the HPC community never even realized what was on the table.</p>
<p>On the career front, I've realized that success is multidimensional. Money is one axis, but so are impact, people, and purpose. The relative importance of each is not always obvious either; they only became clearer to me as I tried different jobs across the space. I've found that the ability to work with like-minded people and the opportunity to learn and share are the most important dimensions to me, but also I recognize that I am privileged in others. Finding stacks of money can be easy for those who work in AI, but there are no shortcuts to building (and retaining!) teams of great people. Anyone who can do the latter well should not be undervalued.</p>
<p>There's a lot more that I didn't have time to organize and write, but I have every intention of continuing to be myself, regardless of where I work, in the future. I will keep writing, posting, and talking about what I'm learning in supercomputing whenever I can. And along those lines, I hope that writing all this out helps others figure out what's important to them and where they want to go.</p>]]></content><author><name>Glenn K. Lockwood&apos;s Blog</name></author><category term="glennklockwood" /><summary type="html"><![CDATA[I recently decided to leave Microsoft after having spent just over three years there, first as a storage product manager, then as a compute engineer. Although I touched many parts of Azure's infrastructure during that time, everything I did was at the intersection of large-scale supercomputing and hyperscale cloud. There was no shortage of interesting systems to figure out and problems to solve, but as I began to wrap my arms around the totality of hyperscale AI training in the cloud, I also began to see the grand challenges that lay ahead.]]></summary></entry><entry><title type="html">ISC‚Äô25 recap</title><link href="https://hpc.social/personal-blog/2025/isc-25-recap/" rel="alternate" type="text/html" title="ISC‚Äô25 recap" /><published>2025-06-24T05:58:00-06:00</published><updated>2025-06-24T05:58:00-06:00</updated><id>https://hpc.social/personal-blog/2025/isc-25-recap</id><content type="html" xml:base="https://hpc.social/personal-blog/2025/isc-25-recap/"><![CDATA[<p>I had the pleasure of attending the 40th annual ISC High Performance conference this month in Hamburg, Germany. It was a delightful way to take the pulse of the high-performance computing community and hear what the top minds in the field are thinking about.</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">The main foyer of Congress Center Hamburg, and the view that greeted me on the first morning of ISC'25.¬†</figcaption></figure>
</div>
<p>The conference felt a little quieter than usual this year, and there didn't seem to be as many big ideas and bold claims as in years past. There was <a href="https://www.theregister.com/2025/06/10/jupiter_europes_top_super/">a new Top 10 system announced</a>, but it was built using previous-generation Hopper GPUs. There were a <a href="https://isc-hpc.com/the-isc-2025-exhibition-sets-new-records/">record number of exhibitors</a>, but many of the big ones (Intel, AMD; the big three cloud providers) were all absent. And while there were some exciting new technologies (like <a href="https://www.tomshardware.com/pc-components/gpus/amd-announces-mi350x-and-mi355x-ai-gpus-claims-up-to-4x-generational-gain-up-to-35x-faster-inference-performance">AMD MI350-series GPUs</a> and <a href="https://ultraethernet.org/ultra-ethernet-consortium-uec-launches-specification-1-0-transforming-ethernet-for-ai-and-hpc-at-scale/">Ultra Ethernet v1.0</a>) debuting during the week, they actually debuted elsewhere and were simply referenced throughout the week's talks.</p>
<p>This year's ISC really felt like the place where the big news of the industry was being repeated in the context of scientific computing instead of being stated for the first time. And maybe this is the future of HPC conferences: rather than being where new technology is announced, perhaps ISC will become where the scientific community tries to figure out how they can use others' technology to solve problems. That idea--figuring out how to make use of whatever the AI industry is releasing--was certainly pervasive throughout the ISC program this year. The conference's theme of "connecting the dots" felt very appropriate as a result; rather than defining new dots, the conference was all about trying to make sense of the dots that have already been drawn.</p>
<p>I took plenty of notes to try to keep track of everything that was being discussed, and as has become tradition, I've tried to summarize some of the key themes in this post.</p>
<h2 style="text-align: left;">Table of contents</h2>
<ul>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#zettascale">Zettascale</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#ozaki-ozaki-ozaki">Ozaki, Ozaki, Ozaki</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#top500">Top500</a><ul>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#jupiter">JUPITER</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#hpcai-system-intersection">HPC-AI system intersection</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#other-new-entrants">Other new entrants</a></li>
</ul>
</li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#hpc-around-the-world">HPC around the world</a><ul>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#hpc-in-china">HPC in China</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#elsewhere-in-asia">Elsewhere in Asia</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#the-middle-east">The Middle East</a></li>
</ul>
</li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#exhibitors">Exhibitors</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#cloud-or-lack-thereof">Cloud, or lack thereof</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#parting-thoughts">Parting thoughts</a></li>
</ul>
<h2 id="zettascale">Zettascale</h2>
<p>Now that exascale is squarely in the rear-view mirror of HPC, an increasing number of high-profile speakers began pushing on zettascale as the next major milestone. Like the early days of exascale, most of the discourse was less about what can be achieved with zettascale and more about the technology challenges that need to be surmounted for HPC to continue moving forward. And to that end, using zettascale to justify tackling big hardware and software challenges wasn't a bad thing, but it felt like every talk about zettascale this year was still more fanciful than anything else.</p>
<p>The opening keynote, "HPC and Al - A Path Towards Sustainable Innovation" was delivered by a duo of CTOs: Mark Papermaster (of AMD) and Scott Atchley (of Oak Ridge Leadership Computing Facility). It was a textbook keynote: it had inspiring plots going up and to the right that showed huge potential! It had scary linear extrapolations showing that staying the course won't do! It had amazing science results enabled by big iron! It even had a surprise product debut in MI355X! ChatGPT couldn't have come up with a better structure for a keynote presentation. But as is my wont, I listened to the talk with a little skepticism and found myself raising an eyebrow a few times.</p>
<p>A part of Papermaster's presentation involved an extrapolation to zettascale by 2035 and claimed that HPC is approaching an "energy wall:"</p>
<div class="separator" style="clear: both; text-align: center;">
<figure><figcaption class="image-caption">Extrapolating ten years on a semilog plot is a great way to cause alarm in people who don't pay close attention to axes.</figcaption></figure></div>
<p>He specifically said that we'd need 1 GW per supercomputer to reach zettascale by 2035 on the current trajectory. He then used this to motivate "holistic co-design" as the only way to reach zettascale, and he went on to talk about all the same things we heard about leading up to exascale: increase locality and integration to reduce power and increase performance.</p>
<p>While I agree that we should aspire to do better than a gigawatt datacenter, this notion that there is an "energy wall" that stands between us and zettascale is a bit farcical; there's nothing special about a 1 GW zettascale supercomputer, just like there was nothing special about 20 MW for exascale. You might argue that building a supercomputer that consumes all the power of a nuclear reactor might be fundamentally more difficult than one that consumes only 20 MW, and you'd be right--which is why the first gigawatt supercomputers probably aren't going to look like the supercomputers of today.</p>
<p>Papermaster's "energy wall" slide reminded me of <a href="https://fee.org/articles/the-great-horse-manure-crisis-of-1894/">the great horse manure crisis of 1984</a>, where people extrapolated from today using an evolutionary, not revolutionary, trajectory. If building a single gigawatt supercomputer is inconceivable, then build four 250 MW supercomputers and put a really fast network between them to support a single, synchronous job. The AI industry is already headed down this road; <a href="https://glennklockwood.com/garden/multicluster-training">Google, Microsoft, and OpenAI have already talked about how they synchronously train across multiple supercomputers</a>, and Microsoft announced their <a href="https://view.officeapps.live.com/op/view.aspx?src=https%3A%2F%2Fmediusdownload.event.microsoft.com%2Ftranscripts%2FD6K5%2FKEY020%2FKEY020.docx%3Fsv%3D2018-03-28%26sr%3Db%26sig%3D0gs30Jf82r%252BresqqpGIGKRSOtKrvicgbpqh5Tdkigpg%253D%26se%3D2025-06-24T18%253A46%253A13Z%26sp%3Dr&amp;wdOrigin=BROWSELINK">400 Tb/s "AI WAN" for this last month</a> as a means to enabling wide-area training.</p>
<p>Granted, it's unlikely that the HPC community will be building massive, distributed supercomputers the way hyperscale is. But I was disappointed that the keynote only went as far as saying "a gigawatt supercomputer is crazy, so we need codesign at the node/rack scale." Codesign to reach zettascale will probably require a whole new approach that, for example, accounts for algorithms that <a href="https://github.com/NVIDIA/nccl/pull/1659">synchronize communication across multiple datacenters</a> and power plants. The infrastructure for that is already forming, with the US developing its Integrated Research Infrastructure (IRI) and Europe shaping up to have over a dozen AI factories. Zettascale by 2035 may very well exist for the scientific computing community, but it'll probably look a lot more like hyperscale zettascale rather than a single massive building. A single machine plugged into a gigawatt nuclear reactor only happens if business-as-usual is extrapolated out another ten years as Papermaster did, and the codesign required to achieve that isn't very meaningful.</p>
<p>Prof. Satoshi Matsuoka also gave a talk on the big stage about <a href="https://glennklockwood.com/garden/systems/FugakuNEXT">Fugaku-NEXT</a>, which Japan has branded as a zettascale system. His vision, which will be realized before 2030, aims to deploy a single, 40 MW supercomputer (much like <a href="https://www.glennklockwood.com/garden/systems/Fugaku">Fugaku</a> was) where:</p>
<ul>
<li>10x-20x speedup comes from hardware improvements</li>
<li>2x-8x speedup comes from mixed precision or emulation (more on this below)</li>
<li>10x-25x speedup comes from surrogate models or physics-informed neural networks</li>
</ul>
<p>The net result is a 200x-4000x speedup over Fugaku. His rationale is that this will result in a system that is effectively equivalent to somewhere between 88 EF and 1.7 ZF FP64. It's not literally doing that many calculations per second, but the science outcomes are equivalent to a brute-force approach using a much larger system.</p>
<p>I thought this approach to reaching zettascale was much more realistic than the Papermaster approach, but it does require the scientific computing community to redefine its metrics of success. If HPL was a bad benchmark for exascale, it is irrelevant to zettascale since it's unlikely that anyone will ever run HPL on a zettascale system. At best, we'll probably see something like <a href="https://hpl-mxp.org">HPL-MxP</a> that captures the 10x-20x hardware speedup and the 2x-8x mixed-precision or emulated FP64 reach hundreds of exaflops, but the 10x-25x from surrogate models will be domain-specific and defy simplistic ranking. If I had to guess, the first zettascale systems will be benchmarked through Gordon Bell prize papers that say things like "simulating this result using conventional FP64 would have required over 1 ZF for 24 hours."</p>
<h2 id="ozaki-ozaki-ozaki">Ozaki, Ozaki, Ozaki</h2>
<p>Although Prof. Matsuoka evoked the 2x-8x speedup from mixed precision or emulation when claiming <a href="https://www.glennklockwood.com/garden/systems/FugakuNEXT">Fugaku-NEXT</a> would be zettascale, he was far from the only speaker to talk about mixed precision and emulation. In fact, it seemed like everyone wanted to talk about emulating FP64, specifically using NVIDIA's low-precision tensor cores and the <a href="https://doi.org/10.1007/s11075-011-9478-1">Ozaki scheme</a> (or its derivatives). By the end of the week, I was absolutely sick of hearing about Ozaki.</p>
<div class="separator" style="clear: both; text-align: center;">
<figure></figure></div>
<p>For the unindoctrinated, this Ozaki scheme (and similar methods with less-catchy names) is a way to emulate matrix-matrix multiplications at high precision using low-precision matrix operations. It's become so hot because, despite requiring more arithmetic operations than a DGEMM implemented using WMMA/MFMA instructions, it can crank out a ton of FP64-equivalent operations per unit time. This is a result of the ridiculously nonlinear increases in throughput of low-precision tensor/matrix cores on modern GPUs; for example, Blackwell GPUs can perform over 100x more 8-bit ops than 64-bit ops despite being being only 8x smaller. As a result, you can burn a ton of 8-bit ops to emulate a single 64-bit matrix operation and still realize a significant net speedup over hardware-native FP64. Matsuoka presented the following slide to illustrate that:</p>
<div class="separator" style="clear: both; text-align: center;"><figure><figcaption class="image-caption">Dr. Uchino's estimates of how many FP64 FLOPS one can emulate using INT8 as presented by Satoshi Matsuoka.</figcaption></figure></div>
<p>Emulation offers a way for scientific apps that need high-precision arithmetic to directly use AI-optimized accelerators that lack FP64 in hardware, so it's worth talking about at conferences like ISC. But it seems like <em>everyone</em> wanted to name-drop Ozaki, and the actual discussion around emulation was generally a rehash of content presented earlier in the year at conferences like <a href="https://blog.glennklockwood.com/2025/03/gtc-2025-recap.html">GTC25</a>.</p>
<p>While hearing about FP64 emulation and Ozaki schemes got tiring throughout the week, I had to remind myself that I hadn't even heard about Ozaki before September 2024 at the Smoky Mountains Conference. The fact that the Ozaki scheme went from relative algorithmic obscurity to being the star of the show in nine months is either a reflection of its incredible importance in scientific computing or a testament to the reach of NVIDIA's marketing.</p>
<p>Cynically, I'll bet that NVIDIA is probably doing everything it can to make sure the world knows about the Ozaki scheme, and ISC was a part of that. When the datasheets for Rubin GPUs are released, I'll bet the performance table has a row claiming a bazillion FP64 FLOPS, and there will be a tiny footnote that clarifies they're citing emulated FP64 precision. They did it with structured sparsity, and I'm sure they'll do it for emulated DGEMM.</p>
<p>Although the Ozaki scheme is perhaps over-hyped considering how narrow its applicability is to the broad range of compute primitives used in scientific computing, I do anticipate that it is the tip of the iceberg. If 2025 was the year of the Ozaki scheme, 2026 may be the year of the emulated FP64 version of FFTs, sparse solvers, stencils, or other key algorithms. We're seeing signs of that already; David Keyes and Hatem Ltaief both presented material at ISC on using mixed-precision matrix operations for other scientific problems, and I mentioned <a href="https://blog.glennklockwood.com/2025/03/gtc-2025-recap.html#for-science">their work in my earlier GTC25 blog</a>. I'm not sure "the Keyes scheme" or "the Ltaief scheme" is as catchy as "the Ozaki scheme," but I expect to hear more about these other emulation techniques before ISC26.</p>
<h2 id="top500">Top500</h2>
<p>On the topic of matrix-matrix multiplication, I can't get too much farther without talking about the Top500 list released at ISC. Although there was no new #1 system, Europe's first exascale system, JUPITER, made its sub-exascale debut. There were also a number of new entries in Top50, and surprisingly, many of them came from companies who offer GPUs-as-a-Service for AI training rather than the usual public-sector sites delivering cycles for scientific research. However, all the new entries were still using previous-generation Hopper GPUs despite huge Blackwell coming online, exposing a perceptible lag between the state of the art in supercomputers for AI and traditional HPC.</p>
<p>As with last year, I felt a growing tension between what the Top500 list brings to the discussion and where the large-scale supercomputing industry is headed. As I wrote earlier, mixed-precision and emulated FP64 was a hot topic in the technical program, but the emphasis of the Top500 session was still squarely on bulk-synchronous FP64 performance. HPL-MxP awards were handed out, but they all wound up in the hands of systems who were also at the top of the regular HPL list. Nobody is submitting HPL-MxP-only scores, and there was no meaningful discussion about the role that the Ozaki scheme will play going forward in Top500's future.</p>
<p>Opining about the long-term future of the Top500 list is a whole separate blog post though, so I'll focus more on what was covered at this year's session.</p>
<h3 id="jupiter">JUPITER</h3>
<p>JUPITER was the only new entrant into the Top 10, and it posted at #4 with an average 793 PF over a hundred-minute run. Though it hasn't broken the 1 EF barrier yet, JUPITER is noteworthy for a few reasons:</p>
<ul>
<li>It is expected to be Europe's first exascale system. Given this HPL run <a href="https://bsky.app/profile/andih.bsky.social/post/3lrvrguvtzc2b">used only 79% of the Booster Module's 5,884 GH200 nodes</a>, some basic extrapolation puts the full-system run just a hair above 1 EF. J√ºlich will either have to run with 100% node availability or get a few extra nodes to exceed 1 EF though.</li>
<li>JUPITER is also now the biggest NVIDIA-based supercomputer on Top500, pushing Microsoft's H100 SXM5 system (Eagle) down to #5. JUPITER is also Eviden's biggest system and a strong affirmation that Europe isn't dependent on HPE/Cray to deliver on-prem systems of this scale.</li>
</ul>
<p>JUPITER was also installed into a modular datacenter, an approach that is emerging as a preferred method for rapidly deploying large GPU systems in Europe. This setup allowed J√ºlich to place shipping container-like modules on a concrete foundation in just a few months. However, because the datacenter is form-fit to the JUPITER system without much extra space, it's impossible to take a glamor shot of the entire machine from far away. As a result, most photos of JUPITER show only the datacenter modules that wrap the supercomputer racks. For example, Prof. Thomas Lippert shared this photo of JUPITER during his presentation:</p>
<div class="separator" style="clear: both; text-align: center;"><figure><figcaption class="image-caption">JUPITER's modular datacenter as seen from a drone flying overhead.</figcaption></figure></div>
<p>As Lippert was describing JUPITER, I couldn't help but compare it to the AI supercomputers I support at my day job. Like JUPITER, our supercomputers (like Eagle) aren't very photogenic because they're crammed into form-fitted buildings, and they are best photographed from the sky rather than the ground. For example, here's a photo of one of Microsoft's big GB200 supercomputers that I presented later in the week:</p>
<div class="separator" style="clear: both; text-align: center;"><figure><figcaption class="image-caption">A slide showing one of Microsoft's big GB200 supercomputers that I presented at the SuperCompCloud workshop later in the week. The big two-story building in the center houses GPUs, and the long white building on the right houses storage and CPU-only nodes.</figcaption></figure></div>
<p>JUPITER may be the first exascale system listed on Top500 that doesn't have fancy rack graphics, but I don't think it will be the last.</p>
<p>I also found myself wondering if these modular datacenters are trading short-term upsides with long-term downsides. While they accelerate deployment time for one-off supercomputers, it wasn't clear to me if these modular structures is reusable. Does the entire datacenter retire along with JUPITER after 5-7 years?</p>
<p>Hyperscalers use modular datacenters too, but the modularity is more coarse-grained to support a wider variety of systems over multiple decades. They're also physically more capacious, allowing them to deploy more CDUs and transformers per rack or row to retrofit them for whatever power and cooling demands evolve into over the full depreciation life of the datacenter building.</p>
<h3 id="hpcai-system-intersection">HPC-AI system intersection</h3>
<p>As with last year, Erich Strohmeier did a walkthrough of Top500 highlights, and he argued that "hyperscale" is defined as anything bigger than 50 MW, and therefore the Top500 list is hyperscale. It wasn't clear what value there was in trying to tie the Top500 list to hyperscale in this way, but there were a few ways in which Top500 is beginning to intersect with hyperscale AI.</p>
<p>Foremost is the way in which some exascale systems have been appearing on the list: they first appear after HPL is run on a big but partially deployed machine, then six months later, the full-system run is listed. Aurora and JUPITER both follow this pattern. What's not obvious is that many massive AI supercomputers also do something like this; for example, the Eagle system's 561 PF run was analogous to <a href="https://top500.org/lists/top500/2023/11/">Aurora's initial 585 PF run</a> or JUPITER's 793 PF run. The difference is that systems like Eagle typically enter production training after that first big tranche of GPUs is online, so there is never an opportunity to run HPL as more of the system powers up. Instead, the production training job simply expands to consume all the new GPUs as new tranches come online.</p>
<p>This iteration of the Top500 list also saw a number of bona fide commercial AI training clusters from smaller GPU-as-a-Service and "AI factory" providers post results, giving the public a view of what these systems actually look like:</p>
<ul>
<li>Nebius listed <a href="https://top500.org/system/180366/">ISEG2</a> at #13 with a 624-node, 202 PF H200 SXM cluster, following their 2023 Top500 debut with a 190-node, 46 PF H100 SXM cluster. Nebius was spun out of Yandex, the Russian tech conglomerate.</li>
<li>Northern Data Group debuted <a href="https://top500.org/system/180378/">Njoerd</a> at #26 with a 244-node H100 SXM cluster. Northern Data Group started out as a German bitcoin mining company.</li>
<li>FPT debuted at #36 with a <a href="https://top500.org/system/180399/">127-node H200 SXM cluster</a> and #38 with a <a href="https://top500.org/system/180387/">127-node H100 SXM cluster</a>. FPT is a Vietnamese technology conglomerate.</li>
</ul>
<p>It's notable that none of these systems resemble the sovereign AI systems or EuroHPC AI Factories cropping up in Europe, which are attached to traditional HPC centers and built on familiar HPC platforms like Cray EX or BullSequana. Rather, they're essentially NVIDIA reference architectures that resemble DGX SuperPods but are stamped out by companies like Supermicro, Gigabyte, and ASUS.</p>
<p>While it's nice of these GPU-as-a-Service companies to participate in the Top500 list, I did not see anyone from these companies in the technical program in any other way. And I did not see anyone from the bigger GPU-as-a-Service providers (CoreWeave, Crusoe, Lambda, etc) contributing either. Thus, while these companies are participating in Top500, it doesn't seem like they're genuinely interested in being a part of the HPC community.</p>
<h3 id="other-new-entrants">Other new entrants</h3>
<p>If you take a step back and look at the ten largest systems that made their debut at ISC'25, they broadly divide into two categories. Here's the list:</p>
<div>
<table style="border-collapse: collapse; font-family: sans-serif; font-size: 0.9em; width: 100%;">
<thead style="background-color: #f2f2f2;">
<tr>
<th style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: left;">Rank</th>
<th style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: left;">System</th>
<th style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: left;">Platform</th>
<th style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: left;">Site</th>
</tr>
</thead>
<tbody>
<tr style="background-color: white;">
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">4</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">JUPITER Booster</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">GH200</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">J√ºlich</td>
</tr>
<tr style="background-color: #f9f9f9;">
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">11</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Isambard-AI phase 2</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">GH200</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Bristol</td>
</tr>
<tr style="background-color: white;">
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">13</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">ISEG2</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">H200 SXM5</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Nebius</td>
</tr>
<tr style="background-color: #f9f9f9;">
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">15</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">ABCI 3.0</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">H200 SXM5</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">AIST</td>
</tr>
<tr style="background-color: white;">
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">17</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Discovery 6</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">GH200</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">ExxonMobil</td>
</tr>
<tr style="background-color: #f9f9f9;">
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">18</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">SSC-24</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">H100 SXM5</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Samsung</td>
</tr>
<tr style="background-color: white;">
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">26</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Njoerd</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">H100 SXM5</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Northern Data Group</td>
</tr>
<tr style="background-color: #f9f9f9;">
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">27</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">ABCI-Q</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">H100 SXM5</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">AIST</td>
</tr>
<tr style="background-color: white;">
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">33</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">AI-03</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">MI210</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Core42</td>
</tr>
<tr style="background-color: #f9f9f9;">
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">36</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">FPT AI Factory Japan</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">H200 SXM5</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">FPT</td>
</tr>
</tbody>
</table>
</div>
<p>Aside from Core42's weird MI210 cluster, every new big system was either GH200 (for traditional HPC) or H100/H200 SXM5 (for AI). This suggests a few interesting things:</p>
<ul>
<li>None of the AI cloud/GPUaaS providers are talking about GH200. It seems that GH200 is squarely for scientific computing, and Hopper HGX systems is preferred for AI at scale.</li>
<li>Despite debuting on Top500 two years ago, H100 is still making its way into the hands of HPC and AI sites. This could mean one of several things:
<ul>
<li>H100 is more affordable now (<a href="https://www.nextplatform.com/2025/05/08/supermicro-hiccups-on-hopper-pulls-40-billion-guidance-for-fiscal-2026/#:~:text=‚ÄúBut%20I%20said,say%20that.‚Äù%20%5Blaughter%5D">Jensen says he can't give them away</a>),</li>
<li>there was a huge backlog of H100 orders, or</li>
<li>it's just taking some places a really long time to get H100 up and running</li>
</ul></li>
<li>Blackwell is not relevant to HPC right now. There are no big Blackwell systems on this list, nor was Blackwell discussed in any sessions I attended during the week. This is despite large GB200 systems being public, up, and benchmarked. For example, <a href="https://github.com/mlcommons/training_results_v5.0/blob/main/IBM%2BCoreWeave%2BNVIDIA/systems/carina_ngpu2496_ngc25.04_nemo.json">CoreWeave, IBM, and NVIDIA ran MLPerf Training across 39 racks (624 nodes) of a GB200 NVL72 system named Carina just last month</a>. They did not appear to bother with HPL, though.</li>
</ul>
<p>From all this, it seems like there is a definite lag forming between what qualifies as "leadership computing" to HPC people and AI people. Today's leadership HPC (Hopper GPUs) is yesterday's leadership AI, and today's leadership AI (Blackwell GPUs) isn't on the radar of leadership HPC yet. Maybe GB200 will begin appearing one or two years later as the AI people move on to Vera-Rubin.</p>
<p>So, if I had to guess, I think the top-end of Top500 in 2027 could look like one of three things:</p>
<ol type="1">
<li>It will contain HPC systems with state-of-the-art, HPC-specific variants of accelerators that are completely irrelevant to AI. Large AI training systems will simply disappear from the list, because HPL has ceased to be a meaningful measure of their capability. GB200/GB300 simply never appear on Top500.</li>
<li>It will contain HPC systems with previous-generation Blackwell accelerators after Jensen (the chief revenue destroyer) gets on stage and tells the world that Blackwell is junk because Rubin is awesome. The AI industry gobbles up all the Rubin GPUs, and HPC picks up the scraps they leave behind.</li>
<li>Top500 starts allowing FP64 emulation, and all bets are off on how ridiculous the top systems' numbers look. In this case, top systems just skip the 1-10 exaflops range and start debuting at tens of exaflops.</li>
</ol>
<p>I have no idea where things will go, but we're starting to see <a href="https://www.nersc.gov/what-we-do/computing-for-science/doudna-system">big HPC deals</a> <a href="https://blogs.nvidia.com/blog/blue-lion-vera-rubin/">targeting Vera Rubin</a> that line up with the same time Rubin will land for the AI industry in 2H2026. So maybe Blackwell is just a hiccup, and option #1 is the most likely outcome.</p>
<h2 id="hpc-around-the-world">HPC around the world</h2>
<p>Though Blackwell's absence from Top500 was easy to overlook, China's continued absence was much more obvious. Even though no new Chinese systems have been listed in a few years now though, representatives from several Chinese supercomputing centers still contributed invited talks throughout the week.</p>
<p>In that context, I appreciated how fully ISC embraces its international scope. I found myself attending a lot of "HPC Around the World" track sessions this year, partly because I work for a multinational corporation and have to stay aware of potential needs outside of the usual US landscape. But there's also been a sharp rise in the amount of serious HPC that is now occurring outside of the USA under the banner of "sovereign AI," and I've been keen to understand how "sovereign AI" compares to the US-based AI infrastructure in which I work.</p>
<p>Before getting too deep into that though, China is worth discussing on its own since they had a such prominent presence in the ISC program this year.</p>
<h3 id="hpc-in-china">HPC in China</h3>
<p>Following the single-track opening keynote on the first day of ISC is the single-track Jack Dongarra Early Career Award Lecture, and this year's talk was given by awardee Prof. Lin Gan from Tsinghua University. In addition, Dr. Yutong Lu gave two separate talks--including the closing keynote--which shed light on the similarities and differences between how China and the US/Europe are tackling the challenges of exascale and beyond.</p>
<p>China is in a position where it does not have access to US-made GPUs, forcing them to develop their own home-grown processors and accelerators to meet their needs for leadership computing. As a result, both speakers gave talks that (refreshingly) revolved around non-GPU technologies as the basis for exascale supercomputers. Although neither Gan nor Lu revealed anything that wasn't already written about in the Gordon Bell prize papers, I took away a few noteworthy observations:</p>
<p><strong>The most public Chinese exascale system is always called the "New Sunway" or "Next Generation Sunway," never "OceanLight"</strong> as has been reported in western media. There still aren't any photos of the machine either, and Dr. Gan used stock diagrams of the predecessor Sunway TaihuLight to represent New Sunway. There was no mention of the Tianhe Xingyi/TH-3 supercomputer at all.</p>
<p><strong>Chinese leadership computing details remain deliberately obfuscated despite the openness to present at ISC.</strong> For example, Lu presented the following English-language table from the <a href="https://www.csiam.org.cn/1003/202411/2246.html">2024 China Top100 HPC list</a>:</p>
<div>
<table style="border-collapse: collapse; font-family: sans-serif; font-size: 0.75em; white-space: nowrap;">
<thead style="background-color: #f2f2f2;">
<tr>
<th style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: left;">No.</th>
<th style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: left;">Vendor</th>
<th style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: left;">System</th>
<th style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: left;">Site</th>
<th style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: left;">Year</th>
<th style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: left;">Application</th>
<th style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">CPU Cores</th>
<th style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">Linpack (Tflops)</th>
<th style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">Peak (Tflops)</th>
<th style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">Efficiency (%)</th>
</tr>
</thead>
<tbody>
<tr style="background-color: white;">
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">1</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Server Provider</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Supercomputing system mainframe system, heterogeneous many-core processor</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Supercomputing Center</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">2023</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">computing service</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">15,974,400</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">487,540</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">620,000</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">78.7</td>
</tr>
<tr style="background-color: #f9f9f9;">
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">2</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Server Provider</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Internet Company Mainframe System, CPU+GPU heterogeneous many-core processor</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Internet company</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">2022</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">computing service</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">460,000</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">208,260</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">390,000</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">53.4</td>
</tr>
<tr style="background-color: white;">
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">3</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Server Provider</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Internet Company Mainframe System, CPU+GPU heterogeneous many-core processor</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Internet company</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">2021</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">computing service</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">285,000</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">125,040</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">240,000</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">52.1</td>
</tr>
<tr style="background-color: #f9f9f9;">
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">4</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">NRCPC</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Sunway TaihuLight, 40960*Sunway SW26010 260C 1.45GHz, customized interconnection</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">NSCC-WX</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">2016</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">supercomputing center</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">10,649,600</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">93,015</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">125,436</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">74.2</td>
</tr>
<tr style="background-color: white;">
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">5</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Server Provider</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Internet Company Mainframe System, CPU+GPU heterogeneous many-core processor</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Internet company</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">2021</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">computing service</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">190,000</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">87,040</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">160,000</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">51.2</td>
</tr>
<tr style="background-color: #f9f9f9;">
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">6</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">NUDT</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Tianhe-2A, TH-IVB-MTX Cluster + 35584*Intel Xeon E5-2692v2 12C 2.2GHz + 35584 Matrix-2000, TH Express-2</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">NSCC-GZ</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">2017</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">supercomputing center</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">427,008</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">61,445</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">100,679</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">61.0</td>
</tr>
<tr style="background-color: white;">
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">7</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Server Provider</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Internet Company Mainframe System, CPU+GPU heterogeneous many-core processor</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Internet company</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">2021</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">computing service</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">120,000</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">55,880</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">110,000</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">50.8</td>
</tr>
<tr style="background-color: #f9f9f9;">
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">8</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Server Provider</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">ShenweiJing Supercomputer System, 1024*SW26010Pro heterogeneous many-core processor 390C MPE 2.1 GHz</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Computing Company</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">2022</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">scientific computing</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">399,360</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">12,912</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">14,362</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">89.9</td>
</tr>
<tr style="background-color: white;">
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">9</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Server Provider</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Supercomputing Center System, 992*SW26010Pro heterogeneous many-core processor 390C MPE 2.1 GHz</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Supercomputing Center</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">2021</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">scientific computing</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">386,880</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">12,569</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">13,913.0</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">90.3</td>
</tr>
<tr style="background-color: #f9f9f9;">
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">10</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">BSCCC/Intel</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">BSCCC T6 Section 5360*Intel Xeon Platinum 9242 homogeneous many-core processor 48C 2.3 GHz, EDR</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">BSCCC</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">2021</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">computing service</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">257,280</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">10,837</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">18,935.0</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">57.2</td>
</tr>
</tbody>
</table>
</div>
<p>The #1 system is almost definitely built on SW26010P processors just like the big New Sunway system that Gan discussed (15,974,400 cores / 390 cores per SW26010P = 40,960 nodes), but it's significantly smaller than the 39M cores on which the work Gan highlighted was run. Clearly, China's biggest systems aren't on their own Top100 list, and their #1 listed system only says its processors are "heterogeneous many-core" despite smaller entries explicitly listing SW26010P (Pro) processors.</p>
<p><strong>Chinese leadership computing struggles aren't being hidden</strong>. Lu specifically called out a "lack of a new system" in 2024, echoing earlier sentiments from other leaders in Chinese HPC who have referred to <a href="https://news.sciencenet.cn/htmlnews/2024/11/534141.shtm">"some difficulties in recent years" and a "cold winter" of HPC</a>. She also said that their leadership systems are "relatively" stable rather than trying to overstate the greatness of Chinese HPC technology. But as with above, she didn't get into specifics; by comparison, Scott Atchley (of Oak Ridge Leadership Computing Facility) specifically quoted a 10-12 hour mean time between job interrupt on Frontier after his keynote. Whether 10-12 hours is "relatively stable" remained unspoken.</p>
<p><strong>Performance portability wasn't a top-line concern despite how hard it seems to port applications to Chinese accelerators.</strong> SW26010P is weird in that it has a host core and offload cores with scratchpads, and its native programming model (Athread) is very CUDA-like as a result. Gan made it seem that China is investing a lot of effort into "fine-grained optimizations" using OpenACC and Athread, and he showed all the ways in which they're rewriting a lot of the kernels and decompositions in complex applications (like <a href="https://www.cesm.ucar.edu/models/cam">CAM</a>) to make this work. This sounds like an performance portability nightmare, yet there wasn't much talk about Chinese equivalents to performance portability frameworks like Kokkos, RAJA, or alpaka.</p>
<p>Lu did name-drop a few frameworks that unify HPC and AI performance portability from around the world:</p>
<div class="separator" style="clear: both; text-align: center;">
<figure><figcaption class="image-caption">Yutong Lu's only reference to software that enhances portability and productivity. Not quite the same as what Kokkos, Raja, and alpaka aim to solve, though.</figcaption></figure></div>
<p>However, these were more about aligning efforts across scientific computing and AI rather than enabling scientific apps to run seamlessly across China's different exascale accelerators.</p>
<p><strong>Application focus areas in China seem similar to everywhere else.</strong> Classical and quantum materials modeling, climate and ocean modeling, electronic structure calculations, and genomics were all mentioned by Gan and Lu in their talks. There was no mention of stockpile stewardship or any defense-related applications of HPC, though I'm sure China is using big supercomputers in these efforts just as US and European nations do. The only unusual application that I noticed was Gan's mention of implementing reverse time migration (RTM) on FPGAs; I've only ever heard of RTM in the context of oil exploration. Though I'm no expert, I didn't think many HPC centers spent a lot of time focusing on that technique. I do know KAUST has done some work optimizing RTM applications with Aramco in the space, but most other national supercomputing centers keep oil and gas at arm's length. Gan's RTM work may be related to earthquake modeling rather than petroleum, but it stood out nonetheless.</p>
<p><strong>Nobody talked about GPUs.</strong> Gan spent a healthy amount of time talking about applying FPGAs and NPUs to scientific problems, but these are areas of research that are on the fringes of mainstream HPC. I'm not sure if this reflected his own interests or priority research directions in China, but given that Chinese researchers cannot procure NVIDIA or AMD GPUs, perhaps FPGAs and NPUs are being pursued as a potential next-best-thing. Necessity truly is the mother of invention, and China might be the driver of a disproportionate amount of innovation around dataflow processing and reduced precision for modeling and simulation workloads.</p>
<p><strong>Nobody talked about storage either.</strong> I'm not sure if this suggests China has a lopsided interest in compute over holistic system design, or if they just talked about their biggest challenges (which are using home-grown accelerators productively). Granted, keynote speakers rarely talk about storage, but I didn't see much participation from China in any of the subsystem-specific sessions I attended either. This is particularly notable since, for a time, Chinese research labs were dominating the IO500 list with their home-made file systems. Networking was mentioned in passing in Lu's closing keynote, but not much beyond another example of technology fragmentation, and there were no specific Chinese interconnects being discussed during the week.</p>
<p><strong>China is in the thick of AI just like the rest of the world.</strong> Lu said that 30% of the cycles on their big HPC systems go to AI, which is right in line with anecdotes from other HPC sites that put their figures at <a href="https://csc.fi/en/media-release/lumis-capacity-in-high-demand-to-be-succeeded-by-an-ai-optimized-supercomputer/?utm_source=chatgpt.com">somewhere up to 50%</a>. She also presented the Chinese taxonomy of the three ways in which AI and scientific computing can mesh together: HPC for AI (training LLMs on supercomputers), HPC by AI (AI for system design and operations), and HPC and AI (AI in the loop with simulation). China is also neck-deep in figuring out how to exploit reduced precision (or "intelligent computing," as Lu branded it) and has pivoted from being "performance driven" (which I took to mean HPL-driven) to "target driven" (which I took to mean scientific outcome-driven). This is consistent with their recent Gordon Bell prize win and non-participation in either Top500 or China Top100.</p>
<p><strong>China is embracing geo-distributed supercomputing and complex workflows</strong>, much like the US. Lu specifically called out "Computility Net," a catchy name that sounded a lot like the US DOE's Integrated Research Infrastructure (IRI). She described it as a national effort to combine supercomputing with "commodity IT" resources (perhaps Chinese cloud?) to enable "resource sharing" through a "service grid." In her closing keynote, she even name-dropped IRI:</p>
<div class="separator" style="clear: both; text-align: center;"><figure><figcaption class="image-caption">The Chinese vision for Computility Net, which seems analogous to the US Integrated Research Infrastructure, as presented by Yutong Lu.</figcaption></figure></div>
<p>She did liken Computility to both IRI in the US and PRACE in the EU though, and in my mind, PRACE is nothing like IRI. Rather, PRACE is more like TeraGrid/XSEDE/ACCESS in that it federates access to HPC systems across different institutions, whereas IRI's ambition is to tightly integrate computational and experimental facilities around the country. But from the above slide, it sounds like Computility Net is closer to IRI since it is coupled to "Supercomputing internet" (akin to ESnet?) and bridging compute and data across eastern and western China.</p>
<h3 id="elsewhere-in-asia">Elsewhere in Asia</h3>
<p>Although Chinese researchers headlined a few sessions at ISC, a number of other Asian nations presented their national supercomputing strategies as well. Japan and Korea have mature, world-class HPC programs, but I was surprised to see how ambitious India has become to catch up. Smaller nations were also represented, but it was clear to me that their focus is spread across midrange HPC, partnering with large centers in Korea/Japan, and innovating around the edges of supercomputing. And perhaps unsurprisingly, every nation represented had a story around both quantum computing and artificial intelligence regardless of how modest their production modsim infrastructure was.</p>
<p><strong>India</strong> appears to rapidly catching up to the US, Europe, and Japan much in the same way China was fifteen years ago. Representatives from C-DAC, the R&amp;D organization that owns the national supercomputing mission in India, gave a far-reaching presentation about India's ambition to achieve exascale by 2030. Their current strategy appears to be broad and capacity-oriented, with forty petascale clusters spread across India for academic, industrial, and domain-specific research. They have a comprehensive, if generic, strategy that involves international collaboration in some regards, reliance on open-source software to fill out their HPC environment story, and home-grown hardware and infrastructure:</p>
<div class="separator" style="clear: both; text-align: center;"><figure><figcaption class="image-caption">India's ambitious strategy towards exascale in 2030. This slide has it all, from home-grown CPUs and networks to five systems deployed in six years.</figcaption></figure></div>
<p>I was surprised to hear about their ambitions to deploy their own CPUs and interconnect though. India is pursuing both ARM and RISC-V for their own CPUs for a future 200 PF system, and they're already deploying their "InfiniBand-like" interconnect, TRINETRA, which uses funny NICs with <a href="https://cdac.in/index.aspx?id=product_details&amp;productId=TrinetraHPCInterconnect">6x100G ports or 10x200G ports</a> rather than fewer, faster serdes. I didn't hear mention of their AI acceleration plans, but rolling their own commercialized CPU and interconnect in itself is a lot to bite off. Given that India is the world's fastest growing economy though, these plans to go from 20 PF in 2025 to 1 EF in 2030 may not be that far-fetched. Perhaps the Indian national strategy will become clearer during the inaugural <a href="https://sc-india.in">Supercomputing India 2025 conferece</a> this December.</p>
<p>The <strong>Korea Institute of Science and Technology Information</strong> also took the stage to describe their next national supercomputer, <a href="https://www.glennklockwood.com/garden/systems/KISTI-6">KISTI-6</a>, which was first announced in May 2025. It will be a 588 PF Cray EX254n system with 2,084 nodes of GH200, similar to <a href="https://www.glennklockwood.com/garden/systems/Alps">Alps</a> and <a href="https://www.glennklockwood.com/garden/systems/Isambard-AI">Isambard-AI</a>. This is quite a step up from its predecessor, which was an air-cooled KNL system, but it's unlikely it will unseat Fugaku; the 588 PF number cited appears to be the sum of 2,084 GH200 nodes, 800 Turin CPU nodes, and 20 H200 SXM5 nodes. The HPL score of its GH200 nodes will place it below <a href="https://www.glennklockwood.com/garden/systems/Alps">Alps</a> and somewhere around 350 PF, likely joining a flood of multi-hundred-petaflops GH200 systems that will appear between now and ISC26.</p>
<p><strong>Singapore (NSCC) and Taiwan (NCHC)</strong> both presented their national programs as well, but they appear to be much more nascent, and the size of their HPC infrastructure was presented as aggregate capacity, not capability. Their strategies involve partnership with Japan or Korea, but both had specific carveouts for both sovereign AI and quantum computing. Interestingly, their use cases for AI both had a strong story about training models that understood the diversity of languages and dialects represented in their nations. For example, it is not unusual for people to switch languages or dialects mid-sentence in Singapore, and the big Western models aren't designed for that reality. Similarly, Taiwan has 16 indigenous tribes with 42 dialects. It seemed like enabling LLMs that reflect the breadth languages used in Singapore and Taiwan have become the responsibility of these nations' respective national supercomputing efforts.</p>
<p>That said, that noble mission didn't seem to be matched with substantial training infrastructure; these localized models will be relying on a couple hundred GPUs here and there, wedged into existing HPC centers. Thus, these sovereign models are probably going to be fine-tuned variants of open models, aligning with my earlier observation that these smaller nations will be innovating around the edges of HPC and AI.</p>
<p><strong>What was missing?</strong> Although Vietnam, Thailand, Malaysia, and other Asian nations have strong HPC programs centered around industrial uses, they were not represented in ISC's HPC Around the World track. Also absent was any meaningful discussion around cloud; while everyone had a throwaway line about cloud in their presentations, the fact that the only big clouds in Asia are Chinese and American probably makes it unappealing to integrate them into the core of these nations' national HPC strategies. Speaking from experience, this is quite different from the attitudes of commercial HPC users across Asia who are all too happy to let someone else run HPC datacenters for them.</p>
<h3 id="the-middle-east">The Middle East</h3>
<p>Although KAUST has been a world-class HPC center in the Middle East for the past fifteen years, AI seems to be where the majority of new investment into HPC is going.</p>
<p>In describing new efforts in Saudi Arabia, Prof. David Keyes casually mentioned the Saudi HUMAIN effort, which will build 500 MW of datacenter capacity and 18,000 GB300 GPUs, after describing the Shaheen-3 GH200 upgrade that "might (barely)" put it back in the Top20 by SC'25. Similarly, Dr. Horst Simon walked through a few of Abu Dhabi's university clusters (each having dozens of GPU nodes) after skating through an announcement that a 5 GW AI campus was also being built in Abu Dhabi. The gap between investment in AI and investment in HPC was striking.</p>
<p>I also had a brief conversation with someone from one of the major Abu Dhabi universities, and I was very surprised to find that I was talking to a real AI practitioner--not an HPC person moonlighting in AI--who spoke at the same depth as the customers with whom I work in my day job. The nature of his work made it clear to me that, despite his university not having a Top500 system, he was familiar with running training and inference at scales and with sophistication that is far beyond the experience of most ISC attendees.</p>
<p>These interactions led me to the conclusion that the Middle East's approach to "sovereign AI" is quite different from Europe's. Rather than building HPC systems with GPUs, letting HPC centers operate them, and calling them sovereign AI platforms, nations like Saudi Arabia and UAE are keeping HPC and AI separate. Like in the US, they are going straight to hyperscale with AI, and they have no preconceived notion that anything resembling a supercomputer must be hosted at a supercomputer center.</p>
<p>Of course, only nations like Saudi Arabia and UAE can afford to do this, because they have trillion-dollar sovereign wealth funds to invest in massive infrastructure buildout that doesn't isn't contingent on public consensus or the latest election cycle. Just as UAE's Core42 can build a 5 GW datacenter campus with little oversight, these nations can easily mis-step and invest a ton of money in an AI technology that turns out to be a flop. In the end, it seems like these Middle Eastern nations are willing to take bigger risks in how they build out their sovereign AI infrastructure, because they are largely starting from a blank sheet of paper. They aren't limiting themselves to 20 MW supercomputers like the HPC world had.</p>
<p>All things being equal, this might turn out to be an advantage over other nations who are more hesitant to deviate from the tried-and-true course of buying a Cray or a Bull, sticking some GPUs in it, and calling it AI. If these Middle Eastern nations do everything right, they stand to get a lot further and move a lot faster in sovereign AI than Europe, and it'll be fascinating to see how quickly they catch up with the sort of frontier AI research being done private industry. But, as with the US AI industry, it doesn't seem like these AI practitioners are going to be attending ISC in the same way European sovereign AI folks do; the roads of HPC and AI seem to run parallel without intersecting in the Middle East.</p>
<h2 id="exhibitors">Exhibitors</h2>
<p>ISC had a <a href="https://isc-hpc.com/the-isc-2025-exhibition-sets-new-records/">record number of exhibitors this year</a>, and as usual, I tried to set aside at least an hour or two to walk the floor and see what technologies are on the horizon. This year, though, the exhibit hall was not a great representation of the rest of the conference. Everyone I talked to about the exhibit said one of two things:</p>
<ol type="1">
<li>There are a LOT of quantum companies.</li>
<li>A lot of big companies were noticeably absent.</li>
</ol>
<p>It also didn't feel like the biggest exhibit ever, partially because of #2, and partially because many of the exhibitors--one in five--was exhibiting for the first time this year. This meant a lot of the booths were small and barebones, and many of them belonged to either companies at the periphery of HPC (such as companies that make dripless couplers for liquid cooling) or small startups who just had a desk, a few pens, and some brochures.</p>
<p>On the first point, it was true--quantum computing was well represented, with 22% of exhibitors identifying as being involved in the field in some form. In fact, quantum felt over-represented, since the ISC technical program certainly didn't have such a large fraction of talks on quantum computing topics. I didn't have time to actually talk with any of these quantum companies though, so wasn't able to get a sense of why the startup ecosystem around quantum computing was so rich in Europe as compared to the US.</p>
<p>While there was an abundance of quantum this year, a number of the big HPC and HPC-adjacent companies were noticeably absent:</p>
<ul>
<li>Amazon, Azure, and Google did not have booths despite having booths last year. Amazon and Google still sponsored the conference at the lowest tier (bronze) though, while Microsoft did not sponsor at all.</li>
<li>Intel had neither booth nor sponsorship despite having the #3 system on Top500. I don't think they held a party this year, either. AMD didn't have a booth, but they sponsored (and gave the opening keynote!)</li>
<li>WEKA neither had a booth nor sponsored the conference this year, although they were the leading sponsor of the Student Cluster Competition. Competitors DDN, VAST, Quobyte, and BeeGFS all had booths, but only VAST sponsored. Curiously, Pure and Scality, which do not big footholds in leadership HPC, did both booths and sponsorship.</li>
</ul>
<p>These companies who chose not to have a booth still sent people to the conference and were conducting meetings as usual, though. This suggests that there's something amiss with how large companies perceive the return on investment of having a booth at ISC. I don't have any insider knowledge here, but I was surprised by the pullback since ISC has historically been very good at incentivizing attendees to walk through the expo hall by putting it between the technical sessions and the food breaks.</p>
<p>As I walked the exhibit floor, I found that prominent booths spanned the whole HPC stack: software, system integrators, component makers (CPUs, GPUs, HBM and DDR, and SSD and HDD), and datacenter infrastructure were all exhibiting. The most eye-catching booths were those with big iron on display: HPE/Cray had a full EX4000 cabinet and CDU on display, and there were a few Eviden BullSequana nodes floating around.</p>
<div class="separator" style="clear: both; text-align: center;"><figure>
<figcaption class="image-caption">The Cray EX4000 cabinet (right) and its CDU (left) on display at the ISC'25 exhibition hall. One of the most eye-catching displays, even they've been on display at ISC and SC for a few years now.</figcaption></figure></div>
<p>Sadly, though, there were no full BullSequana X3000 racks on display. I've still never seen one in real life.</p>
<p>Infrastructure companies like Motivair (who manufactures the CDUs for Cray EX) and Rittal (which I know as a company that manufactures racks) also had big liquid-liquid head exchangers on display with shiny steel piping. Here's a smaller version of the Cray EX CDU that Motivair was displaying:</p>
<div class="separator" style="clear: both; text-align: center;"><figure><figcaption class="image-caption">A close-up view of a smaller liquid-liquid heat exchanger CDU on display at the Motivair booth right next to HPE's. Strangely, the mechanics of these systems dovetails with what I've learned as a part of my other hobby outside of HPC, which is operating a multi-family residential high-rise.</figcaption></figure></div>
<p>I got to chatting with some good folks at Motivair, and I learned that the 1.2 MW variant that is used with Cray EX has a 4" connection--the same size as the water main in <a href="https://glennklockwood.com/garden/LRA">my coop</a>. Since I recently helped with the replacement of my building's water main, this led me down a rabbithole where I realized that the flow rates for this CDU is roughly the same as my apartment building too, which is to say, a single Cray CDU moves as much fluid as a 55-unit apartment building. Incidentally, a single Cray EX cabinet supports roughly the same electrical capacity as my 55-unit building too--I am in the process of replacing our 1,200 A service panel, which comes out to about the same 400 kVA as fully loaded EX.</p>
<p>Aside from the Cray cabinets and CDUs, which are no longer new to ISC, I couldn't put my finger on any particularly outstanding booths this year though. The exhibit felt like a sea of smaller companies, none of which really grabbed me. This isn't to say that big vendors were wholly absent though. Despite not having booths, all three big cloud providers threw parties during the week: AWS and NVIDIA teamed up on a big party with over a thousand registrants, while Google and Microsoft held smaller parties towards the end of the week. HPE also threw a lovely event that was off the beaten path along the Elbe, resulting in a less-crowded affair that made it easy to catch up with old friends.</p>
<p>I may be reading too much into this year's exhibit, but it felt like ISC might be transforming into an event for smaller companies to gain visibility in the HPC market, while larger companies apply their pennies only in the parts of the conference with the highest return. Whether a company chose to have a booth, sponsor the conference, and/or throw a party seemed to defy a consistent pattern though, so perhaps other factors were at play this year.</p>
<h2 id="cloud-or-lack-thereof">Cloud, or lack thereof</h2>
<p>Because I work for a large cloud service provider, I attended as many cloud HPC sessions as I could, and frankly, I was disappointed. The clear message I got by the end of the week was that Europe--or perhaps just ISC--doesn't really care about the cloud. This is quite different from the view in the US, where the emergence of <a href="https://www.glennklockwood.com/garden/systems/Eagle">massive AI supercomputers</a> has begun to shift opinions to the point where <a href="https://www.theregister.com/2024/07/24/oak_ridge_discovery/">the successor to the Frontier supercomputer at OLCF might wind up in the cloud</a>. I suppose cloud is a lot less attractive outside of the US, since all the major cloud providers are US corporations, but the way in which cloud topics were incorporated into the ISC program this year sometimes felt like a box-checking exercise.</p>
<p>For example, I attended the BOF on "Towards a Strategy for Future Research Infrastructures" which I expected to be a place where we discussed the best ways to integrate traditional HPC with stateful services and other workflow components. While cloud was mentioned by just about every panelist, it was almost always in a throwaway statement, lumped in with "the edge" or cited as a vague benefit to "new workflows and interactive analysis" with no further detail. One speaker even cited egress fees as a big challenge which, to me, means they haven't actually talked to a cloud provider in the last five to ten years. If egress fees are what stop you from using the cloud, you're talking to the wrong account team.</p>
<p>I get it though; there are times where cloud often doesn't offer enough obvious benefit for HPC to justify the effort required to figure it out. In those cases, it's incumbent on cloud providers to provide a better story. But I was also disappointed by the invited session called "Bridging the Gap: HPC in the Cloud and Cloud Technologies in HPC," which I hoped would be the place where cloud providers could make this case. Instead, only two of the three CSPs were even invited to speak, and it was clear that the speakers did not all get the same assignment with their invitations. Granted, the CSP for whom I work was the one not invited (so I came in a little biased), but I was surprised by how differently each speaker used their time.</p>
<p>Dr. Maxime Martinasso from CSCS gave a talk from the perspective of trying to add cloud-like capabilities to a supercomputer, which is a recurring pattern across a number of sites (including many in the US DOE) and projects. He explained the way they're creating an infrastructure-as-code domain-specific language that sits on top of <a href="https://www.glennklockwood.com/garden/systems/Alps">Alps</a>, their Cray EX system, to give users the ability to bring their own software stacks (all the way down through Slurm) to the supercomputer. It was clearly a ton of work on CSCS's part to develop this capability, and yet the talk's "future work" slide contained a bunch of features which those of us in the cloud would consider "P0"--priority zero, or essential for a minimum viable product.</p>
<p>By the end of Martinasso's talk, I realized that CSCS's perspective is that, unlike commercial cloud, these cloudy features aren't P0; having a supercomputer on the floor is. He made the case that CSCS has a need to explore diverse computing architectures and accelerators (as evidenced by the five different node types in Alps!), and putting them all on a single RDMA fabric isn't something any cloud provider will do. As a result, adding any new cloud-like capability to the heterogeneous supercomputer is just gravy, and the fact that true cloud is more "cloudy" than Alps is irrelevant since the cloud will never support the intra-fabric heterogeneity that Alps does.</p>
<p>The other two speakers represented big cloud providers, and their talks had a bit more product pitch in them. One speaker talked through the challenges the cloud is facing in trying to fold supercomputing principles into existing cloud infrastructure (a theme I repeated in my talk later in the week) before talking about specific products that have arisen from that. It touched on some interesting technologies that the HPC world hasn't yet adopted (like optical circuit switching--super cool stuff for programmable fabrics), and I learned a few things about how that provider might bring new HPC capabilities to the table for specific workloads.</p>
<p>The other speaker, though, presented a textbook pitch deck. I've give almost the same exact presentation, down to showing the same sort of customer stories and product comparison tables, during customer briefings. Execs in the audience would eat it up while engineers' eyes would glaze over, and having to do that song and dance is partly why I didn't make it as a product manager. <a href="https://bsky.app/profile/glennklockwood.com/post/3lrd3usnt222d">I was incredulous</a> that such a presentation was an invited talk at one of the most prestigious HPC conferences in the world.</p>
<p>This is not to say I was mad at the speaker. He did exactly what one would expect from a leader in the sales side of an organization, hitting all the notes you'd want in a textbook pitch aimed at the C-suite. Rather, I was disappointed by the choice by the session organizers; when you invite someone whose job is driving business at one of the largest cloud providers to speak, you should fully expect a broad and salesy presentation. I don't think it's a stretch to say that most ISC attendees aren't looking for these sorts of high-level talks designed for enterprise decision-makers; they want insight and technical depth.</p>
<p>Was I miffed that a competitor got to give a twenty-minute sales pitch during a session at which I wasn't invited to speak? Absolutely. And do I think I could've given a talk that even the most ardent cloud-hater would find something interesting in it? Probably. But since that didn't happen, the best I can do is complain about it on the Internet and hope that next year's program committee puts more care into organizing an invited speaker session on cloud and HPC.</p>
<p>Thankfully, I was given the opportunity to talk a little about my work at the <a href="https://sites.google.com/view/supercompcloud/isc25-9th-supercompcloud-workshop#h.fur7sdv6h19a">SuperCompCloud workshop</a> on Friday. That workshop felt like what the "Bridging the Gap" invited session should've been, and there were roughly equal parts of presentations on adding cloud-like features to their HPC infrastructure and adding HPC-like features to cloud infrastructure. From my perspective, the workshop was great; I got to see how traditional HPC centers are adopting cloud practices into their operations, and I could explain how we overcame some of the challenges they're facing in Azure. But to my point at the outset of this section--that Europe doesn't really care about the cloud--the majority of speakers at SuperCompCloud were American.</p>
<h2 id="parting-thoughts">Parting thoughts</h2>
<p>As I said at the outset, there were way more sessions that I missed than I attended. In addition, a lot of the big headlines of the week were coincident with, not made at, the conference. A few noteworthy announcements during the week that I won't go into detail about include:</p>
<ol type="1">
<li><a href="https://www.ed.ac.uk/news/university-set-to-host-ps750m-national-supercomputer">¬£750M was awarded to EPCC</a> to deploy what sounds like the UK's first exascale system. This announcement's overlap with ISC was a total coincidence, so EPCC didn't have many details to share.</li>
<li><a href="https://ultraethernet.org/ultra-ethernet-consortium-uec-launches-specification-1-0-transforming-ethernet-for-ai-and-hpc-at-scale/">The Ultra Ethernet Consortium announced the long-awaited version 1 of its spec</a>. I'm not sure how relevant this is to HPC yet, but given how many networking talks compared themselves against InfiniBand, I think there's a lot of appetite for a high-performance, non-proprietary alternative.</li>
<li>Sadly, <a href="https://www.hpcwire.com/2025/06/11/farwell-hpc-guru/">HPC_Guru announced his retirement</a> mid-week as well. It's not clear this was deliberately timed with ISC, but it was acknowledged on the big stage during the ISC closing statements and resulted in a lot of <a href="https://bsky.app/profile/hpcguru.bsky.social/post/3lrcsdbwa522c">recognition</a> <a href="https://x.com/hpc_guru/status/1932688759310725425?s=61">online</a>. I credit HPC_Guru, whoever he is, with a lot of the success I've enjoyed in my career, as he amplified my voice as far back as 2009 when I first started on Twitter. Maybe with his retirement, I should try to do for others what he did for me.</li>
</ol>
<p>And along the lines of reflecting back over the years, this was ISC's 40th anniversary, and the organizers had a few wonderful features to commemorate the milestone. Addison Snell organized a panel where a variety of attendees got to discuss the impact that the conference has had on them over the past 40 years, and I was delighted to find that I was not the only person to <a href="https://glennklockwood.com/garden/ISC-conference#isc-40th-anniversary-panel">reflect back on how ISC has shaped my career</a>. As critical as I can be of specific speakers and sessions when I write up these notes, I do hope it goes without saying that I wouldn't bother doing all this for a conference that wasn't deeply engaging and rewarding to be a part of.</p>
<p>Going back to this year's theme of connecting the dots, I think it's apt. Some ways in which HPC connected dots at ISC this year were obvious; the conference brought together people with a common interest in high-performance computing from across 54 countries and seven continents this year. But this year's conference also made it clear that the role of HPC going forward may be connecting the dots between different technologies being developed for AI, cloud, enterprise, and other markets and the problems in scientific computing that need to be solved.</p>
<p>The latest and greatest Blackwell GPUs barely registered at ISC this year, and the HPC community seems OK with that now. Instead of the focus being on the absolute top-end in high-performance accelerators, HPC's focus was on connecting the dots between last generation's GPUs and today's grand challenges in science. Instead of showcasing the newest innovations in secure computing in the cloud, HPC's focus was in connecting the dots between a few relevant pieces of zero trust and big-iron on-prem supercomputers.</p>
<p>HPC has always been about figuring out ways to use stuff invented for someone else to solve scientific challenges--connecting the dots. Beowulf clusters started that way, GPGPU computing started that way, and emulating DGEMMs (and other primitives) on AI accelerators will probably follow the same pattern. But different nations are drawing different lines between the dots; while the US might draw a shorter line between commercial cloud and HPC at scale, Europe is drawing shorter lines between HPC for scientific computing and HPC for sovereign AI.</p>
<p>If we accept that connecting the dots may be where the HPC community can make the most impact, then it's fitting that ISC chose to carry forward the theme of "connecting the dots" into ISC'26. This break from the tradition of introducing a new tagline each year suggests that, at times, optimizing what we already have can take us further than than pursuing something completely new. After 40 years, ISC remains not only a showcase of innovation, but a reflection of how the HPC community (and its role in the technology landscape) is evolving. If we continue to embrace this theme of stitching together breakthroughs instead of spotlighting them individually, the HPC community is likely to be more relevant than ever alongside--not in spite of--the overwhelming momentum of hyperscale and AI.</p>]]></content><author><name>Glenn K. Lockwood&apos;s Blog</name></author><category term="glennklockwood" /><summary type="html"><![CDATA[I had the pleasure of attending the 40th annual ISC High Performance conference this month in Hamburg, Germany. It was a delightful way to take the pulse of the high-performance computing community and hear what the top minds in the field are thinking about.]]></summary></entry><entry><title type="html">Surfing the Singularity- Adventures in Quantum Chemistry</title><link href="https://hpc.social/personal-blog/2025/surfing-the-singularity-adventures-in-quantum-chemistry/" rel="alternate" type="text/html" title="Surfing the Singularity- Adventures in Quantum Chemistry" /><published>2025-03-11T13:11:00-06:00</published><updated>2025-03-11T13:11:00-06:00</updated><id>https://hpc.social/personal-blog/2025/surfing-the-singularity-adventures-in-quantum-chemistry</id><content type="html" xml:base="https://hpc.social/personal-blog/2025/surfing-the-singularity-adventures-in-quantum-chemistry/"><![CDATA[<div class="separator" style="clear: both; text-align: center;"><br /></div>
<p><br />&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;/div&gt;
<br />&lt;div class="separator" style="clear: both; text-align: center;"&gt;<br />&lt;/div&gt;</p>
<p>In this installment of the Surfing the Singularity blog we go vlog, giving an overview of quantum computing today with application to chemistry. Quantum computing is rapidly advancing, with improvements in machine size, error correction, and scalability. And yet, there's always a desire to drive towards advancements and scientific applications which are just out of reach of today's technologies. New algorithms lead the way.&nbsp;</p>
<p>In this video, will give a brief overview of quantum computing, what it means, where we are on the product roadmaps, and explore an emergent algorithm for pushing the boundaries of chemical modeling beyond what is possible with today's classical machines. Enjoy.&nbsp;</p>
<p>- andy&nbsp;</p>
<p><br /></p>
<p>P.S. Begging your forgiveness for being a YouTube newb...&nbsp;</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;"><br /></div>
<p><br />&lt;p&gt;&lt;/p&gt;</p>]]></content><author><name>Surfing the Singularity</name></author><category term="surfthesing" /><summary type="html"><![CDATA[&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;/div&gt; &lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;/div&gt; In this installment of the Surfing the Singularity blog we go vlog, giving an overview of quantum computing today with application to chemistry. Quantum computing is rapidly advancing, with improvements in machine size, error correction, and scalability. And yet, there's always a desire to drive towards advancements and scientific applications which are just out of reach of today's technologies. New algorithms lead the way.&nbsp; In this video, will give a brief overview of quantum computing, what it means, where we are on the product roadmaps, and explore an emergent algorithm for pushing the boundaries of chemical modeling beyond what is possible with today's classical machines. Enjoy.&nbsp; - andy&nbsp; P.S. Begging your forgiveness for being a YouTube newb...&nbsp; &lt;p&gt;&lt;/p&gt;]]></summary></entry></feed>