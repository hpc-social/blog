<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en-US"><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="https://hpc.social/personal-blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://hpc.social/personal-blog/" rel="alternate" type="text/html" hreflang="en-US" /><updated>2024-12-16T20:13:02-07:00</updated><id>https://hpc.social/personal-blog/feed.xml</id><title type="html">hpc.social - Aggregated Personal Blog</title><subtitle>Shared personal experiences and stories</subtitle><author><name>hpc.social</name><email>info@hpc.social</email></author><entry><title type="html">SC’24 recap</title><link href="https://hpc.social/personal-blog/2024/sc-24-recap/" rel="alternate" type="text/html" title="SC’24 recap" /><published>2024-12-02T07:30:00-07:00</published><updated>2024-12-02T07:30:00-07:00</updated><id>https://hpc.social/personal-blog/2024/sc-24-recap</id><content type="html" xml:base="https://hpc.social/personal-blog/2024/sc-24-recap/"><![CDATA[<p>The premiere annual conference of the high-performance computing community, SC24, was held in Atlanta last week, and
    it attracted a record-shattering number of attendees--<a href="https://www.hpcwire.com/2024/11/20/sc24-half-way-there/">nearly 18,000 registrants</a>, up 28% from last
    year! The conference <i>felt</i> big as well, and there seemed to be a lot more running between sessions, meetings,
    and the exhibition floor. Despite its objectively bigger size though, the content of the conference felt more diffuse this year, and I was left wondering if this reflected my own biases or was a real effect of the AI industry
    beginning to overflow into AI-adjacent technology conferences like SC.</p>
<div class="separator" style="clear: both; text-align: center;"><figure>
</figure></div>
<p></p>
<p>Of course, this isn't to say that SC24 was anything short of a great conference. Some exciting new technologies were
    announced, a new supercomputer beat out Frontier to become the fastest supercomputer on the Top500 list, and I got
    to catch up with a bunch of great people that I only get to see at shows like this. I'll touch on all of these
    things below. But this year felt different from previous SC conferences to me, and I'll try to talk about that too.</p>
<p>There's no great way to arrange all the things I jotted down in my notes, but I've tried to arrange them by what readers may be interested in. Here's the table of contents:</p>
<p></p>
<ol>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#approach">My approach to SC this year</a></li>
<li>
<a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#tech">New technology and announcements</a>
<ol>
<li>
<a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#tech-top500">Top500 and a new #1 system</a>
<ol>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#tech-top500-elcap">#1 - El Capitan</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#tech-top500-hpc6">#5 - Eni HPC6</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#tech-top500-softbank">#16 and #17 - SoftBank CHIE-2 and CHIE-3</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#tech-top500-jeti">#18 - Jülich's JUPITER Exascale Transition Instrument (JETI)</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#tech-top500-reindeer">#32 - Reindeer!</a></li>
</ol>
</li>
<li>
<a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#tech-expo">Technology on the exhibit floor</a>
<ol>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#tech-expo-gb200">GB200</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#tech-expo-ss400">Slingshot 400</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#tech-expo-gg">Grace-Grace for storage?</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#tech-expo-hbv5">Microsoft and AMD's new HBM CPU</a></li>
</ol>
</li>
</ol>
</li>
<li>
<a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#industry">The HPC industry overall</a>
<ol>
<li>
<a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#industry-attendee">What I learned about the average SC technical program attendee</a>
<ol>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#industry-attendee-sustainability">People think sustainability and energy efficiency are the same thing</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#industry-attendee-ai">AI sessions are really scientific computing sessions about AI</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#industry-attendee-ops">AI for operations is not yet real in scientific computing</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#industry-attendee-hyperscale">Some are beginning to realize that HPC exists outside of scientific computing</a></li>
</ol>
</li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#industry-nsf">NSF's broad front vs. DOE's big bets in HPC and AI</a></li>
<li>
<a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#industry-expo">Exhibitor trends</a>
<ol>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#industry-expo-booths">Booths by the numbers</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#industry-expo-gpuaas">Proliferation of GPU-as-a-Service providers</a></li>
</ol>
</li>
</ol>
</li>
<li>
<a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#community">Community and connections</a><ol>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#community-people">Getting to know people</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#community-career">Talking to early career people</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#community-bsky">Shift in social media</a></li>
</ol>
</li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#conclusion">So what's the takeaway?</a></li>
</ol>
<p>Before getting into the details though, I should explain how my perspective shaped what I noticed (and missed) through the conference. And to be clear: <b><i><span style="color: #cc0000;">these are my own personal opinions and do not necessarily reflect those of my employer</span></i></b>. Although Microsoft covered the cost for me to attend SC, I wrote this blog post during my own free time over the Thanksgiving holiday, and nobody had any editorial control over what follows except me.</p>
<p></p>
<h2 id="approach">My approach to SC this year</h2>
<p>Although this is the eleventh SC conference I've attended, it was the first time that I:</p>
<p></p>
<ol>
<li>attended as a <a href="https://blog.glennklockwood.com/2024/08/how-has-life-after-leaving-labs-been.html#hpc-ai-development">practitioner
            of hyperscale AI</a> rather than traditional HPC and scientific computing</li>
<li>attended as a Microsoft engineer (I represented Microsoft as a <a href="https://blog.glennklockwood.com/2024/08/how-has-life-after-leaving-labs-been.html#storage-product-management">product manager</a> at
        SC22 and SC23)</li>
<li>did not attend SC as a designated storage person (since 2013)</li>
</ol>
<p>Because of these changes in my <b><span style="color: #990000;">identity</span></b> as an attendee, I approached the
    conference with a different set of <b><span style="color: #0b5394;">goals</span></b> in mind:</p>
<p>As a <b><span style="color: #990000;">hyperscale/AI person</span></b>, I felt that I should
    prioritize <b><span style="color: #0b5394;">attending all the cloud and AI sessions</span></b> whenever forced to choose between one session or another. I chose to focus on understanding the traditional HPC community's understanding of hyperscale and AI, which meant I had to spend less time in the workshops, panels and BOFs where I built my career.</p>
<p>As an <b><span style="color: #990000;">engineer</span></b> rather than a product manager,
    it wasn't my primary responsibility to run private briefings and gather HPC customers' requirements and feedback. Instead, I prioritized only those meetings where my first-hand
    knowledge of how massive-scale AI training works could have a meaningful impact. This meant I <b><span style="color: #0b5394;">focused on partners and practitioners who also operate in the realm of
            hyperscale</span></b>--think massive, AI-adjacent companies and the HPC centers who have historically
    dominated the very top of the Top500 list.
</p>
<p>One thing I didn't anticipate going into SC24 is that I've inherited a third identity: there are a new cohort of people in HPC who see me as a <b><span style="color: #990000;">long-time community
            member</span></b>. This resulted in a surprising amount of my time being spent <b><span style="color: #0b5394;">talking to students and early career practitioners</span></b> who were looking
    for advice.</p>
<p>These three identities and goals meant I don't many notes to share on the technical program, but I did capture more observations about broader trends in the HPC industry and community.</p>
<h2 id="tech">New technology and announcements</h2>
<div>HPC is all about cutting-edge technology, so that's a fine place to start talking about what was new.</div>
<h3 id="tech-top500">Top500 and a new #1 system</h3>
<p>A cornerstone of every SC conference is the release of the new Top500 list on Monday, and
    this is especially true on years when a new #1 supercomputer is announced. As was widely anticipated in the weeks
    leading up to SC24, El Capitan unseated Frontier as the new #1 supercomputer this year, posting an impressive <a href="https://www.top500.org/system/180307/">1.74 EFLOPS</a> of FP64. In addition though, Frontier grew a
    little (it added 400 nodes), there was a notable new #5 system (Eni's HPC6), and a number of smaller systems appeared that are worth calling
    out.</p>
<h4 id="tech-top500-elcap">#1 - El Capitan</h4>
<p>The highlight of the Top500 list was undoubtedly the debut of El Capitan, Lawrence
    Livermore National Laboratory's massive new MI300A-based exascale supercomputer. Its 1.74 EF score resulted from a
    105-minute HPL run that came in under 30 MW, and a bunch of technical details about the system were disclosed by
    Livermore Computing's CTO, Bronis de Supinski, during an invited talk during the Top500 BOF. Plenty of others
    summarize the system's speeds and feeds (e.g., see <a href="https://www.nextplatform.com/2024/11/18/el-capitan-supercomputer-blazes-the-trail-for-converged-cpu-gpu-compute/">The
        Next Platform's article on El Cap</a>), so I won't do that. However, I will comment on how unusual Bronis' talk
    was.</p>
<p>Foremost, the El Capitan talk seemed haphazard and last-minute. Considering the system took over half a decade of planning and cost at least half a
    billion dollars, El Capitan's unveiling was the most unenthusiastic description of a brand-new #1 supercomputer I've
    ever seen. I can understand that the Livermore folks have debuted plenty of novel #1 systems in their careers, but El
    Capitan is objectively a fascinating system, and running a full-system job for nearly two hours across first-of-a-kind APUs
    is an amazing feat. If community leaders don't get excited about their own groundbreaking achievements, what kind of message should the next generation of HPC professionals take home?</p>
<p>In sharp contrast to the blasé announcement of this new system was the leading slide that was presented to describe the speeds and feeds of El Capitan:</p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>I've never seen a speaker take the main stage and put <i>a photo of himself</i> literally in the center of the slide, in front of the supercomputer they're talking about. I don't know what the communications people at Livermore were trying to do with this graphic, but I don't think it
    was intended to be evocative of the first thing that came to my mind:</p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>The supercomputer is literally named "The Captain," and there's a photo of one dude (the boss of Livermore Computing,
    who is also standing on stage giving the talk) blocking the view of the machine. It wasn't a great look, and it left me feeling very uneasy about what I was witnessing and what message it was sending to the HPC community.</p>
<p>In case it needs to be said, HPC is a team sport. The unveiling of El Capitan (or any other #1 system
    before it) is always the product of dozens, if not hundreds, of people devoting years of their professional lives to
    ensuring it all comes together. It was a big miss, both to those who put in the work, and those who will have
    to put in the work on future systems, to suggest that a single, smiling face comes before the success of the system deployment.
</p>
<h4 id="tech-top500-hpc6">#5 - Eni HPC6</h4>
<p>The other notable entrant to the Top 10 list was HPC6, an industry system deployed by Eni (a major Italian energy
    company) built on MI250X. Oil and gas companies tend to be conservative in the systems they buy since the seismic
    imaging done on their large supercomputers informs hundred-million to billion-dollar investments in drilling a new
    well, and they have much less tolerance for weird architectures than federally funded leadership computing does.
    Thus, Eni's adoption of AMD GPUs in this #5 system is a strong endorsement of their capability in mission-critical
    commercial computing.</p>
<h4 id="tech-top500-softbank">#16 and #17 - SoftBank CHIE-2 and CHIE-3</h4>
<p>SoftBank, the Japanese investment conglomerate who, among other things, owns a significant stake in Arm, made its <a href="https://www.top500.org/site/51045/">Top500 debut with two identical 256-node DGX H100 SuperPODs</a>. While
    not technologically interesting (H100 is getting old), these systems represent significant investment in HPC by
    private industry in Japan and signals that SoftBank is following the lead of large <a href="https://www.nytimes.com/2023/08/16/technology/ai-gpu-chips-shortage.html">American investment groups in
        building private AI clusters for the AI startups in their portfolios</a>. In doing this, SoftBank's investments
    aren't dependent on third-party cloud providers to supply the GPUs to make these startups successful and reduces
    their overall risk.</p>
<p>Although I didn't hear anything about these SoftBank systems at the conference, NVIDIA issued a press statement
    during the NVIDIA AI Summit Japan during the week prior to SC24 that discussed <a href="https://nvidianews.nvidia.com/news/nvidia-and-softbank-accelerate-japans-journey-to-global-ai-powerhouse">SoftBank's
        investment in large NVIDIA supercomputers</a>. The press statement states that these systems will be used "for
    [SoftBank's] own generative AI development and AI-related business, as well as that of universities, research
    institutions and businesses throughout Japan." The release also suggests we can expect B200 and GB200 SuperPODs from
    SoftBank to appear as those technologies come online.</p>
<h4 id="tech-top500-jeti">#18 - Jülich's JUPITER Exascale Transition Instrument (JETI)</h4>
<p>Just below the SoftBank systems was the precursor system to Europe's first exascale system. I was hoping that
    JUPITER, the full exascale system being deployed at FRJ, would appear in the Top 10, but it seems like we'll have to
    wait for ISC25 for that. Still, the JETI system ran HPL across 480 nodes of BullSequana XH3000, the same node that
    will be used in JUPITER, and achieved 83 TFLOPS. By comparison, the full JUPITER system will be over 10x larger ("<a href="https://www.fz-juelich.de/en/ias/jsc/jupiter/tech">roughly 6000 compute nodes</a>" in the Booster), and
    projecting the JETI run (173 TF/node) out to this full JUPITER scale indicates that JUPITER should just squeak over
    the 1.0 EFLOPS line.</p>
<p>In preparation for JUPITER, Eviden had a couple of these BullSequana XH3000 nodes out on display this year:</p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>And if you're interested in more, I've been tracking the technical details of <a href="https://glennklockwood.com/garden/systems/jupiter">JUPITER in my digital garden</a>.</p>
<h4 id="tech-top500-reindeer">#32 - Reindeer!</h4>
<p>Waay down the list was Microsoft's sole new Top500 entry this cycle, an NVIDIA H200 system that ran HPL over 120 ND
    H200 v5 nodes in Azure. It was one of only two conventional (non-Grace) H200 clusters that appeared in the top 100,
    and it had a pretty good efficiency (Rmax/Rpeak &gt; 80%). Microsoft also had a Reindeer node on display at its
    booth:</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>An astute observer may note that this node looks an awful lot like the H100 node used in its Eagle supercomputer,
    which was <a href="https://blog.glennklockwood.com/2023/11/sc23-recap.html">on display at SC23 last year</a>. That's
    because it's the same chassis, just with an upgraded HGX baseboard.</p>
<p>Reindeer was not <i>super</i> exciting, and there were no press releases about it, but I mention it here for a couple
    reasons:</p>
<p></p>
<ul>
<li>One of my teammates did the HPL run and submission, and his group got to come up with the name of the system for
        the purposes of HPL. As it turns out, generating a public name for a Top500 submission involves a comical amount
        of legal and marketing process when it comes from a giant corporation like Microsoft. And as it turns out,
        naming a cluster "Reindeer" has a low probability of offending anyone.</li>
<li>Reindeer is pretty boring--it's a relatively small cluster with a bunch of GPUs. But when you're building out AI
        infrastructure at a pace of <a href="https://build.microsoft.com/en-US/sessions/984ca69a-ffca-4729-bf72-72ea0cd8a5db">5x Eagles (70,000
            GPUs!) per month</a>, you want the clusters that those GPUs go into to be as boring, predictable, and
        automatable as possible. Seeing as how Reindeer only used 960 GPUs but still got #32, it doesn't require much
        math to realize that the big hyperscalers could flood the Top500 list with these cookie-cutter GPU clusters and
        (in this case) make any ranking below #32 completely irrelevant. Heaven help the Top500 list if they ever
        publish an API for submitting new systems; cloud providers' build validation automation could tack a Top500
        submission on at the end of burn-in and permanently ruin the list.</li>
</ul>
<div>On a personal note, the supercomputer grant that gave me my first job in the HPC business <a href="https://www.top500.org/system/177455/">debuted at #48</a>. It's mind-boggling that I now work in a place
    where standing up a #32 system is just day-to-day business.</div>
<p></p>
<h3 id="tech-expo">Technology on the exhibit floor</h3>
<p>The exhibit floor had a few new pieces of HPC technology on display this year that are
    worthy of mention, but a lot of the most HPC-centric exciting stuff actually had a soft debut at <a href="https://blog.glennklockwood.com/2024/05/isc24-recap.html">ISC24 in May</a>. For example, even though SC24 was MI300A's big splash due to
    the El Capitan announcement, some MI300A nodes (such as the <a href="https://glennklockwood.com/garden/nodes/cray-ex255a">Cray EX255a</a>) were on display in Hamburg. However,
    Eviden had their MI300A node (branded XH3406-3) on display at SC24 which was new to me:</p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>I'm unaware of anyone who's actually committed to a large Eviden MI300A system, so I was
    surprised to see that Eviden already has a full blade design. But as with Eni's HPC6 supercomputer, perhaps this is
    a sign that AMD's GPUs (and now APUs) have graduated from being built-to-order science experiments to a technology
    ecosystem that people will want to buy off the rack.</p>
<p>There was also a ton of GH200 on the exhibit hall floor, but again, these node types were
    also on display at ISC24. This wasn't a surprise since a bunch of upcoming European systems have invested in GH200
    already; in addition to JUPITER's 6,000 GH200 nodes described above, <a href="https://www.cscs.ch/computers/alps">CSCS Alps</a> has 2,688 GH200 nodes, and <a href="https://glennklockwood.com/garden/systems/isambard-ai">Bristol's Isambard-AI</a> will have 1,362 GH200
    nodes. All of these systems will have a 1:1 CPU:GPU ratio and an NVL4 domain, suggesting this is the optimal way to
    configure GH200 for HPC workloads. I didn't hear a single mention of GH200 NVL32.</p>
<h4 id="tech-expo-gb200">GB200</h4>
<p>SC24 was the debut of NVIDIA's Blackwell GPU in the flesh, and a bunch of integrators had
    material on GB200 out at their booths. Interestingly, they all followed the same pattern as GH200 with an NVL4
    domain size, and just about every smaller HPC integrator followed a similar pattern where</p>
<p></p>
<ul>
<li>their booth had a standard "NVIDIA Partner" (or "Preferred Partner!") placard on their main desk</li>
<li>they had a bare NVIDIA GB200 baseboard (superchip) on display</li>
<li>there wasn't much other differentiation</li>
</ul>
<p>From this, I gather that not many companies have manufactured GB200 nodes yet, or if they
    have, there aren't enough GB200 boards available to waste them on display models. So, we had to settle for these
    bare NVIDIA-manufactured, 4-GPU + 2-CPU superchip boards:</p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>What struck me is that these are very large FRUs--if a single component (CPU, GPU, voltage
    regulator, DRAM chip, or anything else) goes bad, you have to yank and replace four GPUs and two CPUs. And because
    all the components are soldered down, someone's going to have to do a lot of work to remanufacture these boards to
    avoid throwing out a lot of very expensive, fully functional Blackwell GPUs.</p>
<p>There were a few companies who were further along their GB200 journey and had more
    integrated nodes on display. The HPE Cray booth had this GB200 NVL4 blade (the Cray EX154n) on display:</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>It looks remarkably sparse compared to the super-dense blades that normally slot into the
    Cray EX line, but even with a single NVL4 node per blade, the Cray EX cabinet only supports 56 of these blades,
    leaving 8 blade slots empty in the optimal configuration. I assume this is a limitation of power and cooling.</p>
<p>The booth collateral around this blade suggested its use case is "machine learning and
    sovereign AI" rather than traditional HPC, and that makes sense since each node has 768 GB of HBM3e which is enough
    to support training some pretty large sovereign models. However, the choice to force all I/O traffic on to the
    high-speed network by only leaving room for one piddly node-local NVMe drive (this blade only supports one SSD per
    blade) will make training on this platform very sensitive to the quality of the global storage subsystem. This is
    great if you bundle this blade with all-flash Lustre (like Cray ClusterStor) or DAOS (handy, since <a href="https://bsky.app/profile/adrianjhpc.bsky.social/post/3lba4yfg5fc2a">Intel divested the entire DAOS
        development team to HPE</a>). But it's not how I would build an AI-optimized system.</p>
<p>I suspect the cost-per-FLOP of this Cray GB200 solution is much lower than what a pure-play
    GB200 for LLM training would be. And since GB200 is actually a solid platform for FP64 (thanks to Dan Ernst for <a href="https://bsky.app/profile/ernstdj.bsky.social/post/3lb23ipwnvc26">challenging me on this</a> and sharing
    some <a href="https://arxiv.org/abs/2411.12090">great resources on the topic</a>), I expect to see this node do well
    in situations that are not training frontier LLMs, but rather fine-tuning LLMs, training smaller models, and mixing
    in traditional scientific computing on the same general-purpose HPC/AI system.</p>
<p>Speaking of pure-play LLM training platforms, though, I was glad that very few exhibitors
    were trying to talk up GB200 NVL72 this year. It may have been the case that vendors simply aren't ready to begin
    selling NVL72 yet, but I like to be optimistic and instead believe that the exhibitors who show up to SC24 know that
    the scientific computing community likely won't get enough value out of a 72-GPU coherence domain to justify the
    additional cost and complexity of NVL72. I didn't see a single vendor with a GB200 NVL36 or NVL72 rack on display
    (or a GH200 NVL32, for that matter), and not having to think about NVL72 for the week of SC24 was a nice break from
    my day job.</p>
<p>Perhaps the closest SC24 got to NVL72 was a joint announcement at the beginning of the week
    by Dell and CoreWeave, who announced that <a href="https://www.coreweave.com/blog/coreweave-pushes-boundaries-with-gb200-and-more">they have begun bringing
        GB200 NVL72 racks online</a>. Dell did have a massive, AI-focused booth on the exhibit floor, and they did talk
    up their high-powered, liquid-cooled rack infrastructure. But in addition to supporting GB200 with NVLink Switches,
    I'm sure that rack infrastructure would be equally good at supporting nodes geared more squarely at traditional HPC.
</p>
<h4 id="tech-expo-ss400">Slingshot 400</h4>
<p>HPE Cray also debuted a new 400G Slingshot switch, appropriately named Slingshot 400. I
    didn't get a chance to ask anyone any questions about it, but from the marketing material that came out right before
    the conference, it sounds like a serdes upgrade without any significant changes to Slingshot's L2 protocol.</p>
<p>There was a Slingshot 400 switch for the Cray EX rack on display at their booth, and it
    looked pretty amazing:</p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>It looks way more dense than the original 200G Rosetta switch, and it introduces
    liquid-cooled optics. If you look closely, you can also see a ton of flyover cables connecting the switch ASIC in
    the center to the transceivers near the top; similar flyover cables are showing up in all manner of
    ultra-high-performance networking equipment, likely reflecting the inability to maintain signal integrity across PCB
    traces.</p>
<p>The port density on Slingshot 400 remains the same as it was on 200G Slingshot, so there's
    still only 64 ports per switch, and the fabric scale limits don't increase. In addition, the media is saying that
    Slingshot 400 (and the GB200 blade that will launch with it) won't start appearing until "<a href="https://www.nextplatform.com/2024/11/26/hpe-upgrades-supercomputer-lineup-top-to-bottom-in-2025/">Fall
        2025</a>." Considering 64-port 800G switches (like <a href="https://nvidianews.nvidia.com/news/networking-switches-gpu-computing-ai">NVIDIA's SN5600</a> and <a href="https://www.arista.com/en/company/news/press-release/19493-arista-unveils-etherlink-ai-networking-platforms">Arista's
        7060X6</a>) will have already been on the market by then though, Slingshot 400 will be launching with HPE Cray
    on its back foot.</p>
<p>However, there was a curious statement on the placard accompanying this Slingshot 400
    switch:</p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>It reads, "Ultra Ethernet is the future, HPE Slingshot delivers today!"</p>
<p>Does this suggest that Slingshot 400 is just a stopgap until 800G Ultra Ethernet NICs begin
    appearing? If so, I look forward to seeing HPE Cray jam third-party 800G switch ASICs into the Cray EX liquid-cooled
    form factor at future SC conferences.</p>
<h4 id="tech-expo-gg">Grace-Grace for storage?</h4>
<p>One of the weirder things I saw on the exhibit floor was a scale-out storage server built
    on NVIDIA Grace CPUs that the good folks at WEKA had on display at their booth.</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>Manufactured by Supermicro, this "ARS-121L-NE316R" server (really rolls off the tongue)
    uses a two-socket Grace superchip and its LPDDR5X instead of conventional, socketed CPUs and DDR. The rest of it
    seems like a normal scale-out storage server, with sixteen E3.S SSD slots in the front and four 400G ConnectX-7 or
    BlueField-3 NICs in the back. No fancy dual-controller failover or anything like that; the presumption is that
    whatever storage system you'd install over this server would implement its own erasure coding across drives and
    servers.</p>
<p>At a glance, this might seem like a neat idea for a compute-intensive storage system like
    WEKA or DAOS. However, one thing that you typically want in a storage server is high reliability and repairability,
    features which weren't the optimal design point for these Grace superchips. Specifically,</p>
<p></p>
<ul>
<li>The Grace-Grace superchip turn both CPU sockets into a single FRU. This means that if one CPU goes bad, you're
        shipping the whole board back to NVIDIA rather than just doing a field-swap of a socket.</li>
<li>Grace uses LPDDR5X, whose ECC is not as robust as DDR5. I'm not an expert on memory architecture, but my
        understanding is that the ECC scheme on Grace does not provide ChipKill or row failures. And as with CPU
        failure, if a single DRAM chip goes back, you're throwing out two CPUs and all the DRAM.</li>
<li>There's no way to value-engineer the exact quantity of cores, clock, and DRAM to be optimal for the storage
        software installed on top of these servers.</li>
</ul>
<p>On the upside, though, there might be a cost advantage to using this Grace-Grace server
    over a beefier AMD- or Intel-based server with a bunch of traditional DIMMs. And if you really like NVIDIA products,
    this lets you do NVIDIA storage servers to go with your NVIDIA network and NVIDIA compute. As long as your storage
    software can work with the interrupt rates of such a server (e.g., it supports rebuild-on-read) and the 144 Neoverse
    V2 cores are a good fit for its computational requirements (e.g., calculating complex erasure codes), this server
    makes sense. But building a parallel storage system on LPDDR5X still gives me the willies.</p>
<p>I could also see this thing being useful for certain analytics workloads, especially those
    which may be upstream of LLM training. I look forward to hearing about where this turns up in the field.</p>
<p></p>
<h4 id="tech-expo-hbv5">Microsoft and AMD's new HBM CPU</h4>
<p>The last bit of new and exciting HPC technology that I noted came from my very own employer
    in the form of HBv5, a new, monster four-socket node featuring custom-designed AMD CPUs with HBM. STH wrote up <a href="https://www.servethehome.com/this-is-the-microsoft-azure-hbv5-and-amd-mi300c-nvidia/">an article with
        great photos of HBv5 and its speeds and feeds</a>, but in brief, this single node has:</p>
<p></p>
<ul>
<li>384 physical Zen 4 cores (352 accessible from within the VM) that clock up to 4 GHz</li>
<li>512 GB of HBM3 (up to 450 GB accessible from the VM) with up to 6.9 TB/s STREAM bandwidth</li>
<li>4x NDR InfiniBand NICs clocked at 200G per port</li>
<li>200G Azure Boost NIC (160G accessible from the VM)</li>
<li>8x 1.84 TB NVMe SSDs with up to 50 GB/s read and 30 GB/s write bandwidth</li>
</ul>
<p></p>
<p>The node itself looks kind of wacky as well, because there just isn't a lot on it:</p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>There are the obvious four sockets of AMD EPYC 9V64H, each with 96 physical cores and 128 GB of HBM3, and giant heat
    pipes on top of them since it's 100% air-cooled. But there's no DDR at all, no power converter board (the node is
    powered by a DC bus bar), and just a few flyover cables to connect the PCIe add-in-card cages. There is a separate
    fan board with just two pairs of power cables connecting to the motherboard, and that's really about it.</p>
<p>The front end of the node shows its I/O capabilities which are similarly uncomplicated:</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>There are four NDR InfiniBand cards (one localized to each socket) which are 400G-capable but cabled up at 200G,
    eight E1.S NVMe drives, and a brand-new dual-port Azure Boost 200G NIC. Here's a close-up of the right third of the
    node's front:</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p>This is the first time I've seen an Azure Boost NIC in a server, and it looks
much better integrated than the previous-generation 100G Azure SmartNIC that put the FPGA and hard NIC on separate
boards connected by a funny little pigtail. This older 100G SmartNIC with pigtail was also on display at the Microsoft
booth in an ND MI300X v5 node:</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>And finally, although I am no expert in this new node, I did hang around the people who are all week, and I
    repeatedly heard them answer the same few questions:</p>
<p></p>
<ul>
<li><b>Is this MI300C?</b> It is if you want it to be. You can call it Sally if you want; I don't think it will
        care. But Microsoft calls it HBv5, and the processor name will show up as AMD EPYC 9V64H in /proc/cpuinfo.</li>
<li><b>Is its InfiniBand 1x800 port, 2x400 ports, ...?</b> There are four NDR InfiniBand HCA cards, and each card
        has one full 400G NDR InfiniBand port. However, each port is only connected up to top-of-rack switching at 200G.
        Each InfiniBand HCA hangs off of a different EPYC 9V64H socket so that any memory address can get to
        InfiniBand without having to traverse Infinity Fabric. Running four ports of NDR InfiniBand at half speed is an
        unusual configuration, but that's what's going on here.</li>
<li><b>How can I buy this CPU?</b> EPYC 9V64H are "<a href="https://techcommunity.microsoft.com/blog/azurehighperformancecomputingblog/announcing-azure-hbv5-virtual-machines-a-breakthrough-in-memory-bandwidth-for-hp/4303504">custom
            AMD EPYC processors only available in Azure</a>." This means the only way to access it is by provisioning an
        HBv5 virtual machine in Azure.</li>
</ul>
<div>Amidst all the unrelenting news about new GPUs optimized for AI workloads, it was nice to see something new and
    unique launched squarely for the benefit of traditional scientific computing workloads.</div>
<p></p>
<p></p>
<h2 id="industry">The HPC industry overall</h2>
<div>
<p>New technology announcements are always exciting, but one of the main reasons I attend
        SC and ISC is to figure out the broader trends shaping the HPC industry. What concerns are top of mind for the
        community, and what blind spots remain open across all the conversations happening during the week? Answering
        these questions requires more than just walking the exhibit floor; it involves interpreting the subtext of the
        discussions happening at panels and BOF sessions. However, identifying where the industry needs more information
        or a clearer picture informs a lot of the public-facing talks and activities in which I participate throughout
        the year.</p>
</div>
<h3 id="industry-attendee">What I learned about the average SC technical program attendee</h3>
<p>The biggest realization that I confirmed this week is that <b>the SC conference is not an HPC
        conference; it is a scientific computing conference</b>. I sat in a few sessions where the phrase "HPC
    workflows" was clearly a stand-in for "scientific workflows," and "performance evaluation" still really means "MPI
    and OpenMP profiling." I found myself listening to ideas or hearing about tools that were <em>intellectually</em>
    interesting but ultimately not useful to me because they
    were so entrenched in the traditions of applying HPC to scientific computing. Let's talk about a few ways in which
    this manifested.</p>
<h4 id="industry-attendee-sustainability">People think sustainability and energy efficiency are the same thing</h4>
<p>Take, for example, the topic of sustainability. There were talks, panels, papers, and BOFs
    that touched on the environmental impact of HPC throughout the week, but the vast majority of them really weren't
    talking about sustainability at all; they were talking about energy efficiency. These talks often use the following
    narrative:</p>
<p></p>
<ol>
<li>Energy use from datacenters is predicted to reach some ridiculous number by 2030</li>
<li>We must create more energy-efficient algorithms, processors, and scheduling policies</li>
<li>Here is an idea we tested that reduced the energy consumption without impacting the performance of some
        application or workflow</li>
<li>Sustainability achieved! Success!</li>
</ol>
<p>The problem with this approach is that it declares victory when energy consumption is
    reduced. This is a great result if all you care about is spending less money on electricity for your supercomputer,
    but it completely misses the much greater issue that the electricity required to power an HPC job is often generated
    by burning fossil fuels, and that the carbon emissions that are directly attributable to HPC workloads are
    contributing to global climate change. This blind spot was exemplified by this slide, presented during a talk titled
    "Towards Sustainable Post-Exascale Leadership Computing" at the Sustainable Supercomputing workshop:</p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>I've <a href="https://blog.glennklockwood.com/2024/11/fasst-will-be-does-opportunity-to-adapt.html">written about
        this before</a> and I'll write about it again: FLOPS/Watt and PUE are not
    meaningful metrics by themselves when talking about sustainability. A PUE of 1.01 is not helpful if the datacenter
    that achieves it relies on burning coal for its power. Conversely, a PUE of 1.5 is not bad if all that electricity
    comes from a zero-carbon energy source. The biggest issue that I saw being reinforced at SC this year is that
    claims of "sustainable HPC" are accompanied by the subtext of "as long as I can keep doing everything else the way I
    always have."</p>
<p>There were glimmers of hope, though. Maciej Cytowski from Pawsey presented the opening talk
    at the Sustainable Supercomputing workshop, and he led with the right thing--he acknowledged that 60% of
    the fuel mix that powers Pawsey's supercomputers comes from burning fossil fuels:</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>Rather than patting himself on the back at his low PUE, Dr. Cytowski's described on how
    they built their datacenter atop a large aquifer from which they draw water at 21°C and return it at 30°C to avoid
    using energy-intensive chillers. To further reduce the carbon impact of this water loop, Pawsey also installed over
    200 kW of solar panels on its facility roof to power the water pumps. Given the fact that Pawsey cannot relocate to
    somewhere with a higher ratio of zero-carbon energy on account of its need to be physically near the Square
    Kilometer Array, Cytowski's talk felt like the most substantive discussion on sustainability in HPC that week.</p>
<p>Most other talks and panels on the topic really wanted to equate "sustainability" to "FLOPS
    per Watt" and pretend like where one deploys a supercomputer is not a part of the sustainability discussion. The
    reality is that, if the HPC industry wanted to take sustainability seriously, it would talk less about watts and
    more about tons of CO<sub>2</sub>. Seeing as how the average watt of electricity in Tennessee produces <a href="https://www.epa.gov/egrid/data-explorer">2.75x more carbon</a> than a watt of electricity in Washington,
    the actual environmental impact of fine-tuning Slurm scheduling or fiddling with CPU frequencies is meaningless when
    compared to the benefits that would be gained by deploying that supercomputer next to a hydroelectric dam instead of
    a coal-fired power plant.</p>
<p>I say all this because there are parts of the HPC industry (namely, the part in which I work)
    who <i>are</i> serious about sustainability. And those conversations go beyond simply building supercomputers in
    places where energy is low-carbon (thereby reducing <a href="https://www.epa.gov/climateleadership/scope-1-and-scope-2-inventory-guidance">Scope 2 emissions</a>). They
    include holding suppliers to high standards on reducing the carbon impact of transporting people and material to
    these data centers, reducing the carbon impact of all the excess packaging that accompanies components, and being
    accountable for the impact of everything in the data center after it reaches end of life (termed <a href="https://www.epa.gov/climateleadership/scope-3-inventory-guidance">Scope 3 emissions</a>).</p>
<p>The HPC community--or more precisely, the scientific computing community--is still married
    to the idea that the location of a supercomputer is non-negotiable, and "sustainability" is a nice-to-have secondary
    goal. I was
    hoping that the sessions I attended on sustainability would approach this topic at a level where the
    non-scientific HPC world has been living. Unfortunately, the discussion at SC24, which spanned workshops, BOFs, and
    Green 500, remains largely stuck on the idea that PUE and FLOPS/Watt are the end-all sustainability metrics. Those
    metrics are important, but there are global optimizations that have much greater effects on reducing the
    environmental impact of the HPC industry.</p>
<h4 id="industry-attendee-ai">AI sessions are really scientific computing sessions about AI</h4>
<p>Another area where "HPC" was revealed to really mean "scientific computing" was in the
    topic of AI. I sat in on a few BOFs and panels around AI topics to get a feel for where this community is in
    adopting AI for science, but again, I found the level of discourse to degrade to generic AI banter despite the best
    efforts of panelists and moderators. For example, I sat in the "Foundational Large Language Models for
    High-Performance Computing" BOF session, and Jeff Vetter very clearly defined what a "foundational large language
    model" was at the outset so we could have a productive discussion about their applicability in HPC (or, really,
    scientific computing):</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>The panelists did a good job of outlining their positions. On the upside, LLMs are good for
    performing source code conversion, documenting and validating code, and maximizing continuity in application codes
    that get passed around as graduate students come and go. On the downside, they have a difficult time creating
    efficient parallel code, and they struggle to debug parallel code. And that's probably where the BOF should have
    stopped, because LLMs, as defined at the outset of the session, don't actually have a ton of applicability in
    scientific computing. But as soon as the session opened up to audience questions, the session went off the rails.
</p>
<p>The first question was an extremely basic and nonspecific question: "Is AI a bubble?"</p>
<p>It's fun to ask provocative questions to a panel of experts. I get it. But the question had
    nothing to do with LLMs, any of the position statements presented by panelists, or even HPC or scientific computing.
    It turned a BOF on "LLMs for HPC" into a BOF that might as well have been titled "Let's just talk about AI!" A few
    panelists tried to get things back on track by talking about the successes of surrogate models to simulate physical
    processes, but this reduced the conversation to a point where "LLMs" really meant "any AI model" and "HPC" really
    meant "scientific simulations."</p>
<p>Perhaps the most productive statement to come out of that panel was when Rio Yokota
    asserted that "we" (the scientific community) should not train their own LLMs, because doing so would be
    "unproductive for science." But I, as well as anyone who understands the difference between LLMs and "AI," already
    knew that. And the people who don't understand the difference between an LLM and a surrogate model probably didn't
    pick up on Dr. Yokota's statement, so I suspect the meaning of his contribution was completely lost.</p>
<p>Walking out of that BOF (and, frankly, the other AI-themed BOFs and panels I attended), I
    was disappointed at how superficial the conversation was. This isn't to say these AI sessions were objectively
    <i>bad</i>; rather, I think it reflects the general state of understanding of AI amongst SC attendees. Or perhaps it
    reflects the demographic that is drawn to these sorts of sessions. If the SC community is not ready to have a
    meaningful discussion about AI in the context of HPC or scientific computing, attending BOFs with like-minded peers
    is probably a good place to begin getting immersed.
</p>
<p>But what became clear to me this past week is that SC BOFs and panels with "AI" in their
    title aren't really meant for practitioners of AI. They're meant for scientific computing people who are beginning
    to dabble in AI.</p>
<h4 id="industry-attendee-ops">AI for operations is not yet real in scientific computing</h4>
<p>I was invited to sit on a BOF panel called "Artificial Intelligence and Machine Learning
    for HPC Workload Analysis" following on a successful BOF in which I participated at ISC24. The broad intent was to
    have a discussion around the tools, methods, and neat ideas that HPC practitioners have been using to better
    understand workloads, and each of us panelists was tasked with talking about a project or idea we had in applying
    AI/ML to improve some aspect of workloads.</p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>What emerged from us speakers' lightning talks is that applying AI for operations--in this
    case, understanding user workloads--is nascent. Rather than talking about how we use AI to affect how we design or
    operate supercomputers, all of us seemed to focus more on how we are collecting data and beginning to analyze that
    data using ML techniques. And maybe that's OK, because AI won't ever do anything for workload characterization until
    you have a solid grasp of the telemetry you can capture about those workloads in the first place.</p>
<p>But when we opened the BOF up to discussion with all attendees, despite having a packed
    room, there was very little that the audience had. Our BOF lead, Kadidia Konaté, tried to pull discussion out of the
    room from a couple of different fronts by asking what tools people were using, what challenges they were facing, and
    things along those lines. However, it seemed to me that the majority of the audience was in that room as spectators;
    they didn't know where to start applying AI towards understanding the operations of supercomputers. Folks attended
    to find out the art of the possible, not talk about their own challenges.</p>
<p>As such, the conversation wound up bubbling back up to the safety of traditional topics in
    scientific computing--how is LDMS working out, how do you deal with data storage challenges of collecting telemetry,
    and all the usual things that monitoring and telemetry folks worry about. It's easy to talk about the topics you
    understand, and just as the LLM conversation reverted back to generic AI for science and the sustainability topic
    reverted back to FLOPS/Watt, this topic of AI for operations reverted back to standard telemetry collection.</p>
<h4 id="industry-attendee-hyperscale">Some are beginning to realize that HPC exists outside of scientific computing</h4>
<p>Despite the pervasive belief at SC24 that "HPC" and "scientific computing" are the same thing, there are early signs
    that the leaders in the community are coming to terms with the reality that there is now a significant amount of
    leadership HPC happening outside the scope of the conference. This was most prominent at the part of the Top500 BOF
    where Erich Strohmaier typically discusses trends based on the latest publication of the list.</p>
<p>In years past, Dr. Strohmaier's talk was full of statements that strongly implied that, if a supercomputer is not
    listed on Top500, it simply does not exist. This year was different though: he acknowledged that El Capitan,
    Frontier, and Aurora were "the three exascale systems <u style="font-style: italic;">we are aware of</u>," now being
    clear that there is room for exascale systems to exist that simply never ran HPL, or never submitted HPL results to
    Top500. He explicitly acknowledged again that China has stopped making any Top500 submissions, and although he
    didn't name them outright, he spent a few minutes dancing around "hyperscalers" who have been deploying exascale
    class systems such as <a href="https://glennklockwood.com/garden/systems/meta's-h100-clusters">Meta's H100
        clusters</a> (2x24K H100), <a href="https://glennklockwood.com/garden/systems/colossus">xAI's
        Colossus</a> (100K H100), and the full system behind <a href="https://glennklockwood.com/garden/systems/eagle">Microsoft's Eagle</a> (14K H100 is a "<a href="https://build.microsoft.com/en-US/sessions/984ca69a-ffca-4729-bf72-72ea0cd8a5db">tiny fraction</a>").</p>
<p>Strohmaier did an interesting analysis that estimated the total power of the Top500 list's supercomputers so he could
    compare it to industry buzz around hyperscalers building gigawatt-sized datacenters:</p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>It was a fun analysis where he concluded that there are between 500-600 megawatts of supercomputers on the Top500
    list, and after you factor in storage, PUE, and other ancillary power sources, the whole Top500 list sums up to what
    hyperscalers are talking about sticking into a single datacenter facility.</p>
<p>Although he didn't say it outright, I think the implication here is that the Top500 list is rapidly losing relevance
    in the broad HPC market, because a significant amount of the world's supercomputing capacity <i>and capability</i>
    are absent from the list. Although specific hyperscale supercomputers (like Meta's, xAI's, and Microsoft's) were not
    mentioned outright, their absence from the Top500 list suggests that this list might already be more incomplete than
    it is complete--the sum of the FLOPS or power on the Top500 supercomputers may be less than the sum of the giant
    supercomputers which are known but not listed. This will only get worse as the AI giants keep building systems every
    year while the government is stuck on its 3-5 year procurement cycles.</p>
<p>It follows that the meaning of the Top500 is sprinting towards a place where it is not representative of HPC so much
    as it is representative of <i>the slice of HPC that serves scientific computing</i>. Erich Strohmaier was clearly
    aware of this in his talk this year, and I look forward to seeing how the conversation around the Top500 list
    continues to morph as the years go on.</p>
<h3 id="industry-nsf">NSF's broad front vs. DOE's big bets in HPC and AI</h3>
<p>My career was started at an NSF HPC center and <a href="https://blog.glennklockwood.com/2022/05/life-and-leaving-nersc.html">built up over my years in the
        DOE</a>, so I feel like I owe a debt to the people who provided all the opportunities and mentorship that let me
    get to the place of privilege in the hyperscale/AI industry that I now enjoy. As a result, I find myself still
    spending a lot of my free time thinking about <a href="https://glennklockwood.com/garden/government's-role-in-ai">the role of governments in the changing face of
        HPC</a> (as evidenced by my critiques of <a href="https://blog.glennklockwood.com/2024/10/a-critique-of-call-for-public-ai.html">thinktank reports</a> and <a href="https://blog.glennklockwood.com/2024/11/fasst-will-be-does-opportunity-to-adapt.html">federal RFIs</a>...) and trying to bridge the gap
    in technical understanding between my old colleagues (in DOE, NSF, and European HPC organizations) and whatever they
    call what I work on now (hyperscale AI?).</p>
<p>To that end, I found myself doing quite a bit of <i>business development</i> (more on this later) with government
    types since I think that is where I can
    offer the most impact. I used to be government, and I closely follow the state of their thinking in HPC, but I also
    know what's going on inside the hyperscale and AI world. I also have enough context in both areas to draw a line
    through all the buzzy AI press releases to demonstrate how the momentum of private-sector investment in AI might
    affect the way national HPC
    efforts do business. So, I did a lot of talking to both my old colleagues in DOE and their industry partners in an
    attempt to help them understand how the hyperscale and AI industry thinks about infrastructure, and what they should
    expect in the next year.</p>
<p>More importantly though, I also sat in on a couple of NSF-themed BOFs to get a better understanding of where their
    thinking is, where NAIRR is going, how the NSF's strategy contrasts with DOE's strategy, and where the ambitions of
    the Office of Advanced Cyberinfrastructure might intersect with the trajectory of hyperscale AI.</p>
<p>What I learned was that NSF leadership is aware of everything that the community should be concerned about: the
    growth of data, the increasing need for specialized silicon, the incursion of AI into scientific computing, new
    business models and relationships with industry, and broadening the reach of HPC investments to be globally
    competitive. But beyond that, I struggled to see a cohesive vision for the future of NSF-funded
    supercomputing. </p>
<p>A BOF with a broad range of stakeholders probably isn't the best place to lay out a vision for the future of NSF's
    HPC efforts, and perhaps NSF's vision is best expressed through its funding opportunities and awards. Whichever the
    case may be, it seems like the NSF remains on a path to make incremental progress on a broad front of topics. Its
    Advanced Computing Systems and Services (ACSS) program will continue to fund the acquisition of newer
    supercomputers, and a smorgasbord of other research programs will continue funding efforts across public access to
    open science, cybersecurity, sustainable software, and other areas. My biggest concern is that peanut-buttering
    funding across such a broad portfolio will make net forward progress much slower than taking big bets. Perhaps big
    bets just aren't in the NSF's mission though.</p>
<p>NAIRR was also a topic that came up in every NSF-themed session I attended, but again, I didn't get a clear picture
    of the future. Most of the discussion that I heard was around socializing the resources that are available today
    through NAIRR, suggesting that the pilot's biggest issue is not a lack of HPC resources donated by industry, but
    awareness that NAIRR is a resource that researchers can use. This was reinforced by a survey whose results were
    presented in the NAIRR BOF:</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>It seems like the biggest challenges facing the NSF community relying on NAIRR (which has its own sample bias) is
    that they don't really know where to start even though they have AI resources (both GPUs and model API services) at
    their disposal. In a sense, this is a great position for the NSF since</p>
<p></p>
<ol>
<li>its users need intellectual help more than access to GPU resources, and the NSF has been great at promoting
        education, training, and workforce development.</li>
<li>its users are unlikely to demand the same cutting-edge GPUs that AI industry leaders are snapping up. For
        example, the largest pool of GPUs in NAIRR are A100 GPUs that NVIDIA donated via DGX Cloud; the big AI
        companies moved off of Ampere a year ago and are about to move off of Hopper.</li>
</ol>
<p></p>
<p>However, it also means that there's not a clear role for partnership with many industry players beyond donating
    resources to the NAIRR pilot today in the hopes of selling resources to the full NAIRR tomorrow. I asked what OAC
    leadership thought about moving beyond such a transactional relationship between NSF and industry at one of the BOFs
    I attended, and while the panelists were eager to explore specific answers to that question, I didn't hear any ideas
    that would approach some sort of truly equitable partnership where both parties contributed in-kind.</p>
<p>I also walked away from these NSF sessions struck by how different the NSF HPC community's culture is from that of
    the DOE. NSF BOF attendees seemed focused on getting answers and guidance from NSF leadership, unlike the typical
    DOE gathering, where discussions often revolve around attendees trying to shape priorities to align with their own
    agendas. A room full of DOE people tends to feel like everyone thinks they're the smartest person there, while NSF
    gatherings appear more diverse in the expertise and areas of depth of its constituents. Neither way is inherently
    better or worse, but it will make the full ambition of NAIRR (as an inter-agency collaboration) challenging to
    navigate. This is particularly relevant as DOE is now pursuing its own multi-billion-dollar AI infrastructure
    effort, FASST, that appears to sidestep NAIRR.</p>
<h3 id="industry-expo">Exhibitor trends</h3>
<p>There's no better way to figure out what's going on in the HPC industry than walking the
    exhibit floor each year, because booths cost money and reflect the priorities (and budgets) of all participants.
    This year's exhibit felt physically huge, and walking from one end to the other was an adventure. You can get a
    sense of the scale from this photo I took during the opening gala:</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>Despite having almost 18,000 registrants and the opening gala usually being a
crush of people, the gala this year felt and looked very sparse just because people and booths were more spread out.
There was also a perceptibly larger number of splashy vendors who have historically never attended before who were
promoting downstream HPC technologies like data center cooling and electrical distribution, and there was healthy
speculation online about whether the hugeness of the exhibit this year was due to these new power and cooling companies.</p>
<p></p>
<p>To put these questions to rest, I figured out how to yank down all the exhibitor metadata
    from the conference website so I could do some basic analysis on it.</p>
<h4 id="industry-expo-booths">Booths by the numbers</h4>
<p>The easiest way to find the biggest companies to appear this year was to compare the
    exhibitor list and booth sizes from SC23 to this year and see whose booth went from zero to some big square footage.
</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>I only took the top twenty new vendors, but they broadly fall into a couple of categories:</p>
<p></p>
<ul>
<li><b>Power and cooling</b>: Stulz, Delta, Airedale, Valvoline, Boundary Electric, Schneider Electric, Mara
        </li>
<li><b>Server manufacturing</b>: Wistron, AMI, Pegatron</li>
<li><b>Higher ed</b>: Tennessee Tech, SCRCC</li>
</ul>
<p>There were a couple other companies that must've just missed last SC but aren't new to
        the show (NetApp, Ansys, Samsung, Micron, Broadcom). And curiously, only one new GPU-as-a-Service provider
        (Nebius) showed up this year, suggesting last year was the year of the GPU Cloud.</p>
<p>But to confirm what others had speculated: yes, a significant amount of the new square
        footage of the exhibit floor can be attributed to companies focused on power and cooling. This is an interesting
        indicator that HPC is becoming mainstream, largely thanks to AI demanding ultra-high density of power and
        cooling. But it's also heartening to see a few new exhibitors in higher education making an appearance. Notably,
        SCRCC (South Carolina Research Computing Consortium) is a consortium between Clemon, University of South
        Carolina, and Savannah River National Laboratory that just formed last year, and I look forward to seeing what
        their combined forces can bring to bear.</p>
<p>We can also take a look at whose booths grew the most compared to SC23:</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>This distribution is much more interesting, since the top 20 exhibitors who grew their footprint comprise the
        majority of the growth in existing exhibitors. Cherry-picking a few interesting growers:</p>
<p></p>
<ul>
<li><b>Power and cooling</b>: USystems, Midas, Vertiv</li>
<li><b>Data center/GPUaaS</b>: iM, Iris Energy, and (arguably) Oracle</li>
<li><b>Software</b>: Arc Compute and CIQ</li>
<li><b>Companies facing serious financial or legal troubles</b>: I count at least three! Impressive that they
            are still pouring money into their SC booths.</li>
</ul>
<p>It's also interesting to see HLRS, the German national HPC center, grow so
        significantly. I'm not sure what prompted such a great expansion, but I take it to mean that things have been
        going well there.</p>
<p>Finally, Dell had a massive booth and showing this year. Not only did they grow the
        most since SC23, but they had the single largest booth on the exhibit floor at SC24. This was no doubt a result
        of their great successes in partnering with NVIDIA to land massive GPU buildout deals at places like <a href="https://qz.com/dell-super-micro-computer-stock-elon-musk-ai-nvidia-1851550428">xAI</a> and <a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/dell-reaches-milestone-with-industrys-first-enterprise-ready-nvidia-blackwell-poweredge-xe9712-server-racks">CoreWeave</a>.
        They also had "AI factory" messaging emblazoned all over their marketing material and debuted a nice 200 kW
        liquid-cooled rack that will be the basis for their GB200 NVL72 solution, clearly leaning into the idea that
        they are leaders in AI infrastructure. Despite this messaging being off-beat for the SC audience as I've
        described earlier, their booth was surprisingly full all the time, and I didn't actually get a chance to get in
        there to talk to anyone about what they've been doing.</p>
<p>Equally interesting are the vendors who reduced their footprint at SC24 relative to
        SC23:</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>Reading too much into any of these big shrinkers is pretty easy; while a reduction in
        booth size could suggest business hasn't been as good, it could equally mean that an exhibitor just went
        overboard at SC23 and downsized to correct this year. A few noteworthy exhibitors to call out:</p>
<p></p>
<ul>
<li>Penguin and the Korea Semiconductor Industry Association both cut way back from massive 50x50 booths to
            30x30. Their booths this year were both big, but they weren't massive. Viridien, formerly known as CGG, also
            shrunk from a massive booth to a less-massive 30x40.</li>
<li>Juniper still kept an independent booth, but it is in the <a href="https://www.hpe.com/us/en/newsroom/press-release/2024/01/hpe-to-acquire-juniper-networks-to-accelerate-ai-driven-innovation.html">process
                of being absorbed into HPE</a>. Shrinking makes sense.</li>
<li>Major cloud providers Google and AWS scaled back, but Microsoft did not.</li>
<li>GPU-as-a-Service cloud providers CoreWeave and Lambda both scaled back. Since these GPUaaS providers'
            business models typically rely on courting few big customers, it may make sense to cut back on booth volume.
        </li>
<li>Major AI storage companies DDN, VAST, and (to a lesser degree) Pure also scaled back, while WEKA did not. I
            know business for DDN and VAST has been great this past year, so these may just reflect having gone
            overboard last year.</li>
</ul>
<p>Overall, almost twice as many vendors grew their booths than scaled back, so I'd
        caution anyone against trying to interpret any of this as anything beyond exhibitors right-sizing their booths
        after going all-in last year.</p>
<p>Finally, there are a handful of vendors who disappeared outright after SC23:</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>It is critical to point out that the largest booths to vanish outright were all on the
        smaller size: SUSE, Tenstorrent, and Symbiosys Alliance all disappeared this year, but their booths last year
        were only 20x30. I was surprised to see that Tenstorrent and Arm didn't have booths, but the others are either
        companies I haven't heard of (suggesting the return on investment of showing at SC might've been low), are easy
        to rationalize as only being HPC-adjacent (such as SNIA and DigitalOcean), or simply went bankrupt in the last
        year.</p>
<p>As we say at the business factory, the net-net of the exhibit hall this year is that
        the square footage of booth space increased by 15,000 square feet, so it was in fact bigger, it did take longer
        to walk from one end to the other, and there definitely were a bunch of new power and cooling companies filling
        out the space. Some exhibitors shrank or vanished, but the industry as a whole appears to be moving in a healthy
        direction.</p>
<p>And if you're interested in analyzing this data more yourself, please have a look at <a href="https://github.com/glennklockwood/sc-exhibitors">the data and the Jupyter notebook I used to generate
            the above treemaps on GitHub</a>. If you discover anything interesting, please write about it and post it
        online!</p>
<p></p>
<p></p>
<h4 id="industry-expo-gpuaas">Proliferation of GPU-as-a-Service providers</h4>
<p>As an AI infrastructure person working for a major cloud provider, I kept an eye out for all the companies trying
        to get into the GPU-as-a-Service game. <a href="https://blog.glennklockwood.com/2023/11/sc23-recap.html">I described these players last year as
            "pure-play GPU clouds,"</a> and it seems like the number of options available to customers who want to go
        this route is growing. But I found it telling that a lot of them had booths that were completely
        indistinguishable from each other. Here's an example of one:</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>As best I can tell, these companies are all NVIDIA preferred partners with
    data centers and a willingness to deploy NVIDIA GPUs, NVIDIA SmartNICs, and NVIDIA cloud stack, and sell multi-year
    commitments to consume those GPUs. I tried to accost some of these companies' booth staff to ask them my favorite
    question ("What makes you different from everyone else?"), but most of these companies' booths were staffed by
    people more interested in talking to each other than me.</p>
<p>These GPUaaS providers tend to freak me out, because, as Microsoft's CEO recently stated, these companies are
        often "<a href="https://www.microsoft.com/en-us/Investor/events/FY-2025/earnings-fy-2025-q1">just a bunch of
            tech companies still using VC money to buy a bunch of GPUs</a>." I can't help but feel like this is where
        the AI hype will come back to bite companies who have chosen to build houses upon sand. Walking the SC24 exhibit
        floor is admittedly a very narrow view of this line of business, but it seemed like some of these companies were
        content to buy up huge booths, hang a pretty banner above it, and otherwise leave the booth empty of anything
        beyond a few chairs and some generic value propositions. I didn't feel a lot of hunger or enthusiasm from these
        companies despite the fact that a bunch of them have hundreds of millions of dollars of GPUs effectively sitting
        on credit cards that they are going to have to make payments on for the next five years.</p>
<p>That all said, not all the companies in the GPUaaS are kicking back and letting the money pour in. In particular,
        I spent a few minutes chatting up someone at the CoreWeave booth, and I was surprised to hear about how much
        innovation they're adding on top of their conventional GPUaaS offering. For example, they developed <a href="https://docs.coreweave.com/coreweave-machine-learning-and-ai/training/sunk">Slurm on Kubernetes
            (SUNK)</a> with one of their key customers to close the gap between the fact that CoreWeave exposes its GPU
        service through Kubernetes, but many AI customers have built their stack around Slurm, <a href="https://github.com/NVIDIA/pyxis">pyxis</a>, and <a href="https://github.com/NVIDIA/enroot">enroot</a>.
    </p>
<p>In a weird twist of fate, I later ran into an old acquaintance who turned out to be one of the key CoreWeave
        customers for whom SUNK was developed. He commented that SUNK is the real deal and does exactly what his users
        need which, given the high standards that this person has historically had, is a strong affirmation that SUNK is
        more than just toy software that was developed and thrown on to GitHub for an easy press release. CoreWeave is
        also developing some interesting high-performance object storage caching software, and all of these software
        services are provided at no cost above whatever customers are already paying for their GPU service.</p>
<p>I bring this up because it highlights an emerging distinction in the GPUaaS market, which used to be a homogenous
        sea of bitcoin-turned-AI providers. Of course, many companies still rely on that simple business model: holding
        the bill for rapidly depreciating GPUs that NVIDIA sells and AI startups consume. However, there are now GPUaaS
        providers moving up the value chain by taking on the automation and engineering challenges that model developers
        don't want to deal with. Investing in uncertain projects like new software or diverse technology stacks is
        certainly risky, especially since they may never result in enough revenue to pay for themselves. But having a
        strong point of view, taking a stance, and investing in projects that you feel are right deserves recognition.
        My hat is off to the GPUaaS providers who are willing to take these risks and raise the tide for all of us
        rather than simply sling NVIDIA GPUs to anyone with a bag of money.</p>
<h2 id="community">Community and connections</h2>
<p>As much as I enjoy <i>increasing shareholder value</i>, the part of SC that gives me the
    greatest joy is reconnecting with the HPC community. Knowing I'll get to chat with my favorite people in the
    industry (and meet some new favorite people!) makes the long plane rides, upper respiratory infections, and weird
    hotel rooms completely worth it.</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>I wound up averaging under six hours of sleep per night this year in large part because 9pm
    or 7am were often the only free times I had to meet with people I really wanted to see. I have this unhealthy
    mindset where every hour of every day, from the day I land to the day I leave, is too precious to waste, and it's
    far too easy for me to rationalize that spending an hour talking to someone interesting is worth losing an hour of
    sleep.</p>
<p>But like I said at the outset of this blog post, this year felt different for a few
    reasons, and a lot of them revolve around the fact that I think I'm getting old. Now, it's always fun to say "I'm
    getting old" in a mostly braggadocious way, but this feeling manifested in concrete ways that affected the way I
    experienced the conference:</p>
<p></p>
<ol>
<li>I hit my limit on Monday night and couldn't get home without spending 15 minutes sitting in an unlit playground
        across from the World of Coke. I've always gotten blisters and fatigue, but this was the first time I couldn't
        just cowboy up and muscle through it. To avoid a repeat of this, I wound up "wasting" (see above) a lot more
        time to just get off my feet this year.</li>
<li>This year, I reached the point where I need to start time-box how much time I spend chatting up the folks I
        bump into. I used to just let the good times roll if I ran into someone I knew, but this year I wound up
        spending as much time attending sessions as I did missing sessions because I got caught up in a conversation.
        This isn't a bad thing per se, but I did feel a little sour when I realized I'd made a bad bet on choosing to
        chat instead of attending a session or vice versa, and this bad feeling lingered in the back of my mind just
        about every day.</li>
<li>There weren't a lot of surprises for me at the conference this year, and I worry that I am at risk of losing
        touch with the technical aspects of the conference that get newer attendees excited. Instead of hearing about,
        say, the latest research in interconnects, more of my time was spent mucking it up with the sorts of people in
        the HPC community who I used to find intimidating. On the one hand, hooray me for making it into old boys'
        clubs. But on the other, I don't want to become some HPC greybeard whose last meaningful contribution to the
        industry was twenty years ago.</li>
<li>This is the first year where I've had people accost me <i>and ask me for advice</i>. I've long been accosted by
        strangers because of my online presence, but those interactions were always lighthearted exchanges of "I follow
        you on Twitter" and "Great to meet you. Have an @HPC_Guru pin." This year, I had people specifically ask me for
        advice on industry versus postdoc, AI versus HPC, and what my master plan was when I left NERSC. Even though I
        didn't have any sage advice, I still found it really hard to tell bright-eyed students to go kick rocks just so
        I wouldn't be late for yet another mushy panel on AI.</li>
</ol>
<p>If you read this all and think "boo hoo, poor Glenn is too popular and wise for his own
    good," yeah, I get it. There are worse problems to have. But this was the first year where I felt like what I put
    into the conference was greater than what I got out of it. Presenting at SC used to be at least as good for my
    career as it was useful for my audiences, but it just doesn't count for much given my current role and career stage.
    It felt like some of the magic was gone this year in a way I've never experienced before. </p>
<p></p>
<h3 id="community-people">Getting to know people</h3>
<p>As the years have gone on, I spend an increasing amount of my week having one-on-one
    conversations instead of wandering aimlessly. This year though, I came to SC without really having anything to buy
    or sell:</p>
<p></p>
<ul>
<li>I am not a researcher, so I don't need to pump up the work I'm doing to impress my fellow researchers.</li>
<li>I no longer own a product market segment, so I don't directly influence the customers or vendors with whom my
        employer works.</li>
<li>I don't have any bandwidth in my day job to support any new customers or partnerships, so I don't have a strong
        reason to sell people on partnering with me or my employer. </li>
</ul>
<p>Much to my surprise though, a bunch of my old vendor/partner colleagues still wanted to get
    together to chat this year. Reflecting back, I was surprised to realize that it was these conversations--not the
    ones about business--that were the most fulfilling this year.</p>
<p>I learned about people's hobbies, families, and their philosophies on life, and it was
    amazing to get to know some of the people behind the companies with whom I've long dealt. I was reminded that the
    person is rarely the same as the company, and even behind some of the most aggressive and blusterous tech companies
    are often normal people with the same concerns and moments of self-doubt that everyone else has. I was also reminded
    that good engineers appreciate good engineering regardless of whether it's coming from a competitor or not. The
    public persona of a tech exec may not openly admire a competitor's product, but that doesn't mean they don't know
    good work when they see it.</p>
<p>I also surprised a colleague whose career has been in the DOE labs with an anecdote that
    amounted to the following: even though two companies may be in fierce competition, the people who work for them
    don't have to be. The HPC community is small enough that almost everyone has got a pal at a competing company, and
    when there are deals to be made, people looove to gossip. If one salesperson hears a juicy rumor about a prospective
    customer, odds are that everyone else on the market will hear about it pretty quickly too. Of course, the boundaries
    of confidentiality and professionalism are respected when it matters, but the interpersonal relationships that are
    formed between coworkers and friends don't suddenly disappear when people change jobs.</p>
<p>And so, I guess it would make sense that people still want to talk to me even though I have
    nothing to buy or sell. I love trading gossip just as much as everyone else, and I really enjoyed this aspect of the
    week.</p>
<p></p>
<h3 id="community-career">Talking to early career people</h3>
<p>I also spent an atypically significant amount of my week talking to early career people in
    HPC who knew of me one way or another and wanted career advice. This is the first year I recall having the same
    career conversations with multiple people, and this new phase of my life was perhaps most apparent during the IEEE
    TCHPC/TCPP HPCSC career panel in which I was invited to speak this year.</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>It was an honor to be asked to present on a career panel, but I didn't feel very qualified to give career advice to
    up-and-coming computer science graduate students who want to pursue HPC. I am neither a computer scientist nor a
    researcher, but fortunately for me, my distinguished co-panelists (Drs. Dewi Yokelson, Olga Pearce, YJ Ji, and
    Rabab Alomairy) had plenty of more relevant wisdom to share. And at the end of the panel, there were a few things we
    all seemed to agree on as good advice:</p>
<p></p>
<ol>
<li>Knowing stuff is good, but being able to learn things is better. Being eager to learn and naturally curious
        makes this much easier as well.</li>
<li>The life of a researcher sometimes requires more than working a standard nine-to-five, so it'll be hard to be
        really successful if your heart isn't in it.</li>
<li><a href="https://quoteinvestigator.com/2014/04/06/they-feel/">People will forget what you did or what you said,
            but they remember how you made them feel</a>. Don't be a jerk, because this community is small.</li>
</ol>
<p></p>
<p>In both this panel the one-on-one conversations I had with early career individuals, the best I could offer was the
    truth: I never had a master plan that got me to where I am; I just try out new things until I realize I don't like
    doing them anymore. I never knew what I wanted to be when I grew up, and I still don't really, so it now makes me
    nervous that people have started approaching me with the assumption that I've got it all figured out. Unless I
    torpedo my career and go live on a goat farm though, maybe I should prepare for this to be a significant part of my
    SC experiences going forward.</p>
<h3 id="community-bsky">Shift in social media</h3>
<p>One last, big change in the community aspect of SC this year was the mass-migration of a ton of HPC folks from
    Twitter to Bluesky during the week prior to the conference. I don't really understand what prompted it so suddenly;
    a few of us have been trying for years to get some kind of momentum on other social platforms like Mastodon, but the
    general lack of engagement meant that all the excitement around SC always wound up exclusively on Twitter. This year
    was different though, and Bluesky hit critical mass with the HPC community.</p>
<p>I personally have never experienced an SC conference without Twitter; my first SC was in 2013, and part of what made
    that first conference so exciting was being able to pull up my phone and see what other people were seeing,
    thinking, and doing across the entire convention center via Twitter. Having the social media component to the
    conference made me feel like I was a part of something that first year, and as the years went on, Twitter became an
    increasingly indispensable part of the complete SC experience for me.</p>
<p>This year, though, I decided to <a href="https://x.com/glennklockwood/status/1857571101028790498">try an
        experiment</a> and see what SC would be like if I set Twitter aside and invested my time into Bluesky instead.
</p>
<p>The verdict? <i>It was actually pretty nice.</i></p>
<p>It felt a lot like the SC13 days, where my day ended and began with me popping open Bluesky to see what new <a href="https://bsky.app/hashtag/sc24">#SC24</a> posts were made. And because many of the tech companies and HPC
    centers hadn't yet made it over, the hashtag wasn't clogged up by a bunch of prescheduled marketing blasts that
    buried the posts written by regular old conference attendees who were <a href="https://bsky.app/profile/walkingrandomly.bsky.social/post/3lbazofprgc2y">asking important questions</a>:</p>
<blockquote class="bluesky-embed"><p lang="en">Which booths at #sc24 have coffee? I noticed oracle do. Anyone else?</p>
— Mike Croucher (<a href="https://bsky.app/profile/did:plc:sd6xejkhcmyehbscxb5lz3uq?ref_src=embed">@walkingrandomly.bsky.social</a>) <a href="https://bsky.app/profile/did:plc:sd6xejkhcmyehbscxb5lz3uq/post/3lbazofprgc2y?ref_src=embed">November 18, 2024 at 3:02 PM</a></blockquote>
<p>Of course, I still clogged Bluesky up with my nonsense during the week, but there was an amazing amount of
    engagement by a diversity of thoughtful people--many who came from Twitter, but some whose names and handles I
    didn't recognize.</p>
<p>The volume of traffic on Bluesky during the week did feel a little lower than what it had been on Twitter in years
    past though. I also didn't see as many live posts of technical sessions as they happened, so I couldn't really tell
    whether I was missing something interesting in real time. This may have contributed to why I felt a little less
    connected to the pulse of the conference this year than I had in the past. It also could've been the fact that
    conference was physically smeared out across a massive space though; the sparsity of the convention center was at
    least on par with the sparsity on Bluesky.</p>
<p>At the end of the week, I didn't regret the experiment. In fact, I'll probably be putting more effort into my Bluesky
    account than my Twitter account going forward. To be clear though, this isn't a particularly political decision on
    my part, and I pass no judgment on anyone who wants to use one platform over the other. It's just that I like the
    way I feel when I scroll through my Bluesky feeds, and I don't get that same feeling when I use Twitter.</p>
<h2 id="conclusion">So what's the takeaway?</h2>
<p>SC this year was a great conference by almost every measure, as it always is, but it still felt a little different for me. I'm sure that some of that feeling is the result of my own growth, and my role with respect to the conference seems to be evolving from someone who gets a lot out of the conference to someone who is giving more to the conference. That's not to say that I don't get a lot out of it, though; I had no shortage of wonderful interactions with everyone from technology executives to rising stars who are early in their career, and I learned a lot about both them and me as whole people. But SC24, more than any SC before it, is when I realized this change was happening.</p>
<p>On the technological front, we saw the debut of a new #1 system (emblazoned with the smiling face of Bronis...) and a growing crop of massive, new clusters deployed for commercial applications. The exhibit floor was quantitatively bigger, in large part due to new power and cooling companies who are suddenly relevant to the HPC world thanks to the momentum of AI. At the same time, the SC technical program is clearly separating itself out as a conference focused on scientific computing; the level of discourse around AI remains largely superficial compared to true AI conferences, the role of hyperscalers in the HPC industry is still cast more as a threat than an opportunity.</p>
<p>For my part, I'm still trying to get a grasp on where government agencies like DOE and NSF want to take their AI ambitions so I can try to help build a better mutual understanding between the scientific computing community and the hyperscale AI community. However, it seems like the NSF is progressing slowly on a wide front, while the DOE is doing what DOE does and charging headfirst into a landscape that has changed more than I think they realize.</p>
<p>There's a lot of technical content that I know I missed on account of the increasing time I've been spending on the people and community aspect of the conference, and I'm coming to terms with the idea that this just may be the way SC is from now on. And I think I'm okay with that, since the support of the community is what helped me go from being a bored materials science student into someone whose HPC career advice is worth soliciting in the short span of eleven years. Despite any or all of the cynicism that may come out in the things I say about this conference, SC is always the highlight of my year. I always go into it with excitement, gladly burn the candle at both ends all week, and fly home feeling both grateful for and humbled by everything the HPC community has done and continues to do to keep getting me out of bed in the morning.</p>
<p></p>]]></content><author><name>Glenn K. Lockwood&apos;s Blog</name></author><category term="glennklockwood" /><summary type="html"><![CDATA[The premiere annual conference of the high-performance computing community, SC24, was held in Atlanta last week, and it attracted a record-shattering number of attendees--nearly 18,000 registrants, up 28% from last year! The conference felt big as well, and there seemed to be a lot more running between sessions, meetings, and the exhibition floor. Despite its objectively bigger size though, the content of the conference felt more diffuse this year, and I was left wondering if this reflected my own biases or was a real effect of the AI industry beginning to overflow into AI-adjacent technology conferences like SC. Of course, this isn't to say that SC24 was anything short of a great conference. Some exciting new technologies were announced, a new supercomputer beat out Frontier to become the fastest supercomputer on the Top500 list, and I got to catch up with a bunch of great people that I only get to see at shows like this. I'll touch on all of these things below. But this year felt different from previous SC conferences to me, and I'll try to talk about that too. There's no great way to arrange all the things I jotted down in my notes, but I've tried to arrange them by what readers may be interested in. Here's the table of contents: My approach to SC this year New technology and announcements Top500 and a new #1 system #1 - El Capitan #5 - Eni HPC6 #16 and #17 - SoftBank CHIE-2 and CHIE-3 #18 - Jülich's JUPITER Exascale Transition Instrument (JETI) #32 - Reindeer! Technology on the exhibit floor GB200 Slingshot 400 Grace-Grace for storage? Microsoft and AMD's new HBM CPU The HPC industry overall What I learned about the average SC technical program attendee People think sustainability and energy efficiency are the same thing AI sessions are really scientific computing sessions about AI AI for operations is not yet real in scientific computing Some are beginning to realize that HPC exists outside of scientific computing NSF's broad front vs. DOE's big bets in HPC and AI Exhibitor trends Booths by the numbers Proliferation of GPU-as-a-Service providers Community and connections Getting to know people Talking to early career people Shift in social media So what's the takeaway? Before getting into the details though, I should explain how my perspective shaped what I noticed (and missed) through the conference. And to be clear: these are my own personal opinions and do not necessarily reflect those of my employer. Although Microsoft covered the cost for me to attend SC, I wrote this blog post during my own free time over the Thanksgiving holiday, and nobody had any editorial control over what follows except me. My approach to SC this year Although this is the eleventh SC conference I've attended, it was the first time that I: attended as a practitioner of hyperscale AI rather than traditional HPC and scientific computing attended as a Microsoft engineer (I represented Microsoft as a product manager at SC22 and SC23) did not attend SC as a designated storage person (since 2013) Because of these changes in my identity as an attendee, I approached the conference with a different set of goals in mind: As a hyperscale/AI person, I felt that I should prioritize attending all the cloud and AI sessions whenever forced to choose between one session or another. I chose to focus on understanding the traditional HPC community's understanding of hyperscale and AI, which meant I had to spend less time in the workshops, panels and BOFs where I built my career. As an engineer rather than a product manager, it wasn't my primary responsibility to run private briefings and gather HPC customers' requirements and feedback. Instead, I prioritized only those meetings where my first-hand knowledge of how massive-scale AI training works could have a meaningful impact. This meant I focused on partners and practitioners who also operate in the realm of hyperscale--think massive, AI-adjacent companies and the HPC centers who have historically dominated the very top of the Top500 list. One thing I didn't anticipate going into SC24 is that I've inherited a third identity: there are a new cohort of people in HPC who see me as a long-time community member. This resulted in a surprising amount of my time being spent talking to students and early career practitioners who were looking for advice. These three identities and goals meant I don't many notes to share on the technical program, but I did capture more observations about broader trends in the HPC industry and community. New technology and announcements HPC is all about cutting-edge technology, so that's a fine place to start talking about what was new. Top500 and a new #1 system A cornerstone of every SC conference is the release of the new Top500 list on Monday, and this is especially true on years when a new #1 supercomputer is announced. As was widely anticipated in the weeks leading up to SC24, El Capitan unseated Frontier as the new #1 supercomputer this year, posting an impressive 1.74 EFLOPS of FP64. In addition though, Frontier grew a little (it added 400 nodes), there was a notable new #5 system (Eni's HPC6), and a number of smaller systems appeared that are worth calling out. #1 - El Capitan The highlight of the Top500 list was undoubtedly the debut of El Capitan, Lawrence Livermore National Laboratory's massive new MI300A-based exascale supercomputer. Its 1.74 EF score resulted from a 105-minute HPL run that came in under 30 MW, and a bunch of technical details about the system were disclosed by Livermore Computing's CTO, Bronis de Supinski, during an invited talk during the Top500 BOF. Plenty of others summarize the system's speeds and feeds (e.g., see The Next Platform's article on El Cap), so I won't do that. However, I will comment on how unusual Bronis' talk was. Foremost, the El Capitan talk seemed haphazard and last-minute. Considering the system took over half a decade of planning and cost at least half a billion dollars, El Capitan's unveiling was the most unenthusiastic description of a brand-new #1 supercomputer I've ever seen. I can understand that the Livermore folks have debuted plenty of novel #1 systems in their careers, but El Capitan is objectively a fascinating system, and running a full-system job for nearly two hours across first-of-a-kind APUs is an amazing feat. If community leaders don't get excited about their own groundbreaking achievements, what kind of message should the next generation of HPC professionals take home? In sharp contrast to the blasé announcement of this new system was the leading slide that was presented to describe the speeds and feeds of El Capitan: I've never seen a speaker take the main stage and put a photo of himself literally in the center of the slide, in front of the supercomputer they're talking about. I don't know what the communications people at Livermore were trying to do with this graphic, but I don't think it was intended to be evocative of the first thing that came to my mind: The supercomputer is literally named "The Captain," and there's a photo of one dude (the boss of Livermore Computing, who is also standing on stage giving the talk) blocking the view of the machine. It wasn't a great look, and it left me feeling very uneasy about what I was witnessing and what message it was sending to the HPC community. In case it needs to be said, HPC is a team sport. The unveiling of El Capitan (or any other #1 system before it) is always the product of dozens, if not hundreds, of people devoting years of their professional lives to ensuring it all comes together. It was a big miss, both to those who put in the work, and those who will have to put in the work on future systems, to suggest that a single, smiling face comes before the success of the system deployment. #5 - Eni HPC6 The other notable entrant to the Top 10 list was HPC6, an industry system deployed by Eni (a major Italian energy company) built on MI250X. Oil and gas companies tend to be conservative in the systems they buy since the seismic imaging done on their large supercomputers informs hundred-million to billion-dollar investments in drilling a new well, and they have much less tolerance for weird architectures than federally funded leadership computing does. Thus, Eni's adoption of AMD GPUs in this #5 system is a strong endorsement of their capability in mission-critical commercial computing. #16 and #17 - SoftBank CHIE-2 and CHIE-3 SoftBank, the Japanese investment conglomerate who, among other things, owns a significant stake in Arm, made its Top500 debut with two identical 256-node DGX H100 SuperPODs. While not technologically interesting (H100 is getting old), these systems represent significant investment in HPC by private industry in Japan and signals that SoftBank is following the lead of large American investment groups in building private AI clusters for the AI startups in their portfolios. In doing this, SoftBank's investments aren't dependent on third-party cloud providers to supply the GPUs to make these startups successful and reduces their overall risk. Although I didn't hear anything about these SoftBank systems at the conference, NVIDIA issued a press statement during the NVIDIA AI Summit Japan during the week prior to SC24 that discussed SoftBank's investment in large NVIDIA supercomputers. The press statement states that these systems will be used "for [SoftBank's] own generative AI development and AI-related business, as well as that of universities, research institutions and businesses throughout Japan." The release also suggests we can expect B200 and GB200 SuperPODs from SoftBank to appear as those technologies come online. #18 - Jülich's JUPITER Exascale Transition Instrument (JETI) Just below the SoftBank systems was the precursor system to Europe's first exascale system. I was hoping that JUPITER, the full exascale system being deployed at FRJ, would appear in the Top 10, but it seems like we'll have to wait for ISC25 for that. Still, the JETI system ran HPL across 480 nodes of BullSequana XH3000, the same node that will be used in JUPITER, and achieved 83 TFLOPS. By comparison, the full JUPITER system will be over 10x larger ("roughly 6000 compute nodes" in the Booster), and projecting the JETI run (173 TF/node) out to this full JUPITER scale indicates that JUPITER should just squeak over the 1.0 EFLOPS line. In preparation for JUPITER, Eviden had a couple of these BullSequana XH3000 nodes out on display this year: And if you're interested in more, I've been tracking the technical details of JUPITER in my digital garden. #32 - Reindeer! Waay down the list was Microsoft's sole new Top500 entry this cycle, an NVIDIA H200 system that ran HPL over 120 ND H200 v5 nodes in Azure. It was one of only two conventional (non-Grace) H200 clusters that appeared in the top 100, and it had a pretty good efficiency (Rmax/Rpeak &gt; 80%). Microsoft also had a Reindeer node on display at its booth: An astute observer may note that this node looks an awful lot like the H100 node used in its Eagle supercomputer, which was on display at SC23 last year. That's because it's the same chassis, just with an upgraded HGX baseboard. Reindeer was not super exciting, and there were no press releases about it, but I mention it here for a couple reasons: One of my teammates did the HPL run and submission, and his group got to come up with the name of the system for the purposes of HPL. As it turns out, generating a public name for a Top500 submission involves a comical amount of legal and marketing process when it comes from a giant corporation like Microsoft. And as it turns out, naming a cluster "Reindeer" has a low probability of offending anyone. Reindeer is pretty boring--it's a relatively small cluster with a bunch of GPUs. But when you're building out AI infrastructure at a pace of 5x Eagles (70,000 GPUs!) per month, you want the clusters that those GPUs go into to be as boring, predictable, and automatable as possible. Seeing as how Reindeer only used 960 GPUs but still got #32, it doesn't require much math to realize that the big hyperscalers could flood the Top500 list with these cookie-cutter GPU clusters and (in this case) make any ranking below #32 completely irrelevant. Heaven help the Top500 list if they ever publish an API for submitting new systems; cloud providers' build validation automation could tack a Top500 submission on at the end of burn-in and permanently ruin the list. On a personal note, the supercomputer grant that gave me my first job in the HPC business debuted at #48. It's mind-boggling that I now work in a place where standing up a #32 system is just day-to-day business. Technology on the exhibit floor The exhibit floor had a few new pieces of HPC technology on display this year that are worthy of mention, but a lot of the most HPC-centric exciting stuff actually had a soft debut at ISC24 in May. For example, even though SC24 was MI300A's big splash due to the El Capitan announcement, some MI300A nodes (such as the Cray EX255a) were on display in Hamburg. However, Eviden had their MI300A node (branded XH3406-3) on display at SC24 which was new to me: I'm unaware of anyone who's actually committed to a large Eviden MI300A system, so I was surprised to see that Eviden already has a full blade design. But as with Eni's HPC6 supercomputer, perhaps this is a sign that AMD's GPUs (and now APUs) have graduated from being built-to-order science experiments to a technology ecosystem that people will want to buy off the rack. There was also a ton of GH200 on the exhibit hall floor, but again, these node types were also on display at ISC24. This wasn't a surprise since a bunch of upcoming European systems have invested in GH200 already; in addition to JUPITER's 6,000 GH200 nodes described above, CSCS Alps has 2,688 GH200 nodes, and Bristol's Isambard-AI will have 1,362 GH200 nodes. All of these systems will have a 1:1 CPU:GPU ratio and an NVL4 domain, suggesting this is the optimal way to configure GH200 for HPC workloads. I didn't hear a single mention of GH200 NVL32. GB200 SC24 was the debut of NVIDIA's Blackwell GPU in the flesh, and a bunch of integrators had material on GB200 out at their booths. Interestingly, they all followed the same pattern as GH200 with an NVL4 domain size, and just about every smaller HPC integrator followed a similar pattern where their booth had a standard "NVIDIA Partner" (or "Preferred Partner!") placard on their main desk they had a bare NVIDIA GB200 baseboard (superchip) on display there wasn't much other differentiation From this, I gather that not many companies have manufactured GB200 nodes yet, or if they have, there aren't enough GB200 boards available to waste them on display models. So, we had to settle for these bare NVIDIA-manufactured, 4-GPU + 2-CPU superchip boards: What struck me is that these are very large FRUs--if a single component (CPU, GPU, voltage regulator, DRAM chip, or anything else) goes bad, you have to yank and replace four GPUs and two CPUs. And because all the components are soldered down, someone's going to have to do a lot of work to remanufacture these boards to avoid throwing out a lot of very expensive, fully functional Blackwell GPUs. There were a few companies who were further along their GB200 journey and had more integrated nodes on display. The HPE Cray booth had this GB200 NVL4 blade (the Cray EX154n) on display: It looks remarkably sparse compared to the super-dense blades that normally slot into the Cray EX line, but even with a single NVL4 node per blade, the Cray EX cabinet only supports 56 of these blades, leaving 8 blade slots empty in the optimal configuration. I assume this is a limitation of power and cooling. The booth collateral around this blade suggested its use case is "machine learning and sovereign AI" rather than traditional HPC, and that makes sense since each node has 768 GB of HBM3e which is enough to support training some pretty large sovereign models. However, the choice to force all I/O traffic on to the high-speed network by only leaving room for one piddly node-local NVMe drive (this blade only supports one SSD per blade) will make training on this platform very sensitive to the quality of the global storage subsystem. This is great if you bundle this blade with all-flash Lustre (like Cray ClusterStor) or DAOS (handy, since Intel divested the entire DAOS development team to HPE). But it's not how I would build an AI-optimized system. I suspect the cost-per-FLOP of this Cray GB200 solution is much lower than what a pure-play GB200 for LLM training would be. And since GB200 is actually a solid platform for FP64 (thanks to Dan Ernst for challenging me on this and sharing some great resources on the topic), I expect to see this node do well in situations that are not training frontier LLMs, but rather fine-tuning LLMs, training smaller models, and mixing in traditional scientific computing on the same general-purpose HPC/AI system. Speaking of pure-play LLM training platforms, though, I was glad that very few exhibitors were trying to talk up GB200 NVL72 this year. It may have been the case that vendors simply aren't ready to begin selling NVL72 yet, but I like to be optimistic and instead believe that the exhibitors who show up to SC24 know that the scientific computing community likely won't get enough value out of a 72-GPU coherence domain to justify the additional cost and complexity of NVL72. I didn't see a single vendor with a GB200 NVL36 or NVL72 rack on display (or a GH200 NVL32, for that matter), and not having to think about NVL72 for the week of SC24 was a nice break from my day job. Perhaps the closest SC24 got to NVL72 was a joint announcement at the beginning of the week by Dell and CoreWeave, who announced that they have begun bringing GB200 NVL72 racks online. Dell did have a massive, AI-focused booth on the exhibit floor, and they did talk up their high-powered, liquid-cooled rack infrastructure. But in addition to supporting GB200 with NVLink Switches, I'm sure that rack infrastructure would be equally good at supporting nodes geared more squarely at traditional HPC. Slingshot 400 HPE Cray also debuted a new 400G Slingshot switch, appropriately named Slingshot 400. I didn't get a chance to ask anyone any questions about it, but from the marketing material that came out right before the conference, it sounds like a serdes upgrade without any significant changes to Slingshot's L2 protocol. There was a Slingshot 400 switch for the Cray EX rack on display at their booth, and it looked pretty amazing: It looks way more dense than the original 200G Rosetta switch, and it introduces liquid-cooled optics. If you look closely, you can also see a ton of flyover cables connecting the switch ASIC in the center to the transceivers near the top; similar flyover cables are showing up in all manner of ultra-high-performance networking equipment, likely reflecting the inability to maintain signal integrity across PCB traces. The port density on Slingshot 400 remains the same as it was on 200G Slingshot, so there's still only 64 ports per switch, and the fabric scale limits don't increase. In addition, the media is saying that Slingshot 400 (and the GB200 blade that will launch with it) won't start appearing until "Fall 2025." Considering 64-port 800G switches (like NVIDIA's SN5600 and Arista's 7060X6) will have already been on the market by then though, Slingshot 400 will be launching with HPE Cray on its back foot. However, there was a curious statement on the placard accompanying this Slingshot 400 switch: It reads, "Ultra Ethernet is the future, HPE Slingshot delivers today!" Does this suggest that Slingshot 400 is just a stopgap until 800G Ultra Ethernet NICs begin appearing? If so, I look forward to seeing HPE Cray jam third-party 800G switch ASICs into the Cray EX liquid-cooled form factor at future SC conferences. Grace-Grace for storage? One of the weirder things I saw on the exhibit floor was a scale-out storage server built on NVIDIA Grace CPUs that the good folks at WEKA had on display at their booth. Manufactured by Supermicro, this "ARS-121L-NE316R" server (really rolls off the tongue) uses a two-socket Grace superchip and its LPDDR5X instead of conventional, socketed CPUs and DDR. The rest of it seems like a normal scale-out storage server, with sixteen E3.S SSD slots in the front and four 400G ConnectX-7 or BlueField-3 NICs in the back. No fancy dual-controller failover or anything like that; the presumption is that whatever storage system you'd install over this server would implement its own erasure coding across drives and servers. At a glance, this might seem like a neat idea for a compute-intensive storage system like WEKA or DAOS. However, one thing that you typically want in a storage server is high reliability and repairability, features which weren't the optimal design point for these Grace superchips. Specifically, The Grace-Grace superchip turn both CPU sockets into a single FRU. This means that if one CPU goes bad, you're shipping the whole board back to NVIDIA rather than just doing a field-swap of a socket. Grace uses LPDDR5X, whose ECC is not as robust as DDR5. I'm not an expert on memory architecture, but my understanding is that the ECC scheme on Grace does not provide ChipKill or row failures. And as with CPU failure, if a single DRAM chip goes back, you're throwing out two CPUs and all the DRAM. There's no way to value-engineer the exact quantity of cores, clock, and DRAM to be optimal for the storage software installed on top of these servers. On the upside, though, there might be a cost advantage to using this Grace-Grace server over a beefier AMD- or Intel-based server with a bunch of traditional DIMMs. And if you really like NVIDIA products, this lets you do NVIDIA storage servers to go with your NVIDIA network and NVIDIA compute. As long as your storage software can work with the interrupt rates of such a server (e.g., it supports rebuild-on-read) and the 144 Neoverse V2 cores are a good fit for its computational requirements (e.g., calculating complex erasure codes), this server makes sense. But building a parallel storage system on LPDDR5X still gives me the willies. I could also see this thing being useful for certain analytics workloads, especially those which may be upstream of LLM training. I look forward to hearing about where this turns up in the field. Microsoft and AMD's new HBM CPU The last bit of new and exciting HPC technology that I noted came from my very own employer in the form of HBv5, a new, monster four-socket node featuring custom-designed AMD CPUs with HBM. STH wrote up an article with great photos of HBv5 and its speeds and feeds, but in brief, this single node has: 384 physical Zen 4 cores (352 accessible from within the VM) that clock up to 4 GHz 512 GB of HBM3 (up to 450 GB accessible from the VM) with up to 6.9 TB/s STREAM bandwidth 4x NDR InfiniBand NICs clocked at 200G per port 200G Azure Boost NIC (160G accessible from the VM) 8x 1.84 TB NVMe SSDs with up to 50 GB/s read and 30 GB/s write bandwidth The node itself looks kind of wacky as well, because there just isn't a lot on it: There are the obvious four sockets of AMD EPYC 9V64H, each with 96 physical cores and 128 GB of HBM3, and giant heat pipes on top of them since it's 100% air-cooled. But there's no DDR at all, no power converter board (the node is powered by a DC bus bar), and just a few flyover cables to connect the PCIe add-in-card cages. There is a separate fan board with just two pairs of power cables connecting to the motherboard, and that's really about it. The front end of the node shows its I/O capabilities which are similarly uncomplicated: There are four NDR InfiniBand cards (one localized to each socket) which are 400G-capable but cabled up at 200G, eight E1.S NVMe drives, and a brand-new dual-port Azure Boost 200G NIC. Here's a close-up of the right third of the node's front: This is the first time I've seen an Azure Boost NIC in a server, and it looks much better integrated than the previous-generation 100G Azure SmartNIC that put the FPGA and hard NIC on separate boards connected by a funny little pigtail. This older 100G SmartNIC with pigtail was also on display at the Microsoft booth in an ND MI300X v5 node: And finally, although I am no expert in this new node, I did hang around the people who are all week, and I repeatedly heard them answer the same few questions: Is this MI300C? It is if you want it to be. You can call it Sally if you want; I don't think it will care. But Microsoft calls it HBv5, and the processor name will show up as AMD EPYC 9V64H in /proc/cpuinfo. Is its InfiniBand 1x800 port, 2x400 ports, ...? There are four NDR InfiniBand HCA cards, and each card has one full 400G NDR InfiniBand port. However, each port is only connected up to top-of-rack switching at 200G. Each InfiniBand HCA hangs off of a different EPYC 9V64H socket so that any memory address can get to InfiniBand without having to traverse Infinity Fabric. Running four ports of NDR InfiniBand at half speed is an unusual configuration, but that's what's going on here. How can I buy this CPU? EPYC 9V64H are "custom AMD EPYC processors only available in Azure." This means the only way to access it is by provisioning an HBv5 virtual machine in Azure. Amidst all the unrelenting news about new GPUs optimized for AI workloads, it was nice to see something new and unique launched squarely for the benefit of traditional scientific computing workloads. The HPC industry overall New technology announcements are always exciting, but one of the main reasons I attend SC and ISC is to figure out the broader trends shaping the HPC industry. What concerns are top of mind for the community, and what blind spots remain open across all the conversations happening during the week? Answering these questions requires more than just walking the exhibit floor; it involves interpreting the subtext of the discussions happening at panels and BOF sessions. However, identifying where the industry needs more information or a clearer picture informs a lot of the public-facing talks and activities in which I participate throughout the year. What I learned about the average SC technical program attendee The biggest realization that I confirmed this week is that the SC conference is not an HPC conference; it is a scientific computing conference. I sat in a few sessions where the phrase "HPC workflows" was clearly a stand-in for "scientific workflows," and "performance evaluation" still really means "MPI and OpenMP profiling." I found myself listening to ideas or hearing about tools that were intellectually interesting but ultimately not useful to me because they were so entrenched in the traditions of applying HPC to scientific computing. Let's talk about a few ways in which this manifested. People think sustainability and energy efficiency are the same thing Take, for example, the topic of sustainability. There were talks, panels, papers, and BOFs that touched on the environmental impact of HPC throughout the week, but the vast majority of them really weren't talking about sustainability at all; they were talking about energy efficiency. These talks often use the following narrative: Energy use from datacenters is predicted to reach some ridiculous number by 2030 We must create more energy-efficient algorithms, processors, and scheduling policies Here is an idea we tested that reduced the energy consumption without impacting the performance of some application or workflow Sustainability achieved! Success! The problem with this approach is that it declares victory when energy consumption is reduced. This is a great result if all you care about is spending less money on electricity for your supercomputer, but it completely misses the much greater issue that the electricity required to power an HPC job is often generated by burning fossil fuels, and that the carbon emissions that are directly attributable to HPC workloads are contributing to global climate change. This blind spot was exemplified by this slide, presented during a talk titled "Towards Sustainable Post-Exascale Leadership Computing" at the Sustainable Supercomputing workshop: I've written about this before and I'll write about it again: FLOPS/Watt and PUE are not meaningful metrics by themselves when talking about sustainability. A PUE of 1.01 is not helpful if the datacenter that achieves it relies on burning coal for its power. Conversely, a PUE of 1.5 is not bad if all that electricity comes from a zero-carbon energy source. The biggest issue that I saw being reinforced at SC this year is that claims of "sustainable HPC" are accompanied by the subtext of "as long as I can keep doing everything else the way I always have." There were glimmers of hope, though. Maciej Cytowski from Pawsey presented the opening talk at the Sustainable Supercomputing workshop, and he led with the right thing--he acknowledged that 60% of the fuel mix that powers Pawsey's supercomputers comes from burning fossil fuels: Rather than patting himself on the back at his low PUE, Dr. Cytowski's described on how they built their datacenter atop a large aquifer from which they draw water at 21°C and return it at 30°C to avoid using energy-intensive chillers. To further reduce the carbon impact of this water loop, Pawsey also installed over 200 kW of solar panels on its facility roof to power the water pumps. Given the fact that Pawsey cannot relocate to somewhere with a higher ratio of zero-carbon energy on account of its need to be physically near the Square Kilometer Array, Cytowski's talk felt like the most substantive discussion on sustainability in HPC that week. Most other talks and panels on the topic really wanted to equate "sustainability" to "FLOPS per Watt" and pretend like where one deploys a supercomputer is not a part of the sustainability discussion. The reality is that, if the HPC industry wanted to take sustainability seriously, it would talk less about watts and more about tons of CO2. Seeing as how the average watt of electricity in Tennessee produces 2.75x more carbon than a watt of electricity in Washington, the actual environmental impact of fine-tuning Slurm scheduling or fiddling with CPU frequencies is meaningless when compared to the benefits that would be gained by deploying that supercomputer next to a hydroelectric dam instead of a coal-fired power plant. I say all this because there are parts of the HPC industry (namely, the part in which I work) who are serious about sustainability. And those conversations go beyond simply building supercomputers in places where energy is low-carbon (thereby reducing Scope 2 emissions). They include holding suppliers to high standards on reducing the carbon impact of transporting people and material to these data centers, reducing the carbon impact of all the excess packaging that accompanies components, and being accountable for the impact of everything in the data center after it reaches end of life (termed Scope 3 emissions). The HPC community--or more precisely, the scientific computing community--is still married to the idea that the location of a supercomputer is non-negotiable, and "sustainability" is a nice-to-have secondary goal. I was hoping that the sessions I attended on sustainability would approach this topic at a level where the non-scientific HPC world has been living. Unfortunately, the discussion at SC24, which spanned workshops, BOFs, and Green 500, remains largely stuck on the idea that PUE and FLOPS/Watt are the end-all sustainability metrics. Those metrics are important, but there are global optimizations that have much greater effects on reducing the environmental impact of the HPC industry. AI sessions are really scientific computing sessions about AI Another area where "HPC" was revealed to really mean "scientific computing" was in the topic of AI. I sat in on a few BOFs and panels around AI topics to get a feel for where this community is in adopting AI for science, but again, I found the level of discourse to degrade to generic AI banter despite the best efforts of panelists and moderators. For example, I sat in the "Foundational Large Language Models for High-Performance Computing" BOF session, and Jeff Vetter very clearly defined what a "foundational large language model" was at the outset so we could have a productive discussion about their applicability in HPC (or, really, scientific computing): The panelists did a good job of outlining their positions. On the upside, LLMs are good for performing source code conversion, documenting and validating code, and maximizing continuity in application codes that get passed around as graduate students come and go. On the downside, they have a difficult time creating efficient parallel code, and they struggle to debug parallel code. And that's probably where the BOF should have stopped, because LLMs, as defined at the outset of the session, don't actually have a ton of applicability in scientific computing. But as soon as the session opened up to audience questions, the session went off the rails. The first question was an extremely basic and nonspecific question: "Is AI a bubble?" It's fun to ask provocative questions to a panel of experts. I get it. But the question had nothing to do with LLMs, any of the position statements presented by panelists, or even HPC or scientific computing. It turned a BOF on "LLMs for HPC" into a BOF that might as well have been titled "Let's just talk about AI!" A few panelists tried to get things back on track by talking about the successes of surrogate models to simulate physical processes, but this reduced the conversation to a point where "LLMs" really meant "any AI model" and "HPC" really meant "scientific simulations." Perhaps the most productive statement to come out of that panel was when Rio Yokota asserted that "we" (the scientific community) should not train their own LLMs, because doing so would be "unproductive for science." But I, as well as anyone who understands the difference between LLMs and "AI," already knew that. And the people who don't understand the difference between an LLM and a surrogate model probably didn't pick up on Dr. Yokota's statement, so I suspect the meaning of his contribution was completely lost. Walking out of that BOF (and, frankly, the other AI-themed BOFs and panels I attended), I was disappointed at how superficial the conversation was. This isn't to say these AI sessions were objectively bad; rather, I think it reflects the general state of understanding of AI amongst SC attendees. Or perhaps it reflects the demographic that is drawn to these sorts of sessions. If the SC community is not ready to have a meaningful discussion about AI in the context of HPC or scientific computing, attending BOFs with like-minded peers is probably a good place to begin getting immersed. But what became clear to me this past week is that SC BOFs and panels with "AI" in their title aren't really meant for practitioners of AI. They're meant for scientific computing people who are beginning to dabble in AI. AI for operations is not yet real in scientific computing I was invited to sit on a BOF panel called "Artificial Intelligence and Machine Learning for HPC Workload Analysis" following on a successful BOF in which I participated at ISC24. The broad intent was to have a discussion around the tools, methods, and neat ideas that HPC practitioners have been using to better understand workloads, and each of us panelists was tasked with talking about a project or idea we had in applying AI/ML to improve some aspect of workloads. What emerged from us speakers' lightning talks is that applying AI for operations--in this case, understanding user workloads--is nascent. Rather than talking about how we use AI to affect how we design or operate supercomputers, all of us seemed to focus more on how we are collecting data and beginning to analyze that data using ML techniques. And maybe that's OK, because AI won't ever do anything for workload characterization until you have a solid grasp of the telemetry you can capture about those workloads in the first place. But when we opened the BOF up to discussion with all attendees, despite having a packed room, there was very little that the audience had. Our BOF lead, Kadidia Konaté, tried to pull discussion out of the room from a couple of different fronts by asking what tools people were using, what challenges they were facing, and things along those lines. However, it seemed to me that the majority of the audience was in that room as spectators; they didn't know where to start applying AI towards understanding the operations of supercomputers. Folks attended to find out the art of the possible, not talk about their own challenges. As such, the conversation wound up bubbling back up to the safety of traditional topics in scientific computing--how is LDMS working out, how do you deal with data storage challenges of collecting telemetry, and all the usual things that monitoring and telemetry folks worry about. It's easy to talk about the topics you understand, and just as the LLM conversation reverted back to generic AI for science and the sustainability topic reverted back to FLOPS/Watt, this topic of AI for operations reverted back to standard telemetry collection. Some are beginning to realize that HPC exists outside of scientific computing Despite the pervasive belief at SC24 that "HPC" and "scientific computing" are the same thing, there are early signs that the leaders in the community are coming to terms with the reality that there is now a significant amount of leadership HPC happening outside the scope of the conference. This was most prominent at the part of the Top500 BOF where Erich Strohmaier typically discusses trends based on the latest publication of the list. In years past, Dr. Strohmaier's talk was full of statements that strongly implied that, if a supercomputer is not listed on Top500, it simply does not exist. This year was different though: he acknowledged that El Capitan, Frontier, and Aurora were "the three exascale systems we are aware of," now being clear that there is room for exascale systems to exist that simply never ran HPL, or never submitted HPL results to Top500. He explicitly acknowledged again that China has stopped making any Top500 submissions, and although he didn't name them outright, he spent a few minutes dancing around "hyperscalers" who have been deploying exascale class systems such as Meta's H100 clusters (2x24K H100), xAI's Colossus (100K H100), and the full system behind Microsoft's Eagle (14K H100 is a "tiny fraction"). Strohmaier did an interesting analysis that estimated the total power of the Top500 list's supercomputers so he could compare it to industry buzz around hyperscalers building gigawatt-sized datacenters: It was a fun analysis where he concluded that there are between 500-600 megawatts of supercomputers on the Top500 list, and after you factor in storage, PUE, and other ancillary power sources, the whole Top500 list sums up to what hyperscalers are talking about sticking into a single datacenter facility. Although he didn't say it outright, I think the implication here is that the Top500 list is rapidly losing relevance in the broad HPC market, because a significant amount of the world's supercomputing capacity and capability are absent from the list. Although specific hyperscale supercomputers (like Meta's, xAI's, and Microsoft's) were not mentioned outright, their absence from the Top500 list suggests that this list might already be more incomplete than it is complete--the sum of the FLOPS or power on the Top500 supercomputers may be less than the sum of the giant supercomputers which are known but not listed. This will only get worse as the AI giants keep building systems every year while the government is stuck on its 3-5 year procurement cycles. It follows that the meaning of the Top500 is sprinting towards a place where it is not representative of HPC so much as it is representative of the slice of HPC that serves scientific computing. Erich Strohmaier was clearly aware of this in his talk this year, and I look forward to seeing how the conversation around the Top500 list continues to morph as the years go on. NSF's broad front vs. DOE's big bets in HPC and AI My career was started at an NSF HPC center and built up over my years in the DOE, so I feel like I owe a debt to the people who provided all the opportunities and mentorship that let me get to the place of privilege in the hyperscale/AI industry that I now enjoy. As a result, I find myself still spending a lot of my free time thinking about the role of governments in the changing face of HPC (as evidenced by my critiques of thinktank reports and federal RFIs...) and trying to bridge the gap in technical understanding between my old colleagues (in DOE, NSF, and European HPC organizations) and whatever they call what I work on now (hyperscale AI?). To that end, I found myself doing quite a bit of business development (more on this later) with government types since I think that is where I can offer the most impact. I used to be government, and I closely follow the state of their thinking in HPC, but I also know what's going on inside the hyperscale and AI world. I also have enough context in both areas to draw a line through all the buzzy AI press releases to demonstrate how the momentum of private-sector investment in AI might affect the way national HPC efforts do business. So, I did a lot of talking to both my old colleagues in DOE and their industry partners in an attempt to help them understand how the hyperscale and AI industry thinks about infrastructure, and what they should expect in the next year. More importantly though, I also sat in on a couple of NSF-themed BOFs to get a better understanding of where their thinking is, where NAIRR is going, how the NSF's strategy contrasts with DOE's strategy, and where the ambitions of the Office of Advanced Cyberinfrastructure might intersect with the trajectory of hyperscale AI. What I learned was that NSF leadership is aware of everything that the community should be concerned about: the growth of data, the increasing need for specialized silicon, the incursion of AI into scientific computing, new business models and relationships with industry, and broadening the reach of HPC investments to be globally competitive. But beyond that, I struggled to see a cohesive vision for the future of NSF-funded supercomputing.  A BOF with a broad range of stakeholders probably isn't the best place to lay out a vision for the future of NSF's HPC efforts, and perhaps NSF's vision is best expressed through its funding opportunities and awards. Whichever the case may be, it seems like the NSF remains on a path to make incremental progress on a broad front of topics. Its Advanced Computing Systems and Services (ACSS) program will continue to fund the acquisition of newer supercomputers, and a smorgasbord of other research programs will continue funding efforts across public access to open science, cybersecurity, sustainable software, and other areas. My biggest concern is that peanut-buttering funding across such a broad portfolio will make net forward progress much slower than taking big bets. Perhaps big bets just aren't in the NSF's mission though. NAIRR was also a topic that came up in every NSF-themed session I attended, but again, I didn't get a clear picture of the future. Most of the discussion that I heard was around socializing the resources that are available today through NAIRR, suggesting that the pilot's biggest issue is not a lack of HPC resources donated by industry, but awareness that NAIRR is a resource that researchers can use. This was reinforced by a survey whose results were presented in the NAIRR BOF: It seems like the biggest challenges facing the NSF community relying on NAIRR (which has its own sample bias) is that they don't really know where to start even though they have AI resources (both GPUs and model API services) at their disposal. In a sense, this is a great position for the NSF since its users need intellectual help more than access to GPU resources, and the NSF has been great at promoting education, training, and workforce development. its users are unlikely to demand the same cutting-edge GPUs that AI industry leaders are snapping up. For example, the largest pool of GPUs in NAIRR are A100 GPUs that NVIDIA donated via DGX Cloud; the big AI companies moved off of Ampere a year ago and are about to move off of Hopper. However, it also means that there's not a clear role for partnership with many industry players beyond donating resources to the NAIRR pilot today in the hopes of selling resources to the full NAIRR tomorrow. I asked what OAC leadership thought about moving beyond such a transactional relationship between NSF and industry at one of the BOFs I attended, and while the panelists were eager to explore specific answers to that question, I didn't hear any ideas that would approach some sort of truly equitable partnership where both parties contributed in-kind. I also walked away from these NSF sessions struck by how different the NSF HPC community's culture is from that of the DOE. NSF BOF attendees seemed focused on getting answers and guidance from NSF leadership, unlike the typical DOE gathering, where discussions often revolve around attendees trying to shape priorities to align with their own agendas. A room full of DOE people tends to feel like everyone thinks they're the smartest person there, while NSF gatherings appear more diverse in the expertise and areas of depth of its constituents. Neither way is inherently better or worse, but it will make the full ambition of NAIRR (as an inter-agency collaboration) challenging to navigate. This is particularly relevant as DOE is now pursuing its own multi-billion-dollar AI infrastructure effort, FASST, that appears to sidestep NAIRR. Exhibitor trends There's no better way to figure out what's going on in the HPC industry than walking the exhibit floor each year, because booths cost money and reflect the priorities (and budgets) of all participants. This year's exhibit felt physically huge, and walking from one end to the other was an adventure. You can get a sense of the scale from this photo I took during the opening gala: Despite having almost 18,000 registrants and the opening gala usually being a crush of people, the gala this year felt and looked very sparse just because people and booths were more spread out. There was also a perceptibly larger number of splashy vendors who have historically never attended before who were promoting downstream HPC technologies like data center cooling and electrical distribution, and there was healthy speculation online about whether the hugeness of the exhibit this year was due to these new power and cooling companies. To put these questions to rest, I figured out how to yank down all the exhibitor metadata from the conference website so I could do some basic analysis on it. Booths by the numbers The easiest way to find the biggest companies to appear this year was to compare the exhibitor list and booth sizes from SC23 to this year and see whose booth went from zero to some big square footage. I only took the top twenty new vendors, but they broadly fall into a couple of categories: Power and cooling: Stulz, Delta, Airedale, Valvoline, Boundary Electric, Schneider Electric, Mara Server manufacturing: Wistron, AMI, Pegatron Higher ed: Tennessee Tech, SCRCC There were a couple other companies that must've just missed last SC but aren't new to the show (NetApp, Ansys, Samsung, Micron, Broadcom). And curiously, only one new GPU-as-a-Service provider (Nebius) showed up this year, suggesting last year was the year of the GPU Cloud. But to confirm what others had speculated: yes, a significant amount of the new square footage of the exhibit floor can be attributed to companies focused on power and cooling. This is an interesting indicator that HPC is becoming mainstream, largely thanks to AI demanding ultra-high density of power and cooling. But it's also heartening to see a few new exhibitors in higher education making an appearance. Notably, SCRCC (South Carolina Research Computing Consortium) is a consortium between Clemon, University of South Carolina, and Savannah River National Laboratory that just formed last year, and I look forward to seeing what their combined forces can bring to bear. We can also take a look at whose booths grew the most compared to SC23: This distribution is much more interesting, since the top 20 exhibitors who grew their footprint comprise the majority of the growth in existing exhibitors. Cherry-picking a few interesting growers: Power and cooling: USystems, Midas, Vertiv Data center/GPUaaS: iM, Iris Energy, and (arguably) Oracle Software: Arc Compute and CIQ Companies facing serious financial or legal troubles: I count at least three! Impressive that they are still pouring money into their SC booths. It's also interesting to see HLRS, the German national HPC center, grow so significantly. I'm not sure what prompted such a great expansion, but I take it to mean that things have been going well there. Finally, Dell had a massive booth and showing this year. Not only did they grow the most since SC23, but they had the single largest booth on the exhibit floor at SC24. This was no doubt a result of their great successes in partnering with NVIDIA to land massive GPU buildout deals at places like xAI and CoreWeave. They also had "AI factory" messaging emblazoned all over their marketing material and debuted a nice 200 kW liquid-cooled rack that will be the basis for their GB200 NVL72 solution, clearly leaning into the idea that they are leaders in AI infrastructure. Despite this messaging being off-beat for the SC audience as I've described earlier, their booth was surprisingly full all the time, and I didn't actually get a chance to get in there to talk to anyone about what they've been doing. Equally interesting are the vendors who reduced their footprint at SC24 relative to SC23: Reading too much into any of these big shrinkers is pretty easy; while a reduction in booth size could suggest business hasn't been as good, it could equally mean that an exhibitor just went overboard at SC23 and downsized to correct this year. A few noteworthy exhibitors to call out: Penguin and the Korea Semiconductor Industry Association both cut way back from massive 50x50 booths to 30x30. Their booths this year were both big, but they weren't massive. Viridien, formerly known as CGG, also shrunk from a massive booth to a less-massive 30x40. Juniper still kept an independent booth, but it is in the process of being absorbed into HPE. Shrinking makes sense. Major cloud providers Google and AWS scaled back, but Microsoft did not. GPU-as-a-Service cloud providers CoreWeave and Lambda both scaled back. Since these GPUaaS providers' business models typically rely on courting few big customers, it may make sense to cut back on booth volume. Major AI storage companies DDN, VAST, and (to a lesser degree) Pure also scaled back, while WEKA did not. I know business for DDN and VAST has been great this past year, so these may just reflect having gone overboard last year. Overall, almost twice as many vendors grew their booths than scaled back, so I'd caution anyone against trying to interpret any of this as anything beyond exhibitors right-sizing their booths after going all-in last year. Finally, there are a handful of vendors who disappeared outright after SC23: It is critical to point out that the largest booths to vanish outright were all on the smaller size: SUSE, Tenstorrent, and Symbiosys Alliance all disappeared this year, but their booths last year were only 20x30. I was surprised to see that Tenstorrent and Arm didn't have booths, but the others are either companies I haven't heard of (suggesting the return on investment of showing at SC might've been low), are easy to rationalize as only being HPC-adjacent (such as SNIA and DigitalOcean), or simply went bankrupt in the last year. As we say at the business factory, the net-net of the exhibit hall this year is that the square footage of booth space increased by 15,000 square feet, so it was in fact bigger, it did take longer to walk from one end to the other, and there definitely were a bunch of new power and cooling companies filling out the space. Some exhibitors shrank or vanished, but the industry as a whole appears to be moving in a healthy direction. And if you're interested in analyzing this data more yourself, please have a look at the data and the Jupyter notebook I used to generate the above treemaps on GitHub. If you discover anything interesting, please write about it and post it online! Proliferation of GPU-as-a-Service providers As an AI infrastructure person working for a major cloud provider, I kept an eye out for all the companies trying to get into the GPU-as-a-Service game. I described these players last year as "pure-play GPU clouds," and it seems like the number of options available to customers who want to go this route is growing. But I found it telling that a lot of them had booths that were completely indistinguishable from each other. Here's an example of one: As best I can tell, these companies are all NVIDIA preferred partners with data centers and a willingness to deploy NVIDIA GPUs, NVIDIA SmartNICs, and NVIDIA cloud stack, and sell multi-year commitments to consume those GPUs. I tried to accost some of these companies' booth staff to ask them my favorite question ("What makes you different from everyone else?"), but most of these companies' booths were staffed by people more interested in talking to each other than me. These GPUaaS providers tend to freak me out, because, as Microsoft's CEO recently stated, these companies are often "just a bunch of tech companies still using VC money to buy a bunch of GPUs." I can't help but feel like this is where the AI hype will come back to bite companies who have chosen to build houses upon sand. Walking the SC24 exhibit floor is admittedly a very narrow view of this line of business, but it seemed like some of these companies were content to buy up huge booths, hang a pretty banner above it, and otherwise leave the booth empty of anything beyond a few chairs and some generic value propositions. I didn't feel a lot of hunger or enthusiasm from these companies despite the fact that a bunch of them have hundreds of millions of dollars of GPUs effectively sitting on credit cards that they are going to have to make payments on for the next five years. That all said, not all the companies in the GPUaaS are kicking back and letting the money pour in. In particular, I spent a few minutes chatting up someone at the CoreWeave booth, and I was surprised to hear about how much innovation they're adding on top of their conventional GPUaaS offering. For example, they developed Slurm on Kubernetes (SUNK) with one of their key customers to close the gap between the fact that CoreWeave exposes its GPU service through Kubernetes, but many AI customers have built their stack around Slurm, pyxis, and enroot. In a weird twist of fate, I later ran into an old acquaintance who turned out to be one of the key CoreWeave customers for whom SUNK was developed. He commented that SUNK is the real deal and does exactly what his users need which, given the high standards that this person has historically had, is a strong affirmation that SUNK is more than just toy software that was developed and thrown on to GitHub for an easy press release. CoreWeave is also developing some interesting high-performance object storage caching software, and all of these software services are provided at no cost above whatever customers are already paying for their GPU service. I bring this up because it highlights an emerging distinction in the GPUaaS market, which used to be a homogenous sea of bitcoin-turned-AI providers. Of course, many companies still rely on that simple business model: holding the bill for rapidly depreciating GPUs that NVIDIA sells and AI startups consume. However, there are now GPUaaS providers moving up the value chain by taking on the automation and engineering challenges that model developers don't want to deal with. Investing in uncertain projects like new software or diverse technology stacks is certainly risky, especially since they may never result in enough revenue to pay for themselves. But having a strong point of view, taking a stance, and investing in projects that you feel are right deserves recognition. My hat is off to the GPUaaS providers who are willing to take these risks and raise the tide for all of us rather than simply sling NVIDIA GPUs to anyone with a bag of money. Community and connections As much as I enjoy increasing shareholder value, the part of SC that gives me the greatest joy is reconnecting with the HPC community. Knowing I'll get to chat with my favorite people in the industry (and meet some new favorite people!) makes the long plane rides, upper respiratory infections, and weird hotel rooms completely worth it. I wound up averaging under six hours of sleep per night this year in large part because 9pm or 7am were often the only free times I had to meet with people I really wanted to see. I have this unhealthy mindset where every hour of every day, from the day I land to the day I leave, is too precious to waste, and it's far too easy for me to rationalize that spending an hour talking to someone interesting is worth losing an hour of sleep. But like I said at the outset of this blog post, this year felt different for a few reasons, and a lot of them revolve around the fact that I think I'm getting old. Now, it's always fun to say "I'm getting old" in a mostly braggadocious way, but this feeling manifested in concrete ways that affected the way I experienced the conference: I hit my limit on Monday night and couldn't get home without spending 15 minutes sitting in an unlit playground across from the World of Coke. I've always gotten blisters and fatigue, but this was the first time I couldn't just cowboy up and muscle through it. To avoid a repeat of this, I wound up "wasting" (see above) a lot more time to just get off my feet this year. This year, I reached the point where I need to start time-box how much time I spend chatting up the folks I bump into. I used to just let the good times roll if I ran into someone I knew, but this year I wound up spending as much time attending sessions as I did missing sessions because I got caught up in a conversation. This isn't a bad thing per se, but I did feel a little sour when I realized I'd made a bad bet on choosing to chat instead of attending a session or vice versa, and this bad feeling lingered in the back of my mind just about every day. There weren't a lot of surprises for me at the conference this year, and I worry that I am at risk of losing touch with the technical aspects of the conference that get newer attendees excited. Instead of hearing about, say, the latest research in interconnects, more of my time was spent mucking it up with the sorts of people in the HPC community who I used to find intimidating. On the one hand, hooray me for making it into old boys' clubs. But on the other, I don't want to become some HPC greybeard whose last meaningful contribution to the industry was twenty years ago. This is the first year where I've had people accost me and ask me for advice. I've long been accosted by strangers because of my online presence, but those interactions were always lighthearted exchanges of "I follow you on Twitter" and "Great to meet you. Have an @HPC_Guru pin." This year, I had people specifically ask me for advice on industry versus postdoc, AI versus HPC, and what my master plan was when I left NERSC. Even though I didn't have any sage advice, I still found it really hard to tell bright-eyed students to go kick rocks just so I wouldn't be late for yet another mushy panel on AI. If you read this all and think "boo hoo, poor Glenn is too popular and wise for his own good," yeah, I get it. There are worse problems to have. But this was the first year where I felt like what I put into the conference was greater than what I got out of it. Presenting at SC used to be at least as good for my career as it was useful for my audiences, but it just doesn't count for much given my current role and career stage. It felt like some of the magic was gone this year in a way I've never experienced before.  Getting to know people As the years have gone on, I spend an increasing amount of my week having one-on-one conversations instead of wandering aimlessly. This year though, I came to SC without really having anything to buy or sell: I am not a researcher, so I don't need to pump up the work I'm doing to impress my fellow researchers. I no longer own a product market segment, so I don't directly influence the customers or vendors with whom my employer works. I don't have any bandwidth in my day job to support any new customers or partnerships, so I don't have a strong reason to sell people on partnering with me or my employer.  Much to my surprise though, a bunch of my old vendor/partner colleagues still wanted to get together to chat this year. Reflecting back, I was surprised to realize that it was these conversations--not the ones about business--that were the most fulfilling this year. I learned about people's hobbies, families, and their philosophies on life, and it was amazing to get to know some of the people behind the companies with whom I've long dealt. I was reminded that the person is rarely the same as the company, and even behind some of the most aggressive and blusterous tech companies are often normal people with the same concerns and moments of self-doubt that everyone else has. I was also reminded that good engineers appreciate good engineering regardless of whether it's coming from a competitor or not. The public persona of a tech exec may not openly admire a competitor's product, but that doesn't mean they don't know good work when they see it. I also surprised a colleague whose career has been in the DOE labs with an anecdote that amounted to the following: even though two companies may be in fierce competition, the people who work for them don't have to be. The HPC community is small enough that almost everyone has got a pal at a competing company, and when there are deals to be made, people looove to gossip. If one salesperson hears a juicy rumor about a prospective customer, odds are that everyone else on the market will hear about it pretty quickly too. Of course, the boundaries of confidentiality and professionalism are respected when it matters, but the interpersonal relationships that are formed between coworkers and friends don't suddenly disappear when people change jobs. And so, I guess it would make sense that people still want to talk to me even though I have nothing to buy or sell. I love trading gossip just as much as everyone else, and I really enjoyed this aspect of the week. Talking to early career people I also spent an atypically significant amount of my week talking to early career people in HPC who knew of me one way or another and wanted career advice. This is the first year I recall having the same career conversations with multiple people, and this new phase of my life was perhaps most apparent during the IEEE TCHPC/TCPP HPCSC career panel in which I was invited to speak this year. It was an honor to be asked to present on a career panel, but I didn't feel very qualified to give career advice to up-and-coming computer science graduate students who want to pursue HPC. I am neither a computer scientist nor a researcher, but fortunately for me, my distinguished co-panelists (Drs. Dewi Yokelson, Olga Pearce, YJ Ji, and Rabab Alomairy) had plenty of more relevant wisdom to share. And at the end of the panel, there were a few things we all seemed to agree on as good advice: Knowing stuff is good, but being able to learn things is better. Being eager to learn and naturally curious makes this much easier as well. The life of a researcher sometimes requires more than working a standard nine-to-five, so it'll be hard to be really successful if your heart isn't in it. People will forget what you did or what you said, but they remember how you made them feel. Don't be a jerk, because this community is small. In both this panel the one-on-one conversations I had with early career individuals, the best I could offer was the truth: I never had a master plan that got me to where I am; I just try out new things until I realize I don't like doing them anymore. I never knew what I wanted to be when I grew up, and I still don't really, so it now makes me nervous that people have started approaching me with the assumption that I've got it all figured out. Unless I torpedo my career and go live on a goat farm though, maybe I should prepare for this to be a significant part of my SC experiences going forward. Shift in social media One last, big change in the community aspect of SC this year was the mass-migration of a ton of HPC folks from Twitter to Bluesky during the week prior to the conference. I don't really understand what prompted it so suddenly; a few of us have been trying for years to get some kind of momentum on other social platforms like Mastodon, but the general lack of engagement meant that all the excitement around SC always wound up exclusively on Twitter. This year was different though, and Bluesky hit critical mass with the HPC community. I personally have never experienced an SC conference without Twitter; my first SC was in 2013, and part of what made that first conference so exciting was being able to pull up my phone and see what other people were seeing, thinking, and doing across the entire convention center via Twitter. Having the social media component to the conference made me feel like I was a part of something that first year, and as the years went on, Twitter became an increasingly indispensable part of the complete SC experience for me. This year, though, I decided to try an experiment and see what SC would be like if I set Twitter aside and invested my time into Bluesky instead. The verdict? It was actually pretty nice. It felt a lot like the SC13 days, where my day ended and began with me popping open Bluesky to see what new #SC24 posts were made. And because many of the tech companies and HPC centers hadn't yet made it over, the hashtag wasn't clogged up by a bunch of prescheduled marketing blasts that buried the posts written by regular old conference attendees who were asking important questions: Which booths at #sc24 have coffee? I noticed oracle do. Anyone else? — Mike Croucher (@walkingrandomly.bsky.social) November 18, 2024 at 3:02 PM Of course, I still clogged Bluesky up with my nonsense during the week, but there was an amazing amount of engagement by a diversity of thoughtful people--many who came from Twitter, but some whose names and handles I didn't recognize. The volume of traffic on Bluesky during the week did feel a little lower than what it had been on Twitter in years past though. I also didn't see as many live posts of technical sessions as they happened, so I couldn't really tell whether I was missing something interesting in real time. This may have contributed to why I felt a little less connected to the pulse of the conference this year than I had in the past. It also could've been the fact that conference was physically smeared out across a massive space though; the sparsity of the convention center was at least on par with the sparsity on Bluesky. At the end of the week, I didn't regret the experiment. In fact, I'll probably be putting more effort into my Bluesky account than my Twitter account going forward. To be clear though, this isn't a particularly political decision on my part, and I pass no judgment on anyone who wants to use one platform over the other. It's just that I like the way I feel when I scroll through my Bluesky feeds, and I don't get that same feeling when I use Twitter. So what's the takeaway? SC this year was a great conference by almost every measure, as it always is, but it still felt a little different for me. I'm sure that some of that feeling is the result of my own growth, and my role with respect to the conference seems to be evolving from someone who gets a lot out of the conference to someone who is giving more to the conference. That's not to say that I don't get a lot out of it, though; I had no shortage of wonderful interactions with everyone from technology executives to rising stars who are early in their career, and I learned a lot about both them and me as whole people. But SC24, more than any SC before it, is when I realized this change was happening. On the technological front, we saw the debut of a new #1 system (emblazoned with the smiling face of Bronis...) and a growing crop of massive, new clusters deployed for commercial applications. The exhibit floor was quantitatively bigger, in large part due to new power and cooling companies who are suddenly relevant to the HPC world thanks to the momentum of AI. At the same time, the SC technical program is clearly separating itself out as a conference focused on scientific computing; the level of discourse around AI remains largely superficial compared to true AI conferences, the role of hyperscalers in the HPC industry is still cast more as a threat than an opportunity. For my part, I'm still trying to get a grasp on where government agencies like DOE and NSF want to take their AI ambitions so I can try to help build a better mutual understanding between the scientific computing community and the hyperscale AI community. However, it seems like the NSF is progressing slowly on a wide front, while the DOE is doing what DOE does and charging headfirst into a landscape that has changed more than I think they realize. There's a lot of technical content that I know I missed on account of the increasing time I've been spending on the people and community aspect of the conference, and I'm coming to terms with the idea that this just may be the way SC is from now on. And I think I'm okay with that, since the support of the community is what helped me go from being a bored materials science student into someone whose HPC career advice is worth soliciting in the short span of eleven years. Despite any or all of the cynicism that may come out in the things I say about this conference, SC is always the highlight of my year. I always go into it with excitement, gladly burn the candle at both ends all week, and fly home feeling both grateful for and humbled by everything the HPC community has done and continues to do to keep getting me out of bed in the morning.]]></summary></entry><entry><title type="html">The HPC cluster as a reflection of values</title><link href="https://hpc.social/personal-blog/2024/the-hpc-cluster-as-a-reflection-of-values/" rel="alternate" type="text/html" title="The HPC cluster as a reflection of values" /><published>2024-09-29T22:22:51-06:00</published><updated>2024-09-29T22:22:51-06:00</updated><id>https://hpc.social/personal-blog/2024/the-hpc-cluster-as-a-reflection-of-values</id><content type="html" xml:base="https://hpc.social/personal-blog/2024/the-hpc-cluster-as-a-reflection-of-values/"><![CDATA[<p>Yesterday while I was cooking dinner, I happened to re-watch Bryan Cantrill&#8217;s talk on &#8220;<a href="https://www.youtube.com/watch?v=Xhx970_JKX4">Platform as a Reflection of Values</a>&#8220;. (I watch a lot tech talks while cooking or baking &#8212; I often have trouble focusing on a video unless I&#8217;m doing something with my hands, but if I know a recipe well I can often make it on autopilot.)</p>

<p>If you haven&#8217;t watched this talk before, I encourage checking it out. Cantrill gave it in part to talk about why the node.js community and Joyent didn&#8217;t work well together, but I thought he had some good insights into how values get built into a technical artifact itself, as well as how the community around those artifacts will prioritize certain values.</p>

<p>While I was watching the talk (and chopping some vegetables), I started thinking about what values are most important in the &#8220;HPC cluster platform&#8221;.</p>

<p><span id="more-339"></span></p>

<h2 class="wp-block-heading">Technical values</h2>

<p>This slide from the talk shows some examples of what Cantrill thinks of as platform values:</p>

<figure class="wp-block-image size-full"><img alt="A slide with the title &quot;Some platform values&quot;. The list includes approachability, availability, compatibility, composability, debuggability, expressiveness, extensibility, interoperability, integrity, maintainability, operability, performance, portability, resiliency, rigor, robustness, safety, security, simplicity, thoroughness, transparency, and velocity." class="wp-image-340" height="538" src="https://thinking.ajdecon.org/wp-content/uploads/2024/09/image.png" width="969" /></figure>

<p>A key point from the talk is that all of these are good things! Ideally you want to have <em>all</em> of these things when you build a new platform, whether that&#8217;s a programming language, a cloud platform, or whatever. But any given platform will choose to<em> </em>prioritize some set of values over others. You want them all, but when they come into tension, which ones will win?</p>

<p>One example that Cantrill gives in the talk is the original Unix out of Bell Labs, which prioritized simplicity, composability, and portability. Certainly Unix wanted other features, like performance and maintainability, but if forced into a choice like performance vs simplicity, it would generally choose simplicity. Similarly, he talked about how JavaScript and node.js are built around values like approachability, expressiveness, and velocity, and how that contrasted with values like robustness and debuggability that Joyent valued as a cloud provider.</p>

<h2 class="wp-block-heading">The HPC cluster platform</h2>

<p>When I saw &#8220;HPC cluster platform&#8221;, I&#8217;m loosely talking about the collection of hardware and software that is most often used to build high-performance computing clusters for workloads like scientific research or machine learning training.</p>

<p>This generic platform consists of a large collection of identical compute nodes, orchestrated by a batch scheduler like <a href="https://github.com/SchedMD/slurm">Slurm</a> or <a href="https://github.com/openpbs/openpbs">PBS</a>, and with one or more &#8220;login nodes&#8221; serving as a front-end where users SSH in to prepare and run jobs on the cluster. For multi-node jobs and high-speed storage access, the compute nodes are connected by a very high-speed network, like 100Gb Ethernet or InfiniBand, which needs specific libraries to use effectively. Users on the cluster have access to command-line editors and development tools like compilers and scientific libraries, but mostly interact with the platform in a purely command line environment.</p>

<p>See also, this really ugly Google Draw diagram:</p>

<figure class="wp-block-image size-large"><img alt="A simple diagram showing a login node, a set of compute nodes, and network storage. The login node is connected to compute nodes by a management network. The storage is connected to compute nodes by a high-speed network." class="wp-image-348" height="488" src="https://thinking.ajdecon.org/wp-content/uploads/2024/09/image-1-1024x488.png" width="1024" /></figure>

<p>What values does this platform prioritize? In general, I tend to think that HPC platforms prioritize <em>performance</em>, <em>portability</em>, and <em>approachability</em>.</p>

<p><strong>Performance: </strong>This might seem obvious given the name &#8220;HPC&#8221;, but it&#8217;s worth thinking a little more about. When faced with a choice between performance and some other value, HPC clusters <em>almost always</em> choose performance. <br /><br />Performance is generally performance above cost, with most clusters using expensive compute and networking hardware. It&#8217;s prioritized over observability (&#8220;measurability&#8221; on Cantrill&#8217;s slide?), with most HPC clusters I&#8217;m aware of disabling most active monitoring features if they have a performance cost. It&#8217;s even prioritized above security, often turning off security features if they lead to lower performance or even measurable performance <em>variability</em>.</p>

<p><strong>Portability: </strong>Mindful of the difficulty in writing high-performance, correct scientific code, the HPC platform works reasonably hard to maintain portability to new hardware and software over time. </p>

<p>A lot of this is due to a robust ecosystem of libraries and middleware. Most applications that scale across multiple nodes still use <a href="https://en.wikipedia.org/wiki/Message_Passing_Interface">MPI</a>; code doing linear algebra still depends on long-lived libraries like <a href="https://www.netlib.org/lapack/">LAPACK</a> and <a href="https://www.netlib.org/blas/">BLAS</a>; and platform tools like the scheduler tend to be remarkably stable over time. New hardware features are often abstracted by middleware, especially at the networking level where support is built into your MPI library of choice.</p>

<p>This story isn&#8217;t perfect &#8212; applications usually need recompilation on a new cluster, and still often need major changes to take advantages of new features. That&#8217;s why I chose &#8220;portability&#8221; instead of &#8220;compatibility&#8221;. But as a cluster admin, I&#8217;ve worked with many researchers who have maintained the same app on many different clusters for 10, 20, or even 30 years, which is a pretty impressive portability story.</p>

<p><strong>Approachability: </strong>This one may be controversial! The average HPC cluster can seem pretty arcane, especially for someone new to the platform. But I do think that HPC prioritizes a particular <em>kind</em> of approachability, which is that it is designed to onboard scientific researchers who are not themselves expert developers.</p>

<p>A new user onboarding to a research HPC cluster frequently needs to understand three main tools:</p>

<ul class="wp-block-list">
<li><strong>The Linux shell:</strong> Most HPC cluster environments are entirely command-line oriented (though <a href="https://openondemand.org/">Open OnDemand</a> is helping change this!). You log in with SSH; edit using nano, vim, or emacs; and interact with the system entirely using a shell.</li>



<li><strong>The cluster scheduler: </strong>When you have your application ready to go, you submit your job to a queue using a cluster scheduler like Slurm and wait for it to complete. Cluster schedulers have a lot of moving parts and a user can often find endless knobs to tune, but it&#8217;s easy to get started with just a few commands. (And interestingly, almost all HPC cluster schedulers define their jobs as&#8230; shell scripts! You&#8217;re back to needing to know the shell. Annoying, sure, but at least it ain&#8217;t YAML!)</li>



<li><a href="https://modules.readthedocs.io/en/latest/"><strong>Environment modules</strong></a>: This tool allows the cluster admins to provide a large library of libraries and tools, with specific versions, such that a cluster user just needs to type &#8220;module load openmpi/3&#8221;. While the tool munges the shell environment variables as needed to set up PATH, LD_LIBRARY_PATH, etc just so.</li>
</ul>

<p>Now if this doesn&#8217;t sound like a robust software engineering environment&#8230; it isn&#8217;t! There are endless things that can go wrong, especially with environment modules interacting with the user&#8217;s own shell rc files and who knows what else. And there&#8217;s very little in this environment to encourage best practices like linting, pinned library versions, or even version control at all!</p>

<p>But this environment is <em>approachable</em>&#8230; if you&#8217;re a graduate student in a field like physics or biology, running an existing application or writing your own simulation or data processing code. But who never got to take a class on software engineering, and where the code itself is not a first class deliverable. The deliverable is the published paper.</p>

<h2 class="wp-block-heading">But what about all those other values?</h2>

<p>They&#8217;re still important! But the point of this exercise is to think about which values are will &#8220;win&#8221; when they come into tension. And I do think that, if you look at HPC clusters in general, this is the set of values that will win.</p>

<p>Availability is important, but not if that work costs us (much) performance. Velocity is great, but we&#8217;ll de-prioritize it in the name of workload portability. Security is essential &#8212; but we don&#8217;t want to make it harder to onboard new grad students&#8230;</p>

<h2 class="wp-block-heading">You cluster is not the generic platform (and neither is mine)</h2>

<p>A last point I want to make is that there&#8217;s actually <em>no such thing</em> as the &#8220;generic HPC cluster platform&#8221;. Each individual cluster, at a university or company or government lab, is often configured in a unique way based on the hardware, performance goals, and whims of the person setting it up.</p>

<p>Because of this, each <em>individual</em> HPC cluster may prioritize different values. A cluster at a national lab may choose security at the expense of approachability; or a different cluster may choose to sacrifice portability in the name of velocity if they&#8217;re developing on a new hardware or software system.</p>

<p>(Also, the systems I build as part of my day job also make <em>very</em> different choices than the &#8220;generic&#8221; cluster would. To a first approximation, I think I&#8217;d say we choose performance/debuggability/portability/security&#8230; but we also make different choices depending on what we&#8217;re building!)</p>

<p>But I still think that <em>performance</em>, <em>portability</em>, and <em>approachability</em> represent the most common platform values I&#8217;ve seen in the HPC field as a whole. And I think the tools and practices we use bias towards those values.</p>

<p>However&#8230; all of that is what I thought about while making dinner! If you think a different set of values makes more sense, feel free to <a href="mailto:ajdecon@ajdecon.org">send me an email</a> and let me know. <img alt="😉" class="wp-smiley" src="https://s.w.org/images/core/emoji/15.0.3/72x72/1f609.png" style="height: 1em;" /></p>]]></content><author><name>Thinking Out Loud</name></author><category term="ajdecon" /><summary type="html"><![CDATA[Yesterday while I was cooking dinner, I happened to re-watch Bryan Cantrill&#8217;s talk on &#8220;Platform as a Reflection of Values&#8220;. (I watch a lot tech talks while cooking or baking &#8212; I often have trouble focusing on a video unless I&#8217;m doing something with my hands, but if I know a recipe well I can often make it on autopilot.)]]></summary></entry><entry><title type="html">How has life after leaving the Labs been going?</title><link href="https://hpc.social/personal-blog/2024/how-has-life-after-leaving-the-labs-been-going/" rel="alternate" type="text/html" title="How has life after leaving the Labs been going?" /><published>2024-08-04T20:21:00-06:00</published><updated>2024-08-04T20:21:00-06:00</updated><id>https://hpc.social/personal-blog/2024/how-has-life-after-leaving-the-labs-been-going-</id><content type="html" xml:base="https://hpc.social/personal-blog/2024/how-has-life-after-leaving-the-labs-been-going/"><![CDATA[<p>June 2024 marked two years since I <a href="http://blog.glennklockwood.com/2022/05/life-and-leaving-nersc.html">left my job at one of the world's most prestigious government HPC centers for a job in one of the world's largest technology corporations</a>. In that time, the world of HPC has changed
dramatically; just six months after I started, ChatGPT was released and triggered a gold rush in AI that is now overshadowing traditional scientific computing. This shift brought about massive HPC
deployments led by hyperscalers, challenging the long-held belief that only national governments could
deploy and operate world-leading supercomputers. <a href="http://blog.glennklockwood.com/2024/05/isc24-recap.html">My experiences at
ISC'24 this past summer</a> made clear to me that the traditional HPC community is now rethinking their role the industry, and some individuals who built their careers in public HPC are revisiting their
assumption that world-class HPC systems are limited to the public institutions that have
historically dominated the <a href="https://www.top500.org/lists/top500/list/2024/06/">top of the Top500 list</a>. I
had no idea things would unfold this way when I left my job at NERSC back in 2022, and I've been remarkably lucky to
now be a part of the largest forces driving this huge shift in HPC.</p>
<div class="separator" style="clear: both; text-align: center;">
<figure style="display: inline-block; margin-left: 1em; margin-right: 1em;">

<figcaption style="font-size: 14px; margin-top: 5px;">One of my new offices. Nicer than my old government office, and it has free food, but it's a ninety-minute drive each way.</figcaption>
</figure>
</div>
<p>In the spirit of openness and helping others who are facing similar career decisions, I thought I would follow up on my <a href="http://blog.glennklockwood.com/2022/05/life-and-leaving-nersc.html">Life and leaving
NERSC</a> post by sharing how my professional journey from DOE HPC into cloud HPC has been going. I'll first explain the path I've traveled over these past two years, then answer some of the most
common questions I've been asked about this transition.</p>
<p>As a forewarning, this is not a typical technology-focused post, and most of this might be obvious to people who already work in Big Tech. Here are the questions on which I reflected:</p>
<ol>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#first-two-years">What happened during my first two years in Corporate America?</a></li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#what-i-do">So what do I actually do?</a>
<ol>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#storage-product-management">Storage product management</a></li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#hpc-ai-development">HPC/AI development</a></li>
</ol>
</li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#am-i-happy">Am I happy with my decision and the new job?</a>
<ol>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#am-i-happy-broadly-yes">Broadly, yes</a></li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#long-time-no">But for a long time, no</a></li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#finally-yes">Finally, yes</a></li>
</ol>
</li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#industry-does-better">What does industry do better than the Labs?</a>
<ol>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#accountability">Accountability</a></li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#pace-and-decision-making">Pace and decision making</a></li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#relevance">Relevance</a></li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#technically-security">Technically: security</a></li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#pay-good">But the pay is good, right?</a></li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#work-life-balance">How's work-life balance?</a></li>
</ol>
</li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#what-i-miss">Do you miss anything about working at the lab?</a>
<ol>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#freedom-to-have-an-off-day">Freedom to have an off day</a></li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#travel">Travel</a></li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#openness">Openness</a></li>
</ol>
</li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#regret-decision">Would you still have left NERSC knowing what you know now?</a></li>
</ol>
<h2 id="first-two-years">What happened during my first two years in Corporate America?</h2>
<p>I published my <a href="http://blog.glennklockwood.com/2022/05/life-and-leaving-nersc.html">Life and leaving NERSC</a> blog post on a
Thursday, which was my last day working at NERSC. The following Monday was my first day at the new job, and being
hired as 100% remote, it didn't feel that different; I was just booting up a Lenovo laptop (yuck) instead of a
MacBook, using Teams and Outlook instead of Slack, GSuite, and Zoom, and that sort of thing.</p>
<p>However, the job was undeniably different; whereas I used to be an engineer at NERSC, I was hired to be a "Principal Product
Manager" within the cloud storage organization which was responsible for all object, disk, and file storage
services. Although my title was "product manager," I wasn't a people manager, and
I didn't manage any specific storage products. Rather, my responsibility was to act as an HPC-focused overlay
across all cloud storage services, and my job was to represent the interests of HPC users to all the people who did manage specific storage products. I didn't define product or feature roadmaps myself, but I could help those responsible for each product or service understand how to shape their roadmaps to benefit HPC workloads.
</p>
<p>I struggled in this position for a variety of reasons, so after I gave the new role
an honest six to nine months, I decided that being a storage product manager just wasn't a good fit for me.
Unfortunately, I reached this decision after the <a href="https://www.nytimes.com/2022/07/21/business/yield-curve-inversion.html">yield curve inverted</a> and
<a href="https://www.theverge.com/2023/1/18/23560315/microsoft-job-cuts-layoffs-2023-tech">mass-layoffs and hiring
freezes</a> were implemented, so there weren't a lot of places to go other than back to a government lab.
Although I wasn't thriving as a storage product manager, I did have allies that helped me navigate my
day-to-day struggles, and I decided to wait until more opportunities opened up and learn as much about
product management as I could in the meantime.</p>
<div class="separator" style="clear: both; text-align: center;">
<figure style="display: inline-block; margin-left: 1em; margin-right: 1em;">

<figcaption style="font-size: 14px; margin-top: 5px;">The yield curve inverted a month after I started my new job. Not great timing.</figcaption>
</figure>
</div>
<p>After a little over a year as a storage product manager, a new engineering role opened up within a sister team in our
HPC/AI infrastructure organization. After discussing the needs and nature of the work with the hiring manager, I applied for the job, went through the
interview process, and was eventually given a verbal offer to join his team in June 2023. Unfortunately, the global
economic outlook was still uncertain, and I wound up sitting in a holding pattern (as a storage product
manager) from June 2023 to November 2023. It wasn't until the week of SC'23 that I finally got the written offer
letter, and I spent December wrapping up loose ends within the storage organization.</p>
<p>On January 2, 2024, I began my new (and current) role within the company. The move was completely lateral, but I changed job titles from "Product Manager" to "Software Engineer," and I
changed organizations from storage to specialized compute.</p>
<p>I say all this because my experiences in making the professional transition from government HPC to cloud HPC are colored by the fact that I really changed jobs twice. I've had both product management and engineering/development roles, and I've been in both storage and HPC
organizations.</p>
<h2 id="what-i-do">So what do I actually do?</h2>
<p>I've had two very different roles within the same orbit of HPC/AI infrastructure, so I'll
describe them separately to give you a sense of the breadth of HPC roles possible.</p>
<h3 id="storage-product-management">Storage product management</h3>
<p>As a <b>storage product manager</b> (PM), I was an inch deep but a mile wide on every storage service, every commercial HPC workload, and
all the ways in which those two could touch each other. I'd guess that only 25% of my day-to-day work required deep
expertise in HPC; the remainder was either business-centric or required only understanding HPC in broad strokes. This
was quite unlike the things I'd done earlier in my career in the public sector, since there's not an equivalent to
what a product manager does within the DOE Labs.
</p>
<p>For example, I spent a lot of my time as a storage PM explaining the basics of HPC I/O to different teams within the
company. When most cloud people think "storage," they are really thinking about either enterprise storage (things
like virtual disks for virtual machines) or content distribution (think serving up content for web apps). The
concept of hundreds or thousands of VMs all writing to the same place at the same time is standard practice in the
HPC world, but in the cloud world, this is a <a href="https://www.microsoft.com/en-us/security/business/security-101/what-is-a-ddos-attack?msockid=2008901357a56c4518b3840856e96dad">DDoS
attack</a>. Since my organization was responsible for all storage, not just HPC storage, there were a lot of
people who simply never had to think about the challenges that HPC people take for granted, and it could be challenging (as the new guy) to convince seasoned cloud storage PMs that some workloads legitimately need hundreds of gigabytes per second of bandwidth.</p>
<p>As a PM, I also wound up doing a fair amount of business reporting. For example, object storage is used by all manner of cloud
customers, so prioritizing features that specifically help HPC customers required understanding how many HPC
customers actually used it. How do you define whether a workload is really an HPC workloads or not? In
DOE, we'd waste hours debating stuff like this for no real purpose, but when I became a product manager, I had to
define this to make the business case that we needed to develop a certain feature that would only be used by HPC workloads.</p>
<p>Finally, I did a fair amount of actual product and project management work. Get on the phone with a customer,
write down what they do, and turn those into requirements. Do that a bunch of times, then synthesize a more general
requirements document. Review it with leadership. Get approval to assign developers to work on the features to meet
those requirements. Ask other teams to develop features you need for your feature. Negotiate with everyone on
development priorities in the next six months. Track progress of the development team. Produce demos to show that
progress is being made. Present progress to leadership. That sort of thing. It's similar to being a PI on a research
grant, except I had customers, dependencies, and ultimate accountability.</p>
<p>As far as technical work, a lot of it revolved around meeting customers and internal partner teams where they were in
terms of their knowledge of HPC. I did a fair amount of technical marketing; I would come up with
the ways people should think about combining storage services together in their HPC workflows, then figure out
how to communicate that to audiences with vastly different levels of technical understanding. For example, I didn't
own our <a href="https://learn.microsoft.com/en-us/azure/azure-managed-lustre/amlfs-overview">Lustre product</a>, <a href="https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction">object storage
product</a>, or <a href="https://learn.microsoft.com/en-us/azure/virtual-machines/sizes/high-performance-compute/hb-family">HPC CPU
node product</a>, but I owned <a href="https://techcommunity.microsoft.com/t5/azure-high-performance-computing/azure-managed-lustre-not-your-grandparents-parallel-file-system/ba-p/3889946">the
story around how we envisioned all three services worked well together</a>. This meant I would create slides and
narratives around this, then present them to anyone from our sales teams (who often had limited HPC-specific experience) to
the world's leading HPC centers.</p>
<p>I also sometimes helped development teams accurately test their storage systems against HPC workloads. For example,
when ChatGPT exploded, everyone wanted to know how well their storage service worked for training large language
models. I would talk to the engineers who trained LLMs, infer what their I/O patterns would be based on
their description of how they did training, then design a benchmark that our developers could follow to
emulate that LLM training workflow. Since I understood both the workload and the storage technology, it was often faster for me to translate between AI engineers and storage engineers rather than have them speak directly.</p>
<h3 id="hpc-ai-development">HPC/AI development</h3>
<p>As an <b>HPC/AI engineer</b>, my work is a lot more technical and focused. I'm on a "white-glove support team"
that works directly with large, strategic customers in HPC and AI, so rather than working with dozens of customers and connect them to dozens of storage technologies, I work with one or two customers and the specific technologies on which they build their HPC or AI clusters. Because of this, I'd wager 95% of my day-to-day work is
technical.</p>
<p>I don't spend much time in a terminal by virtue of my relative seniority. Instead, I sit in on a lot of internal meetings and represent the perspective of our strategic HPC and AI customers. For example, if we are trying to decide which CPU to
include in our next HPC-optimized CPU node, I might work with our benchmarking engineers to develop a representative
benchmark and then interpret the results with the node's product managers. I'm not the person running the benchmark
myself; instead, I might ask hard questions that the customer might ask, help decide the next experiments to run,
and backstop our engineers if the customer starts poking too many holes in the work.</p>
<p>I also function as a system architect at times; if a customer shows up with unusually large or complex
HPC system requirements, I'll help translate the customer requirement (e.g., "We need 10 TB/s of storage bandwidth)
for individual product teams (e.g., "they will be using N compute nodes and accessing storage via a network with
this topology and tapering, likely running an application that has this pattern, ..."). This often requires understanding what the compute, network, <i>and</i> storage product teams are doing and being able to explain it all in
whatever terms each team understands. I also wind up sitting in on customer meetings and asking critical questions
so that we can make informed design tradeoffs.</p>
<p>I do write code, but no more than I did when I was a system architect at NERSC. For example, I might pull PDU
telemetry from across a data center to help determine if oversubscribing the power for a future cluster would
impact workloads. The code itself is pretty straightforward statistical analysis, but interpreting it requires an
understanding of a bunch of things ranging from the workload running on the nodes to how nodes are distributed
across PDUs, racks, rows, halls, and buildings.</p>
<p>The remaining 5% of my work is not very technical and involves things I opt into because it's interesting or
the right thing to do. This might be spending time providing historical context for a business strategy document or showing up at a meeting to help explain the customer perspective to a finance or sales team.</p>
<h2 id="am-i-happy">Am I happy with my decision and the new job?</h2>
<p>Yes, no, and yes.</p>
<h3 id="am-i-happy-broadly-yes">Broadly, yes</h3>
<p>I am glad I made the decision to leave NERSC and take on a job in Big Tech for a couple of
high-level reasons.</p>
<p>As a product manager, I learned a lot about how businesses and corporations work to a
degree that I never did when I worked at a startup and I never would have if I stayed with the government. Not only
do I now know what the difference between gross and operating margin is, but I <i>get</i> it because I've had to
build <a href="https://www.investopedia.com/terms/c/cogs.asp">COGS</a> and pricing models that could sustain and grow a new product. I know exactly how to
price cloud services (or any product or service, really) and where that money goes. I now pay much more attention to
quarterly earnings reports, and I have a more confident opinion on what different elements of these reports say
about a technology company's trajectory. This has equipped me with what feels like a much more complete
understanding of the HPC industry overall.</p>
<p>I'm also glad to work at a company that generally tries to do the right things. It
is investing heavily towards being <a href="https://blogs.microsoft.com/blog/2020/01/16/microsoft-will-be-carbon-negative-by-2030/">carbon
negative</a> (rather than just buying carbon offsets) while others are <a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/elon-musks-new-worlds-fastest-ai-data-center-is-powered-by-massive-portable-power-generators-to-sidestep-electricity-supply-constraints">burning gas inefficiently</a> in a race to be #1. It also <a href="https://givebutter.com/blog/companies-that-match-donations">matches
every donation I make to 501(c)3 nonprofits</a> which is a huge benefit that matches up with the ways in
which I try to share my good fortune with others. And it beats employees over the heads with a strong, positive <a href="https://careers.microsoft.com/v2/global/en/culture">corporate culture</a> which holds managers and leaders
accountable for the wellness of their employees. These sorts of things don't meaningfully exist in government, and
there are a lot of big corporations out prioritize short-term profits over the longer-term benefits that come from investing in sustainability and philanthropy.</p>
<h3 id="long-time-no">But for a long time, no</h3>
<p>However, I was unhappy for my first eighteen months.</p>
<p>I took a gamble on storage product management being as interesting and fulfilling as
engineering when I decided to step into this new job, and I lost that bet. I quickly came to realize that there's a big difference between
being a <u>storage person in an HPC organization</u> and being an <u>HPC person in a storage organization</u>.</p>
<p>When I worked in an HPC organization like NERSC, I was used to being the odd man out
because parallel storage is a complicated topic that most HPC folks don't <i>really</i> understand. Despite that, everyone is still generally like-minded and appreciates the same things; everyone knows what MPI and InfiniBand
are, and everybody knows what a checkpoint and restart might look like.</p>
<p>Conversely, when I worked in a storage organization, I was an odd man out because nobody
really understood HPC. The average engineer only had a vague notion of what MPI or
InfiniBand accomplished. If you don't understand that MPI is what lets hundreds of servers all work on the same
distributed problem at once, it's easy to forget that an MPI application will also cause hundreds of servers to
all write data at once. And if you've never used an MPI barrier, it's hard to internalize the fact that the whole
application stops until the slowest process finishes writing.</p>
<p>Instead of worrying about tightly coupled applications, I realized that storage people
worry about data availability and durability above all else. After all, storage's #1 job is to not lose data. In
contrast, it's not unusual for an HPC user to have hundreds of terabytes of data vanish because they forgot to copy
it off of scratch before it got purged. This sharp difference in priorities--data durability versus
performance--causes friction, because at the end of the day, what's good for HPC (high bandwidth and low latency) is
usually bad for storage (high durability and availability).</p>
<div class="separator" style="clear: both; text-align: center;">
<figure style="display: inline-block; margin-left: 1em; margin-right: 1em;">

<figcaption style="font-size: 14px; margin-top: 5px;">The landscape of storage for HPC and storage for enterprises as I see it. If you care about one but work with people who care about the other, expect friction.</figcaption>
</figure>
</div>
<p>These are technological differences, but they result in a persistent, elevated level of latent stress that never goes away. People tend to worry about the things they understand, and people tend to ask for help
about the things that worry them. What this meant for me is that I spent a lot of time focusing on things that
everyone understood (like market trends, revenue, and general indicators of performance) instead of hard problems
unique to large-scale HPC. And because I was never solving the hard problems, I never got the gratification of feeling like I accomplished something that, as I learned, is an important motivator to me.</p>
<p>To be clear, I realize that I made the decision to focus on problems that other people brought me
rather than carve out a space for me to work on the problems I felt were important. I'm sure that someone who was
more tenacious and unafraid to pursue challenges that nobody else understood would have a very different experience as a PM. But after about a year, I realized that what I value and enjoy doing just isn't
aligned with what a successful storage PM needs to be successful. I realized I didn't want to keep doing what I was doing
for another five years, so I decided to stop.</p>
<h3 id="finally-yes">Finally, yes</h3>
<p>I quite enjoy my role in HPC/AI engineering and development now, as it's similar to
what I used to do in the DOE. I have to learn about how different hardware, software,
and systems work, and I have a lot of room to focus on challenges that play to my strengths and interests.
For example, I love engaging with the HPC community, and my job still allows me to go out to the big HPC conferences to do that. At the same time, I also like getting into the guts of system behavior, and I still get to spend at least an hour or two a week doing something quantitative.</p>
<p>My day-to-day is also steeped in that familiar feel of working in an HPC organization.
Every cluster has a name that gets bandied about in meetings, and they have the same familiar challenges--fabric disruptions, firmware
upgrades, flaky nodes, and the like. The standard responsibilities are also all there; some teams perform system
administration, others support users, and some of us focus on future system designs. But the cluster names aren't nearly as creative as those in the public sector (<a href="https://www.top500.org/system/180236/">Eagle's</a> real name sounds like a serial number). And they look pretty boring too; there are no fancy rack graphics.</p>
<div class="separator" style="clear: both; text-align: center;">
<figure style="display: inline-block; margin-left: 1em; margin-right: 1em;">

<figcaption style="font-size: 14px; margin-top: 5px;">Five racks of a cloud GPU cluster that runs ND H100 v5-series VMs. <a href="https://www.youtube.com/watch?v=ntKZ5CibuIQ">Source</a></figcaption></figure></div>
<p>There are also teams that have no analogue in the traditional HPC world, like those who
are responsible for things ranging from the <a href="https://techcommunity.microsoft.com/t5/azure-infrastructure-blog/announcing-the-general-availability-of-azure-boost/ba-p/3981384">smart NICs</a> and software-defined networks to profits and losses. This is
what keeps things interesting; I can just as easily spend an hour reviewing benchmark results from the latest GPU
with my teammates as I can learning how the control systems for <a href="https://news.microsoft.com/source/features/ai/in-house-chips-silicon-to-service-to-meet-ai-demand/">liquid
heat exchangers</a> affect system reliability or <a href="https://www.osha.gov/noise/standards">data center
safety</a>. When things are quiet and no fires are burning, going to work can sometimes feel like going to a big
playground full of HPC and HPC-adjacent technology.</p>
<p>Don't get me wrong; it's still a job, and there are still unpleasant tasks and
uncomfortable situations. Working at a cloud provider means a lot of processes are designed to be slow and steady,
and some teams struggle to understand why anyone would want to reboot every node in a cluster at once--such an event would be a massive outage in general-purpose cloud! But working in an HPC organization means that when these
situations arise, I'm no longer the <i>odd HPC guy</i>--I'm on the <i>odd HPC team</i>.</p>
<h2 id="industry-does-better">What does industry do better than the Labs?</h2>
<h3 id="accountability">Accountability</h3>
<p>Organizational planning happens <a href="https://devblogs.microsoft.com/azure-sdk/planning-2021/">twice a year</a>,
and this planning is the time when teams all get on the same page about what
work to prioritize in the next six months (a <i>semester</i>). Teams coordinate dependent work with each other,
trades horses on what the priority of each request is, and at the end of planning, have committed agreements about what work
will be done in the next semester. The progress on that work is tracked throughout the semester, delays and
interrupts are accounted, and there's an escalation path up through the ranks of management and leadership if
priorities cannot be agreed upon by individual teams.</p>
<p>The DOE Labs operate much more loosely in my experience. There, people tend
to work on whatever pet projects they want until they lose interest. If a project is funded by a research grant, there are loose
deliverables and timelines (write X papers per year), but at the end of the day, nothing really bad happens if the
work progresses slowly or its quality is poor. There's no penalty if a research grant results in a piece of software
that nobody uses or a paper that nobody reads. The value of the work is largely intellectual, and as a result, it's perfectly
possible to have a long career at a DOE lab, churning out papers and software, that lacks any
lasting impact.</p>
<p>Tying money to the value of work can make accountability much more black and white. If you pay a team of engineers a
million dollars a year to develop a new service that only increases revenue by a million dollars a year, that
service is going to be scrutinized every time prioritization happens. Is there a way to increase its revenue through
better features or better positioning? It'll be a product manager's job to go figure that out. If the answer comes
back as "no," then that service might be put on a shelf and its engineering team reassigned to work on something
that has a greater impact. Those engineers don't get to decide that they keep wanting to work on the service that
has limited demonstrable value.</p>
<p>At the same time, managers are accountable for the wellbeing of their team and the teams underneath them. All
employees fill out regular, semi-anonymized surveys on different aspects of job satisfaction, and the results of
these surveys roll up all the way to the top of the company. If employees are disgruntled, their managers know it,
and those managers' managers know it, and everyone up the chain is accountable for improving those scores. Sometimes that results
in increased hiring so engineers don't feel overworked. Other times it means reorganizing people and teams to align
them with the work they are good at performing. And if nothing works and a team's morale keeps declining, maybe it's because of
the manager--and the manager gets replaced.</p>
<h3 id="pace-and-decision-making">Pace and decision making</h3>
<p>Because managers and leaders are accountable, I've also found them to be much more empowered to just do what they
feel is the right thing to do. Whereas no big decision in the DOE Labs can be made without reviews, panels,
strategic offsites, more reviews, and presentations to headquarters--all of which could add months or
years to a project--the direction can move on a dime because all it takes is one executive to sign off and accept
full responsibility for the consequences of their decision. Getting the approval to staff up and pursue a good idea
often requires only winning over one or two key people, not an army of feds in Germantown or an anonymous review
panel who isn't conversant in what you're proposing.</p>
<p>And again, sometimes money makes decisions much easier to make. For example, a few people at ISC'24 asked me why we
didn't re-do the <a href="https://www.top500.org/system/180236/">Top500 run for Eagle</a> to beat Aurora since the
SC'23 scoring was so close. The decision process can be as simple as this:</p>
<p></p>
<ul>
<li>According to the <a href="https://top500.org/lists/top500/2024/06/download/TOP500_202406.xlsx">Top500 list's raw
data</a>, Eagle achieved 561,200 TFlop/s using an Nmax of 11,796,480.</li>
<li>Knowing that HPL's walltime is (flop count / Rmax) and HPL's flop count is (2/3 * Nmax^3), you can calculate
that the HPL walltime for this run was 1,950 seconds or 0.512 hours.</li>
<li>The <a href="https://azure.microsoft.com/en-us/pricing/calculator/">public list price</a> for an Eagle
node (ND96isr H100 v5) is something like $60 an hour.</li>
<li>The HPL run used 1,800 such nodes.</li>
</ul>
<p>Give the above, during the half hour it would take to run HPL, those same nodes could be
running a production workload which would have resulted in $58,000 in revenue. That is, the <i>opportunity cost</i>
of re-running HPL is at least $58,000 in lost revenue. In reality, it would take time to boot up and configure the
cluster of virtual machines and do a few scale-up runs which would tie up the nodes for a couple hours, making
this opportunity cost closer to a couple hundred thousand dollars.</p>
<p>Is getting a marginally higher Top500 score worth a couple hundred thousand dollars if your
machine is already listed and had its day in the sun? I don't need an executive to answer that question. But in the
public HPC space, who's to say what the opportunity cost is? If HPL wasn't running twice a year on Frontier, are the
dozen or so lattice QCD jobs that would be running instead worth a couple hundred thousand dollars?</p>
<p></p>
<h3 id="relevance">Relevance</h3>
<p>I might be more vain than I thought when I worked for the government, because I really enjoy being able to talk about the work that I do
with the general public now. When people ask, "What work do you do?" and I respond with, "Have you ever heard of Copilot or
ChatGPT?" there is almost always a conversation that follows. People may not really understand how artificial intelligence and
large language models work, but they've played with those technologies and have opinions and questions. Sometimes the conversation is about big-picture stuff like "will AI take over the world?" At other times it's specific like "what do you think about
AI's effect on global climate change?" Because I am steeped in all aspects of AI in my day-to-day work, I can
usually speak intelligently about any dimension of the AI industry when my neighbors ask.</p>
<div class="separator" style="clear: both; text-align: center;">
<figure style="display: inline-block; margin-left: 1em; margin-right: 1em;">

<figcaption style="font-size: 14px; margin-top: 5px;">Every blog post these days needs at least one AI-generated picture, so here is a picture generated by DALL-E that "captures the essence of explaining AI concepts to neighbors in a friendly, approachable setting." But more poignantly, my team directly supports the supercomputers that trained the model that generates these pictures.</figcaption>
</figure>
</div>
<p>This was a much bigger challenge when I worked in the public sector. When I told people that I worked at Lawrence Berkeley
National Lab, nobody knew what I was talking about half of the time. The other half of the time, people would think I worked on
nuclear weapons because Lawrence Livermore National Lab has a confusingly similar name and geography. And if the
conversation ever got as far as what people did on the supercomputers I supported, it would rapidly tail off once
all parties (including me) realized that cosmological hydrodynamics and quantum Monte Carlo don't really make for great conversation since they don't touch people's everyday lives.</p>
<p>This isn't to say that the work done at the Labs isn't important. But the general public doesn't understand it, and
to a large degree, doesn't really care about it. I realize that being able to impress your neighbors with what you
do isn't at the top of the list of most people's job requirements, but I get a lot of satisfaction out of it.</p>
<h3 id="technically-security">Technically: security</h3>
<p>HPC doesn't really worry about cybersecurity. Every HPC center has a security group and does scans and threat
modeling, but at the end of the day, the security practices on all the largest supercomputers in the public sector
are roughly the same as they were twenty years ago. Users ssh into a login node, and once you're inside, you have
access to everything. You can see everyone else who's logged in, you can see everyone who chmodded their home
directory to be +777, and the only thing separating you from everyone else is the Linux kernel. Passwordless ssh is
everywhere, and often times, passwordless ssh for the root user is everywhere.</p>
<p>This does not fly with paying commercial HPC and AI customers in the cloud who use supercomputing to develop better
products faster than their competitors. For example, both <a href="https://www.synopsys.com/blogs/chip-design/eda-in-the-cloud-snug-2023.html">Arm and AMD have publicly
stated that they perform a lot of their silicon design simulations using HPC in the cloud</a>. What would happen
if both AMD and Arm used the same cluster and one accidentally made their project directory world-readable? Should
domain scientists' understanding of how POSIX file permissions work really be the last line of defense against a
next-generation CPU or GPU's specs being leaked to the competition?</p>
<p>I had to quickly learn about modern security practices when I started doing HPC in the commercial cloud out of
necessity. I'm still nowhere close to being a security expert, but two years has been long enough for me to now
cringe when I talk to my colleagues in the traditional HPC community about how they protect against threats. It's
not really their fault that most of the HPC community hasn't adopted modern practices, because the tools and
practices required to do it right aren't easy to set up, automate, and maintain from scratch.</p>
<p>For example, basic LDAP is a short path to allowing users to log into a cluster's nodes, but if those users also need
to authenticate themselves to REST services that support an HPC workflow across multiple clusters, you have to start building a Rube Goldberg machine of software on top of LDAP. Similarly, sticking every user on their own overlay network is great to limit the blast radius of a
compromised account. However, automating the configuration of VXLAN tunnel endpoints as nodes get allocated and deallocated to
jobs requires a lot of fancy orchestration that is either very complicated to build and maintain yourself or very
expensive to buy and maintain. As a result, HPC just accepts the risk. Cloud has
figured all this out though, and the price of providing this security infrastructure is included in the cost of
cloud-based supercomputers.</p>
<h3 id="pay-good">But the pay is good, right?</h3>
<p>Like I said before I left the public sector, <a href="http://blog.glennklockwood.com/2022/05/life-and-leaving-nersc.html">my base salary is
comparable to what I got at the lab</a>. It's actually gotten less competitive because <a href="https://www.theregister.com/2023/05/11/microsoft_pay_freeze/">all salaries were frozen</a> when I was first eligible for a raise. So, after considering the effects of inflation, my paycheck is a little lower than what it was in the government two years ago.</p>
<p>What's different is the bonus structure which simply does not exist in the government or university world. For those
who aren't familiar with how bonuses work in the tech industry, I'll share how it works for me:</p>
<p></p>
<ul>
<li>In the first year, I was awarded two signing bonuses: one in cash, one in stock. Half of the cash bonus was paid
out up-front, and the other half was paid out after I had been there a year. The stock grant cannot be touched
during the first year because it had a one-year "cliff."</li>
<li>On my one-year anniversary, I got the second half of my cash signing bonus, and my signing stock grant began
"vesting."</li>
<li>After a year, I was also eligible for an annual performance-based raise, cash bonus, and stock bonus.</li>
<ul>
<li>Because of the economy, my annual raise was zero.</li>
<li>The cash bonus was paid out in a lump sum, similar to my cash signing bonus.</li>
<li>The stock bonus was awarded all at once but follows a multi-year "vesting schedule" which means I am only
actually given fractions of the total award over time. However, these bonuses don't have a "cliff" and begin
vesting immediately.</li>
</ul>
<li>Every year thereafter, I am eligible for an annual raise, cash bonus, and another stock bonus.</li>
</ul>
<p>The way stock bonuses work was the least intuitive part to me, but since it's such a significant part of total compensation, it's worth spelling
out for anyone who's considering an offer that includes this:</p>
<p></p>
<ul>
<li>Stock bonuses are defined in terms of dollar values. For example, let's say I got a signing stock bonus of $1000
with a one-year cliff that vests quarterly (every three months) over five years.</li>
<li>On the day that stock bonus is awarded, my employer converts that $1000 value into company stock based on the
market value that day. If stocks are $50 per share, I am awarded 20 shares. My employer hangs on to those shares
on my behalf, so I can't actually do anything with them yet.</li>
<li>Since I have a five-year vesting schedule and the award vests quarterly, my shares will vest twenty times (four
quarters, five years). Coincidentally, since I have 20 shares, I will get one share per quarter.</li>
<li>However, because I have a one-year cliff, I get all four quarters of my first year at my one-year anniversary.
So, four shares should appear in my brokerage account on my one-year anniversary. Once a share is in my
brokerage account, I can do whatever I want with it (like sell it immediately!)</li>
<li>Every quarter thereafter, one more share vests and appears in my brokerage account.</li>
</ul>
<p>Assuming I get a stock bonus as part of my overall annual bonus, this means that stock
awards pile up and vest every year. This is tricky for two reasons:</p>
<p></p>
<ol>
<li>Although my initial stock award was $1,000 in the above example, that amount was converted to stock the day it
was awarded. <i>Assuming I am doing a good job and increasing the value of my employer's stock</i>, the value of
those shares will increase while they're vesting. This means by the time the first four shares of my award
vested at my one-year anniversary, they were worth more than the $50 per share they represented when they were
awarded. More broadly, the value of a stock bonus tends to increase over time, making the true cash value of a
$1000 stock bonus worth a lot more than $1000 by the time it completely vests.</li>
<li>Every year's stock award comes with its own multi-year vesting period, which means at any given time, I have
multiple years' bonuses all vesting at once. This also means that at any given time, I have a bunch of unvested
stock that's worth a lot of money that I can't yet spend. If I quit my job though, all these unvested shares
vanish into thin air.</li>
</ol>
<p>These two factors make up the golden handcuffs that people often talk about in industry.
The longer I stick around, the more unvested stock I have hanging over my head, and it usually becomes increasingly
valuable (yet inaccessible!) over time. The reality is that if you've put in a few years in Big Tech, you might have
years' worth of base salary tied up in unvested stock that all goes away if you quit.</p>
<p>The end result is that although base salary is competitive with what you can make in a government HPC facility, there's a significant cash bonus that falls out of the sky once a year, and
an appreciable amount of stock appears in your brokerage account every couple of months which you can turn around
and sell for more cash. Depending on seniority and performance, these bonuses can add up to a significant fraction
of base salary.</p>
<p>Finally, the above is consistent with what I've seen firsthand at two companies in Big Tech but may be different based on the role and the company. For example, field-facing roles in sales and support may be completely different beasts, and private companies and startups load things differently due to the value of equity.</p>
<h3 id="work-life-balance">How's work-life balance?</h3>
<p>It hasn't been different than working in the government. Just like at a lab or university, some people
work around the clock while others stick pretty close to the standard workday. There may be a higher concentration
of Type A personalities who put in a lot of time in Big Tech, and this may pressure others to keep up and also put
in long hours, but there's rarely been an occasion where a manager expects staff to routinely work nights and
weekends. Doing so would probably result in negative employee satisfaction scores which would roll up and eventually
have to be addressed.</p>
<p>Of course, there are cases where working odd hours is required to get the job done. Because
I work for a global organization, I've had to get up early to meet with teams or customers in Europe. I've also had
to stay up late to meet with teams or customers in Asia. And in some particularly annoying days, I've had to do both
and wind up working from 5am to 8pm. But I never felt that I had no choice in the matter; I pulled these hours
because it was the right thing to do at the time. And I don't see this as being too different from the days when I'd
work sixteen-hour days, seven days a week, for the entire month of March to put together a paper for SC. Or days
when I'm at SC and am preparing talks, meeting with partners, and otherwise hustling from 8am to 1am for five days
straight.</p>
<p>One big difference is the fact that my employer offers discretionary time off ("unlimited vacation"). This is a divisive topic in industry, but I see it as a positive for work-life balance
because it underscores an emphasis on <i>outcomes</i> rather than <i>output</i>. I can take an afternoon
off or enjoy a long weekend with little fanfare, because <i>productivity</i> is infinitely more valuable that <i>presence</i>. As
long as I do what needs to get done, I don't have to worry about timing vacations to ensure I am banking enough time
off in between.</p>
<h2 id="what-i-miss">Do you miss anything about working at the lab?</h2>
<div>Absolutely. There are a bunch of appealing things about working in a DOE lab (or an NSF center)
that I've had to give up since coming to industry.</div>
<h3 id="freedom-to-have-an-off-day">Freedom to have an off day</h3>
<p>Right before I finished graduate school, I had
a conversation with <a href="https://engineering.lehigh.edu/faculty/edmund-webb-iii">Professor Edmund Webb</a> soon
after he became a professor after a decade-long career at Sandia National Labs about life at the Labs. He said that,
after becoming a professor, he lost the ability to just close the door to his office and focus on
something he needed to get done for a day. I didn't really grasp what this meant at the time, but I totally get it now. The
DOE might be one of the few places where you can take a day--maybe even a week--and just close your door to
everything else that's going on around you to focus on what you want to do. In the case of professorship, there's always students requiring attention; in industry, it's customers and partners.</p>
<p>I think this difference results from two factors: very few things in public
HPC are very urgent, and the Labs are stocked full of independent, free-thinking Ph.D. types. There's rarely a
penalty if something is late by a day (or two years! Remember when <a href="https://insidehpc.com/2020/10/doe-under-secretary-for-science-dabbars-exascale-update-frontier-to-be-first-aurora-to-be-monitored/">Aurora
was called "A21?"</a>), but there can be huge payoff in prestige if one of your wacky side projects turns out to be
something useful (this is how <a href="https://docs.nersc.gov/development/containers/shifter/">Shifter</a> came to be). By comparison, working at a giant corporation often means there are a bunch of interdependencies
on others, and the odds of any one of your 200,000 coworkers sending you a Teams message asking for help is just a lot higher than it is at a 70-person supercomputer center. The culture is much more team-oriented, and being a one-person army isn't incentivized as much.</p>
<h3 id="travel">Travel</h3>
<p>Part of my job within the DOE complex was to go around the country (and the world) and be smart, and secondarily,
show that my lab hired smart people and did smart things. If headquarters wanted to make sure that the supercomputer
they were about to spend $500M on was technically sound, I'd sometimes get invited to go sit in on a review and try
to poke holes in the design. If a European HPC project wanted to ensure they were including a global perspective on
some dimension of future HPC strategy, I'd sometimes get invited to give a talk about how I view the world of data.
And if these reviews and workshops happened to be in awesome places around the world--oh well!</p>
<p>I feel a lot more self-conscious about requesting approval to attend these sorts of boondoggles as an engineer now
because the first question I have to answer is, "Is this trip business critical?" If there's a direct line of sight
between me giving a talk at a workshop and a specific business strategy, I can say "yes" with a straight face. But it's
hard to accept an invitation to fly off to Switzerland to give a 30-minute talk when I know that my attendance isn't
going to move any needles.</p>
<h3 id="openness">Openness</h3>
<p>Just like it's no longer my job to travel the world and just be smart, it's not my job to write about the work that I
(or my team) does. I miss writing papers and giving technical talks, because the process of putting together
coherent thoughts around a technical topic is one of the ways I really come to understand it. There's also a lot of
really wild ideas that we're pursuing at scale that the scientific computing
community has never considered, but there are two factors that work against being open about these things:</p>
<p></p>
<ol>
<li>In terms of prioritization, my time is always better spent solving problems, or at least documenting them for
internal audiences who fully grasp the context around them, than writing about them in a way that the rest of
the world can understand. It's hard to justify the time to write a retrospective or a study unless there's a
strategic advantage behind it.</li>
<li>The customers I support typically do not want the world knowing what they're doing. There is an AI arms race
happening right now, and having the technical sophistication to utilize massive-scale supercomputers effectively
is a competitive advantage. In the traditional HPC community, only national security is comparable to the level
of secrecy involved, and none of the intelligence agencies are openly contributing to the state of the art in
HPC either.</li>
</ol>
<div>So instead of making conference papers and presentations, these days I make more internal papers and presentations.
I'm trying to figure out ways to publish interesting technical anecdotes on my website (for example, I maintain <a href="https://www.glennklockwood.com/ai/ai-requirements.html">a collection of LLM training requirements as I am exposed to them</a>), but it's a lot of extra work to disentangle the proprietary bits from my work notes to do this.</div>
<p></p>
<p>Related to openness is also freedom to speak my mind in public forums. I had the most latitude to blast my
opinions out on to the Internet when I was still early in my career and nobody listened to me, but I've had to get
progressively less opinionated over the years. At this point, I abide by a written corporate social media policy
which, although very reasonable in what it requests (don't slander competitors, always be transparent about who employs you), it stops me from commenting on news as much as I used to since so many tech
companies qualify as competitors in some dimension.</p>
<h2 id="regret-decision">Would you still have left knowing what you know now?</h2>
<p>Yes. I still stand by just about everything I wrote in my <a href="http://blog.glennklockwood.com/2022/05/life-and-leaving-nersc.html">original blog post</a>; at the time, I just needed a change, and I
found the change that I was looking for. Without immersing myself in the world of cloud, I would have
never learned about virtualization, physical infrastructure, or modern security to the degree that I have. And the fact that I
stumbled into what has become one of the leading companies in AI at the dawn of generative AI was an extremely lucky
coincidence.</p>
<p>However, this doesn't mean that I now turn my nose up at doing HPC in the public sector.
There are many unique aspects to working at a DOE lab or NSF center that have no parallel in industry. I also believe that I am
the sum of the experiences that led me to where I work today, and I would never have gotten the opportunity to write
this retrospective if I didn't learn everything I did working in the DOE and NSF.</p>
<p>And perhaps above all else, there is something attractive about public service that I haven't been
able to shake in the last two years. I still dial in to <a href="https://science.osti.gov/ascr/ascac">ASCAC
meetings</a> to see what the world of public HPC and scientific computing is thinking and doing, and I still try
to contribute time and attention to working groups like <a href="https://www.nitrd.gov/coordination-areas/lsn/magic/">NITRD's MAGIC</a>. I write lengthy blog posts in a <a href="http://blog.glennklockwood.com/2024/05/isc24-recap.html">futile attempt to caution the leaders in public-sector HPC</a> against
rejecting AI workloads in commercial clouds as HPC. And every time I learn some slick way we deal with hard technological or sociological issues at work, I still file it away in the "good ideas for when I go
back" folder in the back of my mind.</p>
<p>I don't have any near-term plans on going anywhere though. Like I said before, there are
still plenty of days when dialing into work is like going to the playground. Amazing things are happening in the
world of HPC infrastructure at scale now that the world is pouring money into AI, and the rate of scale and
innovation is no longer constrained to <a href="https://www.llnl.gov/article/48101/powering-llnl-prepares-exascale-massive-energy-water-upgrade">40 MW</a>
and <a href="https://www.olcf.ornl.gov/wp-content/uploads/OLCF-6-RFP-Cover-Letter-07-19-2024.pdf">$500M</a> per
supercomputer like it was when public-sector HPC was setting the bar for leadership. There is a whole new exciting world of challenges and possibilities when you start thinking about building supercomputers that consume <a href="https://www.datacenterdynamics.com/en/news/aws-acquires-talens-nuclear-data-center-campus-in-pennsylvania/">hundreds of megawatts of power</a>.</p>
<p>Like I wrote two years ago, I don't think any government has the appetite to build data centers for scientific computing that are larger than today's 50 MW exascale facilities. This means that government HPC centers will never have a reason to explore the exciting world of 100+ MW supercomputers or work on the wacky problems that arise at that scale. Consequently, the biggest and most challenging problems in HPC--at least in terms of infrastructure and systems design at scale--are becoming unique to industry, not public HPC.</p>
<p>I got into HPC because I enjoy working on large, complex systems. Considering where I am at this stage of my life, what I want to accomplish in the rest of my career, and what gets me out of bed in the morning, I feel like I wound up in the right place for now. I have no regrets.</p>]]></content><author><name>Glenn K. Lockwood&apos;s Blog</name></author><category term="glennklockwood" /><summary type="html"><![CDATA[June 2024 marked two years since I left my job at one of the world's most prestigious government HPC centers for a job in one of the world's largest technology corporations. In that time, the world of HPC has changed dramatically; just six months after I started, ChatGPT was released and triggered a gold rush in AI that is now overshadowing traditional scientific computing. This shift brought about massive HPC deployments led by hyperscalers, challenging the long-held belief that only national governments could deploy and operate world-leading supercomputers. My experiences at ISC'24 this past summer made clear to me that the traditional HPC community is now rethinking their role the industry, and some individuals who built their careers in public HPC are revisiting their assumption that world-class HPC systems are limited to the public institutions that have historically dominated the top of the Top500 list. I had no idea things would unfold this way when I left my job at NERSC back in 2022, and I've been remarkably lucky to now be a part of the largest forces driving this huge shift in HPC.]]></summary></entry><entry><title type="html">Advanced LSF resource connector configuration on IBM Cloud - part III</title><link href="https://hpc.social/personal-blog/2024/advanced-lsf-resource-connector-configuration-on-ibm-cloud-part-iii/" rel="alternate" type="text/html" title="Advanced LSF resource connector configuration on IBM Cloud - part III" /><published>2024-08-01T13:08:20-06:00</published><updated>2024-08-01T13:08:20-06:00</updated><id>https://hpc.social/personal-blog/2024/advanced-lsf-resource-connector-configuration-on-ibm-cloud-part-iii</id><content type="html" xml:base="https://hpc.social/personal-blog/2024/advanced-lsf-resource-connector-configuration-on-ibm-cloud-part-iii/"><![CDATA[<p><strong>Overview</strong></p>

<p>This is the third instalment in a series of blogs covering advanced configuration topics for LSF resource connector. The earlier parts in the series can be found here: <a href="https://community.ibm.com/community/user/cloud/blogs/gbor-samu/2023/11/09/advanced-resource-connector-configuration-on-ibm-c">part I</a>, <a href="https://community.ibm.com/community/user/cloud/blogs/gbor-samu/2024/03/20/advanced-lsf-resource-connector-configuration-on-i">part II</a>.</p>

<p>As hinted in the closing of part II, this instalment will cover running Docker workloads on cloud instances which are dynamically managed by the LSF resource connector. The cloud environment in this example is <a href="https://cloud.ibm.com/catalog/content/terraform-1623200063-71606cab-c6e1-4f95-a47a-2ce541dcbed8-global">IBM Cloud</a>. To understand more about LSF resource connector, please read the earlier parts in the blog series.</p>

<p><a href="https://www.ibm.com/products/hpc-workload-management">LSF</a> provides a framework for the management and execution of containerized workloads. It supports the following container runtimes: Docker, NVIDIA Docker, Shifter, Singularity, Podman and Enroot. The LSF <a href="https://www.ibm.com/docs/en/spectrum-lsf/10.1.0?topic=lsf-configuring-containers">documentation</a> provides configuration steps for the supported container runtimes. Once configured, this capability is effectively transparent from the end user perspective.</p>

<p><strong>Enable Docker support</strong></p>

<p>First we need to enable support in LSF to run Docker containers. This is covered in detail in the LSF <a href="https://www.ibm.com/docs/en/spectrum-lsf/10.1.0?topic=containers-lsf-docker">documentation</a> and also something which I wrote about previously in the blog post <a href="https://medium.com/ibm-data-ai/jack-of-all-containers-e0d7fd0633b3">Jack of all containers</a>. The following steps will assume that the configuration steps have been completed.</p>

<p>LSF uses a Boolean resource named <em>docker</em> to identify hosts where the Docker runtime is available. This Boolean resource needs to be set on the compute nodes which are dynamically started by LSF resource connector.</p>

<p>In our example, an insecure Docker repository (using http) has been setup on the LSF manager host in the cluster with hostname <em>lsf-mgmt-host</em>. This will serve as the repository to host an OpenFOAM Docker container which has been prepared according to the procedures documented <a href="https://community.ibm.com/community/user/cloud/blogs/john-welch/2020/02/12/building-an-openfoam-ready-container-for-lsf">here</a>. This blog will not go into detail on the creation of the insecure Docker registry. On the LSF management node, below is the output showing the available images. We see the OpenFoam image is available both locally and via http on port 5000.</p>

<div class="highlight"><pre><code class="language-plaintext"># docker image ls
REPOSITORY                   TAG           IMAGE ID      CREATED        SIZE
localhost/openfoam/openfoam  v1912_update  bce4eb059f36  11 days ago    6.71 GB
localhost:5000/openfoam      v1912_update  bce4eb059f36  11 days ago    6.71 GB
docker.io/library/registry   2             6a3edb1d5eb6  10 months ago  26 MB</code></pre></div>

<p><strong>Note</strong> An insecure Docker registry was used in this example for simplicity and is not recommended in production.</p>

<p>As was the case in part II of the blog series, the <em>user_data.sh</em> script will be used for multiple purposes here:</p>

<ul>
<li>Set <em>docker</em> Boolean variable on dynamic compute nodes</li>
<li>Install Docker CE runtime and relevant support packages</li>
<li>Add user(s) to the docker group (<em>/etc/group</em>)</li>
<li>Configuration to point to insecure Docker registry on LSF management host
<em>lsf-mgmt-host</em></li>
</ul>
<p>The following updates were made to the <em>user_data.sh</em> script. See comments inline for details.</p>

<div class="highlight"><pre><code class="language-plaintext">$ diff -u4 ./user_data.sh ./user_data_sh.org
--- ./user_data.sh	2024-07-29 18:44:24.483146000 +0000
+++ ./user_data_sh.org	2024-07-11 14:34:47.688341000 +0000
@@ -29,25 +29,8 @@
 
 #!/bin/bash
 # shellcheck disable=all
 
-# 
-# The following steps will add the Docker CE repo, install the latest Docker CE
-# version along with supporting packages. It will create a Docker Linux group
-# and add the lsfadmin user to that group. Furthermore, it will create
-# the /etc/docker/daemon.json file pointing to the insecure Docker registry
-# which has been configured on the LSF management host. Finally it will
-# start Docker. Note that the hostname lsf-mgmt-host for the insecure-registries
-# configuration of Docker needs to be updated accordingly. 
-# 
-yum-config-manager --add-repo https://download.docker.com/linux/rhel/docker-ce.repo -y 
-dnf install htop hwloc hwloc-libs libevent stress stress-ng python36 docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin -y  &gt;&gt; $logfile 2&gt;&amp;1
-ln -s /usr/bin/python3 /usr/bin/python
-groupadd docker &gt;&gt; $logfile 2&gt;&amp;1
-usermod -aG docker lsfadmin  &gt;&gt; $logfile 2&gt;&amp;1 
-echo -e "{\n \"insecure-registries\" : [ \”lsf-mgmt-host:5000\" ]\n }" &gt;&gt; /etc/docker/daemon.json 
-systemctl start docker &gt;&gt; $logfile 2&gt;&amp;1  
-
 if [ "$compute_user_data_vars_ok" != "1" ]; then
   echo 2&gt;&amp;1 "fatal: vars block is missing"
   exit 1
 fi
@@ -225,15 +208,8 @@
 else
   echo "Can not get instance ID" &gt;&gt; $logfile
 fi
 
-# 
-# Add the docker Boolean variable to the LSF_LOCAL_RESOURCES variable in
-# the lsf.conf file on the compute hosts. This will ensure that the host
-# is tagged with the docker variable. 
-# 
-sed -i "s/\(LSF_LOCAL_RESOURCES=.*\)\"/\1 [resource docker]\"/" $LSF_CONF_FILE &gt;&gt; $logfile 2&gt;&amp;1 
-
 #Update LSF Tuning on dynamic hosts
 LSF_TUNABLES="etc/sysctl.conf"
 echo 'vm.overcommit_memory=1' &gt;&gt; $LSF_TUNABLES
 echo 'net.core.rmem_max=26214400' &gt;&gt; $LSF_TUNABLES</code></pre></div>

<p><strong>Application profile configuration</strong></p>

<p>Next, we configure the LSF application profile for the OpenFOAM Docker container which has been loaded into the insecure Docker registry on the LSF management host. LSF application profiles can be used to define common job parameters for the same job type. This includes the container and container runtime definition. Learn more about LSF application profiles <a href="https://www.ibm.com/docs/en/spectrum-lsf/10.1.0?topic=lsf-application-profiles">here</a>.</p>

<p>On the LSF management node, the following application profile is defined in <em>$LSF_ENVDIR/lsbatch/&lt;clustername&gt;/configdir/lsb.applications</em>. Note that the hostname <em>lsf-mgmt-host</em> must point to the hostname where the insecure Docker repository has been setup in your environment. Additionally the volume specification <em>-v /mnt/vpcstorage/data</em> is specific to this environment and can be adjusted or removed as needed.</p>

<div class="highlight"><pre><code class="language-plaintext">….
….
Begin Application
NAME = openfoam
DESCRIPTION = Example OpenFOAM application
CONTAINER = docker[image(lsf-mgmt-host:5000/openfoam:v1912_update) \
   options(--rm --net=host --ipc=host \
   --cap-add=SYS_PTRACE \
   -v /etc/passwd:/etc/passwd \
   -v /etc/group:/etc/group \
   -v /mnt/vpcstorage/data:/mnt/vpcstorage/data \ 
   ) starter(root)]   
EXEC_DRIVER = context[user(lsfadmin)] \
   starter[/opt/ibm/lsf_worker/10.1/linux3.10-glibc2.17-x86_64/etc/docker-starter.py] \
   controller[/opt/ibm/lsf_worker/10.1/linux3.10-glibc2.17-x86_64/etc/docker-control.py] \
   monitor[/opt/ibm/lsf_worker/10.1/linux3.10-glibc2.17-x86_64/etc/docker-monitor.py]
End Application
….
….</code></pre></div>

<p>In order to make the above change take effect, run the <em>badmin reconfig</em> command as the defined LSF administrator. The LSF <em>bapp</em> command can be used to check the newly defined configuration for LSF application profile <em>openfoam</em>.</p>

<div class="highlight"><pre><code class="language-plaintext">$ badmin reconfig

Checking configuration files ...

No errors found.

Reconfiguration initiated

$ bapp -l openfoam

APPLICATION NAME: openfoam
 -- Example OpenFOAM application

STATISTICS:
   NJOBS     PEND      RUN    SSUSP    USUSP      RSV 
       0        0        0        0        0        0

PARAMETERS:

CONTAINER: docker[image(lsf-mgmt-host:5000/openfoam:v1912_update)    options(--rm --net=host --ipc=host    --cap-add=SYS_PTRACE    -v /etc/passwd:/etc/passwd    -v /etc/group:/etc/group    -v /mnt/vpcstorage/data:/mnt/vpcstorage/data    ) starter(root)]
EXEC_DRIVER: 
    context[user(lsfadmin)]
    starter[/opt/ibm/lsf_worker/10.1/linux3.10-glibc2.17-x86_64/etc/docker-starter.py]
    controller[/opt/ibm/lsf_worker/10.1/linux3.10-glibc2.17-x86_64/etc/docker-control.py]
    monitor[/opt/ibm/lsf_worker/10.1/linux3.10-glibc2.17-x86_64/etc/docker-monitor.py]</code></pre></div>

<p><strong>Submitting workload</strong></p>

<p>With all of the configuration in place, it’s now time to submit an OpenFOAM workload. For this, LSF Application Center is used. The OpenFOAM application template is available on the Spectrum Computing github <a href="https://github.com/IBMSpectrumComputing/lsf-integrations">here</a>. The OpenFOAM application template is configured to use the <em>openfoam</em> application profile. An example job is submitted and it runs to completion successfully. In the screenshot below, we see that the openfoam Docker container is executed.</p>

<figure><img src="https://www.gaborsamu.com/images/openfoam_job.jpg" />
</figure>

<p>The LSF <em>bjobs</em> and <em>bhist</em> output from the job follows below:</p>

<div class="highlight"><pre><code class="language-plaintext">$ bjobs -l 2613

Job &lt;2613&gt;, Job Name &lt;myOpenFoam_run_motorBike&gt;, User &lt;lsfadmin&gt;, Project &lt;defa
                     ult&gt;, Application &lt;openfoam&gt;, Status &lt;RUN&gt;, Queue &lt;normal&gt;
                     , Command &lt;/mnt/lsf/repository-path/lsfadmin/myOpenFoam_ru
                     n_1722358195200AuWDY/motorBike/bsub.myOpenFoam_run&gt;, Share
                      group charged &lt;/lsfadmin&gt;
Tue Jul 30 16:49:55: Submitted from host &lt;gsamu-hpc-demo-mgmt-1-a844-001&gt;, CWD 
                     &lt;/mnt/lsf/repository-path/lsfadmin/myOpenFoam_run_17223581
                     95200AuWDY/motorBike&gt;, Specified CWD &lt;/mnt/lsf/repository-
                     path/lsfadmin/myOpenFoam_run_1722358195200AuWDY/motorBike&gt;
                     , Output File &lt;/mnt/lsf/repository-path/lsfadmin/myOpenFoa
                     m_run_1722358195200AuWDY/motorBike/output.lsfadmin.txt&gt;, E
                     rror File &lt;/mnt/lsf/repository-path/lsfadmin/myOpenFoam_ru
                     n_1722358195200AuWDY/motorBike/error.lsfadmin.txt&gt;, Notify
                      when job begins/ends, 6 Task(s), Requested Resources &lt;spa
                     n[hosts=1]&gt;;
Tue Jul 30 16:55:23: Started 6 Task(s) on Host(s) &lt;gsamu-hpc-demo-10-241-0-137&gt;
                     &lt;gsamu-hpc-demo-10-241-0-137&gt; &lt;gsamu-hpc-demo-10-241-0-137
                     &gt; &lt;gsamu-hpc-demo-10-241-0-137&gt; &lt;gsamu-hpc-demo-10-241-0-1
                     37&gt; &lt;gsamu-hpc-demo-10-241-0-137&gt;, Allocated 6 Slot(s) on 
                     Host(s) &lt;gsamu-hpc-demo-10-241-0-137&gt; &lt;gsamu-hpc-demo-10-2
                     41-0-137&gt; &lt;gsamu-hpc-demo-10-241-0-137&gt; &lt;gsamu-hpc-demo-10
                     -241-0-137&gt; &lt;gsamu-hpc-demo-10-241-0-137&gt; &lt;gsamu-hpc-demo-
                     10-241-0-137&gt;, Execution Home &lt;/home/lsfadmin&gt;, Execution 
                     CWD &lt;/mnt/lsf/repository-path/lsfadmin/myOpenFoam_run_1722
                     358195200AuWDY/motorBike&gt;;
Tue Jul 30 17:03:33: Resource usage collected.
                     The CPU time used is 1411 seconds.
                     MEM: 928 Mbytes;  SWAP: 0 Mbytes;  NTHREAD: 41
                     PGID: 18426;  PIDs: 18426 18427 18428 20088 
                     PGID: 20374;  PIDs: 20374 20388 20800 21385 
                     PGID: 21389;  PIDs: 21389 
                     PGID: 21390;  PIDs: 21390 
                     PGID: 21391;  PIDs: 21391 
                     PGID: 21392;  PIDs: 21392 
                     PGID: 21393;  PIDs: 21393 
                     PGID: 21394;  PIDs: 21394 


 MEMORY USAGE:
 MAX MEM: 982 Mbytes;  AVG MEM: 422 Mbytes; MEM Efficiency: 0.00%

 CPU USAGE:
 CPU PEAK: 5.89 ;  CPU PEAK DURATION: 63 second(s)
 CPU AVERAGE EFFICIENCY: 42.81% ;  CPU PEAK EFFICIENCY: 98.15%

 SCHEDULING PARAMETERS:
           r15s   r1m  r15m   ut      pg    io   ls    it    tmp    swp    mem
 loadSched   -     -     -     -       -     -    -     -     -      -      -  
 loadStop    -     -     -     -       -     -    -     -     -      -      -  

 RESOURCE REQUIREMENT DETAILS:
 Combined: select[(docker) &amp;&amp; (type == any)] order[r15s:pg] span[hosts=1]
 Effective: select[(docker) &amp;&amp; (type == any)] order[r15s:pg] span[hosts=1] </code></pre></div>

<div class="highlight"><pre><code class="language-plaintext">$ bhist -l 2613

Job &lt;2613&gt;, Job Name &lt;myOpenFoam_run_motorBike&gt;, User &lt;lsfadmin&gt;, Project &lt;defa
                     ult&gt;, Application &lt;openfoam&gt;, Command &lt;/mnt/lsf/repository
                     -path/lsfadmin/myOpenFoam_run_1722358195200AuWDY/motorBike
                     /bsub.myOpenFoam_run&gt;
Tue Jul 30 16:49:55: Submitted from host &lt;gsamu-hpc-demo-mgmt-1-a844-001&gt;, to Q
                     ueue &lt;normal&gt;, CWD &lt;/mnt/lsf/repository-path/lsfadmin/myOp
                     enFoam_run_1722358195200AuWDY/motorBike&gt;, Specified CWD &lt;/
                     mnt/lsf/repository-path/lsfadmin/myOpenFoam_run_1722358195
                     200AuWDY/motorBike&gt;, Output File &lt;/mnt/lsf/repository-path
                     /lsfadmin/myOpenFoam_run_1722358195200AuWDY/motorBike/outp
                     ut.lsfadmin.txt&gt;, Error File &lt;/mnt/lsf/repository-path/lsf
                     admin/myOpenFoam_run_1722358195200AuWDY/motorBike/error.ls
                     fadmin.txt&gt;, Notify when job begins/ends, 6 Task(s), Reque
                     sted Resources &lt;span[hosts=1]&gt;;
Tue Jul 30 16:55:23: Dispatched 6 Task(s) on Host(s) &lt;gsamu-hpc-demo-10-241-0-1
                     37&gt; &lt;gsamu-hpc-demo-10-241-0-137&gt; &lt;gsamu-hpc-demo-10-241-0
                     -137&gt; &lt;gsamu-hpc-demo-10-241-0-137&gt; &lt;gsamu-hpc-demo-10-241
                     -0-137&gt; &lt;gsamu-hpc-demo-10-241-0-137&gt;, Allocated 6 Slot(s)
                      on Host(s) &lt;gsamu-hpc-demo-10-241-0-137&gt; &lt;gsamu-hpc-demo-
                     10-241-0-137&gt; &lt;gsamu-hpc-demo-10-241-0-137&gt; &lt;gsamu-hpc-dem
                     o-10-241-0-137&gt; &lt;gsamu-hpc-demo-10-241-0-137&gt; &lt;gsamu-hpc-d
                     emo-10-241-0-137&gt;, Effective RES_REQ &lt;select[(docker) &amp;&amp; (
                     type == any)] order[r15s:pg] span[hosts=1] &gt;;
Tue Jul 30 16:55:23: Starting (Pid 18426);
Tue Jul 30 16:55:24: Running with execution home &lt;/home/lsfadmin&gt;, Execution CW
                     D &lt;/mnt/lsf/repository-path/lsfadmin/myOpenFoam_run_172235
                     8195200AuWDY/motorBike&gt;, Execution Pid &lt;18426&gt;;
Tue Jul 30 17:04:01: Done successfully. The CPU time used is 1535.1 seconds;
Tue Jul 30 17:04:02: Post job process done successfully;


MEMORY USAGE:
MAX MEM: 982 Mbytes;  AVG MEM: 431 Mbytes; MEM Efficiency: 0.00%

CPU USAGE:
CPU PEAK: 5.92 ;  CPU PEAK DURATION: 63 second(s)
CPU AVERAGE EFFICIENCY: 50.67% ;  CPU PEAK EFFICIENCY: 98.68%

Summary of time in seconds spent in various states by  Tue Jul 30 17:04:02
  PEND     PSUSP    RUN      USUSP    SSUSP    UNKWN    TOTAL
  328      0        518      0        0        0        846         </code></pre></div>

<p><strong>Conclusion</strong></p>

<p>The <em>user_data.sh</em> script of LSF resource connector allows a high degree of customization for cloud compute resources that dynamically join the LSF cluster. We’ve demonstrated how it can be used to tag cloud compute resources with a specific LSF Boolean resource in addition to the ability to install specific packages and do configuration customization. This is a simplified example, but illustrates this point.</p>]]></content><author><name>Ramblings of a supercomputing enthusiast.</name></author><category term="gaborsamu" /><summary type="html"><![CDATA[Overview]]></summary></entry><entry><title type="html">ISC’24 recap</title><link href="https://hpc.social/personal-blog/2024/isc-24-recap/" rel="alternate" type="text/html" title="ISC’24 recap" /><published>2024-05-28T05:24:00-06:00</published><updated>2024-05-28T05:24:00-06:00</updated><id>https://hpc.social/personal-blog/2024/isc-24-recap</id><content type="html" xml:base="https://hpc.social/personal-blog/2024/isc-24-recap/"><![CDATA[<p>I had the great pleasure of attending the ISC High Performance conference this month, marking the fifth time I've attended what has become one of my top must-attend industry conferences of the year. This year was particularly meaningful to me because it is the first time that:</p>
<p></p>
<ol style="text-align: left;"><li>I attended ISC as a Microsoft employee. This is also the first time I've attended any HPC conference since I changed my focus from storage into AI infrastructure.</li><li>I attended ISC in-person since before the pandemic. It's also the first time I've visited Hamburg which turned out to be an absolute delight.</li></ol>
<p></p>
<p>Although registrations have been lower since the pandemic, this year's final registration count was over 3,400 attendees, and there was no shortage of old and new colleagues to bump into walking between the sessions at the beautiful Congress Center Hamburg.</p>
<div class="separator" style="clear: both; text-align: center;"></div>
<p><br />&lt;p&gt;This year’s theme was “Reinvent HPC,” and that idea—that HPC needs to reinvent itself—was pervasive throughout the program. The whole industry had been pulling towards exascale for the better part of a decade, and now that there are two exaflop systems on Top500 and the dust is settling, it feels like everyone is struggling to figure out what’s next. Is it quantum? AI?&lt;/p&gt;</p>
<p>It was difficult for me to draw a line through all the topics worth reviewing at this year's ISC, as it was a very dense four days packed with a variety of topics, discussions, vendors, and events. I only experienced a fraction of everything there was to be seen since so many interesting sessions overlapped, but I thought it might be worthwhile to share my perspective of the conference and encourage others to do the same.<span></span></p>
<p></p>
<div id="toc">
<h2>Table of Contents</h2>
<ul>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#section1">Reinventing HPC (and blast those hyperscalers!)</a>
<ul>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#section11">Kathy Yelick's opening keynote</a></li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#section12">Closing keynotes on the future</a></li>
</ul>
</li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#section2">Top500 and Aurora</a>
<ul>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#section21">#1 - Frontier</a></li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#section22">#2 - Aurora</a></li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#section23">#3 - Eagle</a></li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#section24">Other notable tidbits</a></li>
</ul>
</li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#section3">Everyone is an AI expert!</a>
<ul>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#section31">The Exascale AI Synergies LLM Workflows BOF</a></li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#section32">AI Systems for Science and Zettascale</a></li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#section33">Real applications of generative AI for science</a></li>
</ul>
</li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#section4">High Performance Software Foundation</a></li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#section5">Quantum computing</a></li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#section6">Reinvent HPC to include urgent computing?</a>
<ul>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#section61">The Urgent Computing focus session</a></li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#section62">The Interactive and Urgent HPC workshop</a></li>
</ul>
</li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#section7">Concluding thoughts</a></li>
</ul>
</div>
<h2 id="section1">Reinventing HPC (and blast those hyperscalers!)</h2>
<p>The need to reinvent HPC was the prevailing theme of the conference from the very first session; with the listing of Aurora as the second system on Top500 to break the 1 exaflops barrier, the community is in search of a new milestone to drive research (and funding!). At the same time, commercial AI has rapidly risen up largely in an independent, parallel effort with a speed and scale that begs the question: how important was the decade-long drive to break the exaflops barrier if the AI industry could catch up so quickly without the help of the institutions that have historically posted the top HPL scores? If the commercial AI industry overtakes scientific computing as the world leader in deploying at scale, how can “HPC” be reinvented so it can continue to claim leadership in another dimension?</p>
<h3 id="section11">Kathy Yelick's opening keynote</h3>
<p>ISC’s opening keynote was given by Kathy Yelick, where she provided commentary on two recent government-commissioned reports on the future of HPC:</p>
<p></p>
<ol style="text-align: left;"><li><a href="https://nap.nationalacademies.org/catalog/26916/charting-a-path-in-a-shifting-technical-and-geopolitical-landscape">Charting a Path in a Shifting Technical and Geopolitical Landscape: Post-Exascale Computing for the National Nuclear Security Administration</a>, commissioned by the National Academies</li><li><a href="https://www.osti.gov/biblio/1989107">Can the United States Maintain Its Leadership in High-Performance Computing?</a>, commissioned by the US Department of Energy’s Advanced Scientific Computing Research program</li></ol>
<p style="text-align: left;">Living up to her reputation, Dr. Yelick’s talk was fast and insightful, describing the insatiable demand for computing driven by scientific research, the struggle to expose continuing amounts of parallelism to make use of newer processors, and some promising directions to address that disconnect. However, her talk started in a direction that I didn’t like when she went into describing the disruptors that necessitate reinventing HPC:</p>
<div class="separator" style="clear: both; text-align: center;"></div>
<p style="text-align: left;">The above slide implied that AI, quantum, or cloud may pose an existential threat to the HPC community gathered at ISC this year; this immediately raised my hackles, as it cast the relationship between “HPC” and “AI”/“cloud” as having some sort of adversarial tension. As the talk went on, I realized that “HPC” didn’t really mean “high-performance computing” to her. Rather, it was used to refer to something much more narrowly scoped—high-performance computing <i>to solve scientific problems</i>. Slide after slide, the presentation kept doubling down on this idea that “HPC” as the audience knows it is being threatened. For example, Yelick talked through this slide:</p>
<div class="separator" style="clear: both; text-align: center;"></div>
<p style="text-align: left;">The picture she painted is that “HPC” (denoted by companies with blue bars) no longer has influence over technology providers because the “hyperscalers” (green bars) have such an outsized amount of investment. She then used this to call on the audience to think about ways “we” could influence “them” to produce technologies that are useful for both scientific computing and low-precision AI workloads.</p>
<p style="text-align: left;">Her talk culminated in this slide:</p>
<div class="separator" style="clear: both; text-align: center;"></div>
<p style="text-align: left;">Which was accompanied by this conclusion:</p>
<p style="text-align: left;"></p>
<blockquote>"So what’s a post-exascale strategic for the scientific community? It's the beat 'em or join 'em strategy. The beat 'em strategy says we’re going to design our own processors. [...] The join 'em strategy says let's leverage the AI hardware that's out there. [...] The sort of sneaky way of doing this is getting embedded in the AI community and trying to convince them that in order to make AI better for commercial AI applications, you really want to have certain features. Like don't throw away your 64-bit arithmetic and things like that."</blockquote>
<p></p>
<p>I found myself getting increasingly unsettled through the keynote, because this "us versus them" mentality put me, a long-standing member of this HPC community, in the camp of "them." It was as if I was suddenly an outsider in a conference that I've been attending for years just because I no longer work for an organization that has been doing HPC since the early days of computing. Even though the clusters I support use the same NVIDIA and AMD GPUs, the same InfiniBand fabrics, and the same Lustre file systems that "HPC" uses, I am no longer in "HPC" because I am "hyperscale" or "cloud" or "AI."</p>
<p>The underlying message is one I get; GPUs are trending in a direction that favors massive gains in lower-precision computation over FP64 performance. And the cost of HBM is driving the overall value (in FP64 FLOPS per dollar) of accelerators backwards for the first time in the history of scientific computing. But the thesis that the scientific computing community needs to be sneaky to influence the hyperscale or AI players seemed way off the mark to me. What seemed absent was the recognition that many of the "hyperscalers" are her former coworkers and remain her colleagues, and "they" sit in the same audiences at the same conferences and share the same stages as the "HPC" community. All that is true because "HPC" is not somehow different than "cloud" or "AI" or "hyperscale." If there really is a desire to influence the hyperscale and AI industry, the first step should be to internalize that there is no "us" and "them."</p>
<h3 id="section12">Closing keynotes on the future</h3>
<p>Just as the conference was opened with a talk about this "us versus them" mentality, it was closed with a talk about "us versus them" in a keynote session titled, "Reinventing HPC with Specialized Architectures and New Applications Workflows" which had two speakers followed by Q&amp;A.</p>
<h4>Chiplets for modular HPC</h4>
<p>John Shalf gave one half of the closing keynote, where he gave his usual rally for investments in chiplets and specialized processors for HPC:</p>
<div class="separator" style="clear: both; text-align: center;"></div>
<p style="text-align: left;">He gives a variant of this talk at every ISC, but this year he lasered in on this notion that the "HPC" community needs to do what the "hyperscalers" do and use chiplets to develop custom ASICs. It was an energetic and impassioned talk, but this notion that hyperscalers are already executing on his idea for the future sounded a little funny to me seeing as how I now work for one of these hyperscalers and his message didn't resonate.</p>
<div class="separator" style="clear: both; text-align: center;"></div>
<p style="text-align: left;">If you really follow the money, as Shalf suggested, a huge amount of it is flowing into GPUs, not specialized processors. It wasn't clear to me what specialization he was thinking of when he referred to custom silicon being developed by the likes of Meta, Google, AWS, and Microsoft; it's true that these companies are developing their own silicon, but those efforts are largely addressing cost, risk, and supply, not improving performance beyond more general-purpose silicon like GPUs. And it turns out that a significant fraction of the (non-US) HPC community is already developing custom silicon for the same reasons as the hyperscalers; Japan, China, and Europe are all developing their own indigenous processors or accelerators for scientific computing at leadership scales. In that sense, Shalf was preaching to the choir given that, on the international stage, his government is the odd one out of the custom silicon game.</p>
<p style="text-align: left;">He also suggested a dichotomy where the HPC community would either have to just (1) make every scientific problem an AI problem or (2) join this journey towards making domain-specific accelerators, ignoring the significant, unexplored runway offered by using mixed precision arithmetic in scientific applications. He called for partnering with hyperscalers, but his examples of implementing a RISC-V-based stencil accelerator and a SambaNova-based DFT processor didn't draw a clear line to the core missions of the large hyperscalers he extolled. He briefly said that partnering would benefit hyperscalers by addressing some capital cost challenges, but seeing as how the annual capital expenditures of the hyperscalers outstrips those of the US national HPC effort by orders of magnitude, I couldn't understand what the hyperscalers would stand to gain by partnering in this way.</p>
<h4 style="text-align: left;">Integrating HPC, AI, and workflows</h4>
<p style="text-align: left;">Rosa Badia gave the second half of the closing keynote where she proposed ideas around complex scientific workflows and the novel requirements to support them. This talk felt a lot more familiar, as the focus was squarely on solving scientific computing challenges by connecting traditional HPC resources together in nontraditional ways using software whose focus goes beyond cranking out floating point arithmetic.</p>
<p style="text-align: left;">As she spoke, I couldn't help but see parallels between the challenges she presented and the sort of technologies we live and breathe every day in cloud services.  For example, she showed this slide:</p>
<div class="separator" style="clear: both; text-align: center;"></div>
<p style="text-align: left;">Dr. Badia obviously wanted to make a cloud-tie in by calling this "HPC Workflows as a Service," but what I'm not sure she realized is that this model almost exactly describes platform-as-a-service frameworks that already exist in commercial clouds. For example,</p>
<p style="text-align: left;"></p>
<ul style="text-align: left;"><li>What she calls a "Data Catalog" is a public or private object storage account (a blob container, an S3 bucket) or a PaaS abstraction built atop them</li><li>What she calls a "Software Catalog" is a container registry (Azure Container Registry, Amazon Elastic Container Registry) or an abstraction built atop them</li><li>A "Workflow Description" is something like an <a href="https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-component-pipeline-python">AzureML pipeline</a> or <a href="https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_model_building_pipeline.html">SageMaker pipeline</a></li><li>A "Workflow Registry" is just a Github repository containing pipelines</li><li>The "Portal" is the web UI provided by AzureML or SageMaker</li></ul>
<p></p>
<p style="text-align: left;">I don't think there's anything truly new here; the challenges she described lie in wedging these workflows into HPC infrastructure which lacks the platform features like robust identity and access management (i.e., something better than LDAP that supports more modern authentication and authorization flows and finer-grained access controls) and data management (i.e., something better than a parallel file system that depends on POSIX users, groups, and permissions and implicit trust of clients).</p>
<p style="text-align: left;">She went on to describe a workflow data management system that reinvented a bunch of infrastructure that is already baked into commercial cloud object stores like Azure Blob and AWS S3:</p>
<div class="separator" style="clear: both; text-align: center;"></div>
<p style="text-align: left;">As she was describing the requirements for such a workflow data management layer, it struck me that what the scientific data community calls "<a href="https://en.wikipedia.org/wiki/FAIR_data">FAIR principles</a>" are the same basic requirements for operating in commercial environments where data may be subject to strict privacy and compliance regulations. The notion of findable data may be aspirational for scientific datasets, but when a company is having to find datasets because it's being sued or subpoenaed, findability is a bare-minimum requirement for any data management system. Similarly, tracking the provenance of data may be a nice-to-have for scientific data, but it is a hard requirement when establishing a secure software supply chain. Cloud storage systems solved many of these challenges a long time ago, and I can't help but wonder if this idea that workflows in HPC pose a new set of challenges is another manifestation of "us" not realizing "they" might have done something useful and applicable for science.</p>
<p style="text-align: left;">Badia's final slide had a particularly poignant statement which read, "Systems can only be justified if we have applications that need them." I think she was trying to call for more investment in application development to exploit new systems, but I think the inverse is also true. If modern scientific applications truly require more complex orchestration of compute and data, maybe the scientific computing community should stop building computing platforms that make it really difficult to integrate different systems.</p>
<p style="text-align: left;">Again, "HPC" is not the opposite of "cloud;" it's not an either/or decision. There are technologies and tools that were designed from the beginning to simplify the secure connection of services and resources; they just weren't invented by the HPC community.</p>
<h2 id="section2">Top500 and Aurora</h2>
<p>One of the cornerstones of ISC is the semiannual release of the Top500 list, and unlike at SC, the Top500 announcements and awards do not overlap with any other sessions, so it tends to have a higher profile and draw all attendees. This go-around, there were no dramatic changes in the Top 10; the new Alps system at CSCS was the only new entry, and the order of the top five systems remained the same. Notably, though, Aurora posted a significantly higher score than at SC'23 and broke through the exaflops barrier using 87% of the system, cementing its place as the second exascale system listed. But let's start at the top.</p>
<h3 id="section21">#1 - Frontier</h3>
<p>Frontier at Oak Ridge remained #1, but it squeezed twelve more petaflops out of the same node count and is now just over 1.2 EF. Nothing groundbreaking, but it's clear evidence that ORNL is continuing to tune the performance of Frontier at full system scale.</p>
<h3 id="section22">#2 - Aurora</h3>
<p>Aurora, on the other hand, finally eked over the exaflops line with 1.012 EF using 87% of the system's total 63,744 GPUs. Rick Stevens gave a short talk about the achievement which is summed up on this slide:</p>
<div class="separator" style="clear: both; text-align: center;"></div>
<p>I was a little surprised by how honest Stevens was in this talk; the typical game that is played is that you stand up on stage, talk about how great of a partnership you had with your partners to realize this achievement, extol the virtues of the technologies on which your system was built, and talk about how this HPL score is just the start of a lot of great science.</p>
<p>Stevens didn't do that though.</p>
<p>He started out by telling the conference that Intel had bad product names, then explained that their low Graph500 and HPCG scores were the result of their exclusive focus on breaking the exaflops barrier with HPL, implying they didn't have time or ability to run Graph500 or HPCG at the same 87%-89% scale as their HPL and HPL-MxP runs. Based on this, it sounds like Aurora is still a ways away from being stable at scale, and we're unlikely to see any Gordon Bell-nominated papers at SC'24 this November.</p>
<p>After this session, folks seemed to relish in <a href="https://x.com/hpc_guru/status/1792018127464874176?s=61">dunking on Aurora</a>; its <a href="https://x.com/hpc_guru/status/1790248333120000500?s=61">window to be #1 is likely to have closed</a> and it has <a href="https://x.com/hpc_guru/status/1790273734730985865?s=61">some power efficiency issues</a>. But I don't think anyone involved with the Aurora project needs to be told that; if what Stevens implied is true, the folks at ALCF, Intel, and HPE have been struggling for a long time now, and topping out over 10<sup>18</sup> was a hard-sought, major milestone to be celebrated. The Aurora project has been thrown more curveballs than I would have ever guessed a single HPC project could have, so all parties deserve credit for sticking it through all this way rather than just walking away. With any luck, Aurora will stabilize in the next six months, and we'll see full-scale runs of Top500, Graph500, HPCG, and science apps by November.</p>
<h3 id="section23">#3 - Eagle</h3>
<p style="text-align: left;">The third highest system on the list was Eagle, whose HPL score was not updated since the system was first listed at SC'23 last year. Through a few twists of fate, I wound up being the person who accepted the award on-stage, and I now have a Top500 award for the #3 system sitting in my home office. Here's a photo of me goofing around with it:</p>
<div class="separator" style="clear: both; text-align: center;"></div>
<p style="text-align: left;">It's not entirely inappropriate that I was the one to accept it since my teammates are the ones carrying pagers for the on-call rotation of that system, and we were also the hands-on-keyboard when that HPL run was conducted. Still, it was a bit surreal to walk on-stage to pick up such a noteworthy award immediately following two actually important people (both of whom have "director" in their titles) accepting the same award. By comparison, most of my career highlights to date have been just trolling HPC people on Twitter (as the esteemed Horst Simon actually said out loud as I was leaving the stage!)</p>
<p style="text-align: left;">It was weird.</p>
<p style="text-align: left;">That said, I take this to mean that it is now my duty to be the friendly face from Microsoft who can speak intelligently about the #3 system on Top500. To that end, I'll answer some questions that I was asked at ISC about the system and Azure HPC clusters in general below. <i>None of this is new or secret information!</i></p>
<p style="text-align: left;"></p>
<ul style="text-align: left;"><li><b>Why didn't you run HPL again and post a higher score to beat Aurora?</b> Because the day after that HPL run completed, that system was put into production. Once systems are in production, people are paying to use them, and taking a time-out to re-run HPL costs a ton of money in either real dollars (if a customer runs it) or lost revenue (if the HPL run is blocking customer workloads). This is quite different from public-sector HPC systems which never have to pay for themselves.</li><li><b>Can I get access to Eagle for a Gordon Bell run or to test software?</b> That's not really how it works. Whereas a traditional supercomputer might allow users to ssh in and submit jobs to a Slurm queue, cloud-based supercomputers allow users to deploy virtual machines through a REST API. Those virtual machines can allow ssh, run Slurm, and support MPI jobs like HPL, but that OS environment is managed by Azure users, not Azure itself. You can get a taste for what's required to run a basic MPI job by reading some instructions I wrote on <a href="https://www.glennklockwood.com/cloud/mpi-cluster.html">provisioning an MPI cluster on Azure</a>.</li><li><b>Is it just a bunch of GPU nodes scattered around a bunch of data centers?</b> No, all the nodes on any given Azure HPC cluster (like Eagle) share an InfiniBand fabric. There are countless InfiniBand clusters in Azure, but each one is a real supercomputer by any definition of a supercomputer, and they are designed to run tightly coupled job across all their GPUs.</li><li><b>What parallel file system does it use?</b> Don't think about it that way. You can provision a Lustre file system and mount that to any or all cluster nodes if you want to, or you can access data directly from object storage.</li><li><b>Are there any photos of it?</b> You can see a photo of one of the Microsoft-designed nodes that comprise the system on my <a href="https://blog.glennklockwood.com/2023/11/sc23-recap.html">SC'23 recap blog post</a>. Beyond that, there's not much to look at because Azure HPC clusters are not meant to be photogenic like, say, Cray supercomputers. There's no rack graphics (or even rack doors!). It's just tons and tons of air-cooled racks with InfiniBand optics coming out of each one. Maybe the only unique thing is that the racks are painted white instead of the typical black. Not sure why.</li></ul>
<div>Getting back to that false separation between "HPC" and "cloud," Eagle is strong evidence that they aren't different. What the "hyperscalers" do is not that different from what traditional HPC centers do. Perhaps the biggest difference is that cloud supercomputers get all the benefits of cloud infrastructure like software-defined infrastructure like virtual machines and virtual networking, integration with identity and access management that transcends simple Linux UIDs/GIDs, and the flexibility to integrate with whatever storage systems or ancillary services you want from any compute node.</div>
<p></p>
<h3 id="section24">Other notable tidbits</h3>
<p>It is tradition for Erich Strohmaier to talk through some highlights and trends of the latest Top500 list every time a new one is announced, and in the past, <a href="https://x.com/glennklockwood/status/1140637729182683136?s=61">I've been critical</a> of how he's presented conclusions from the list with this implicit assumption that computers that never post to Top500 simply don't exist. This year felt different, because Dr. Strohmaier made the explicit statement that China has completely stopped submitting to Top500. Their exascale systems aren't listed, but neither are any new systems in the past three years at the bottom. They simply don't play the game anymore, making it undeniable that Top500 is no longer an authoritative list.</p>
<p>Just as the whole conference's theme was reinventing HPC, I felt a sense that even the most stalwart proponents of Top500 are now recognizing the need to reinvent the Top500 list. Kathy Yelick said as much during her keynote ("Shall we replace Top500? What are the metrics in post-exascale computing that are important?"), and Erich implored the audience to help expand the <a href="https://hpl-mxp.org/">HPL-MxP</a> (formerly HPL-AI; an HPL-like benchmark that can use the mixed-precision capabilities of tensor cores) list. Nobody seems to know how to quantify what makes a leadership supercomputer nowadays, but accepting that HPL scores (or appearing on the Top500 list!) won't cut it is a good first step.</p>
<p>That all said, Top500 is still a valuable way to track technology trends in the industry. For example, this edition of the list where NVIDIA's new Grace-Hopper node started appearing in force. The only new entrant in the Top 10 was the <a href="https://www.top500.org/system/180259/">270 PF GH200</a> component of <a href="https://www.cscs.ch/computers/alps">CSCS's Alps system</a>, and HPEhad these EX254n GH200 blades on display on the show floor.</p>
<div class="separator" style="clear: both; text-align: center;"></div>
<p>To HPE/Cray's credit, they seem to have gotten the system up and running with Slingshot without the delays that plagued early Cray EX systems like Frontier and Aurora. Hopefully this is a sign that the Cray EX platform and Slingshot-11 have graduated from being risky and not-quite-production-ready.</p>
<p>The other notable entrants on this year's Top500 are a trio of <a href="https://www.top500.org/system/180283/">early MI300A APU-based Cray systems</a> being built around the El Capitan program at Lawrence Livermore National Laboratory. This is a positive sign that MI300A is up and running at modest scale, and HPE also had one of these EX255a blades on display at their booth:</p>
<div class="separator" style="clear: both; text-align: center;"></div>
<p>The strong showing of MI300A suggests that we may see El Capitan take the top spot in the next edition of the Top500 list coming in November.</p>
<h2 id="section3">Everyone is an AI expert!</h2>
<p>Since I now work on a team responsible for AI infrastructure, I tried attending as many of the AI-focused talks and panels as I could this year. Unsurprisingly, these sessions largely carried the same undertones of "reinventing HPC," and speakers opined on how AI would affect scientific computing and offered examples of what their institutions were doing to extend their leadership in the HPC space into the AI space. There was a fair amount of grasping going on (as there always is when AI is discussed at non-AI conferences), but this year I was struck by how confused so many speakers and attendees were about concepts related to applying AI.</p>
<p>To be clear: I am no expert in AI. However, my day job requires that I be steeped in some of the largest AI training workloads on the largest AI supercomputers on the planet, and I have to have a cursory understanding of the latest model architectures and techniques to anticipate how future system designs will have to evolve. It's from this perspective that I made the following observation: there are a lot of HPC people speaking very confidently about AI based on an outdated understanding of the state of the art. The AI industry generally moves much faster than the government-funded research community, and I couldn't help but wonder if some community leaders assumed that the AI industry today is the same as it was the last time they wrote their AI grant proposal.</p>
<p>Of course, there were also some really insightful perspectives on AI for science shared as well. Let's talk through some examples of both.</p>
<h3 id="section31">The Exascale AI Synergies LLM Workflows BOF</h3>
<p>This realization that the ISC community is not keeping up with the AI community first slapped me in the face when I ducked into a BOF session titled, "<a href="https://isc.app.swapcard.com/event/isc-high-performance-2024/planning/UGxhbm5pbmdfMTgyNjgxMQ==">Tales of Exascales – AI and HPC Supercomputing Platforms Synergies for Large Language Models (LLMs) and Scientific Workflows</a>." I sometimes wonder if the organizers who propose titles like that are intentionally creating word salad, but in this case, it was apt session name; the discourse around HPC and AI was all over the board throughout the hour.</p>
<p>The session started on a strong, positive note by Simon McIntosh-Smith describing Bristol's new <a href="https://www.bristol.ac.uk/news/2023/september/isambard-ai.html">Isambard-AI system</a>, a GH200-based Cray supercomputer funded under the broad charge of "AI research." While I'm usually skeptical of such nebulously defined "AI research" machines, Dr. McIntosh-Smith's description of the project quickly checked a bunch of boxes on how a real AI research platform should be developed. In particular,</p>
<p><b>Isambard-AI was developed and deployed at the pace of AI rather than HPC for scientific computing</b>. Whereas government-funded, large-scale HPC systems typically take years to procure, Simon said that the first discussions started in August 2023, and in the nine months that followed, they had built the site, the team, and the system itself to the degree that <a href="https://www.top500.org/system/180257/">a piece of the final system is already on Top500</a>. By comparison, LLNL's El Capitan supercomputer also debuted on Top500 this month, but <a href="https://www.energy.gov/articles/does-nnsa-signs-600-million-contract-build-its-first-exascale-supercomputer">its contract was signed five years ago</a>, and its procurement began <a href="https://web.archive.org/web/20200605114639/https://asc.llnl.gov/coral-2-benchmarks/">at least two years before that</a>. The AI industry would not exist if the systems it trains on took seven years to procure.</p>
<p><b>Isambard-AI deliberately avoided exotic AI accelerators to remain future-proof</b>. Simon rightly pointed out that the AI industry moves too quickly to anticipate whether a bespoke AI accelerator would even be relevant to whatever the hottest model architecture will be in a year. GPUs were chosen because they are the most flexible way to accelerate the widest range of AI workloads, regardless of if they are dense models, sparse models, inferencing, training, and whatever level of quantization makes sense. The reality is that cutting-edge research is done on GPUs, so aligning an AI supercomputer on the same technology will ensure that the algorithms developed by industry are immediately usable for scientific research.</p>
<p><b>A reasonable definition of "AI for science" was defined from the outset</b>. Rather than blurting out "we need to research AI!" and asking for a sack of money to buy GPUs, Simon outlined a vision of training AI models using data generated by physical simulation on a more conventional HPC system. Training models on models to create surrogate models is not particularly new, but it does establish a few reasonable architectural decisions such as having a robust data management and sharing platform, close coupling to the HPC system performing simulation, and aligning software stacks and programming environments as closely as possible.</p>
<p>Simon's contribution to the discussion stood out to me as the most impressive, and the discourse seemed to fall into a trap of familiarity following. Rather than focusing on the new and exciting prospects of AI, some panelists and audience members wanted to focus on the aspects of AI they understood. For example, an uncomfortable time was spent on a back-and-forth on how HPC centers can support Kubernetes and random I/O (which is what defines AI vs. HPC?) instead of Slurm and Lustre. If your biggest challenge in delivering infrastructure to support AI workloads is figuring out how to deploy both Kubernetes and Slurm, you haven’t even reached the starting line. This is a trivial issue in cloud environments, where entire AI clusters can be built up and torn down in minutes. Again, this is evidence that the scientific computing community isn’t ready to keep pace with the AI industry.</p>
<p>I jotted down a few of the questions and comments that I heard during this BOF that seem to reflect the level of familiarity the average ISC attendee has with AI:</p>
<p></p>
<ul style="text-align: left;"><li><b>"Would be nice if there were more models for science."</b> I wasn't sure sure what this means. All the leading LLMs are pretty good at "science," and domain-specific models aren't readily transferable between different science domains or problems.</li><li>Scientific problems <b>"have to validate outputs for correctness, unlike LLMs."</b> I think the speaker was making a sidelong reference to hallucinations, but like with any model (large language or physics-based), validating outputs for correctness is certainly necessary and readily possible.</li><li><b>"The demands of inference of LLMs are completely different from those for training. How do you buy inference infrastructure?"</b> I wonder where this notion came from. If your infrastructure can train a model, it can definitely inference that model. Cost-optimizing infrastructure for inferencing is a separate matter (you can cut corners for inferencing that you wouldn't want to cut for training), as is building the service infrastructure around inferencing to deliver inferencing as a service. But I don't think that's what this question was about.</li><li><b>"Working safely with sensitive data / isolating workloads on big shared clusters."</b> This is a problem that arises only when you try to wedge AI workloads into infrastructure designed for traditional physics-based simulation. If you have sensitive data, don't use big shared clusters. Provision separate clusters for each security domain on a shared, zero-trust infrastructure.</li><li><b>"How different are the files and filesystem access while training for LLMs, image generation models, reinforcement learning?"</b> This question reflects a general misunderstanding of data and storage in HPC overall; how data is organized into files and how that data is accessed by a workload is an arbitrary decision made by the application developer. You can organize piles of text into one giant file or a million little files.</li></ul>
<p></p>
<p>There were a few questions that came up that touched on deeper issues on which the HPC community should reflect:</p>
<ul><li><b>"What are the first steps for scientific groups wanting to get ready for using AI in the future?"</b> This is probably the purest question raised in the entire session, and I think this is something the scientific computing community as a whole needs to figure out. What does "using AI" really mean for scientific groups? Is it training models? Fine-tuning models? Inferencing using pre-trained models on HPC infrastructure? Is it integrating simulation applications with separately managed inferencing services? Who manages those inferencing services? Does inferencing even require HPC resources, or can suitable models run on a few CPU cores? I think the first step to answering this question is ensuring that the scientific computing community reaches a common baseline level of understanding of "using AI" means. And a lot of that probably means ignoring what some self-professed AI experts in the HPC community claim is the future.</li><li><b>"Care to predict what that ChatGPT moment will be for AI for Science? Had it already happened?"</b> This question was addressed directly by panelist Séverine Habert who rightly pointed out that the ChatGPT moment occurred when a complex and esoteric topic was suddenly put in the hands of hundreds of millions of laypeople across the world. It was the moment that the common person walking on the street could suddenly interact with the most cutting-edge technology that had been previously understandable only to the headiest of researchers in industry and academia. That will likely never happen in AI for science because science, by definition, requires a higher baseline of education and understanding than the average layperson has.</li><li><b>"How to effectively train the existing workforce when we are already struggling to retain talent in research/academia?"</b> This question strikes at the same theme that Kathy Yelick's opening keynote confronted: what is the role of the scientific computing community now that it turns out that you don't need decades of institutional experience to deploy and use HPC resources at leadership scale? As offensive as it may sound, perhaps the public-sector HPC community should accept that their role is not training future researchers and academics, but training future practitioners of AI in industry. This is how the wider tech industry generally works; neither startups nor tech giants make hires assuming those people will still be around in ten years. Why does the public-sector HPC industry think otherwise?</li></ul>
<p>Finally, I was also struck but how fiercely the discourse clung to the idea that large language models are the answer to all AI problems in science. I get that this panel was focused on exascale, and LLM training is one of the rare cases where AI requires exascale computing capabilities. But there was no acknowledgment that trillion-parameter models are not actually a good idea for most scientific applications.</p>
<h3 id="section32">AI Systems for Science and Zettascale</h3>
<p style="text-align: left;">This singular focus on creating massive LLMs for science was front-and-center in a talk given by Rick Stevens titled "<a href="https://isc.app.swapcard.com/event/isc-high-performance-2024/planning/UGxhbm5pbmdfMTg4MTE0Mg==">The Decade Ahead: Building Frontier AI Systems for Science and the Path to Zettascale</a>." The overall thesis that I heard was something like...</p>
<div><ol style="text-align: left;"><li>Science needs its own trillion-parameter foundation models</li><li>Training trillion-parameter foundation models requires a lot of GPUs</li><li>We need $25 billion from the U.S. government</li></ol><p style="text-align: left;">However, Stevens never answered a very basic question: what does a foundation model for science do that any other foundation model cannot do?</p>
<p style="text-align: left;">He showed slides like this which really don't sound like foundation models for science as much as a generic AI assistants:</p>
<div class="separator" style="clear: both; text-align: center;"></div>
<p style="text-align: left;">Is the scientific computing HPC community really the most qualified bunch to reinvent what existing foundation models like GPT-4 or Claude 3 have already done? Even if you argue that these proprietary models aren't as good at "science" as they could be, who would have a better chance of addressing this with a billion dollars of federal funding: the companies who developed GPT or Claude, or a collection of government scientists starting from scratch?</p>
<p style="text-align: left;">I think the answer to this question was in other parts of Stevens' talk. For example, he started with this slide:</p>
</div>
<div class="separator" style="clear: both; text-align: center;"></div>
<p>While robust requirements are good when there's no urgency, this slide is also a tacit admission that the government takes years to general a perspective on AI. Do you think the creators of Llama-3 or Mistral Large gathered wide community input from over 1,300 researchers before deciding to build a supercomputer and train a model? Even if science needs its own foundation models, this slide is strong evidence that, by the time the scientific HPC community agrees on a path forward, that path will be years out of date relative to what the commercial AI industry is doing.</p>
<p>A great example of this already happening is the basic premise that creating a foundation model with a trillion parameters is the best way to apply AI to solve science problems. This certainly was the leading thought two years ago, when transformer scaling laws were published that suggested that the best way to get better-performing LLMs was to simply add more parameters to your transformer and train on more data. But there's a reason all the leading models have stopped advertising how many parameters they use.</p>
<p>Dealing with massive transformers is really expensive. They're not only really expensive to train, but they're really expensive to use for inferencing too. This has led to a bunch of innovation to develop model architectures and approaches to training that result in dramatically higher quality outputs from a fixed parameter count. Dense transformer architectures with a trillion parameters have become the blunt instrument in developing foundation models since 2022, so it took me by surprise to hear Stevens put so much stock into this notion that the need for a trillion-parameter model is essential for science.</p>
<p>To repeat myself, I am no expert in AI. I've never been <a href="https://www.energy.senate.gov/services/files/CF8309D8-C0A1-40C7-944F-CF71EF523FF8">called in front of Congress to talk about AI</a> or been <a href="https://isc.app.swapcard.com/event/isc-high-performance-2024/planning/UGxhbm5pbmdfMTg4MTE0Mg==">invited to give talks on the topic at ISC</a>. There might be something basic that I am missing here. But when I look at the science drivers for AI:</p>
<div class="separator" style="clear: both; text-align: center;"></div>
<p>I <i>know</i> that you do not need to train your own trillion-parameter model to do most of this stuff. Even the use cases that do require generative AI, like code generation and math theory, don't actually require trillions of parameters. Small language models, such as that described in <a href="https://arxiv.org/abs/2306.11644">Textbooks Are All You Need</a> (published in 2023, after the reports Stevens cited in his talk), can produce amazing results with very small models when you train them using high-quality data instead of garbage from Reddit. And when you create or fine-tune a small language model for a specific science domain, not only do you save yourself from having to buy a billion-dollar supercomputer for training, but you get a model that is much more accessible to scientists around the world because they won't need a million dollars' worth of GPUs to inference with it.</p>
<p>So, if there's one question that was never answered across any of the AI-themed sessions at ISC this year, it is this: Why does science need to train its own large language models? My intuition is that either fine-tuning existing large language models or training small language models for domain-specific applications, would be a better investment in actually advancing science. However, if we cynically assume the real goal of LLMs-for-science is to justify buying massive GPU systems, suddenly a lot of the talks given at ISC on this topic make a lot more sense.</p>
<h3 id="section33">Real applications of generative AI for science</h3>
<p>As frustrated as I got sitting through sessions on AI where it sometimes felt like the blind leading the blind, there was one really good session on actual applications of generative AI for science.</p>
<p><b>Mohamed Wahib </b>of RIKEN gave an insightful presentation on the unique challenges of using generative AI in science. His summary slide touched on a lot of the key challenges:</p>
<div class="separator" style="clear: both; text-align: center;"></div>
<p>And his actual talk focused largely on the model and data aspects of generative AI. What struck me is that the challenges he described reflected the experience of someone who has actually tried to do what many other AI experts at the conference were claiming would be the future. For example,</p>
<p></p>
<ul style="text-align: left;"><li>He recognized the importance of <b>training scientific models with high-quality datasets</b>, not just garbage scraped off of social media. This means not only scraping or generating high quality data for training, but curating and attributing that data and applying reinforcement learning with human feedback as the model is being trained. This is uniquely challenging when creating models for scientific applications, as managing the quality of scientific data requires deep domain expertise. This contrasts with a generic chat bot whose inputs and outputs can often be assessed by anyone with a basic education.</li><li>He also talked about the tendency of <b>scientific data to be highly multimodal and multidimensional</b>. Whereas multimodal chatbots may combine text and vision, scientific data often contains observations of the same phenomenon from many different sensors (for example, pressure, temperature, density, strain fields, ...), and the output of a generative model for science may require multiple modalities as well.  These capabilities are not well developed in LLMs designed for human language.</li><li>Dr. Wahib also pointed out that scientific datasets tend to be huge compared to text and images, and this may require developing ways for models to have <b>context windows can fit multi-petabyte datasets' tokens</b> to identify long-range correlations. Relatedly, he also pointed out that <b>tokenization of scientific data</b> is a new set of challenges unique to this community, since industry has been focused on tokenizing low-dimensional data such as text, audio, and images.</li></ul>
<p></p>
<p>The good news is that industry's quest towards both commercializing generative AI and achieving AGI will touch on some of these challenges soon. For example, training domain-specific models using high-quality datasets is an essential component of the small language models I described in the previous section, and these small language models are what will enable privacy-preserving and cost-effective generative AI on laptops and phones. Effectively infinite context windows are also a major hurdle on the path to AGI, as industry is hard at work developing AI agents that can remember every conversation you've ever had with them. Finding more scalable approaches to attention that do not sacrifice accuracy are a part of this.</p>
<p><b>François Lanusse</b>, currently at the Flatiron Institute, also gave a nice presentation that clearly explained how generative AI can be used to solve inverse problems—that is, figuring out the causes or conditions that resulted in a collection of measurements. A precise example he used applied generative AI to figure out what an image distorted by gravitational lensing might look like in the absence of those distortions. As I understood it, he trained a diffusion model to understand the relationship between images that are affected by gravitational lensing and the masses that cause lensing through simulation. He then used that model instead of an oversimplified Gaussian model as part of a larger method to solve the inverse problem of un-distorting the image.</p>
<p>The details of exactly what he did were a little over my head, but the insight piece for me is that combining generative AI and science in practice is not as straightforward as asking ChatGPT what the undistorted version of a telescope image is. Rather, almost all of the standard, science-informed approach to solving the inverse problem remained the same; the role of generative AI was simply to replace an oversimplified part of the iterative process (the Annealed Hamiltonian Monte Carlo method) to help it converge on better answers. It really is a combination of simulation and AI, rather than an outright substitution or surrogate model.</p>
<p>Dr. Lanusse also showed this slide which demonstrated how this approach can be generalized to other scientific domains:</p>
<div class="separator" style="clear: both; text-align: center;"></div>
<p>The general approach of pretraining, fine-tuning ("adapt"), and combining foundation models with other physics-based models seems reasonable, although I admit I have a difficult time wrapping my head around exactly how broadly scoped he envisions any given pretrained foundation model to be. I can see such a model trained on extensive sky survey data being useful for a number of astrophysical and cosmological tasks, but it's less clear to me how such a model might be useful in unrelated domains like, say, genomics.</p>
<p>You might also ask why I think this vision of foundation models for science is reasonable while Rick Stevens' vision didn't ring true; the difference is in scale! The foundation models cited on Lanusse's slide are vision transformers which have many orders of magnitude fewer parameters than the trillion-parameter models that others talk about. Whereas a trillion-parameter model might need to be distributed over dozens of H100 GPUs just to produce one inference result, the largest of the vision transformers can probably be squeezed on to a single high-end desktop GPU. Again, <i>you don't need billion-dollar supercomputers to train these models for science</i>.</p>
<p><b>Frank Noé</b> from Microsoft Research then talked about how generative AI can be applied to solve problems in simulating biological systems. Like the talk before his, Dr. Noé followed this pattern where a larger, physics-based framework had one statistical technique replaced by a method based on generative AI, and then a physics-based model is used to quantify the likelihood that the result is reasonable. He contrasted this with convention approaches (to, say, protein folding) where you just simulate for really long times in the hopes that your simulation randomly wanders into a situation where you capture a rare event.</p>
<p>His talk wasn't about generative AI as much as the previous speakers, but he offered a litany of ways in which AI models can be useful to molecular modeling:</p>
<p></p>
<ul style="text-align: left;"><li><b>Markov state models</b> provide a statistical framework that lets you replace one long simulation (that hopefully captures every possible scenario) with a bunch of short, chopped-up simulations that hopefully capture every possible in parallel. He cited an example that took 20,000 GPU-days on V100 GPUs that would've otherwise taken a million GPU-years if done in one long simulation.</li><li><b>Coarse-grained models</b> use machine learning to develop surrogate models to simulate the physics of relatively uninteresting parts of molecular systems. The example he used was simulating the water molecules surrounding a biomolecule; water can be very difficult to accurately model, and the example he cited led to a surrogate model that was 100x faster than directly simulating water molecules.</li><li><b>Boltzmann generators</b> can generate 3D molecular structures based on a known probability distribution defined by the energy states of the system. This is another fast way to find rare but stable molecular configurations without having to throw darts at a dartboard.</li></ul>
<p></p>
<p>What struck me is that, in all these cases, the AI model is never generating results that are blindly trusted. Instead, they generate molecular configurations which are then fed into physics-based models which can quantify how likely they are to be valid.</p>
<p>Both Lanusse's and Noé's examples of combining AI and simulation painted a picture to me where generative AI can be really useful in solving problems where a researcher would otherwise have to make educated guesses about what physical phenomenon is really happening based on incomplete information. So long as there is a way to apply a physics-based model to check the accuracy of each guess, generative AI can be trained to predict the relationships between incomplete information and what's really going on and get to probable answers much faster than relying on physics alone.</p>
<p>More broadly, I couldn't help but think about the <a href="https://www.youtube.com/watch?v=Jfv5XCMj2c0">Sora video showing pirate ships battling in a cup of coffee</a> as I left this session. Like that video, these talks demonstrated that it's possible to train generative AI models to reproduce physical phenomena (like the fluid dynamics of coffee) without explicitly embedding any laws of physics (like the Navier-Stokes equations) into the model itself and still get really compelling results. The part of this that was lacking from the Sora video—but was present in these talks—was closing the loop between generated results and the laws of physics by feeding those generated results back into the laws of physics to figure out if they are probable.</p>
<h2 id="section4">High Performance Software Foundation</h2>
<p style="text-align: left;">ISC'24 wasn't all about AI though! I wound up attending the launch of the <a href="https://www.hpsf.io/">High Performance Software Foundation</a> (HPSF), a new Linux Foundation effort spearheaded by Todd Gamblin and Christian Trott (from Livermore and Sandia, respectively) aimed to promote the sustainability of the software packages relied upon within the high-performance computing community.</p>
<p style="text-align: left;">I haven't paid close attention to HPC software in a long time since most of my work was in platform architecture and storage systems, so a lot of the background context remains a little murky to me. That said, it seems like HPSF was formed to be like the Cloud Native Computing Foundation for the HPC community in that:</p>
<p></p>
<ul style="text-align: left;"><li>it will serve as a neutral home for software projects that aren't tied to any single university or government institution</li><li>it provides mechanisms to ensure that critical HPC software can continue to be maintained if its original author gets hit by a bus</li><li>it will help with the marketing, promotion, and marketing of HPC software</li></ul>
<p></p>
<p>Its governance seems pretty reasonable, with different levels of membership being accompanied by different levels of rights and obligations:</p>
<div class="separator" style="clear: both; text-align: center;"><span style="text-align: left;"> </span></div>
<p>There is a Governing Board is comprised of paying members (and predominantly those who pay the most), while the Technical Advisory Council carries out the more technical tasks of forming working groups and onboarding projects.</p>
<p>There are three levels of membership, and the highest (premier) has a $175,000 per year buy-in and comes with a seat on the Governing Board. Right now, the founding seats are held by AWS, HPE, LLNL, and Sandia.</p>
<p>Below that is a general membership tier whose cost is on a sliding scale based on the organization size, and AMD, Intel, NVIDIA, Kitware, ORNL, LANL, and Argonne have all committed at this level.  The associate tier is below that, and it is free to nonprofits but comes with no voting rights.</p>
<p>It seemed like the exact functions that HPSF will have beyond this governing structure are not fully baked yet, though there were six "prospective" working groups that provide a general scope of what the HPSF will be doing:</p>
<div class="separator" style="clear: both; text-align: center;"></div>
<p>My read of the description of these working groups is that</p>
<p></p>
<ul style="text-align: left;"><li><b>CI/testing</b> will supply resources (GPUs) on which HPSF projects' code can be automatically tested.</li><li><b>Software stacks</b> will maintain E4S.</li><li><b>User engagement</b> sounds like it will figure out what users of HPSF projects' software are looking for. It sounds like this will provide some product management-like support for projects.</li><li><b>Facility engagement</b> is probably like user engagement, but for the sites deploying code on behalf of their users. Again, this sounds like product management functions.</li><li><b>Security</b> sounded like stewarding SBOM-like stuff for member projects' software.</li><li><b>Benchmarking</b> would make a framework for benchmarking HPC applications.</li></ul>
<p></p>
<p>That all said, it still wasn't clear what exactly HPSF would do; what would all those membership dues go towards supporting? Based on some Q&amp;A during this BOF and follow-up afterwards, I pieced together the following:</p>
<p></p>
<ul style="text-align: left;"><li>HPSF will <i>not</i> be funding developers, much in the same way that OpenSFS doesn't fund Lustre development. That said, <a href="https://x.com/tgamblin/status/1790018859816153327">Todd Gamblin later said</a> that not funding software development was a financial constraint more than a policy one, with the implication that if more members join, there may be opportunity for HPSF to fund projects.</li><li>HPSF likely will be hosting events and conferences (perhaps like the CNCF hosts KubeCon), providing scholarships, developing and providing training related to member projects, and "increasing collaboration" (whatever that may mean!).</li></ul>
<div>HPSF also has some influence and ownership over its member projects:</div>
<p></p>
<ul style="text-align: left;"><li>HPSF will co-own its projects' GitHub repos to ensure continuity in case the other repo owner abandons it.</li><li>HPSF will own the domain for the project for the same reasons as above.</li><li>Member projects still manage their own software development, roadmaps, releases, and the like. The HPSF won't dictate the technical direction of projects.</li><li>HPSF will own the trademark and logos of its member projects so it can prevent corporations from profiting off of repackaging products without respecting trademark.</li></ul>
<p style="text-align: left;">This establishes an interesting new direction for the sorts of software projects that are likely to become member projects. Historically, such projects developed by the member organizations (i.e., DOE labs) have been wholly controlled by the labs that funded the work, and those software projects lived and died at the whims of the government funding. The HPSF offers a new vehicle for software projects to live on beyond the end of the grants that created them, but at the same time, it requires that the DOE surrender control of the work that it sponsored.</p>
<p style="text-align: left;">I left the session still wondering a few pretty major things, likely borne out of my own ignorance of how similar organizations (like CNCF or the Apache Foundation) work:</p>
<p style="text-align: left;"></p>
<ol style="text-align: left;"><li>How does a software project actually become a member project? The HPSF folks said that the Technical Advisory Committee onboards new projects, but what is the bar if I have an open-source project used by the community that I no longer want to maintain myself? I assume it's not a pay-to-play arrangement since that defeats the purpose of sustaining software after its seed funding runs out.</li><li>What do stakeholders actually get out of joining HPSF? I see obvious value for organizations (like the DOE labs) who develop open-source software but may not want to be exclusively responsible for sustaining it forever. But would an HPC facility get any obvious benefit from joining and paying dues if it is simply a consumer of member projects' software? What does a cloud vendor like AWS get by being a premiere member? Is HPSF just a way to get someone else to cover the overheads of maintaining <a href="https://github.com/awslabs">open-source software that comes out of, say, R&amp;D organizations</a> rather than product organizations?</li></ol>
<p></p>
<p></p>
<p>Hopefully the answers to these questions become clearer as the foundation gets off the ground and we get to see what member organizations contribute under the HPSF banner.</p>
<p>Ultimately though, I see this as a really positive direction for the HPC software community that might help resolve some uncertainty around key pieces of HPC software that have uncertain ownership. For example, I wound up as a maintainer of the IOR and mdtest benchmark because I was the last one to touch it when its previous maintainer lost interest/funding. I don't even work in I/O performance anymore, but the community still uses this benchmark in virtually every procurement of parallel file systems either directly or through IO500. It would be wonderful if such an important tool didn't rest on my shoulders and had a more concrete governance structure given how important it is.</p>
<h2 id="section5">Quantum computing</h2>
<p style="text-align: left;">Besides AI and cloud, quantum computing was cited in Kathy Yelick's opening keynote as the third disruptor to HPC for scientific computing. At the time, I thought citing quantum was just an obligation of any opening keynote speaker, but quantum computing was particularly high-profile at ISC this year. I was surprised to see over a dozen quantum computing companies on the vendor exhibition floor, many of whom were Europe-based startups.</p>
<p style="text-align: left;">In addition, this year's Hans Meuer award (for best research paper) was given to a paper on quantum computing by Camps et al. This is particularly notable since this is the first time that the Meuer award has ever been given to a paper on a topic that isn't some hardcore traditional HPC like MPI or OpenMP advancements; by comparison, this award has never been given to any papers on AI topics. Granted, the winning paper was specifically about how to use conventional HPC to solve quantum problems, but this recognition of research in quantum computing makes a powerful statement: quantum computing research is high-performance computing research.</p>
<h2 id="section6">Reinvent HPC to include urgent computing?</h2>
<p style="text-align: left;">I was invited to give a lightning talk at the <a href="https://www.interactivehpc.com">Workshop on Interactive and Urgent High-Performance Computing</a> on Thursday, and urgent/interactive HPC is not something I'd really paid attention to in the past. So as not to sound like an ignorant fool going into that workshop, I opted to sit in on a focus session titled "Urgent Computing" on Tuesday. I had two goals:</p>
<p style="text-align: left;"></p>
<ol style="text-align: left;"><li>Make sure I understood the HPC problems that fall under urgent and interactive computing so I could hold an intelligent conversation on this topic at the Thursday workshop, and</li><li>See if there are any opportunities for cloud HPC to provide unique value to the challenges faced by folks working in urgent HPC</li></ol>
<div>I'll describe what I came away with through these lenses.</div>
<p></p>
<h3 id="section61">The Urgent Computing focus session</h3>
<p style="text-align: left;">What I learned from the focus session is that urgent computing is not a very well-defined set of application areas and challenges. Rather, it's another manifestation of reinventing HPC to include any kind of computation for scientific purposes.</p>
<p style="text-align: left;">Much to my surprise, this "Urgent Computing" focus session was actually a session on IoT and edge computing for science. Several speakers spoke about getting data from edge sensors on drones or telephone poles into some centralized location for lightweight data analysis, and the "urgent" part of the problem came from the hypothetical use cases of analyzing this sensor data to respond to natural disasters. There wasn't much mention of anything requiring HPC-like computing resources; at best, a few talks made unclear references to using AI models for data analysis, but it felt like grasping:</p>
<div class="separator" style="clear: both; text-align: center;"></div>
<p style="text-align: left;">The above conclusion slide was presented by one of the speakers, and to be honest, I don't understand what any of it means. Granted, I know very little about urgent computing, IoT, or edge computing so there may be some domain jargon here that's throwing me off. But based on this, as someone working in the area of HPC and AI in the cloud, I don't think I have a role to play here. I'm sure <i>cloud computing</i> can help, but the challenges would be in general-purpose cloud rather than HPC.</p>
<h3 id="section62">The Interactive and Urgent HPC workshop</h3>
<p style="text-align: left;">Fortunately for me, the Thursday workshop on Interactive and Urgent HPC was much less about edge/IoT and more about developing software infrastructure and workflows that allow scientific data analysis of large datasets to happen before the results become obsolete. It was a fascinating workshop for learning about specific science drivers that require fast access to HPC resources, and how different HPC providers are enabling that through non-traditional services and policies. Below are a few highlights.</p>
<p style="text-align: left;"><b>Sam Welborn (NERSC)</b> presented his team's efforts to convert a streaming data workflow from its current file-based approach into one that streamed directly into compute node memory. The specific use case was the initial data processing for image information coming off of a scanning transmission electron microscope at 480 Gbps, totaling 750 GB per shot. As he described it, the current technique involves streaming those data to files at the microscope, then copying those files to the parallel file system of a remote supercomputer, then reading, processing, and writing that data within the HPC environment to prepare it for downstream analysis tasks. And for what it's worth, this is how I've always seen "streaming" HPC workflows actually work; they're actually using file transfers, and the performance of both the file system at the source and destination are in the critical path.</p>
<p style="text-align: left;">The problem with this approach is that parallel file systems on HPC systems tend to be super flaky, and there's no real reason to bounce data through a storage system if you're just going to pick it up and process it. So, Dr. Welborn showed a true streaming workflow that skipped this file step and used ZeroMQ push sockets at the microscope and pull sockets on the HPC compute nodes to do a direct memory-to-memory transfer:</p>
<div class="separator" style="clear: both; text-align: center;"></div>
<p style="text-align: left;">Seeing software like ZeroMQ used to enable communication in an HPC environment instead of forcing this workflow to fit into the MPI paradigm is an encouraging sign in my eyes. ZeroMQ, despite not using purpose-built HPC technology like RDMA, is the right tool for this sort of job since it supports much better resilience characteristics than messaging libraries designed for tightly coupled HPC jobs. Workflows like this that combine beefy GPU nodes with software developed in the commercial tech space suggest that the world of HPC is willing to abandon not-invented-here ideology.</p>
<p style="text-align: left;">It wasn't clear to me that there's a great opportunity for cloud HPC to be uniquely useful in use cases like this; while you certainly can provision beefy CPU and GPU nodes with InfiniBand in Azure, cloud services can't obviously simplify this ZeroMQ-based workflow beyond just supplying general-purpose VMs on which the orchestration services can run. Had this team stuck with a file-based streaming mechanism, the performance SLAs on cloud storage (like object or ephemeral Lustre) would provide a more reliable experience to ensure the data transfer happened in near-real-time. But the better solution to unpredictable file system performance is to do exactly what was done here: skip the file system entirely.</p>
<p style="text-align: left;">Just to keep the speaker honest, I asked why this computation couldn't simply be done at the same place as the telescope generating the data. After all, if the telescope always generates 750 GB per shot, you should be able to buy a couple GPU servers that are ideally sized to process that exact workload in the time between images. There were actually two answers: one from Sam and one from an audience member:</p>
<p style="text-align: left;"></p>
<ol style="text-align: left;"><li>Sam said that you can process this workflow locally, but that the goal of this work was to prepare for a future microscope (or another instrument) that could not. He also insightfully pointed out that there's tremendous value in getting the data into the HPC environment because of all the services that can be used to work on that data later. I envisioned doing things like using a Jupyter notebook to further process the data, serve it up through a web UI, and similar tasks that cannot be done if the data is stuck inside a microscope room.</li><li>An audience member also pointed out that sticking GPU nodes in the same room as electron microscopes can result in enough noise and vibration to disrupt the actual scope. This was a great point! In the days before I started working in HPC, I was training to become an electron microscopist, and I worked in a lab where we had <a href="https://ifmd.lehigh.edu/research-stem">water-cooled walls</a> to avoid the problems that would be caused by air conditioning breezes. There's no way a loud server would've worked in there.</li></ol>
<p style="text-align: left;"><b>Toshio Endo (Tokyo Tech)</b> gave an interesting talk on how they enable urgent/interactive compute jobs on their batch-scheduled TSUBAME4.0 supercomputer by doing, frankly, unnatural things. Rather than holding aside some nodes for interactive use as is common practice, his work found that a lot of user jobs do not completely use all resources on each compute node they reserve:</p>
<div class="separator" style="clear: both; text-align: center;"></div>
<p style="text-align: left;">I had to do a double-take when I saw this: even though 65%-80% of the nodes on the supercomputer were allocated to user jobs, less than 7% of the GPUs were actually being utilized.</p>
<p style="text-align: left;">Dr. Endo's hypothesis was that if nodes were suitably subdivided and jobs were allowed to oversubscribe CPUs, GPUs, and memory on a compute node without impacting performance too much, they could deliver real-time access to HPC resources without having to create a separate pool of nodes only for interactive uses. He defined success as the slowdown of a shared job being 1/k if k jobs shared the same node; for example, if four jobs were all running on the same node, each one taking four times as long to complete would be acceptable, but any longer would not. He then went on to show that the best way to accomplish this is using <a href="https://slurm.schedmd.com/gang_scheduling.html">Slurm's gang scheduling</a>, where each job takes turns having exclusive access to all the CPUs and GPUs on a node. The alternative (just letting the OS context switch) was no good.</p>
<p style="text-align: left;">While a fascinating study in how to provide zero wait time to jobs in exchange for reduced performance, this whole mechanism of using gang scheduling to exploit low resource utilization seems like jamming a square peg into a round hole. If a workload doesn't (or can't) use all the GPUs on a node, then that's not the right node for the job; I feel like a more appealing solution would simply be to offer a heterogeneous mix of nodes based on the demands of the workload mix. This is hard to do if you're buying monolithic supercomputers since you're stuck with whatever node mix you've got for five years, but there is another way to buy supercomputers!</p>
<p style="text-align: left;">I won't pretend like dynamically provisioning different flavors of CPU- and GPU-based nodes interconnected with InfiniBand in the cloud doesn't come with a cost; the convenience of being able to slosh a cluster makeup between CPU-heavy and GPU-heavy nodes will be more expensive than committing to use the same makeup of node flavors for multiple years. But if you're paying for GPUs that are only being used 7% of the time, surely it's cheaper to pay a higher cost for GPUs when you need them if it also allows you to not pay for them 93% of the time when they're idle.</p>
<p style="text-align: left;">Bjoern Enders (NERSC) gave the first lightning talk where he presented the exploration they're making into enabling real-time and urgent computation. They're currently going in three parallel directions to provide this capability:</p>
<p style="text-align: left;"></p>
<ol style="text-align: left;"><li>Reservations, a process by which a user can request a specific number of nodes for a specific period of time, and Slurm ensures that many nodes are available for the exclusive use of that user by the time the reservation starts. He said that implementing this at NERSC is costly and rigid because it requires a human administrator to perform manual steps to register the reservation with Slurm. </li><li>Realtime queues, where a few nodes are held from the regular batch queue and only special real-time users can submit jobs to them. Dr. Enders said that NERSC is extremely selective about who can access this queue for obvious reasons: if too many people use it, it will back up just like the regular batch queues do.</li><li>Jupyter Hub, which utilizes job preemption and backfill under the hood. If a user requests a Jupyter job, Slurm will pre-empt a job that was submitted to a preemptible queue to satisfy the Jupyter request. However, if there are no preemptible jobs running, the Jupyter job will fail to launch after waiting for ten minutes.</li></ol>
<p style="text-align: left;">To provide compute resources to back up these scheduling capabilities, they also deployed a new set of compute nodes that can be dynamically attached to different supercomputers they have to support urgent workloads even during downtimes.  Called "Perlmutter on Demand" (POD), it sounded like a separate set of Cray EX racks that can be assigned to either the Perlmutter supercomputer, or if Perlmutter is down for maintenance, either their smaller Alvarez or Muller supercomputers which share the same Cray EX architecture. What wasn't clear to me is how the Slingshot fabrics of these nodes interact; perhaps POD has its own fabric, and only the control plane owning those racks are what changes.</p>
<p style="text-align: left;">He showed a slide of explorations they're doing with this POD infrastructure, but as with Dr. Endo's talk, this seemed a bit like a square peg in a round hole:</p>
<div class="separator" style="clear: both; text-align: center;"></div>
<p style="text-align: left;">All of this sounds aligned with the strengths of what HPC in a cloud environment can deliver, and some of the big challenges (like figuring out the ideal node count to reserve for interactive jobs) are problems specific to Slurm and its mechanism for scheduling. There's a lot more flexibility to rapidly provision HPC resources in cloud environments because, unlike the case where Slurm is scheduling jobs on a single cluster, cloud resource managers can schedule across any number of clusters independently. For example, if an urgent workload needing only four GPU nodes suddenly appears, it doesn't necessarily have to be scheduled on the same InfiniBand fabric that a large hero job is running on. Since the urgent job and the hero job don't need to talk to each other, cloud resource managers can go find a GPU cluster with a little more flex in them to provision those resources quickly.</p>
<p style="text-align: left;">Automating the process of reservations is also a bit of a game of catch-up, though my guess is that this is more a matter of someone having a weekend to sit down and write the REST service that manages incoming reservation requests. Although there's not a direct analog for reservations like this in Azure, AWS has a feature called <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-capacity-blocks.html">AWS Capacity Blocks</a> that does exactly this: if you know you'll want a certain number of GPU nodes sometime in the future, Capacity Blocks let you reserve them ahead of time through an API.</p>
<p></p>
<p></p>
<p>Finally, <b>I represented Microsoft</b> and gave a lightning talk that riffed on a lot of what I've been writing about in this blog post: HPC seems to be reinventing a lot of things that the cloud has already figured out how to do. The illustrious Nick Brown was kind enough to <a href="https://twitter.com/nickbrownhpc/status/1791129551218590207?s=21&amp;t=7LM0hNWEuk95n8Z_CNZvPg">snap a photo of one of my slides and post it on Twitter</a>:</p>
<div class="separator" style="clear: both; text-align: center;"></div>
<p>My thesis was that the way urgent HPC workflows are triggered, scheduled, run, and reported on follows the same pattern that inferencing-as-a-service services (like Copilot and ChatGPT) are implemented under the hood, right down to executing multi-node jobs on InfiniBand clusters. The difference is that these cloud workflows are built on the foundation of really nice cloud services that provide security, scalability, monitoring, and hands-free management that were originally developed for commercial (not HPC!) customers. My argument was that, even if you don't want to pay cloud providers to run urgent HPC workflows as a managed service, you can use these services (and the software infrastructure on which they're built) as a blueprint for how to build these capabilities in your own HPC environments.</p>
<h2 id="section7">Concluding thoughts</h2>
<p>The ISC'24 conference was fantastic, and I am glad it has not lost the unique elements that made me want to attend in the years prior to the pandemic. It's still that smaller, intimate, and focused HPC conference that brings the community together. Although a lot of my synopsis above may sound critical of the content presented over the four days I attended, the fact that I've had so much to write down in this blog post is a testament to the value I really get out of attending: it makes me sit down and think critically about the way the HPC community is evolving, what the leading minds in the field are thinking, and where I might be able to contribute the most in the coming year.</p>
<p>I never much paid attention to the annual taglines of conferences like ISC, but this year's "Reinvent HPC" really resonated. The HPC community is at a crossroads. Exascale computing for science is now in the rear-view mirror, and large-scale AI is all the rage across the computing industry at large. But for the first time ever, this new direction in at-scale computing is happening without the inclusion of the people and organizations who've historically driven innovation in HPC. Whereas institutions like Oak Ridge, RIKEN, Cray, and Fujitsu defined the future of computing for decades, hundred-person startups like OpenAI and Anthropic are now paving the way in partnership with companies like Microsoft and Amazon.</p>
<p>HPC needs to be reinvented, if for no other reason than to decide whether the HPC community wants to be inclusive of new frontiers in computing that they do not lead. Does the HPC community want AI to be considered a part of HPC?</p>
<p>Judging from many speakers and panelists, the answer may be "no." To many, it sounded like AI is just another industry that's sucking all the air (and GPUs) out of the room; it's a distraction that is pulling funding and public interest away from solving real problems. It's not something worth understanding, it's not something that uses the familiar tools and libraries, and it's not the product of decades of steady, government-funded improvements. AI is "them" and HPC is "us."</p>
<p>Personally, I'd like the answer to be "yes" though. Now that I'm on the other side of the table, supporting AI for a cloud provider, I can say that the technical challenges I face at Microsoft are the same technical challenges I faced in the DOE. The desire to deeply understand systems, optimize applications, and put world-class computing infrastructure in the hands of people who do amazing things is the same. And as the days go by, many of the faces I see are the same; instead of wearing DOE or Cray badges, my lifelong colleagues are now wearing NVIDIA or Microsoft badges.</p>
<p>All this applies equally to whether cloud is HPC or not. The HPC community needs to reinvent itself to be inclusive of <i>everyone</i> working towards solving the same problems of computing at scale. Stop talking about people who work on commercial AI in cloud-based supercomputers as if they aren't in the room. They are in the room. Often near the front row, snapping photos, and angrily posting commentary on Twitter about how you're getting it all wrong.</p>
<div class="separator" style="clear: both; text-align: center;"></div>
<p style="text-align: left;">HPC has historically been used to solve scientific problems, whether to expand our understanding of the university, to find the next best place to drill an oil well, or to model the safety of aging nuclear weapons. The fact that HPC is now being used to solve squishier problems related to natural language or image generation does not change the essence of HPC. And whether that HPC is delivered through physical nodes and networks or virtualized nodes and networks is irrelevant, as long as those resources are still delivering high performance. AI is just as much HPC as scientific computing is, and cloud is just as much HPC as OLCF, R-CCS, or CSCS is.</p>
<p style="text-align: left;">So perhaps HPC doesn't need to be reinvented as much as the mindset of its community does.</p>
<p style="text-align: left;">That all said, I am genuinely impressed by how quickly ISC'24 has been reinventing itself in recent years. It wasn't too long ago that all its keynote speakers were greybeards from a predictable pool of public HPC centers all saying the same things year after year. It's wonderful to see a greater diversity of perspectives on the main stage and torches passing on to the next generation of leading figures in the field. And it was not lost on me that, for the first time in the history of this conference, Thomas Sterling did not deliver the closing keynote. As much fun as I had poking fun at his meandering and often-off-the-mark conjectures every year, it was delightful to be exposed to something new this year.</p>
<p style="text-align: left;">I'm hopeful that ISC will continue to get better year over year, and ISC'25 will feel more inclusive of me despite the fact that I am now one of those hyperscale cloud AI people. So long as I still feel like it's my community, though, I will keep showing up in Germany every summer.</p>]]></content><author><name>Glenn K. Lockwood&apos;s Blog</name></author><category term="glennklockwood" /><summary type="html"><![CDATA[I had the great pleasure of attending the ISC High Performance conference this month, marking the fifth time I've attended what has become one of my top must-attend industry conferences of the year. This year was particularly meaningful to me because it is the first time that: I attended ISC as a Microsoft employee. This is also the first time I've attended any HPC conference since I changed my focus from storage into AI infrastructure.I attended ISC in-person since before the pandemic. It's also the first time I've visited Hamburg which turned out to be an absolute delight. Although registrations have been lower since the pandemic, this year's final registration count was over 3,400 attendees, and there was no shortage of old and new colleagues to bump into walking between the sessions at the beautiful Congress Center Hamburg. &lt;p&gt;This year’s theme was “Reinvent HPC,” and that idea—that HPC needs to reinvent itself—was pervasive throughout the program. The whole industry had been pulling towards exascale for the better part of a decade, and now that there are two exaflop systems on Top500 and the dust is settling, it feels like everyone is struggling to figure out what’s next. Is it quantum? AI?&lt;/p&gt; It was difficult for me to draw a line through all the topics worth reviewing at this year's ISC, as it was a very dense four days packed with a variety of topics, discussions, vendors, and events. I only experienced a fraction of everything there was to be seen since so many interesting sessions overlapped, but I thought it might be worthwhile to share my perspective of the conference and encourage others to do the same. Table of Contents Reinventing HPC (and blast those hyperscalers!) Kathy Yelick's opening keynote Closing keynotes on the future Top500 and Aurora #1 - Frontier #2 - Aurora #3 - Eagle Other notable tidbits Everyone is an AI expert! The Exascale AI Synergies LLM Workflows BOF AI Systems for Science and Zettascale Real applications of generative AI for science High Performance Software Foundation Quantum computing Reinvent HPC to include urgent computing? The Urgent Computing focus session The Interactive and Urgent HPC workshop Concluding thoughts Reinventing HPC (and blast those hyperscalers!) The need to reinvent HPC was the prevailing theme of the conference from the very first session; with the listing of Aurora as the second system on Top500 to break the 1 exaflops barrier, the community is in search of a new milestone to drive research (and funding!). At the same time, commercial AI has rapidly risen up largely in an independent, parallel effort with a speed and scale that begs the question: how important was the decade-long drive to break the exaflops barrier if the AI industry could catch up so quickly without the help of the institutions that have historically posted the top HPL scores? If the commercial AI industry overtakes scientific computing as the world leader in deploying at scale, how can “HPC” be reinvented so it can continue to claim leadership in another dimension? Kathy Yelick's opening keynote ISC’s opening keynote was given by Kathy Yelick, where she provided commentary on two recent government-commissioned reports on the future of HPC: Charting a Path in a Shifting Technical and Geopolitical Landscape: Post-Exascale Computing for the National Nuclear Security Administration, commissioned by the National AcademiesCan the United States Maintain Its Leadership in High-Performance Computing?, commissioned by the US Department of Energy’s Advanced Scientific Computing Research program Living up to her reputation, Dr. Yelick’s talk was fast and insightful, describing the insatiable demand for computing driven by scientific research, the struggle to expose continuing amounts of parallelism to make use of newer processors, and some promising directions to address that disconnect. However, her talk started in a direction that I didn’t like when she went into describing the disruptors that necessitate reinventing HPC: The above slide implied that AI, quantum, or cloud may pose an existential threat to the HPC community gathered at ISC this year; this immediately raised my hackles, as it cast the relationship between “HPC” and “AI”/“cloud” as having some sort of adversarial tension. As the talk went on, I realized that “HPC” didn’t really mean “high-performance computing” to her. Rather, it was used to refer to something much more narrowly scoped—high-performance computing to solve scientific problems. Slide after slide, the presentation kept doubling down on this idea that “HPC” as the audience knows it is being threatened. For example, Yelick talked through this slide: The picture she painted is that “HPC” (denoted by companies with blue bars) no longer has influence over technology providers because the “hyperscalers” (green bars) have such an outsized amount of investment. She then used this to call on the audience to think about ways “we” could influence “them” to produce technologies that are useful for both scientific computing and low-precision AI workloads. Her talk culminated in this slide: Which was accompanied by this conclusion: "So what’s a post-exascale strategic for the scientific community? It's the beat 'em or join 'em strategy. The beat 'em strategy says we’re going to design our own processors. [...] The join 'em strategy says let's leverage the AI hardware that's out there. [...] The sort of sneaky way of doing this is getting embedded in the AI community and trying to convince them that in order to make AI better for commercial AI applications, you really want to have certain features. Like don't throw away your 64-bit arithmetic and things like that." I found myself getting increasingly unsettled through the keynote, because this "us versus them" mentality put me, a long-standing member of this HPC community, in the camp of "them." It was as if I was suddenly an outsider in a conference that I've been attending for years just because I no longer work for an organization that has been doing HPC since the early days of computing. Even though the clusters I support use the same NVIDIA and AMD GPUs, the same InfiniBand fabrics, and the same Lustre file systems that "HPC" uses, I am no longer in "HPC" because I am "hyperscale" or "cloud" or "AI." The underlying message is one I get; GPUs are trending in a direction that favors massive gains in lower-precision computation over FP64 performance. And the cost of HBM is driving the overall value (in FP64 FLOPS per dollar) of accelerators backwards for the first time in the history of scientific computing. But the thesis that the scientific computing community needs to be sneaky to influence the hyperscale or AI players seemed way off the mark to me. What seemed absent was the recognition that many of the "hyperscalers" are her former coworkers and remain her colleagues, and "they" sit in the same audiences at the same conferences and share the same stages as the "HPC" community. All that is true because "HPC" is not somehow different than "cloud" or "AI" or "hyperscale." If there really is a desire to influence the hyperscale and AI industry, the first step should be to internalize that there is no "us" and "them." Closing keynotes on the future Just as the conference was opened with a talk about this "us versus them" mentality, it was closed with a talk about "us versus them" in a keynote session titled, "Reinventing HPC with Specialized Architectures and New Applications Workflows" which had two speakers followed by Q&amp;A. Chiplets for modular HPC John Shalf gave one half of the closing keynote, where he gave his usual rally for investments in chiplets and specialized processors for HPC: He gives a variant of this talk at every ISC, but this year he lasered in on this notion that the "HPC" community needs to do what the "hyperscalers" do and use chiplets to develop custom ASICs. It was an energetic and impassioned talk, but this notion that hyperscalers are already executing on his idea for the future sounded a little funny to me seeing as how I now work for one of these hyperscalers and his message didn't resonate. If you really follow the money, as Shalf suggested, a huge amount of it is flowing into GPUs, not specialized processors. It wasn't clear to me what specialization he was thinking of when he referred to custom silicon being developed by the likes of Meta, Google, AWS, and Microsoft; it's true that these companies are developing their own silicon, but those efforts are largely addressing cost, risk, and supply, not improving performance beyond more general-purpose silicon like GPUs. And it turns out that a significant fraction of the (non-US) HPC community is already developing custom silicon for the same reasons as the hyperscalers; Japan, China, and Europe are all developing their own indigenous processors or accelerators for scientific computing at leadership scales. In that sense, Shalf was preaching to the choir given that, on the international stage, his government is the odd one out of the custom silicon game. He also suggested a dichotomy where the HPC community would either have to just (1) make every scientific problem an AI problem or (2) join this journey towards making domain-specific accelerators, ignoring the significant, unexplored runway offered by using mixed precision arithmetic in scientific applications. He called for partnering with hyperscalers, but his examples of implementing a RISC-V-based stencil accelerator and a SambaNova-based DFT processor didn't draw a clear line to the core missions of the large hyperscalers he extolled. He briefly said that partnering would benefit hyperscalers by addressing some capital cost challenges, but seeing as how the annual capital expenditures of the hyperscalers outstrips those of the US national HPC effort by orders of magnitude, I couldn't understand what the hyperscalers would stand to gain by partnering in this way. Integrating HPC, AI, and workflows Rosa Badia gave the second half of the closing keynote where she proposed ideas around complex scientific workflows and the novel requirements to support them. This talk felt a lot more familiar, as the focus was squarely on solving scientific computing challenges by connecting traditional HPC resources together in nontraditional ways using software whose focus goes beyond cranking out floating point arithmetic. As she spoke, I couldn't help but see parallels between the challenges she presented and the sort of technologies we live and breathe every day in cloud services.  For example, she showed this slide: Dr. Badia obviously wanted to make a cloud-tie in by calling this "HPC Workflows as a Service," but what I'm not sure she realized is that this model almost exactly describes platform-as-a-service frameworks that already exist in commercial clouds. For example, What she calls a "Data Catalog" is a public or private object storage account (a blob container, an S3 bucket) or a PaaS abstraction built atop themWhat she calls a "Software Catalog" is a container registry (Azure Container Registry, Amazon Elastic Container Registry) or an abstraction built atop themA "Workflow Description" is something like an AzureML pipeline or SageMaker pipelineA "Workflow Registry" is just a Github repository containing pipelinesThe "Portal" is the web UI provided by AzureML or SageMaker I don't think there's anything truly new here; the challenges she described lie in wedging these workflows into HPC infrastructure which lacks the platform features like robust identity and access management (i.e., something better than LDAP that supports more modern authentication and authorization flows and finer-grained access controls) and data management (i.e., something better than a parallel file system that depends on POSIX users, groups, and permissions and implicit trust of clients). She went on to describe a workflow data management system that reinvented a bunch of infrastructure that is already baked into commercial cloud object stores like Azure Blob and AWS S3: As she was describing the requirements for such a workflow data management layer, it struck me that what the scientific data community calls "FAIR principles" are the same basic requirements for operating in commercial environments where data may be subject to strict privacy and compliance regulations. The notion of findable data may be aspirational for scientific datasets, but when a company is having to find datasets because it's being sued or subpoenaed, findability is a bare-minimum requirement for any data management system. Similarly, tracking the provenance of data may be a nice-to-have for scientific data, but it is a hard requirement when establishing a secure software supply chain. Cloud storage systems solved many of these challenges a long time ago, and I can't help but wonder if this idea that workflows in HPC pose a new set of challenges is another manifestation of "us" not realizing "they" might have done something useful and applicable for science. Badia's final slide had a particularly poignant statement which read, "Systems can only be justified if we have applications that need them." I think she was trying to call for more investment in application development to exploit new systems, but I think the inverse is also true. If modern scientific applications truly require more complex orchestration of compute and data, maybe the scientific computing community should stop building computing platforms that make it really difficult to integrate different systems. Again, "HPC" is not the opposite of "cloud;" it's not an either/or decision. There are technologies and tools that were designed from the beginning to simplify the secure connection of services and resources; they just weren't invented by the HPC community. Top500 and Aurora One of the cornerstones of ISC is the semiannual release of the Top500 list, and unlike at SC, the Top500 announcements and awards do not overlap with any other sessions, so it tends to have a higher profile and draw all attendees. This go-around, there were no dramatic changes in the Top 10; the new Alps system at CSCS was the only new entry, and the order of the top five systems remained the same. Notably, though, Aurora posted a significantly higher score than at SC'23 and broke through the exaflops barrier using 87% of the system, cementing its place as the second exascale system listed. But let's start at the top. #1 - Frontier Frontier at Oak Ridge remained #1, but it squeezed twelve more petaflops out of the same node count and is now just over 1.2 EF. Nothing groundbreaking, but it's clear evidence that ORNL is continuing to tune the performance of Frontier at full system scale. #2 - Aurora Aurora, on the other hand, finally eked over the exaflops line with 1.012 EF using 87% of the system's total 63,744 GPUs. Rick Stevens gave a short talk about the achievement which is summed up on this slide: I was a little surprised by how honest Stevens was in this talk; the typical game that is played is that you stand up on stage, talk about how great of a partnership you had with your partners to realize this achievement, extol the virtues of the technologies on which your system was built, and talk about how this HPL score is just the start of a lot of great science. Stevens didn't do that though. He started out by telling the conference that Intel had bad product names, then explained that their low Graph500 and HPCG scores were the result of their exclusive focus on breaking the exaflops barrier with HPL, implying they didn't have time or ability to run Graph500 or HPCG at the same 87%-89% scale as their HPL and HPL-MxP runs. Based on this, it sounds like Aurora is still a ways away from being stable at scale, and we're unlikely to see any Gordon Bell-nominated papers at SC'24 this November. After this session, folks seemed to relish in dunking on Aurora; its window to be #1 is likely to have closed and it has some power efficiency issues. But I don't think anyone involved with the Aurora project needs to be told that; if what Stevens implied is true, the folks at ALCF, Intel, and HPE have been struggling for a long time now, and topping out over 1018 was a hard-sought, major milestone to be celebrated. The Aurora project has been thrown more curveballs than I would have ever guessed a single HPC project could have, so all parties deserve credit for sticking it through all this way rather than just walking away. With any luck, Aurora will stabilize in the next six months, and we'll see full-scale runs of Top500, Graph500, HPCG, and science apps by November. #3 - Eagle The third highest system on the list was Eagle, whose HPL score was not updated since the system was first listed at SC'23 last year. Through a few twists of fate, I wound up being the person who accepted the award on-stage, and I now have a Top500 award for the #3 system sitting in my home office. Here's a photo of me goofing around with it: It's not entirely inappropriate that I was the one to accept it since my teammates are the ones carrying pagers for the on-call rotation of that system, and we were also the hands-on-keyboard when that HPL run was conducted. Still, it was a bit surreal to walk on-stage to pick up such a noteworthy award immediately following two actually important people (both of whom have "director" in their titles) accepting the same award. By comparison, most of my career highlights to date have been just trolling HPC people on Twitter (as the esteemed Horst Simon actually said out loud as I was leaving the stage!) It was weird. That said, I take this to mean that it is now my duty to be the friendly face from Microsoft who can speak intelligently about the #3 system on Top500. To that end, I'll answer some questions that I was asked at ISC about the system and Azure HPC clusters in general below. None of this is new or secret information! Why didn't you run HPL again and post a higher score to beat Aurora? Because the day after that HPL run completed, that system was put into production. Once systems are in production, people are paying to use them, and taking a time-out to re-run HPL costs a ton of money in either real dollars (if a customer runs it) or lost revenue (if the HPL run is blocking customer workloads). This is quite different from public-sector HPC systems which never have to pay for themselves.Can I get access to Eagle for a Gordon Bell run or to test software? That's not really how it works. Whereas a traditional supercomputer might allow users to ssh in and submit jobs to a Slurm queue, cloud-based supercomputers allow users to deploy virtual machines through a REST API. Those virtual machines can allow ssh, run Slurm, and support MPI jobs like HPL, but that OS environment is managed by Azure users, not Azure itself. You can get a taste for what's required to run a basic MPI job by reading some instructions I wrote on provisioning an MPI cluster on Azure.Is it just a bunch of GPU nodes scattered around a bunch of data centers? No, all the nodes on any given Azure HPC cluster (like Eagle) share an InfiniBand fabric. There are countless InfiniBand clusters in Azure, but each one is a real supercomputer by any definition of a supercomputer, and they are designed to run tightly coupled job across all their GPUs.What parallel file system does it use? Don't think about it that way. You can provision a Lustre file system and mount that to any or all cluster nodes if you want to, or you can access data directly from object storage.Are there any photos of it? You can see a photo of one of the Microsoft-designed nodes that comprise the system on my SC'23 recap blog post. Beyond that, there's not much to look at because Azure HPC clusters are not meant to be photogenic like, say, Cray supercomputers. There's no rack graphics (or even rack doors!). It's just tons and tons of air-cooled racks with InfiniBand optics coming out of each one. Maybe the only unique thing is that the racks are painted white instead of the typical black. Not sure why. Getting back to that false separation between "HPC" and "cloud," Eagle is strong evidence that they aren't different. What the "hyperscalers" do is not that different from what traditional HPC centers do. Perhaps the biggest difference is that cloud supercomputers get all the benefits of cloud infrastructure like software-defined infrastructure like virtual machines and virtual networking, integration with identity and access management that transcends simple Linux UIDs/GIDs, and the flexibility to integrate with whatever storage systems or ancillary services you want from any compute node. Other notable tidbits It is tradition for Erich Strohmaier to talk through some highlights and trends of the latest Top500 list every time a new one is announced, and in the past, I've been critical of how he's presented conclusions from the list with this implicit assumption that computers that never post to Top500 simply don't exist. This year felt different, because Dr. Strohmaier made the explicit statement that China has completely stopped submitting to Top500. Their exascale systems aren't listed, but neither are any new systems in the past three years at the bottom. They simply don't play the game anymore, making it undeniable that Top500 is no longer an authoritative list. Just as the whole conference's theme was reinventing HPC, I felt a sense that even the most stalwart proponents of Top500 are now recognizing the need to reinvent the Top500 list. Kathy Yelick said as much during her keynote ("Shall we replace Top500? What are the metrics in post-exascale computing that are important?"), and Erich implored the audience to help expand the HPL-MxP (formerly HPL-AI; an HPL-like benchmark that can use the mixed-precision capabilities of tensor cores) list. Nobody seems to know how to quantify what makes a leadership supercomputer nowadays, but accepting that HPL scores (or appearing on the Top500 list!) won't cut it is a good first step. That all said, Top500 is still a valuable way to track technology trends in the industry. For example, this edition of the list where NVIDIA's new Grace-Hopper node started appearing in force. The only new entrant in the Top 10 was the 270 PF GH200 component of CSCS's Alps system, and HPEhad these EX254n GH200 blades on display on the show floor. To HPE/Cray's credit, they seem to have gotten the system up and running with Slingshot without the delays that plagued early Cray EX systems like Frontier and Aurora. Hopefully this is a sign that the Cray EX platform and Slingshot-11 have graduated from being risky and not-quite-production-ready. The other notable entrants on this year's Top500 are a trio of early MI300A APU-based Cray systems being built around the El Capitan program at Lawrence Livermore National Laboratory. This is a positive sign that MI300A is up and running at modest scale, and HPE also had one of these EX255a blades on display at their booth: The strong showing of MI300A suggests that we may see El Capitan take the top spot in the next edition of the Top500 list coming in November. Everyone is an AI expert! Since I now work on a team responsible for AI infrastructure, I tried attending as many of the AI-focused talks and panels as I could this year. Unsurprisingly, these sessions largely carried the same undertones of "reinventing HPC," and speakers opined on how AI would affect scientific computing and offered examples of what their institutions were doing to extend their leadership in the HPC space into the AI space. There was a fair amount of grasping going on (as there always is when AI is discussed at non-AI conferences), but this year I was struck by how confused so many speakers and attendees were about concepts related to applying AI. To be clear: I am no expert in AI. However, my day job requires that I be steeped in some of the largest AI training workloads on the largest AI supercomputers on the planet, and I have to have a cursory understanding of the latest model architectures and techniques to anticipate how future system designs will have to evolve. It's from this perspective that I made the following observation: there are a lot of HPC people speaking very confidently about AI based on an outdated understanding of the state of the art. The AI industry generally moves much faster than the government-funded research community, and I couldn't help but wonder if some community leaders assumed that the AI industry today is the same as it was the last time they wrote their AI grant proposal. Of course, there were also some really insightful perspectives on AI for science shared as well. Let's talk through some examples of both. The Exascale AI Synergies LLM Workflows BOF This realization that the ISC community is not keeping up with the AI community first slapped me in the face when I ducked into a BOF session titled, "Tales of Exascales – AI and HPC Supercomputing Platforms Synergies for Large Language Models (LLMs) and Scientific Workflows." I sometimes wonder if the organizers who propose titles like that are intentionally creating word salad, but in this case, it was apt session name; the discourse around HPC and AI was all over the board throughout the hour. The session started on a strong, positive note by Simon McIntosh-Smith describing Bristol's new Isambard-AI system, a GH200-based Cray supercomputer funded under the broad charge of "AI research." While I'm usually skeptical of such nebulously defined "AI research" machines, Dr. McIntosh-Smith's description of the project quickly checked a bunch of boxes on how a real AI research platform should be developed. In particular, Isambard-AI was developed and deployed at the pace of AI rather than HPC for scientific computing. Whereas government-funded, large-scale HPC systems typically take years to procure, Simon said that the first discussions started in August 2023, and in the nine months that followed, they had built the site, the team, and the system itself to the degree that a piece of the final system is already on Top500. By comparison, LLNL's El Capitan supercomputer also debuted on Top500 this month, but its contract was signed five years ago, and its procurement began at least two years before that. The AI industry would not exist if the systems it trains on took seven years to procure. Isambard-AI deliberately avoided exotic AI accelerators to remain future-proof. Simon rightly pointed out that the AI industry moves too quickly to anticipate whether a bespoke AI accelerator would even be relevant to whatever the hottest model architecture will be in a year. GPUs were chosen because they are the most flexible way to accelerate the widest range of AI workloads, regardless of if they are dense models, sparse models, inferencing, training, and whatever level of quantization makes sense. The reality is that cutting-edge research is done on GPUs, so aligning an AI supercomputer on the same technology will ensure that the algorithms developed by industry are immediately usable for scientific research. A reasonable definition of "AI for science" was defined from the outset. Rather than blurting out "we need to research AI!" and asking for a sack of money to buy GPUs, Simon outlined a vision of training AI models using data generated by physical simulation on a more conventional HPC system. Training models on models to create surrogate models is not particularly new, but it does establish a few reasonable architectural decisions such as having a robust data management and sharing platform, close coupling to the HPC system performing simulation, and aligning software stacks and programming environments as closely as possible. Simon's contribution to the discussion stood out to me as the most impressive, and the discourse seemed to fall into a trap of familiarity following. Rather than focusing on the new and exciting prospects of AI, some panelists and audience members wanted to focus on the aspects of AI they understood. For example, an uncomfortable time was spent on a back-and-forth on how HPC centers can support Kubernetes and random I/O (which is what defines AI vs. HPC?) instead of Slurm and Lustre. If your biggest challenge in delivering infrastructure to support AI workloads is figuring out how to deploy both Kubernetes and Slurm, you haven’t even reached the starting line. This is a trivial issue in cloud environments, where entire AI clusters can be built up and torn down in minutes. Again, this is evidence that the scientific computing community isn’t ready to keep pace with the AI industry. I jotted down a few of the questions and comments that I heard during this BOF that seem to reflect the level of familiarity the average ISC attendee has with AI: "Would be nice if there were more models for science." I wasn't sure sure what this means. All the leading LLMs are pretty good at "science," and domain-specific models aren't readily transferable between different science domains or problems.Scientific problems "have to validate outputs for correctness, unlike LLMs." I think the speaker was making a sidelong reference to hallucinations, but like with any model (large language or physics-based), validating outputs for correctness is certainly necessary and readily possible."The demands of inference of LLMs are completely different from those for training. How do you buy inference infrastructure?" I wonder where this notion came from. If your infrastructure can train a model, it can definitely inference that model. Cost-optimizing infrastructure for inferencing is a separate matter (you can cut corners for inferencing that you wouldn't want to cut for training), as is building the service infrastructure around inferencing to deliver inferencing as a service. But I don't think that's what this question was about."Working safely with sensitive data / isolating workloads on big shared clusters." This is a problem that arises only when you try to wedge AI workloads into infrastructure designed for traditional physics-based simulation. If you have sensitive data, don't use big shared clusters. Provision separate clusters for each security domain on a shared, zero-trust infrastructure."How different are the files and filesystem access while training for LLMs, image generation models, reinforcement learning?" This question reflects a general misunderstanding of data and storage in HPC overall; how data is organized into files and how that data is accessed by a workload is an arbitrary decision made by the application developer. You can organize piles of text into one giant file or a million little files. There were a few questions that came up that touched on deeper issues on which the HPC community should reflect: "What are the first steps for scientific groups wanting to get ready for using AI in the future?" This is probably the purest question raised in the entire session, and I think this is something the scientific computing community as a whole needs to figure out. What does "using AI" really mean for scientific groups? Is it training models? Fine-tuning models? Inferencing using pre-trained models on HPC infrastructure? Is it integrating simulation applications with separately managed inferencing services? Who manages those inferencing services? Does inferencing even require HPC resources, or can suitable models run on a few CPU cores? I think the first step to answering this question is ensuring that the scientific computing community reaches a common baseline level of understanding of "using AI" means. And a lot of that probably means ignoring what some self-professed AI experts in the HPC community claim is the future."Care to predict what that ChatGPT moment will be for AI for Science? Had it already happened?" This question was addressed directly by panelist Séverine Habert who rightly pointed out that the ChatGPT moment occurred when a complex and esoteric topic was suddenly put in the hands of hundreds of millions of laypeople across the world. It was the moment that the common person walking on the street could suddenly interact with the most cutting-edge technology that had been previously understandable only to the headiest of researchers in industry and academia. That will likely never happen in AI for science because science, by definition, requires a higher baseline of education and understanding than the average layperson has."How to effectively train the existing workforce when we are already struggling to retain talent in research/academia?" This question strikes at the same theme that Kathy Yelick's opening keynote confronted: what is the role of the scientific computing community now that it turns out that you don't need decades of institutional experience to deploy and use HPC resources at leadership scale? As offensive as it may sound, perhaps the public-sector HPC community should accept that their role is not training future researchers and academics, but training future practitioners of AI in industry. This is how the wider tech industry generally works; neither startups nor tech giants make hires assuming those people will still be around in ten years. Why does the public-sector HPC industry think otherwise? Finally, I was also struck but how fiercely the discourse clung to the idea that large language models are the answer to all AI problems in science. I get that this panel was focused on exascale, and LLM training is one of the rare cases where AI requires exascale computing capabilities. But there was no acknowledgment that trillion-parameter models are not actually a good idea for most scientific applications. AI Systems for Science and Zettascale This singular focus on creating massive LLMs for science was front-and-center in a talk given by Rick Stevens titled "The Decade Ahead: Building Frontier AI Systems for Science and the Path to Zettascale." The overall thesis that I heard was something like... Science needs its own trillion-parameter foundation modelsTraining trillion-parameter foundation models requires a lot of GPUsWe need $25 billion from the U.S. governmentHowever, Stevens never answered a very basic question: what does a foundation model for science do that any other foundation model cannot do? He showed slides like this which really don't sound like foundation models for science as much as a generic AI assistants: Is the scientific computing HPC community really the most qualified bunch to reinvent what existing foundation models like GPT-4 or Claude 3 have already done? Even if you argue that these proprietary models aren't as good at "science" as they could be, who would have a better chance of addressing this with a billion dollars of federal funding: the companies who developed GPT or Claude, or a collection of government scientists starting from scratch? I think the answer to this question was in other parts of Stevens' talk. For example, he started with this slide: While robust requirements are good when there's no urgency, this slide is also a tacit admission that the government takes years to general a perspective on AI. Do you think the creators of Llama-3 or Mistral Large gathered wide community input from over 1,300 researchers before deciding to build a supercomputer and train a model? Even if science needs its own foundation models, this slide is strong evidence that, by the time the scientific HPC community agrees on a path forward, that path will be years out of date relative to what the commercial AI industry is doing. A great example of this already happening is the basic premise that creating a foundation model with a trillion parameters is the best way to apply AI to solve science problems. This certainly was the leading thought two years ago, when transformer scaling laws were published that suggested that the best way to get better-performing LLMs was to simply add more parameters to your transformer and train on more data. But there's a reason all the leading models have stopped advertising how many parameters they use. Dealing with massive transformers is really expensive. They're not only really expensive to train, but they're really expensive to use for inferencing too. This has led to a bunch of innovation to develop model architectures and approaches to training that result in dramatically higher quality outputs from a fixed parameter count. Dense transformer architectures with a trillion parameters have become the blunt instrument in developing foundation models since 2022, so it took me by surprise to hear Stevens put so much stock into this notion that the need for a trillion-parameter model is essential for science. To repeat myself, I am no expert in AI. I've never been called in front of Congress to talk about AI or been invited to give talks on the topic at ISC. There might be something basic that I am missing here. But when I look at the science drivers for AI: I know that you do not need to train your own trillion-parameter model to do most of this stuff. Even the use cases that do require generative AI, like code generation and math theory, don't actually require trillions of parameters. Small language models, such as that described in Textbooks Are All You Need (published in 2023, after the reports Stevens cited in his talk), can produce amazing results with very small models when you train them using high-quality data instead of garbage from Reddit. And when you create or fine-tune a small language model for a specific science domain, not only do you save yourself from having to buy a billion-dollar supercomputer for training, but you get a model that is much more accessible to scientists around the world because they won't need a million dollars' worth of GPUs to inference with it. So, if there's one question that was never answered across any of the AI-themed sessions at ISC this year, it is this: Why does science need to train its own large language models? My intuition is that either fine-tuning existing large language models or training small language models for domain-specific applications, would be a better investment in actually advancing science. However, if we cynically assume the real goal of LLMs-for-science is to justify buying massive GPU systems, suddenly a lot of the talks given at ISC on this topic make a lot more sense. Real applications of generative AI for science As frustrated as I got sitting through sessions on AI where it sometimes felt like the blind leading the blind, there was one really good session on actual applications of generative AI for science. Mohamed Wahib of RIKEN gave an insightful presentation on the unique challenges of using generative AI in science. His summary slide touched on a lot of the key challenges: And his actual talk focused largely on the model and data aspects of generative AI. What struck me is that the challenges he described reflected the experience of someone who has actually tried to do what many other AI experts at the conference were claiming would be the future. For example, He recognized the importance of training scientific models with high-quality datasets, not just garbage scraped off of social media. This means not only scraping or generating high quality data for training, but curating and attributing that data and applying reinforcement learning with human feedback as the model is being trained. This is uniquely challenging when creating models for scientific applications, as managing the quality of scientific data requires deep domain expertise. This contrasts with a generic chat bot whose inputs and outputs can often be assessed by anyone with a basic education.He also talked about the tendency of scientific data to be highly multimodal and multidimensional. Whereas multimodal chatbots may combine text and vision, scientific data often contains observations of the same phenomenon from many different sensors (for example, pressure, temperature, density, strain fields, ...), and the output of a generative model for science may require multiple modalities as well.  These capabilities are not well developed in LLMs designed for human language.Dr. Wahib also pointed out that scientific datasets tend to be huge compared to text and images, and this may require developing ways for models to have context windows can fit multi-petabyte datasets' tokens to identify long-range correlations. Relatedly, he also pointed out that tokenization of scientific data is a new set of challenges unique to this community, since industry has been focused on tokenizing low-dimensional data such as text, audio, and images. The good news is that industry's quest towards both commercializing generative AI and achieving AGI will touch on some of these challenges soon. For example, training domain-specific models using high-quality datasets is an essential component of the small language models I described in the previous section, and these small language models are what will enable privacy-preserving and cost-effective generative AI on laptops and phones. Effectively infinite context windows are also a major hurdle on the path to AGI, as industry is hard at work developing AI agents that can remember every conversation you've ever had with them. Finding more scalable approaches to attention that do not sacrifice accuracy are a part of this. François Lanusse, currently at the Flatiron Institute, also gave a nice presentation that clearly explained how generative AI can be used to solve inverse problems—that is, figuring out the causes or conditions that resulted in a collection of measurements. A precise example he used applied generative AI to figure out what an image distorted by gravitational lensing might look like in the absence of those distortions. As I understood it, he trained a diffusion model to understand the relationship between images that are affected by gravitational lensing and the masses that cause lensing through simulation. He then used that model instead of an oversimplified Gaussian model as part of a larger method to solve the inverse problem of un-distorting the image. The details of exactly what he did were a little over my head, but the insight piece for me is that combining generative AI and science in practice is not as straightforward as asking ChatGPT what the undistorted version of a telescope image is. Rather, almost all of the standard, science-informed approach to solving the inverse problem remained the same; the role of generative AI was simply to replace an oversimplified part of the iterative process (the Annealed Hamiltonian Monte Carlo method) to help it converge on better answers. It really is a combination of simulation and AI, rather than an outright substitution or surrogate model. Dr. Lanusse also showed this slide which demonstrated how this approach can be generalized to other scientific domains: The general approach of pretraining, fine-tuning ("adapt"), and combining foundation models with other physics-based models seems reasonable, although I admit I have a difficult time wrapping my head around exactly how broadly scoped he envisions any given pretrained foundation model to be. I can see such a model trained on extensive sky survey data being useful for a number of astrophysical and cosmological tasks, but it's less clear to me how such a model might be useful in unrelated domains like, say, genomics. You might also ask why I think this vision of foundation models for science is reasonable while Rick Stevens' vision didn't ring true; the difference is in scale! The foundation models cited on Lanusse's slide are vision transformers which have many orders of magnitude fewer parameters than the trillion-parameter models that others talk about. Whereas a trillion-parameter model might need to be distributed over dozens of H100 GPUs just to produce one inference result, the largest of the vision transformers can probably be squeezed on to a single high-end desktop GPU. Again, you don't need billion-dollar supercomputers to train these models for science. Frank Noé from Microsoft Research then talked about how generative AI can be applied to solve problems in simulating biological systems. Like the talk before his, Dr. Noé followed this pattern where a larger, physics-based framework had one statistical technique replaced by a method based on generative AI, and then a physics-based model is used to quantify the likelihood that the result is reasonable. He contrasted this with convention approaches (to, say, protein folding) where you just simulate for really long times in the hopes that your simulation randomly wanders into a situation where you capture a rare event. His talk wasn't about generative AI as much as the previous speakers, but he offered a litany of ways in which AI models can be useful to molecular modeling: Markov state models provide a statistical framework that lets you replace one long simulation (that hopefully captures every possible scenario) with a bunch of short, chopped-up simulations that hopefully capture every possible in parallel. He cited an example that took 20,000 GPU-days on V100 GPUs that would've otherwise taken a million GPU-years if done in one long simulation.Coarse-grained models use machine learning to develop surrogate models to simulate the physics of relatively uninteresting parts of molecular systems. The example he used was simulating the water molecules surrounding a biomolecule; water can be very difficult to accurately model, and the example he cited led to a surrogate model that was 100x faster than directly simulating water molecules.Boltzmann generators can generate 3D molecular structures based on a known probability distribution defined by the energy states of the system. This is another fast way to find rare but stable molecular configurations without having to throw darts at a dartboard. What struck me is that, in all these cases, the AI model is never generating results that are blindly trusted. Instead, they generate molecular configurations which are then fed into physics-based models which can quantify how likely they are to be valid. Both Lanusse's and Noé's examples of combining AI and simulation painted a picture to me where generative AI can be really useful in solving problems where a researcher would otherwise have to make educated guesses about what physical phenomenon is really happening based on incomplete information. So long as there is a way to apply a physics-based model to check the accuracy of each guess, generative AI can be trained to predict the relationships between incomplete information and what's really going on and get to probable answers much faster than relying on physics alone. More broadly, I couldn't help but think about the Sora video showing pirate ships battling in a cup of coffee as I left this session. Like that video, these talks demonstrated that it's possible to train generative AI models to reproduce physical phenomena (like the fluid dynamics of coffee) without explicitly embedding any laws of physics (like the Navier-Stokes equations) into the model itself and still get really compelling results. The part of this that was lacking from the Sora video—but was present in these talks—was closing the loop between generated results and the laws of physics by feeding those generated results back into the laws of physics to figure out if they are probable. High Performance Software Foundation ISC'24 wasn't all about AI though! I wound up attending the launch of the High Performance Software Foundation (HPSF), a new Linux Foundation effort spearheaded by Todd Gamblin and Christian Trott (from Livermore and Sandia, respectively) aimed to promote the sustainability of the software packages relied upon within the high-performance computing community. I haven't paid close attention to HPC software in a long time since most of my work was in platform architecture and storage systems, so a lot of the background context remains a little murky to me. That said, it seems like HPSF was formed to be like the Cloud Native Computing Foundation for the HPC community in that: it will serve as a neutral home for software projects that aren't tied to any single university or government institutionit provides mechanisms to ensure that critical HPC software can continue to be maintained if its original author gets hit by a busit will help with the marketing, promotion, and marketing of HPC software Its governance seems pretty reasonable, with different levels of membership being accompanied by different levels of rights and obligations:   There is a Governing Board is comprised of paying members (and predominantly those who pay the most), while the Technical Advisory Council carries out the more technical tasks of forming working groups and onboarding projects. There are three levels of membership, and the highest (premier) has a $175,000 per year buy-in and comes with a seat on the Governing Board. Right now, the founding seats are held by AWS, HPE, LLNL, and Sandia. Below that is a general membership tier whose cost is on a sliding scale based on the organization size, and AMD, Intel, NVIDIA, Kitware, ORNL, LANL, and Argonne have all committed at this level.  The associate tier is below that, and it is free to nonprofits but comes with no voting rights. It seemed like the exact functions that HPSF will have beyond this governing structure are not fully baked yet, though there were six "prospective" working groups that provide a general scope of what the HPSF will be doing: My read of the description of these working groups is that CI/testing will supply resources (GPUs) on which HPSF projects' code can be automatically tested.Software stacks will maintain E4S.User engagement sounds like it will figure out what users of HPSF projects' software are looking for. It sounds like this will provide some product management-like support for projects.Facility engagement is probably like user engagement, but for the sites deploying code on behalf of their users. Again, this sounds like product management functions.Security sounded like stewarding SBOM-like stuff for member projects' software.Benchmarking would make a framework for benchmarking HPC applications. That all said, it still wasn't clear what exactly HPSF would do; what would all those membership dues go towards supporting? Based on some Q&amp;A during this BOF and follow-up afterwards, I pieced together the following: HPSF will not be funding developers, much in the same way that OpenSFS doesn't fund Lustre development. That said, Todd Gamblin later said that not funding software development was a financial constraint more than a policy one, with the implication that if more members join, there may be opportunity for HPSF to fund projects.HPSF likely will be hosting events and conferences (perhaps like the CNCF hosts KubeCon), providing scholarships, developing and providing training related to member projects, and "increasing collaboration" (whatever that may mean!). HPSF also has some influence and ownership over its member projects: HPSF will co-own its projects' GitHub repos to ensure continuity in case the other repo owner abandons it.HPSF will own the domain for the project for the same reasons as above.Member projects still manage their own software development, roadmaps, releases, and the like. The HPSF won't dictate the technical direction of projects.HPSF will own the trademark and logos of its member projects so it can prevent corporations from profiting off of repackaging products without respecting trademark. This establishes an interesting new direction for the sorts of software projects that are likely to become member projects. Historically, such projects developed by the member organizations (i.e., DOE labs) have been wholly controlled by the labs that funded the work, and those software projects lived and died at the whims of the government funding. The HPSF offers a new vehicle for software projects to live on beyond the end of the grants that created them, but at the same time, it requires that the DOE surrender control of the work that it sponsored. I left the session still wondering a few pretty major things, likely borne out of my own ignorance of how similar organizations (like CNCF or the Apache Foundation) work: How does a software project actually become a member project? The HPSF folks said that the Technical Advisory Committee onboards new projects, but what is the bar if I have an open-source project used by the community that I no longer want to maintain myself? I assume it's not a pay-to-play arrangement since that defeats the purpose of sustaining software after its seed funding runs out.What do stakeholders actually get out of joining HPSF? I see obvious value for organizations (like the DOE labs) who develop open-source software but may not want to be exclusively responsible for sustaining it forever. But would an HPC facility get any obvious benefit from joining and paying dues if it is simply a consumer of member projects' software? What does a cloud vendor like AWS get by being a premiere member? Is HPSF just a way to get someone else to cover the overheads of maintaining open-source software that comes out of, say, R&amp;D organizations rather than product organizations? Hopefully the answers to these questions become clearer as the foundation gets off the ground and we get to see what member organizations contribute under the HPSF banner. Ultimately though, I see this as a really positive direction for the HPC software community that might help resolve some uncertainty around key pieces of HPC software that have uncertain ownership. For example, I wound up as a maintainer of the IOR and mdtest benchmark because I was the last one to touch it when its previous maintainer lost interest/funding. I don't even work in I/O performance anymore, but the community still uses this benchmark in virtually every procurement of parallel file systems either directly or through IO500. It would be wonderful if such an important tool didn't rest on my shoulders and had a more concrete governance structure given how important it is. Quantum computing Besides AI and cloud, quantum computing was cited in Kathy Yelick's opening keynote as the third disruptor to HPC for scientific computing. At the time, I thought citing quantum was just an obligation of any opening keynote speaker, but quantum computing was particularly high-profile at ISC this year. I was surprised to see over a dozen quantum computing companies on the vendor exhibition floor, many of whom were Europe-based startups. In addition, this year's Hans Meuer award (for best research paper) was given to a paper on quantum computing by Camps et al. This is particularly notable since this is the first time that the Meuer award has ever been given to a paper on a topic that isn't some hardcore traditional HPC like MPI or OpenMP advancements; by comparison, this award has never been given to any papers on AI topics. Granted, the winning paper was specifically about how to use conventional HPC to solve quantum problems, but this recognition of research in quantum computing makes a powerful statement: quantum computing research is high-performance computing research. Reinvent HPC to include urgent computing? I was invited to give a lightning talk at the Workshop on Interactive and Urgent High-Performance Computing on Thursday, and urgent/interactive HPC is not something I'd really paid attention to in the past. So as not to sound like an ignorant fool going into that workshop, I opted to sit in on a focus session titled "Urgent Computing" on Tuesday. I had two goals: Make sure I understood the HPC problems that fall under urgent and interactive computing so I could hold an intelligent conversation on this topic at the Thursday workshop, andSee if there are any opportunities for cloud HPC to provide unique value to the challenges faced by folks working in urgent HPC I'll describe what I came away with through these lenses. The Urgent Computing focus session What I learned from the focus session is that urgent computing is not a very well-defined set of application areas and challenges. Rather, it's another manifestation of reinventing HPC to include any kind of computation for scientific purposes. Much to my surprise, this "Urgent Computing" focus session was actually a session on IoT and edge computing for science. Several speakers spoke about getting data from edge sensors on drones or telephone poles into some centralized location for lightweight data analysis, and the "urgent" part of the problem came from the hypothetical use cases of analyzing this sensor data to respond to natural disasters. There wasn't much mention of anything requiring HPC-like computing resources; at best, a few talks made unclear references to using AI models for data analysis, but it felt like grasping: The above conclusion slide was presented by one of the speakers, and to be honest, I don't understand what any of it means. Granted, I know very little about urgent computing, IoT, or edge computing so there may be some domain jargon here that's throwing me off. But based on this, as someone working in the area of HPC and AI in the cloud, I don't think I have a role to play here. I'm sure cloud computing can help, but the challenges would be in general-purpose cloud rather than HPC. The Interactive and Urgent HPC workshop Fortunately for me, the Thursday workshop on Interactive and Urgent HPC was much less about edge/IoT and more about developing software infrastructure and workflows that allow scientific data analysis of large datasets to happen before the results become obsolete. It was a fascinating workshop for learning about specific science drivers that require fast access to HPC resources, and how different HPC providers are enabling that through non-traditional services and policies. Below are a few highlights. Sam Welborn (NERSC) presented his team's efforts to convert a streaming data workflow from its current file-based approach into one that streamed directly into compute node memory. The specific use case was the initial data processing for image information coming off of a scanning transmission electron microscope at 480 Gbps, totaling 750 GB per shot. As he described it, the current technique involves streaming those data to files at the microscope, then copying those files to the parallel file system of a remote supercomputer, then reading, processing, and writing that data within the HPC environment to prepare it for downstream analysis tasks. And for what it's worth, this is how I've always seen "streaming" HPC workflows actually work; they're actually using file transfers, and the performance of both the file system at the source and destination are in the critical path. The problem with this approach is that parallel file systems on HPC systems tend to be super flaky, and there's no real reason to bounce data through a storage system if you're just going to pick it up and process it. So, Dr. Welborn showed a true streaming workflow that skipped this file step and used ZeroMQ push sockets at the microscope and pull sockets on the HPC compute nodes to do a direct memory-to-memory transfer: Seeing software like ZeroMQ used to enable communication in an HPC environment instead of forcing this workflow to fit into the MPI paradigm is an encouraging sign in my eyes. ZeroMQ, despite not using purpose-built HPC technology like RDMA, is the right tool for this sort of job since it supports much better resilience characteristics than messaging libraries designed for tightly coupled HPC jobs. Workflows like this that combine beefy GPU nodes with software developed in the commercial tech space suggest that the world of HPC is willing to abandon not-invented-here ideology. It wasn't clear to me that there's a great opportunity for cloud HPC to be uniquely useful in use cases like this; while you certainly can provision beefy CPU and GPU nodes with InfiniBand in Azure, cloud services can't obviously simplify this ZeroMQ-based workflow beyond just supplying general-purpose VMs on which the orchestration services can run. Had this team stuck with a file-based streaming mechanism, the performance SLAs on cloud storage (like object or ephemeral Lustre) would provide a more reliable experience to ensure the data transfer happened in near-real-time. But the better solution to unpredictable file system performance is to do exactly what was done here: skip the file system entirely. Just to keep the speaker honest, I asked why this computation couldn't simply be done at the same place as the telescope generating the data. After all, if the telescope always generates 750 GB per shot, you should be able to buy a couple GPU servers that are ideally sized to process that exact workload in the time between images. There were actually two answers: one from Sam and one from an audience member: Sam said that you can process this workflow locally, but that the goal of this work was to prepare for a future microscope (or another instrument) that could not. He also insightfully pointed out that there's tremendous value in getting the data into the HPC environment because of all the services that can be used to work on that data later. I envisioned doing things like using a Jupyter notebook to further process the data, serve it up through a web UI, and similar tasks that cannot be done if the data is stuck inside a microscope room.An audience member also pointed out that sticking GPU nodes in the same room as electron microscopes can result in enough noise and vibration to disrupt the actual scope. This was a great point! In the days before I started working in HPC, I was training to become an electron microscopist, and I worked in a lab where we had water-cooled walls to avoid the problems that would be caused by air conditioning breezes. There's no way a loud server would've worked in there. Toshio Endo (Tokyo Tech) gave an interesting talk on how they enable urgent/interactive compute jobs on their batch-scheduled TSUBAME4.0 supercomputer by doing, frankly, unnatural things. Rather than holding aside some nodes for interactive use as is common practice, his work found that a lot of user jobs do not completely use all resources on each compute node they reserve: I had to do a double-take when I saw this: even though 65%-80% of the nodes on the supercomputer were allocated to user jobs, less than 7% of the GPUs were actually being utilized. Dr. Endo's hypothesis was that if nodes were suitably subdivided and jobs were allowed to oversubscribe CPUs, GPUs, and memory on a compute node without impacting performance too much, they could deliver real-time access to HPC resources without having to create a separate pool of nodes only for interactive uses. He defined success as the slowdown of a shared job being 1/k if k jobs shared the same node; for example, if four jobs were all running on the same node, each one taking four times as long to complete would be acceptable, but any longer would not. He then went on to show that the best way to accomplish this is using Slurm's gang scheduling, where each job takes turns having exclusive access to all the CPUs and GPUs on a node. The alternative (just letting the OS context switch) was no good. While a fascinating study in how to provide zero wait time to jobs in exchange for reduced performance, this whole mechanism of using gang scheduling to exploit low resource utilization seems like jamming a square peg into a round hole. If a workload doesn't (or can't) use all the GPUs on a node, then that's not the right node for the job; I feel like a more appealing solution would simply be to offer a heterogeneous mix of nodes based on the demands of the workload mix. This is hard to do if you're buying monolithic supercomputers since you're stuck with whatever node mix you've got for five years, but there is another way to buy supercomputers! I won't pretend like dynamically provisioning different flavors of CPU- and GPU-based nodes interconnected with InfiniBand in the cloud doesn't come with a cost; the convenience of being able to slosh a cluster makeup between CPU-heavy and GPU-heavy nodes will be more expensive than committing to use the same makeup of node flavors for multiple years. But if you're paying for GPUs that are only being used 7% of the time, surely it's cheaper to pay a higher cost for GPUs when you need them if it also allows you to not pay for them 93% of the time when they're idle. Bjoern Enders (NERSC) gave the first lightning talk where he presented the exploration they're making into enabling real-time and urgent computation. They're currently going in three parallel directions to provide this capability: Reservations, a process by which a user can request a specific number of nodes for a specific period of time, and Slurm ensures that many nodes are available for the exclusive use of that user by the time the reservation starts. He said that implementing this at NERSC is costly and rigid because it requires a human administrator to perform manual steps to register the reservation with Slurm. Realtime queues, where a few nodes are held from the regular batch queue and only special real-time users can submit jobs to them. Dr. Enders said that NERSC is extremely selective about who can access this queue for obvious reasons: if too many people use it, it will back up just like the regular batch queues do.Jupyter Hub, which utilizes job preemption and backfill under the hood. If a user requests a Jupyter job, Slurm will pre-empt a job that was submitted to a preemptible queue to satisfy the Jupyter request. However, if there are no preemptible jobs running, the Jupyter job will fail to launch after waiting for ten minutes. To provide compute resources to back up these scheduling capabilities, they also deployed a new set of compute nodes that can be dynamically attached to different supercomputers they have to support urgent workloads even during downtimes.  Called "Perlmutter on Demand" (POD), it sounded like a separate set of Cray EX racks that can be assigned to either the Perlmutter supercomputer, or if Perlmutter is down for maintenance, either their smaller Alvarez or Muller supercomputers which share the same Cray EX architecture. What wasn't clear to me is how the Slingshot fabrics of these nodes interact; perhaps POD has its own fabric, and only the control plane owning those racks are what changes. He showed a slide of explorations they're doing with this POD infrastructure, but as with Dr. Endo's talk, this seemed a bit like a square peg in a round hole: All of this sounds aligned with the strengths of what HPC in a cloud environment can deliver, and some of the big challenges (like figuring out the ideal node count to reserve for interactive jobs) are problems specific to Slurm and its mechanism for scheduling. There's a lot more flexibility to rapidly provision HPC resources in cloud environments because, unlike the case where Slurm is scheduling jobs on a single cluster, cloud resource managers can schedule across any number of clusters independently. For example, if an urgent workload needing only four GPU nodes suddenly appears, it doesn't necessarily have to be scheduled on the same InfiniBand fabric that a large hero job is running on. Since the urgent job and the hero job don't need to talk to each other, cloud resource managers can go find a GPU cluster with a little more flex in them to provision those resources quickly. Automating the process of reservations is also a bit of a game of catch-up, though my guess is that this is more a matter of someone having a weekend to sit down and write the REST service that manages incoming reservation requests. Although there's not a direct analog for reservations like this in Azure, AWS has a feature called AWS Capacity Blocks that does exactly this: if you know you'll want a certain number of GPU nodes sometime in the future, Capacity Blocks let you reserve them ahead of time through an API. Finally, I represented Microsoft and gave a lightning talk that riffed on a lot of what I've been writing about in this blog post: HPC seems to be reinventing a lot of things that the cloud has already figured out how to do. The illustrious Nick Brown was kind enough to snap a photo of one of my slides and post it on Twitter: My thesis was that the way urgent HPC workflows are triggered, scheduled, run, and reported on follows the same pattern that inferencing-as-a-service services (like Copilot and ChatGPT) are implemented under the hood, right down to executing multi-node jobs on InfiniBand clusters. The difference is that these cloud workflows are built on the foundation of really nice cloud services that provide security, scalability, monitoring, and hands-free management that were originally developed for commercial (not HPC!) customers. My argument was that, even if you don't want to pay cloud providers to run urgent HPC workflows as a managed service, you can use these services (and the software infrastructure on which they're built) as a blueprint for how to build these capabilities in your own HPC environments. Concluding thoughts The ISC'24 conference was fantastic, and I am glad it has not lost the unique elements that made me want to attend in the years prior to the pandemic. It's still that smaller, intimate, and focused HPC conference that brings the community together. Although a lot of my synopsis above may sound critical of the content presented over the four days I attended, the fact that I've had so much to write down in this blog post is a testament to the value I really get out of attending: it makes me sit down and think critically about the way the HPC community is evolving, what the leading minds in the field are thinking, and where I might be able to contribute the most in the coming year. I never much paid attention to the annual taglines of conferences like ISC, but this year's "Reinvent HPC" really resonated. The HPC community is at a crossroads. Exascale computing for science is now in the rear-view mirror, and large-scale AI is all the rage across the computing industry at large. But for the first time ever, this new direction in at-scale computing is happening without the inclusion of the people and organizations who've historically driven innovation in HPC. Whereas institutions like Oak Ridge, RIKEN, Cray, and Fujitsu defined the future of computing for decades, hundred-person startups like OpenAI and Anthropic are now paving the way in partnership with companies like Microsoft and Amazon. HPC needs to be reinvented, if for no other reason than to decide whether the HPC community wants to be inclusive of new frontiers in computing that they do not lead. Does the HPC community want AI to be considered a part of HPC? Judging from many speakers and panelists, the answer may be "no." To many, it sounded like AI is just another industry that's sucking all the air (and GPUs) out of the room; it's a distraction that is pulling funding and public interest away from solving real problems. It's not something worth understanding, it's not something that uses the familiar tools and libraries, and it's not the product of decades of steady, government-funded improvements. AI is "them" and HPC is "us." Personally, I'd like the answer to be "yes" though. Now that I'm on the other side of the table, supporting AI for a cloud provider, I can say that the technical challenges I face at Microsoft are the same technical challenges I faced in the DOE. The desire to deeply understand systems, optimize applications, and put world-class computing infrastructure in the hands of people who do amazing things is the same. And as the days go by, many of the faces I see are the same; instead of wearing DOE or Cray badges, my lifelong colleagues are now wearing NVIDIA or Microsoft badges. All this applies equally to whether cloud is HPC or not. The HPC community needs to reinvent itself to be inclusive of everyone working towards solving the same problems of computing at scale. Stop talking about people who work on commercial AI in cloud-based supercomputers as if they aren't in the room. They are in the room. Often near the front row, snapping photos, and angrily posting commentary on Twitter about how you're getting it all wrong. HPC has historically been used to solve scientific problems, whether to expand our understanding of the university, to find the next best place to drill an oil well, or to model the safety of aging nuclear weapons. The fact that HPC is now being used to solve squishier problems related to natural language or image generation does not change the essence of HPC. And whether that HPC is delivered through physical nodes and networks or virtualized nodes and networks is irrelevant, as long as those resources are still delivering high performance. AI is just as much HPC as scientific computing is, and cloud is just as much HPC as OLCF, R-CCS, or CSCS is. So perhaps HPC doesn't need to be reinvented as much as the mindset of its community does. That all said, I am genuinely impressed by how quickly ISC'24 has been reinventing itself in recent years. It wasn't too long ago that all its keynote speakers were greybeards from a predictable pool of public HPC centers all saying the same things year after year. It's wonderful to see a greater diversity of perspectives on the main stage and torches passing on to the next generation of leading figures in the field. And it was not lost on me that, for the first time in the history of this conference, Thomas Sterling did not deliver the closing keynote. As much fun as I had poking fun at his meandering and often-off-the-mark conjectures every year, it was delightful to be exposed to something new this year. I'm hopeful that ISC will continue to get better year over year, and ISC'25 will feel more inclusive of me despite the fact that I am now one of those hyperscale cloud AI people. So long as I still feel like it's my community, though, I will keep showing up in Germany every summer.]]></summary></entry><entry><title type="html">Centralized system and LSF logging on a Turing Pi system</title><link href="https://hpc.social/personal-blog/2024/centralized-system-and-lsf-logging-on-a-turing-pi-system/" rel="alternate" type="text/html" title="Centralized system and LSF logging on a Turing Pi system" /><published>2024-04-05T12:34:38-06:00</published><updated>2024-04-05T12:34:38-06:00</updated><id>https://hpc.social/personal-blog/2024/centralized-system-and-lsf-logging-on-a-turing-pi-system</id><content type="html" xml:base="https://hpc.social/personal-blog/2024/centralized-system-and-lsf-logging-on-a-turing-pi-system/"><![CDATA[<p>Logs are one of those indispensable things in IT when things go wrong. Having worked in technical support for software products in a past life, I’ve likely looked at hundreds (or more) logs over the years, helping to identify issues. So, I really appreciate the importance of logs, but I can honestly say that I never really thought about a logging strategy for the systems on my home network - primarily those running Linux.</p>

<p>One of my longtime friends, <a href="https://peter.czanik.hu/">Peter Czanik</a>, who also works in IT, happens to be a logging guru as well as an IBM Champion for Power Systems (yeah!). So it’s only natural that we get to talking about logging. He is often complaining that even at IT security conferences people are unaware of the importance of central logging. So, why is it so important? For security it’s obvious: logs are stored independently from the compromised system, so they cannot be modified or deleted by the attacker. But central logging is beneficial for the HPC operator as well. First of all, it’s availability. You can read the logs even if one of your nodes becomes unreachable. Instead of trying to breath life into the failed node, you can just take a look at the logs and see a broken hard drive, or a similar deadly problem. And it is also convenience, as all logs are available at a single location. Logging into each node on the 3 node cluster to check locally saved logs is inconvenient but doable. On a 10 node cluster it takes a long time. On a 100 node cluster a couple of working days. While, if your logs are collected to a central location, maybe a single grep command, or search in a Kibana or similar web interface.</p>

<p>Those who follow my blog will know that I’ve been tinkering with a Turing Pi V1 system lately. You can read my latest post <a href="https://www.gaborsamu.com/blog/turingpi_noctua/">here</a>. For me, the Turing Pi has always been a cluster in a box. My Turing Pi is fully populated with 7 compute modules. I’ve designed Node 1 to be the NFS server and LSF manager for the cluster. LSF is a workload scheduler for high-performance computing (HPC) from IBM. Naturally I turned to Peter for his guidance on this, and the result is this blog. Peter recommended that I  use <a href="https://www.syslog-ng.com/">syslog-ng</a> for log aggregation and also helped me through some of my first steps with <em>syslog-ng</em>. And the goal was to aggregate both the system (syslog) as well as LSF logs on Node 1. TL;DR it was easy to get it all working. But I encourage you to read on to better understand the nuances and necessary configuration both syslog-ng and LSF that was needed.</p>

<p><strong>The environment</strong></p>

<p>The following software has been deployed on the Turing Pi:</p>

<ul>
<li>Raspberry Pi OS (<em>2023-02-21-raspios-bullseye-arm64-lite.img</em>)</li>
<li>syslog-ng 3 – (3.28.1 as supplied with Raspberry Pi OS)</li>
<li>IBM LSF Standard Edition V10.1.0.13</li>
</ul>
<p>The Turing Pi system is configured as follows:</p>

<p>Node 1 (<em>turingpi</em>) is the manager node of this cluster in a box and has by far the most storage. Naturally we want to use that as the centralized logging server.</p>

<hr />

<table>
<thead>
<tr>
<th><strong>Node</strong></th>
<th><strong>Hostname</strong></th>
<th><strong>Hardware</strong></th>
<th><strong>Notes</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>turingpi</td>
<td>CM3+</td>
<td>LSF manager, NFS server, 128GB SDcard</td>
</tr>
<tr>
<td>2</td>
<td>kemeny</td>
<td>CM3</td>
<td>4GB eMMC flash</td>
</tr>
<tr>
<td>3</td>
<td>neumann</td>
<td>CM3+</td>
<td>8GB SDcard</td>
</tr>
<tr>
<td>4</td>
<td>szilard</td>
<td>CM3+</td>
<td>8GB SDcard</td>
</tr>
<tr>
<td>5</td>
<td>teller</td>
<td>CM3+</td>
<td>8GB SDcard</td>
</tr>
<tr>
<td>6</td>
<td>vonkarman</td>
<td>CM3+</td>
<td>8GB SDcard</td>
</tr>
<tr>
<td>7</td>
<td>wigner</td>
<td>CM3+</td>
<td>8GB SDcard</td>
</tr>
</tbody>
</table>
<hr />

<p><strong>Syslog-ng &amp; LSF setup</strong></p>

<ol>
<li>Raspberry Pi OS configures <em>rsyslog</em> out of the box. The first step is to install <em>syslog-ng</em> on Node 1 in the environment. Note that installing syslog-ng automatically disables <em>rsyslog</em> on the nodes.</li>
</ol>
<p><details>
  <strong>Output of <em>apt update; apt-get install syslog-ng -y</em>. Click to expand</strong>
  <div class="highlight"><pre><code class="language-python">root<span style="color: #a6e22e;">@turingpi</span>:<span style="color: #f92672;">~</span><span style="color: #75715e;"># apt update; apt-get install syslog-ng -y </span>
Hit:<span style="color: #ae81ff;">1</span> http:<span style="color: #f92672;">//</span>security<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian<span style="color: #f92672;">-</span>security bullseye<span style="color: #f92672;">-</span>security InRelease
Hit:<span style="color: #ae81ff;">2</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye InRelease                                                        
Hit:<span style="color: #ae81ff;">3</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">-</span>updates InRelease                                                
Hit:<span style="color: #ae81ff;">4</span> https:<span style="color: #f92672;">//</span>repos<span style="color: #f92672;">.</span>influxdata<span style="color: #f92672;">.</span>com<span style="color: #f92672;">/</span>debian stable InRelease                                                   
Hit:<span style="color: #ae81ff;">5</span> https:<span style="color: #f92672;">//</span>repos<span style="color: #f92672;">.</span>influxdata<span style="color: #f92672;">.</span>com<span style="color: #f92672;">/</span>debian bullseye InRelease                                                 
Hit:<span style="color: #ae81ff;">6</span> http:<span style="color: #f92672;">//</span>archive<span style="color: #f92672;">.</span>raspberrypi<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye InRelease                                  
Hit:<span style="color: #ae81ff;">7</span> https:<span style="color: #f92672;">//</span>packagecloud<span style="color: #f92672;">.</span>io<span style="color: #f92672;">/</span>ookla<span style="color: #f92672;">/</span>speedtest<span style="color: #f92672;">-</span>cli<span style="color: #f92672;">/</span>debian bullseye InRelease                     
Reading package lists<span style="color: #f92672;">...</span> Done
Building dependency tree<span style="color: #f92672;">...</span> Done
Reading state information<span style="color: #f92672;">...</span> Done
All packages are up to date<span style="color: #f92672;">.</span>
Reading package lists<span style="color: #f92672;">...</span> Done
Building dependency tree<span style="color: #f92672;">...</span> Done
Reading state information<span style="color: #f92672;">...</span> Done
The following additional packages will be installed:
  libbson<span style="color: #f92672;">-</span><span style="color: #ae81ff;">1.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">0</span> libdbi1 libesmtp6 libhiredis0<span style="color: #ae81ff;">.14</span> libivykis0 libmaxminddb0 libmongoc<span style="color: #f92672;">-</span><span style="color: #ae81ff;">1.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">0</span> libmongocrypt0
  libnet1 libprotobuf<span style="color: #f92672;">-</span>c1 librabbitmq4 librdkafka1 libriemann<span style="color: #f92672;">-</span>client0 libsnappy1v5 libsnmp<span style="color: #f92672;">-</span>base libsnmp40
  syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>core syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>add<span style="color: #f92672;">-</span>contextual<span style="color: #f92672;">-</span>data syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>amqp syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>examples
  syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>extra syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>geoip2 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>getent syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>graphite syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>http
  syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>map<span style="color: #f92672;">-</span>value<span style="color: #f92672;">-</span>pairs syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>mongodb syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>python syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>rdkafka
  syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>redis syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>riemann syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>slog syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>smtp syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>snmp
  syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>sql syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>stardate syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>stomp syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>xml<span style="color: #f92672;">-</span>parser
Suggested packages:
  mmdb<span style="color: #f92672;">-</span>bin snmp<span style="color: #f92672;">-</span>mibs<span style="color: #f92672;">-</span>downloader rabbitmq<span style="color: #f92672;">-</span>server graphite<span style="color: #f92672;">-</span>web mongodb<span style="color: #f92672;">-</span>server libdbd<span style="color: #f92672;">-</span>mysql libdbd<span style="color: #f92672;">-</span>pgsql
  libdbd<span style="color: #f92672;">-</span>sqlite3 activemq
The following packages will be REMOVED:
  rsyslog
The following NEW packages will be installed:
  libbson<span style="color: #f92672;">-</span><span style="color: #ae81ff;">1.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">0</span> libdbi1 libesmtp6 libhiredis0<span style="color: #ae81ff;">.14</span> libivykis0 libmaxminddb0 libmongoc<span style="color: #f92672;">-</span><span style="color: #ae81ff;">1.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">0</span> libmongocrypt0
  libnet1 libprotobuf<span style="color: #f92672;">-</span>c1 librabbitmq4 librdkafka1 libriemann<span style="color: #f92672;">-</span>client0 libsnappy1v5 libsnmp<span style="color: #f92672;">-</span>base libsnmp40
  syslog<span style="color: #f92672;">-</span>ng syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>core syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>add<span style="color: #f92672;">-</span>contextual<span style="color: #f92672;">-</span>data syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>amqp syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>examples
  syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>extra syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>geoip2 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>getent syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>graphite syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>http
  syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>map<span style="color: #f92672;">-</span>value<span style="color: #f92672;">-</span>pairs syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>mongodb syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>python syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>rdkafka
  syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>redis syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>riemann syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>slog syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>smtp syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>snmp
  syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>sql syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>stardate syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>stomp syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>xml<span style="color: #f92672;">-</span>parser
<span style="color: #ae81ff;">0</span> upgraded, <span style="color: #ae81ff;">39</span> newly installed, <span style="color: #ae81ff;">1</span> to remove <span style="color: #f92672;">and</span> <span style="color: #ae81ff;">0</span> <span style="color: #f92672;">not</span> upgraded<span style="color: #f92672;">.</span>
Need to get <span style="color: #ae81ff;">7</span>,<span style="color: #ae81ff;">015</span> kB of archives<span style="color: #f92672;">.</span>
After this operation, <span style="color: #ae81ff;">15.1</span> MB of additional disk space will be used<span style="color: #f92672;">.</span>
Get:<span style="color: #ae81ff;">1</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 libbson<span style="color: #f92672;">-</span><span style="color: #ae81ff;">1.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">0</span> arm64 <span style="color: #ae81ff;">1.17.6</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span> [<span style="color: #ae81ff;">69.7</span> kB]
Get:<span style="color: #ae81ff;">2</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 libmongocrypt0 arm64 <span style="color: #ae81ff;">1.1.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span> [<span style="color: #ae81ff;">114</span> kB]
Get:<span style="color: #ae81ff;">3</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 libsnappy1v5 arm64 <span style="color: #ae81ff;">1.1.8</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span> [<span style="color: #ae81ff;">17.2</span> kB]
Get:<span style="color: #ae81ff;">4</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 libmongoc<span style="color: #f92672;">-</span><span style="color: #ae81ff;">1.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">0</span> arm64 <span style="color: #ae81ff;">1.17.6</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span> [<span style="color: #ae81ff;">257</span> kB]
Get:<span style="color: #ae81ff;">5</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 libivykis0 arm64 <span style="color: #ae81ff;">0.42.4</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span> [<span style="color: #ae81ff;">25.3</span> kB]
Get:<span style="color: #ae81ff;">6</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 libnet1 arm64 <span style="color: #ae81ff;">1.1.6</span><span style="color: #f92672;">+</span>dfsg<span style="color: #f92672;">-</span><span style="color: #ae81ff;">3.1</span> [<span style="color: #ae81ff;">56.8</span> kB]
Get:<span style="color: #ae81ff;">7</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>core arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">591</span> kB]
Get:<span style="color: #ae81ff;">8</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>mongodb arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">37.9</span> kB]
Get:<span style="color: #ae81ff;">9</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 libdbi1 arm64 <span style="color: #ae81ff;">0.9.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">6</span> [<span style="color: #ae81ff;">27.8</span> kB]
Get:<span style="color: #ae81ff;">10</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>sql arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">41.5</span> kB]
Get:<span style="color: #ae81ff;">11</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 libesmtp6 arm64 <span style="color: #ae81ff;">1.0.6</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">4.3</span> [<span style="color: #ae81ff;">52.0</span> kB]
Get:<span style="color: #ae81ff;">12</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 libhiredis0<span style="color: #ae81ff;">.14</span> arm64 <span style="color: #ae81ff;">0.14.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span> [<span style="color: #ae81ff;">33.7</span> kB]
Get:<span style="color: #ae81ff;">13</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 libmaxminddb0 arm64 <span style="color: #ae81ff;">1.5.2</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span> [<span style="color: #ae81ff;">29.6</span> kB]
Get:<span style="color: #ae81ff;">14</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 libprotobuf<span style="color: #f92672;">-</span>c1 arm64 <span style="color: #ae81ff;">1.3.3</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span><span style="color: #f92672;">+</span>b2 [<span style="color: #ae81ff;">26.8</span> kB]
Get:<span style="color: #ae81ff;">15</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 librabbitmq4 arm64 <span style="color: #ae81ff;">0.10.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span> [<span style="color: #ae81ff;">39.7</span> kB]
Get:<span style="color: #ae81ff;">16</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 librdkafka1 arm64 <span style="color: #ae81ff;">1.6.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span> [<span style="color: #ae81ff;">515</span> kB]
Get:<span style="color: #ae81ff;">17</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 libriemann<span style="color: #f92672;">-</span>client0 arm64 <span style="color: #ae81ff;">1.10.4</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>b2 [<span style="color: #ae81ff;">21.9</span> kB]
Get:<span style="color: #ae81ff;">18</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 libsnmp<span style="color: #f92672;">-</span>base all <span style="color: #ae81ff;">5.9</span><span style="color: #f92672;">+</span>dfsg<span style="color: #f92672;">-</span><span style="color: #ae81ff;">4</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">1</span>,<span style="color: #ae81ff;">736</span> kB]
Get:<span style="color: #ae81ff;">19</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 libsnmp40 arm64 <span style="color: #ae81ff;">5.9</span><span style="color: #f92672;">+</span>dfsg<span style="color: #f92672;">-</span><span style="color: #ae81ff;">4</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">2</span>,<span style="color: #ae81ff;">497</span> kB]
Get:<span style="color: #ae81ff;">20</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng all <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">25.9</span> kB]
Get:<span style="color: #ae81ff;">21</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>add<span style="color: #f92672;">-</span>contextual<span style="color: #f92672;">-</span>data arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">40.5</span> kB]
Get:<span style="color: #ae81ff;">22</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>amqp arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">48.8</span> kB]
Get:<span style="color: #ae81ff;">23</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>examples arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">57.3</span> kB]
Get:<span style="color: #ae81ff;">24</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>extra all <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">35.7</span> kB]
Get:<span style="color: #ae81ff;">25</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>geoip2 arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">36.9</span> kB]
Get:<span style="color: #ae81ff;">26</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>graphite arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">29.4</span> kB]
Get:<span style="color: #ae81ff;">27</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>http arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">50.5</span> kB]
Get:<span style="color: #ae81ff;">28</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>python arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">69.9</span> kB]
Get:<span style="color: #ae81ff;">29</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>rdkafka arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">41.5</span> kB]
Get:<span style="color: #ae81ff;">30</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>redis arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">37.6</span> kB]
Get:<span style="color: #ae81ff;">31</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>riemann arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">40.1</span> kB]
Get:<span style="color: #ae81ff;">32</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>slog arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">63.3</span> kB]
Get:<span style="color: #ae81ff;">33</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>smtp arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">38.0</span> kB]
Get:<span style="color: #ae81ff;">34</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>snmp arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">42.5</span> kB]
Get:<span style="color: #ae81ff;">35</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>stomp arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">39.1</span> kB]
Get:<span style="color: #ae81ff;">36</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>xml<span style="color: #f92672;">-</span>parser arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">34.7</span> kB]
Get:<span style="color: #ae81ff;">37</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>getent arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">29.5</span> kB]
Get:<span style="color: #ae81ff;">38</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>map<span style="color: #f92672;">-</span>value<span style="color: #f92672;">-</span>pairs arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">34.0</span> kB]
Get:<span style="color: #ae81ff;">39</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>stardate arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">28.6</span> kB]
Fetched <span style="color: #ae81ff;">7</span>,<span style="color: #ae81ff;">015</span> kB <span style="color: #f92672;">in</span> <span style="color: #ae81ff;">5</span>s (<span style="color: #ae81ff;">1</span>,<span style="color: #ae81ff;">311</span> kB<span style="color: #f92672;">/</span>s)           
Extracting templates <span style="color: #f92672;">from</span> packages: <span style="color: #ae81ff;">100</span><span style="color: #f92672;">%</span>
(Reading database <span style="color: #f92672;">...</span> <span style="color: #ae81ff;">90182</span> files <span style="color: #f92672;">and</span> directories currently installed<span style="color: #f92672;">.</span>)
Removing rsyslog (<span style="color: #ae81ff;">8.2102.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package libbson<span style="color: #f92672;">-</span><span style="color: #ae81ff;">1.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">0.</span>
(Reading database <span style="color: #f92672;">...</span> <span style="color: #ae81ff;">90124</span> files <span style="color: #f92672;">and</span> directories currently installed<span style="color: #f92672;">.</span>)
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">00</span><span style="color: #f92672;">-</span>libbson<span style="color: #f92672;">-</span><span style="color: #ae81ff;">1.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">0_1.17.6</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking libbson<span style="color: #f92672;">-</span><span style="color: #ae81ff;">1.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">0</span> (<span style="color: #ae81ff;">1.17.6</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>) <span style="color: #f92672;">...</span>
Selecting previously unselected package libmongocrypt0:arm64<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">01</span><span style="color: #f92672;">-</span>libmongocrypt0_1<span style="color: #ae81ff;">.1.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking libmongocrypt0:arm64 (<span style="color: #ae81ff;">1.1.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>) <span style="color: #f92672;">...</span>
Selecting previously unselected package libsnappy1v5:arm64<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">02</span><span style="color: #f92672;">-</span>libsnappy1v5_1<span style="color: #ae81ff;">.1.8</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking libsnappy1v5:arm64 (<span style="color: #ae81ff;">1.1.8</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>) <span style="color: #f92672;">...</span>
Selecting previously unselected package libmongoc<span style="color: #f92672;">-</span><span style="color: #ae81ff;">1.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">0.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">03</span><span style="color: #f92672;">-</span>libmongoc<span style="color: #f92672;">-</span><span style="color: #ae81ff;">1.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">0_1.17.6</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking libmongoc<span style="color: #f92672;">-</span><span style="color: #ae81ff;">1.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">0</span> (<span style="color: #ae81ff;">1.17.6</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>) <span style="color: #f92672;">...</span>
Selecting previously unselected package libivykis0:arm64<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">04</span><span style="color: #f92672;">-</span>libivykis0_0<span style="color: #ae81ff;">.42.4</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking libivykis0:arm64 (<span style="color: #ae81ff;">0.42.4</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>) <span style="color: #f92672;">...</span>
Selecting previously unselected package libnet1:arm64<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">05</span><span style="color: #f92672;">-</span>libnet1_1<span style="color: #ae81ff;">.1.6</span><span style="color: #f92672;">+</span>dfsg<span style="color: #f92672;">-</span><span style="color: #ae81ff;">3.1</span>_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking libnet1:arm64 (<span style="color: #ae81ff;">1.1.6</span><span style="color: #f92672;">+</span>dfsg<span style="color: #f92672;">-</span><span style="color: #ae81ff;">3.1</span>) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>core<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">06</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>core_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>core (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>mongodb<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">07</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>mongodb_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>mongodb (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package libdbi1:arm64<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">08</span><span style="color: #f92672;">-</span>libdbi1_0<span style="color: #ae81ff;">.9.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">6</span>_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking libdbi1:arm64 (<span style="color: #ae81ff;">0.9.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">6</span>) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>sql<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">09</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>sql_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>sql (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package libesmtp6<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">10</span><span style="color: #f92672;">-</span>libesmtp6_1<span style="color: #ae81ff;">.0.6</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">4.3</span>_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking libesmtp6 (<span style="color: #ae81ff;">1.0.6</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">4.3</span>) <span style="color: #f92672;">...</span>
Selecting previously unselected package libhiredis0<span style="color: #ae81ff;">.14</span>:arm64<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">11</span><span style="color: #f92672;">-</span>libhiredis0<span style="color: #ae81ff;">.14_0.14.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking libhiredis0<span style="color: #ae81ff;">.14</span>:arm64 (<span style="color: #ae81ff;">0.14.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>) <span style="color: #f92672;">...</span>
Selecting previously unselected package libmaxminddb0:arm64<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">12</span><span style="color: #f92672;">-</span>libmaxminddb0_1<span style="color: #ae81ff;">.5.2</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking libmaxminddb0:arm64 (<span style="color: #ae81ff;">1.5.2</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>) <span style="color: #f92672;">...</span>
Selecting previously unselected package libprotobuf<span style="color: #f92672;">-</span>c1:arm64<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">13</span><span style="color: #f92672;">-</span>libprotobuf<span style="color: #f92672;">-</span>c1_1<span style="color: #ae81ff;">.3.3</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span><span style="color: #f92672;">+</span>b2_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking libprotobuf<span style="color: #f92672;">-</span>c1:arm64 (<span style="color: #ae81ff;">1.3.3</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span><span style="color: #f92672;">+</span>b2) <span style="color: #f92672;">...</span>
Selecting previously unselected package librabbitmq4:arm64<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">14</span><span style="color: #f92672;">-</span>librabbitmq4_0<span style="color: #ae81ff;">.10.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking librabbitmq4:arm64 (<span style="color: #ae81ff;">0.10.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>) <span style="color: #f92672;">...</span>
Selecting previously unselected package librdkafka1:arm64<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">15</span><span style="color: #f92672;">-</span>librdkafka1_1<span style="color: #ae81ff;">.6.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking librdkafka1:arm64 (<span style="color: #ae81ff;">1.6.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>) <span style="color: #f92672;">...</span>
Selecting previously unselected package libriemann<span style="color: #f92672;">-</span>client0:arm64<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">16</span><span style="color: #f92672;">-</span>libriemann<span style="color: #f92672;">-</span>client0_1<span style="color: #ae81ff;">.10.4</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>b2_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking libriemann<span style="color: #f92672;">-</span>client0:arm64 (<span style="color: #ae81ff;">1.10.4</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>b2) <span style="color: #f92672;">...</span>
Selecting previously unselected package libsnmp<span style="color: #f92672;">-</span>base<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">17</span><span style="color: #f92672;">-</span>libsnmp<span style="color: #f92672;">-</span>base_5<span style="color: #ae81ff;">.9</span><span style="color: #f92672;">+</span>dfsg<span style="color: #f92672;">-</span><span style="color: #ae81ff;">4</span><span style="color: #f92672;">+</span>deb11u1_all<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking libsnmp<span style="color: #f92672;">-</span>base (<span style="color: #ae81ff;">5.9</span><span style="color: #f92672;">+</span>dfsg<span style="color: #f92672;">-</span><span style="color: #ae81ff;">4</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package libsnmp40:arm64<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">18</span><span style="color: #f92672;">-</span>libsnmp40_5<span style="color: #ae81ff;">.9</span><span style="color: #f92672;">+</span>dfsg<span style="color: #f92672;">-</span><span style="color: #ae81ff;">4</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking libsnmp40:arm64 (<span style="color: #ae81ff;">5.9</span><span style="color: #f92672;">+</span>dfsg<span style="color: #f92672;">-</span><span style="color: #ae81ff;">4</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">19</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_all<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>add<span style="color: #f92672;">-</span>contextual<span style="color: #f92672;">-</span>data<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">20</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>add<span style="color: #f92672;">-</span>contextual<span style="color: #f92672;">-</span>data_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>add<span style="color: #f92672;">-</span>contextual<span style="color: #f92672;">-</span>data (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>amqp<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">21</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>amqp_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>amqp (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>examples<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">22</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>examples_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>examples (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>extra<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">23</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>extra_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_all<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>extra (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>geoip2<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">24</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>geoip2_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>geoip2 (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>graphite<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">25</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>graphite_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>graphite (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>http<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">26</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>http_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>http (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>python<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">27</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>python_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>python (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>rdkafka<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">28</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>rdkafka_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>rdkafka (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>redis<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">29</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>redis_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>redis (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>riemann<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">30</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>riemann_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>riemann (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>slog<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">31</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>slog_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>slog (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>smtp<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">32</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>smtp_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>smtp (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>snmp<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">33</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>snmp_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>snmp (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>stomp<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">34</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>stomp_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>stomp (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>xml<span style="color: #f92672;">-</span>parser<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">35</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>xml<span style="color: #f92672;">-</span>parser_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>xml<span style="color: #f92672;">-</span>parser (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>getent<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">36</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>getent_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>getent (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>map<span style="color: #f92672;">-</span>value<span style="color: #f92672;">-</span>pairs<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">37</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>map<span style="color: #f92672;">-</span>value<span style="color: #f92672;">-</span>pairs_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>map<span style="color: #f92672;">-</span>value<span style="color: #f92672;">-</span>pairs (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>stardate<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">38</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>stardate_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>stardate (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up librabbitmq4:arm64 (<span style="color: #ae81ff;">0.10.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>) <span style="color: #f92672;">...</span>
Setting up libdbi1:arm64 (<span style="color: #ae81ff;">0.9.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">6</span>) <span style="color: #f92672;">...</span>
Setting up libsnmp<span style="color: #f92672;">-</span>base (<span style="color: #ae81ff;">5.9</span><span style="color: #f92672;">+</span>dfsg<span style="color: #f92672;">-</span><span style="color: #ae81ff;">4</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up libmaxminddb0:arm64 (<span style="color: #ae81ff;">1.5.2</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>) <span style="color: #f92672;">...</span>
Setting up libesmtp6 (<span style="color: #ae81ff;">1.0.6</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">4.3</span>) <span style="color: #f92672;">...</span>
Setting up libnet1:arm64 (<span style="color: #ae81ff;">1.1.6</span><span style="color: #f92672;">+</span>dfsg<span style="color: #f92672;">-</span><span style="color: #ae81ff;">3.1</span>) <span style="color: #f92672;">...</span>
Setting up libprotobuf<span style="color: #f92672;">-</span>c1:arm64 (<span style="color: #ae81ff;">1.3.3</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span><span style="color: #f92672;">+</span>b2) <span style="color: #f92672;">...</span>
Setting up libsnappy1v5:arm64 (<span style="color: #ae81ff;">1.1.8</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>) <span style="color: #f92672;">...</span>
Setting up libsnmp40:arm64 (<span style="color: #ae81ff;">5.9</span><span style="color: #f92672;">+</span>dfsg<span style="color: #f92672;">-</span><span style="color: #ae81ff;">4</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up libbson<span style="color: #f92672;">-</span><span style="color: #ae81ff;">1.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">0</span> (<span style="color: #ae81ff;">1.17.6</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>) <span style="color: #f92672;">...</span>
Setting up libivykis0:arm64 (<span style="color: #ae81ff;">0.42.4</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>) <span style="color: #f92672;">...</span>
Setting up libriemann<span style="color: #f92672;">-</span>client0:arm64 (<span style="color: #ae81ff;">1.10.4</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>b2) <span style="color: #f92672;">...</span>
Setting up librdkafka1:arm64 (<span style="color: #ae81ff;">1.6.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>) <span style="color: #f92672;">...</span>
Setting up libhiredis0<span style="color: #ae81ff;">.14</span>:arm64 (<span style="color: #ae81ff;">0.14.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>) <span style="color: #f92672;">...</span>
Setting up libmongocrypt0:arm64 (<span style="color: #ae81ff;">1.1.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>) <span style="color: #f92672;">...</span>
Setting up libmongoc<span style="color: #f92672;">-</span><span style="color: #ae81ff;">1.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">0</span> (<span style="color: #ae81ff;">1.17.6</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>core (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Created symlink <span style="color: #f92672;">/</span>etc<span style="color: #f92672;">/</span>systemd<span style="color: #f92672;">/</span>system<span style="color: #f92672;">/</span>multi<span style="color: #f92672;">-</span>user<span style="color: #f92672;">.</span>target<span style="color: #f92672;">.</span>wants<span style="color: #f92672;">/</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>service <span style="color: #960050; background-color: #1e0010;">→</span> <span style="color: #f92672;">/</span>lib<span style="color: #f92672;">/</span>systemd<span style="color: #f92672;">/</span>system<span style="color: #f92672;">/</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>service<span style="color: #f92672;">.</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>examples (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>xml<span style="color: #f92672;">-</span>parser (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>stomp (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>riemann (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>stardate (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>geoip2 (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>getent (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>amqp (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>python (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>smtp (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>snmp (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>extra (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>rdkafka (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>graphite (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>add<span style="color: #f92672;">-</span>contextual<span style="color: #f92672;">-</span>data (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>mongodb (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>http (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>slog (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>map<span style="color: #f92672;">-</span>value<span style="color: #f92672;">-</span>pairs (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>sql (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>redis (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Processing triggers <span style="color: #66d9ef;">for</span> man<span style="color: #f92672;">-</span>db (<span style="color: #ae81ff;">2.9.4</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span>) <span style="color: #f92672;">...</span>
Processing triggers <span style="color: #66d9ef;">for</span> libc<span style="color: #f92672;">-</span>bin (<span style="color: #ae81ff;">2.31</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">13</span><span style="color: #f92672;">+</span>rpt2<span style="color: #f92672;">+</span>rpi1<span style="color: #f92672;">+</span>deb11u8) <span style="color: #f92672;">...</span>
Scanning processes<span style="color: #f92672;">...</span>                                                                                         
Scanning processor microcode<span style="color: #f92672;">...</span>                                                                               
Scanning linux images<span style="color: #f92672;">...</span>                                                                                      

Running kernel seems to be up<span style="color: #f92672;">-</span>to<span style="color: #f92672;">-</span>date<span style="color: #f92672;">.</span>

Failed to check <span style="color: #66d9ef;">for</span> processor microcode upgrades<span style="color: #f92672;">.</span>

No services need to be restarted<span style="color: #f92672;">.</span>

No containers need to be restarted<span style="color: #f92672;">.</span>

No user sessions are running outdated binaries<span style="color: #f92672;">.</span>
</code></pre></div>

</details>

<br />
<!-- raw HTML omitted -->
2. With <em>syslog-ng</em> installed, it’s now time to build the configuration for it. A new configuration file <em>fromnet.conf</em> is shown below, in which a <em>syslog-ng</em> destination is created which will aggregate logs from the Turing Pi nodes in <em>/var/log/fromnet</em> in plain text format. Additionally, the logs will be written in JSON format to the file <em>/var/log/fromnet.json</em>.</p>

<div class="highlight"><pre><code class="language-plaintext">root@turingpi:~# cat /etc/syslog-ng/fromnet.conf 
# source
source s_fromnet {
  syslog(port(601));
};
# destination 
destination d_fromnet {
  file("/var/log/fromnet");
  file("/var/log/fromnet.json" template("$(format-json --scope rfc5424 --scope dot-nv-pairs
        --rekey .* --shift 1 --scope nv-pairs)\n") );
};
# log path
log {
  source(s_fromnet);
  destination(d_fromnet);
}; </code></pre></div>

<ol start="3">
<li>Unless we only want to see source IP addresses in the collected logs, it’s necessary to update the <em>syslog-ng</em> configuration file <em>/etc/syslog-ng/syslog-ng.conf</em> to record the hostnames from which the log messages have originated. This is done by adding the <em>keep_hostname(yes)</em> parameter to the options section as follows:</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext">....
....
# First, set some global options. 
options { chain_hostnames(off); flush_lines(0); use_dns(no); use_fqdn(no);          
        keep_hostname(yes);dns_cache(no); owner("root"); group("adm"); perm(0640); 
        stats_freq(0); bad_hostname("^gconfd$"); 
};
....
....</code></pre></div>

<ol start="4">
<li>Next, the IBM LSF configuration is updated to prevent the creation of local logfiles for the LSF daemons. This is done by commenting the <em>LSF_LOGDIR</em> option in the configuration file <em>$LSF_ENVDIR/lsf.conf</em>. At the same time, we also set <em>LSF_LOG_MASK=LOG_DEBUG</em> for testing purposes to enable verbose logging for the LSF daemons.</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext">....
....
# Daemon log messages
# LSF_LOGDIR=/opt/ibm/lsf/log
LSF_LOG_MASK=LOG_DEBUG
....
....</code></pre></div>

<ol start="5">
<li>Finally, to make the changes take effect, both syslog-ng and LSF are restarted.</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext">root@turingpi:~# systemctl restart syslog-ng 
root@turingpi:~# . /opt/ibm/lsf/conf/profile.lsf  
root@turingpi:~# lsf_daemons restart 
Stopping the LSF subsystem 
Starting the LSF subsystem</code></pre></div>

<ol start="6">
<li>With the configuration ready on the centralized logging server, host <em>turingpi</em>, we now turn our attention to Nodes 2-7 in the cluster. Here we’ll use the <em>parallel-ssh</em> tool to streamline some operations. We start with the installation of <em>syslog-ng</em> across Nodes 2-7. Note that the output of the installation of <em>syslog-ng</em> across the compute nodes has been truncated.</li>
</ol>
<p><details>
  <strong>Truncated output of <em>parallel-ssh -h /opt/workers -i &ldquo;apt-get install syslog-ng -y&rdquo;</em>. Click to expand</strong>
  <div class="highlight"><pre><code class="language-python">root<span style="color: #a6e22e;">@turingpi</span>:<span style="color: #f92672;">~</span><span style="color: #75715e;"># parallel-ssh -h /opt/workers -i "apt-get install syslog-ng -y" </span>
[<span style="color: #ae81ff;">1</span>] <span style="color: #ae81ff;">13</span>:<span style="color: #ae81ff;">57</span>:<span style="color: #ae81ff;">07</span> [SUCCESS] kemeny
Reading package lists<span style="color: #f92672;">...</span>
Building dependency tree<span style="color: #f92672;">...</span>
Reading state information<span style="color: #f92672;">...</span>
The following additional packages will be installed:
  libbson<span style="color: #f92672;">-</span><span style="color: #ae81ff;">1.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">0</span> libdbi1 libesmtp6 libhiredis0<span style="color: #ae81ff;">.14</span> libivykis0 libmaxminddb0
  libmongoc<span style="color: #f92672;">-</span><span style="color: #ae81ff;">1.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">0</span> libmongocrypt0 libnet1 libprotobuf<span style="color: #f92672;">-</span>c1 librabbitmq4
  librdkafka1 libriemann<span style="color: #f92672;">-</span>client0 libsensors<span style="color: #f92672;">-</span>config libsensors5 libsnappy1v5
  libsnmp<span style="color: #f92672;">-</span>base libsnmp40 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>core syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>add<span style="color: #f92672;">-</span>contextual<span style="color: #f92672;">-</span>data
  syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>amqp syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>examples syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>extra
  syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>geoip2 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>getent syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>graphite
  syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>http syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>map<span style="color: #f92672;">-</span>value<span style="color: #f92672;">-</span>pairs syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>mongodb
  syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>python syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>rdkafka syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>redis
  syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>riemann syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>slog syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>smtp
  syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>snmp syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>sql syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>stardate
  syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>stomp syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>xml<span style="color: #f92672;">-</span>parser
Suggested packages:
  mmdb<span style="color: #f92672;">-</span>bin lm<span style="color: #f92672;">-</span>sensors snmp<span style="color: #f92672;">-</span>mibs<span style="color: #f92672;">-</span>downloader rabbitmq<span style="color: #f92672;">-</span>server graphite<span style="color: #f92672;">-</span>web
  mongodb<span style="color: #f92672;">-</span>server libdbd<span style="color: #f92672;">-</span>mysql libdbd<span style="color: #f92672;">-</span>pgsql libdbd<span style="color: #f92672;">-</span>sqlite3 activemq
The following packages will be REMOVED:
  rsyslog
The following NEW packages will be installed:
  libbson<span style="color: #f92672;">-</span><span style="color: #ae81ff;">1.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">0</span> libdbi1 libesmtp6 libhiredis0<span style="color: #ae81ff;">.14</span> libivykis0 libmaxminddb0
  libmongoc<span style="color: #f92672;">-</span><span style="color: #ae81ff;">1.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">0</span> libmongocrypt0 libnet1 libprotobuf<span style="color: #f92672;">-</span>c1 librabbitmq4
  librdkafka1 libriemann<span style="color: #f92672;">-</span>client0 libsensors<span style="color: #f92672;">-</span>config libsensors5 libsnappy1v5
  libsnmp<span style="color: #f92672;">-</span>base libsnmp40 syslog<span style="color: #f92672;">-</span>ng syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>core
  syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>add<span style="color: #f92672;">-</span>contextual<span style="color: #f92672;">-</span>data syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>amqp syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>examples
  syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>extra syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>geoip2 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>getent
  syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>graphite syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>http syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>map<span style="color: #f92672;">-</span>value<span style="color: #f92672;">-</span>pairs
  syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>mongodb syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>python syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>rdkafka
  syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>redis syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>riemann syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>slog
  syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>smtp syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>snmp syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>sql
  syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>stardate syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>stomp syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>xml<span style="color: #f92672;">-</span>parser
<span style="color: #ae81ff;">0</span> upgraded, <span style="color: #ae81ff;">41</span> newly installed, <span style="color: #ae81ff;">1</span> to remove <span style="color: #f92672;">and</span> <span style="color: #ae81ff;">0</span> <span style="color: #f92672;">not</span> upgraded<span style="color: #f92672;">.</span>
Need to get <span style="color: #ae81ff;">7</span>,<span style="color: #ae81ff;">098</span> kB of archives<span style="color: #f92672;">.</span>
After this operation, <span style="color: #ae81ff;">15.3</span> MB of additional disk space will be used<span style="color: #f92672;">.</span>
Get:<span style="color: #ae81ff;">1</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 libbson<span style="color: #f92672;">-</span><span style="color: #ae81ff;">1.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">0</span> arm64 <span style="color: #ae81ff;">1.17.6</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span> [<span style="color: #ae81ff;">69.7</span> kB]
Get:<span style="color: #ae81ff;">2</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 libmongocrypt0 arm64 <span style="color: #ae81ff;">1.1.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span> [<span style="color: #ae81ff;">114</span> kB]
Get:<span style="color: #ae81ff;">3</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 libsnappy1v5 arm64 <span style="color: #ae81ff;">1.1.8</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span> [<span style="color: #ae81ff;">17.2</span> kB]
Get:<span style="color: #ae81ff;">4</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 libmongoc<span style="color: #f92672;">-</span><span style="color: #ae81ff;">1.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">0</span> arm64 <span style="color: #ae81ff;">1.17.6</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span> [<span style="color: #ae81ff;">257</span> kB]
Get:<span style="color: #ae81ff;">5</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 libivykis0 arm64 <span style="color: #ae81ff;">0.42.4</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span> [<span style="color: #ae81ff;">25.3</span> kB]
Get:<span style="color: #ae81ff;">6</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 libnet1 arm64 <span style="color: #ae81ff;">1.1.6</span><span style="color: #f92672;">+</span>dfsg<span style="color: #f92672;">-</span><span style="color: #ae81ff;">3.1</span> [<span style="color: #ae81ff;">56.8</span> kB]
Get:<span style="color: #ae81ff;">7</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>core arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">591</span> kB]
Get:<span style="color: #ae81ff;">8</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>mongodb arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">37.9</span> kB]
Get:<span style="color: #ae81ff;">9</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 libdbi1 arm64 <span style="color: #ae81ff;">0.9.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">6</span> [<span style="color: #ae81ff;">27.8</span> kB]
Get:<span style="color: #ae81ff;">10</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>sql arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">41.5</span> kB]
Get:<span style="color: #ae81ff;">11</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 libesmtp6 arm64 <span style="color: #ae81ff;">1.0.6</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">4.3</span> [<span style="color: #ae81ff;">52.0</span> kB]
Get:<span style="color: #ae81ff;">12</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 libhiredis0<span style="color: #ae81ff;">.14</span> arm64 <span style="color: #ae81ff;">0.14.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span> [<span style="color: #ae81ff;">33.7</span> kB]
Get:<span style="color: #ae81ff;">13</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 libmaxminddb0 arm64 <span style="color: #ae81ff;">1.5.2</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span> [<span style="color: #ae81ff;">29.6</span> kB]
Get:<span style="color: #ae81ff;">14</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 libprotobuf<span style="color: #f92672;">-</span>c1 arm64 <span style="color: #ae81ff;">1.3.3</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span><span style="color: #f92672;">+</span>b2 [<span style="color: #ae81ff;">26.8</span> kB]
Get:<span style="color: #ae81ff;">15</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 librabbitmq4 arm64 <span style="color: #ae81ff;">0.10.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span> [<span style="color: #ae81ff;">39.7</span> kB]
Get:<span style="color: #ae81ff;">16</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 librdkafka1 arm64 <span style="color: #ae81ff;">1.6.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span> [<span style="color: #ae81ff;">515</span> kB]
Get:<span style="color: #ae81ff;">17</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 libriemann<span style="color: #f92672;">-</span>client0 arm64 <span style="color: #ae81ff;">1.10.4</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>b2 [<span style="color: #ae81ff;">21.9</span> kB]
Get:<span style="color: #ae81ff;">18</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 libsensors<span style="color: #f92672;">-</span>config all <span style="color: #ae81ff;">1</span>:<span style="color: #ae81ff;">3.6.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">7</span> [<span style="color: #ae81ff;">32.3</span> kB]
Get:<span style="color: #ae81ff;">19</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 libsensors5 arm64 <span style="color: #ae81ff;">1</span>:<span style="color: #ae81ff;">3.6.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">7</span> [<span style="color: #ae81ff;">51.2</span> kB]
Get:<span style="color: #ae81ff;">20</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 libsnmp<span style="color: #f92672;">-</span>base all <span style="color: #ae81ff;">5.9</span><span style="color: #f92672;">+</span>dfsg<span style="color: #f92672;">-</span><span style="color: #ae81ff;">4</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">1</span>,<span style="color: #ae81ff;">736</span> kB]
Get:<span style="color: #ae81ff;">21</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 libsnmp40 arm64 <span style="color: #ae81ff;">5.9</span><span style="color: #f92672;">+</span>dfsg<span style="color: #f92672;">-</span><span style="color: #ae81ff;">4</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">2</span>,<span style="color: #ae81ff;">497</span> kB]
Get:<span style="color: #ae81ff;">22</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng all <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">25.9</span> kB]
Get:<span style="color: #ae81ff;">23</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>add<span style="color: #f92672;">-</span>contextual<span style="color: #f92672;">-</span>data arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">40.5</span> kB]
Get:<span style="color: #ae81ff;">24</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>amqp arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">48.8</span> kB]
Get:<span style="color: #ae81ff;">25</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>examples arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">57.3</span> kB]
Get:<span style="color: #ae81ff;">26</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>extra all <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">35.7</span> kB]
Get:<span style="color: #ae81ff;">27</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>geoip2 arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">36.9</span> kB]
Get:<span style="color: #ae81ff;">28</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>graphite arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">29.4</span> kB]
Get:<span style="color: #ae81ff;">29</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>http arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">50.5</span> kB]
Get:<span style="color: #ae81ff;">30</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>python arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">69.9</span> kB]
Get:<span style="color: #ae81ff;">31</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>rdkafka arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">41.5</span> kB]
Get:<span style="color: #ae81ff;">32</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>redis arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">37.6</span> kB]
Get:<span style="color: #ae81ff;">33</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>riemann arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">40.1</span> kB]
Get:<span style="color: #ae81ff;">34</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>slog arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">63.3</span> kB]
Get:<span style="color: #ae81ff;">35</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>smtp arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">38.0</span> kB]
Get:<span style="color: #ae81ff;">36</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>snmp arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">42.5</span> kB]
Get:<span style="color: #ae81ff;">37</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>stomp arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">39.1</span> kB]
Get:<span style="color: #ae81ff;">38</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>xml<span style="color: #f92672;">-</span>parser arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">34.7</span> kB]
Get:<span style="color: #ae81ff;">39</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>getent arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">29.5</span> kB]
Get:<span style="color: #ae81ff;">40</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>map<span style="color: #f92672;">-</span>value<span style="color: #f92672;">-</span>pairs arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">34.0</span> kB]
Get:<span style="color: #ae81ff;">41</span> http:<span style="color: #f92672;">//</span>deb<span style="color: #f92672;">.</span>debian<span style="color: #f92672;">.</span>org<span style="color: #f92672;">/</span>debian bullseye<span style="color: #f92672;">/</span>main arm64 syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>stardate arm64 <span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1 [<span style="color: #ae81ff;">28.6</span> kB]
Fetched <span style="color: #ae81ff;">7</span>,<span style="color: #ae81ff;">098</span> kB <span style="color: #f92672;">in</span> <span style="color: #ae81ff;">2</span>s (<span style="color: #ae81ff;">3</span>,<span style="color: #ae81ff;">566</span> kB<span style="color: #f92672;">/</span>s)
(Reading database <span style="color: #f92672;">...</span> <span style="color: #ae81ff;">37650</span> files <span style="color: #f92672;">and</span> directories currently installed<span style="color: #f92672;">.</span>)
Removing rsyslog (<span style="color: #ae81ff;">8.2102.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package libbson<span style="color: #f92672;">-</span><span style="color: #ae81ff;">1.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">0.</span>
(Reading database <span style="color: #f92672;">...</span> <span style="color: #ae81ff;">37592</span> files <span style="color: #f92672;">and</span> directories currently installed<span style="color: #f92672;">.</span>)
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">00</span><span style="color: #f92672;">-</span>libbson<span style="color: #f92672;">-</span><span style="color: #ae81ff;">1.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">0_1.17.6</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking libbson<span style="color: #f92672;">-</span><span style="color: #ae81ff;">1.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">0</span> (<span style="color: #ae81ff;">1.17.6</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>) <span style="color: #f92672;">...</span>
Selecting previously unselected package libmongocrypt0:arm64<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">01</span><span style="color: #f92672;">-</span>libmongocrypt0_1<span style="color: #ae81ff;">.1.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking libmongocrypt0:arm64 (<span style="color: #ae81ff;">1.1.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>) <span style="color: #f92672;">...</span>
Selecting previously unselected package libsnappy1v5:arm64<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">02</span><span style="color: #f92672;">-</span>libsnappy1v5_1<span style="color: #ae81ff;">.1.8</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking libsnappy1v5:arm64 (<span style="color: #ae81ff;">1.1.8</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>) <span style="color: #f92672;">...</span>
Selecting previously unselected package libmongoc<span style="color: #f92672;">-</span><span style="color: #ae81ff;">1.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">0.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">03</span><span style="color: #f92672;">-</span>libmongoc<span style="color: #f92672;">-</span><span style="color: #ae81ff;">1.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">0_1.17.6</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking libmongoc<span style="color: #f92672;">-</span><span style="color: #ae81ff;">1.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">0</span> (<span style="color: #ae81ff;">1.17.6</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>) <span style="color: #f92672;">...</span>
Selecting previously unselected package libivykis0:arm64<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">04</span><span style="color: #f92672;">-</span>libivykis0_0<span style="color: #ae81ff;">.42.4</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking libivykis0:arm64 (<span style="color: #ae81ff;">0.42.4</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>) <span style="color: #f92672;">...</span>
Selecting previously unselected package libnet1:arm64<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">05</span><span style="color: #f92672;">-</span>libnet1_1<span style="color: #ae81ff;">.1.6</span><span style="color: #f92672;">+</span>dfsg<span style="color: #f92672;">-</span><span style="color: #ae81ff;">3.1</span>_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking libnet1:arm64 (<span style="color: #ae81ff;">1.1.6</span><span style="color: #f92672;">+</span>dfsg<span style="color: #f92672;">-</span><span style="color: #ae81ff;">3.1</span>) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>core<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">06</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>core_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>core (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>mongodb<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">07</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>mongodb_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>mongodb (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package libdbi1:arm64<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">08</span><span style="color: #f92672;">-</span>libdbi1_0<span style="color: #ae81ff;">.9.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">6</span>_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking libdbi1:arm64 (<span style="color: #ae81ff;">0.9.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">6</span>) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>sql<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">09</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>sql_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>sql (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package libesmtp6<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">10</span><span style="color: #f92672;">-</span>libesmtp6_1<span style="color: #ae81ff;">.0.6</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">4.3</span>_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking libesmtp6 (<span style="color: #ae81ff;">1.0.6</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">4.3</span>) <span style="color: #f92672;">...</span>
Selecting previously unselected package libhiredis0<span style="color: #ae81ff;">.14</span>:arm64<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">11</span><span style="color: #f92672;">-</span>libhiredis0<span style="color: #ae81ff;">.14_0.14.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking libhiredis0<span style="color: #ae81ff;">.14</span>:arm64 (<span style="color: #ae81ff;">0.14.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>) <span style="color: #f92672;">...</span>
Selecting previously unselected package libmaxminddb0:arm64<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">12</span><span style="color: #f92672;">-</span>libmaxminddb0_1<span style="color: #ae81ff;">.5.2</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking libmaxminddb0:arm64 (<span style="color: #ae81ff;">1.5.2</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>) <span style="color: #f92672;">...</span>
Selecting previously unselected package libprotobuf<span style="color: #f92672;">-</span>c1:arm64<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">13</span><span style="color: #f92672;">-</span>libprotobuf<span style="color: #f92672;">-</span>c1_1<span style="color: #ae81ff;">.3.3</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span><span style="color: #f92672;">+</span>b2_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking libprotobuf<span style="color: #f92672;">-</span>c1:arm64 (<span style="color: #ae81ff;">1.3.3</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span><span style="color: #f92672;">+</span>b2) <span style="color: #f92672;">...</span>
Selecting previously unselected package librabbitmq4:arm64<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">14</span><span style="color: #f92672;">-</span>librabbitmq4_0<span style="color: #ae81ff;">.10.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking librabbitmq4:arm64 (<span style="color: #ae81ff;">0.10.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>) <span style="color: #f92672;">...</span>
Selecting previously unselected package librdkafka1:arm64<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">15</span><span style="color: #f92672;">-</span>librdkafka1_1<span style="color: #ae81ff;">.6.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking librdkafka1:arm64 (<span style="color: #ae81ff;">1.6.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>) <span style="color: #f92672;">...</span>
Selecting previously unselected package libriemann<span style="color: #f92672;">-</span>client0:arm64<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">16</span><span style="color: #f92672;">-</span>libriemann<span style="color: #f92672;">-</span>client0_1<span style="color: #ae81ff;">.10.4</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>b2_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking libriemann<span style="color: #f92672;">-</span>client0:arm64 (<span style="color: #ae81ff;">1.10.4</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>b2) <span style="color: #f92672;">...</span>
Selecting previously unselected package libsensors<span style="color: #f92672;">-</span>config<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">17</span><span style="color: #f92672;">-</span>libsensors<span style="color: #f92672;">-</span>config_1<span style="color: #f92672;">%</span><span style="color: #ae81ff;">3</span>a3<span style="color: #ae81ff;">.6.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">7</span>_all<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking libsensors<span style="color: #f92672;">-</span>config (<span style="color: #ae81ff;">1</span>:<span style="color: #ae81ff;">3.6.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">7</span>) <span style="color: #f92672;">...</span>
Selecting previously unselected package libsensors5:arm64<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">18</span><span style="color: #f92672;">-</span>libsensors5_1<span style="color: #f92672;">%</span><span style="color: #ae81ff;">3</span>a3<span style="color: #ae81ff;">.6.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">7</span>_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking libsensors5:arm64 (<span style="color: #ae81ff;">1</span>:<span style="color: #ae81ff;">3.6.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">7</span>) <span style="color: #f92672;">...</span>
Selecting previously unselected package libsnmp<span style="color: #f92672;">-</span>base<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">19</span><span style="color: #f92672;">-</span>libsnmp<span style="color: #f92672;">-</span>base_5<span style="color: #ae81ff;">.9</span><span style="color: #f92672;">+</span>dfsg<span style="color: #f92672;">-</span><span style="color: #ae81ff;">4</span><span style="color: #f92672;">+</span>deb11u1_all<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking libsnmp<span style="color: #f92672;">-</span>base (<span style="color: #ae81ff;">5.9</span><span style="color: #f92672;">+</span>dfsg<span style="color: #f92672;">-</span><span style="color: #ae81ff;">4</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package libsnmp40:arm64<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">20</span><span style="color: #f92672;">-</span>libsnmp40_5<span style="color: #ae81ff;">.9</span><span style="color: #f92672;">+</span>dfsg<span style="color: #f92672;">-</span><span style="color: #ae81ff;">4</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking libsnmp40:arm64 (<span style="color: #ae81ff;">5.9</span><span style="color: #f92672;">+</span>dfsg<span style="color: #f92672;">-</span><span style="color: #ae81ff;">4</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">21</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_all<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>add<span style="color: #f92672;">-</span>contextual<span style="color: #f92672;">-</span>data<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">22</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>add<span style="color: #f92672;">-</span>contextual<span style="color: #f92672;">-</span>data_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>add<span style="color: #f92672;">-</span>contextual<span style="color: #f92672;">-</span>data (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>amqp<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">23</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>amqp_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>amqp (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>examples<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">24</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>examples_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>examples (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>extra<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">25</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>extra_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_all<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>extra (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>geoip2<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">26</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>geoip2_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>geoip2 (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>graphite<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">27</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>graphite_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>graphite (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>http<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">28</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>http_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>http (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>python<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">29</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>python_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>python (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>rdkafka<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">30</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>rdkafka_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>rdkafka (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>redis<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">31</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>redis_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>redis (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>riemann<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">32</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>riemann_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>riemann (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>slog<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">33</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>slog_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>slog (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>smtp<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">34</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>smtp_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>smtp (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>snmp<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">35</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>snmp_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>snmp (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>stomp<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">36</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>stomp_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>stomp (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>xml<span style="color: #f92672;">-</span>parser<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">37</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>xml<span style="color: #f92672;">-</span>parser_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>xml<span style="color: #f92672;">-</span>parser (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>getent<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">38</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>getent_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>getent (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>map<span style="color: #f92672;">-</span>value<span style="color: #f92672;">-</span>pairs<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">39</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>map<span style="color: #f92672;">-</span>value<span style="color: #f92672;">-</span>pairs_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>map<span style="color: #f92672;">-</span>value<span style="color: #f92672;">-</span>pairs (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Selecting previously unselected package syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>stardate<span style="color: #f92672;">.</span>
Preparing to unpack <span style="color: #f92672;">.../</span><span style="color: #ae81ff;">40</span><span style="color: #f92672;">-</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>stardate_3<span style="color: #ae81ff;">.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1_arm64<span style="color: #f92672;">.</span>deb <span style="color: #f92672;">...</span>
Unpacking syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>stardate (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up librabbitmq4:arm64 (<span style="color: #ae81ff;">0.10.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>) <span style="color: #f92672;">...</span>
Setting up libdbi1:arm64 (<span style="color: #ae81ff;">0.9.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">6</span>) <span style="color: #f92672;">...</span>
Setting up libsnmp<span style="color: #f92672;">-</span>base (<span style="color: #ae81ff;">5.9</span><span style="color: #f92672;">+</span>dfsg<span style="color: #f92672;">-</span><span style="color: #ae81ff;">4</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up libmaxminddb0:arm64 (<span style="color: #ae81ff;">1.5.2</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>) <span style="color: #f92672;">...</span>
Setting up libsensors<span style="color: #f92672;">-</span>config (<span style="color: #ae81ff;">1</span>:<span style="color: #ae81ff;">3.6.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">7</span>) <span style="color: #f92672;">...</span>
Setting up libesmtp6 (<span style="color: #ae81ff;">1.0.6</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">4.3</span>) <span style="color: #f92672;">...</span>
Setting up libnet1:arm64 (<span style="color: #ae81ff;">1.1.6</span><span style="color: #f92672;">+</span>dfsg<span style="color: #f92672;">-</span><span style="color: #ae81ff;">3.1</span>) <span style="color: #f92672;">...</span>
Setting up libprotobuf<span style="color: #f92672;">-</span>c1:arm64 (<span style="color: #ae81ff;">1.3.3</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span><span style="color: #f92672;">+</span>b2) <span style="color: #f92672;">...</span>
Setting up libsnappy1v5:arm64 (<span style="color: #ae81ff;">1.1.8</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>) <span style="color: #f92672;">...</span>
Setting up libbson<span style="color: #f92672;">-</span><span style="color: #ae81ff;">1.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">0</span> (<span style="color: #ae81ff;">1.17.6</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>) <span style="color: #f92672;">...</span>
Setting up libivykis0:arm64 (<span style="color: #ae81ff;">0.42.4</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>) <span style="color: #f92672;">...</span>
Setting up libriemann<span style="color: #f92672;">-</span>client0:arm64 (<span style="color: #ae81ff;">1.10.4</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>b2) <span style="color: #f92672;">...</span>
Setting up libsensors5:arm64 (<span style="color: #ae81ff;">1</span>:<span style="color: #ae81ff;">3.6.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">7</span>) <span style="color: #f92672;">...</span>
Setting up librdkafka1:arm64 (<span style="color: #ae81ff;">1.6.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>) <span style="color: #f92672;">...</span>
Setting up libhiredis0<span style="color: #ae81ff;">.14</span>:arm64 (<span style="color: #ae81ff;">0.14.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>) <span style="color: #f92672;">...</span>
Setting up libmongocrypt0:arm64 (<span style="color: #ae81ff;">1.1.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>) <span style="color: #f92672;">...</span>
Setting up libsnmp40:arm64 (<span style="color: #ae81ff;">5.9</span><span style="color: #f92672;">+</span>dfsg<span style="color: #f92672;">-</span><span style="color: #ae81ff;">4</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up libmongoc<span style="color: #f92672;">-</span><span style="color: #ae81ff;">1.0</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">0</span> (<span style="color: #ae81ff;">1.17.6</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">1</span>) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>core (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Created symlink <span style="color: #f92672;">/</span>etc<span style="color: #f92672;">/</span>systemd<span style="color: #f92672;">/</span>system<span style="color: #f92672;">/</span>multi<span style="color: #f92672;">-</span>user<span style="color: #f92672;">.</span>target<span style="color: #f92672;">.</span>wants<span style="color: #f92672;">/</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>service <span style="color: #960050; background-color: #1e0010;">→</span> <span style="color: #f92672;">/</span>lib<span style="color: #f92672;">/</span>systemd<span style="color: #f92672;">/</span>system<span style="color: #f92672;">/</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>service<span style="color: #f92672;">.</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>examples (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>xml<span style="color: #f92672;">-</span>parser (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>stomp (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>riemann (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>stardate (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>geoip2 (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>getent (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>amqp (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>python (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>smtp (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>snmp (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>extra (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>rdkafka (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>graphite (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>add<span style="color: #f92672;">-</span>contextual<span style="color: #f92672;">-</span>data (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>mongodb (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>http (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>slog (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>map<span style="color: #f92672;">-</span>value<span style="color: #f92672;">-</span>pairs (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>sql (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">-</span>mod<span style="color: #f92672;">-</span>redis (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Setting up syslog<span style="color: #f92672;">-</span>ng (<span style="color: #ae81ff;">3.28.1</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span><span style="color: #f92672;">+</span>deb11u1) <span style="color: #f92672;">...</span>
Processing triggers <span style="color: #66d9ef;">for</span> man<span style="color: #f92672;">-</span>db (<span style="color: #ae81ff;">2.9.4</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">2</span>) <span style="color: #f92672;">...</span>
Processing triggers <span style="color: #66d9ef;">for</span> libc<span style="color: #f92672;">-</span>bin (<span style="color: #ae81ff;">2.31</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">13</span><span style="color: #f92672;">+</span>rpt2<span style="color: #f92672;">+</span>rpi1<span style="color: #f92672;">+</span>deb11u8) <span style="color: #f92672;">...</span>
Stderr: debconf: unable to initialize frontend: Dialog
debconf: (TERM <span style="color: #f92672;">is</span> <span style="color: #f92672;">not</span> set, so the dialog frontend <span style="color: #f92672;">is</span> <span style="color: #f92672;">not</span> usable<span style="color: #f92672;">.</span>)
debconf: falling back to frontend: Readline
debconf: unable to initialize frontend: Readline
debconf: (This frontend requires a controlling tty<span style="color: #f92672;">.</span>)
debconf: falling back to frontend: Teletype
dpkg<span style="color: #f92672;">-</span>preconfigure: unable to re<span style="color: #f92672;">-</span>open stdin: 
<span style="color: #f92672;">....</span>
<span style="color: #f92672;">....</span>
</code></pre></div>

</details>

<br />
<!-- raw HTML omitted -->
7. Following the installation of <em>syslog-ng</em> across Nodes 2-7. We verify that the installation was successful by checking the <em>syslog-ng</em> service status.</p>

<p><details>
  <strong>Output of <em>parallel-ssh -h /opt/workers -i &ldquo;systemctl status syslog-ng&rdquo;</em>. Click to expand</strong>
  <div class="highlight"><pre><code class="language-python">root<span style="color: #a6e22e;">@turingpi</span>:<span style="color: #f92672;">~</span><span style="color: #75715e;"># parallel-ssh -h /opt/workers -i "systemctl status syslog-ng" </span>
[<span style="color: #ae81ff;">1</span>] <span style="color: #ae81ff;">14</span>:<span style="color: #ae81ff;">03</span>:<span style="color: #ae81ff;">46</span> [SUCCESS] kemeny
<span style="color: #960050; background-color: #1e0010;">●</span> syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>service <span style="color: #f92672;">-</span> System Logger Daemon
     Loaded: loaded (<span style="color: #f92672;">/</span>lib<span style="color: #f92672;">/</span>systemd<span style="color: #f92672;">/</span>system<span style="color: #f92672;">/</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>service; enabled; vendor preset: enabled)
     Active: active (running) since Thu <span style="color: #ae81ff;">2024</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">03</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">13</span>:<span style="color: #ae81ff;">57</span>:<span style="color: #ae81ff;">01</span> EDT; <span style="color: #ae81ff;">6</span>min ago
       Docs: man:syslog<span style="color: #f92672;">-</span>ng(<span style="color: #ae81ff;">8</span>)
   Main PID: <span style="color: #ae81ff;">28694</span> (syslog<span style="color: #f92672;">-</span>ng)
      Tasks: <span style="color: #ae81ff;">2</span> (limit: <span style="color: #ae81ff;">779</span>)
        CPU: <span style="color: #ae81ff;">40.228</span>s
     CGroup: <span style="color: #f92672;">/</span>system<span style="color: #f92672;">.</span>slice<span style="color: #f92672;">/</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>service
             <span style="color: #960050; background-color: #1e0010;">└─</span><span style="color: #ae81ff;">28694</span> <span style="color: #f92672;">/</span>usr<span style="color: #f92672;">/</span>sbin<span style="color: #f92672;">/</span>syslog<span style="color: #f92672;">-</span>ng <span style="color: #f92672;">-</span>F

Mar <span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">13</span>:<span style="color: #ae81ff;">57</span>:<span style="color: #ae81ff;">00</span> kemeny systemd[<span style="color: #ae81ff;">1</span>]: Starting System Logger Daemon<span style="color: #f92672;">...</span>
Mar <span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">13</span>:<span style="color: #ae81ff;">57</span>:<span style="color: #ae81ff;">01</span> kemeny syslog<span style="color: #f92672;">-</span>ng[<span style="color: #ae81ff;">28694</span>]: DIGEST<span style="color: #f92672;">-</span>MD5 common mech free
Mar <span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">13</span>:<span style="color: #ae81ff;">57</span>:<span style="color: #ae81ff;">01</span> kemeny systemd[<span style="color: #ae81ff;">1</span>]: Started System Logger Daemon<span style="color: #f92672;">.</span>
[<span style="color: #ae81ff;">2</span>] <span style="color: #ae81ff;">14</span>:<span style="color: #ae81ff;">03</span>:<span style="color: #ae81ff;">50</span> [SUCCESS] vonkarman
<span style="color: #960050; background-color: #1e0010;">●</span> syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>service <span style="color: #f92672;">-</span> System Logger Daemon
     Loaded: loaded (<span style="color: #f92672;">/</span>lib<span style="color: #f92672;">/</span>systemd<span style="color: #f92672;">/</span>system<span style="color: #f92672;">/</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>service; enabled; vendor preset: enabled)
     Active: active (running) since Thu <span style="color: #ae81ff;">2024</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">03</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">13</span>:<span style="color: #ae81ff;">57</span>:<span style="color: #ae81ff;">49</span> EDT; <span style="color: #ae81ff;">5</span>min ago
       Docs: man:syslog<span style="color: #f92672;">-</span>ng(<span style="color: #ae81ff;">8</span>)
   Main PID: <span style="color: #ae81ff;">27486</span> (syslog<span style="color: #f92672;">-</span>ng)
      Tasks: <span style="color: #ae81ff;">2</span> (limit: <span style="color: #ae81ff;">779</span>)
        CPU: <span style="color: #ae81ff;">2</span>min <span style="color: #ae81ff;">5.540</span>s
     CGroup: <span style="color: #f92672;">/</span>system<span style="color: #f92672;">.</span>slice<span style="color: #f92672;">/</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>service
             <span style="color: #960050; background-color: #1e0010;">└─</span><span style="color: #ae81ff;">27486</span> <span style="color: #f92672;">/</span>usr<span style="color: #f92672;">/</span>sbin<span style="color: #f92672;">/</span>syslog<span style="color: #f92672;">-</span>ng <span style="color: #f92672;">-</span>F

Mar <span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">13</span>:<span style="color: #ae81ff;">57</span>:<span style="color: #ae81ff;">44</span> vonkarman systemd[<span style="color: #ae81ff;">1</span>]: Starting System Logger Daemon<span style="color: #f92672;">...</span>
Mar <span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">13</span>:<span style="color: #ae81ff;">57</span>:<span style="color: #ae81ff;">46</span> vonkarman syslog<span style="color: #f92672;">-</span>ng[<span style="color: #ae81ff;">27486</span>]: DIGEST<span style="color: #f92672;">-</span>MD5 common mech free
Mar <span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">13</span>:<span style="color: #ae81ff;">57</span>:<span style="color: #ae81ff;">49</span> vonkarman systemd[<span style="color: #ae81ff;">1</span>]: Started System Logger Daemon<span style="color: #f92672;">.</span>
[<span style="color: #ae81ff;">3</span>] <span style="color: #ae81ff;">14</span>:<span style="color: #ae81ff;">03</span>:<span style="color: #ae81ff;">51</span> [SUCCESS] teller
<span style="color: #960050; background-color: #1e0010;">●</span> syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>service <span style="color: #f92672;">-</span> System Logger Daemon
     Loaded: loaded (<span style="color: #f92672;">/</span>lib<span style="color: #f92672;">/</span>systemd<span style="color: #f92672;">/</span>system<span style="color: #f92672;">/</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>service; enabled; vendor preset: enabled)
     Active: active (running) since Thu <span style="color: #ae81ff;">2024</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">03</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">13</span>:<span style="color: #ae81ff;">57</span>:<span style="color: #ae81ff;">39</span> EDT; <span style="color: #ae81ff;">6</span>min ago
       Docs: man:syslog<span style="color: #f92672;">-</span>ng(<span style="color: #ae81ff;">8</span>)
   Main PID: <span style="color: #ae81ff;">24821</span> (syslog<span style="color: #f92672;">-</span>ng)
      Tasks: <span style="color: #ae81ff;">2</span> (limit: <span style="color: #ae81ff;">779</span>)
        CPU: <span style="color: #ae81ff;">2</span>min <span style="color: #ae81ff;">262</span>ms
     CGroup: <span style="color: #f92672;">/</span>system<span style="color: #f92672;">.</span>slice<span style="color: #f92672;">/</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>service
             <span style="color: #960050; background-color: #1e0010;">└─</span><span style="color: #ae81ff;">24821</span> <span style="color: #f92672;">/</span>usr<span style="color: #f92672;">/</span>sbin<span style="color: #f92672;">/</span>syslog<span style="color: #f92672;">-</span>ng <span style="color: #f92672;">-</span>F

Mar <span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">13</span>:<span style="color: #ae81ff;">57</span>:<span style="color: #ae81ff;">38</span> teller systemd[<span style="color: #ae81ff;">1</span>]: Starting System Logger Daemon<span style="color: #f92672;">...</span>
Mar <span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">13</span>:<span style="color: #ae81ff;">57</span>:<span style="color: #ae81ff;">38</span> teller syslog<span style="color: #f92672;">-</span>ng[<span style="color: #ae81ff;">24821</span>]: DIGEST<span style="color: #f92672;">-</span>MD5 common mech free
Mar <span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">13</span>:<span style="color: #ae81ff;">57</span>:<span style="color: #ae81ff;">39</span> teller systemd[<span style="color: #ae81ff;">1</span>]: Started System Logger Daemon<span style="color: #f92672;">.</span>
[<span style="color: #ae81ff;">4</span>] <span style="color: #ae81ff;">14</span>:<span style="color: #ae81ff;">03</span>:<span style="color: #ae81ff;">53</span> [SUCCESS] neumann
<span style="color: #960050; background-color: #1e0010;">●</span> syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>service <span style="color: #f92672;">-</span> System Logger Daemon
     Loaded: loaded (<span style="color: #f92672;">/</span>lib<span style="color: #f92672;">/</span>systemd<span style="color: #f92672;">/</span>system<span style="color: #f92672;">/</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>service; enabled; vendor preset: enabled)
     Active: active (running) since Thu <span style="color: #ae81ff;">2024</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">03</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">13</span>:<span style="color: #ae81ff;">57</span>:<span style="color: #ae81ff;">39</span> EDT; <span style="color: #ae81ff;">6</span>min ago
       Docs: man:syslog<span style="color: #f92672;">-</span>ng(<span style="color: #ae81ff;">8</span>)
   Main PID: <span style="color: #ae81ff;">27734</span> (syslog<span style="color: #f92672;">-</span>ng)
      Tasks: <span style="color: #ae81ff;">2</span> (limit: <span style="color: #ae81ff;">779</span>)
        CPU: <span style="color: #ae81ff;">1</span>min <span style="color: #ae81ff;">43.504</span>s
     CGroup: <span style="color: #f92672;">/</span>system<span style="color: #f92672;">.</span>slice<span style="color: #f92672;">/</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>service
             <span style="color: #960050; background-color: #1e0010;">└─</span><span style="color: #ae81ff;">27734</span> <span style="color: #f92672;">/</span>usr<span style="color: #f92672;">/</span>sbin<span style="color: #f92672;">/</span>syslog<span style="color: #f92672;">-</span>ng <span style="color: #f92672;">-</span>F

Mar <span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">13</span>:<span style="color: #ae81ff;">57</span>:<span style="color: #ae81ff;">38</span> neumann systemd[<span style="color: #ae81ff;">1</span>]: Starting System Logger Daemon<span style="color: #f92672;">...</span>
Mar <span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">13</span>:<span style="color: #ae81ff;">57</span>:<span style="color: #ae81ff;">38</span> neumann syslog<span style="color: #f92672;">-</span>ng[<span style="color: #ae81ff;">27734</span>]: DIGEST<span style="color: #f92672;">-</span>MD5 common mech free
Mar <span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">13</span>:<span style="color: #ae81ff;">57</span>:<span style="color: #ae81ff;">39</span> neumann systemd[<span style="color: #ae81ff;">1</span>]: Started System Logger Daemon<span style="color: #f92672;">.</span>
[<span style="color: #ae81ff;">5</span>] <span style="color: #ae81ff;">14</span>:<span style="color: #ae81ff;">03</span>:<span style="color: #ae81ff;">53</span> [SUCCESS] wigner
<span style="color: #960050; background-color: #1e0010;">●</span> syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>service <span style="color: #f92672;">-</span> System Logger Daemon
     Loaded: loaded (<span style="color: #f92672;">/</span>lib<span style="color: #f92672;">/</span>systemd<span style="color: #f92672;">/</span>system<span style="color: #f92672;">/</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>service; enabled; vendor preset: enabled)
     Active: active (running) since Thu <span style="color: #ae81ff;">2024</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">03</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">13</span>:<span style="color: #ae81ff;">57</span>:<span style="color: #ae81ff;">37</span> EDT; <span style="color: #ae81ff;">6</span>min ago
       Docs: man:syslog<span style="color: #f92672;">-</span>ng(<span style="color: #ae81ff;">8</span>)
   Main PID: <span style="color: #ae81ff;">27512</span> (syslog<span style="color: #f92672;">-</span>ng)
      Tasks: <span style="color: #ae81ff;">2</span> (limit: <span style="color: #ae81ff;">779</span>)
        CPU: <span style="color: #ae81ff;">1</span>min <span style="color: #ae81ff;">49.643</span>s
     CGroup: <span style="color: #f92672;">/</span>system<span style="color: #f92672;">.</span>slice<span style="color: #f92672;">/</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>service
             <span style="color: #960050; background-color: #1e0010;">└─</span><span style="color: #ae81ff;">27512</span> <span style="color: #f92672;">/</span>usr<span style="color: #f92672;">/</span>sbin<span style="color: #f92672;">/</span>syslog<span style="color: #f92672;">-</span>ng <span style="color: #f92672;">-</span>F

Mar <span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">13</span>:<span style="color: #ae81ff;">57</span>:<span style="color: #ae81ff;">36</span> wigner systemd[<span style="color: #ae81ff;">1</span>]: Starting System Logger Daemon<span style="color: #f92672;">...</span>
Mar <span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">13</span>:<span style="color: #ae81ff;">57</span>:<span style="color: #ae81ff;">36</span> wigner syslog<span style="color: #f92672;">-</span>ng[<span style="color: #ae81ff;">27512</span>]: DIGEST<span style="color: #f92672;">-</span>MD5 common mech free
Mar <span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">13</span>:<span style="color: #ae81ff;">57</span>:<span style="color: #ae81ff;">37</span> wigner systemd[<span style="color: #ae81ff;">1</span>]: Started System Logger Daemon<span style="color: #f92672;">.</span>
[<span style="color: #ae81ff;">6</span>] <span style="color: #ae81ff;">14</span>:<span style="color: #ae81ff;">03</span>:<span style="color: #ae81ff;">57</span> [SUCCESS] szilard
<span style="color: #960050; background-color: #1e0010;">●</span> syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>service <span style="color: #f92672;">-</span> System Logger Daemon
     Loaded: loaded (<span style="color: #f92672;">/</span>lib<span style="color: #f92672;">/</span>systemd<span style="color: #f92672;">/</span>system<span style="color: #f92672;">/</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>service; enabled; vendor preset: enabled)
     Active: active (running) since Thu <span style="color: #ae81ff;">2024</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">03</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">13</span>:<span style="color: #ae81ff;">57</span>:<span style="color: #ae81ff;">35</span> EDT; <span style="color: #ae81ff;">6</span>min ago
       Docs: man:syslog<span style="color: #f92672;">-</span>ng(<span style="color: #ae81ff;">8</span>)
   Main PID: <span style="color: #ae81ff;">24136</span> (syslog<span style="color: #f92672;">-</span>ng)
      Tasks: <span style="color: #ae81ff;">5</span> (limit: <span style="color: #ae81ff;">779</span>)
        CPU: <span style="color: #ae81ff;">2</span>min <span style="color: #ae81ff;">10.257</span>s
     CGroup: <span style="color: #f92672;">/</span>system<span style="color: #f92672;">.</span>slice<span style="color: #f92672;">/</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>service
             <span style="color: #960050; background-color: #1e0010;">└─</span><span style="color: #ae81ff;">24136</span> <span style="color: #f92672;">/</span>usr<span style="color: #f92672;">/</span>sbin<span style="color: #f92672;">/</span>syslog<span style="color: #f92672;">-</span>ng <span style="color: #f92672;">-</span>F

Mar <span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">13</span>:<span style="color: #ae81ff;">57</span>:<span style="color: #ae81ff;">34</span> szilard systemd[<span style="color: #ae81ff;">1</span>]: Starting System Logger Daemon<span style="color: #f92672;">...</span>
Mar <span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">13</span>:<span style="color: #ae81ff;">57</span>:<span style="color: #ae81ff;">34</span> szilard syslog<span style="color: #f92672;">-</span>ng[<span style="color: #ae81ff;">24136</span>]: DIGEST<span style="color: #f92672;">-</span>MD5 common mech free
Mar <span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">13</span>:<span style="color: #ae81ff;">57</span>:<span style="color: #ae81ff;">35</span> szilard systemd[<span style="color: #ae81ff;">1</span>]: Started System Logger Daemon<span style="color: #f92672;">.</span>
</code></pre></div>

</details>

<br />
<!-- raw HTML omitted -->
8. Create the  configuration file <em>send.conf</em> in <em>/opt</em> on host <em>turingpi</em>. Note that <em>/opt</em> is an NFS export on <em>turingpi</em> and is NFS mounted by all of the compute nodes. This file will set the HOST field to the local hostname for log messages that are sent. This in done in the subsequent steps where <em>“placeholder”</em> will be replaced using a <em>sed</em> operation with the local hostname. Additionally, a data source <em>s_hpc</em> is defined which will scan <em>/opt/ibm/lsf/log</em> for the presence of LSF daemon logfiles.</p>

<div class="highlight"><pre><code class="language-plaintext"> 
root@turingpi:/# cat /opt/send.conf
rewrite r_host { set("placeholder", value("HOST")); };

destination d_net {
  syslog("turingpi" port(601));
};
source s_hpc {
  wildcard-file(
      base-dir("/opt/ibm/lsf/log")
      filename-pattern("*.log.*")
      recursive(no)
      follow-freq(1)
  );
};
log {
  source(s_src);
  source(s_hpc);
  rewrite(r_host); 
  destination(d_net);
};</code></pre></div>

<ol start="9">
<li>On Nodes 2-7, copy the file <em>/opt/send.conf</em> to <em>/etc/syslog-ng/conf.d/send.conf</em>.</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext"> 
root@turingpi:/# parallel-ssh -h /opt/workers -i "cp /opt/send.conf /etc/syslog-ng/conf.d" 
[1] 14:19:29 [SUCCESS] kemeny
[2] 14:19:30 [SUCCESS] vonkarman
[3] 14:19:30 [SUCCESS] wigner
[4] 14:19:30 [SUCCESS] szilard
[5] 14:19:30 [SUCCESS] teller
[6] 14:19:31 [SUCCESS] neumann</code></pre></div>

<ol start="10">
<li>Using <em>sed</em>, replace the <em>“placeholder”</em> string in <em>/etc/syslog-ng/conf.d/send.conf</em> with the local hostname. And we also double check that the change was correctly made.</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext"> 
root@turingpi:/# parallel-ssh -h /opt/workers -i 'HOST=`hostname`; sed -i "s/placeholder/$HOST/g" /etc/syslog-ng/conf.d/send.conf' 
[1] 14:38:09 [SUCCESS] kemeny
[2] 14:38:09 [SUCCESS] teller
[3] 14:38:09 [SUCCESS] vonkarman
[4] 14:38:09 [SUCCESS] wigner
[5] 14:38:09 [SUCCESS] neumann
[6] 14:38:09 [SUCCESS] szilard</code></pre></div>

<p><details>
  <strong>Output of <em>parallel-ssh -h /opt/workers -i &ldquo;cat /etc/syslog-ng/conf.d/send.conf&rdquo;</em>. Click to expand</strong>
  <div class="highlight"><pre><code class="language-python">root<span style="color: #a6e22e;">@turingpi</span>:<span style="color: #f92672;">/</span><span style="color: #75715e;"># parallel-ssh -h /opt/workers -i "cat /etc/syslog-ng/conf.d/send.conf" [1] 14:38:33 [SUCCESS] kemeny</span>
rewrite r_host { set(<span style="color: #e6db74;">"kemeny"</span>, value(<span style="color: #e6db74;">"HOST"</span>)); };

destination d_net {
  syslog(<span style="color: #e6db74;">"turingpi"</span> port(<span style="color: #ae81ff;">601</span>));
};
source s_hpc {
  wildcard<span style="color: #f92672;">-</span>file(
      base<span style="color: #f92672;">-</span>dir(<span style="color: #e6db74;">"/opt/ibm/lsf/log"</span>)
      filename<span style="color: #f92672;">-</span>pattern(<span style="color: #e6db74;">"*.log.*"</span>)
      recursive(no)
      follow<span style="color: #f92672;">-</span>freq(<span style="color: #ae81ff;">1</span>)
  );
};
log {
  source(s_sys);
  source(s_hpc);
  rewrite(r_host); 
  destination(d_net);
};

[<span style="color: #ae81ff;">2</span>] <span style="color: #ae81ff;">14</span>:<span style="color: #ae81ff;">38</span>:<span style="color: #ae81ff;">33</span> [SUCCESS] teller
rewrite r_host { set(<span style="color: #e6db74;">"teller"</span>, value(<span style="color: #e6db74;">"HOST"</span>)); };

destination d_net {
  syslog(<span style="color: #e6db74;">"turingpi"</span> port(<span style="color: #ae81ff;">601</span>));
};
source s_hpc {
  wildcard<span style="color: #f92672;">-</span>file(
      base<span style="color: #f92672;">-</span>dir(<span style="color: #e6db74;">"/opt/ibm/lsf/log"</span>)
      filename<span style="color: #f92672;">-</span>pattern(<span style="color: #e6db74;">"*.log.*"</span>)
      recursive(no)
      follow<span style="color: #f92672;">-</span>freq(<span style="color: #ae81ff;">1</span>)
  );
};
log {
  source(s_sys);
  source(s_hpc);
  rewrite(r_host); 
  destination(d_net);
};

[<span style="color: #ae81ff;">3</span>] <span style="color: #ae81ff;">14</span>:<span style="color: #ae81ff;">38</span>:<span style="color: #ae81ff;">33</span> [SUCCESS] neumann
rewrite r_host { set(<span style="color: #e6db74;">"neumann"</span>, value(<span style="color: #e6db74;">"HOST"</span>)); };

destination d_net {
  syslog(<span style="color: #e6db74;">"turingpi"</span> port(<span style="color: #ae81ff;">601</span>));
};
source s_hpc {
  wildcard<span style="color: #f92672;">-</span>file(
      base<span style="color: #f92672;">-</span>dir(<span style="color: #e6db74;">"/opt/ibm/lsf/log"</span>)
      filename<span style="color: #f92672;">-</span>pattern(<span style="color: #e6db74;">"*.log.*"</span>)
      recursive(no)
      follow<span style="color: #f92672;">-</span>freq(<span style="color: #ae81ff;">1</span>)
  );
};
log {
  source(s_sys);
  source(s_hpc);
  rewrite(r_host); 
  destination(d_net);
};

[<span style="color: #ae81ff;">4</span>] <span style="color: #ae81ff;">14</span>:<span style="color: #ae81ff;">38</span>:<span style="color: #ae81ff;">33</span> [SUCCESS] szilard
rewrite r_host { set(<span style="color: #e6db74;">"szilard"</span>, value(<span style="color: #e6db74;">"HOST"</span>)); };

destination d_net {
  syslog(<span style="color: #e6db74;">"turingpi"</span> port(<span style="color: #ae81ff;">601</span>));
};
source s_hpc {
  wildcard<span style="color: #f92672;">-</span>file(
      base<span style="color: #f92672;">-</span>dir(<span style="color: #e6db74;">"/opt/ibm/lsf/log"</span>)
      filename<span style="color: #f92672;">-</span>pattern(<span style="color: #e6db74;">"*.log.*"</span>)
      recursive(no)
      follow<span style="color: #f92672;">-</span>freq(<span style="color: #ae81ff;">1</span>)
  );
};
log {
  source(s_sys);
  source(s_hpc);
  rewrite(r_host); 
  destination(d_net);
};

[<span style="color: #ae81ff;">5</span>] <span style="color: #ae81ff;">14</span>:<span style="color: #ae81ff;">38</span>:<span style="color: #ae81ff;">33</span> [SUCCESS] wigner
rewrite r_host { set(<span style="color: #e6db74;">"wigner"</span>, value(<span style="color: #e6db74;">"HOST"</span>)); };

destination d_net {
  syslog(<span style="color: #e6db74;">"turingpi"</span> port(<span style="color: #ae81ff;">601</span>));
};
source s_hpc {
  wildcard<span style="color: #f92672;">-</span>file(
      base<span style="color: #f92672;">-</span>dir(<span style="color: #e6db74;">"/opt/ibm/lsf/log"</span>)
      filename<span style="color: #f92672;">-</span>pattern(<span style="color: #e6db74;">"*.log.*"</span>)
      recursive(no)
      follow<span style="color: #f92672;">-</span>freq(<span style="color: #ae81ff;">1</span>)
  );
};
log {
  source(s_sys);
  source(s_hpc);
  rewrite(r_host); 
  destination(d_net);
};

[<span style="color: #ae81ff;">6</span>] <span style="color: #ae81ff;">14</span>:<span style="color: #ae81ff;">38</span>:<span style="color: #ae81ff;">33</span> [SUCCESS] vonkarman
rewrite r_host { set(<span style="color: #e6db74;">"vonkarman"</span>, value(<span style="color: #e6db74;">"HOST"</span>)); };

destination d_net {
  syslog(<span style="color: #e6db74;">"turingpi"</span> port(<span style="color: #ae81ff;">601</span>));
};
source s_hpc {
  wildcard<span style="color: #f92672;">-</span>file(
      base<span style="color: #f92672;">-</span>dir(<span style="color: #e6db74;">"/opt/ibm/lsf/log"</span>)
      filename<span style="color: #f92672;">-</span>pattern(<span style="color: #e6db74;">"*.log.*"</span>)
      recursive(no)
      follow<span style="color: #f92672;">-</span>freq(<span style="color: #ae81ff;">1</span>)
  );
};
log {
  source(s_sys);
  source(s_hpc);
  rewrite(r_host); 
  destination(d_net);
};
</code></pre></div>

</details>

<br />
<!-- raw HTML omitted -->
11. Finally, <em>syslog-ng</em> is restarted on Nodes 2-7 and the status of the service is checked to ensure that there are no errors.</p>

<div class="highlight"><pre><code class="language-plaintext"> 
root@turingpi:/opt# parallel-ssh -h /opt/workers -i "systemctl restart syslog-ng" 
[1] 14:49:03 [SUCCESS] kemeny
[2] 14:49:05 [SUCCESS] szilard
[3] 14:49:06 [SUCCESS] vonkarman
[4] 14:49:06 [SUCCESS] neumann
[5] 14:49:06 [SUCCESS] teller
[6] 14:49:07 [SUCCESS] wigner</code></pre></div>

<p><details>
  <strong>Output of <em>parallel-ssh -h /opt/workers -i &ldquo;systemctl status syslog-ng&rdquo;</em>. Click to expand</strong>
  <div class="highlight"><pre><code class="language-python">root<span style="color: #a6e22e;">@turingpi</span>:<span style="color: #f92672;">/</span>opt<span style="color: #75715e;"># parallel-ssh -h /opt/workers -i "systemctl status syslog-ng" </span>
[<span style="color: #ae81ff;">1</span>] <span style="color: #ae81ff;">14</span>:<span style="color: #ae81ff;">49</span>:<span style="color: #ae81ff;">31</span> [SUCCESS] kemeny
<span style="color: #960050; background-color: #1e0010;">●</span> syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>service <span style="color: #f92672;">-</span> System Logger Daemon
     Loaded: loaded (<span style="color: #f92672;">/</span>lib<span style="color: #f92672;">/</span>systemd<span style="color: #f92672;">/</span>system<span style="color: #f92672;">/</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>service; enabled; vendor preset: enabled)
     Active: active (running) since Thu <span style="color: #ae81ff;">2024</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">03</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">14</span>:<span style="color: #ae81ff;">49</span>:<span style="color: #ae81ff;">03</span> EDT; <span style="color: #ae81ff;">28</span>s ago
       Docs: man:syslog<span style="color: #f92672;">-</span>ng(<span style="color: #ae81ff;">8</span>)
   Main PID: <span style="color: #ae81ff;">34982</span> (syslog<span style="color: #f92672;">-</span>ng)
      Tasks: <span style="color: #ae81ff;">2</span> (limit: <span style="color: #ae81ff;">779</span>)
        CPU: <span style="color: #ae81ff;">398</span>ms
     CGroup: <span style="color: #f92672;">/</span>system<span style="color: #f92672;">.</span>slice<span style="color: #f92672;">/</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>service
             <span style="color: #960050; background-color: #1e0010;">└─</span><span style="color: #ae81ff;">34982</span> <span style="color: #f92672;">/</span>usr<span style="color: #f92672;">/</span>sbin<span style="color: #f92672;">/</span>syslog<span style="color: #f92672;">-</span>ng <span style="color: #f92672;">-</span>F

Mar <span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">14</span>:<span style="color: #ae81ff;">49</span>:<span style="color: #ae81ff;">02</span> kemeny systemd[<span style="color: #ae81ff;">1</span>]: Starting System Logger Daemon<span style="color: #f92672;">...</span>
Mar <span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">14</span>:<span style="color: #ae81ff;">49</span>:<span style="color: #ae81ff;">02</span> kemeny syslog<span style="color: #f92672;">-</span>ng[<span style="color: #ae81ff;">34982</span>]: DIGEST<span style="color: #f92672;">-</span>MD5 common mech free
Mar <span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">14</span>:<span style="color: #ae81ff;">49</span>:<span style="color: #ae81ff;">03</span> kemeny systemd[<span style="color: #ae81ff;">1</span>]: Started System Logger Daemon<span style="color: #f92672;">.</span>
[<span style="color: #ae81ff;">2</span>] <span style="color: #ae81ff;">14</span>:<span style="color: #ae81ff;">49</span>:<span style="color: #ae81ff;">33</span> [SUCCESS] vonkarman
<span style="color: #960050; background-color: #1e0010;">●</span> syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>service <span style="color: #f92672;">-</span> System Logger Daemon
     Loaded: loaded (<span style="color: #f92672;">/</span>lib<span style="color: #f92672;">/</span>systemd<span style="color: #f92672;">/</span>system<span style="color: #f92672;">/</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>service; enabled; vendor preset: enabled)
     Active: active (running) since Thu <span style="color: #ae81ff;">2024</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">03</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">14</span>:<span style="color: #ae81ff;">49</span>:<span style="color: #ae81ff;">06</span> EDT; <span style="color: #ae81ff;">25</span>s ago
       Docs: man:syslog<span style="color: #f92672;">-</span>ng(<span style="color: #ae81ff;">8</span>)
   Main PID: <span style="color: #ae81ff;">33710</span> (syslog<span style="color: #f92672;">-</span>ng)
      Tasks: <span style="color: #ae81ff;">2</span> (limit: <span style="color: #ae81ff;">779</span>)
        CPU: <span style="color: #ae81ff;">934</span>ms
     CGroup: <span style="color: #f92672;">/</span>system<span style="color: #f92672;">.</span>slice<span style="color: #f92672;">/</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>service
             <span style="color: #960050; background-color: #1e0010;">└─</span><span style="color: #ae81ff;">33710</span> <span style="color: #f92672;">/</span>usr<span style="color: #f92672;">/</span>sbin<span style="color: #f92672;">/</span>syslog<span style="color: #f92672;">-</span>ng <span style="color: #f92672;">-</span>F

Mar <span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">14</span>:<span style="color: #ae81ff;">49</span>:<span style="color: #ae81ff;">03</span> vonkarman systemd[<span style="color: #ae81ff;">1</span>]: Starting System Logger Daemon<span style="color: #f92672;">...</span>
Mar <span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">14</span>:<span style="color: #ae81ff;">49</span>:<span style="color: #ae81ff;">03</span> vonkarman syslog<span style="color: #f92672;">-</span>ng[<span style="color: #ae81ff;">33710</span>]: DIGEST<span style="color: #f92672;">-</span>MD5 common mech free
Mar <span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">14</span>:<span style="color: #ae81ff;">49</span>:<span style="color: #ae81ff;">06</span> vonkarman systemd[<span style="color: #ae81ff;">1</span>]: Started System Logger Daemon<span style="color: #f92672;">.</span>
[<span style="color: #ae81ff;">3</span>] <span style="color: #ae81ff;">14</span>:<span style="color: #ae81ff;">49</span>:<span style="color: #ae81ff;">33</span> [SUCCESS] neumann
<span style="color: #960050; background-color: #1e0010;">●</span> syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>service <span style="color: #f92672;">-</span> System Logger Daemon
     Loaded: loaded (<span style="color: #f92672;">/</span>lib<span style="color: #f92672;">/</span>systemd<span style="color: #f92672;">/</span>system<span style="color: #f92672;">/</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>service; enabled; vendor preset: enabled)
     Active: active (running) since Thu <span style="color: #ae81ff;">2024</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">03</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">14</span>:<span style="color: #ae81ff;">49</span>:<span style="color: #ae81ff;">06</span> EDT; <span style="color: #ae81ff;">25</span>s ago
       Docs: man:syslog<span style="color: #f92672;">-</span>ng(<span style="color: #ae81ff;">8</span>)
   Main PID: <span style="color: #ae81ff;">34000</span> (syslog<span style="color: #f92672;">-</span>ng)
      Tasks: <span style="color: #ae81ff;">2</span> (limit: <span style="color: #ae81ff;">779</span>)
        CPU: <span style="color: #ae81ff;">959</span>ms
     CGroup: <span style="color: #f92672;">/</span>system<span style="color: #f92672;">.</span>slice<span style="color: #f92672;">/</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>service
             <span style="color: #960050; background-color: #1e0010;">└─</span><span style="color: #ae81ff;">34000</span> <span style="color: #f92672;">/</span>usr<span style="color: #f92672;">/</span>sbin<span style="color: #f92672;">/</span>syslog<span style="color: #f92672;">-</span>ng <span style="color: #f92672;">-</span>F

Mar <span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">14</span>:<span style="color: #ae81ff;">49</span>:<span style="color: #ae81ff;">03</span> neumann systemd[<span style="color: #ae81ff;">1</span>]: Starting System Logger Daemon<span style="color: #f92672;">...</span>
Mar <span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">14</span>:<span style="color: #ae81ff;">49</span>:<span style="color: #ae81ff;">03</span> neumann syslog<span style="color: #f92672;">-</span>ng[<span style="color: #ae81ff;">34000</span>]: DIGEST<span style="color: #f92672;">-</span>MD5 common mech free
Mar <span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">14</span>:<span style="color: #ae81ff;">49</span>:<span style="color: #ae81ff;">06</span> neumann systemd[<span style="color: #ae81ff;">1</span>]: Started System Logger Daemon<span style="color: #f92672;">.</span>
[<span style="color: #ae81ff;">4</span>] <span style="color: #ae81ff;">14</span>:<span style="color: #ae81ff;">49</span>:<span style="color: #ae81ff;">33</span> [SUCCESS] wigner
<span style="color: #960050; background-color: #1e0010;">●</span> syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>service <span style="color: #f92672;">-</span> System Logger Daemon
     Loaded: loaded (<span style="color: #f92672;">/</span>lib<span style="color: #f92672;">/</span>systemd<span style="color: #f92672;">/</span>system<span style="color: #f92672;">/</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>service; enabled; vendor preset: enabled)
     Active: active (running) since Thu <span style="color: #ae81ff;">2024</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">03</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">14</span>:<span style="color: #ae81ff;">49</span>:<span style="color: #ae81ff;">07</span> EDT; <span style="color: #ae81ff;">25</span>s ago
       Docs: man:syslog<span style="color: #f92672;">-</span>ng(<span style="color: #ae81ff;">8</span>)
   Main PID: <span style="color: #ae81ff;">33941</span> (syslog<span style="color: #f92672;">-</span>ng)
      Tasks: <span style="color: #ae81ff;">2</span> (limit: <span style="color: #ae81ff;">779</span>)
        CPU: <span style="color: #ae81ff;">1.115</span>s
     CGroup: <span style="color: #f92672;">/</span>system<span style="color: #f92672;">.</span>slice<span style="color: #f92672;">/</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>service
             <span style="color: #960050; background-color: #1e0010;">└─</span><span style="color: #ae81ff;">33941</span> <span style="color: #f92672;">/</span>usr<span style="color: #f92672;">/</span>sbin<span style="color: #f92672;">/</span>syslog<span style="color: #f92672;">-</span>ng <span style="color: #f92672;">-</span>F

Mar <span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">14</span>:<span style="color: #ae81ff;">49</span>:<span style="color: #ae81ff;">03</span> wigner systemd[<span style="color: #ae81ff;">1</span>]: Starting System Logger Daemon<span style="color: #f92672;">...</span>
Mar <span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">14</span>:<span style="color: #ae81ff;">49</span>:<span style="color: #ae81ff;">04</span> wigner syslog<span style="color: #f92672;">-</span>ng[<span style="color: #ae81ff;">33941</span>]: DIGEST<span style="color: #f92672;">-</span>MD5 common mech free
Mar <span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">14</span>:<span style="color: #ae81ff;">49</span>:<span style="color: #ae81ff;">07</span> wigner systemd[<span style="color: #ae81ff;">1</span>]: Started System Logger Daemon<span style="color: #f92672;">.</span>
[<span style="color: #ae81ff;">5</span>] <span style="color: #ae81ff;">14</span>:<span style="color: #ae81ff;">49</span>:<span style="color: #ae81ff;">34</span> [SUCCESS] szilard
<span style="color: #960050; background-color: #1e0010;">●</span> syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>service <span style="color: #f92672;">-</span> System Logger Daemon
     Loaded: loaded (<span style="color: #f92672;">/</span>lib<span style="color: #f92672;">/</span>systemd<span style="color: #f92672;">/</span>system<span style="color: #f92672;">/</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>service; enabled; vendor preset: enabled)
     Active: active (running) since Thu <span style="color: #ae81ff;">2024</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">03</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">14</span>:<span style="color: #ae81ff;">49</span>:<span style="color: #ae81ff;">05</span> EDT; <span style="color: #ae81ff;">26</span>s ago
       Docs: man:syslog<span style="color: #f92672;">-</span>ng(<span style="color: #ae81ff;">8</span>)
   Main PID: <span style="color: #ae81ff;">30348</span> (syslog<span style="color: #f92672;">-</span>ng)
      Tasks: <span style="color: #ae81ff;">2</span> (limit: <span style="color: #ae81ff;">779</span>)
        CPU: <span style="color: #ae81ff;">816</span>ms
     CGroup: <span style="color: #f92672;">/</span>system<span style="color: #f92672;">.</span>slice<span style="color: #f92672;">/</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>service
             <span style="color: #960050; background-color: #1e0010;">└─</span><span style="color: #ae81ff;">30348</span> <span style="color: #f92672;">/</span>usr<span style="color: #f92672;">/</span>sbin<span style="color: #f92672;">/</span>syslog<span style="color: #f92672;">-</span>ng <span style="color: #f92672;">-</span>F

Mar <span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">14</span>:<span style="color: #ae81ff;">49</span>:<span style="color: #ae81ff;">03</span> szilard systemd[<span style="color: #ae81ff;">1</span>]: Starting System Logger Daemon<span style="color: #f92672;">...</span>
Mar <span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">14</span>:<span style="color: #ae81ff;">49</span>:<span style="color: #ae81ff;">03</span> szilard syslog<span style="color: #f92672;">-</span>ng[<span style="color: #ae81ff;">30348</span>]: DIGEST<span style="color: #f92672;">-</span>MD5 common mech free
Mar <span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">14</span>:<span style="color: #ae81ff;">49</span>:<span style="color: #ae81ff;">05</span> szilard systemd[<span style="color: #ae81ff;">1</span>]: Started System Logger Daemon<span style="color: #f92672;">.</span>
[<span style="color: #ae81ff;">6</span>] <span style="color: #ae81ff;">14</span>:<span style="color: #ae81ff;">49</span>:<span style="color: #ae81ff;">34</span> [SUCCESS] teller
<span style="color: #960050; background-color: #1e0010;">●</span> syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>service <span style="color: #f92672;">-</span> System Logger Daemon
     Loaded: loaded (<span style="color: #f92672;">/</span>lib<span style="color: #f92672;">/</span>systemd<span style="color: #f92672;">/</span>system<span style="color: #f92672;">/</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>service; enabled; vendor preset: enabled)
     Active: active (running) since Thu <span style="color: #ae81ff;">2024</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">03</span><span style="color: #f92672;">-</span><span style="color: #ae81ff;">28</span> <span style="color: #ae81ff;">14</span>:<span style="color: #ae81ff;">49</span>:<span style="color: #ae81ff;">06</span> EDT; <span style="color: #ae81ff;">25</span>s ago
       Docs: man:syslog<span style="color: #f92672;">-</span>ng(<span style="color: #ae81ff;">8</span>)
   Main PID: <span style="color: #ae81ff;">31034</span> (syslog<span style="color: #f92672;">-</span>ng)
      Tasks: <span style="color: #ae81ff;">2</span> (limit: <span style="color: #ae81ff;">779</span>)
        CPU: <span style="color: #ae81ff;">965</span>ms
     CGroup: <span style="color: #f92672;">/</span>system<span style="color: #f92672;">.</span>slice<span style="color: #f92672;">/</span>syslog<span style="color: #f92672;">-</span>ng<span style="color: #f92672;">.</span>service
             <span style="color: #960050; background-color: #1e0010;">└─</span><span style="color: #ae81ff;">31034</span> <span style="color: #f92672;">/</span>usr<span style="color: #f92672;">/</span>sbin<span style="color: #f92672;">/</span>syslog<span style="color: #f92672;">-</span>ng <span style="color: #f92672;">-</span>F
</code></pre></div>

</details>

<br />
<!-- raw HTML omitted -->
<strong>Does it work?</strong></p>

<p>The answer to this question is an emphatic YES!</p>

<p>Let’s begin with a simple test running the <em>logger</em> command on all of the compute nodes, while monitoring <em>/var/log/fromnet</em> on host <em>turingpi</em>.</p>

<div class="highlight"><pre><code class="language-plaintext"> 
root@turingpi:/home/lsfadmin# date; parallel-ssh -h /opt/workers -i 'HOST=`hostname`; logger This is a test from node $HOST. Do not panic!' 
Wed  3 Apr 21:41:45 EDT 2024 
[1] 21:41:46 [SUCCESS] teller 
[2] 21:41:46 [SUCCESS] neumann 
[3] 21:41:46 [SUCCESS] wigner 
[4] 21:41:46 [SUCCESS] kemeny 
[5] 21:41:46 [SUCCESS] szilard 
[6] 21:41:46 [SUCCESS] vonkarman

root@turingpi:/var/log# tail -f fromnet |grep panic 
Apr  3 21:41:46 szilard root[10918]: This is a test from node szilard. Do not panic! 
Apr  3 21:41:46 wigner root[11011]: This is a test from node wigner. Do not panic! 
Apr  3 21:41:46 neumann root[11121]: This is a test from node neumann. Do not panic! 
Apr  3 21:41:46 kemeny root[11029]: This is a test from node kemeny. Do not panic! 
Apr  3 21:41:46 teller root[10875]: This is a test from node teller. Do not panic! 
Apr  3 21:41:46 vonkarman root[10805]: This is a test from node vonkarman. Do not panic!</code></pre></div>

<p>Next, let’s look at whether the LSF logging is also captured. Here we simply restart the LSF daemons on Nodes 2-7 and monitor the <em>/var/log/fromnet</em> file. The full output can be viewed below.</p>

<p><details>
  <strong>Output of <em>tail -f /var/log/fromnet</em>. Click to expand</strong>
  <div class="highlight"><pre><code class="language-python">root<span style="color: #a6e22e;">@turingpi</span>:<span style="color: #f92672;">/</span>var<span style="color: #f92672;">/</span>log<span style="color: #75715e;"># tail -f fromnet </span>
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">41</span>:<span style="color: #ae81ff;">57</span> vonkarman systemd[<span style="color: #ae81ff;">10786</span>]: systemd<span style="color: #f92672;">-</span>exit<span style="color: #f92672;">.</span>service: Succeeded<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">41</span>:<span style="color: #ae81ff;">57</span> vonkarman systemd[<span style="color: #ae81ff;">10786</span>]: Finished Exit the Session<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">41</span>:<span style="color: #ae81ff;">57</span> vonkarman systemd[<span style="color: #ae81ff;">10786</span>]: Reached target Exit the Session<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">41</span>:<span style="color: #ae81ff;">57</span> vonkarman systemd[<span style="color: #ae81ff;">1</span>]: user<span style="color: #f92672;">@</span><span style="color: #ae81ff;">0.</span>service: Succeeded<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">41</span>:<span style="color: #ae81ff;">57</span> vonkarman systemd[<span style="color: #ae81ff;">1</span>]: Stopped User Manager <span style="color: #66d9ef;">for</span> UID <span style="color: #ae81ff;">0.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">41</span>:<span style="color: #ae81ff;">57</span> vonkarman systemd[<span style="color: #ae81ff;">1</span>]: Stopping User Runtime Directory <span style="color: #f92672;">/</span>run<span style="color: #f92672;">/</span>user<span style="color: #f92672;">/</span><span style="color: #ae81ff;">0.</span><span style="color: #f92672;">..</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">41</span>:<span style="color: #ae81ff;">57</span> vonkarman systemd[<span style="color: #ae81ff;">1</span>]: run<span style="color: #f92672;">-</span>user<span style="color: #f92672;">-</span><span style="color: #ae81ff;">0.</span>mount: Succeeded<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">41</span>:<span style="color: #ae81ff;">57</span> vonkarman systemd[<span style="color: #ae81ff;">1</span>]: user<span style="color: #f92672;">-</span>runtime<span style="color: #f92672;">-</span>dir<span style="color: #f92672;">@</span><span style="color: #ae81ff;">0.</span>service: Succeeded<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">41</span>:<span style="color: #ae81ff;">57</span> vonkarman systemd[<span style="color: #ae81ff;">1</span>]: Stopped User Runtime Directory <span style="color: #f92672;">/</span>run<span style="color: #f92672;">/</span>user<span style="color: #f92672;">/</span><span style="color: #ae81ff;">0.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">41</span>:<span style="color: #ae81ff;">57</span> vonkarman systemd[<span style="color: #ae81ff;">1</span>]: Removed slice User Slice of UID <span style="color: #ae81ff;">0.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">30</span> wigner dhcpcd[<span style="color: #ae81ff;">493</span>]: eth0: Router Advertisement <span style="color: #f92672;">from</span> fe80::da58:d7ff:fe00:<span style="color: #ae81ff;">6</span>d83 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">57</span> szilard sshd[<span style="color: #ae81ff;">11234</span>]: Accepted publickey <span style="color: #66d9ef;">for</span> root <span style="color: #f92672;">from</span> <span style="color: #ae81ff;">192.168.1.172</span> port <span style="color: #ae81ff;">52600</span> ssh2: ED25519 S
HA256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">57</span> szilard sshd[<span style="color: #ae81ff;">11234</span>]: pam_unix(sshd:session): session opened <span style="color: #66d9ef;">for</span> user root(uid<span style="color: #f92672;">=</span><span style="color: #ae81ff;">0</span>) by (uid<span style="color: #f92672;">=</span><span style="color: #ae81ff;">0</span>) 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> szilard systemd[<span style="color: #ae81ff;">1</span>]: Created slice User Slice of UID <span style="color: #ae81ff;">0.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> szilard systemd[<span style="color: #ae81ff;">1</span>]: Starting User Runtime Directory <span style="color: #f92672;">/</span>run<span style="color: #f92672;">/</span>user<span style="color: #f92672;">/</span><span style="color: #ae81ff;">0.</span><span style="color: #f92672;">..</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> szilard systemd<span style="color: #f92672;">-</span>logind[<span style="color: #ae81ff;">382</span>]: New session <span style="color: #ae81ff;">30</span> of user root<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> szilard systemd[<span style="color: #ae81ff;">1</span>]: Finished User Runtime Directory <span style="color: #f92672;">/</span>run<span style="color: #f92672;">/</span>user<span style="color: #f92672;">/</span><span style="color: #ae81ff;">0.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> szilard systemd[<span style="color: #ae81ff;">1</span>]: Starting User Manager <span style="color: #66d9ef;">for</span> UID <span style="color: #ae81ff;">0.</span><span style="color: #f92672;">..</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> szilard systemd[<span style="color: #ae81ff;">11237</span>]: pam_unix(systemd<span style="color: #f92672;">-</span>user:session): session opened <span style="color: #66d9ef;">for</span> user root(uid<span style="color: #f92672;">=</span><span style="color: #ae81ff;">0</span>) by
(uid<span style="color: #f92672;">=</span><span style="color: #ae81ff;">0</span>) 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">57</span> wigner sshd[<span style="color: #ae81ff;">11342</span>]: Accepted publickey <span style="color: #66d9ef;">for</span> root <span style="color: #f92672;">from</span> <span style="color: #ae81ff;">192.168.1.172</span> port <span style="color: #ae81ff;">60388</span> ssh2: ED25519 SH
A256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">57</span> wigner sshd[<span style="color: #ae81ff;">11342</span>]: pam_unix(sshd:session): session opened <span style="color: #66d9ef;">for</span> user root(uid<span style="color: #f92672;">=</span><span style="color: #ae81ff;">0</span>) by (uid<span style="color: #f92672;">=</span><span style="color: #ae81ff;">0</span>) 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> wigner systemd[<span style="color: #ae81ff;">1</span>]: Created slice User Slice of UID <span style="color: #ae81ff;">0.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> wigner systemd[<span style="color: #ae81ff;">1</span>]: Starting User Runtime Directory <span style="color: #f92672;">/</span>run<span style="color: #f92672;">/</span>user<span style="color: #f92672;">/</span><span style="color: #ae81ff;">0.</span><span style="color: #f92672;">..</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> wigner systemd<span style="color: #f92672;">-</span>logind[<span style="color: #ae81ff;">383</span>]: New session <span style="color: #ae81ff;">30</span> of user root<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> wigner systemd[<span style="color: #ae81ff;">1</span>]: Finished User Runtime Directory <span style="color: #f92672;">/</span>run<span style="color: #f92672;">/</span>user<span style="color: #f92672;">/</span><span style="color: #ae81ff;">0.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> wigner systemd[<span style="color: #ae81ff;">1</span>]: Starting User Manager <span style="color: #66d9ef;">for</span> UID <span style="color: #ae81ff;">0.</span><span style="color: #f92672;">..</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> wigner systemd[<span style="color: #ae81ff;">11345</span>]: pam_unix(systemd<span style="color: #f92672;">-</span>user:session): session opened <span style="color: #66d9ef;">for</span> user root(uid<span style="color: #f92672;">=</span><span style="color: #ae81ff;">0</span>) by 
(uid<span style="color: #f92672;">=</span><span style="color: #ae81ff;">0</span>) 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">57</span> neumann sshd[<span style="color: #ae81ff;">11436</span>]: Accepted publickey <span style="color: #66d9ef;">for</span> root <span style="color: #f92672;">from</span> <span style="color: #ae81ff;">192.168.1.172</span> port <span style="color: #ae81ff;">55144</span> ssh2: ED25519 S
HA256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">57</span> neumann sshd[<span style="color: #ae81ff;">11436</span>]: pam_unix(sshd:session): session opened <span style="color: #66d9ef;">for</span> user root(uid<span style="color: #f92672;">=</span><span style="color: #ae81ff;">0</span>) by (uid<span style="color: #f92672;">=</span><span style="color: #ae81ff;">0</span>) 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">57</span> neumann systemd[<span style="color: #ae81ff;">1</span>]: Created slice User Slice of UID <span style="color: #ae81ff;">0.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">57</span> neumann systemd[<span style="color: #ae81ff;">1</span>]: Starting User Runtime Directory <span style="color: #f92672;">/</span>run<span style="color: #f92672;">/</span>user<span style="color: #f92672;">/</span><span style="color: #ae81ff;">0.</span><span style="color: #f92672;">..</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> neumann systemd<span style="color: #f92672;">-</span>logind[<span style="color: #ae81ff;">398</span>]: New session <span style="color: #ae81ff;">30</span> of user root<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> neumann systemd[<span style="color: #ae81ff;">1</span>]: Finished User Runtime Directory <span style="color: #f92672;">/</span>run<span style="color: #f92672;">/</span>user<span style="color: #f92672;">/</span><span style="color: #ae81ff;">0.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> neumann systemd[<span style="color: #ae81ff;">1</span>]: Starting User Manager <span style="color: #66d9ef;">for</span> UID <span style="color: #ae81ff;">0.</span><span style="color: #f92672;">..</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> neumann systemd[<span style="color: #ae81ff;">11439</span>]: pam_unix(systemd<span style="color: #f92672;">-</span>user:session): session opened <span style="color: #66d9ef;">for</span> user root(uid<span style="color: #f92672;">=</span><span style="color: #ae81ff;">0</span>) by
(uid<span style="color: #f92672;">=</span><span style="color: #ae81ff;">0</span>) 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">57</span> kemeny sshd[<span style="color: #ae81ff;">11345</span>]: Accepted publickey <span style="color: #66d9ef;">for</span> root <span style="color: #f92672;">from</span> <span style="color: #ae81ff;">192.168.1.172</span> port <span style="color: #ae81ff;">59830</span> ssh2: ED25519 SH
A256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">57</span> kemeny sshd[<span style="color: #ae81ff;">11345</span>]: pam_unix(sshd:session): session opened <span style="color: #66d9ef;">for</span> user root(uid<span style="color: #f92672;">=</span><span style="color: #ae81ff;">0</span>) by (uid<span style="color: #f92672;">=</span><span style="color: #ae81ff;">0</span>) 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> kemeny systemd[<span style="color: #ae81ff;">1</span>]: Created slice User Slice of UID <span style="color: #ae81ff;">0.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> kemeny systemd[<span style="color: #ae81ff;">1</span>]: Starting User Runtime Directory <span style="color: #f92672;">/</span>run<span style="color: #f92672;">/</span>user<span style="color: #f92672;">/</span><span style="color: #ae81ff;">0.</span><span style="color: #f92672;">..</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> kemeny systemd<span style="color: #f92672;">-</span>logind[<span style="color: #ae81ff;">386</span>]: New session <span style="color: #ae81ff;">30</span> of user root<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> kemeny systemd[<span style="color: #ae81ff;">1</span>]: Finished User Runtime Directory <span style="color: #f92672;">/</span>run<span style="color: #f92672;">/</span>user<span style="color: #f92672;">/</span><span style="color: #ae81ff;">0.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> kemeny systemd[<span style="color: #ae81ff;">1</span>]: Starting User Manager <span style="color: #66d9ef;">for</span> UID <span style="color: #ae81ff;">0.</span><span style="color: #f92672;">..</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> kemeny systemd[<span style="color: #ae81ff;">11348</span>]: pam_unix(systemd<span style="color: #f92672;">-</span>user:session): session opened <span style="color: #66d9ef;">for</span> user root(uid<span style="color: #f92672;">=</span><span style="color: #ae81ff;">0</span>) by 
(uid<span style="color: #f92672;">=</span><span style="color: #ae81ff;">0</span>) 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">57</span> teller sshd[<span style="color: #ae81ff;">11189</span>]: Accepted publickey <span style="color: #66d9ef;">for</span> root <span style="color: #f92672;">from</span> <span style="color: #ae81ff;">192.168.1.172</span> port <span style="color: #ae81ff;">35310</span> ssh2: ED25519 SH
A256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">57</span> teller sshd[<span style="color: #ae81ff;">11189</span>]: pam_unix(sshd:session): session opened <span style="color: #66d9ef;">for</span> user root(uid<span style="color: #f92672;">=</span><span style="color: #ae81ff;">0</span>) by (uid<span style="color: #f92672;">=</span><span style="color: #ae81ff;">0</span>) 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> teller systemd[<span style="color: #ae81ff;">1</span>]: Created slice User Slice of UID <span style="color: #ae81ff;">0.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> teller systemd[<span style="color: #ae81ff;">1</span>]: Starting User Runtime Directory <span style="color: #f92672;">/</span>run<span style="color: #f92672;">/</span>user<span style="color: #f92672;">/</span><span style="color: #ae81ff;">0.</span><span style="color: #f92672;">..</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> teller systemd<span style="color: #f92672;">-</span>logind[<span style="color: #ae81ff;">382</span>]: New session <span style="color: #ae81ff;">30</span> of user root<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> teller systemd[<span style="color: #ae81ff;">1</span>]: Finished User Runtime Directory <span style="color: #f92672;">/</span>run<span style="color: #f92672;">/</span>user<span style="color: #f92672;">/</span><span style="color: #ae81ff;">0.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> teller systemd[<span style="color: #ae81ff;">1</span>]: Starting User Manager <span style="color: #66d9ef;">for</span> UID <span style="color: #ae81ff;">0.</span><span style="color: #f92672;">..</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> teller systemd[<span style="color: #ae81ff;">11192</span>]: pam_unix(systemd<span style="color: #f92672;">-</span>user:session): session opened <span style="color: #66d9ef;">for</span> user root(uid<span style="color: #f92672;">=</span><span style="color: #ae81ff;">0</span>) by 
(uid<span style="color: #f92672;">=</span><span style="color: #ae81ff;">0</span>) 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">57</span> vonkarman sshd[<span style="color: #ae81ff;">11118</span>]: Accepted publickey <span style="color: #66d9ef;">for</span> root <span style="color: #f92672;">from</span> <span style="color: #ae81ff;">192.168.1.172</span> port <span style="color: #ae81ff;">48654</span> ssh2: ED25519
SHA256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> vonkarman sshd[<span style="color: #ae81ff;">11118</span>]: pam_unix(sshd:session): session opened <span style="color: #66d9ef;">for</span> user root(uid<span style="color: #f92672;">=</span><span style="color: #ae81ff;">0</span>) by (uid<span style="color: #f92672;">=</span><span style="color: #ae81ff;">0</span>) 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> vonkarman systemd[<span style="color: #ae81ff;">1</span>]: Created slice User Slice of UID <span style="color: #ae81ff;">0.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> vonkarman systemd[<span style="color: #ae81ff;">1</span>]: Starting User Runtime Directory <span style="color: #f92672;">/</span>run<span style="color: #f92672;">/</span>user<span style="color: #f92672;">/</span><span style="color: #ae81ff;">0.</span><span style="color: #f92672;">..</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> vonkarman systemd<span style="color: #f92672;">-</span>logind[<span style="color: #ae81ff;">382</span>]: New session <span style="color: #ae81ff;">29</span> of user root<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> vonkarman systemd[<span style="color: #ae81ff;">1</span>]: Finished User Runtime Directory <span style="color: #f92672;">/</span>run<span style="color: #f92672;">/</span>user<span style="color: #f92672;">/</span><span style="color: #ae81ff;">0.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> vonkarman systemd[<span style="color: #ae81ff;">1</span>]: Starting User Manager <span style="color: #66d9ef;">for</span> UID <span style="color: #ae81ff;">0.</span><span style="color: #f92672;">..</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> vonkarman systemd[<span style="color: #ae81ff;">11121</span>]: pam_unix(systemd<span style="color: #f92672;">-</span>user:session): session opened <span style="color: #66d9ef;">for</span> user root(uid<span style="color: #f92672;">=</span><span style="color: #ae81ff;">0</span>) 
by (uid<span style="color: #f92672;">=</span><span style="color: #ae81ff;">0</span>) 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> neumann systemd[<span style="color: #ae81ff;">11439</span>]: Queued start job <span style="color: #66d9ef;">for</span> default target Main User Target<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> neumann systemd[<span style="color: #ae81ff;">11439</span>]: Created slice User Application Slice<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> neumann systemd[<span style="color: #ae81ff;">11439</span>]: Reached target Paths<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> neumann systemd[<span style="color: #ae81ff;">11439</span>]: Reached target Timers<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> neumann systemd[<span style="color: #ae81ff;">11439</span>]: Listening on GnuPG network certificate management daemon<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> neumann systemd[<span style="color: #ae81ff;">11439</span>]: Listening on GnuPG cryptographic agent <span style="color: #f92672;">and</span> passphrase cache (access fo
r web browsers)<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> neumann systemd[<span style="color: #ae81ff;">11439</span>]: Listening on GnuPG cryptographic agent <span style="color: #f92672;">and</span> passphrase cache (restricte
d)<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> neumann systemd[<span style="color: #ae81ff;">11439</span>]: Listening on GnuPG cryptographic agent (ssh<span style="color: #f92672;">-</span>agent emulation)<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> neumann systemd[<span style="color: #ae81ff;">11439</span>]: Listening on GnuPG cryptographic agent <span style="color: #f92672;">and</span> passphrase cache<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> neumann systemd[<span style="color: #ae81ff;">11439</span>]: Reached target Sockets<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> neumann systemd[<span style="color: #ae81ff;">11439</span>]: Reached target Basic System<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> neumann systemd[<span style="color: #ae81ff;">11439</span>]: Reached target Main User Target<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> neumann systemd[<span style="color: #ae81ff;">11439</span>]: Startup finished <span style="color: #f92672;">in</span> <span style="color: #ae81ff;">379</span>ms<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> neumann systemd[<span style="color: #ae81ff;">1</span>]: Started User Manager <span style="color: #66d9ef;">for</span> UID <span style="color: #ae81ff;">0.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> neumann systemd[<span style="color: #ae81ff;">1</span>]: Started Session <span style="color: #ae81ff;">30</span> of user root<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> teller systemd[<span style="color: #ae81ff;">11192</span>]: Queued start job <span style="color: #66d9ef;">for</span> default target Main User Target<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> teller systemd[<span style="color: #ae81ff;">11192</span>]: Created slice User Application Slice<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> teller systemd[<span style="color: #ae81ff;">11192</span>]: Reached target Paths<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> teller systemd[<span style="color: #ae81ff;">11192</span>]: Reached target Timers<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> teller systemd[<span style="color: #ae81ff;">11192</span>]: Listening on GnuPG network certificate management daemon<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> teller systemd[<span style="color: #ae81ff;">11192</span>]: Listening on GnuPG cryptographic agent <span style="color: #f92672;">and</span> passphrase cache (access <span style="color: #66d9ef;">for</span>
web browsers)<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> teller systemd[<span style="color: #ae81ff;">11192</span>]: Listening on GnuPG cryptographic agent <span style="color: #f92672;">and</span> passphrase cache (restricted
)<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> teller systemd[<span style="color: #ae81ff;">11192</span>]: Listening on GnuPG cryptographic agent (ssh<span style="color: #f92672;">-</span>agent emulation)<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> teller systemd[<span style="color: #ae81ff;">11192</span>]: Listening on GnuPG cryptographic agent <span style="color: #f92672;">and</span> passphrase cache<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> teller systemd[<span style="color: #ae81ff;">11192</span>]: Reached target Sockets<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> teller systemd[<span style="color: #ae81ff;">11192</span>]: Reached target Basic System<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> teller systemd[<span style="color: #ae81ff;">11192</span>]: Reached target Main User Target<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> teller systemd[<span style="color: #ae81ff;">11192</span>]: Startup finished <span style="color: #f92672;">in</span> <span style="color: #ae81ff;">373</span>ms<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> teller systemd[<span style="color: #ae81ff;">1</span>]: Started User Manager <span style="color: #66d9ef;">for</span> UID <span style="color: #ae81ff;">0.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> teller systemd[<span style="color: #ae81ff;">1</span>]: Started Session <span style="color: #ae81ff;">30</span> of user root<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> vonkarman systemd[<span style="color: #ae81ff;">11121</span>]: Queued start job <span style="color: #66d9ef;">for</span> default target Main User Target<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> vonkarman systemd[<span style="color: #ae81ff;">11121</span>]: Created slice User Application Slice<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> vonkarman systemd[<span style="color: #ae81ff;">11121</span>]: Reached target Paths<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> vonkarman systemd[<span style="color: #ae81ff;">11121</span>]: Reached target Timers<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> vonkarman systemd[<span style="color: #ae81ff;">11121</span>]: Listening on GnuPG network certificate management daemon<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> vonkarman systemd[<span style="color: #ae81ff;">11121</span>]: Listening on GnuPG cryptographic agent <span style="color: #f92672;">and</span> passphrase cache (access 
<span style="color: #66d9ef;">for</span> web browsers)<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> vonkarman systemd[<span style="color: #ae81ff;">11121</span>]: Listening on GnuPG cryptographic agent <span style="color: #f92672;">and</span> passphrase cache (restric
ted)<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> vonkarman systemd[<span style="color: #ae81ff;">11121</span>]: Listening on GnuPG cryptographic agent (ssh<span style="color: #f92672;">-</span>agent emulation)<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> vonkarman systemd[<span style="color: #ae81ff;">11121</span>]: Listening on GnuPG cryptographic agent <span style="color: #f92672;">and</span> passphrase cache<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> vonkarman systemd[<span style="color: #ae81ff;">11121</span>]: Reached target Sockets<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> vonkarman systemd[<span style="color: #ae81ff;">11121</span>]: Reached target Basic System<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> vonkarman systemd[<span style="color: #ae81ff;">11121</span>]: Reached target Main User Target<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> vonkarman systemd[<span style="color: #ae81ff;">11121</span>]: Startup finished <span style="color: #f92672;">in</span> <span style="color: #ae81ff;">392</span>ms<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> vonkarman systemd[<span style="color: #ae81ff;">1</span>]: Started User Manager <span style="color: #66d9ef;">for</span> UID <span style="color: #ae81ff;">0.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> vonkarman systemd[<span style="color: #ae81ff;">1</span>]: Started Session <span style="color: #ae81ff;">29</span> of user root<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> szilard systemd[<span style="color: #ae81ff;">11237</span>]: Queued start job <span style="color: #66d9ef;">for</span> default target Main User Target<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> szilard systemd[<span style="color: #ae81ff;">11237</span>]: Created slice User Application Slice<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> szilard systemd[<span style="color: #ae81ff;">11237</span>]: Reached target Paths<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> szilard systemd[<span style="color: #ae81ff;">11237</span>]: Reached target Timers<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> szilard systemd[<span style="color: #ae81ff;">11237</span>]: Listening on GnuPG network certificate management daemon<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> szilard systemd[<span style="color: #ae81ff;">11237</span>]: Listening on GnuPG cryptographic agent <span style="color: #f92672;">and</span> passphrase cache (access fo
r web browsers)<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> szilard systemd[<span style="color: #ae81ff;">11237</span>]: Listening on GnuPG cryptographic agent <span style="color: #f92672;">and</span> passphrase cache (restricte
d)<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> szilard systemd[<span style="color: #ae81ff;">11237</span>]: Listening on GnuPG cryptographic agent (ssh<span style="color: #f92672;">-</span>agent emulation)<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> szilard systemd[<span style="color: #ae81ff;">11237</span>]: Listening on GnuPG cryptographic agent <span style="color: #f92672;">and</span> passphrase cache<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> szilard systemd[<span style="color: #ae81ff;">11237</span>]: Reached target Sockets<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> szilard systemd[<span style="color: #ae81ff;">11237</span>]: Reached target Basic System<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> szilard systemd[<span style="color: #ae81ff;">11237</span>]: Reached target Main User Target<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> szilard systemd[<span style="color: #ae81ff;">11237</span>]: Startup finished <span style="color: #f92672;">in</span> <span style="color: #ae81ff;">385</span>ms<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> szilard systemd[<span style="color: #ae81ff;">1</span>]: Started User Manager <span style="color: #66d9ef;">for</span> UID <span style="color: #ae81ff;">0.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> szilard systemd[<span style="color: #ae81ff;">1</span>]: Started Session <span style="color: #ae81ff;">30</span> of user root<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> wigner systemd[<span style="color: #ae81ff;">11345</span>]: Queued start job <span style="color: #66d9ef;">for</span> default target Main User Target<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> wigner systemd[<span style="color: #ae81ff;">11345</span>]: Created slice User Application Slice<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> wigner systemd[<span style="color: #ae81ff;">11345</span>]: Reached target Paths<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> wigner systemd[<span style="color: #ae81ff;">11345</span>]: Reached target Timers<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> wigner systemd[<span style="color: #ae81ff;">11345</span>]: Listening on GnuPG network certificate management daemon<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> wigner systemd[<span style="color: #ae81ff;">11345</span>]: Listening on GnuPG cryptographic agent <span style="color: #f92672;">and</span> passphrase cache (access <span style="color: #66d9ef;">for</span>
web browsers)<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> wigner systemd[<span style="color: #ae81ff;">11345</span>]: Listening on GnuPG cryptographic agent <span style="color: #f92672;">and</span> passphrase cache (restricted
)<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> wigner systemd[<span style="color: #ae81ff;">11345</span>]: Listening on GnuPG cryptographic agent (ssh<span style="color: #f92672;">-</span>agent emulation)<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> wigner systemd[<span style="color: #ae81ff;">11345</span>]: Listening on GnuPG cryptographic agent <span style="color: #f92672;">and</span> passphrase cache<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> wigner systemd[<span style="color: #ae81ff;">11345</span>]: Reached target Sockets<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> wigner systemd[<span style="color: #ae81ff;">11345</span>]: Reached target Basic System<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> wigner systemd[<span style="color: #ae81ff;">11345</span>]: Reached target Main User Target<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> wigner systemd[<span style="color: #ae81ff;">11345</span>]: Startup finished <span style="color: #f92672;">in</span> <span style="color: #ae81ff;">375</span>ms<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> wigner systemd[<span style="color: #ae81ff;">1</span>]: Started User Manager <span style="color: #66d9ef;">for</span> UID <span style="color: #ae81ff;">0.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> wigner systemd[<span style="color: #ae81ff;">1</span>]: Started Session <span style="color: #ae81ff;">30</span> of user root<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> kemeny systemd[<span style="color: #ae81ff;">11348</span>]: Queued start job <span style="color: #66d9ef;">for</span> default target Main User Target<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> kemeny systemd[<span style="color: #ae81ff;">11348</span>]: Created slice User Application Slice<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> kemeny systemd[<span style="color: #ae81ff;">11348</span>]: Reached target Paths<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> kemeny systemd[<span style="color: #ae81ff;">11348</span>]: Reached target Timers<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> kemeny systemd[<span style="color: #ae81ff;">11348</span>]: Listening on GnuPG network certificate management daemon<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> kemeny systemd[<span style="color: #ae81ff;">11348</span>]: Listening on GnuPG cryptographic agent <span style="color: #f92672;">and</span> passphrase cache (access <span style="color: #66d9ef;">for</span>
web browsers)<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> kemeny systemd[<span style="color: #ae81ff;">11348</span>]: Listening on GnuPG cryptographic agent <span style="color: #f92672;">and</span> passphrase cache (restricted
)<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> kemeny systemd[<span style="color: #ae81ff;">11348</span>]: Listening on GnuPG cryptographic agent (ssh<span style="color: #f92672;">-</span>agent emulation)<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> kemeny systemd[<span style="color: #ae81ff;">11348</span>]: Listening on GnuPG cryptographic agent <span style="color: #f92672;">and</span> passphrase cache<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> kemeny systemd[<span style="color: #ae81ff;">11348</span>]: Reached target Sockets<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> kemeny systemd[<span style="color: #ae81ff;">11348</span>]: Reached target Basic System<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> kemeny systemd[<span style="color: #ae81ff;">11348</span>]: Reached target Main User Target<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> kemeny systemd[<span style="color: #ae81ff;">11348</span>]: Startup finished <span style="color: #f92672;">in</span> <span style="color: #ae81ff;">400</span>ms<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> kemeny systemd[<span style="color: #ae81ff;">1</span>]: Started User Manager <span style="color: #66d9ef;">for</span> UID <span style="color: #ae81ff;">0.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">58</span> kemeny systemd[<span style="color: #ae81ff;">1</span>]: Started Session <span style="color: #ae81ff;">30</span> of user root<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> kemeny res[<span style="color: #ae81ff;">691</span>]: term_handler: Received signal <span style="color: #ae81ff;">15</span>, exiting 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> kemeny lim[<span style="color: #ae81ff;">688</span>]: term_handler: Received signal <span style="color: #ae81ff;">15</span>, exiting 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> kemeny sbatchd[<span style="color: #ae81ff;">693</span>]: Daemon on host <span style="color: #f92672;">&lt;</span>kemeny<span style="color: #f92672;">&gt;</span> received signal <span style="color: #f92672;">&lt;</span><span style="color: #ae81ff;">15</span><span style="color: #f92672;">&gt;</span>; exiting 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> kemeny lsf_daemons[<span style="color: #ae81ff;">11434</span>]: Stopping the LSF subsystem 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> kemeny systemd[<span style="color: #ae81ff;">1</span>]: lsfd<span style="color: #f92672;">.</span>service: Succeeded<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> kemeny systemd[<span style="color: #ae81ff;">1</span>]: lsfd<span style="color: #f92672;">.</span>service: Consumed <span style="color: #ae81ff;">11</span>min <span style="color: #ae81ff;">56.744</span>s CPU time<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> szilard lim[<span style="color: #ae81ff;">685</span>]: term_handler: Received signal <span style="color: #ae81ff;">15</span>, exiting 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> szilard res[<span style="color: #ae81ff;">687</span>]: term_handler: Received signal <span style="color: #ae81ff;">15</span>, exiting 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> szilard sbatchd[<span style="color: #ae81ff;">689</span>]: Daemon on host <span style="color: #f92672;">&lt;</span>szilard<span style="color: #f92672;">&gt;</span> received signal <span style="color: #f92672;">&lt;</span><span style="color: #ae81ff;">15</span><span style="color: #f92672;">&gt;</span>; exiting 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> vonkarman lim[<span style="color: #ae81ff;">686</span>]: term_handler: Received signal <span style="color: #ae81ff;">15</span>, exiting 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> vonkarman sbatchd[<span style="color: #ae81ff;">690</span>]: Daemon on host <span style="color: #f92672;">&lt;</span>vonkarman<span style="color: #f92672;">&gt;</span> received signal <span style="color: #f92672;">&lt;</span><span style="color: #ae81ff;">15</span><span style="color: #f92672;">&gt;</span>; exiting 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> vonkarman res[<span style="color: #ae81ff;">688</span>]: term_handler: Received signal <span style="color: #ae81ff;">15</span>, exiting 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> teller lim[<span style="color: #ae81ff;">683</span>]: term_handler: Received signal <span style="color: #ae81ff;">15</span>, exiting 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> teller res[<span style="color: #ae81ff;">689</span>]: term_handler: Received signal <span style="color: #ae81ff;">15</span>, exiting 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> teller sbatchd[<span style="color: #ae81ff;">691</span>]: Daemon on host <span style="color: #f92672;">&lt;</span>teller<span style="color: #f92672;">&gt;</span> received signal <span style="color: #f92672;">&lt;</span><span style="color: #ae81ff;">15</span><span style="color: #f92672;">&gt;</span>; exiting 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> teller lsf_daemons[<span style="color: #ae81ff;">11294</span>]: Stopping the LSF subsystem 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> wigner lim[<span style="color: #ae81ff;">719</span>]: term_handler: Received signal <span style="color: #ae81ff;">15</span>, exiting 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> wigner res[<span style="color: #ae81ff;">722</span>]: term_handler: Received signal <span style="color: #ae81ff;">15</span>, exiting 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> wigner sbatchd[<span style="color: #ae81ff;">724</span>]: Daemon on host <span style="color: #f92672;">&lt;</span>wigner<span style="color: #f92672;">&gt;</span> received signal <span style="color: #f92672;">&lt;</span><span style="color: #ae81ff;">15</span><span style="color: #f92672;">&gt;</span>; exiting 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> wigner lsf_daemons[<span style="color: #ae81ff;">11438</span>]: Stopping the LSF subsystem 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> neumann res[<span style="color: #ae81ff;">713</span>]: term_handler: Received signal <span style="color: #ae81ff;">15</span>, exiting 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> neumann sbatchd[<span style="color: #ae81ff;">715</span>]: Daemon on host <span style="color: #f92672;">&lt;</span>neumann<span style="color: #f92672;">&gt;</span> received signal <span style="color: #f92672;">&lt;</span><span style="color: #ae81ff;">15</span><span style="color: #f92672;">&gt;</span>; exiting 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> neumann lim[<span style="color: #ae81ff;">711</span>]: term_handler: Received signal <span style="color: #ae81ff;">15</span>, exiting 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> neumann lsf_daemons[<span style="color: #ae81ff;">11540</span>]: Stopping the LSF subsystem 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> neumann sshd[<span style="color: #ae81ff;">11436</span>]: Received disconnect <span style="color: #f92672;">from</span> <span style="color: #ae81ff;">192.168.1.172</span> port <span style="color: #ae81ff;">55144</span>:<span style="color: #ae81ff;">11</span>: disconnected by use
r 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> neumann sshd[<span style="color: #ae81ff;">11436</span>]: Disconnected <span style="color: #f92672;">from</span> user root <span style="color: #ae81ff;">192.168.1.172</span> port <span style="color: #ae81ff;">55144</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> szilard lsf_daemons[<span style="color: #ae81ff;">11331</span>]: Stopping the LSF subsystem 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> szilard sshd[<span style="color: #ae81ff;">11234</span>]: Received disconnect <span style="color: #f92672;">from</span> <span style="color: #ae81ff;">192.168.1.172</span> port <span style="color: #ae81ff;">52600</span>:<span style="color: #ae81ff;">11</span>: disconnected by use
r 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> szilard sshd[<span style="color: #ae81ff;">11234</span>]: Disconnected <span style="color: #f92672;">from</span> user root <span style="color: #ae81ff;">192.168.1.172</span> port <span style="color: #ae81ff;">52600</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> szilard sshd[<span style="color: #ae81ff;">11234</span>]: pam_unix(sshd:session): session closed <span style="color: #66d9ef;">for</span> user root 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> szilard res[<span style="color: #ae81ff;">11357</span>]: res<span style="color: #f92672;">/</span>get_hostInfo: ls_gethostinfo() failed<span style="color: #f92672;">.</span> Server host LIM configuration i
s <span style="color: #f92672;">not</span> ready yet<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> szilard systemd<span style="color: #f92672;">-</span>logind[<span style="color: #ae81ff;">382</span>]: Session <span style="color: #ae81ff;">30</span> logged out<span style="color: #f92672;">.</span> Waiting <span style="color: #66d9ef;">for</span> processes to exit<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> szilard res[<span style="color: #ae81ff;">11357</span>]: cg_load_hierarchies: Please use the LSF package <span style="color: #66d9ef;">with</span> higher glibc version 
to enable LSF cgroup v2 support<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> szilard systemd[<span style="color: #ae81ff;">1</span>]: lsfd<span style="color: #f92672;">.</span>service: Succeeded<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> szilard systemd[<span style="color: #ae81ff;">1</span>]: lsfd<span style="color: #f92672;">.</span>service: Consumed <span style="color: #ae81ff;">1</span>h <span style="color: #ae81ff;">17</span>min <span style="color: #ae81ff;">44.040</span>s CPU time<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> neumann sshd[<span style="color: #ae81ff;">11436</span>]: pam_unix(sshd:session): session closed <span style="color: #66d9ef;">for</span> user root 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> neumann systemd<span style="color: #f92672;">-</span>logind[<span style="color: #ae81ff;">398</span>]: Session <span style="color: #ae81ff;">30</span> logged out<span style="color: #f92672;">.</span> Waiting <span style="color: #66d9ef;">for</span> processes to exit<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> neumann res[<span style="color: #ae81ff;">11559</span>]: res<span style="color: #f92672;">/</span>get_hostInfo: ls_gethostinfo() failed<span style="color: #f92672;">.</span> Server host LIM configuration i
s <span style="color: #f92672;">not</span> ready yet<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> neumann res[<span style="color: #ae81ff;">11559</span>]: cg_load_hierarchies: Please use the LSF package <span style="color: #66d9ef;">with</span> higher glibc version 
to enable LSF cgroup v2 support<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> neumann systemd[<span style="color: #ae81ff;">1</span>]: lsfd<span style="color: #f92672;">.</span>service: Succeeded<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> neumann systemd[<span style="color: #ae81ff;">1</span>]: lsfd<span style="color: #f92672;">.</span>service: Consumed <span style="color: #ae81ff;">1</span>h <span style="color: #ae81ff;">17</span>min <span style="color: #ae81ff;">21.135</span>s CPU time<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> teller sshd[<span style="color: #ae81ff;">11189</span>]: Received disconnect <span style="color: #f92672;">from</span> <span style="color: #ae81ff;">192.168.1.172</span> port <span style="color: #ae81ff;">35310</span>:<span style="color: #ae81ff;">11</span>: disconnected by user 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> teller sshd[<span style="color: #ae81ff;">11189</span>]: Disconnected <span style="color: #f92672;">from</span> user root <span style="color: #ae81ff;">192.168.1.172</span> port <span style="color: #ae81ff;">35310</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> teller sshd[<span style="color: #ae81ff;">11189</span>]: pam_unix(sshd:session): session closed <span style="color: #66d9ef;">for</span> user root 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> teller systemd<span style="color: #f92672;">-</span>logind[<span style="color: #ae81ff;">382</span>]: Session <span style="color: #ae81ff;">30</span> logged out<span style="color: #f92672;">.</span> Waiting <span style="color: #66d9ef;">for</span> processes to exit<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> teller res[<span style="color: #ae81ff;">11307</span>]: res<span style="color: #f92672;">/</span>get_hostInfo: ls_gethostinfo() failed<span style="color: #f92672;">.</span> Server host LIM configuration <span style="color: #f92672;">is</span>
<span style="color: #f92672;">not</span> ready yet<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> teller res[<span style="color: #ae81ff;">11307</span>]: cg_load_hierarchies: Please use the LSF package <span style="color: #66d9ef;">with</span> higher glibc version t
o enable LSF cgroup v2 support<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> teller res[<span style="color: #ae81ff;">11307</span>]: term_handler: Received signal <span style="color: #ae81ff;">15</span>, exiting 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> teller lim[<span style="color: #ae81ff;">11305</span>]: term_handler: Received signal <span style="color: #ae81ff;">15</span>, exiting 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> teller systemd[<span style="color: #ae81ff;">1</span>]: lsfd<span style="color: #f92672;">.</span>service: Succeeded<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> teller systemd[<span style="color: #ae81ff;">1</span>]: lsfd<span style="color: #f92672;">.</span>service: Consumed <span style="color: #ae81ff;">1</span>h <span style="color: #ae81ff;">17</span>min <span style="color: #ae81ff;">47.675</span>s CPU time<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> teller sbatchd[<span style="color: #ae81ff;">11309</span>]: cg_load_hierarchies: Please use the LSF package <span style="color: #66d9ef;">with</span> higher glibc versi
on to enable LSF cgroup v2 support<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> kemeny sshd[<span style="color: #ae81ff;">11345</span>]: Received disconnect <span style="color: #f92672;">from</span> <span style="color: #ae81ff;">192.168.1.172</span> port <span style="color: #ae81ff;">59830</span>:<span style="color: #ae81ff;">11</span>: disconnected by user 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> kemeny sshd[<span style="color: #ae81ff;">11345</span>]: Disconnected <span style="color: #f92672;">from</span> user root <span style="color: #ae81ff;">192.168.1.172</span> port <span style="color: #ae81ff;">59830</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> kemeny sshd[<span style="color: #ae81ff;">11345</span>]: pam_unix(sshd:session): session closed <span style="color: #66d9ef;">for</span> user root 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> kemeny systemd<span style="color: #f92672;">-</span>logind[<span style="color: #ae81ff;">386</span>]: Session <span style="color: #ae81ff;">30</span> logged out<span style="color: #f92672;">.</span> Waiting <span style="color: #66d9ef;">for</span> processes to exit<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> kemeny res[<span style="color: #ae81ff;">11467</span>]: res<span style="color: #f92672;">/</span>get_hostInfo: ls_gethostinfo() failed<span style="color: #f92672;">.</span> Server host LIM configuration <span style="color: #f92672;">is</span>
<span style="color: #f92672;">not</span> ready yet<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> kemeny res[<span style="color: #ae81ff;">11467</span>]: cg_load_hierarchies: Please use the LSF package <span style="color: #66d9ef;">with</span> higher glibc version t
o enable LSF cgroup v2 support<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> vonkarman lsf_daemons[<span style="color: #ae81ff;">11215</span>]: Stopping the LSF subsystem 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> vonkarman sshd[<span style="color: #ae81ff;">11118</span>]: Received disconnect <span style="color: #f92672;">from</span> <span style="color: #ae81ff;">192.168.1.172</span> port <span style="color: #ae81ff;">48654</span>:<span style="color: #ae81ff;">11</span>: disconnected by u
ser 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> vonkarman sshd[<span style="color: #ae81ff;">11118</span>]: Disconnected <span style="color: #f92672;">from</span> user root <span style="color: #ae81ff;">192.168.1.172</span> port <span style="color: #ae81ff;">48654</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> vonkarman sshd[<span style="color: #ae81ff;">11118</span>]: pam_unix(sshd:session): session closed <span style="color: #66d9ef;">for</span> user root 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> vonkarman systemd<span style="color: #f92672;">-</span>logind[<span style="color: #ae81ff;">382</span>]: Session <span style="color: #ae81ff;">29</span> logged out<span style="color: #f92672;">.</span> Waiting <span style="color: #66d9ef;">for</span> processes to exit<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> vonkarman res[<span style="color: #ae81ff;">11241</span>]: res<span style="color: #f92672;">/</span>get_hostInfo: ls_gethostinfo() failed<span style="color: #f92672;">.</span> Server host LIM configuration
<span style="color: #f92672;">is</span> <span style="color: #f92672;">not</span> ready yet<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> vonkarman res[<span style="color: #ae81ff;">11241</span>]: cg_load_hierarchies: Please use the LSF package <span style="color: #66d9ef;">with</span> higher glibc versio
n to enable LSF cgroup v2 support<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> vonkarman systemd[<span style="color: #ae81ff;">1</span>]: lsfd<span style="color: #f92672;">.</span>service: Succeeded<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> vonkarman systemd[<span style="color: #ae81ff;">1</span>]: lsfd<span style="color: #f92672;">.</span>service: Consumed <span style="color: #ae81ff;">1</span>h <span style="color: #ae81ff;">17</span>min <span style="color: #ae81ff;">34.650</span>s CPU time<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> wigner sshd[<span style="color: #ae81ff;">11342</span>]: Received disconnect <span style="color: #f92672;">from</span> <span style="color: #ae81ff;">192.168.1.172</span> port <span style="color: #ae81ff;">60388</span>:<span style="color: #ae81ff;">11</span>: disconnected by user 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> wigner sshd[<span style="color: #ae81ff;">11342</span>]: Disconnected <span style="color: #f92672;">from</span> user root <span style="color: #ae81ff;">192.168.1.172</span> port <span style="color: #ae81ff;">60388</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> wigner sshd[<span style="color: #ae81ff;">11342</span>]: pam_unix(sshd:session): session closed <span style="color: #66d9ef;">for</span> user root 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> wigner res[<span style="color: #ae81ff;">11464</span>]: res<span style="color: #f92672;">/</span>get_hostInfo: ls_gethostinfo() failed<span style="color: #f92672;">.</span> Server host LIM configuration <span style="color: #f92672;">is</span>
<span style="color: #f92672;">not</span> ready yet<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> wigner systemd<span style="color: #f92672;">-</span>logind[<span style="color: #ae81ff;">383</span>]: Session <span style="color: #ae81ff;">30</span> logged out<span style="color: #f92672;">.</span> Waiting <span style="color: #66d9ef;">for</span> processes to exit<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> wigner res[<span style="color: #ae81ff;">11464</span>]: cg_load_hierarchies: Please use the LSF package <span style="color: #66d9ef;">with</span> higher glibc version t
o enable LSF cgroup v2 support<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> wigner systemd[<span style="color: #ae81ff;">1</span>]: lsfd<span style="color: #f92672;">.</span>service: Succeeded<span style="color: #f92672;">.</span> 
Apr  <span style="color: #ae81ff;">3</span> <span style="color: #ae81ff;">21</span>:<span style="color: #ae81ff;">44</span>:<span style="color: #ae81ff;">59</span> wigner systemd[<span style="color: #ae81ff;">1</span>]: lsfd<span style="color: #f92672;">.</span>service: Consumed <span style="color: #ae81ff;">1</span>h <span style="color: #ae81ff;">17</span>min <span style="color: #ae81ff;">44.610</span>s CPU time<span style="color: #f92672;">.</span>
</code></pre></div>

</details>

<br />
<!-- raw HTML omitted -->
As expected, we observed that LSF log messages are written to the fromnet file. And importantly each entry contains the hostname, so that we can identify the origin of the message.</p>

<p><strong>Conclusion</strong></p>

<p>What started out as a chat about logging, grew into an idea of a blog, for which I am thankful for the collaboration of Peter. We’ve illustrated an example here of how to setup centralized logging on a Turing Pi system with syslog-ng to collect system and LSF logs.</p>

<p>Of course collecting log messages centrally is just the start of a journey. It is an important step as it allows for significantly easier debugging and troubleshooting. You can store logs to databases for easier search. And once you better understand which log messages are important, you can even potentially parse those and generate alersts from them or dashboards. All of these help you to make sure that your HPC system runs smoothly and with minimal downtime. For me this was a learning experience and I&rsquo;ll be looking how I can implement more broadly centralized logging in my home network.</p>]]></content><author><name>Ramblings of a supercomputing enthusiast.</name></author><category term="gaborsamu" /><summary type="html"><![CDATA[Logs are one of those indispensable things in IT when things go wrong. Having worked in technical support for software products in a past life, I’ve likely looked at hundreds (or more) logs over the years, helping to identify issues. So, I really appreciate the importance of logs, but I can honestly say that I never really thought about a logging strategy for the systems on my home network - primarily those running Linux.]]></summary></entry><entry><title type="html">4 turning and 7 chilling</title><link href="https://hpc.social/personal-blog/2024/4-turning-and-7-chilling/" rel="alternate" type="text/html" title="4 turning and 7 chilling" /><published>2024-03-21T18:09:30-06:00</published><updated>2024-03-21T18:09:30-06:00</updated><id>https://hpc.social/personal-blog/2024/4-turning-and-7-chilling</id><content type="html" xml:base="https://hpc.social/personal-blog/2024/4-turning-and-7-chilling/"><![CDATA[<p><strong>How to keep your cool</strong></p>

<p>I&rsquo;m back again and revisiting the Turing Pi V1 board. This time the focus isn&rsquo;t on software, but rather cooling. In my previous write-up <a href="https://www.gaborsamu.com/blog/turingpi_hpl/">Pi in the sky? A compute cluster in mini ITX form factor</a>, I used a USB fan I had at hand to keep the temperature of the compute modules in check during the Linpack runs. Although the fan was a seriously sketchy one, it did the job, and prevented throttling of the compute modules under high load, albeit with much noise. Clearly not content with this mediocre setup I pondered what other solutions I could quickly come up with.</p>

<p>Looking in my electronics spare parts bin, I came across 2 spare Noctua 40x40x20mm fans, part number <em>NF-A4x20 PWM</em>. I found that these fans fit well on the Turing Pi board perpendicular to the compute modules. I measured that for full cooling coverage of the compute modules I&rsquo;d need 4 such fans, side by side. However before investing in two more fans, I needed to confirm that they had enough oomph (yes that&rsquo;s a technical term) to keep things cool.</p>

<p>So my plan was to first test two fans cooling half of the modules. However, to test these fans out, I first needed to get a hold of some USB to 3/4-pin fan power adapter cables. Once I had these adapters, I used a thick elastic band to bind the 2 fans together, and connected them to the USB for power using the adapters and give them a whirl - pun intended. Of course I fell back on Linpack to get the compute modules busy.</p>

<p>The results were promising enough that I immediately ordered two more fans and adapters to complete the setup which is shown in the photo below. A thick elastic band was used again to fasten the remaining 2 fans together. Of course, the setup will be made more robust to ensure that fans will stay in place. And I&rsquo;ll do a bit of work on cable management.</p>

<figure><img src="https://www.gaborsamu.com/images/turingpi_noctua.jpg" />
</figure>

<p><strong>Totally chill</strong></p>

<p>The view of the dashboard (see below) speaks for itself. Under heavy load running Linpack, the compute modules don&rsquo;t exceed 50C. This is about 10 degrees cooler than what I saw with that USB desk fan. So I&rsquo;d consider that a result. Plus the Noctua fans are so much quieter and will be much more durable in the long run.</p>

<figure><img src="https://www.gaborsamu.com/images/turingpi_dashboard_noctua.png" />
</figure>

<p><strong>Conclusion</strong></p>

<p>So where does the title of my blog come from? It&rsquo;s inspired by the slogan &ldquo;6 turning and 4 burning&rdquo; of the B-36 Peacemaker strategic bomber! You can see the B-36 in all it&rsquo;s glory in this short <a href="https://youtu.be/9kQ2X84PRvY?si=q9FZmWFavXHbPcE8">excerpt</a> from the 1955 film <em>Strategic Air Command</em> starring Jimmy Stewart. You could say I have eclectic taste in films. Plus the B-36 has always fascinated me with it&rsquo;s combination of jet and piston engines. As for this blog, 4 turning obviously refers to the 4 Noctua fans turning. And 7 chilling refers to the 7 CM3 modules that now keep their cool under pressure. With a more suitable cooling solution in place, especially as the warmer days arrive, I can now refocus my attention to the software side of things. And as always, stay cool!</p>]]></content><author><name>Ramblings of a supercomputing enthusiast.</name></author><category term="gaborsamu" /><summary type="html"><![CDATA[How to keep your cool]]></summary></entry><entry><title type="html">Advanced LSF resource connector configuration on IBM Cloud - part II</title><link href="https://hpc.social/personal-blog/2024/advanced-lsf-resource-connector-configuration-on-ibm-cloud-part-ii/" rel="alternate" type="text/html" title="Advanced LSF resource connector configuration on IBM Cloud - part II" /><published>2024-03-19T20:40:52-06:00</published><updated>2024-03-19T20:40:52-06:00</updated><id>https://hpc.social/personal-blog/2024/advanced-lsf-resource-connector-configuration-on-ibm-cloud-part-ii</id><content type="html" xml:base="https://hpc.social/personal-blog/2024/advanced-lsf-resource-connector-configuration-on-ibm-cloud-part-ii/"><![CDATA[<p><strong>Overview</strong></p>

<p>Back in November 2023 I authored a blog titled <a href="https://www.gaborsamu.com/blog/lsf_rc_ibmcloud_part1/">Advanced LSF resource connector configuration on IBM Cloud - part I</a>. As I signed off in that post, I mentioned that there would be a follow-on post to cover some more advanced configuration topics on LSF resource connector.  And that’s the topic of this article today.</p>

<p>To recap, the <a href="https://www.ibm.com/products/hpc-workload-management">IBM LSF</a> resource connector functionality enables LSF clusters to dynamically spin-up cloud instances from supported resource providers in the cloud based on workload demand, and to destroy those instances when no longer required.</p>

<p>The LSF resource connector intelligently choses the most appropriate cloud instance type for a given job from the templates that have been defined by the administrator. This is done automatically and is transparent from the end user perspective. What if the job requires to run on a very specific instance type? In this post we’ll show you how this is feasible by defining an LSF string resource, along with the necessary configuration of LSF resource connector and a supporting script. This will allow users to submit jobs to LSF with a resource requirement string specifying the cloud instance type desired.</p>

<p>For the example below, we’ll be using an IBM LSF environment which has been deployed on IBM Cloud using the extensive deployment automation that is available via the IBM Cloud catalog for <a href="https://cloud.ibm.com/catalog/content/terraform-1623200063-71606cab-c6e1-4f95-a47a-2ce541dcbed8-global">IBM LSF</a>. Using this automation, you can deploy an LSF cluster in about 10 minutes time including the creation of the virtual private cloud (VPC), networking, security, bastion node, NFS server node, LSF management nodes and optionally LSF Application Center.</p>

<p><strong>What is user_data.sh?</strong></p>

<p>We’ll start with a brief description of the LSF resource connector <em>user_data.sh</em> script. This script will play an important part in the configuration of the compute servers as we’ll see. The <em>user_data.sh</em> script is used to start-up the LSF daemons on the compute instances launched by LSF resource connector. It also crucially enables admins to configure settings, including LSF settings, which is what we’ll be using the in example below.</p>

<p><strong>Specifying the cloud instance type</strong></p>

<p>By default, the LSF resource connector intelligently chooses the cloud instance profile type based upon the job submission parameters. For example, it considers things like the number of processor requested, the memory requested just to name of few. And it will startup the compute instance or instances from the available configured templates which most closely matches the job requirement.</p>

<p>What if you need to request a very specific compute instance type for the work that you’ve submitted based upon other, site-specific needs?  Here we will show exactly how you can achieve this.</p>

<p><strong>Let the configuration begin!</strong></p>

<p>We begin with updating the LSF configuration to create a new string resource called profile. In the configuration file <em>$LSF_ENVDIR/lsf.shared</em>, define the new string resource profile in the <em>Resource</em> section.</p>

<div class="highlight"><pre><code class="language-plaintext">….
….
Begin Resource
RESOURCENAME	TYPE	    INTERVAL	INCREASING	DESCRIPTION        # Keywords
profile 	  String   ()       ()             (IBM Cloud Gen2 profile type)
End Resource
….
….</code></pre></div>

<p>To make the change take effect, reconfigure the LSF cluster with the LSF command <em>lsadmin reconfig</em>.</p>

<div class="highlight"><pre><code class="language-plaintext"># lsadmin reconfig -v

Checking configuration files ...


EGO 3.4.0 build 1599999, Jan 04 2023
Copyright International Business Machines Corp. 1992, 2016.
US Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM Corp.

  binary type: linux3.10-glibc2.17-x86_64
Reading configuration from /opt/ibm/lsf/conf/lsf.conf
Mar 13 16:57:25 2024 1478621 6 3.4.0 Lim starting...
Mar 13 16:57:25 2024 1478621 6 3.4.0 LIM is running in advanced workload execution mode.
Mar 13 16:57:25 2024 1478621 6 3.4.0 Master LIM is not running in EGO_DISABLE_UNRESOLVABLE_HOST mode.
Mar 13 16:57:25 2024 1478621 5 3.4.0 /opt/ibm/lsf/10.1/linux3.10-glibc2.17-x86_64/etc/lim -C
Mar 13 16:57:26 2024 1478621 6 3.4.0 LIM is running as IBM Spectrum LSF Standard Edition.
Mar 13 16:57:26 2024 1478621 6 3.4.0 reCheckClass: numhosts 1 so reset exchIntvl to 15.00
Mar 13 16:57:26 2024 1478621 6 3.4.0 Checking Done.
---------------------------------------------------------
No errors found.

Restart only the master candidate hosts? [y/n] n
Do you really want to restart LIMs on all hosts? [y/n] y
Restart LIM on &lt;icgen2host-AAA-BBB-CCC-DDD&gt; ...... done</code></pre></div>

<p>Now, check that the profile variable has been setup properly. This can be done using the LSF <em>lsinfo</em> command.</p>

<div class="highlight"><pre><code class="language-plaintext"># lsinfo |grep profile
profile        String   N/A   IBM Cloud Gen2 profile type</code></pre></div>

<p>Now we’re ready to update the LSF resource connector templates to add the <em>profile</em> string variable. For this example, there are two templates defined for IBM Cloud profile types <em>bx2-4x16</em> and <em>mx2-16x128</em> in the configuration file <em>$LSF_ENVDIR/resource_connector/ibmcloudgen2/conf</em>. Within the template definition, the <em>profile</em> string variable is defined, and a value is set for each respective profile type. Note that the “-“ character cannot be used in the LSF string variables, and in place of that the “_” character is used. The specified profile string for each respective template is defined and used as the selection criteria by LSF resource connector. Then the <em>userData</em> field is used to ensure that this value gets passed and set in the compute instance that is started by the LSF resource connector when the <em>user_data.sh</em> script is run.</p>

<table>
<thead>
<tr>
<th style="text-align: left;">Instance type</th>
<th>LSF <em>profile</em> variable string value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">bx2-4x16</td>
<td>bx2_4x16</td>
</tr>
<tr>
<td style="text-align: left;">mx2-16x128</td>
<td>mx2_16x128</td>
</tr>
</tbody>
</table>
<hr />

<p><strong><em>ibmcloudgen2_templates.json</em>, with <em>profile</em> configured (obfuscated)</strong></p>

<div class="highlight"><pre><code class="language-plaintext">{
    "templates": [
        {
            "templateId": "Template-1",
            "maxNumber": 2,
            "attributes": {
                "type": ["String", "X86_64"],
                "ncores": ["Numeric", "2"],
                "ncpus": ["Numeric", "4"],
                "mem": ["Numeric", "16384"],
                "icgen2host": ["Boolean", "1"], 
                "profile":["String","bx2_4x16"]
            },
            "imageId": "aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff",
            "subnetId": "aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff",
            "vpcId": "aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff",
            "vmType": "bx2-4x16",
            "userData":"profile=bx2_4x16",
            "securityGroupIds": ["aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff"],
            "resourceGroupId": "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa",
            "sshkey_id": "aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff",
            "priority": "10", 
            "region": "us-east",
            "zone": "us-east-1" 
        },

       {
            "templateId": "Template-2",
            "maxNumber": 2,
            "attributes": {
                "type": ["String", "X86_64"],
                "ncores": ["Numeric", "8"],
                "ncpus": ["Numeric", "16"],
                "mem": ["Numeric", "131072"],
                "icgen2host": ["Boolean", "1"],
		       "profile":["String","mx2_16x128"]
            },
            "imageId": "aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff",
            "subnetId": "aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff",
            "vpcId": "aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff",
            "vmType": "mx2-16x128",
            "userData":"profile=mx2_16x128",
            "securityGroupIds": ["aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff"],
            "resourceGroupId": "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa",
            "sshkey_id": "aaaa-bbbbbbbb-cccc-dddd-eeee-ffffffffffff",
            "priority": "5",
            "region": "us-east",
            "zone": "us-east-1"
        }

    ]
}</code></pre></div>

<p>Now the <em>user_data.sh</em> script is required to be updated in order to set the value of the <em>profile</em> variable in the LSF resourcemap based upon what was requested by the user. This will be added to the LSF configuration during the bootup of the dynamic cloud instances. For more information about the LSF resourcemap read <a href="https://www.ibm.com/docs/en/spectrum-lsf/10.1.0?topic=resources-configure-lsfclustercluster-name-resourcemap-section">here</a>.</p>

<p><strong>user_data.sh script portion</strong></p>

<div class="highlight"><pre><code class="language-plaintext">….
….
# Set value of profile variable in the LSF resourcemap. This is based
# on the profile value requested at job submission time. 
if [ -n "$profile" ]; then
sed -i "s/\(LSF_LOCAL_RESOURCES=.*\)\"/\1 [resourcemap $profile*profile]\"/" $LSF_CONF_FILE
echo "update LSF_LOCAL_RESOURCES in $LSF_CONF_FILE successfully, add [resourcemap ${profile}*profile]" &gt;&gt; $logfile
else
echo "profile doesn't exist in environment variable" &gt;&gt; $logfile
fi
….
….</code></pre></div>

<p>With all of the configuration in place, it’s now time to test things out. Initially, a stress job is submitted requesting 4 cores is submitted without requesting a specific compute profile. In this case, the LSF resource connector will chose the most appropriate instance type from the configured templates. In our configuration the templates for instance types <em>bx2-4x16</em> and <em>mx2-16x128</em> are configured. Given this, we expect the LSF resource connector to startup a <em>bx2-4x16</em> instance to satisfy the requirements for this example job.</p>

<div class="highlight"><pre><code class="language-plaintext">$ bsub -n 4 -q normal -o /mnt/data/%J.out /usr/bin/stress --cpu 4 --vm-bytes 8192MB --timeout 60s 
Job &lt;3811&gt; is submitted to queue &lt;normal&gt;.</code></pre></div>

<p>After a few moments, we see that a new host, <em>icgen2host-XXX-YYY-ZZZ-44</em> joins the LSF cluster and the job enters run state. We note that this is a host with the characteristics of 4 cores, and 16GB RAM, which matches the type <em>bx2-4x16</em>.</p>

<div class="highlight"><pre><code class="language-plaintext">$ lsload -w
HOST_NAME               status  r15s   r1m  r15m   ut    pg  ls    it   tmp   swp   mem
icgen2host-XXX-YYY-ZZZ-37      ok   0.4   0.1   0.1   2%   0.0   1    16   40G    0
icgen2host-XXX-YYY-ZZZ-44      ok   0.9   0.2   0.1  19%   0.0   0     0   88G    0M 14.9G

$ lshosts -w
HOST_NAME                       type       model  cpuf ncpus maxmem maxswp server RESOURCES
icgen2host-XXX-YYY-ZZZ-37        X86_64    Intel_E5  12.5     4  15.4G      -    Yes (mg docker ParaView)
icgen2host-XXX-YYY-ZZZ-44		 X86_64    Intel_E5  12.5     4  15.5G      -    Dyn (icgen2host docker)

$ bjobs -l -r

Job &lt;3811&gt;, User &lt;lsfadmin&gt;, Project &lt;default&gt;, Status &lt;RUN&gt;, Queue &lt;normal&gt;, C
                     ommand &lt;/usr/bin/stress --cpu 4 --vm-bytes 8192MB --timeou
                     t 60s&gt;, Share group charged &lt;/lsfadmin&gt;
Mon Mar 18 19:42:22: Submitted from host &lt;icgen2host-XXX-YYY-ZZZ-37&gt;, CWD &lt;$HOME&gt;,
                      Output File &lt;/mnt/data/3811.out&gt;, 4 Task(s);
Mon Mar 18 19:45:11: Started 4 Task(s) on Host(s) &lt;icgen2host-XXX-YYY-ZZZ-44&gt; &lt;icg
                     en2host-XXX-YYY-ZZZ-44&gt; &lt;icgen2host-XXX-YYY-ZZZ-44&gt; &lt;icgen2host-
                     XXX-YYY-ZZZ-44&gt;, Allocated 4 Slot(s) on Host(s) &lt;icgen2host-X
                     XX-YYY-ZZZ-44&gt; &lt;icgen2host-XXX-YYY-ZZZ-44&gt; &lt;icgen2host-XXX-YYY-ZZZ-
                     44&gt; &lt;icgen2host-XXX-YYY-ZZZ-44&gt;, Execution Home &lt;/home/lsfadm
                     in&gt;, Execution CWD &lt;/home/lsfadmin&gt;;
Mon Mar 18 19:45:47: Resource usage collected.
                     The CPU time used is 142 seconds.
                     MEM: 3 Mbytes;  SWAP: 0 Mbytes;  NTHREAD: 8
                     PGID: 2169;  PIDs: 2169 2170 2172 2173 2174 2175 2176 


 MEMORY USAGE:
 MAX MEM: 3 Mbytes;  AVG MEM: 3 Mbytes; MEM Efficiency: 0.00%

 CPU USAGE:
 CPU PEAK: 0.00 ;  CPU PEAK DURATION: 0 second(s)
 CPU AVERAGE EFFICIENCY: 0.00% ;  CPU PEAK EFFICIENCY: 0.00%

 SCHEDULING PARAMETERS:
           r15s   r1m  r15m   ut      pg    io   ls    it    tmp    swp    mem
 loadSched   -     -     -     -       -     -    -     -     -      -      -  
 loadStop    -     -     -     -       -     -    -     -     -      -      -  

 RESOURCE REQUIREMENT DETAILS:
 Combined: select[type == local] order[r15s:pg]
 Effective: select[type == local] order[r15s:pg] 

$ bhist -l 3811

Job &lt;3811&gt;, User &lt;lsfadmin&gt;, Project &lt;default&gt;, Command &lt;/usr/bin/stress --cpu 
                     4 --vm-bytes 8192MB --timeout 60s&gt;
Mon Mar 18 19:42:22: Submitted from host &lt;icgen2host-XXX-YYY-ZZZ-37&gt;, to Queue &lt;no
                     rmal&gt;, CWD &lt;$HOME&gt;, Output File &lt;/mnt/data/%J.out&gt;, 4 Task
                     (s);
Mon Mar 18 19:45:11: Dispatched 4 Task(s) on Host(s) &lt;icgen2host-XXX-YYY-ZZZ-44&gt; &lt;
                     icgen2host-XXX-YYY-ZZZ-44&gt; &lt;icgen2host-XXX-YYY-ZZZ-44&gt; &lt;icgen2ho
                     st-XXX-YYY-ZZZ-44&gt;, Allocated 4 Slot(s) on Host(s) &lt;icgen2hos
                     t-XXX-YYY-ZZZ-44&gt; &lt;icgen2host-XXX-YYY-ZZZ-44&gt; &lt;icgen2host-XXX-YYY
                     -ZZZ-44&gt; &lt;icgen2host-XXX-YYY-ZZZ-44&gt;, Effective RES_REQ &lt;select
                     [type == local] order[r15s:pg] &gt;;
Mon Mar 18 19:45:11: Starting (Pid 2169);
Mon Mar 18 19:45:11: Running with execution home &lt;/home/lsfadmin&gt;, Execution CW
                     D &lt;/home/lsfadmin&gt;, Execution Pid &lt;2169&gt;;
Mon Mar 18 19:46:11: Done successfully. The CPU time used is 239.3 seconds;
Mon Mar 18 19:46:12: Post job process done successfully;


MEMORY USAGE:
MAX MEM: 3 Mbytes;  AVG MEM: 2 Mbytes; MEM Efficiency: 0.00%

CPU USAGE:
CPU PEAK: 3.98 ;  CPU PEAK DURATION: 60 second(s)
CPU AVERAGE EFFICIENCY: 99.58% ;  CPU PEAK EFFICIENCY: 99.58%

Summary of time in seconds spent in various states by  Mon Mar 18 19:46:12
  PEND     PSUSP    RUN      USUSP    SSUSP    UNKWN    TOTAL
  169      0        60       0        0        0        229         </code></pre></div>

<p>Next, let’s submit the same job, but explicitly requesting the profile type <em>mx2-16x128</em>. To do this, we add the resource requirement requesting profile to be equal to <em>mx2_16x128</em> as follows:</p>

<div class="highlight"><pre><code class="language-plaintext">$ bsub -q normal -R "profile==mx2_16x128" -n 4 -o /mnt/data/%J.out /usr/bin/stress --cpu 4 --vm-bytes 8192MB --timeout 60s 
Job &lt;3813&gt; is submitted to queue &lt;normal&gt;.</code></pre></div>

<p>After a few moments, a dynamic host with 16 CPUs and 128 GB RAM joins the cluster, which corresponds to instance type <em>mx2-16x128</em>.</p>

<div class="highlight"><pre><code class="language-plaintext">$ lshosts -w
HOST_NAME                       type       model  cpuf ncpus maxmem maxswp server RESOURCES
icgen2host-XXX-YYY-ZZZ-37        X86_64    Intel_E5  12.5     4  15.4G      -    Yes (mg docker ParaView)
icgen2host-XXX-YYY-ZZZ-46        X86_64    Intel_E5  12.5    16 125.7G      -    Dyn (icgen2host docker)

$ lsload -w
HOST_NAME               status  r15s   r1m  r15m   ut    pg  ls    it   tmp   swp   mem
icgen2host-XXX-YYY-ZZZ-37      ok   0.0   0.1   0.1   3%   0.0   1     2   40G    0M 12.2G
icgen2host-XXX-YYY-ZZZ-46      ok   1.6   0.4   0.1  17%   0.0   0     0   88G    0M 123.8G

$ bjobs -l -r

Job &lt;3813&gt;, User &lt;lsfadmin&gt;, Project &lt;default&gt;, Status &lt;RUN&gt;, Queue &lt;normal&gt;, C
                     ommand &lt;/usr/bin/stress --cpu 4 --vm-bytes 8192MB --timeou
                     t 60s&gt;, Share group charged &lt;/lsfadmin&gt;
Mon Mar 18 20:27:52: Submitted from host &lt;icgen2host-XXX-YYY-ZZZ-37&gt;, CWD &lt;$HOME&gt;,
                      Output File &lt;/mnt/data/3813.out&gt;, 4 Task(s), Requested Re
                     sources &lt;profile==mx2_16x128&gt;;
Mon Mar 18 20:30:01: Started 4 Task(s) on Host(s) &lt;icgen2host-XXX-YYY-ZZZ-46&gt; &lt;icg
                     en2host-XXX-YYY-ZZZ-46&gt; &lt;icgen2host-XXX-YYY-ZZZ-46&gt; &lt;icgen2host-
                     XXX-YYY-ZZZ-46&gt;, Allocated 4 Slot(s) on Host(s) &lt;icgen2host                     -XXX-YYY-ZZZ-46&gt; &lt;icgen2host-XXX-YYY-ZZZ-46&gt; &lt;icgen2host-XXX-Y                     YY-ZZZ-46&gt; &lt;icgen2host-XXX-YYY-ZZZ-46&gt;, Execution Home &lt;/home/lsfadm
                     in&gt;, Execution CWD &lt;/home/lsfadmin&gt;;

 MEMORY USAGE:
 MEM Efficiency: 0.00%

 CPU USAGE:
 CPU PEAK: 0.00 ;  CPU PEAK DURATION: 0 second(s)
 CPU AVERAGE EFFICIENCY: 0.00% ;  CPU PEAK EFFICIENCY: 0.00%

 SCHEDULING PARAMETERS:
           r15s   r1m  r15m   ut      pg    io   ls    it    tmp    swp    mem
 loadSched   -     -     -     -       -     -    -     -     -      -      -  
 loadStop    -     -     -     -       -     -    -     -     -      -      -  

 RESOURCE REQUIREMENT DETAILS:
 Combined: select[(profile == mx2_16x128 ) &amp;&amp; (type == any)] order[r15s:pg]
 Effective: select[(profile == mx2_16x128 ) &amp;&amp; (type == any)] order[r15s:pg] </code></pre></div>

<p>Additionally, using the LSF lshosts command with the -s option, we can view information about the resources in the environment. We see in particular the profile resource is set to the value <em>mx2_16x128</em> for the dynamic host <em>icgen2host-XXX-YYY-ZZZ-46</em>.</p>

<div class="highlight"><pre><code class="language-plaintext">$ lshosts -s
RESOURCE                                VALUE       LOCATION
rc_account                            default       icgen2host-XXX-YYY-ZZZ-46 
profile                            mx2_16x128       icgen2host-XXX-YYY-ZZZ-46 
instanceID               0757_95e39240-22e7-4734-8fd5-9988ab247801  
                                                    icgen2host-XXX-YYY-ZZZ-46 </code></pre></div>

<p>We have shown the great flexibility that LSF provides for configuring the resource connector capability. Generally speaking, LSF provides many open interfaces which allow site specific configuration or customization to be realized. In the next blog in this series, we’ll take a closer look at running Docker jobs under LSF on dynamic cloud resources.</p>]]></content><author><name>Ramblings of a supercomputing enthusiast.</name></author><category term="gaborsamu" /><summary type="html"><![CDATA[Overview]]></summary></entry><entry><title type="html">Pi in the sky? A compute cluster in mini ITX form factor</title><link href="https://hpc.social/personal-blog/2024/pi-in-the-sky-a-compute-cluster-in-mini-itx-form-factor/" rel="alternate" type="text/html" title="Pi in the sky? A compute cluster in mini ITX form factor" /><published>2024-03-04T17:07:46-07:00</published><updated>2024-03-04T17:07:46-07:00</updated><id>https://hpc.social/personal-blog/2024/pi-in-the-sky-a-compute-cluster-in-mini-itx-form-factor</id><content type="html" xml:base="https://hpc.social/personal-blog/2024/pi-in-the-sky-a-compute-cluster-in-mini-itx-form-factor/"><![CDATA[<p><strong>Overview</strong></p>

<p>It&rsquo;s taken me a while to get the wheels off the ground in 2024 in terms of blogging. This blog idea has been in the works actually for some time. Back in 2021, I wrote a blog titled <a href="https://www.gaborsamu.com/blog/new_novena/">Late to the party and a few bits short</a>. This was a tongue in cheek title for a blog on the Novena Desktop System, which is based on a 32-bit processor, hence a few bits short. And late to the party referring to the fact that I was very late to purchase a second-hand Novena system.</p>

<p>This blog is similar in that it&rsquo;s about the original Turing Pi V1 system which was released back in 2021 when the Turing Pi V2 launch was imminent. The Turing Pi V1 is a 7 node cluster in a mini-ITX form factor. It&rsquo;s based on the Raspberry Pi CM3(+) modules. This was really an impulse purchase the dark days of COVID. And as I found out, getting a hold of RPi CM3&rsquo;s was much harder than expected. As luck would have it, I even eventually found a source via an online marketplace here in Southern Ontario that was not charging and arm and a leg for them. I purchased a total of 7 CM3+ modules with no onboard storage and relied upon SD cards for storage. As (bad) luck would have it, I ended up having to purchase a CM3 with onboard storage because one of the SD card slots is defecting on the board; the spring mechanism doesn&rsquo;t work properly. And as we&rsquo;ll see later on, this also had an unusual side effect when running Linpack.</p>

<p>I&rsquo;ve had the fully populated system for about 6 months now. And although the Turing Pi V1 is old news at this stage, I still wanted to write a bit about my experience with it. And of course, because it&rsquo;s a cluster, I definitely wanted to put it through it&rsquo;s paces running Linpack.</p>

<p>The official Turing Pi V1 <a href="https://docs.turingpi.com/docs/turing-pi1-intro-specs">documentation</a> was my goto for the system setup. The cluster was installed with the latest (at the time) Raspberry Pi OS (<em>2023-02-21-raspios-bullseye-arm64-lite.img</em>) based on Debian 11 (Bullseye).</p>

<p>The following additional software packages were installed/compiled. Note that the head node of the cluster acts as an NFS server for the remaining cluster nodes (<em>/opt</em>).</p>

<ul>
<li>Arm Optimizing Compilers V22.0.2 (for Ubuntu-20.04)</li>
<li>OpenMPI V4.1.5 (compiled with LSF support)</li>
<li>IBM Spectrum LSF v10.1.0.13</li>
<li>HPL V2.3 (compiled with Arm Optimizing Compilers)</li>
</ul>
<p>Here is the output of the LSF lshosts command. We see 6 CM3+ systems detected, and one CM3. Note that this required additional LSF configuration.</p>

<div class="highlight"><pre><code class="language-plaintext">lsfadmin@turingpi:/opt/HPC/hpl-2.3 $ lshosts -w
HOST_NAME                       type       model  cpuf ncpus maxmem maxswp server RESOURCES
turingpi                  LINUX_ARM64     CM3plus   6.0     4   910M   100M    Yes (mg)
neumann                   LINUX_ARM64     CM3plus   6.0     4   910M   100M    Yes ()
teller                    LINUX_ARM64     CM3plus   6.0     4   910M   100M    Yes ()
szilard                   LINUX_ARM64     CM3plus   6.0     4   910M   100M    Yes ()
wigner                    LINUX_ARM64     CM3plus   6.0     4   910M   100M    Yes ()
kemeny                    LINUX_ARM64         CM3   5.0     4   910M   100M    Yes ()
vonkarman                 LINUX_ARM64     CM3plus   6.0     4   910M   100M    Yes ()</code></pre></div>

<p>Those with a keen eye will note that the majority of the cluster nodes are named after Hungarian scient
ists:</p>

<ul>
<li><em>Neumann János</em> (John von Neumann)</li>
<li><em>Teller Ede</em> (Edward Teller)</li>
<li><em>Szilárd Leó</em> (Leo Szilard)</li>
<li><em>Wigner Jenő</em> (Eugene Wigner)</li>
<li><em>Kemény János</em> (John Kemeny)</li>
<li><em>Kármán Tódor</em> (Theodore von Karman)</li>
</ul>
<p>The odd one out here is of course turingpi, which is the name of the head node of the cluster, and is o
f course named after Alan Turing. But I digress.</p>

<p>For completeness, HPL V2.3 was compiled using the Arm Optimizing Compilers with the follwing flags:</p>

<ul>
<li>CCFLAGS      = $(HPL_DEFS) -Ofast -mcpu=native -fomit-frame-pointer</li>
<li>LINKER       = armclang -armpl -lamath -lm -Ofast -mcpu=native -fomit-frame-pointer</li>
</ul>
<p>For the first HPL run, we submit the job requesting a total of 24 cores. There are a total of 28 cores
in the cluster, but we&rsquo;ve isolated the head node of the cluster as it&rsquo;s the NFS server for the environm
ent. We see that the head node turingpi shows a closed status here, meaning that it won&rsquo;t accept any jo
bs from LSF.</p>

<div class="highlight"><pre><code class="language-plaintext">lsfadmin@turingpi:/opt/HPC/hpl-2.3/bin/cm3_3 $ bhosts
HOST_NAME          STATUS       JL/U    MAX  NJOBS    RUN  SSUSP  USUSP    RSV 
kemeny             ok              -      4      0      0      0      0      0
neumann            ok              -      4      0      0      0      0      0
szilard            ok              -      4      0      0      0      0      0
teller             ok              -      4      0      0      0      0      0
turingpi           closed          -      4      0      0      0      0      0
vonkarman          ok              -      4      0      0      0      0      0
wigner             ok              -      4      0      0      0      0      0</code></pre></div>

<p><strong>Turing up the heat - literally</strong></p>

<p>Submit HPL using the LSF <em>bsub</em> command requesting 24 cores in the cluser with core affinity specified.</p>

<div class="highlight"><pre><code class="language-plaintext">lsfadmin@turingpi:/opt/HPC/hpl-2.3/bin/cm3_3 $ bsub -n 24 -R "affinity[core(1)]" -Is mpirun --mca btl_t
cp_if_exclude lo,docker0 ./xhpl
Job &lt;41861&gt; is submitted to default queue &lt;interactive&gt;.
&lt;&lt;Waiting for dispatch ...&gt;&gt;
&lt;&lt;Starting on neumann&gt;&gt;
================================================================================
HPLinpack 2.3  --  High-Performance Linpack benchmark  --   December 2, 2018
Written by A. Petitet and R. Clint Whaley,  Innovative Computing Laboratory, UTK
Modified by Piotr Luszczek, Innovative Computing Laboratory, UTK
Modified by Julien Langou, University of Colorado Denver
================================================================================

An explanation of the input/output parameters follows:
T/V    : Wall time / encoded variant.
N      : The order of the coefficient matrix A.
NB     : The partitioning blocking factor.
P      : The number of process rows.
Q      : The number of process columns.
Time   : Time in seconds to solve the linear system.
Gflops : Rate of execution for solving the linear system.

The following parameter values will be used:

N      :   15968 
NB     :      48       96      192 
PMAP   : Row-major process mapping
P      :       4        6 
Q      :       6        4 
PFACT  :   Right 
NBMIN  :       4 
NDIV   :       2 
RFACT  :   Crout 
BCAST  :  1ringM 
DEPTH  :       1 
SWAP   : Mix (threshold = 64)
L1     : transposed form
U      : transposed form
EQUIL  : yes
ALIGN  : 8 double precision words

--------------------------------------------------------------------------------

- The matrix A is randomly generated for each test.
- The following scaled residual check will be computed:
      ||Ax-b||_oo / ( eps * ( || x ||_oo * || A ||_oo + || b ||_oo ) * N )
- The relative machine precision (eps) is taken to be               1.110223e-16
- Computational tests pass if scaled residuals are less than                16.0

--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
An MPI communication peer process has unexpectedly disconnected.  This
usually indicates a failure in the peer process (e.g., a crash or
otherwise exiting without calling MPI_FINALIZE first).

Although this local MPI process will likely now behave unpredictably
(it may even hang or crash), the root cause of this problem is the
failure of the peer -- that is what you need to investigate.  For
example, there may be a core file that you can examine.  More
generally: such peer hangups are frequently caused by application bugs
or other external events.

  Local host: teller
  Local PID:  2253
  Peer host:  kemeny
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun noticed that process rank 23 with PID 2448 on node kemeny exited on signal 4 (Illegal instructio
n).
--------------------------------------------------------------------------</code></pre></div>

<p>We see above that the MPI rank(s) fail on host kemeny, which happens to be the CM3 module (not CM3+). E
ven though I compiled HPL natively on kemeny this issue persists. So ultimately, the HPL run was limite
d to the 5 remaining CM3+ nodes (i.e. 20 cores).</p>

<p>Next, we submit HPL requesting 20 cores (all on CM3+ modules). Core affinity is specified, and we reque
st specifically the model type &ldquo;CM3plus&rdquo;. The job was submitted interactively and the output follows:</p>

<div class="highlight"><pre><code class="language-plaintext">lsfadmin@turingpi:/opt/HPC/hpl-2.3/bin/cm3_3 $ bsub -n 20 -Is -R "select[model==CM3plus] affinity[core(
1)]" mpirun --mca btl_tcp_if_exclude lo,docker0 ./xhpl
Job &lt;41865&gt; is submitted to default queue &lt;interactive&gt;.
&lt;&lt;Waiting for dispatch ...&gt;&gt;
&lt;&lt;Starting on vonkarman&gt;&gt;
================================================================================
HPLinpack 2.3  --  High-Performance Linpack benchmark  --   December 2, 2018
Written by A. Petitet and R. Clint Whaley,  Innovative Computing Laboratory, UTK
Modified by Piotr Luszczek, Innovative Computing Laboratory, UTK
Modified by Julien Langou, University of Colorado Denver
================================================================================

An explanation of the input/output parameters follows:
T/V    : Wall time / encoded variant.
N      : The order of the coefficient matrix A.
NB     : The partitioning blocking factor.
P      : The number of process rows.
Q      : The number of process columns.
Time   : Time in seconds to solve the linear system.
Gflops : Rate of execution for solving the linear system.

The following parameter values will be used:

N      :   15968 
NB     :      48       96      192 
PMAP   : Row-major process mapping
P      :       4        5 
Q      :       5        4 
PFACT  :   Right 
NBMIN  :       4 
NDIV   :       2 
RFACT  :   Crout 
BCAST  :  1ringM 
DEPTH  :       1 
SWAP   : Mix (threshold = 64)
L1     : transposed form
U      : transposed form
EQUIL  : yes
ALIGN  : 8 double precision words

--------------------------------------------------------------------------------

- The matrix A is randomly generated for each test.
- The following scaled residual check will be computed:
      ||Ax-b||_oo / ( eps * ( || x ||_oo * || A ||_oo + || b ||_oo ) * N )
- The relative machine precision (eps) is taken to be               1.110223e-16
- Computational tests pass if scaled residuals are less than                16.0

================================================================================
T/V                N    NB     P     Q               Time                 Gflops
--------------------------------------------------------------------------------
WR11C2R4       15968    48     4     5             327.96             8.2776e+00
HPL_pdgesv() start time Sun Mar  3 20:29:45 2024

HPL_pdgesv() end time   Sun Mar  3 20:35:13 2024

--------------------------------------------------------------------------------
||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   2.74851526e-03 ...... PASSED
================================================================================
T/V                N    NB     P     Q               Time                 Gflops
--------------------------------------------------------------------------------
WR11C2R4       15968    96     4     5             315.71             8.5987e+00
HPL_pdgesv() start time Sun Mar  3 20:35:18 2024

HPL_pdgesv() end time   Sun Mar  3 20:40:34 2024

--------------------------------------------------------------------------------
||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   3.82600703e-03 ...... PASSED
================================================================================
T/V                N    NB     P     Q               Time                 Gflops
--------------------------------------------------------------------------------
WR11C2R4       15968   192     4     5             319.93             8.4854e+00
HPL_pdgesv() start time Sun Mar  3 20:40:38 2024

HPL_pdgesv() end time   Sun Mar  3 20:45:58 2024

--------------------------------------------------------------------------------
||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   2.56990081e-03 ...... PASSED
================================================================================
T/V                N    NB     P     Q               Time                 Gflops
--------------------------------------------------------------------------------
WR11C2R4       15968    48     5     4             342.36             7.9293e+00
HPL_pdgesv() start time Sun Mar  3 20:46:03 2024

HPL_pdgesv() end time   Sun Mar  3 20:51:45 2024

--------------------------------------------------------------------------------
||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   2.89956630e-03 ...... PASSED
================================================================================
T/V                N    NB     P     Q               Time                 Gflops
--------------------------------------------------------------------------------
WR11C2R4       15968    96     5     4             313.72             8.6531e+00
HPL_pdgesv() start time Sun Mar  3 20:51:50 2024

HPL_pdgesv() end time   Sun Mar  3 20:57:04 2024

--------------------------------------------------------------------------------
||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   3.04113830e-03 ...... PASSED
================================================================================
T/V                N    NB     P     Q               Time                 Gflops
--------------------------------------------------------------------------------
WR11C2R4       15968   192     5     4             312.48             8.6877e+00
HPL_pdgesv() start time Sun Mar  3 20:57:08 2024

HPL_pdgesv() end time   Sun Mar  3 21:02:21 2024

--------------------------------------------------------------------------------
||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   3.30812017e-03 ...... PASSED
================================================================================

Finished      6 tests with the following results:
              6 tests completed and passed residual checks,
              0 tests completed and failed residual checks,
              0 tests skipped because of illegal input values.
--------------------------------------------------------------------------------

End of Tests.
================================================================================</code></pre></div>

<p>We oberved during the HPL run that the CPU temperatures exceeded 80 degrees Celsius:</p>

<div class="highlight"><pre><code class="language-plaintext">root@turingpi:/home/lsfadmin# parallel-ssh -h /opt/workers -i "/opt/tools/cputemp.sh"
[1] 20:47:30 [SUCCESS] kemeny
Current CPU temperature is 61.22 degrees Celsius.
[2] 20:47:30 [SUCCESS] teller
Current CPU temperature is 82.21 degrees Celsius.
[3] 20:47:30 [SUCCESS] wigner
Current CPU temperature is 82.74 degrees Celsius.
[4] 20:47:31 [SUCCESS] szilard
Current CPU temperature is 82.21 degrees Celsius.
[5] 20:47:31 [SUCCESS] neumann
Current CPU temperature is 82.74 degrees Celsius.
[6] 20:47:31 [SUCCESS] vonkarman
Current CPU temperature is 83.28 degrees Celsius.

root@turingpi:/home/lsfadmin# parallel-ssh -h /opt/workers -i "/usr/bin/vcgencmd measure_clock arm" 
[1] 20:47:42 [SUCCESS] kemeny
frequency(48)=1199998000
[2] 20:47:43 [SUCCESS] szilard
frequency(48)=1034000000
[3] 20:47:43 [SUCCESS] teller
frequency(48)=980000000
[4] 20:47:44 [SUCCESS] wigner
frequency(48)=926000000
[5] 20:47:44 [SUCCESS] neumann
frequency(48)=818000000
[6] 20:47:44 [SUCCESS] vonkarman
frequency(48)=872000000</code></pre></div>

<p>And of course with high temperatures come CPU throttling. Clearly, with this thermal situation the run
of HPL was not going to be optimal.</p>

<p><strong>Giant Tiger to the rescue</strong></p>

<p>Even for those in Canada this may see like a very strange reference. <a href="https://www.gianttiger.com/">Giant Tiger</a> is a discount store ch
ain which sell everything from A through Z. Unfortunately the local &ldquo;GT Boutique&rdquo; as call it closed dow
n this past January. I happened to purchase on a whim a USB powered desktop fan at the GT Boutique abou
t a year ago. The idea was to help keep me cool at my keyboard during the hot summer days. But in this
case, it was just what was needed to provide a bit of active cooling to the Turing Pi system.</p>

<p>Repeating the run of HPL with the &ldquo;highly advanced active cooling&rdquo; measures in place, we were able to u
p the HPL results a tad while helping to preserve the life of the cluster nodes. And the results show g
oing from 8.65 GFlops with passive cooling to 9.5 GFlops with the active cooling.</p>

<div class="highlight"><pre><code class="language-plaintext">lsfadmin@turingpi:/opt/HPC/hpl-2.3/bin/cm3_3 $ bsub -n 20 -Is -R "select[model==CM3plus] affinity[core(
1)]" mpirun --mca btl_tcp_if_exclude lo,docker0 ./xhpl
Job &lt;41866&gt; is submitted to default queue &lt;interactive&gt;.
&lt;&lt;Waiting for dispatch ...&gt;&gt;
&lt;&lt;Starting on teller&gt;&gt;
================================================================================
HPLinpack 2.3  --  High-Performance Linpack benchmark  --   December 2, 2018
Written by A. Petitet and R. Clint Whaley,  Innovative Computing Laboratory, UTK
Modified by Piotr Luszczek, Innovative Computing Laboratory, UTK
Modified by Julien Langou, University of Colorado Denver
================================================================================

An explanation of the input/output parameters follows:
T/V    : Wall time / encoded variant.
N      : The order of the coefficient matrix A.
NB     : The partitioning blocking factor.
P      : The number of process rows.
Q      : The number of process columns.
Time   : Time in seconds to solve the linear system.
Gflops : Rate of execution for solving the linear system.

The following parameter values will be used:

N      :   15968 
NB     :      48       96      192 
PMAP   : Row-major process mapping
P      :       4        5 
Q      :       5        4 
PFACT  :   Right 
NBMIN  :       4 
NDIV   :       2 
RFACT  :   Crout 
BCAST  :  1ringM 
DEPTH  :       1 
SWAP   : Mix (threshold = 64)
L1     : transposed form
U      : transposed form
EQUIL  : yes
ALIGN  : 8 double precision words

--------------------------------------------------------------------------------

- The matrix A is randomly generated for each test.
- The following scaled residual check will be computed:
      ||Ax-b||_oo / ( eps * ( || x ||_oo * || A ||_oo + || b ||_oo ) * N )
- The relative machine precision (eps) is taken to be               1.110223e-16
- Computational tests pass if scaled residuals are less than                16.0

================================================================================
T/V                N    NB     P     Q               Time                 Gflops
--------------------------------------------------------------------------------
WR11C2R4       15968    48     4     5             319.43             8.4985e+00
HPL_pdgesv() start time Sun Mar  3 21:15:42 2024

HPL_pdgesv() end time   Sun Mar  3 21:21:01 2024

--------------------------------------------------------------------------------
||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   2.74851526e-03 ...... PASSED
================================================================================
T/V                N    NB     P     Q               Time                 Gflops
--------------------------------------------------------------------------------
WR11C2R4       15968    96     4     5             296.94             9.1423e+00
HPL_pdgesv() start time Sun Mar  3 21:21:05 2024

HPL_pdgesv() end time   Sun Mar  3 21:26:02 2024

--------------------------------------------------------------------------------
||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   3.82600703e-03 ...... PASSED
================================================================================
T/V                N    NB     P     Q               Time                 Gflops
--------------------------------------------------------------------------------
WR11C2R4       15968   192     4     5             289.03             9.3926e+00
HPL_pdgesv() start time Sun Mar  3 21:26:06 2024

HPL_pdgesv() end time   Sun Mar  3 21:30:55 2024
--------------------------------------------------------------------------------
||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   2.56990081e-03 ...... PASSED
================================================================================
T/V                N    NB     P     Q               Time                 Gflops
--------------------------------------------------------------------------------
WR11C2R4       15968    48     5     4             316.20             8.5855e+00
HPL_pdgesv() start time Sun Mar  3 21:30:59 2024

HPL_pdgesv() end time   Sun Mar  3 21:36:15 2024

--------------------------------------------------------------------------------
||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   2.89956630e-03 ...... PASSED
================================================================================
T/V                N    NB     P     Q               Time                 Gflops
--------------------------------------------------------------------------------
WR11C2R4       15968    96     5     4             285.87             9.4961e+00
HPL_pdgesv() start time Sun Mar  3 21:36:19 2024

HPL_pdgesv() end time   Sun Mar  3 21:41:05 2024

--------------------------------------------------------------------------------
||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   3.04113830e-03 ...... PASSED
================================================================================
T/V                N    NB     P     Q               Time                 Gflops
--------------------------------------------------------------------------------
WR11C2R4       15968   192     5     4             284.69             9.5355e+00
HPL_pdgesv() start time Sun Mar  3 21:41:09 2024

HPL_pdgesv() end time   Sun Mar  3 21:45:53 2024

--------------------------------------------------------------------------------
||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   3.30812017e-03 ...... PASSED
================================================================================

Finished      6 tests with the following results:
              6 tests completed and passed residual checks,
              0 tests completed and failed residual checks,
              0 tests skipped because of illegal input values.
--------------------------------------------------------------------------------

End of Tests.
================================================================================</code></pre></div>

<p>And during the runtime, we see that no throttling occurrred and the CPU temperatures hovered in the hig
h 50&rsquo;s to low 60 degree Celsius range.</p>

<div class="highlight"><pre><code class="language-plaintext">root@turingpi:/home/lsfadmin# parallel-ssh -h /opt/workers -i "/opt/tools/cputemp.sh"
[1] 21:41:25 [SUCCESS] kemeny
Current CPU temperature is 36.48 degrees Celsius.
[2] 21:41:25 [SUCCESS] teller
Current CPU temperature is 58.53 degrees Celsius.
[3] 21:41:25 [SUCCESS] vonkarman
Current CPU temperature is 58.00 degrees Celsius.
[4] 21:41:25 [SUCCESS] neumann
Current CPU temperature is 55.84 degrees Celsius.
[5] 21:41:25 [SUCCESS] szilard
Current CPU temperature is 61.76 degrees Celsius.
[6] 21:41:25 [SUCCESS] wigner
Current CPU temperature is 55.31 degrees Celsius.

root@turingpi:/home/lsfadmin# parallel-ssh -h /opt/workers -i "/usr/bin/vcgencmd measure_clock arm" 
[1] 21:41:29 [SUCCESS] kemeny
frequency(48)=1200000000
[2] 21:41:29 [SUCCESS] teller
frequency(48)=1200000000
[3] 21:41:29 [SUCCESS] vonkarman
frequency(48)=1200000000
[4] 21:41:29 [SUCCESS] wigner
frequency(48)=1200000000
[5] 21:41:29 [SUCCESS] neumann
frequency(48)=1200000000
[6] 21:41:29 [SUCCESS] szilard
frequency(48)=1200002000</code></pre></div>

<p><strong>Wrap up</strong></p>

<p>I always liked the idea of a small cluster that you could easily take with you. That&rsquo;s why I&rsquo;m strongly
considering the Turing Pi V2.5, which can work with the much more powerful CM4 omdules, among other ve
ry capable modules. Budget allowing, I hope to purchase a Turing Pi V2.5 sometime in 2024. As always st
ay tuned for more exciting high performance computing tales. And at the end of the day, a compute clust
er in a mini ITX format isn&rsquo;t a pie in the sky idea. For me, it&rsquo;s a great tool for learning!</p>]]></content><author><name>Ramblings of a supercomputing enthusiast.</name></author><category term="gaborsamu" /><summary type="html"><![CDATA[Overview]]></summary></entry><entry><title type="html">Profiling the XRootD Monitoring Collector</title><link href="https://hpc.social/personal-blog/2024/profiling-the-xrootd-monitoring-collector/" rel="alternate" type="text/html" title="Profiling the XRootD Monitoring Collector" /><published>2024-01-31T05:00:00-07:00</published><updated>2024-01-31T05:00:00-07:00</updated><id>https://hpc.social/personal-blog/2024/profiling-the-xrootd-monitoring-collector</id><content type="html" xml:base="https://hpc.social/personal-blog/2024/profiling-the-xrootd-monitoring-collector/"><![CDATA[<p>The <a href="https://github.com/opensciencegrid/xrootd-monitoring-collector">XRootD Monitoring Collector</a> (collector) receives file transfer accounting messages from <a href="https://xrootd.slac.stanford.edu/">XRootD</a> servers.
This transfer information is parsed by the collector and sent to the GRACC accounting database for visualization.
Each transfer will generate multiple messages:</p>

<ol>
  <li>Connection message with client information</li>
  <li>Token information</li>
  <li>File open with file name</li>
  <li>Transfer updates (potentially multiple)</li>
  <li>File close with statistics about bytes read and written</li>
  <li>Disconnection</li>
</ol>

<p>We can see 1000+ messages a second from XRootD servers across the OSG.  But, recently the collector has not been able to keep up.  Below is the traffic of messages to the collector from the OSG’s Message Bus:</p>

<figure class="">
  <img alt="this is a placeholder image" src="https://derekweitzel.com/images/posts/profiling-xrootd-collector/before-optimization-mq.png" /><figcaption>
      Message bus traffic before optimization

    </figcaption></figure>

<p>The graph is from the message bus’s perspective, so publish is incoming to the message bus, and deliver is sending to consumers (the Collector).  We are receiving (Publish) ~1550 messages a second, while the collector is only able to process (Deliver) ~500 messages a second.  1550 messages a second is higher than our average, but we need to be able to process data as fast as it comes.  Messages that are not processed will wait on the queue.  If the queue gets too large (maximum is set to 1 Million messages) then the messages will be deleted, losing valuable transfer accounting data.  At a defecit 1000 messages a second, it would only take ~16 minutes to fill the queue.  It is clear that we missed data for a significant amount of time.</p>

<h2 id="profiling">Profiling</h2>

<p>The first step to optimizing the XRootD Monitoring Collector is to profile the current process.  Profiling is the process of measuring the performance of the collector to identify bottlenecks and areas for improvement.</p>

<p>For profiling, I created a development environment on the <a href="https://nationalresearchplatform.org/">National Research Platform (NRP)</a> to host the collector.  I started a <a href="https://docs.nationalresearchplatform.org/userdocs/jupyter/jupyterhub-service/">jupyter notebook on the NRP</a>, and used VSCode to edit the collector code and a Jupyter notebook to process the data.  I used the <a href="https://docs.python.org/3/library/profile.html">cProfile</a> package built into python to perform the profiling.
I modified the collector to output a profile update every 10 seconds so I could see the progress of the collector.</p>

<p>After profiling, I used <a href="https://jiffyclub.github.io/snakeviz/">snakeviz</a> to visualize the profile.  Below is a visualization of the profile before any optimization.  The largest consumer of processing time was DNS resoluiton, highlighted in the below image in purple.</p>

<figure class="">
  <img alt="this is a placeholder image" src="https://derekweitzel.com/images/posts/profiling-xrootd-collector/before-optimization-profile.png" /><figcaption>
      Snakeviz profile.  Purple is the DNS resolution function

    </figcaption></figure>

<p>The collector uses DNS to resolve the hostnames for all IPs it receives in order to provide a human friendly name for clients and servers.  Significant DNS resolution is expected as the collector is receiving messages from many different hosts.  However, the DNS resolution is taking up a significant amount of time and is a bottleneck for the collector.</p>

<h2 id="improvement">Improvement</h2>

<p>After reviewing the profile, <a href="https://github.com/opensciencegrid/xrootd-monitoring-collector/pull/43">I added a cache to the DNS resolution</a> so that the collecotr only needs to resolve the host once every 24 hours.  When I profiled after making the change, I saw a significant improvement in DNS resolution time.  Below is another visualization of the profile after the DNS caching, purple is the DNS resolution.</p>

<figure class="">
  <img alt="this is a placeholder image" src="https://derekweitzel.com/images/posts/profiling-xrootd-collector/after-optimization-profile.png" /><figcaption>
      Snakeviz profile.  Purple is the DNS resolution function

    </figcaption></figure>

<p>Notice that the DNS resolution is a much smaller portion of the overall running time when compared to the previous profile.</p>

<p>In the following graph, I show the time spent on DNS resolution over time for both before and after the optimization.  I would expect DNS resolution to increase for both, but as you can see, the increase after adding DNS caching is much slower.</p>

<figure class="">
  <img alt="this is a placeholder image" src="https://derekweitzel.com/images/posts/profiling-xrootd-collector/dns-resolution.png" /><figcaption>
      Growth of DNS resolution time

    </figcaption></figure>

<h2 id="production">Production</h2>

<p>When we applied the changes into production, we saw a significant improvement in the collector’s ability to process messages.  Below is the graph of the OSG’s Message Bus after the change:</p>

<figure class="">
  <img alt="this is a placeholder image" src="https://derekweitzel.com/images/posts/profiling-xrootd-collector/edited-production-mq.png" /><figcaption>
      RabbitMQ Message Parsing

    </figcaption></figure>

<p>The incoming messages decreased, but the collector is now able to process messages as fast as they are received.  This is a significant improvement over the previous state.  I suspect that the decrease in incoming messages is due to server load of sending more outgoing messages to the improved collector.  The message bus can slow down the incoming messages under heavier load.</p>

<h2 id="conclusions-and-future-work">Conclusions and Future Work</h2>

<p>Since we implemented the cache for DNS resolution, the collector has been able to keep up with the incoming messages.  This is a significant improvement over the previous state.  Over time, we expect the DNS cache to capture nearly all of the hosts, and the DNS resolution time to decrease even further.</p>

<p>We continue to look for optimizations to the collector.  When looking at the output from the most recent profile, we noticed the collector is spending a significant amount of time in the logging functions.  By default, we have debug logging turned on.  We will look at turning off debug logging in the future.</p>

<p>Additionally, the collector is spending a lot of time polling for messages.  In fact, the message bus is receiving ~1500 messages a second, which is increasing the load on the message bus.  After reading through optimizations for RabbitMQ, it appears that less but larger messages are better for the message bus.  We will look at batching messages in the future.</p>]]></content><author><name>Derek Weitzel&apos;s Blog</name></author><category term="dweitzel" /><summary type="html"><![CDATA[The XRootD Monitoring Collector (collector) receives file transfer accounting messages from XRootD servers. This transfer information is parsed by the collector and sent to the GRACC accounting database for visualization. Each transfer will generate multiple messages:]]></summary></entry></feed>