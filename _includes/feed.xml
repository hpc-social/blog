<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en-US"><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="https://hpc.social/personal-blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://hpc.social/personal-blog/" rel="alternate" type="text/html" hreflang="en-US" /><updated>2025-12-29T20:47:57-07:00</updated><id>https://hpc.social/personal-blog/feed.xml</id><title type="html">hpc.social - Aggregated Personal Blog</title><subtitle>Shared personal experiences and stories</subtitle><author><name>hpc.social</name><email>info@hpc.social</email></author><entry><title type="html">Large software systems</title><link href="https://hpc.social/personal-blog/2025/large-software-systems/" rel="alternate" type="text/html" title="Large software systems" /><published>2025-12-28T19:50:27-07:00</published><updated>2025-12-28T19:50:27-07:00</updated><id>https://hpc.social/personal-blog/2025/large-software-systems</id><content type="html" xml:base="https://hpc.social/personal-blog/2025/large-software-systems/"><![CDATA[<p>In <em><a href="https://seangoedecke.com/nobody-knows-how-software-products-work/">Nobody understands how large software products work</a></em>, Sean Goedecke makes a number of good points about how difficult it is to really grasp large software systems.</p>

<p>In particular, some features impact every part of the system in unforeseen ways:</p>

<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p>Why are these features complicated? Because&nbsp;<strong>they affect every single other feature you build</strong>. If you add organizations and policy controls, you must build a policy control for every new feature you add. If you localize your product, you must include translations for every new feature. And so on. Eventually you’re in a position where you’re trying to figure out whether a self-hosted enterprise customer in the EU is entitled to access a particular feature, and&nbsp;<em>nobody knows</em>&nbsp;&#8211; you have to go and read through the code or do some experimenting to figure it out.</p>

</blockquote>

<p>Sean also points out that eventually the code itself has to be the source of truth, and debugging requires deep investigation of the continually-changing system.</p>

<p>I’ve seen this happen in a bunch of different orgs, and it does seem to be true, especially for products with a large number of collaborating teams. I would add that in addition to the code itself, you often need to have conversations with the relevant teams to discern intent and history. Documentation only goes so far, eventually you need talk to people.</p>]]></content><author><name>Thinking Out Loud</name></author><category term="ajdecon" /><summary type="html"><![CDATA[In Nobody understands how large software products work, Sean Goedecke makes a number of good points about how difficult it is to really grasp large software systems.]]></summary></entry><entry><title type="html">SC’25 recap</title><link href="https://hpc.social/personal-blog/2025/sc-25-recap/" rel="alternate" type="text/html" title="SC’25 recap" /><published>2025-12-01T14:34:00-07:00</published><updated>2025-12-01T14:34:00-07:00</updated><id>https://hpc.social/personal-blog/2025/sc-25-recap</id><content type="html" xml:base="https://hpc.social/personal-blog/2025/sc-25-recap/"><![CDATA[<p>
  The annual SC conference was held last week, drawing over
  <a href="https://www.hpcwire.com/2025/11/19/sc25-observations-more-pumps-than-processors/">16,000 registrants and 560 exhibitors</a>
  to in St. Louis, Missouri to talk about high-performance computing, artificial
  intelligence, infrastructure, and science. It was my tenth time attending
  in-person (12th overall), and as is always the case, it was a great week to
  reconnect with colleagues, hear what people are worrying about, and get a
  finger on the pulse of the now-rapidly changing HPC industry.
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">Outside the SC'25 convention center on the only clear day of the week.</figcaption>
</figure>
</div>
<p>Although every SC I've attended always felt a little different from the
  previous year, this one felt quite different. Part of that results from my own
  personal circumstances: this is the first year I attended as an employee of
  VAST Data, and so the people with whom I met and the technical problems to
  which I paid attention were certainly biased towards those most relevant to my
  work. But the backdrop of the whole conference has also shifted. It's been
  three SC conferences since ChatGPT came out, and it's now undeniable that AI
  isn't simply on the horizon; it's shaping the field of HPC and scientific
  computing. What used to be an argument of "<a href="https://blog.glennklockwood.com/2024/05/isc24-recap.html#section11">us vs. them</a>" is now more like "them (and us?)"<span></span></p>
<p></p>
<p>
  As has become tradition, I'm sharing some of my thoughts from the week with
  the world in the hopes that someone finds this interesting and insightful.
  I've roughly organized them into two areas big themes and the exhibition hall.
</p>
<ul style="text-align: left;">
<li>
<a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#big-themes">Big themes</a>
<ul>
<li>
<a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#theme-1-the-big-number-is-losing-its-shine">Theme 1: The big number is losing its shine</a>
<ul>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#top500">Top500</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#the-gordon-bell-prize">The Gordon Bell Prize</a></li>
<li>
<a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#fixing-problems-caused-by-the-big-number">Fixing problems caused by the big number</a>
</li>
</ul>
</li>
<li>
<a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#theme-2-hpc-policy-is-becoming-ai-policy">Theme 2: HPC policy is becoming AI policy</a>
</li>
<li>
<a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#theme-3-ai-discourse-is-growing-up">Theme 3: AI discourse is growing up</a>
<ul>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#agentic-workflows">Agentic workflows</a></li>
<li>
<a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#data-and-agentcentric-service-infrastructure">Data and agent-centric service infrastructure</a>
</li>
</ul>
</li>
</ul>
</li>
<li>
<a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#the-exhibit-hall">The exhibit hall</a>
<ul>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#by-the-numbers">By the numbers</a></li>
<li>
<a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#interesting-new-technology">Interesting new technology</a>
<ul>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#dell-ir700">Dell IR700</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#hpe-cray-gx5000">HPE Cray GX5000</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="big-themes">Big themes</h2>
<p>
  HPC has always been at the center of a tension between keeping things the same
  (supercomputers are the most stable the day they are turned off) and pushing
  the technological envelope (which is the fastest way to unlock new discovery).
  The desire to push the envelope has always been a "pull" towards the future;
  researchers first led with kooky ideas (like DAOS and Kokkos), and as those
  ideas turn from research into production, they make new technologies (like
  all-flash and AMD GPUs) accessible to scientists.
</p>
<p>
  What hasn't historically happened, though, is a strong "push" towards the
  future. Scientific HPC centers push themselves to justify building the next
  big supercomputer, but it's been a given that there will always be another big
  machine, so this push has been internal and gentle. Combined with the
  not-so-urgent pull of HPC researchers, every center has gotten a new machine
  every five years or so.
</p>
<p>
  This is the year where it became clear to me that AI is now exerting a strong
  push on the HPC industry--a shove even, forcing HPC centers around the world
  to align themselves on an AI mission if they want to survive. All the
  big-money HPC systems being announced this year are clearly being positioned
  as AI-first and AI-motivated, and these announcements are going well beyond
  simply peppering "AI" throughout the press release and otherwise acting as if
  it was business-as-usual. This is the first SC where I saw scientists,
  architects, and decision-makers being being forced to confront real tradeoffs
  favor either HPC or AI, and they are beginning to choose AI.
</p>
<p>
  This push-and-pull on HPC towards the future manifested in three big themes.
</p>
<h3 id="theme-1-the-big-number-is-losing-its-shine">
  Theme 1: The big number is losing its shine
</h3>
<p>
  HPC has long organized itself around treating the big machine and the big
  number as its top priority, and this is why the two largest HPC conferences of
  the year honor the semiannual release of the Top500 list on their main stage.
  However, this year felt like the first time that one number (that somehow
  reflects "performance") dominated the conversation. Instead, the discourse was
  more diffuse and discussed "performance and x" or "the supercomputer and x."
</p>
<h4 id="top500">Top500</h4>
<p>
  The place where this was most evident to me was at the
  <a href="https://sc25.conference-program.com/presentation/?id=bof117&amp;sess=sess409">Top500 BOF</a>, where the latest list was unveiled.
</p>
<p>
  The biggest announcement was that Europe now has its first benchmark-confirmed
  exascale system in JUPITER, which ran a full-system HPL at
  <a href="https://mastodon.social/@andih/115566907716591104">1,000,184 TFLOPS</a>
  for two hours and seven minutes. However, JUPITER didn't get any stage time at
  the BOF since, like Aurora, it actually debuted on a previous list with a
  sub-exascale run. This run pushed it over the finish exascale finish line, but
  if the Top500 list metadata is to be believed, the run used 100% of JUPITER's
  5,884 nodes to break the barrier--a feat that is unlikely to be reproduced on
  any production applications, since it is rare to have zero failed nodes in any
  large-scale production environment.
</p>
<p>
  So, while there was little fanfare for Europe in breaking the exaflops barrier
  with its new big machine and big number, there were some big
  announcements--one overt, and others more muted.
</p>
<p>
  The biggest news was that <strong>the Top500 list is changing hands</strong>.
  Whereas it has historically been controlled by three people--Jack Dongarra,
  Horst Simon, and Erich Strohmaier--it will be transitioning to be
  community-controlled under the stewardship of ACM SIGHPC. Dongarra, Simon, and
  Strohmaier will still be on the steering committee under the ACM stewardship,
  but this new governance structure opens the doors for new ideas to breathe new
  life into the way systems are ranked and, more broadly, how "performance" is
  meant to be interpreted from Rmax.
</p>
<p>
  At present, the list (and related lists) are bound by rules that, in the
  present day of reduced-precision accelerators, make little sense. For example,
  using the Ozaki scheme within the LU decomposition is not allowed by Top500
  despite the fact that it can produce the same answer with the same numerical
  accuracy much faster than hardware FP64. And while the HPL-MxP benchmark does
  allow solving the same problem using more creative methods, Strohmaier
  highlighted a problem there too: it never dictated how to deal with multiple
  levels of mixed precision until AIST broke the rankings. AIST ran HPL-MxP at
  both 16-bit and 8-bit precisions, resulting in their ABCI 3.0 system
  simultaneously ranking at #6 and #10.
</p>
<p>
  These sorts of issues make it easy to question the value of leaderboards like
  Top500 or HPL-MxP, as their definition of "performance" becomes increasingly
  further divorced from how large supercomputers are really used. The past few
  years have shown that there hasn't been the time or energy to get ahead of
  these ambiguities amongst the three men maintaining the list, so transitioning
  it to ACM will hopefully be a positive move that will give the list a chance
  to be revitalized.
</p>
<p>
  To their credit,
  <strong>the incipient stagnation of the Top500 list</strong> was called out by
  Strohmaier during his analysis of the list, acknowledging that "growth has
  tremendously slowed down compared to what it used to be" and "we don't have
  proof of what is actually the reason for that:"
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">All the key highlights of this SC's Top500 list.</figcaption>
</figure>
</div>
<p>China has stopped submitting, the AI and hyperscale providers really never
  started submitting, and retired systems are being thrown off the list long
  before they fall off the bottom. To me, this was a tacit acknowledgment that
  the list does not have a bright future out to 2030 unless it is modernized to
  be relevant to the way in which today's largest systems are actually being
  used--which is not DGEMM.
</p>
<p>
  The final surprising acknowledgment during Strohmaier's talk was that
  <strong>the list is trailing the state of the art in hardware</strong> by
  quite a bit. He pointed out that Blackwell systems are only now starting to
  appear even though they've been shipping in volume for the better part of a
  year. While he hypothesized that there is "uneasiness" about Blackwell in an
  HPC context, the reality is that there are no Blackwells for HPC until the
  Blackwell orders for hyperscale AI have been fulfilled. HPC is second in line,
  and even then, the only Blackwells I could find on this year's Top500 list
  were NVL8 configurations--not the NVL72 configurations that have been filling
  up hyperscale datacenters like
  <a href="https://glennklockwood.com/garden/systems/Fairwater">Fairwater</a>.
</p>
<p>
  Strohmaier pointed out that Blackwell, by virtue of its HBM3e (vs. Hopper's
  HBM3), is showing up higher on the HPCG list (which is a memory bandwidth
  test) than on Top500 (which is an FP64 FLOPS test). He phrased this as
  evidence that "not everything is bad for the HPC community," but I would have
  phrased my conclusion a little differently:
</p>
<ol type="1">
<li>
    Blackwell is actually great for HPC, because most real workloads are
    memory-bandwidth bound, not FLOPS bound. The fact that B200 offers similar
    FP64 FLOPS at higher memory bandwidth means that real applications will get
    higher effective use of those FP64 FLOPS.
  </li>
<li>
    Despite the above, Blackwell doesn't perform well on Top500 because HPL
    doesn't reflect the reality that memory bandwidth is important. It follows
    that HPL doesn't reflect the reality of real HPC applications. A Blackwell
    system can be significantly better for real HPC applications than a
    comparably sized Hopper system even though it may rank lower than Hopper on
    Top500.
  </li>
<li>
    Blackwell isn't showing up in volume now because the HPC community is second
    in line. The HPC community isn't uneasy as much as it is completely locked
    out. The first NVIDIA-based exascale system debuted in November 2025 despite
    its GPU being three years old, suggesting that if big Blackwell systems ever
    appear on Top500, it'll happen in 2026-2027.
  </li>
</ol>
<p>
  All of this is a roundabout way of showing that the big number--in this case,
  the HPL score--no longer leads meaningful conversation around how useful a
  system is for science.
</p>
<h4 id="the-gordon-bell-prize">The Gordon Bell Prize</h4>
<p>
  Another major indicator of the changing tide away from the big number was the
  work that won this year's
  <a href="https://awards.acm.org/bell">Gordon Bell Prize</a>. The winning
  paper, titled "<a href="https://arxiv.org/html/2504.16344v2">Real-time Bayesian inference at extreme scale: A digital twin for tsunami
    early warning applied to the Cascadia subduction zone</a>," wasn't the typical case of running a huge simulation for a few hours and
  reporting some result. Rather, it described a four-step workflow that
  culminates in the desired insight popping out of a computation that runs
  across only 128 nodes and completes in less than 0.2 seconds. Furthermore, the
  hero run part could be decomposed into trivially parallel components, allowing
  the bulk of the computation to be geographically distributed across HPC
  centers or GPUs spread across on-prem and cloud providers.
</p>
<p>
  My understanding of the work is that there was a massive "offline" computation
  to precompute a few key matrices (Phase 1) followed by two shorter offline
  steps that turn those matrices into the core of the digital twin. The last
  step, which was "online" and designed to be computed in real-time, could then
  take this core and solve the input problem with extremely low latency. This
  workflow front-loads a hero run in such a way that, if an earthquake were to
  occur, the risk of tsunami could be calculated in less than a second using
  only modest compute resources and the precomputed core.
</p>
<p>
  The authors eschewed methods that generated tons of FLOPS in favor of methods
  that were less FLOPS-efficient but got to the answer faster. In the authors'
  own words:
</p>
<blockquote>
<p>
    As shown in Fig. <a href="https://arxiv.org/html/2504.16344v2#S7.F7">7</a>, higher FLOP/s does not necessarily lead to faster time-to-solution. On
    MI300A nodes of <em>El Capitan</em>, the best-performing
    implementation, Fused PA, achieves a lower percentage (5.2%) of theoretical
    peak FLOP/s than Fused MF (5.5%) but is faster.
  </p>
</blockquote>
<p>
  Interestingly, the hero computation here was embarrassingly parallel(ish) as
  well; in their demonstration run, the hero run (Phase 1) was broken into 621
  independent calculations each requiring 128 nodes (512 A100 GPUs) for about an
  hour. Because they are independent, these tasks could be parallelized across
  multiple HPC centers as well, and my understanding of the data volumes
  involved are modest; Phase 1 would require a single shared copy of the input
  mesh (a hundred GiB?) per HPC center, and each of the 621 tasks would output
  around 8 GiB which would have to be copied back.
</p>
<p>
  While I don't understand the mathematics behind this work, the paper took what
  would've been a huge exascale-class mathematical problem ("10 years on a
  sustained 1 EFLOP/s machine") and reformulated it into a workflow that solves
  the problem faster and more usefully. Instead of brute-forcing the problem
  with a big supercomputer, they split it into separate offline and online
  parts, and this naturally allowed the most computationally expensive part to
  be geographically distributable.
</p>
<p>
  This work surrendered the need for a single big machine, and it didn't produce
  a big-number result. But it did win the Gordon Bell Prize, again signaling
  that the HPC community is beginning to look beyond performance-only and think
  about awarding innovation according to outcomes, not just FLOPS.
</p>
<p>
  The talk for this paper can be viewed
  <a href="https://sc25.conference-program.com/presentation/?id=gb106&amp;sess=sess577">here in the SC25 Digital Experience</a>.
</p>
<h4 id="fixing-problems-caused-by-the-big-number">
  Fixing problems caused by the big number
</h4>
<p>
  Most of my perception around the HPC community beginning to de-emphasize the
  singular big machine or big number arose from organic interactions I had with
  colleagues and customers though. It's hard to summarize how these
  conversations went, but the
  <a href="https://sc25.conference-program.com/presentation/?id=bof197&amp;sess=sess439">Lustre Community BoF</a>
  is a good example of what I saw elsewhere.
</p>
<p>
  Lustre has long been the gold standard in high-performance parallel I/O in the
  HPC community because it was designed from day one to deliver high bandwidth
  above all else. As a result, Lustre already has the big number solved in many
  ways, and events like the Lustre BOF are a great case study in what it looks
  like for a performance-first technology to be pushed into adapting to deliver
  more than just a big number.
</p>
<p>
  First, the ever-innovative Stéphane Thiell from Stanford discussed the process
  and tooling he developed to enable online capacity expansion of a Lustre file
  system. The basis for it was a distributed, fault-tolerant tool he developed
  that uses redis, lfs find, and lfs migrate to manage the state of file
  migrations across Lustre servers as the file system is rebalanced. While a
  part of me thought this was a great tool that would be super helpful for many
  others, another part of me was kind of horrified.
</p>
<p>
  Maybe I've been spoiled by working in hyperscale and AI these past three
  years, but online capacity expansion and rebalancing is a built-in capability
  of all distributed storage systems these days. All the major cloud object
  stores do this, as do all modern parallel file systems including Quobyte,
  VAST, and WEKA. Of course, none of these modern systems are as efficient (on a
  per-CPU core or per-SSD basis) as Lustre at delivering peak performance. But
  Stéphane's talk made me realize the price that's paid for this great
  performance.
</p>
<p>
  Andreas Dilger and others went on to talk about Lustre futures, and as they
  were speaking, I noticed that nobody was talking about performance
  improvements to Lustre. Rather, feature development was focused on catching up
  in every other dimension--data governance, reliability, manageability, and
  others. For example, Andreas talked a bit about the upcoming "multi-tenancy"
  features coming to Lustre:
</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">It's a lot of work to retrofit multitenancy into a performance-first file system.</figcaption>
</figure>
</div>

<p>I put “multi-tenancy” in quotes because these changes really represent
trying to back into a security posture that is fundamentally different from the
one that Lustre was designed around. In the pursuit of performance, Lustre (as
with most other HPC technologies) was designed assuming that security was
someone else’s problem. By the time someone could log into a system that could
mount a Lustre file system, they had already been authenticated, and it was up
to the OS on each compute node to authorize any interactions with Lustre itself.
This is the “implicit trust” model.</p>
<p></p>
<p>
  The problem, of course, is that the rest of the world has adopted a "zero
  trust" model which makes many things (except performance!) generally easier.
  Compliance is easier when the system assumes that everything is encrypted as a
  default and key management can be delegated to a third party. Because Lustre
  didn't do this from the outset, it is going through this process of
  retrofitting encryption in various places and using a mixture of nodemaps,
  UID/GID maps, and shared secrets to patch over all the places where trust was
  fundamentally implicit.
</p>
<p>
  Later on in the BOF, panelists acknowledged (some half-heartedly) that
  manageability of Lustre was a barrier. One panelist admitted that it took five
  years of work to almost get to the point where a Lustre update can be done
  without crashing applications. Another panelist said that multitenancy in
  Lustre is easy <em>if you follow a million steps</em>, and that his company
  was developing script-based ways to simplify this. While the idea of using
  scripts to simplify operations is not bad, from a secure supply chain
  standpoint, relying on third-party bash scripts to enable features required
  for legal compliance is horrifying.
</p>
<p>
  I don't mean to pick on Lustre alone here; other HPC technologies such as
  InfiniBand, Slurm, and DAOS are facing the same reality: retrofitting modern
  requirements like security and manageability into architectures that
  prioritized performance and scalability over everything else are now going
  through similar contortions to meet modern requirements around data
  governance. For those HPC centers who do not have to worry about compliance
  (which is most of open-science computing), these technologies will continue to
  be just fine.
</p>
<p>
  However, the
  <a href="https://blocksandfiles.com/2025/11/18/vast-data-dell-versity-and-spectra-logic-are-shining-storage-stars-on-taccs-horizon/">successes of these modern file systems</a>
<a href="https://blocksandfiles.com/2025/07/04/vast-doudna-supercomputer-storage/">across leading HPC centers</a>
  and the proliferation of alternative technologies such as
  <a href="https://nrp.ai">Kubernetes-based HPC</a> and
  <a href="https://blogs.microsoft.com/blog/2025/11/12/infinite-scale-the-architecture-behind-the-azure-ai-superfactory/?utm_source=chatgpt.com">MRC over Ethernet</a>
  tells me that HPC coming around to the idea that marginal increases in
  performance are no longer worth missing out on factors that weigh heavily on
  day-to-day operations like manageability, reliability, and flexibility.
</p>
<h3 id="theme-2-hpc-policy-is-becoming-ai-policy">
  Theme 2: HPC policy is becoming AI policy
</h3>
<p>
  Some of the biggest news at SC was not actually showcased at the conference
  despite being what many people wanted to talk about in side conversations: HPC
  policy is rapidly becoming AI policy, resulting in a slew of huge (but poorly
  defined) "public-private partnerships."
</p>
<p>
  As a bit of background, the Oak Ridge Leadership Computing facility announced
  its next system, Discovery, in late October--this was the result of a
  "typical" supercomputer procurement process that
  <a href="https://www.nextplatform.com/2023/10/02/the-first-peeks-at-the-doe-post-exascale-supercomputers/">first came into the public eye in 2023</a>. However, the Discovery announcement also included mention of a smaller
  system, Lux, which will "<a href="https://www.olcf.ornl.gov/2025/10/27/ornl-amd-and-hpe-to-deliver-does-newest-ai-supercomputers-discovery-and-lux/">leverage the Oracle Cloud Infrastructure (OCI)</a>" (whatever that means) to provide earlier access to AMD MI355X GPUs ahead of
  Discovery's full-scale deployment.
</p>
<p>
  Then, two days later, Argonne National Laboratory announced a
  <a href="https://www.energy.gov/articles/energy-department-announces-new-partnership-nvidia-and-oracle-build-largest-doe-ai">similar arrangement with Oracle Cloud and NVIDIA</a>
  to deliver a small (Lux-sized) GPU supercomputer named Equinox, followed by a
  much-larger 100,000-GPU supercomputer named Solstice. Neither Equinox nor
  Solstice are attached to a "typical" supercomputer procurement; the follow-on
  to Aurora, to be named
  <a href="https://intro-hpc-bootcamp.alcf.anl.gov/sites/hpc/files/2025-09/WelcomeToHPC_Papka.pdf">Helios</a>, is
  <a href="https://www.alcf.anl.gov/draft-technical-requirements-alcf-4-system">still in planning</a>
  and will be deployed in 2028. This strongly suggests that, whatever
  "public-private partnership" means to the DOE, it is not the same as the
  typical leadership computing systems; it is its own AI-centric program.
</p>
<p>
  At SC itself, Evangelos Floros (EuroHPC's head of infrastructure) also
  mentioned the "need for public-private partnerships" to realize EuroHPC's goal
  of building "AI Gigafactories" with "100,000 advanced AI processors" across
  Europe.
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">"Need for public-private partnerships" to fund AI factories is recognized by EuroHPC too.</figcaption>
</figure>
</div>
<p>Again, what exactly this "public-private partnership" model entails in Europe
  was never really defined.
</p>
<p>
  What was clear is that both American and European efforts are declaring the
  need to build massive (100K+ GPU) supercomputers for AI, the traditional HPC
  centers will be the public stewards of them, and "public-private partnerships"
  are the only way to realize them since governments alone cannot foot the bill.
</p>
<p>
  The Top500 BOF also included a short, awkward talk by Rick Stevens titled "The
  DOE AI Initiatives" that amounted to Stevens saying he had nothing to say.
  What really happened, I suspect, is that DOE's new "<a href="https://genesis.energy.gov">Genesis Mission</a>," which was announced the week <em>after</em> the SC conference, was a week
  late and therefore couldn't be discussed as originally planned. If Stevens had
  been able to describe the Genesis Mission, though, I'm sure he would've also
  described "public-private partnership" as a key aspect, since the same
  language is used in the
  <a href="https://www.whitehouse.gov/presidential-actions/2025/11/launching-the-genesis-mission/">Executive Order that established Genesis</a>. And I'm sure his description would've been no clearer about what this
  really means than what EuroHPC or the OCI/DOE descriptions have stated.
</p>
<p>
  Most revealing was my observation that, even outside of the proper conference
  program, nobody really knew what any of this meant. I talked to plenty of my
  colleagues from both government HPC and hyperscale cloud organizations, and
  the only consistent message was that there aren't many concrete facts backing
  up the the press releases right now. It appears that these partnerships were
  brokered far outside the usual channels that large supercomputer procurements
  are normally done, and the people in charge of actually delivering on the
  promises of the press releases are still figuring out what is possible.
</p>
<p>
  Connecting the dots between Lux/Equinox/Solstice, Genesis, and a recent
  <a href="https://www.energy.gov/sites/default/files/2025-04/RFI%20to%20Inform%20Public%20Bids%20to%20Construct%20AI%20Infrastructure%20%28website%20copy%29.pdf">RFI</a>
  and
  <a href="https://sam.gov/workspace/contract/opp/7864e8f4d61f42dc811ba095a41c8368/view">RFP</a>
  from DOE to allow
  <a href="https://www.energy.gov/articles/doe-announces-site-selection-ai-data-center-and-energy-infrastructure-development-federal">hyperscalers to build AI factories on federal land</a>, it appears that what is happening is...
</p>
<ul>
<li>
    The DOE has a bunch of land that is adjacent to the National Labs that is
    undeveloped but has the infrastructure to support massive AI factories.
    Specifically named is a 110-acre parcel at Argonne that can accommodate up
    to 1 GW "AI data park," and a 100-acre parcel at Oak Ridge with up to 800
    MW. These details were disclosed in
    <a href="https://www.energy.gov/sites/default/files/2025-04/RFI%20to%20Inform%20Public%20Bids%20to%20Construct%20AI%20Infrastructure%20%28website%20copy%29.pdf">an RFI they issued earlier in the spring</a>.
  </li>
<li>
    The
    <a href="https://www.energy.gov/articles/energy-department-announces-new-partnership-nvidia-and-oracle-build-largest-doe-ai">Solstice press release</a>
    specifically said that DOE envisions "shared investments and shared
    computing power between government and industry." Given the RFI/RFP were
    about land leases, these public-private partnerships may involve splitting
    the costs of space/power/cooling (the land and infrastructure being leased)
    and the capital/operations (the supercomputer cloud services being built)
    between the Labs and Oracle.
  </li>
</ul>
<p>
  A potential model for operations is that cloud providers are allowed to build
  and operate commercial AI cloud services adjacent to the DOE HPC facilities in
  exchange for the DOE Genesis Mission being entitled to some of those AI cloud
  capabilities. Exactly how much supercomputing resources hyperscalers like OCI
  would give to DOE, and exactly how much it would cost the DOE Labs to serve as
  landlords, is probably still undefined. But seeing as how power is the single
  biggest limiter in AI these days, I expect this model will only spread costs
  around, not actually lower them.
</p>
<p>
  If this is indeed how Genesis plays out, this would establish a bizarre new
  way for the government to acquire HPC (or AI) capabilities that completely
  sidesteps the standard procurement model. Instead of plunking down a hundred
  million dollars a year to finance a new leadership supercomputer, we might be
  moving into a world where the Labs plunk down a hundred million dollars a year
  to cover the costs of power, space, and cooling for a cloud provider. And
  instead of owning a leadership supercomputer, these national HPC facilities
  wind up consuming HPC (well, AI) resources from cloud providers--hopefully at
  a cost that reflects the fact that the cloud providers are also profiting from
  cycles being sold off of these machines to commercial AI customers.
</p>
<p>
  But again, this is all speculation based on the consistencies I heard
  throughout the conference and the experience I had trying to build these sorts
  of partnership with the HPC community while I worked at Microsoft. I may be
  right, or I may be wildly wrong. There are probably only a handful of people
  in the world with a clear idea of what these partnerships are meant to look
  like right now, and they are all way above the heads of the people at the HPC
  centers who will be tasked with executing on the vision.
</p>
<p>
  Selfishly, I am also left with a bit of heartburn over all of this news. I put
  a lot of personal time and energy into giving the HPC community the
  information it needed to feel comfortable about partnering with hyperscale AI
  infrastructure providers while I was at Microsoft, and it often felt like a
  Sisyphean task. Within months of me giving up and moving on from my career at
  a cloud provider, seeing a complete reversal of policy from the leadership HPC
  folks--and to see the "other guy" in pole position--is a bit of a slap in the
  face.
</p>
<p>
  I also couldn't help but notice that the cloud provider in all the headlines
  in the US didn't seem to demonstrate a very strong and unified presence at SC
  this year. Comically, they didn't even use their own brand's colors for their
  booth on the exhibit floor. And the color scheme they did use left no room for
  Oak Ridge's Lux system, which will be AMD-based, to be showcased.
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">Oracle's booth at SC25. Their brand color is red, not green. Or so I thought.</figcaption>
</figure>
</div>
<p>Though I may have read too much into this, it feels like these public-private
  partnerships are not necessarily composed of equal partners with equal levels
  of commitment.
</p>
<p>
  More broadly, I left the conference concerned that the discourse happening
  around these cloud-HPC/AI integrations--at least in the US--appears to have
  regressed compared to where it was when I worked at Microsoft. Many of the
  things we had to figure out years ago (cybersecurity models, impacts on jobs
  at the HPC centers) seem to have reset to zero. And sidestepping the
  procurement processes for leadership computing to enable these public-private
  partnerships will either require significant new funding (of which Genesis
  provides none; the executive order as-written appears to recolor existing
  money) or robbing Peter (the budget funding the next generation of leadership
  HPCs) to pay Paul (the cloud providers serving up compute resources for AI).
  As a result, I can envision a future where all of the money that used to fund
  leadership computing for science becomes money to fund commercial AI
  factories, resulting in a slow evaporation of the LCFs as their HPC
  capabilities shrink in size and relevance.
</p>
<p>
  Though there's lots more to be said on this topic, it's all based on
  conjecture. So, maybe the best thing to do is quietly wait and see.
</p>
<h3 id="theme-3-ai-discourse-is-growing-up">
  Theme 3: AI discourse is growing up
</h3>
<p>
  This was the first SC where it felt like the discourse around AI's role in the
  future of scientific computing actually carried some substance. Whereas
  previous years saw talk that mostly revolved around basic ideas like "do LLMs
  hallucinate too much?" or "can ChatGPT write MPI code?," I sat in on a number
  of interesting talks and conversations that skipped the question of "is AI
  useful?" and went straight to "this is how AI is proving useful to us."
</p>
<p>
  Maybe it's related to the previous theme: HPC money is becoming AI money, so
  AI research is becoming required to stay afloat. Or maybe it's because 2025
  has been the year of agentic AI, and agents allow LLMs to be integrated much
  more surgically into complex workflows. Or maybe confirmation bias led me to
  sit in sessions and talk with people who are at the frontier of applying AI to
  scientific discovery. Whatever the case may be, I was glad to hear so much
  discussion from researchers around the importance of all the connective tissue
  required to operationalize AI in scientific computing.
</p>
<h4 id="agentic-workflows">Agentic workflows</h4>
<p>
  A great example of this was the
  <a href="https://aiexscale.github.io">1st International Symposium on Artificial Intelligence and Extreme-Scale
    Workflows</a>, which happened on Friday. One of the invited speakers, Dr. Katrin Heitmann,
  connected a lot of dots in my head with a talk she gave on how massive-scale,
  physics-based simulation workflows can benefit from agentic AI.
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">Heitmann's vision on how agentic approaches can augment (but not replace) humans in complex scientific workflows.</figcaption>
</figure>
</div>
<p>The crux of the challenge faced by most massive-scale simulation (like
  <a href="https://cpac.hep.anl.gov/projects/hacc/">HACC</a>, the cosmology code
  for which she is famous) is that they generate massive amounts of data. The
  <a href="https://www.anl.gov/cels/article/simulating-the-cosmos-frontiere-sets-new-record-with-trillionparticle-universe-model">most recent HACC run</a>
  generated hundreds of terabytes of compressed data per checkpoint and over a
  hundred petabytes of data in the end; this cosmological simulation serves as a
  reference dataset from which downstream cosmological research can draw when
  exploring targeted questions. The challenge, of course, is finding relevant
  pieces of the simulated universe from amidst a hundred petabytes of raw data.
</p>
<p>
  Dr. Heitmann's premise is that agents and tools have very specific scopes and
  capabilities, and researchers have control over which of these tools they wish
  to use. However, they can hand off these tools to an agentic workflow to let
  it autonomously sift through all of the data, looking for specific features
  within the simulated universe that are relevant. A specific example she gave
  was the process of examining 500 million galaxy clusters; with an agentic,
  AI-driven approach, a postdoc was able to interactively sift through these
  objects without examining each one individually. For truly interesting
  objects, a separate agent could go search the literature and provide an
  explanation as to why it may be interesting, absolving the postdoc from having
  to make round trips between the dataset and external literature.
</p>
<p>
  That all said, it was clear from this talk (and others) that integrating
  agentic AI into scientific inquiry is still in its early days. But what I
  appreciated about this talk (and the entire workshop) is that it sidestepped
  pedestrian questions about trustworthiness by acknowledging that the goal
  isn't full autonomy, but rather, enabling researchers to do things faster.
  There is still a human at the start and the end of the workflow just as there
  always has been, but agents can reduce the number of times a human must be in
  the loop.
</p>
<h4 id="data-and-agentcentric-service-infrastructure">
  Data and agent-centric service infrastructure
</h4>
<p>
  Even when AI wasn't the main topic of discussion, it was clear to me at this
  SC that AI is influencing the way researchers are thinking about the
  infrastructure surrounding supercomputers. A great example of this was the
  keynote at the <a href="https://www.pdsw.org/index.shtml">PDSW workshop</a>,
  given by the ever-insightful
  <a href="https://sc25.conference-program.com/presentation/?id=misc185&amp;sess=sess202">Dr. Rob Ross, where he offered a retrospective on the work his team has
    done over the last two decades</a>, what he felt they got right, what they missed, and what's ahead.
</p>
<p>
  Towards the end of his presentation, he made the case that "science is
  increasingly multi-modal." But rather than talk about multimodality in the AI
  sense, he was emphasizing that there's more to scientific computing than
  performance:
</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">Domain science, provenance, search, and resilience are equal partners to performance in scientific computing.</figcaption>
</figure>
</div>

<p>Taken at face value, this slide positions performance on equal footing
with domain science, provenance, findability, and his argument was that we’ve
moved beyond the world where the only storage problem that HPC faces is
checkpointing. Just as Dr. Heitmann would say on Friday, Dr. Ross’ argument was
that the increasing volume of scientific data coming out of both exascale
simulation and scientific instruments is driving the field towards more
automation. And with automation comes a greater need to understand data
provenance–after all, if automation produces a surprising result, a human
ultimately has to go back and understand exactly how the automation generated
that result.</p>
<p></p>
<p>
  He also point out that in this coming world of automation-by-necessity,
  infrastructure itself might have to be rethought. After all, traditional
  technologies like parallel file systems were designed to make the lives of
  human researchers easier; when the primary consumer of data becomes AI agents,
  not humans, there may be better ways to organize and expose data than through
  files and directories. A human might repeatedly cd and ls to find a specific
  dataset on a file system, whereas an agent use a query a flat index to find
  the same data in a single step.
</p>
<p>
  At the end of the same PDSW workshop, I was fortunate enough to contribute to
  <a href="https://sc25.conference-program.com/presentation/?id=miscp112&amp;sess=sess202">a panel</a>
  where many of these same themes--how will data systems change as AI plays a
  greater role in scientific discovery--were discussed. Although we touched on a
  lot of topics, what stuck with me was a general acknowledgment that, while HPC
  has always talked about data management and provenance as being important,
  they were always treated as a "nice to have" rather than a "must have."
  However, as was echoed across many presentations (including the two I
  described above), governance and provenance are now becoming non-negotiable as
  larger datasets drive us towards AI-driven automation.
</p>
<p>
  Regardless of what you think about AI's ability to accelerate scientific
  discovery, I left SC with the feeling that AI is forcing the HPC community to
  grow up with regards to how seriously it takes data management:
</p>
<ul>
<li>
    The size and velocity of datasets generated by simulation or experiment is
    growing beyond any single person's ability to analyze it by hand. The
    complexity of these data are also making it harder to develop
    herustics-based or analytical approaches to combing through all of it.
  </li>
<li>
    The best path forward to understanding these data is through AI (via
    purpose-built models for analysis) or AI-driven data exploration (via
    autonomous, agentic workflows).
  </li>
<li>
    Automation or autonomous workflows will always act under authority delegated
    to them by human researchers, meaning there is a growing need to be able to
    inspect how these workflows arrived at the conclusions they generate.
  </li>
<li>
    Understanding how an answer was achieved requires significantly better data
    management features such as governance, provenance, and auditability. A
    result is ultimately only useful if a human can trust it, and that trust
    comes from understanding which data informed that conclusion, how that data
    was created, and how it was modified over time.
  </li>
</ul>
<p>
  Put differently, checkpointing was the main concern of I/O research because
  I/O performance was the first scalability issue that scientific computing ran
  into as supercomputers and scientific instruments got bigger. However, we're
  now at a point where issues ancillary to performance have reached the limits
  of scalability. Dr. Ross's multi-modal slide indicate that provenance,
  indices/search, and resilience are some examples of these new barriers, but
  there are plenty more as well.
</p>
<p>
  In a sense, this theme is the opposite side of the same coin as the first
  theme I discussed--that the big number is losing its shine. The hardest
  questions going forward aren't the obvious ones about scaling performance;
  they are about scaling everything else to keep up. AI seems to be the
  technology that has cleared a path to these data management hurdles, but the
  benefits of adopting strong data management practices and systems will extend
  far beyond the reach of just enabling AI-based automation.
</p>
<h2 id="the-exhibit-hall">The exhibit hall</h2>
<p>
  The exhibit hall has long been one of my favorite parts of attending SC
  because it's a great way to get a feeling for what technologies and vendors
  are hot, where the innovation is trending, and what sorts of commercial
  problems are worth solving. Every year I feel like I have less and less time
  to walk the exhibit hall though, and the layout and composition of this year's
  exhibition meant I only saw a small fraction of what I wanted to see in the
  few days it was up.
</p>
<p>
  The most common comment I heard about the exhibit this year is captured in
  Doug Eadline's article,
  <a href="https://www.hpcwire.com/2025/11/26/sc25-observations-more-pumps-than-processors/">SC25 Observations: More Pumps than Processors</a>
  (which is well worth the read!). The same commentary was repeated throughout
  the OCP conference in October as well, suggesting that there is a lot of money
  to be made (or at least the prospect of money) in helping datacenters get
  outfitted for the liquid cooling demanded by the next generation of
  large-scale GPU infrastructure. However, I found the overwhelming amount of
  space devoted to liquid cooling companies acutely problematic at SC25 this
  year for two reasons:
</p>
<ol type="1">
<li>
<strong>Most SC attendees have nothing to do with liquid cooling</strong>. A
    colleague of mine who operates supercomputers for the energy sector asked
    one of these big liquid cooling vendors what he could do to actually engage
    with them. After all, he doesn't buy liquid cooling infrastructure; he buys
    whole supercomputers that come with heat exchangers and CDUs that are
    integrated into the solution. The vendor had no good answer, because the
    reality is that the typical supercomputer user or buyer has no say over what
    piping, coolant, or exchangers are used inside the machine itself. The whole
    point of buying an integrated supercomputer is to not have to deal with that
    level of details.
  </li>
<li>
<strong>These liquid cooling vendors soaked up a ton of floor space</strong>. A few of these physical infrastructure providers had massive (50x50)
    booths sprinkled across the exhibit hall. Combined with the fact that the
    average SC attendee has nothing to do with liquid cooling meant that the
    booths that were more likely to be relevant to a typical attendee were much
    further apart than they had to be.
  </li>
</ol>
<p>
  The end result was that the exhibit hall was absolutely gargantuan and yet
  information-sparse. In fact, this year saw a secondary exhibit hall in the old
  football stadium serve as overflow space, because the entire primary exhibit
  hall was full. What's worse is that this overflow space was (as best as I
  could tell) completely disconnected from the main hall, and the only time I
  ever saw it was from the dining area used to serve lunch for the tutorials.
</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">The exhibit hall's overflow space being set up in the former football stadium.</figcaption>
</figure>
</div>

<p>I would’ve been furious if if I had been stuck with a booth in this
overflow space, because I can’t imagine the foot traffic in there was very high.
I personally couldn’t even find the entrance to this second exhibition area in
the few hours I had to look for it.</p>
<p></p>
<p>
  I can't help but think the SC organizers leaned far too much into booking up
  as much space (and therefore exhibitor dollars) as possible without thinking
  about the dilutive effects of having such a massive vendor count. Some vendors
  definitely benefitted from having a good location near one of the hall
  entrances, but I also heard a nontrivial amount of grumbling around how little
  traffic there was at some of the big booths. It wouldn't surprise me if there
  was a contraction of the HPC mainstays at SC26.
</p>
<h3 id="by-the-numbers">By the numbers</h3>
<p>
  Rather than rely solely on anecdotes though, it's also fun to take a
  quantitative look at the changes in exhibitors relative to last year. Since I
  spent the time figuring out how to generate tree maps for my SC24 recap last
  year, I figured I should re-run the same analysis to compare SC25 to SC24.
</p>
<p>
  Of the biggest booths who were exhibiting for the first time this year, it
  should be no surprise that the two biggest new entrants were Danfoss (liquid
  cooling infrastructure) and Mitsubishi Heavy Industries (gas turbines and
  other large-scale infrastructure):
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>
 <figcaption class="image-caption">New exhibitors with the largest booths.</figcaption></figure>
</div>
<p>Of the other top new exhibitors, some (Solidigm, Sandisk, C-DAC, MinIO, and
  University of Missouri Quantum Innovation Center) were quite relevant to the
  typical SC attendee. Arm was also back after having skipped SC24. But there
  were scores of new exhibitors whose services and products seem much more
  relevant to very niche aspects of physical datacenter infrastructure.
</p>
<p>
  Of the exhibitors who didn't show up to SC25 but had big booths at SC24, there
  was a diverse mix of markets:
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">Vendors who didn't show up to SC'25 but had big booths at SC'24.</figcaption>
</figure>
</div>
<p>Sadly, higher ed and government popped up on this list (see
  <a href="https://www.hpcwire.com/2025/11/26/sc25-observations-more-pumps-than-processors/">Doug Eadline's take on this for more</a>). A bunch of datacenter infrastructure providers also vanished, including
  Valvoline and Boundary Electric; this suggests that some of the top new
  vendors of this year (Danfoss, Mitsubishi) may similarly vanish entirely next
  year after realizing that SC isn't really their crowd. But I was also
  surprised to see some big names in AI vanish; Iris Energy (IREN) is a GPU
  cloud provider that just inked a multi-billion dollar deal with Microsoft;
  Ingrasys manufactures much of the world's GB200 NVL72 infrastructure; Groq,
  Sambanova, and SambaNova also inexplicably vanished.
</p>
<p>
  Perhaps more interesting are the top growers; these vendors exhibited both
  last year and this year, but went significantly larger on their booth sizes:
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">Biggest increases in booth size at SC'25 vs. SC'24.</figcaption>
</figure>
</div>
<p>Legrand, which provides datacenter infrastructure bits, likely grew as a
  result of it acquiring USystems and merging USystems' booth with Legrand's
  booth this year. The other big booth expansions are mostly household names
  though; Gates, EBARA, and GRC are cooling vendors that the typical SC attendee
  can't do much with, but the others are organizations with whom a researcher or
  HPC datacenter operator might actually talk to.
</p>
<p>
  Finally, the top contractions in booth space are a mix of service providers,
  HPC facilities or research centers, and component suppliers:
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">Biggest decreases in booth size at SC'25 vs. SC'24.</figcaption>
</figure>
</div>
<p>Of the biggest vendors who downsized, Carahsoft is a component reseller and
  service provider, Stulz is a liquid cooling company, HLRS is a German
  supercomputer center, and Viridien is an HPC services company that primarily
  serves the energy sector. It is surprising to see AWS shrink while Microsoft
  grew, and it is doubly surprising to see Oracle shrink when it's at the center
  of the biggest HPC deployment news of the season. Given that these booth sizes
  are chosen a year in advance, this may speak to how unexpected the turn of
  events were that resulted in Oracle carrying the cloud services end of DOE's
  big public-private partnerships.
</p>
<h3 id="interesting-new-technology">Interesting new technology</h3>
<p>
  For reasons I'll discuss later, I didn't have much time to walk the exhibit
  hall floor. Combined with the fact that everything was so spread out and
  diffuse, I just didn't get a great sense of what interesting new technology
  was being introduced this year beyond what tended to stick out. And amidst all
  the giant CDUs and liquid cooling infrastructure, it was hard for anything to
  stick out except really big compute cabinets.
</p>
<h4 id="dell-ir700">Dell IR700</h4>
<p>
  Dell's booth had a fully loaded IR7000 rack on display (as they did at
  <a href="https://blog.glennklockwood.com/2025/03/gtc-2025-recap.html#dells-480-kw-ir7000">GTC earlier in the year</a>) with 36 GB200 NVL4 sleds. At 50OU high (almost eight feet tall), this thing
  is physically huge:
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">Dell's 50OU IR7000 rack, fully loaded. This is what TACC Horizon will be built from.</figcaption>
</figure>
</div>
<p>Unlike the version they had on display at GTC though, this one had both the
  front door and a full rear-door heat exchanger installed:
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">HUGE rear-door heat exchanger on the back of the Dell IR7000 rack.</figcaption>
</figure>
</div>
<p>What's notable about this platform is that we now know that it is the basis
  for both
  <a href="https://tacc.utexas.edu/systems/horizon/">TACC's upcoming Horizon system</a>
  (which will have
  <a href="https://glennklockwood.com/garden/systems/Horizon">28 of these fully loaded racks</a>) and
  <a href="https://www.nersc.gov/what-we-do/computing-for-science/doudna-system">NERSC's upcoming Doudna system</a>
  (which will have Vera Rubin rather than Blackwell). This rack was nominally
  designed for hyperscale AI and is the basis for Dell's GB200 NVL72 (XE9712)
  deployments at places like CoreWeave and xAI, which means that it'll be
  thoroughly tested at scale long before TACC or NERSC have it up and running.
  This is the opposite of what has historically happened: before AI, it was
  usually government HPC that had to debug new rack-scale architectures before
  industry would touch it.
</p>
<h4 id="hpe-cray-gx5000">HPE Cray GX5000</h4>
<p>
  However, government HPC will still have a chance to debug a new supercomputing
  platform in the recently announced
  <a href="https://glennklockwood.com/garden/Cray-GX">Cray GX</a> (formally
  called "the HPE Cray Supercomputing GX platform"), which is the successor to
  the current Cray EX platform. This is the platform that the
  <a href="https://glennklockwood.com/garden/systems/Discovery">Discovery supercomputer</a>
  at OLCF will use, and HPE had a CPU-only blade (<a href="https://glennklockwood.com/garden/nodes/Cray-GX250">Cray GX250</a>) and a rack mockup on display at SC:
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">HPE's new GX blade form factor. This one appears to be the GX250, the 8-socket CPU-only blade.</figcaption>
</figure>
</div>
<p>It's hard to tell the size of this blade from the photo, but if you look at
  the relative size of the CPU socket and the DIMM slots, you can get a sense of
  how physically massive it is--it's like a coffee table. It also isn't
  perfectly rectangular; Cray decided to put this unusual protrusion on the
  front of the blades which is where the four NICs and eight E1.S SSDs are
  housed:
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">A look at the side of the Cray GX blade's "nose" showing the side-mounted NIC ports.</figcaption>
</figure>
</div>
<p>This nose(?) adds more surface area to the front of the rack, and it makes
  more sense when you see a rack full of these nodes. HPE had a full GX5000 rack
  with mocked-up cardboard nodes in their booth as well:
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">Fully loaded GX5000 rack. The nodes were cardboard, but pretty nice cardboard.</figcaption>
</figure>
</div>
<p>By having the NIC ports (which are Slingshot 400) face the sides of the rack
  rather than stick out the front, the bend radius of all that copper doesn't
  have to be quite as dramatic to route it along the sides of these node noses.
  And unlike previous Cray designs, there's also no midplane or backplane that
  connect the nodes in a rack to the rack-local switches; everything connects
  through discrete copper or optical cables.
</p>
<p>
  At the center of the rack is a liquid-cooled switch chassis, and each rack can
  support either 8-, 16-, or 32-switch configurations. Each switch is a 64-port
  Slingshot 400 switch, and I think the premise is that a single GX5000 rack is
  always exactly one dragonfly group. If you want a smaller group, you use a
  switch chassis with fewer switches.
</p>
<p>
  Interestingly, this GX will also support non-Slingshot Ethernet and XDR
  InfiniBand switches. Given that both XDR InfiniBand and 800G Ethernet are
  shipping today and have twice the bandwidth that Slingshot 400 will have when
  it starts shipping in a year, perhaps the Slingshot 400 option is just a
  stopgap until HPE's investments in Ultra Ethernet result in a product. The
  lack of a network backplane in the rack also makes it easier for the rack to
  accommodate the non-dragonfly topologies that would be required for InfiniBand
  or Ethernet.
</p>
<p>
  The rear of the rack is remarkably unremarkable in that it simply contains a
  rear bus bar and the liquid cooling manifolds and mates. In this sense, the
  rack looks very
  <a href="https://www.opencompute.org/documents/open-rack-base-specification-version-3-pdf">OCP-like</a>; the boring stuff is in the back, everything exciting is serviced from the
  front, and the rack itself is passive plumbing. Like any OCP ORv3 rack, power
  shelves slot in just as server blades do, and they use the same liquid cooling
  infrastructure as the rest of the rack. They power the bus bar, and the blades
  and switches draw from the same bus bar.
</p>
<p>
  Compared to an ORv3 rack though, these GX racks are wider and shorter. The
  width probably offers more flexibility for future NVIDIA or AMD GPU boards,
  but I was surprised that Cray didn't go ultra tall like Dell's 50OU IR7000. I
  was also surprised to hear that Cray is launching GX with a 400 kW cabinet
  design; power appears to already be a limiting factor in the nodes launching
  with GX. A single 400 kW GX rack can support
</p>
<ul>
<li>40 CPU-only blades (81,920 cores of Venice)</li>
<li>28 AMD Venice+MI430X blades (112 GPUs)</li>
<li>24 NVIDIA Vera+Rubin blades (192 GPUs)</li>
</ul>
<p>
  For reference, the demo GX5000 rack pictured above had only 29 blades and 16
  switches. I assume that fitting 40 blades into the rack requires using the
  smallest dragonfly group possible.
</p>
<p>
  On the cooling front, the GX5000 rack will launch with support for the same
  1.6 MW CDUs as the current Cray EX platform. I heard talk of a neat sidecar
  CDU option as well, but the person with whom I spoke at the HPE booth said
  that would come a little later.
</p>
<p>
  Overall, I was surprised by how un-exotic the new Cray GX platform is compared
  to what the AI world has been doing with ORv3 racks. The fact that Cray and
  Dell's designs are more similar than different suggests that the HPC/AI world
  is converging on a place where the future is uncertain, and flexibility is
  more important that highly engineered racks that optimize for very specific
  nodes and networks. It also suggests that the real value of buying Cray is
  higher up the stack; liquid cooling, power delivery, and rack integration is
  becoming commoditized thanks to AI.
</p>
<p>
  I was also surprised that Cray's next-generation design is not obviously
  superior to what the hyperscale community is designing. Whereas the GX rack
  caps out at 400 kW, Dell's will allegedly scale up to 480 kW. That said,
  today's IR7000 racks shipping for Horizon are only 215 kW (for GPU racks) and
  100 kW (for CPU-only racks) according to a talk given by Dan Stanzione:
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">The physical configuration of TACC's upcoming Horizon supercomputer.</figcaption>
</figure>
</div>
<p>So until the final specifications for the Rubin GPU are released, I suspect we
  won't know whether Cray still leads the pack in terms of compute density, or
  if Dell made the better bet by aligning its supercomputing platform on a
  standard OCP rack design.
</p>]]></content><author><name>Glenn K. Lockwood&apos;s Blog</name></author><category term="glennklockwood" /><summary type="html"><![CDATA[The annual SC conference was held last week, drawing over 16,000 registrants and 560 exhibitors to in St. Louis, Missouri to talk about high-performance computing, artificial intelligence, infrastructure, and science. It was my tenth time attending in-person (12th overall), and as is always the case, it was a great week to reconnect with colleagues, hear what people are worrying about, and get a finger on the pulse of the now-rapidly changing HPC industry.]]></summary></entry><entry><title type="html">The trap of prioritizing impact</title><link href="https://hpc.social/personal-blog/2025/the-trap-of-prioritizing-impact/" rel="alternate" type="text/html" title="The trap of prioritizing impact" /><published>2025-09-20T14:46:41-06:00</published><updated>2025-09-20T14:46:41-06:00</updated><id>https://hpc.social/personal-blog/2025/the-trap-of-prioritizing-impact</id><content type="html" xml:base="https://hpc.social/personal-blog/2025/the-trap-of-prioritizing-impact/"><![CDATA[<p>(I wrote this originally as a comment in <a href="https://randsinrepose.com/welcome-to-rands-leadership-slack/">RLS </a>in response to a staff-level engineer who was frustrated at how little they got to code anymore, and it resonated with enough folks that maybe it’s worth sharing here!)</p>

<p>There’s a trap I’ve seen a lot of staff+ folks fall into where they over-prioritize the idea that they should always be doing “the right, most effective thing for the company”. When I see engineers complain that they don’t get to code enough, I often suspect they’ve fallen prey to this.</p>

<p>I say <strong><em>that’s a trap</em></strong>! because I see people do this at the expense of their own job satisfaction and growth, which is bad for both them and (eventually) for the company which is likely to lose them.</p>

<p>I don’t blame people for falling into this trap, it’s what we’re rewarded for. I’ve fallen into it! I have stopped doing technical work I cared about, prioritized #impact, and fought fires wherever they arose. I have spent all my time mentoring and teaching and none coding. The result was often grateful colleagues, but also burnout and leaving jobs I otherwise liked.</p>

<p>Whereas when I’ve allowed myself to be like 30% selfish — picking some of my work because it was fun and technical, even when doing so was not the “most impactful” thing I could do — I was happier, learned more, and stayed in roles longer.</p>

<p>An example: I worked on a team that was doing capacity planning poorly and was buying too much hardware. (On-prem, physical hardware.) I could have solved the problem with a spreadsheet, but that was boring and made my soul hurt.</p>

<p>What I did instead was dig into how our container scheduling platform worked, and wrote a nifty little CLI tool that would look at the team’s configured workloads and spit out a capacity requirement calculation. It took about three times as long as the spreadsheet would have, but it was fun and accomplished the same goal and gave me some experience in the container platform. And it wasn’t that much of a time sink.</p>

<p>Was that better for the company? No idea. I hope it was — I hear the tool is still maintained and no one has replaced it with a spreadsheet yet! But that’s a happy accident.</p>

<p>Was it better for me? Absolutely! It was a bit selfish, but it made an otherwise tedious task more fun and I learned some useful tricks.</p>

<p>So — if you wish you had more time to code… go code a bit more. Don’t let the idea of being more effective guilt you into giving it up. Your career is your career and you should enjoy it.</p>]]></content><author><name>Thinking Out Loud</name></author><category term="ajdecon" /><summary type="html"><![CDATA[(I wrote this originally as a comment in RLS in response to a staff-level engineer who was frustrated at how little they got to code anymore, and it resonated with enough folks that maybe it’s worth sharing here!)]]></summary></entry><entry><title type="html">Lessons learned from three years in cloud supercomputing</title><link href="https://hpc.social/personal-blog/2025/lessons-learned-from-three-years-in-cloud-supercomputing/" rel="alternate" type="text/html" title="Lessons learned from three years in cloud supercomputing" /><published>2025-07-11T05:26:00-06:00</published><updated>2025-07-11T05:26:00-06:00</updated><id>https://hpc.social/personal-blog/2025/lessons-learned-from-three-years-in-cloud-supercomputing</id><content type="html" xml:base="https://hpc.social/personal-blog/2025/lessons-learned-from-three-years-in-cloud-supercomputing/"><![CDATA[<p>I recently decided to leave Microsoft after having spent just over three years there, first as a storage product manager, then as a compute engineer. Although I touched many parts of Azure's infrastructure during that time, everything I did was at the intersection of large-scale supercomputing and hyperscale cloud. There was no shortage of interesting systems to figure out and problems to solve, but as I began to wrap my arms around the totality of hyperscale AI training in the cloud, I also began to see the grand challenges that lay ahead.</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">Outside Microsoft's Silicon Valley Campus minutes after I was escorted off the premises.</figcaption>
</figure>
</div>
<p>Although many of those challenges would probably be fun and exciting to tackle, the more I learned, the more I found myself asking the same two questions: what did I want to do with the rest of my career, and was the path I was following going in the right direction? I spent a lot of time thinking about this, and my decision to leave Microsoft ultimately reflects the answer at which I arrived. But rather than indulge myself by recounting my introspection, I thought I would share some of the things that I learned while at Microsoft in the hopes that others find value in my experience.</p>
<p>To that end, I've split this post into two sections:</p>
<ol type="1">
<li>Things I've observed about <a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#hpc"><strong>HPC and technology trends</strong></a> from the perspective of a cloud/hyperscale/AI practitioner and provider, and</li>
<li>Things I've realized about <a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#career"><strong>jobs and careers</strong></a> from the perspective of someone who's now worked in <a href="https://www.sdsc.edu/">academia</a>, a <a href="https://www.cnbc.com/2019/09/12/10x-genomics-txg-biotech-start-up-surges-in-ipo-debut.html">successful startup</a>, <a href="https://www.nersc.gov/">government</a>, and now <a href="https://www.microsoft.com/">Big Tech</a> and is about halfway through his career</li>
</ol>
<p>I consider this to be the concluding chapter of a three-part series that began with <a href="https://blog.glennklockwood.com/2022/05/life-and-leaving-nersc.html">Life and leaving NERSC</a> and continued with <a href="https://blog.glennklockwood.com/2024/08/how-has-life-after-leaving-labs-been.html">How has life after leaving the Labs been going</a>.</p>
<p>Also, please note that I authored this the day after my employment at Microsoft ended, and I was not beholden to any company or organization at the time of writing. <i>The views expressed below are mine alone</i>.</p>
<!--<ul><ul><li><a href="#hpc">HPC</a><ul><li><a href="#hpc-wants-to-be-like-the-cloud-not-in-it">HPC wants to be like the cloud, not in it</a></li><li><a href="#cloud-is-expensive-but-not-for-the-reasons-most-think">Cloud is expensive, but not for the reasons most think</a></li><li><a href="#although-sometimes-it-is">...Although sometimes it is</a></li><li><a href="#influencing-the-cloud-is-hard">Influencing the cloud is hard</a></li></ul></li><li><a href="#career">Career</a><ul><li><a href="#people-matter">People matter</a></li><li><a href="#company-culture-matters-too">Company culture matters, too</a></li><li><a href="#being-good-at-things-isnt-always-a-job">Being good at things isn't always a job</a></li><li><a href="#you-dont-have-to-be-your-employer">You don't have to be your employer</a></li><li><a href="#happiness-sometimes-costs-money">Happiness sometimes costs money</a></li></ul></li><li><a href="#final-thoughts">Final thoughts</a></li></ul></ul>-->
<h2 id="hpc">HPC</h2>
<p>Everything I did at Microsoft touched supercomputers in one way or another, and my day job was exclusively supporting Microsoft's largest AI training supercomputers. Despite that, I did a lot of moonlighting in support of Azure's Federal business, and this is how I justified giving talks at events like like <a href="https://sites.google.com/lbl.gov/nersc50-nug/home">NERSC@50</a>, <a href="https://sc24.supercomputing.org">SC</a>, and <a href="https://www.glennklockwood.com/garden/Salishan">Salishan</a> in my last year. It's also what let me straddle both worlds: I had a rare, first-hand knowledge of how the <a href="https://www.glennklockwood.com/garden/systems/Eagle">de facto largest supercomputers in the world</a> were built and used, and I had a front-row seat for how leaders in the traditional supercomputing world perceived (and sometimes misunderstood) what we were doing in the cloud.</p>
<p>Before I get into specific observations though, I should clarify some nomenclature that I will use throughout:</p>
<ul>
<li><strong>Supercomputers</strong> are the piles of compute nodes with a high-speed interconnect that are designed to solve one big problem in parallel. This is a generic term to describe the instrument, not its workload.</li>
<li><strong>HPC</strong>, <strong>traditional HPC</strong>, <strong>modsim</strong>, and <strong>scientific computing</strong> all refer to the ecosystem built around using something like MPI to solve a problem rooted in some type of science. Every big supercomputer run by DOE, procured through EuroHPC, and sited at the world-famous, government-funded supercomputer centers falls into this category.</li>
<li><strong>Cloud</strong>, <strong>hyperscale</strong>, and <strong>AI training</strong> all refer to the ecosystem built to train large language models. The supercomputers are run by hyperscale companies like Microsoft, Amazon, or Meta whose backgrounds have not historically been in the world of supercomputing.</li>
</ul>
<p>I realize that these are not very precise, but they're the easiest way to contrast what I learned inside Microsoft (a hyperscale cloud) with the world I came from prior (traditional HPC).</p>
<h3 id="hpc-wants-to-be-like-the-cloud-not-in-it">HPC wants to be like the cloud, not in it</h3>
<p>When I left NERSC in May 2022, <a href="https://blog.glennklockwood.com/2022/05/life-and-leaving-nersc.html">I speculated that the future of large-scale supercomputer centers</a> would be follow one of two paths:</p>
<ol type="1">
<li>They develop and squish cloud technologies into their supercomputers to make them more cloud-like, or</li>
<li>They abandon the idea of buying individual systems and instead enter into long-term relationships where flagship HPC systems are colocated inside cloud datacenters sited in places with low-cost, low-carbon power.</li>
</ol>
<p>I was hoping that the desire to continue building systems after passing the exascale milestone would make the next click-stop follow path #2, but early indications (across the global HPC landscape) are that the community has chosen path #1.</p>
<p>HPC centers around the world are embracing the idea of cloudifying on-prem supercomputers by adding virtualization, containerization, and integration with other services to enable complex workflows. And as a part of that, they're reinventing many of the technology integrations that have always been first-class citizens in cloud: CSCS added capabilities to create <a href="https://www.cscs.ch/publications/news/2024/new-research-infrastructure-alps-supercomputer-inaugurated">"versatile software-defined clusters" on their latest Cray system, Alps</a>. NERSC's next system, Doudna, is envisioned to allow its users to "<a href="https://www.vastdata.com/sharedeverything/how-nersc-is-rewriting-the-role-of-the-supercomputer">move from programming the supercomputer to programming the datacenter</a>." But none of these systems are actually using commercial cloud services in non-trivial ways.</p>
<p>In the year or two that followed ChatGPT, the notion of large-scale supercomputers in the cloud was a green field, and cloud providers were open to chasing all sorts of silly ideas. This made it the ideal time for the leadership HPC computing community to get a seat at the hyperscale table. Although their budgets couldn't compete with AI, HPC centers could've drafted on the investments of AI buildout and offered the societal impacts of using GPUs for science as a nice complement to the societal impacts of using GPUs for AI training.</p>
<p>Much to my dismay, though, that window of opportunity was spent decrying the investment in hyperscale and AI rather than trying to exploit it; that window was the year of "<a href="https://blog.glennklockwood.com/2024/05/isc24-recap.html#section11">us versus them</a>." And unfortunately, that window has essentially closed as accountants and CFOs have now sharpened their pencils and are searching for returns on the investments made in GPU infrastructure. The intrinsic value of supercomputing infrastructure in the cloud has been reduced to the point where <a href="https://www.theregister.com/2024/10/31/microsoft_q1_fy_2025/">Microsoft's CEO outright said they were turning away customers who just wanted to pay for GPU clusters</a>, because higher-quality revenue could be made from inferencing services that use those same GPUs.</p>
<p>So even if the HPC community woke up tomorrow and realized the long-term benefits of partnering with commercial clouds (instead of trying to copy them), I don't think cloud providers would respond with the same enthusiasm to meet in the middle now as they would have a year or two ago. I don't think this was a deliberate decision on behalf of the cloud providers, and they may not even fully realize this change. But the future of hyperscale supercomputing is rapidly crystallizing, and because HPC wasn't present in the solution, there's no room for it in the final structure.</p>
<h3 id="cloud-is-expensive-but-not-for-the-reasons-most-think">Cloud is expensive, but not for the reasons most think</h3>
<p>It's been easy to write off the cloud as too expensive for HPC, and most people do silly math based on public list prices for VMs to justify their position. The narrative usually goes something like, "<a href="https://info.ornl.gov/sites/publications/Files/Pub202373.pdf">if a single GPU VM costs $40/hr, then running 10,000 of them for five years will cost 17X more than our on-prem supercomputer!</a>" That's not how it works, and nobody pays that price. That $40/hr is the maximum possible price, and it includes the cost to the cloud provider of keeping nodes idle in the event that someone shows up and suddenly wants to use one on-demand.</p>
<p>But even if you cut out all the profit for the cloud provider and just look at the cost of the physical infrastructure, building a supercomputer in the cloud is just more expensive than putting a bunch of whitebox nodes into a traditional HPC datacenter. There's a couple reasons for this, and here are a couple in no particular order:</p>
<p><strong>High availability</strong>: Every cloud datacenter has redundant power, and most of them have <em>very</em> redundant power. This is provisioned independently of whatever goes inside of that datacenter, so when you deploy a 10 MW supercomputer inside a 10 MW cloud datacenter, that comes with at least 10 MW of backup diesel generators, UPSes, and the electrical infrastructure. HPC workloads don't really need this, but it's hard to deploy HPC in the cloud without a ton of generators and UPSes coming along for the ride. This is changing with AI-specific cloud datacenters now being built, but these AI datacenters still have way more redundant power than a typical on-prem HPC datacenter. Building a cloud datacenter with the minimal redundancy that a traditional HPC datacenter has would mean that facility couldn't ever be used for anything but HPC, and that would undercut the overall flexibility upon which cloud economics are built.</p>
<p><strong>Cloud-side infrastructure</strong>: Every compute node has to be attached to the frontend cloud network in addition to a backend high-speed network like InfiniBand, unlike a traditional supercomputer where nodes are only attached to one high-speed network. While the cost of the smart NIC in each node is just a couple hundred dollars, every cloud supercomputer has to have a complete frontend network built out to support every single compute node--that's a ton of switches, routers, and fiber that must be properly provisioned all the way up to the cloud region in which those nodes are deployed. This frontend network is what enables all the cool cloud features on every node (full SDN, integration with other cloud services, etc), but these features aren't generally worth their cost when running meat-and-potatoes HPC workloads like MPI jobs by themselves. Their value only really shines through when executing complex workflows that, for example, couple an MPI job with stateful services and globally accessible data sharing with fine-grained access controls, all fully automated through programmable APIs and full RBAC.</p>
<p><strong>AI-optimized system architecture</strong>: AI-optimized GPU supercomputers contain a bunch of components that your typical Cray or Eviden simply wouldn't have. I wrote about the <a href="https://www.glennklockwood.com/garden/differences-between-AI-and-HPC">differences between AI and HPC supercomputers elsewhere</a>, but in brief, AI workloads specifically benefit from having tens of terabytes of local SSDs and all-optical (no copper) RDMA fabrics. These add to the COGS (cost of goods sold) of an AI-optimized supercomputer, meaning that that a supercomputer with a thousand GPUs designed for AI is going to be more expensive than one designed for scientific computing no matter where it's deployed. And cloud providers are all optimizing their supercomputers for AI.</p>
<p>There's a bunch of other cloud "stuff" that is required as well; every cloud region has a first footprint which is a LOT of general-purpose servers and storage that is required to support the basic cloud control plane. Before any user-facing cloud resources (including supercomputers) can be deployed, there has to be tens or hundreds of racks of this cloud "stuff" that is up and running. And although the cost of that first footprint is amortized over many customers in larger or older cloud regions, larger single-use infrastructures (like supercomputers) carry a proportionally larger fraction of the cost to deploy the first footprint.</p>
<p>So when you look at the cost of running a single compute node in a cloud supercomputer, there are a bunch of extra ingredients baked in that you wouldn't get by just signing a check over to an OEM:</p>
<ul>
<li>a high availability SLA, afforded in part by all those generators and UPSes</li>
<li>slick cloud service integrations, privacy features, virtual networking, afforded by that frontend cloud network</li>
<li>better performance for AI training or inferencing workloads, afforded by extra SSDs and all-optical interconnects</li>
<li>a bunch of other typical TCO stuff--the power consumed by the node, the opportunity cost of free floor tiles in your datacenter, and the engineers and technicians that keep it all running</li>
</ul>
<p>Ultimately, someone needs to pay for all of these extra ingredients. Cloud providers <em>could</em> just eat the costs themselves and sell the supercomputing service at a price comparable to what a customer would pay for an on-prem supercomputer--and sometimes they do. But this dilutes the profitability of the deal, and it increases the risks of the cloud provider losing money if unexpected issues arise during execution. Losing money is objectively bad business, so it's usually cloud customers who are paying for all these extra capabilities regardless of if they use them or not.</p>
<p>So if all you want to do is run big MPI jobs, and you have no use for the extra availability, cloud integrations, privacy and security, and programmable infrastructure, sure--the price per-node is going to be higher in the cloud than on-prem. You're paying for a bunch of features that you don't need.</p>
<h3 id="although-sometimes-it-is">...Although sometimes it is</h3>
<p>Sometimes buying a supercomputer in the cloud is straight up more expensive because of the value it provides though. For example, I remember a case where a large AI company needed to train a big LLM on many thousands of GPUs, so they signed an agreement which gave them exclusive access to a cloud supercomputer that strongly resembled a specific GPU system in the DOE complex. Because I used to work in the DOE, I knew how much DOE paid to buy their GPU cluster, and I also knew that three years of maintenance was included in that cost.</p>
<p>What amazed me is what this AI company was willing to pay (roughly) the same price that DOE paid for their on-prem supercomputer, but in exchange, get exclusive access to a comparably capable cloud supercomputer (same GPUs model, similar GPU count, similar interconnect) for <em>one year only</em>. Put differently, being able to use a big, cutting-edge GPU cluster was worth up to 3x more to this AI company than it was to the DOE.</p>
<p>While it may sound like I'm spilling secrets here, the reality is that anyone working for a cloud provider wouldn't be able to tell which AI deal I was describing here--they all look like this, and they're all willing to spend significantly more than the HPC community for the same compute capability. This gives you a sense of the real value that AI companies place on all the benefits that cloud-based supercomputers can provide.</p>
<p>This isn't all bad for HPC, though. Every fat deal with an AI company means that there can be another deal with an HPC center that has slim margins. For example, let's say an AI company is willing to pay a billion dollars for a supercomputer whose TCO is only $330M--that means the cloud provider gets 67% margin. If the cloud provider's overall margin target is 50%, that means it can sell an identical supercomputer to an HPC customer at zero profit (for $330M) and still walk away happy. Thus, it is possible for the price of a supercomputer for HPC to be subsidized by all the money that the AI industry is throwing into supercomputing. Whether or not a cloud provider ever cuts deals like this is a business decision though--and as I said earlier, I don't think they're as open to silly ideas now as they used to be.</p>
<p>The real hurdle that I was never able to overcome out, though, is a result of the fact that there is finite expertise in HPC and AI in the world. HPC-AI is ultimately a zero-sum game, and every hour spent working with an HPC customer is usually an hour that isn't being spent working with a much more profitable AI customer. I constantly ran into this problem working in hyperscale AI; my full-time job was to deal with AI customers, but I enjoyed interacting with HPC customers too. As a result, I had to do a lot of my the HPC-specific work (preparing conference presentations, for example) on nights, weekends, and vacations. It was just hard to tell people that I couldn't help improve job uptime on a massive training run because I was preparing a talk for a workshop that, frankly, might be openly hostile to my message.</p>
<h3 id="influencing-the-cloud-is-hard">Influencing the cloud is hard</h3>
<p>Because the difference in investment is so big between HPC and AI, many of the carrots that the HPC community has traditionally dangled in front of HPC vendors aren't very enticing to the hyperscale AI community. For example, both US and European HPC programs have relied heavily on non-recurring engineering (NRE) contracts with industry partners to incentivize the creation of products that are well-suited for scientific computing; <a href="https://www.energy.gov/articles/department-energy-awards-six-research-contracts-totaling-258-million-accelerate-us">PathFoward</a> and <a href="https://research-and-innovation.ec.europa.eu/funding/funding-opportunities/funding-programmes-and-open-calls/horizon-2020_en">Horizon 2020</a> both come to mind as well-funded, successful efforts on this front.</p>
<p>However, HPC is the only customer community that really tries to do this, and it echoes a time when the HPC community was at the forefront of scale and innovation. Nowadays, the prospect of accepting $1M/year NRE contract to implement XYZ is completely unappetizing to a hyperscaler; it would probably cost more than $1M/year just to figure out how a company with <a href="https://www.microsoft.com/investor/reports/ar24/">$250 billion in annual revenue</a> can handle such an unusual type of contract and payment. Add to to this the weird intellectual property rules (like disentangling a <a href="https://www.energy.gov/gc/articles/advance-patent-waiver-wa2017-007?utm_source=chatgpt.com">40% cost sharing advance waiver</a> for a tiny project within a multi-billion-dollar business), and it can become a corporate quagmire to go anywhere near NRE projects. Companies with well-insulated HPC silos can probably manage this better, but part of hyperscale economics is that everything overlaps with everything else as much as possible across supercomputing, general-purpose computing, hardware, and software.</p>
<p>As a result of this, I really struggled to understand how a $20M/year service contract and a $1M/year NRE contract is materially different from a $21M/year service contract in the cloud world. For most (non-HPC) cloud customers, the RFP comes in saying "we need XYZ" and some product manager notes customer demand for XYZ. If the demand is large enough, the feature winds up on roadmap, and the cloud provider develops it as a part of regular business. If there is no other demand, then an NRE contract isn't really going to change that; maintaining feature XYZ long-term will cost far more than a couple million dollars, so implementing it would be a bad decision. This isn't unique to cloud, for what it's worth; while there are some successful HPC NRE stories, there are far more NRE-originated products that had no product-market fit and were <a href="https://cug.org/proceedings/cug2016_proceedings/includes/files/pap105s2-file1.pdf">simply abandoned</a> after the associated supercomputer was retired.</p>
<p>As best as I can tell, NRE has become a way for big HPC customers to maintain the illusion that they are influencing hyperscalers. A hyperscaler could propose some NRE, and an HPC buyer could fund it, and there could be weekly meetings where the two get together and pretend like they're collaborating and codesigning. The hyperscaler could write milestone reports, and they could attend quarterly business reviews with the customer. But this feels like an act. You simply can't move a $250B/year company that isn't solely organized around supercomputing with the lure of a couple million a year.</p>
<p>This is not to say that NRE and codesign have no purpose in HPC! I'm sure component vendors (GPUs, networking, and the like) can make minor tweaks that offer big upside for the HPC community. But I learned that, as in several other dimensions, the HPC community is being pushed towards buying whatever is already on the truck, and NRE isn't going to have the impact that it once did.</p>
<h2 id="career">Career</h2>
<p>In addition to learning about how the hyperscale supercomputer world works in practice, my time at Microsoft exposed me to a segment of the supercomputing community that I didn't know existed: junior software engineers who were unwittingly thrown into the deep end of HPC straight out of college and were desperate to find their footing in both the technology and their careers overall. Maybe the most impactful work I did in the past three years was not technical at all, but instead came through some internal talks I gave on my professional journey in HPC and the one-on-one conversations that followed.</p>
<p>Since I've gotten such positive feedback when I talk and write about this aspect of HPC, I'll also share some things I've learned about choosing the right employer and job during my time at Microsoft.</p>
<h3 id="people-matter">People matter</h3>
<p>I learned that the right team matters more than the right job. It is profoundly important to me that I get to work with people with the same level of passion and curiosity, even if we are working on different problems.</p>
<p>In retrospect, I realize that I have been very lucky that my career has progressed through organizations that were packed to the gills with people with whom I shared values. They wanted to go to conferences to share their work, they wanted to hear about how others are solving similar challenges, and they weren't afraid to present (and challenge) new ideas. As I learned over the last three years though, I think these traits are acutely concentrated in the HPC world since HPC itself originated from academia and a culture of independence and self-direction. They certainly aren't universal to all workplaces.</p>
<p>To be clear, I am not saying that my coworkers at Microsoft weren't passionate or curious. But I did learn that, at big tech companies, you can have a perfectly successful career by keeping your head down and cranking away at the tasks given to you. If the work changes one day, it's actually a virtue to be able to walk away from the old project and turn your complete attention to a new one. Did the company just <a href="https://www.theverge.com/2024/10/1/24259369/microsoft-hololens-2-discontinuation-support">cancel the product you've been working on</a>? No problem. If you were good at writing code for Windows update, you'll probably be just fine at coordinating planned maintenances for supercomputers. A colleague of mine called these people "survivors," because they will do the best they can with whatever they're given.</p>
<p>While this agility is great if you love programming, it can also engender numbness and dispassion for any specific application area. If a "survivor" can just as easily program for HoloLens as they can for GPU telemetry, they also likely don't really <em>care</em> about either HoloLens or GPUs. This isn't a bad thing, and I am certainly not passing judgment on people who don't care about GPUs. But it does mean that it's harder for someone who really cares about GPUs to connect with a teammate who really doesn't. And this has many knock-on effects in day-to-day work; it's only natural for people who share common values to help each other out, while relative strangers are less likely to go that extra mile. Finding that common ground to promote "some person on team X" to "my trusted colleague on team X" is that much harder.</p>
<p>This difficulty in finding my community amidst all the survivors is what led me to look outside of my company to find my people. I went to events like the <a href="https://www.olcf.ornl.gov/tag/smoky-mountain-conference/">Smoky Mountains Conference</a> and <a href="https://sites.google.com/lbl.gov/nersc50-nug/home">NERSC@50</a> and took the stage to literally beg the HPC community to give me a reason to work with them. By the letter of my job description, I was never supposed to be on stage; I was supposed to spending all my time behind my desk, thinking about the reliability of our biggest supercomputers. But I liked working with the people in the HPC community, and I liked working with our HPC sales organization, because we all shared common values; we were passionate about HPC and the mission of advancing scientific computing. So, I wound up spending a lot of time working on simple things with HPC folks and not enough time doing my day job.</p>
<h3 id="company-culture-matters-too">Company culture matters, too</h3>
<p>In an organization where individuals don't often share a lot of common ground, I learned that it's incumbent upon everyone to make a deliberate effort to maintain a culture of working together and helping each other out. A positive workplace culture won't happen by itself across a massive organization. To this end, Satya has a bunch of corporate culture mantras that are often repeated to keep reminding people of the way employees should treat each other.</p>
<p>For example, he has a mantra of "<a href="https://www.msn.com/en-us/money/other/how-satya-nadella-created-a-learn-it-all-culture-at-microsoft-to-help-it-become-a-3-trillion-powerhouse/ar-BB1qWoRY">be a learn-it-all, not a know-it-all</a>." But I found that many people struggled to really understand how to do this in practice; when confronted with a tough problem ("your database keeps timing out when we point a thousand nodes at it at once"), it's often too easy to just be a know-it-all ("nobody else does that, so you are doing it wrong") rather than a learn-it-all ("why are you doing it that way?"). And the older a company is, the harder it is for decades-long veterans to maintain openness to new challenges in the silo they've built around themselves.</p>
<p>I've worked with HPC users for long enough to know that this attitude is pervasive anywhere you put a bunch of smart people with different perspectives into a room. However, it wasn't until I came to Microsoft that I learned that there's something to be gained by explicitly and repeatedly reminding people that they should strive to understand at least as much as they try to explain. Should I ever find myself in a leadership position, this is definitely a mantra I will carry with me and repeat to others, and I will credit my time at Microsoft with appreciating how to really live this mentality, not just parrot it.</p>
<h3 id="being-good-at-things-isnt-always-a-job">Being good at things isn't always a job</h3>
<p>People tell me that I'm pretty good at a bunch of stuff: figuring out how technologies work, explaining complex concepts in understandable ways, and taking a critical look at data and figuring out what's missing. And I enjoy doing these things; this is why I post to <a href="https://blog.glennklockwood.com/">my blog</a>, maintain <a href="https://www.glennklockwood.com/garden/">my digital garden</a>, and love <a href="https://www.youtube.com/playlist?list=PLtPey-3r1oZS0S5pPcWq-L4yrT9-R0gIm">getting on stage and giving presentations</a>. But people also say that, because I'm good at these things, there'd be no shortage of opportunities for me in the HPC industry should I ever go looking.</p>
<p>However, I've learned that a <em>job</em> has to be an amalgamation of <em>responsibilities</em> that create value, and connecting "things I'm good at" with "things that need to be done" is not always straightforward. For example, if I am <em>good at</em> learning things and share what I learned with others, what kind of jobs actually turn that into a <em>responsibility</em>?</p>
<ul>
<li><strong>Developers</strong> don't really do this at all. Their job is really to keep those git commits coming. Sometimes this requires learning new things, but writing blog posts or giving talks is not in the job description, so they don't count for much on performance reviews.</li>
<li><strong>Product managers</strong> do a little of this. I had to learn a few things and then repeat them a lot when I was a PM. Over and over. To customers, to executives, to partner teams. It was 5% learning and 95% sharing.</li>
<li><strong>Salespeople</strong> also do a little of this. They have to stay current on customer needs and product features, then repeat them a lot.</li>
<li><strong>System architects</strong> do a fair amount of this. I had to learn about what technologies are on the horizon, figure out how to piece them into an idea that could be implemented, then explain why it'd all be a good idea to others.</li>
<li><strong>Educators</strong> do a lot of this. The technology industry is always moving, so learning is required to stay up to date. They also get to be selective about the ideas worth sharing and downplay the rest.</li>
</ul>
<p>Each one of these roles has its own downsides too; for example, product managers and salespeople often have to nag people a lot, which I don't think anyone likes. And many of these roles require sharing knowledge with people who really don't want to hear it. After all, what customer is eager to talk to every salesperson who comes in the door?</p>
<p>Trying to find the ideal job is not just a matter of being good at many things; it's a matter of finding specific jobs that contain a maximal number of things you're good at and a minimal number of things you don't want to do. It's an NP-hard problem, and I've come to realize that the only way to solve it is through trial-and-error. I'm sure some people get lucky and figure out the optimal path on their first try, but for the rest of us, the only way to approach the optimal path is to continuously reflect and not longer on a known-suboptimal path for any longer than is necessary.</p>
<p>I've given up on trying to find the perfect job, because I've learned that it probably doesn't exist. I'm good at some things, I'm bad at some things; I enjoy some responsibilities, and I dislike some responsibilities. As with every other job I've had, I learned a lot about all four of these categories during my time at Microsoft, and my choice of next step has been informed by that. I don't expect it to be perfect, but I have high hopes that it will be a step in the right direction.</p>
<h3 id="you-dont-have-to-be-your-employer">You don't have to be your employer</h3>
<p>When I left the government for a corporate job, one of my biggest worries was losing credibility with peers whose opinions I respected. It's easy to dismiss the viewpoint of someone at a large vendor with a rationalization like, "of course they'd say that; it's their job," but I learned that the HPC community isn't so reductive. People are smart, and most were willing to engage with the quality of my ideas before checking the affiliation on my conference badge.</p>
<p>The trick, of course, was finding ways to share ideas in a way that didn't upset my corporate overlords but had substantive value to my audience. I think I figured this out, and in short, I found that leading with honesty and precision works best. The HPC community was built on sharing experiences and learnings about what does and doesn't work, so embracing that--rather than name-dropping products and making hyperbolic claims--seemed to keep me getting invited back to the HPC conferences and workshops that I wanted to attend.</p>
<p>I wasn't completely intentional in building whatever credibility I've gained over the last three years, but I was intentional in avoiding work that would clearly compromise it. I never want to be accused of misrepresenting the limits of my understanding, so I will never present a slide containing statements or plots that I can't substantiate. I also never want to be accused of misrepresenting the truth, so I am as forthright as possible in disclosing when I do (or don't) have an incentive to say something.</p>
<p>Because I stayed true to myself, I think I was the same person at Microsoft as I was at NERSC or SDSC. That continuity helped my peers quickly recalibrate after I became a vendor, and I think this helped me do more than if I had gone all-in on the role of a cloud spokesperson. Of course, there were times when I had to take on an employer-specific persona, but that's just business, and I've found that peers recognize that this is just a part of the game that we all must play.</p>
<p>The result of all this wasn't clear to me until after I started telling people I was leaving Microsoft. There are a bunch of HPC-specific projects I undertook on the side (e.g., reviewing and advising on research, serving on panels), and I started notifying people that I would have to find other Microsoft engineers to take over these obligations since I was leaving. Much to my surprise though, everyone responded the same way: the request to have me help was specifically to me, not my employer. Short of any conflicts of interest, they didn't care who employed me and valued my contributions regardless of who was signing my paychecks.</p>
<p>So, after three years working for an HPC vendor, I have learned that most people won't define you by your employer as long as you don't define yourself by your employer. It is possible to work for a company that sells HPC and still maintain your own identity as a person, but it requires thoughtful effort and a supportive (or indifferent!) employer. If you act like a company shill, you will be regarded as one, but not many jobs in industry actually <em>require</em> that to fulfill your responsibilities.</p>
<h3 id="happiness-sometimes-costs-money">Happiness sometimes costs money</h3>
<p>I think most people would agree that, while money can't buy happiness, it certainly helps. What I didn't realize until recently, though, is a reciprocal truth: sometimes happiness costs money.</p>
<p>A year ago, I wrote about <a href="https://blog.glennklockwood.com/2024/08/how-has-life-after-leaving-labs-been.html#pay-good">how the pay in industry compares to working at the national labs</a>, and I described how my golden handcuffs were structured. An optimist might say that these vesting schedules are a way to keep a happy employee from being lured away, but I think it's equally common that these are truly handcuffs. They are a constant reminder that, even in the darkest of days, there is a six-figure reason to grit one's teeth and persevere.</p>
<p>I've come to realize that there is an adverse correlation between a few factors:</p>
<ul>
<li>Smaller organizations offer more flexibility to mold a job around your preferences, because there is more work scope spread across fewer people.</li>
<li>Larger organizations can afford to offer larger total compensation, but flexibility is limited to the scope of any single team.</li>
</ul>
<p>I kind of thought about it like this:</p>
<div class="separator" style="clear: both; text-align: center;"></div>
<p>When I realized that I should explore other paths, I had to determine where in this continuum I wanted to wind up: do I care more about a fat paycheck, or do I care more about enjoying my day-to-day responsibilities? And once offers started coming in, exactly how much of a pay cut was I willing to take in exchange for the flexibility that I would receive?</p>
<p>By the time I handed in my resignation at Microsoft, I knew exactly how much this happiness was worth to me. Alternatively, I found out how much opportunity cost I was willing to pay for the ability (hopefully!) to reconnect with my day-to-day work. The calculus was an interesting exercise involving a bunch of Monte Carlo simulation which I won't detail here, but as it turns out, I was willing to pay a lot of money for the chance to do something that aligned more completely with what I wanted to do with the rest of my career. In the end, I gave up hundreds of thousands in unvested stock, and I am taking a six-figure pay cut in annual base pay when I start my next job. For me, though, this was a fair price to pay.</p>
<h2 id="final-thoughts">Final thoughts</h2>
<p>After three years in the world of hyperscale supercomputing, I have come away with two major learnings that now shape how I think about the future.</p>
<p>On the technical front, I think the HPC community has chosen to keep going its own way and reinvent the cloud rather than work meaningfully with hyperscale cloud providers. There was a brief window of opportunity where <a href="https://idiomorigins.org/origin/if-the-mountain-wont-come-to-muhammad-then-muhammed-must-go-to-the-mountain">the mountain may have actually come to Muhammed</a>, and the trajectory of scientific computing could have fundamentally changed to align with the growth trajectory of hyperscale AI. However, I don't think the HPC community was ready to take a big swing during those early days post-ChatGPT or do an earnest assessment of what that future could've looked like. I also worry that the window has closed, and the HPC community never even realized what was on the table.</p>
<p>On the career front, I've realized that success is multidimensional. Money is one axis, but so are impact, people, and purpose. The relative importance of each is not always obvious either; they only became clearer to me as I tried different jobs across the space. I've found that the ability to work with like-minded people and the opportunity to learn and share are the most important dimensions to me, but also I recognize that I am privileged in others. Finding stacks of money can be easy for those who work in AI, but there are no shortcuts to building (and retaining!) teams of great people. Anyone who can do the latter well should not be undervalued.</p>
<p>There's a lot more that I didn't have time to organize and write, but I have every intention of continuing to be myself, regardless of where I work, in the future. I will keep writing, posting, and talking about what I'm learning in supercomputing whenever I can. And along those lines, I hope that writing all this out helps others figure out what's important to them and where they want to go.</p>]]></content><author><name>Glenn K. Lockwood&apos;s Blog</name></author><category term="glennklockwood" /><summary type="html"><![CDATA[I recently decided to leave Microsoft after having spent just over three years there, first as a storage product manager, then as a compute engineer. Although I touched many parts of Azure's infrastructure during that time, everything I did was at the intersection of large-scale supercomputing and hyperscale cloud. There was no shortage of interesting systems to figure out and problems to solve, but as I began to wrap my arms around the totality of hyperscale AI training in the cloud, I also began to see the grand challenges that lay ahead.]]></summary></entry><entry><title type="html">ISC’25 recap</title><link href="https://hpc.social/personal-blog/2025/isc-25-recap/" rel="alternate" type="text/html" title="ISC’25 recap" /><published>2025-06-24T05:58:00-06:00</published><updated>2025-06-24T05:58:00-06:00</updated><id>https://hpc.social/personal-blog/2025/isc-25-recap</id><content type="html" xml:base="https://hpc.social/personal-blog/2025/isc-25-recap/"><![CDATA[<p>I had the pleasure of attending the 40th annual ISC High Performance conference this month in Hamburg, Germany. It was a delightful way to take the pulse of the high-performance computing community and hear what the top minds in the field are thinking about.</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">The main foyer of Congress Center Hamburg, and the view that greeted me on the first morning of ISC'25. </figcaption></figure>
</div>
<p>The conference felt a little quieter than usual this year, and there didn't seem to be as many big ideas and bold claims as in years past. There was <a href="https://www.theregister.com/2025/06/10/jupiter_europes_top_super/">a new Top 10 system announced</a>, but it was built using previous-generation Hopper GPUs. There were a <a href="https://isc-hpc.com/the-isc-2025-exhibition-sets-new-records/">record number of exhibitors</a>, but many of the big ones (Intel, AMD; the big three cloud providers) were all absent. And while there were some exciting new technologies (like <a href="https://www.tomshardware.com/pc-components/gpus/amd-announces-mi350x-and-mi355x-ai-gpus-claims-up-to-4x-generational-gain-up-to-35x-faster-inference-performance">AMD MI350-series GPUs</a> and <a href="https://ultraethernet.org/ultra-ethernet-consortium-uec-launches-specification-1-0-transforming-ethernet-for-ai-and-hpc-at-scale/">Ultra Ethernet v1.0</a>) debuting during the week, they actually debuted elsewhere and were simply referenced throughout the week's talks.</p>
<p>This year's ISC really felt like the place where the big news of the industry was being repeated in the context of scientific computing instead of being stated for the first time. And maybe this is the future of HPC conferences: rather than being where new technology is announced, perhaps ISC will become where the scientific community tries to figure out how they can use others' technology to solve problems. That idea--figuring out how to make use of whatever the AI industry is releasing--was certainly pervasive throughout the ISC program this year. The conference's theme of "connecting the dots" felt very appropriate as a result; rather than defining new dots, the conference was all about trying to make sense of the dots that have already been drawn.</p>
<p>I took plenty of notes to try to keep track of everything that was being discussed, and as has become tradition, I've tried to summarize some of the key themes in this post.</p>
<h2 style="text-align: left;">Table of contents</h2>
<ul>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#zettascale">Zettascale</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#ozaki-ozaki-ozaki">Ozaki, Ozaki, Ozaki</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#top500">Top500</a><ul>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#jupiter">JUPITER</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#hpcai-system-intersection">HPC-AI system intersection</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#other-new-entrants">Other new entrants</a></li>
</ul>
</li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#hpc-around-the-world">HPC around the world</a><ul>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#hpc-in-china">HPC in China</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#elsewhere-in-asia">Elsewhere in Asia</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#the-middle-east">The Middle East</a></li>
</ul>
</li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#exhibitors">Exhibitors</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#cloud-or-lack-thereof">Cloud, or lack thereof</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#parting-thoughts">Parting thoughts</a></li>
</ul>
<h2 id="zettascale">Zettascale</h2>
<p>Now that exascale is squarely in the rear-view mirror of HPC, an increasing number of high-profile speakers began pushing on zettascale as the next major milestone. Like the early days of exascale, most of the discourse was less about what can be achieved with zettascale and more about the technology challenges that need to be surmounted for HPC to continue moving forward. And to that end, using zettascale to justify tackling big hardware and software challenges wasn't a bad thing, but it felt like every talk about zettascale this year was still more fanciful than anything else.</p>
<p>The opening keynote, "HPC and Al - A Path Towards Sustainable Innovation" was delivered by a duo of CTOs: Mark Papermaster (of AMD) and Scott Atchley (of Oak Ridge Leadership Computing Facility). It was a textbook keynote: it had inspiring plots going up and to the right that showed huge potential! It had scary linear extrapolations showing that staying the course won't do! It had amazing science results enabled by big iron! It even had a surprise product debut in MI355X! ChatGPT couldn't have come up with a better structure for a keynote presentation. But as is my wont, I listened to the talk with a little skepticism and found myself raising an eyebrow a few times.</p>
<p>A part of Papermaster's presentation involved an extrapolation to zettascale by 2035 and claimed that HPC is approaching an "energy wall:"</p>
<div class="separator" style="clear: both; text-align: center;">
<figure><figcaption class="image-caption">Extrapolating ten years on a semilog plot is a great way to cause alarm in people who don't pay close attention to axes.</figcaption></figure></div>
<p>He specifically said that we'd need 1 GW per supercomputer to reach zettascale by 2035 on the current trajectory. He then used this to motivate "holistic co-design" as the only way to reach zettascale, and he went on to talk about all the same things we heard about leading up to exascale: increase locality and integration to reduce power and increase performance.</p>
<p>While I agree that we should aspire to do better than a gigawatt datacenter, this notion that there is an "energy wall" that stands between us and zettascale is a bit farcical; there's nothing special about a 1 GW zettascale supercomputer, just like there was nothing special about 20 MW for exascale. You might argue that building a supercomputer that consumes all the power of a nuclear reactor might be fundamentally more difficult than one that consumes only 20 MW, and you'd be right--which is why the first gigawatt supercomputers probably aren't going to look like the supercomputers of today.</p>
<p>Papermaster's "energy wall" slide reminded me of <a href="https://fee.org/articles/the-great-horse-manure-crisis-of-1894/">the great horse manure crisis of 1984</a>, where people extrapolated from today using an evolutionary, not revolutionary, trajectory. If building a single gigawatt supercomputer is inconceivable, then build four 250 MW supercomputers and put a really fast network between them to support a single, synchronous job. The AI industry is already headed down this road; <a href="https://glennklockwood.com/garden/multicluster-training">Google, Microsoft, and OpenAI have already talked about how they synchronously train across multiple supercomputers</a>, and Microsoft announced their <a href="https://view.officeapps.live.com/op/view.aspx?src=https%3A%2F%2Fmediusdownload.event.microsoft.com%2Ftranscripts%2FD6K5%2FKEY020%2FKEY020.docx%3Fsv%3D2018-03-28%26sr%3Db%26sig%3D0gs30Jf82r%252BresqqpGIGKRSOtKrvicgbpqh5Tdkigpg%253D%26se%3D2025-06-24T18%253A46%253A13Z%26sp%3Dr&amp;wdOrigin=BROWSELINK">400 Tb/s "AI WAN" for this last month</a> as a means to enabling wide-area training.</p>
<p>Granted, it's unlikely that the HPC community will be building massive, distributed supercomputers the way hyperscale is. But I was disappointed that the keynote only went as far as saying "a gigawatt supercomputer is crazy, so we need codesign at the node/rack scale." Codesign to reach zettascale will probably require a whole new approach that, for example, accounts for algorithms that <a href="https://github.com/NVIDIA/nccl/pull/1659">synchronize communication across multiple datacenters</a> and power plants. The infrastructure for that is already forming, with the US developing its Integrated Research Infrastructure (IRI) and Europe shaping up to have over a dozen AI factories. Zettascale by 2035 may very well exist for the scientific computing community, but it'll probably look a lot more like hyperscale zettascale rather than a single massive building. A single machine plugged into a gigawatt nuclear reactor only happens if business-as-usual is extrapolated out another ten years as Papermaster did, and the codesign required to achieve that isn't very meaningful.</p>
<p>Prof. Satoshi Matsuoka also gave a talk on the big stage about <a href="https://glennklockwood.com/garden/systems/FugakuNEXT">Fugaku-NEXT</a>, which Japan has branded as a zettascale system. His vision, which will be realized before 2030, aims to deploy a single, 40 MW supercomputer (much like <a href="https://www.glennklockwood.com/garden/systems/Fugaku">Fugaku</a> was) where:</p>
<ul>
<li>10x-20x speedup comes from hardware improvements</li>
<li>2x-8x speedup comes from mixed precision or emulation (more on this below)</li>
<li>10x-25x speedup comes from surrogate models or physics-informed neural networks</li>
</ul>
<p>The net result is a 200x-4000x speedup over Fugaku. His rationale is that this will result in a system that is effectively equivalent to somewhere between 88 EF and 1.7 ZF FP64. It's not literally doing that many calculations per second, but the science outcomes are equivalent to a brute-force approach using a much larger system.</p>
<p>I thought this approach to reaching zettascale was much more realistic than the Papermaster approach, but it does require the scientific computing community to redefine its metrics of success. If HPL was a bad benchmark for exascale, it is irrelevant to zettascale since it's unlikely that anyone will ever run HPL on a zettascale system. At best, we'll probably see something like <a href="https://hpl-mxp.org">HPL-MxP</a> that captures the 10x-20x hardware speedup and the 2x-8x mixed-precision or emulated FP64 reach hundreds of exaflops, but the 10x-25x from surrogate models will be domain-specific and defy simplistic ranking. If I had to guess, the first zettascale systems will be benchmarked through Gordon Bell prize papers that say things like "simulating this result using conventional FP64 would have required over 1 ZF for 24 hours."</p>
<h2 id="ozaki-ozaki-ozaki">Ozaki, Ozaki, Ozaki</h2>
<p>Although Prof. Matsuoka evoked the 2x-8x speedup from mixed precision or emulation when claiming <a href="https://www.glennklockwood.com/garden/systems/FugakuNEXT">Fugaku-NEXT</a> would be zettascale, he was far from the only speaker to talk about mixed precision and emulation. In fact, it seemed like everyone wanted to talk about emulating FP64, specifically using NVIDIA's low-precision tensor cores and the <a href="https://doi.org/10.1007/s11075-011-9478-1">Ozaki scheme</a> (or its derivatives). By the end of the week, I was absolutely sick of hearing about Ozaki.</p>
<div class="separator" style="clear: both; text-align: center;">
<figure></figure></div>
<p>For the unindoctrinated, this Ozaki scheme (and similar methods with less-catchy names) is a way to emulate matrix-matrix multiplications at high precision using low-precision matrix operations. It's become so hot because, despite requiring more arithmetic operations than a DGEMM implemented using WMMA/MFMA instructions, it can crank out a ton of FP64-equivalent operations per unit time. This is a result of the ridiculously nonlinear increases in throughput of low-precision tensor/matrix cores on modern GPUs; for example, Blackwell GPUs can perform over 100x more 8-bit ops than 64-bit ops despite being being only 8x smaller. As a result, you can burn a ton of 8-bit ops to emulate a single 64-bit matrix operation and still realize a significant net speedup over hardware-native FP64. Matsuoka presented the following slide to illustrate that:</p>
<div class="separator" style="clear: both; text-align: center;"><figure><figcaption class="image-caption">Dr. Uchino's estimates of how many FP64 FLOPS one can emulate using INT8 as presented by Satoshi Matsuoka.</figcaption></figure></div>
<p>Emulation offers a way for scientific apps that need high-precision arithmetic to directly use AI-optimized accelerators that lack FP64 in hardware, so it's worth talking about at conferences like ISC. But it seems like <em>everyone</em> wanted to name-drop Ozaki, and the actual discussion around emulation was generally a rehash of content presented earlier in the year at conferences like <a href="https://blog.glennklockwood.com/2025/03/gtc-2025-recap.html">GTC25</a>.</p>
<p>While hearing about FP64 emulation and Ozaki schemes got tiring throughout the week, I had to remind myself that I hadn't even heard about Ozaki before September 2024 at the Smoky Mountains Conference. The fact that the Ozaki scheme went from relative algorithmic obscurity to being the star of the show in nine months is either a reflection of its incredible importance in scientific computing or a testament to the reach of NVIDIA's marketing.</p>
<p>Cynically, I'll bet that NVIDIA is probably doing everything it can to make sure the world knows about the Ozaki scheme, and ISC was a part of that. When the datasheets for Rubin GPUs are released, I'll bet the performance table has a row claiming a bazillion FP64 FLOPS, and there will be a tiny footnote that clarifies they're citing emulated FP64 precision. They did it with structured sparsity, and I'm sure they'll do it for emulated DGEMM.</p>
<p>Although the Ozaki scheme is perhaps over-hyped considering how narrow its applicability is to the broad range of compute primitives used in scientific computing, I do anticipate that it is the tip of the iceberg. If 2025 was the year of the Ozaki scheme, 2026 may be the year of the emulated FP64 version of FFTs, sparse solvers, stencils, or other key algorithms. We're seeing signs of that already; David Keyes and Hatem Ltaief both presented material at ISC on using mixed-precision matrix operations for other scientific problems, and I mentioned <a href="https://blog.glennklockwood.com/2025/03/gtc-2025-recap.html#for-science">their work in my earlier GTC25 blog</a>. I'm not sure "the Keyes scheme" or "the Ltaief scheme" is as catchy as "the Ozaki scheme," but I expect to hear more about these other emulation techniques before ISC26.</p>
<h2 id="top500">Top500</h2>
<p>On the topic of matrix-matrix multiplication, I can't get too much farther without talking about the Top500 list released at ISC. Although there was no new #1 system, Europe's first exascale system, JUPITER, made its sub-exascale debut. There were also a number of new entries in Top50, and surprisingly, many of them came from companies who offer GPUs-as-a-Service for AI training rather than the usual public-sector sites delivering cycles for scientific research. However, all the new entries were still using previous-generation Hopper GPUs despite huge Blackwell coming online, exposing a perceptible lag between the state of the art in supercomputers for AI and traditional HPC.</p>
<p>As with last year, I felt a growing tension between what the Top500 list brings to the discussion and where the large-scale supercomputing industry is headed. As I wrote earlier, mixed-precision and emulated FP64 was a hot topic in the technical program, but the emphasis of the Top500 session was still squarely on bulk-synchronous FP64 performance. HPL-MxP awards were handed out, but they all wound up in the hands of systems who were also at the top of the regular HPL list. Nobody is submitting HPL-MxP-only scores, and there was no meaningful discussion about the role that the Ozaki scheme will play going forward in Top500's future.</p>
<p>Opining about the long-term future of the Top500 list is a whole separate blog post though, so I'll focus more on what was covered at this year's session.</p>
<h3 id="jupiter">JUPITER</h3>
<p>JUPITER was the only new entrant into the Top 10, and it posted at #4 with an average 793 PF over a hundred-minute run. Though it hasn't broken the 1 EF barrier yet, JUPITER is noteworthy for a few reasons:</p>
<ul>
<li>It is expected to be Europe's first exascale system. Given this HPL run <a href="https://bsky.app/profile/andih.bsky.social/post/3lrvrguvtzc2b">used only 79% of the Booster Module's 5,884 GH200 nodes</a>, some basic extrapolation puts the full-system run just a hair above 1 EF. Jülich will either have to run with 100% node availability or get a few extra nodes to exceed 1 EF though.</li>
<li>JUPITER is also now the biggest NVIDIA-based supercomputer on Top500, pushing Microsoft's H100 SXM5 system (Eagle) down to #5. JUPITER is also Eviden's biggest system and a strong affirmation that Europe isn't dependent on HPE/Cray to deliver on-prem systems of this scale.</li>
</ul>
<p>JUPITER was also installed into a modular datacenter, an approach that is emerging as a preferred method for rapidly deploying large GPU systems in Europe. This setup allowed Jülich to place shipping container-like modules on a concrete foundation in just a few months. However, because the datacenter is form-fit to the JUPITER system without much extra space, it's impossible to take a glamor shot of the entire machine from far away. As a result, most photos of JUPITER show only the datacenter modules that wrap the supercomputer racks. For example, Prof. Thomas Lippert shared this photo of JUPITER during his presentation:</p>
<div class="separator" style="clear: both; text-align: center;"><figure><figcaption class="image-caption">JUPITER's modular datacenter as seen from a drone flying overhead.</figcaption></figure></div>
<p>As Lippert was describing JUPITER, I couldn't help but compare it to the AI supercomputers I support at my day job. Like JUPITER, our supercomputers (like Eagle) aren't very photogenic because they're crammed into form-fitted buildings, and they are best photographed from the sky rather than the ground. For example, here's a photo of one of Microsoft's big GB200 supercomputers that I presented later in the week:</p>
<div class="separator" style="clear: both; text-align: center;"><figure><figcaption class="image-caption">A slide showing one of Microsoft's big GB200 supercomputers that I presented at the SuperCompCloud workshop later in the week. The big two-story building in the center houses GPUs, and the long white building on the right houses storage and CPU-only nodes.</figcaption></figure></div>
<p>JUPITER may be the first exascale system listed on Top500 that doesn't have fancy rack graphics, but I don't think it will be the last.</p>
<p>I also found myself wondering if these modular datacenters are trading short-term upsides with long-term downsides. While they accelerate deployment time for one-off supercomputers, it wasn't clear to me if these modular structures is reusable. Does the entire datacenter retire along with JUPITER after 5-7 years?</p>
<p>Hyperscalers use modular datacenters too, but the modularity is more coarse-grained to support a wider variety of systems over multiple decades. They're also physically more capacious, allowing them to deploy more CDUs and transformers per rack or row to retrofit them for whatever power and cooling demands evolve into over the full depreciation life of the datacenter building.</p>
<h3 id="hpcai-system-intersection">HPC-AI system intersection</h3>
<p>As with last year, Erich Strohmeier did a walkthrough of Top500 highlights, and he argued that "hyperscale" is defined as anything bigger than 50 MW, and therefore the Top500 list is hyperscale. It wasn't clear what value there was in trying to tie the Top500 list to hyperscale in this way, but there were a few ways in which Top500 is beginning to intersect with hyperscale AI.</p>
<p>Foremost is the way in which some exascale systems have been appearing on the list: they first appear after HPL is run on a big but partially deployed machine, then six months later, the full-system run is listed. Aurora and JUPITER both follow this pattern. What's not obvious is that many massive AI supercomputers also do something like this; for example, the Eagle system's 561 PF run was analogous to <a href="https://top500.org/lists/top500/2023/11/">Aurora's initial 585 PF run</a> or JUPITER's 793 PF run. The difference is that systems like Eagle typically enter production training after that first big tranche of GPUs is online, so there is never an opportunity to run HPL as more of the system powers up. Instead, the production training job simply expands to consume all the new GPUs as new tranches come online.</p>
<p>This iteration of the Top500 list also saw a number of bona fide commercial AI training clusters from smaller GPU-as-a-Service and "AI factory" providers post results, giving the public a view of what these systems actually look like:</p>
<ul>
<li>Nebius listed <a href="https://top500.org/system/180366/">ISEG2</a> at #13 with a 624-node, 202 PF H200 SXM cluster, following their 2023 Top500 debut with a 190-node, 46 PF H100 SXM cluster. Nebius was spun out of Yandex, the Russian tech conglomerate.</li>
<li>Northern Data Group debuted <a href="https://top500.org/system/180378/">Njoerd</a> at #26 with a 244-node H100 SXM cluster. Northern Data Group started out as a German bitcoin mining company.</li>
<li>FPT debuted at #36 with a <a href="https://top500.org/system/180399/">127-node H200 SXM cluster</a> and #38 with a <a href="https://top500.org/system/180387/">127-node H100 SXM cluster</a>. FPT is a Vietnamese technology conglomerate.</li>
</ul>
<p>It's notable that none of these systems resemble the sovereign AI systems or EuroHPC AI Factories cropping up in Europe, which are attached to traditional HPC centers and built on familiar HPC platforms like Cray EX or BullSequana. Rather, they're essentially NVIDIA reference architectures that resemble DGX SuperPods but are stamped out by companies like Supermicro, Gigabyte, and ASUS.</p>
<p>While it's nice of these GPU-as-a-Service companies to participate in the Top500 list, I did not see anyone from these companies in the technical program in any other way. And I did not see anyone from the bigger GPU-as-a-Service providers (CoreWeave, Crusoe, Lambda, etc) contributing either. Thus, while these companies are participating in Top500, it doesn't seem like they're genuinely interested in being a part of the HPC community.</p>
<h3 id="other-new-entrants">Other new entrants</h3>
<p>If you take a step back and look at the ten largest systems that made their debut at ISC'25, they broadly divide into two categories. Here's the list:</p>
<div>
<table style="border-collapse: collapse; font-family: sans-serif; font-size: 0.9em; width: 100%;">
<thead style="background-color: #f2f2f2;">
<tr>
<th style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: left;">Rank</th>
<th style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: left;">System</th>
<th style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: left;">Platform</th>
<th style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: left;">Site</th>
</tr>
</thead>
<tbody>
<tr style="background-color: white;">
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">4</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">JUPITER Booster</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">GH200</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Jülich</td>
</tr>
<tr style="background-color: #f9f9f9;">
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">11</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Isambard-AI phase 2</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">GH200</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Bristol</td>
</tr>
<tr style="background-color: white;">
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">13</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">ISEG2</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">H200 SXM5</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Nebius</td>
</tr>
<tr style="background-color: #f9f9f9;">
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">15</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">ABCI 3.0</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">H200 SXM5</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">AIST</td>
</tr>
<tr style="background-color: white;">
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">17</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Discovery 6</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">GH200</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">ExxonMobil</td>
</tr>
<tr style="background-color: #f9f9f9;">
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">18</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">SSC-24</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">H100 SXM5</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Samsung</td>
</tr>
<tr style="background-color: white;">
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">26</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Njoerd</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">H100 SXM5</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Northern Data Group</td>
</tr>
<tr style="background-color: #f9f9f9;">
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">27</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">ABCI-Q</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">H100 SXM5</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">AIST</td>
</tr>
<tr style="background-color: white;">
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">33</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">AI-03</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">MI210</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Core42</td>
</tr>
<tr style="background-color: #f9f9f9;">
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">36</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">FPT AI Factory Japan</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">H200 SXM5</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">FPT</td>
</tr>
</tbody>
</table>
</div>
<p>Aside from Core42's weird MI210 cluster, every new big system was either GH200 (for traditional HPC) or H100/H200 SXM5 (for AI). This suggests a few interesting things:</p>
<ul>
<li>None of the AI cloud/GPUaaS providers are talking about GH200. It seems that GH200 is squarely for scientific computing, and Hopper HGX systems is preferred for AI at scale.</li>
<li>Despite debuting on Top500 two years ago, H100 is still making its way into the hands of HPC and AI sites. This could mean one of several things:
<ul>
<li>H100 is more affordable now (<a href="https://www.nextplatform.com/2025/05/08/supermicro-hiccups-on-hopper-pulls-40-billion-guidance-for-fiscal-2026/#:~:text=“But%20I%20said,say%20that.”%20%5Blaughter%5D">Jensen says he can't give them away</a>),</li>
<li>there was a huge backlog of H100 orders, or</li>
<li>it's just taking some places a really long time to get H100 up and running</li>
</ul></li>
<li>Blackwell is not relevant to HPC right now. There are no big Blackwell systems on this list, nor was Blackwell discussed in any sessions I attended during the week. This is despite large GB200 systems being public, up, and benchmarked. For example, <a href="https://github.com/mlcommons/training_results_v5.0/blob/main/IBM%2BCoreWeave%2BNVIDIA/systems/carina_ngpu2496_ngc25.04_nemo.json">CoreWeave, IBM, and NVIDIA ran MLPerf Training across 39 racks (624 nodes) of a GB200 NVL72 system named Carina just last month</a>. They did not appear to bother with HPL, though.</li>
</ul>
<p>From all this, it seems like there is a definite lag forming between what qualifies as "leadership computing" to HPC people and AI people. Today's leadership HPC (Hopper GPUs) is yesterday's leadership AI, and today's leadership AI (Blackwell GPUs) isn't on the radar of leadership HPC yet. Maybe GB200 will begin appearing one or two years later as the AI people move on to Vera-Rubin.</p>
<p>So, if I had to guess, I think the top-end of Top500 in 2027 could look like one of three things:</p>
<ol type="1">
<li>It will contain HPC systems with state-of-the-art, HPC-specific variants of accelerators that are completely irrelevant to AI. Large AI training systems will simply disappear from the list, because HPL has ceased to be a meaningful measure of their capability. GB200/GB300 simply never appear on Top500.</li>
<li>It will contain HPC systems with previous-generation Blackwell accelerators after Jensen (the chief revenue destroyer) gets on stage and tells the world that Blackwell is junk because Rubin is awesome. The AI industry gobbles up all the Rubin GPUs, and HPC picks up the scraps they leave behind.</li>
<li>Top500 starts allowing FP64 emulation, and all bets are off on how ridiculous the top systems' numbers look. In this case, top systems just skip the 1-10 exaflops range and start debuting at tens of exaflops.</li>
</ol>
<p>I have no idea where things will go, but we're starting to see <a href="https://www.nersc.gov/what-we-do/computing-for-science/doudna-system">big HPC deals</a> <a href="https://blogs.nvidia.com/blog/blue-lion-vera-rubin/">targeting Vera Rubin</a> that line up with the same time Rubin will land for the AI industry in 2H2026. So maybe Blackwell is just a hiccup, and option #1 is the most likely outcome.</p>
<h2 id="hpc-around-the-world">HPC around the world</h2>
<p>Though Blackwell's absence from Top500 was easy to overlook, China's continued absence was much more obvious. Even though no new Chinese systems have been listed in a few years now though, representatives from several Chinese supercomputing centers still contributed invited talks throughout the week.</p>
<p>In that context, I appreciated how fully ISC embraces its international scope. I found myself attending a lot of "HPC Around the World" track sessions this year, partly because I work for a multinational corporation and have to stay aware of potential needs outside of the usual US landscape. But there's also been a sharp rise in the amount of serious HPC that is now occurring outside of the USA under the banner of "sovereign AI," and I've been keen to understand how "sovereign AI" compares to the US-based AI infrastructure in which I work.</p>
<p>Before getting too deep into that though, China is worth discussing on its own since they had a such prominent presence in the ISC program this year.</p>
<h3 id="hpc-in-china">HPC in China</h3>
<p>Following the single-track opening keynote on the first day of ISC is the single-track Jack Dongarra Early Career Award Lecture, and this year's talk was given by awardee Prof. Lin Gan from Tsinghua University. In addition, Dr. Yutong Lu gave two separate talks--including the closing keynote--which shed light on the similarities and differences between how China and the US/Europe are tackling the challenges of exascale and beyond.</p>
<p>China is in a position where it does not have access to US-made GPUs, forcing them to develop their own home-grown processors and accelerators to meet their needs for leadership computing. As a result, both speakers gave talks that (refreshingly) revolved around non-GPU technologies as the basis for exascale supercomputers. Although neither Gan nor Lu revealed anything that wasn't already written about in the Gordon Bell prize papers, I took away a few noteworthy observations:</p>
<p><strong>The most public Chinese exascale system is always called the "New Sunway" or "Next Generation Sunway," never "OceanLight"</strong> as has been reported in western media. There still aren't any photos of the machine either, and Dr. Gan used stock diagrams of the predecessor Sunway TaihuLight to represent New Sunway. There was no mention of the Tianhe Xingyi/TH-3 supercomputer at all.</p>
<p><strong>Chinese leadership computing details remain deliberately obfuscated despite the openness to present at ISC.</strong> For example, Lu presented the following English-language table from the <a href="https://www.csiam.org.cn/1003/202411/2246.html">2024 China Top100 HPC list</a>:</p>
<div>
<table style="border-collapse: collapse; font-family: sans-serif; font-size: 0.75em; white-space: nowrap;">
<thead style="background-color: #f2f2f2;">
<tr>
<th style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: left;">No.</th>
<th style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: left;">Vendor</th>
<th style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: left;">System</th>
<th style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: left;">Site</th>
<th style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: left;">Year</th>
<th style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: left;">Application</th>
<th style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">CPU Cores</th>
<th style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">Linpack (Tflops)</th>
<th style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">Peak (Tflops)</th>
<th style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">Efficiency (%)</th>
</tr>
</thead>
<tbody>
<tr style="background-color: white;">
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">1</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Server Provider</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Supercomputing system mainframe system, heterogeneous many-core processor</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Supercomputing Center</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">2023</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">computing service</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">15,974,400</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">487,540</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">620,000</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">78.7</td>
</tr>
<tr style="background-color: #f9f9f9;">
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">2</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Server Provider</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Internet Company Mainframe System, CPU+GPU heterogeneous many-core processor</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Internet company</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">2022</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">computing service</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">460,000</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">208,260</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">390,000</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">53.4</td>
</tr>
<tr style="background-color: white;">
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">3</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Server Provider</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Internet Company Mainframe System, CPU+GPU heterogeneous many-core processor</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Internet company</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">2021</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">computing service</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">285,000</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">125,040</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">240,000</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">52.1</td>
</tr>
<tr style="background-color: #f9f9f9;">
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">4</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">NRCPC</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Sunway TaihuLight, 40960*Sunway SW26010 260C 1.45GHz, customized interconnection</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">NSCC-WX</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">2016</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">supercomputing center</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">10,649,600</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">93,015</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">125,436</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">74.2</td>
</tr>
<tr style="background-color: white;">
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">5</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Server Provider</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Internet Company Mainframe System, CPU+GPU heterogeneous many-core processor</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Internet company</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">2021</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">computing service</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">190,000</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">87,040</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">160,000</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">51.2</td>
</tr>
<tr style="background-color: #f9f9f9;">
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">6</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">NUDT</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Tianhe-2A, TH-IVB-MTX Cluster + 35584*Intel Xeon E5-2692v2 12C 2.2GHz + 35584 Matrix-2000, TH Express-2</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">NSCC-GZ</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">2017</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">supercomputing center</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">427,008</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">61,445</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">100,679</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">61.0</td>
</tr>
<tr style="background-color: white;">
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">7</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Server Provider</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Internet Company Mainframe System, CPU+GPU heterogeneous many-core processor</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Internet company</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">2021</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">computing service</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">120,000</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">55,880</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">110,000</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">50.8</td>
</tr>
<tr style="background-color: #f9f9f9;">
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">8</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Server Provider</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">ShenweiJing Supercomputer System, 1024*SW26010Pro heterogeneous many-core processor 390C MPE 2.1 GHz</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Computing Company</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">2022</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">scientific computing</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">399,360</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">12,912</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">14,362</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">89.9</td>
</tr>
<tr style="background-color: white;">
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">9</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Server Provider</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Supercomputing Center System, 992*SW26010Pro heterogeneous many-core processor 390C MPE 2.1 GHz</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">Supercomputing Center</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">2021</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">scientific computing</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">386,880</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">12,569</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">13,913.0</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">90.3</td>
</tr>
<tr style="background-color: #f9f9f9;">
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">10</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">BSCCC/Intel</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">BSCCC T6 Section 5360*Intel Xeon Platinum 9242 homogeneous many-core processor 48C 2.3 GHz, EDR</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">BSCCC</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">2021</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px;">computing service</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">257,280</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">10,837</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">18,935.0</td>
<td style="border: 1px solid rgb(204, 204, 204); padding: 8px; text-align: right;">57.2</td>
</tr>
</tbody>
</table>
</div>
<p>The #1 system is almost definitely built on SW26010P processors just like the big New Sunway system that Gan discussed (15,974,400 cores / 390 cores per SW26010P = 40,960 nodes), but it's significantly smaller than the 39M cores on which the work Gan highlighted was run. Clearly, China's biggest systems aren't on their own Top100 list, and their #1 listed system only says its processors are "heterogeneous many-core" despite smaller entries explicitly listing SW26010P (Pro) processors.</p>
<p><strong>Chinese leadership computing struggles aren't being hidden</strong>. Lu specifically called out a "lack of a new system" in 2024, echoing earlier sentiments from other leaders in Chinese HPC who have referred to <a href="https://news.sciencenet.cn/htmlnews/2024/11/534141.shtm">"some difficulties in recent years" and a "cold winter" of HPC</a>. She also said that their leadership systems are "relatively" stable rather than trying to overstate the greatness of Chinese HPC technology. But as with above, she didn't get into specifics; by comparison, Scott Atchley (of Oak Ridge Leadership Computing Facility) specifically quoted a 10-12 hour mean time between job interrupt on Frontier after his keynote. Whether 10-12 hours is "relatively stable" remained unspoken.</p>
<p><strong>Performance portability wasn't a top-line concern despite how hard it seems to port applications to Chinese accelerators.</strong> SW26010P is weird in that it has a host core and offload cores with scratchpads, and its native programming model (Athread) is very CUDA-like as a result. Gan made it seem that China is investing a lot of effort into "fine-grained optimizations" using OpenACC and Athread, and he showed all the ways in which they're rewriting a lot of the kernels and decompositions in complex applications (like <a href="https://www.cesm.ucar.edu/models/cam">CAM</a>) to make this work. This sounds like an performance portability nightmare, yet there wasn't much talk about Chinese equivalents to performance portability frameworks like Kokkos, RAJA, or alpaka.</p>
<p>Lu did name-drop a few frameworks that unify HPC and AI performance portability from around the world:</p>
<div class="separator" style="clear: both; text-align: center;">
<figure><figcaption class="image-caption">Yutong Lu's only reference to software that enhances portability and productivity. Not quite the same as what Kokkos, Raja, and alpaka aim to solve, though.</figcaption></figure></div>
<p>However, these were more about aligning efforts across scientific computing and AI rather than enabling scientific apps to run seamlessly across China's different exascale accelerators.</p>
<p><strong>Application focus areas in China seem similar to everywhere else.</strong> Classical and quantum materials modeling, climate and ocean modeling, electronic structure calculations, and genomics were all mentioned by Gan and Lu in their talks. There was no mention of stockpile stewardship or any defense-related applications of HPC, though I'm sure China is using big supercomputers in these efforts just as US and European nations do. The only unusual application that I noticed was Gan's mention of implementing reverse time migration (RTM) on FPGAs; I've only ever heard of RTM in the context of oil exploration. Though I'm no expert, I didn't think many HPC centers spent a lot of time focusing on that technique. I do know KAUST has done some work optimizing RTM applications with Aramco in the space, but most other national supercomputing centers keep oil and gas at arm's length. Gan's RTM work may be related to earthquake modeling rather than petroleum, but it stood out nonetheless.</p>
<p><strong>Nobody talked about GPUs.</strong> Gan spent a healthy amount of time talking about applying FPGAs and NPUs to scientific problems, but these are areas of research that are on the fringes of mainstream HPC. I'm not sure if this reflected his own interests or priority research directions in China, but given that Chinese researchers cannot procure NVIDIA or AMD GPUs, perhaps FPGAs and NPUs are being pursued as a potential next-best-thing. Necessity truly is the mother of invention, and China might be the driver of a disproportionate amount of innovation around dataflow processing and reduced precision for modeling and simulation workloads.</p>
<p><strong>Nobody talked about storage either.</strong> I'm not sure if this suggests China has a lopsided interest in compute over holistic system design, or if they just talked about their biggest challenges (which are using home-grown accelerators productively). Granted, keynote speakers rarely talk about storage, but I didn't see much participation from China in any of the subsystem-specific sessions I attended either. This is particularly notable since, for a time, Chinese research labs were dominating the IO500 list with their home-made file systems. Networking was mentioned in passing in Lu's closing keynote, but not much beyond another example of technology fragmentation, and there were no specific Chinese interconnects being discussed during the week.</p>
<p><strong>China is in the thick of AI just like the rest of the world.</strong> Lu said that 30% of the cycles on their big HPC systems go to AI, which is right in line with anecdotes from other HPC sites that put their figures at <a href="https://csc.fi/en/media-release/lumis-capacity-in-high-demand-to-be-succeeded-by-an-ai-optimized-supercomputer/?utm_source=chatgpt.com">somewhere up to 50%</a>. She also presented the Chinese taxonomy of the three ways in which AI and scientific computing can mesh together: HPC for AI (training LLMs on supercomputers), HPC by AI (AI for system design and operations), and HPC and AI (AI in the loop with simulation). China is also neck-deep in figuring out how to exploit reduced precision (or "intelligent computing," as Lu branded it) and has pivoted from being "performance driven" (which I took to mean HPL-driven) to "target driven" (which I took to mean scientific outcome-driven). This is consistent with their recent Gordon Bell prize win and non-participation in either Top500 or China Top100.</p>
<p><strong>China is embracing geo-distributed supercomputing and complex workflows</strong>, much like the US. Lu specifically called out "Computility Net," a catchy name that sounded a lot like the US DOE's Integrated Research Infrastructure (IRI). She described it as a national effort to combine supercomputing with "commodity IT" resources (perhaps Chinese cloud?) to enable "resource sharing" through a "service grid." In her closing keynote, she even name-dropped IRI:</p>
<div class="separator" style="clear: both; text-align: center;"><figure><figcaption class="image-caption">The Chinese vision for Computility Net, which seems analogous to the US Integrated Research Infrastructure, as presented by Yutong Lu.</figcaption></figure></div>
<p>She did liken Computility to both IRI in the US and PRACE in the EU though, and in my mind, PRACE is nothing like IRI. Rather, PRACE is more like TeraGrid/XSEDE/ACCESS in that it federates access to HPC systems across different institutions, whereas IRI's ambition is to tightly integrate computational and experimental facilities around the country. But from the above slide, it sounds like Computility Net is closer to IRI since it is coupled to "Supercomputing internet" (akin to ESnet?) and bridging compute and data across eastern and western China.</p>
<h3 id="elsewhere-in-asia">Elsewhere in Asia</h3>
<p>Although Chinese researchers headlined a few sessions at ISC, a number of other Asian nations presented their national supercomputing strategies as well. Japan and Korea have mature, world-class HPC programs, but I was surprised to see how ambitious India has become to catch up. Smaller nations were also represented, but it was clear to me that their focus is spread across midrange HPC, partnering with large centers in Korea/Japan, and innovating around the edges of supercomputing. And perhaps unsurprisingly, every nation represented had a story around both quantum computing and artificial intelligence regardless of how modest their production modsim infrastructure was.</p>
<p><strong>India</strong> appears to rapidly catching up to the US, Europe, and Japan much in the same way China was fifteen years ago. Representatives from C-DAC, the R&amp;D organization that owns the national supercomputing mission in India, gave a far-reaching presentation about India's ambition to achieve exascale by 2030. Their current strategy appears to be broad and capacity-oriented, with forty petascale clusters spread across India for academic, industrial, and domain-specific research. They have a comprehensive, if generic, strategy that involves international collaboration in some regards, reliance on open-source software to fill out their HPC environment story, and home-grown hardware and infrastructure:</p>
<div class="separator" style="clear: both; text-align: center;"><figure><figcaption class="image-caption">India's ambitious strategy towards exascale in 2030. This slide has it all, from home-grown CPUs and networks to five systems deployed in six years.</figcaption></figure></div>
<p>I was surprised to hear about their ambitions to deploy their own CPUs and interconnect though. India is pursuing both ARM and RISC-V for their own CPUs for a future 200 PF system, and they're already deploying their "InfiniBand-like" interconnect, TRINETRA, which uses funny NICs with <a href="https://cdac.in/index.aspx?id=product_details&amp;productId=TrinetraHPCInterconnect">6x100G ports or 10x200G ports</a> rather than fewer, faster serdes. I didn't hear mention of their AI acceleration plans, but rolling their own commercialized CPU and interconnect in itself is a lot to bite off. Given that India is the world's fastest growing economy though, these plans to go from 20 PF in 2025 to 1 EF in 2030 may not be that far-fetched. Perhaps the Indian national strategy will become clearer during the inaugural <a href="https://sc-india.in">Supercomputing India 2025 conferece</a> this December.</p>
<p>The <strong>Korea Institute of Science and Technology Information</strong> also took the stage to describe their next national supercomputer, <a href="https://www.glennklockwood.com/garden/systems/KISTI-6">KISTI-6</a>, which was first announced in May 2025. It will be a 588 PF Cray EX254n system with 2,084 nodes of GH200, similar to <a href="https://www.glennklockwood.com/garden/systems/Alps">Alps</a> and <a href="https://www.glennklockwood.com/garden/systems/Isambard-AI">Isambard-AI</a>. This is quite a step up from its predecessor, which was an air-cooled KNL system, but it's unlikely it will unseat Fugaku; the 588 PF number cited appears to be the sum of 2,084 GH200 nodes, 800 Turin CPU nodes, and 20 H200 SXM5 nodes. The HPL score of its GH200 nodes will place it below <a href="https://www.glennklockwood.com/garden/systems/Alps">Alps</a> and somewhere around 350 PF, likely joining a flood of multi-hundred-petaflops GH200 systems that will appear between now and ISC26.</p>
<p><strong>Singapore (NSCC) and Taiwan (NCHC)</strong> both presented their national programs as well, but they appear to be much more nascent, and the size of their HPC infrastructure was presented as aggregate capacity, not capability. Their strategies involve partnership with Japan or Korea, but both had specific carveouts for both sovereign AI and quantum computing. Interestingly, their use cases for AI both had a strong story about training models that understood the diversity of languages and dialects represented in their nations. For example, it is not unusual for people to switch languages or dialects mid-sentence in Singapore, and the big Western models aren't designed for that reality. Similarly, Taiwan has 16 indigenous tribes with 42 dialects. It seemed like enabling LLMs that reflect the breadth languages used in Singapore and Taiwan have become the responsibility of these nations' respective national supercomputing efforts.</p>
<p>That said, that noble mission didn't seem to be matched with substantial training infrastructure; these localized models will be relying on a couple hundred GPUs here and there, wedged into existing HPC centers. Thus, these sovereign models are probably going to be fine-tuned variants of open models, aligning with my earlier observation that these smaller nations will be innovating around the edges of HPC and AI.</p>
<p><strong>What was missing?</strong> Although Vietnam, Thailand, Malaysia, and other Asian nations have strong HPC programs centered around industrial uses, they were not represented in ISC's HPC Around the World track. Also absent was any meaningful discussion around cloud; while everyone had a throwaway line about cloud in their presentations, the fact that the only big clouds in Asia are Chinese and American probably makes it unappealing to integrate them into the core of these nations' national HPC strategies. Speaking from experience, this is quite different from the attitudes of commercial HPC users across Asia who are all too happy to let someone else run HPC datacenters for them.</p>
<h3 id="the-middle-east">The Middle East</h3>
<p>Although KAUST has been a world-class HPC center in the Middle East for the past fifteen years, AI seems to be where the majority of new investment into HPC is going.</p>
<p>In describing new efforts in Saudi Arabia, Prof. David Keyes casually mentioned the Saudi HUMAIN effort, which will build 500 MW of datacenter capacity and 18,000 GB300 GPUs, after describing the Shaheen-3 GH200 upgrade that "might (barely)" put it back in the Top20 by SC'25. Similarly, Dr. Horst Simon walked through a few of Abu Dhabi's university clusters (each having dozens of GPU nodes) after skating through an announcement that a 5 GW AI campus was also being built in Abu Dhabi. The gap between investment in AI and investment in HPC was striking.</p>
<p>I also had a brief conversation with someone from one of the major Abu Dhabi universities, and I was very surprised to find that I was talking to a real AI practitioner--not an HPC person moonlighting in AI--who spoke at the same depth as the customers with whom I work in my day job. The nature of his work made it clear to me that, despite his university not having a Top500 system, he was familiar with running training and inference at scales and with sophistication that is far beyond the experience of most ISC attendees.</p>
<p>These interactions led me to the conclusion that the Middle East's approach to "sovereign AI" is quite different from Europe's. Rather than building HPC systems with GPUs, letting HPC centers operate them, and calling them sovereign AI platforms, nations like Saudi Arabia and UAE are keeping HPC and AI separate. Like in the US, they are going straight to hyperscale with AI, and they have no preconceived notion that anything resembling a supercomputer must be hosted at a supercomputer center.</p>
<p>Of course, only nations like Saudi Arabia and UAE can afford to do this, because they have trillion-dollar sovereign wealth funds to invest in massive infrastructure buildout that doesn't isn't contingent on public consensus or the latest election cycle. Just as UAE's Core42 can build a 5 GW datacenter campus with little oversight, these nations can easily mis-step and invest a ton of money in an AI technology that turns out to be a flop. In the end, it seems like these Middle Eastern nations are willing to take bigger risks in how they build out their sovereign AI infrastructure, because they are largely starting from a blank sheet of paper. They aren't limiting themselves to 20 MW supercomputers like the HPC world had.</p>
<p>All things being equal, this might turn out to be an advantage over other nations who are more hesitant to deviate from the tried-and-true course of buying a Cray or a Bull, sticking some GPUs in it, and calling it AI. If these Middle Eastern nations do everything right, they stand to get a lot further and move a lot faster in sovereign AI than Europe, and it'll be fascinating to see how quickly they catch up with the sort of frontier AI research being done private industry. But, as with the US AI industry, it doesn't seem like these AI practitioners are going to be attending ISC in the same way European sovereign AI folks do; the roads of HPC and AI seem to run parallel without intersecting in the Middle East.</p>
<h2 id="exhibitors">Exhibitors</h2>
<p>ISC had a <a href="https://isc-hpc.com/the-isc-2025-exhibition-sets-new-records/">record number of exhibitors this year</a>, and as usual, I tried to set aside at least an hour or two to walk the floor and see what technologies are on the horizon. This year, though, the exhibit hall was not a great representation of the rest of the conference. Everyone I talked to about the exhibit said one of two things:</p>
<ol type="1">
<li>There are a LOT of quantum companies.</li>
<li>A lot of big companies were noticeably absent.</li>
</ol>
<p>It also didn't feel like the biggest exhibit ever, partially because of #2, and partially because many of the exhibitors--one in five--was exhibiting for the first time this year. This meant a lot of the booths were small and barebones, and many of them belonged to either companies at the periphery of HPC (such as companies that make dripless couplers for liquid cooling) or small startups who just had a desk, a few pens, and some brochures.</p>
<p>On the first point, it was true--quantum computing was well represented, with 22% of exhibitors identifying as being involved in the field in some form. In fact, quantum felt over-represented, since the ISC technical program certainly didn't have such a large fraction of talks on quantum computing topics. I didn't have time to actually talk with any of these quantum companies though, so wasn't able to get a sense of why the startup ecosystem around quantum computing was so rich in Europe as compared to the US.</p>
<p>While there was an abundance of quantum this year, a number of the big HPC and HPC-adjacent companies were noticeably absent:</p>
<ul>
<li>Amazon, Azure, and Google did not have booths despite having booths last year. Amazon and Google still sponsored the conference at the lowest tier (bronze) though, while Microsoft did not sponsor at all.</li>
<li>Intel had neither booth nor sponsorship despite having the #3 system on Top500. I don't think they held a party this year, either. AMD didn't have a booth, but they sponsored (and gave the opening keynote!)</li>
<li>WEKA neither had a booth nor sponsored the conference this year, although they were the leading sponsor of the Student Cluster Competition. Competitors DDN, VAST, Quobyte, and BeeGFS all had booths, but only VAST sponsored. Curiously, Pure and Scality, which do not big footholds in leadership HPC, did both booths and sponsorship.</li>
</ul>
<p>These companies who chose not to have a booth still sent people to the conference and were conducting meetings as usual, though. This suggests that there's something amiss with how large companies perceive the return on investment of having a booth at ISC. I don't have any insider knowledge here, but I was surprised by the pullback since ISC has historically been very good at incentivizing attendees to walk through the expo hall by putting it between the technical sessions and the food breaks.</p>
<p>As I walked the exhibit floor, I found that prominent booths spanned the whole HPC stack: software, system integrators, component makers (CPUs, GPUs, HBM and DDR, and SSD and HDD), and datacenter infrastructure were all exhibiting. The most eye-catching booths were those with big iron on display: HPE/Cray had a full EX4000 cabinet and CDU on display, and there were a few Eviden BullSequana nodes floating around.</p>
<div class="separator" style="clear: both; text-align: center;"><figure>
<figcaption class="image-caption">The Cray EX4000 cabinet (right) and its CDU (left) on display at the ISC'25 exhibition hall. One of the most eye-catching displays, even they've been on display at ISC and SC for a few years now.</figcaption></figure></div>
<p>Sadly, though, there were no full BullSequana X3000 racks on display. I've still never seen one in real life.</p>
<p>Infrastructure companies like Motivair (who manufactures the CDUs for Cray EX) and Rittal (which I know as a company that manufactures racks) also had big liquid-liquid head exchangers on display with shiny steel piping. Here's a smaller version of the Cray EX CDU that Motivair was displaying:</p>
<div class="separator" style="clear: both; text-align: center;"><figure><figcaption class="image-caption">A close-up view of a smaller liquid-liquid heat exchanger CDU on display at the Motivair booth right next to HPE's. Strangely, the mechanics of these systems dovetails with what I've learned as a part of my other hobby outside of HPC, which is operating a multi-family residential high-rise.</figcaption></figure></div>
<p>I got to chatting with some good folks at Motivair, and I learned that the 1.2 MW variant that is used with Cray EX has a 4" connection--the same size as the water main in <a href="https://glennklockwood.com/garden/LRA">my coop</a>. Since I recently helped with the replacement of my building's water main, this led me down a rabbithole where I realized that the flow rates for this CDU is roughly the same as my apartment building too, which is to say, a single Cray CDU moves as much fluid as a 55-unit apartment building. Incidentally, a single Cray EX cabinet supports roughly the same electrical capacity as my 55-unit building too--I am in the process of replacing our 1,200 A service panel, which comes out to about the same 400 kVA as fully loaded EX.</p>
<p>Aside from the Cray cabinets and CDUs, which are no longer new to ISC, I couldn't put my finger on any particularly outstanding booths this year though. The exhibit felt like a sea of smaller companies, none of which really grabbed me. This isn't to say that big vendors were wholly absent though. Despite not having booths, all three big cloud providers threw parties during the week: AWS and NVIDIA teamed up on a big party with over a thousand registrants, while Google and Microsoft held smaller parties towards the end of the week. HPE also threw a lovely event that was off the beaten path along the Elbe, resulting in a less-crowded affair that made it easy to catch up with old friends.</p>
<p>I may be reading too much into this year's exhibit, but it felt like ISC might be transforming into an event for smaller companies to gain visibility in the HPC market, while larger companies apply their pennies only in the parts of the conference with the highest return. Whether a company chose to have a booth, sponsor the conference, and/or throw a party seemed to defy a consistent pattern though, so perhaps other factors were at play this year.</p>
<h2 id="cloud-or-lack-thereof">Cloud, or lack thereof</h2>
<p>Because I work for a large cloud service provider, I attended as many cloud HPC sessions as I could, and frankly, I was disappointed. The clear message I got by the end of the week was that Europe--or perhaps just ISC--doesn't really care about the cloud. This is quite different from the view in the US, where the emergence of <a href="https://www.glennklockwood.com/garden/systems/Eagle">massive AI supercomputers</a> has begun to shift opinions to the point where <a href="https://www.theregister.com/2024/07/24/oak_ridge_discovery/">the successor to the Frontier supercomputer at OLCF might wind up in the cloud</a>. I suppose cloud is a lot less attractive outside of the US, since all the major cloud providers are US corporations, but the way in which cloud topics were incorporated into the ISC program this year sometimes felt like a box-checking exercise.</p>
<p>For example, I attended the BOF on "Towards a Strategy for Future Research Infrastructures" which I expected to be a place where we discussed the best ways to integrate traditional HPC with stateful services and other workflow components. While cloud was mentioned by just about every panelist, it was almost always in a throwaway statement, lumped in with "the edge" or cited as a vague benefit to "new workflows and interactive analysis" with no further detail. One speaker even cited egress fees as a big challenge which, to me, means they haven't actually talked to a cloud provider in the last five to ten years. If egress fees are what stop you from using the cloud, you're talking to the wrong account team.</p>
<p>I get it though; there are times where cloud often doesn't offer enough obvious benefit for HPC to justify the effort required to figure it out. In those cases, it's incumbent on cloud providers to provide a better story. But I was also disappointed by the invited session called "Bridging the Gap: HPC in the Cloud and Cloud Technologies in HPC," which I hoped would be the place where cloud providers could make this case. Instead, only two of the three CSPs were even invited to speak, and it was clear that the speakers did not all get the same assignment with their invitations. Granted, the CSP for whom I work was the one not invited (so I came in a little biased), but I was surprised by how differently each speaker used their time.</p>
<p>Dr. Maxime Martinasso from CSCS gave a talk from the perspective of trying to add cloud-like capabilities to a supercomputer, which is a recurring pattern across a number of sites (including many in the US DOE) and projects. He explained the way they're creating an infrastructure-as-code domain-specific language that sits on top of <a href="https://www.glennklockwood.com/garden/systems/Alps">Alps</a>, their Cray EX system, to give users the ability to bring their own software stacks (all the way down through Slurm) to the supercomputer. It was clearly a ton of work on CSCS's part to develop this capability, and yet the talk's "future work" slide contained a bunch of features which those of us in the cloud would consider "P0"--priority zero, or essential for a minimum viable product.</p>
<p>By the end of Martinasso's talk, I realized that CSCS's perspective is that, unlike commercial cloud, these cloudy features aren't P0; having a supercomputer on the floor is. He made the case that CSCS has a need to explore diverse computing architectures and accelerators (as evidenced by the five different node types in Alps!), and putting them all on a single RDMA fabric isn't something any cloud provider will do. As a result, adding any new cloud-like capability to the heterogeneous supercomputer is just gravy, and the fact that true cloud is more "cloudy" than Alps is irrelevant since the cloud will never support the intra-fabric heterogeneity that Alps does.</p>
<p>The other two speakers represented big cloud providers, and their talks had a bit more product pitch in them. One speaker talked through the challenges the cloud is facing in trying to fold supercomputing principles into existing cloud infrastructure (a theme I repeated in my talk later in the week) before talking about specific products that have arisen from that. It touched on some interesting technologies that the HPC world hasn't yet adopted (like optical circuit switching--super cool stuff for programmable fabrics), and I learned a few things about how that provider might bring new HPC capabilities to the table for specific workloads.</p>
<p>The other speaker, though, presented a textbook pitch deck. I've give almost the same exact presentation, down to showing the same sort of customer stories and product comparison tables, during customer briefings. Execs in the audience would eat it up while engineers' eyes would glaze over, and having to do that song and dance is partly why I didn't make it as a product manager. <a href="https://bsky.app/profile/glennklockwood.com/post/3lrd3usnt222d">I was incredulous</a> that such a presentation was an invited talk at one of the most prestigious HPC conferences in the world.</p>
<p>This is not to say I was mad at the speaker. He did exactly what one would expect from a leader in the sales side of an organization, hitting all the notes you'd want in a textbook pitch aimed at the C-suite. Rather, I was disappointed by the choice by the session organizers; when you invite someone whose job is driving business at one of the largest cloud providers to speak, you should fully expect a broad and salesy presentation. I don't think it's a stretch to say that most ISC attendees aren't looking for these sorts of high-level talks designed for enterprise decision-makers; they want insight and technical depth.</p>
<p>Was I miffed that a competitor got to give a twenty-minute sales pitch during a session at which I wasn't invited to speak? Absolutely. And do I think I could've given a talk that even the most ardent cloud-hater would find something interesting in it? Probably. But since that didn't happen, the best I can do is complain about it on the Internet and hope that next year's program committee puts more care into organizing an invited speaker session on cloud and HPC.</p>
<p>Thankfully, I was given the opportunity to talk a little about my work at the <a href="https://sites.google.com/view/supercompcloud/isc25-9th-supercompcloud-workshop#h.fur7sdv6h19a">SuperCompCloud workshop</a> on Friday. That workshop felt like what the "Bridging the Gap" invited session should've been, and there were roughly equal parts of presentations on adding cloud-like features to their HPC infrastructure and adding HPC-like features to cloud infrastructure. From my perspective, the workshop was great; I got to see how traditional HPC centers are adopting cloud practices into their operations, and I could explain how we overcame some of the challenges they're facing in Azure. But to my point at the outset of this section--that Europe doesn't really care about the cloud--the majority of speakers at SuperCompCloud were American.</p>
<h2 id="parting-thoughts">Parting thoughts</h2>
<p>As I said at the outset, there were way more sessions that I missed than I attended. In addition, a lot of the big headlines of the week were coincident with, not made at, the conference. A few noteworthy announcements during the week that I won't go into detail about include:</p>
<ol type="1">
<li><a href="https://www.ed.ac.uk/news/university-set-to-host-ps750m-national-supercomputer">£750M was awarded to EPCC</a> to deploy what sounds like the UK's first exascale system. This announcement's overlap with ISC was a total coincidence, so EPCC didn't have many details to share.</li>
<li><a href="https://ultraethernet.org/ultra-ethernet-consortium-uec-launches-specification-1-0-transforming-ethernet-for-ai-and-hpc-at-scale/">The Ultra Ethernet Consortium announced the long-awaited version 1 of its spec</a>. I'm not sure how relevant this is to HPC yet, but given how many networking talks compared themselves against InfiniBand, I think there's a lot of appetite for a high-performance, non-proprietary alternative.</li>
<li>Sadly, <a href="https://www.hpcwire.com/2025/06/11/farwell-hpc-guru/">HPC_Guru announced his retirement</a> mid-week as well. It's not clear this was deliberately timed with ISC, but it was acknowledged on the big stage during the ISC closing statements and resulted in a lot of <a href="https://bsky.app/profile/hpcguru.bsky.social/post/3lrcsdbwa522c">recognition</a> <a href="https://x.com/hpc_guru/status/1932688759310725425?s=61">online</a>. I credit HPC_Guru, whoever he is, with a lot of the success I've enjoyed in my career, as he amplified my voice as far back as 2009 when I first started on Twitter. Maybe with his retirement, I should try to do for others what he did for me.</li>
</ol>
<p>And along the lines of reflecting back over the years, this was ISC's 40th anniversary, and the organizers had a few wonderful features to commemorate the milestone. Addison Snell organized a panel where a variety of attendees got to discuss the impact that the conference has had on them over the past 40 years, and I was delighted to find that I was not the only person to <a href="https://glennklockwood.com/garden/ISC-conference#isc-40th-anniversary-panel">reflect back on how ISC has shaped my career</a>. As critical as I can be of specific speakers and sessions when I write up these notes, I do hope it goes without saying that I wouldn't bother doing all this for a conference that wasn't deeply engaging and rewarding to be a part of.</p>
<p>Going back to this year's theme of connecting the dots, I think it's apt. Some ways in which HPC connected dots at ISC this year were obvious; the conference brought together people with a common interest in high-performance computing from across 54 countries and seven continents this year. But this year's conference also made it clear that the role of HPC going forward may be connecting the dots between different technologies being developed for AI, cloud, enterprise, and other markets and the problems in scientific computing that need to be solved.</p>
<p>The latest and greatest Blackwell GPUs barely registered at ISC this year, and the HPC community seems OK with that now. Instead of the focus being on the absolute top-end in high-performance accelerators, HPC's focus was on connecting the dots between last generation's GPUs and today's grand challenges in science. Instead of showcasing the newest innovations in secure computing in the cloud, HPC's focus was in connecting the dots between a few relevant pieces of zero trust and big-iron on-prem supercomputers.</p>
<p>HPC has always been about figuring out ways to use stuff invented for someone else to solve scientific challenges--connecting the dots. Beowulf clusters started that way, GPGPU computing started that way, and emulating DGEMMs (and other primitives) on AI accelerators will probably follow the same pattern. But different nations are drawing different lines between the dots; while the US might draw a shorter line between commercial cloud and HPC at scale, Europe is drawing shorter lines between HPC for scientific computing and HPC for sovereign AI.</p>
<p>If we accept that connecting the dots may be where the HPC community can make the most impact, then it's fitting that ISC chose to carry forward the theme of "connecting the dots" into ISC'26. This break from the tradition of introducing a new tagline each year suggests that, at times, optimizing what we already have can take us further than than pursuing something completely new. After 40 years, ISC remains not only a showcase of innovation, but a reflection of how the HPC community (and its role in the technology landscape) is evolving. If we continue to embrace this theme of stitching together breakthroughs instead of spotlighting them individually, the HPC community is likely to be more relevant than ever alongside--not in spite of--the overwhelming momentum of hyperscale and AI.</p>]]></content><author><name>Glenn K. Lockwood&apos;s Blog</name></author><category term="glennklockwood" /><summary type="html"><![CDATA[I had the pleasure of attending the 40th annual ISC High Performance conference this month in Hamburg, Germany. It was a delightful way to take the pulse of the high-performance computing community and hear what the top minds in the field are thinking about.]]></summary></entry><entry><title type="html">Surfing the Singularity- Adventures in Quantum Chemistry</title><link href="https://hpc.social/personal-blog/2025/surfing-the-singularity-adventures-in-quantum-chemistry/" rel="alternate" type="text/html" title="Surfing the Singularity- Adventures in Quantum Chemistry" /><published>2025-03-11T13:11:00-06:00</published><updated>2025-03-11T13:11:00-06:00</updated><id>https://hpc.social/personal-blog/2025/surfing-the-singularity-adventures-in-quantum-chemistry</id><content type="html" xml:base="https://hpc.social/personal-blog/2025/surfing-the-singularity-adventures-in-quantum-chemistry/"><![CDATA[<div class="separator" style="clear: both; text-align: center;"><br /></div>
<p><br />&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;/div&gt;
<br />&lt;div class="separator" style="clear: both; text-align: center;"&gt;<br />&lt;/div&gt;</p>
<p>In this installment of the Surfing the Singularity blog we go vlog, giving an overview of quantum computing today with application to chemistry. Quantum computing is rapidly advancing, with improvements in machine size, error correction, and scalability. And yet, there's always a desire to drive towards advancements and scientific applications which are just out of reach of today's technologies. New algorithms lead the way.&nbsp;</p>
<p>In this video, will give a brief overview of quantum computing, what it means, where we are on the product roadmaps, and explore an emergent algorithm for pushing the boundaries of chemical modeling beyond what is possible with today's classical machines. Enjoy.&nbsp;</p>
<p>- andy&nbsp;</p>
<p><br /></p>
<p>P.S. Begging your forgiveness for being a YouTube newb...&nbsp;</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;"><br /></div>
<p><br />&lt;p&gt;&lt;/p&gt;</p>]]></content><author><name>Surfing the Singularity</name></author><category term="surfthesing" /><summary type="html"><![CDATA[&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;/div&gt; &lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;/div&gt; In this installment of the Surfing the Singularity blog we go vlog, giving an overview of quantum computing today with application to chemistry. Quantum computing is rapidly advancing, with improvements in machine size, error correction, and scalability. And yet, there's always a desire to drive towards advancements and scientific applications which are just out of reach of today's technologies. New algorithms lead the way.&nbsp; In this video, will give a brief overview of quantum computing, what it means, where we are on the product roadmaps, and explore an emergent algorithm for pushing the boundaries of chemical modeling beyond what is possible with today's classical machines. Enjoy.&nbsp; - andy&nbsp; P.S. Begging your forgiveness for being a YouTube newb...&nbsp; &lt;p&gt;&lt;/p&gt;]]></summary></entry><entry><title type="html">LLM training without a parallel file system</title><link href="https://hpc.social/personal-blog/2025/llm-training-without-a-parallel-file-system/" rel="alternate" type="text/html" title="LLM training without a parallel file system" /><published>2025-02-02T03:59:00-07:00</published><updated>2025-02-02T03:59:00-07:00</updated><id>https://hpc.social/personal-blog/2025/llm-training-without-a-parallel-file-system</id><content type="html" xml:base="https://hpc.social/personal-blog/2025/llm-training-without-a-parallel-file-system/"><![CDATA[<p>The illustrious Jeff Denworth recently posted a hot take across social media, claiming that training large language models (LLMs) doesn't require massive, expensive parallel file systems:</p>
<p>
</p>
<p><br /></p>
<p>As someone who's been working on <a href="https://glennklockwood.com/garden/systems/Eagle">one of the largest supercomputers on the planet</a>--one that has no parallel file system at all--I was surprised by how many incredulous or curious responses followed. I guess supercomputers and parallel file systems are like peas and carrots in so many people's minds that the idea of being able to run a massive parallel compute job without a massive parallel file system is so unintuitive that it is unbelievable.</p>
<p>I've given talks about how LLM training uses storage in the past, but I realized I've never written it down. So, for the benefit of humankind, let's talk about how these supercomputers without parallel file systems work.<span></span></p>
<p></p>
<div class="separator" style="clear: both; display: none; text-align: center;"></div>
<h2 style="text-align: left;">The workload</h2>
<p>Though the actual model training on giant GPU supercomputers gets all the attention, the full process of training an LLM is a little more involved. A colleague of mine at Microsoft gave <a href="https://www.sniadeveloper.org/events/agenda/session/670">a great overview of this storage-centric, end-to-end picture at SNIA SDC24</a>; broadly, training an LLM involves the following steps:</p>
<p></p>
<ol style="text-align: left;"><li><b>Data ingestion</b>: This is where crawlers scrape the Internet and pull down raw html, images, videos, and other media. These raw data are indexed and shoved into a data warehouse. At scale, this can be hundreds or thousands of petabytes of data for <a href="https://glennklockwood.com/garden/frontier-model">frontier models</a>.</li><li><b>Data preparation</b>: This is where the raw data is converted into tokenized data. It amounts to a huge data analytics problem that uses well-documented text and image processing pipelines that filter, deduplicate, and otherwise clean the raw garbage on the Internet using frameworks like Apache Spark. The hundreds of petabytes of input get reduced down by 10x-1000x.</li><li><b>Model training</b>: This is where the tokenized data is shoveled through the LLM on giant GPU clusters in little batches. As the data is processed, the model weights are updated, and those weights are checkpointed to storage. If a compute node crashes and the job fails, that checkpoint is used to restart, just like a traditional scientific HPC application. There might be fine-tuning and the like happening as part of this too, but I won't talk about that.</li><li><b>Model deployment and inferencing</b>: This is where the final model is copied across giant fields of inferencing servers, and a web service sits in front of it all to transform REST API requests into actual inferencing queries that run on the GPUs. This isn't training, but we'll talk about it anyway.</li></ol>
<p style="text-align: left;">To understand why a parallel file system offers no particular benefit to any of these steps, let's take a closer look at what's going on in each one.</p>
<h3 style="text-align: left;">Data ingestion</h3>
<p style="text-align: left;">Data ingestion is a widely distributed process that involves minimal computation; you just need a lot of Internet-facing network connectivity and CPU cores to drive independent processes connecting to other people's public HTTP servers. I don't know a lot about what this process looks like, because it never relies on anything resembling a supercomputer.</p>
<p style="text-align: left;">To the best of my knowledge, data ingestion just pulls HTML, images, or video streams from the Internet and packs them into <i>data containers</i>. As it is packing webpages into these files, it is building a separate <i>index</i> that stores metadata about the webpage (URL, encoding, date of access) and its location (the file in which the webpage's contents are stored and the byte offset within that file). Thousands of VMs might be performing these tasks completely independently, and because they do not need to synchronize with each other at any step, it can be better to distribute these scrapers around the world rather than centralize all of them in a single datacenter.</p>
<p style="text-align: left;">While one <i>could</i> store each scraped HTML page in a file that's organized in a parallel file system, accessing those files would be very slow--a full crawl of all the data would require scanning hundreds of billions of little files. So instead of implementing <i>data containers</i> using files and the <i>index</i> using a file system directory tree, it's better to implement data containers on top of object stores and use a distributed key-value store for the index. The fact that scraped data is write-once (and therefore doesn't need features like file locking or read-modify-write), is a natural fit for object stores' design around object immutability.</p>
<h3 style="text-align: left;">Data preparation</h3>
<p style="text-align: left;">Once raw data is indexed and saved in object stores, the first phase of computation comes into play. I've documented this data processing pipeline on my <a href="https://glennklockwood.com/garden/LLM-training-datasets#computational-requirements">LLM training datasets page</a>, but a lot of it amounts to running Apache Spark-like pipelines that chew through all the raw data in a trivially parallel way.</p>
<p style="text-align: left;">These data processing pipelines are very well defined from the days when Hadoop was all the rage, and their data access patterns map well to the strengths of object stores. Each processing task might read a couple hundred megabytes of data from an object all at once, process it in-memory, then dump it back out to objects all at once. File systems offer no benefit here, because each task reads once and writes once rather than skipping around inside individual objects.</p>
<p style="text-align: left;">There is a significant compute workload here, and there are points in the data processing pipeline where global synchronization of all tasks is required. Specifically, the process of deduplicating input data--which is <a href="https://arxiv.org/abs/2107.06499">a critical step to getting a high-quality model these days</a>--requires comparing every piece of data to every other piece of data. As a result, this data preparation phase is often done in a centralized location that is adjacent the object store containing all the raw data scraped from the previous step. The clusters used for data processing can resemble traditional CPU-based supercomputers (think a system like <a href="https://tacc.utexas.edu/systems/frontera/">TACC's Frontera</a>), and in some cases, they might even have full RDMA fabrics to accelerate the all-to-all deduplication step.</p>
<p style="text-align: left;">Critically, this data processing step is not done on the GPU nodes that actually train the model. Data processing is usually limited by I/O bandwidth to storage, and you never want your GPUs stalling out because they're waiting for data. Parallel file system vendors might tell you that the only way to avoid this GPU starvation issue is to plug every GPU node into a super-fast parallel file system, but the reality is that people just do this I/O-heavy step on completely separate supercomputers before training on GPUs ever begins.</p>
<p style="text-align: left;">CPU nodes are significantly cheaper than GPUs, so buying cheap object storage and a cheap CPU cluster is more cost-effective than buying an expensive file system and wasting your GPU nodes on trivially parallel text processing tasks. To illustrate this, consider some normalized list prices from Azure:</p>
<p style="text-align: left;"></p>
<ul style="text-align: left;"><li>$1.00 gets you a 96-core general-purpose VM with 384 GB of RAM</li><li>$1.65 gets you a 176-core HPC-optimized VM with NDR InfiniBand and 768 GB of RAM</li><li>$22.55 gets you a 96-core, 8x H100 GPU VM with NDR InfiniBand</li></ul>
<div>Given that GPUs don't give you a 13x-22x speedup for data processing despite the 13x-22x the price, it makes no sense to perform this data processing on GPU nodes inline with training.</div>
<p></p>
<p style="text-align: left;">One could argue that the GPUs are sitting idle while the data processing cluster is working anyway, but rest assured that AI model shops have no shortage of work to keep their GPUs busy. Data processing for the next model on a CPU cluster often happens at the same time the current model is being trained on the GPU cluster. In cases where there isn't enough work to keep both CPU and GPU clusters busy around the clock, also remember that most of this stuff happens in the cloud, and cloud providers can sell those idle CPU or GPU cycles to another customer in between training campaigns.</p>
<h3 style="text-align: left;">Model training</h3>
<p style="text-align: left;">Huge, distributed training jobs are where most people would think a fast parallel file system is required for both reading input data and writing out checkpoints. After all, the need for fast checkpointing and restart were the primary driver behind the creation of parallel file systems.</p>
<p style="text-align: left;">While parallel file systems certainly <i>can</i> be used for training, they are not the most cost-effective or scalable way to train across tens of thousands of GPUs. To better illustrate the reasons why this is, let's consider the processes of reading inputs and writing checkpoints separately.</p>
<h4 style="text-align: left;">Reading training data</h4>
<p style="text-align: left;">Training a model on GPUs, whether it be on one or a thousand nodes, follows a simple cycle (this is a "step" in LLM training parlance) that's repeated over and over:</p>
<p style="text-align: left;"></p>
<ol style="text-align: left;"><li>A batch of tokenized data is loaded into GPU memory</li><li>That data is then processed through the neural network and the model weights are adjusted</li><li>All GPUs synchronize their updated weights</li></ol>
<p style="text-align: left;">It's tempting to imagine the I/O load generated by step #1 as being the same as it would be for a traditional HPC job: data is read from a parallel file system into compute memory at the start of every single step:</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;"></div>
<p style="text-align: left;">In years past, storage vendors would've insisted that this repeated, random re-reading of input data at every step requires a super-fast parallel file system to keep up. However, two factors make that untrue:</p>
<p style="text-align: left;"></p>
<ol style="text-align: left;"><li>The input data isn't millions of little text or image files. As described in the data ingest and data processing steps, these small files are packaged into large objects before the GPUs ever see them.</li><li>Tokenized data is very dense compared to raw input, so the amount of bytes being read over the course of hundreds or thousands of steps is actually quite small.</li></ol>
<p></p>
<p style="text-align: left;">To quantify #2, consider the <a href="https://arxiv.org/abs/2407.21783">Llama-3 405b model</a>, which was trained on a significant fraction of the public Internet--15.6 <i>trillion</i> tokens. That sounds like a lot of information until you realize that <a href="https://glennklockwood.com/garden/LLM-training-datasets#tokenized-data">the size of a typical token is between 3 and 5 bytes</a> depending on the tokenizer and encoding. This means that the entire 405-billion parameter Llama-3 model, which was trained using 16,000 GPUs, only had to load 60 TB of tokens from storage. That divides out to 3.75 GB of tokens processed by each GPU over the entire course of a 54-day run.</p>
<p style="text-align: left;">When you consider how few bytes are required to train an LLM, it should become clear that the biggest I/O challenge in the performance-critical training loop isn't raw bandwidth; it's performance variability. As such, the best way to ensure that GPUs do not stall out due to read requests is to eliminate as much I/O performance variability as possible. To do this, you have to minimize the sources of contention that might arise between the storage devices and the network that connects them to the GPUs. While you <i>can</i> do this using sophisticated quality-of-service in both the storage servers and interconnect, there is an easier way.</p>
<div class="separator" style="clear: both; text-align: center;"></div>
<p style="text-align: left;">Just stick some local SSDs in every GPU node.</p>
<p style="text-align: left;">This ensures that no contention will occur when loading data from storage into the GPU, because the only network between them is the PCIe on the node. In addition, using node-local NVMe allows storage capacity and storage performance to scale linearly with GPU performance. By comparison, a remote storage system (whether it be parallel file or object) won't get any bigger or faster as you add more GPUs to the training job, resulting in each GPU losing efficiency due to I/O as more GPUs are added to the training job.</p>
<p style="text-align: left;">In practice, model training uses local SSDs like this:</p>
<div class="separator" style="clear: both; text-align: center;"></div>
<p style="text-align: left;">At the start of a training job, data is read from remote storage into the local SSDs in a distributed fashion <i>once</i>. Because the tokenized data is so small, many replicas of the entire dataset can be stored across the job's GPU nodes as well; for example, if you were to train Llama-3 405b on NVIDIA DGX H100 nodes, <b>you could fit the entire training dataset (all 60 TB of it) on just three nodes</b> since each node comes with 30 TB of local SSD. Given that the model was trained on 16,000 GPUs (2,000 nodes), that translates to storing hundreds of replicas of the entire training set. This has a few major benefits:</p>
<p style="text-align: left;"></p>
<ol style="text-align: left;"><li>GPUs never have to wait for shared storage to return data before they can compute. Everything they need is on the local SSDs.</li><li>When a GPU node fails, its input data can be recovered from a surviving GPU node over the backend InfiniBand. After training starts, input data never has to be read from shared storage again.</li><li>It's common to scale up training over time by adding more GPUs (more data-parallel domains) to the job as it stabilizes. When this happens, I/O performance scales linearly because these new GPUs never have to fight over shared storage.</li></ol>
<p></p>
<p style="text-align: left;">A reasonable critique of this approach is that data management becomes more complicated; either the training framework has to keep track of which SSDs and nodes have copies of which input data, or a distributed, client-side shared namespace like <a href="https://www.weka.io/resources/solution-brief/weka-data-platform-converged-mode/">WEKA Converged Mode</a> or <a href="https://docs.coreweave.com/docs/products/storage/object-storage/concepts/lota">CoreWeave LOTA</a> has to sit between your application and your data. In practice though, frontier models are trained for exactly one epoch; that is, <a href="https://glennklockwood.com/garden/scaling-laws#applying-scaling-laws">every input token is processed exactly one time to achieve optimal model quality</a>. Because no two GPUs will ever need to read the same input token, there's never a need to copy input tokens between nodes inside the training loop. </p>
<p style="text-align: left;">I also acknowledge that the above description is greatly simplified; the entire node-local SSD capacity cannot be filled with input data, as space is also needed for checkpoints and other temporary data. However, the fact remains that super high-bandwidth or super high-capacity parallel file systems are not necessary for loading input tokens during training. AI training clusters are built with a ton of local SSDs to do the heavy lifting, and the input data for LLMs is small enough to fit in just a handful of GPU nodes.</p>
<h4 style="text-align: left;">Writing model checkpoints</h4>
<p style="text-align: left;">Though the read workload of LLM training is modest at best, the write workload can be quite intense at scale because the probability of failure increases superlinearly with the size of the training job. However, unlike with scientific HPC jobs, <b>the checkpoint size does not scale as a function of the job size</b>; the checkpoint for a 405 billion-parameter model trained on 16,000 nodes is the same size as the checkpoint for that model trained on three nodes. This is a result of the fact that every training step is followed by a global synchronization which makes each data-parallel copy of the model identical. Only one copy of those model weights, which amounts to under a hundred terabytes for state-of-the-art LLMs, needs to be saved:</p>
<div class="separator" style="clear: both; text-align: center;"><span style="text-align: left;"> </span></div>
<p style="text-align: left;">Kartik and Colleen Tartow at VAST wrote <a href="https://www.vastdata.com/blog/a-checkpoint-on-checkpoints-in-llms">a quantitative breakdown of the true I/O requirements of checkpointing</a>, and they illustrate how even a trillion-parameter model can achieve 99.7% forward progress (only 0.3% time spent checkpointing) when training across 3,072 GPUs with a modest 273 GB/s file system. A parallel file system is not required to get that level of performance; for example, HDD-based <a href="https://x.com/glennklockwood/status/1795548752628867132">Azure Blob achieved over 1 TB/s when benchmarked with IOR</a> for writes at scale.</p>
<p style="text-align: left;">As with reading input tokens though, the real goal for checkpointing at scale is to remove any dependence on shared storage from the training loop entirely. And again, the best way to do this is to simply checkpoint to node-local storage. However, special care must be taken to ensure that the checkpoints don't get lost when a node crashes.</p>
<p style="text-align: left;">In practice, LLM training is now done with asynchronous, multilevel checkpointing. This technique provides the scalability of checkpointing to node-local storage and the durability of shared storage:</p>
<div class="separator" style="clear: both; text-align: center;"></div>
<p style="text-align: left;">The key to this checkpointing process is hierarchical data synchronization:</p>
<p style="text-align: left;"></p>
<ol style="text-align: left;"><li><b>Model weights are first copied from GPU memory into the node's CPU memory</b> after every training step. This checkpoint is governed by the CPU-GPU bandwidth (either PCIe or NVLink/Infinity Fabric), and a 500 GB checkpoint can complete in a second. The benefit of checkpointing to DRAM is that the GPU can unblock and begin computing the next step very quickly. However, this checkpoint in DRAM is not protected and will be lost if the node crashes.</li><li>To protect against node crashes, the <b>checkpoint is then asynchronously copied from CPU DRAM to a neighbor node's local SSD</b> using RDMA. Now if a node crashes, it can restore from a checkpoint that is stored on its neighboring node's SSD via InfiniBand. Reading and writing a 500 GB checkpoint to neighboring SSDs might take ten seconds, so this asynchronous replication might be done for every tenth DRAM checkpoint.</li><li>To store many checkpoints long-term, <b>checkpoints are also asynchronously copied from node-local SSD to shared storage</b>. This might take a minute or two per 500 GB checkpoint, so this last-level checkpoint copy might be done once every ten minutes.</li></ol>
<p style="text-align: left;">This hierarchical checkpointing scheme allows the GPUs to spend only a second checkpointing while being able to recover from job, node, and even cluster-level failures by tailoring the checkpoint tiering frequencies to the performance of each storage tier being used. The cost of recovering from a catastrophic failure might be re-computing up to ten minutes worth of training, but given the rarity of such events, this scheme balances the performance (and risks) of checkpointing to DRAM against hard drive prices (and suffering their performance) for a durable object store.</p>
<p style="text-align: left;">To this latter point, the requirements of the shared storage system at the bottom of this checkpointing hierarchy are very modest:</p>
<p style="text-align: left;"></p>
<ul style="text-align: left;"><li>The checkpoint only needs to complete in the time between successive last-level checkpoint copies. If the 500 GB checkpoint is drained to shared storage only once every ten minutes, our shared storage only needs to deliver 1 GB/s of total bandwidth.</li><li>The write pattern from node-local NVMe to shared storage is arbitrary, because it is a simple copy operation of a fully formed checkpoint file. Unlike direct-to-storage checkpoints, there are no weirdly shaped tensors being serialized into a file on the fly; rather, opaque bits are streaming from a local checkpoint file into a remote object using whatever transfer size and parallelism gives the highest write bandwidth.</li></ul>
<p>This combination of modest write bandwidth and simple, sequential, large-block writes is ideally suited for object stores. This isn't to say a parallel file system cannot work here, but this checkpointing scheme does not benefit from directory structure, fine-grained consistency semantics, or any of the other complexities that drive up the cost of parallel file systems.</p>
<p>The catch, of course, is that checkpointing using these schemes can be complicated to implement. Fortunately, a <a href="https://www.linkedin.com/posts/jeffreydenworth_reducing-model-checkpointing-times-by-over-activity-7289273345269800960-zBj7">growing number of training frameworks</a> support both writing and restoring checkpoints using asynchronous and hierarchical approaches. Model developers never have to worry about interacting with specific files or objects; instead, the framework manages data locality during checkpoint and restart underneath a high-level API.</p>
<h3 style="text-align: left;">Model deployment and inferencing</h3>
<p style="text-align: left;">Once a model is trained, putting it into production as an inferencing service is the final step of its lifecycle. From a storage and I/O standpoint, this is a lot more complicated than training because it marries an enterprise service delivery model (failover, load balancing, authentication, and scaling) with copies of a trained model running across HPC infrastructure. When you hear vendors talking about key-value stores, vector databases, and RAG, that is all happening at this stage.</p>
<p style="text-align: left;">Setting aside everything but the storage attached to the GPU cluster though, the I/O requirements of inferencing are relatively straightforward:</p>
<p style="text-align: left;"></p>
<ol style="text-align: left;"><li>When provisioning a GPU node for inferencing, model weights must be loaded from shared storage as fast as possible.</li><li>When using an LLM to search documents, a vector database is required to perform the similarity search that augments the LLM query with the relevant documents. This is the basis for RAG.</li><li>Key-value caches are often used to reduce the latency for different parts of the inferencing pipeline by storing context including the conversation or frequently accessed contextual documents.</li><li>As the inferencing demand evolves, different models and weights may be swapped in and out of individual GPU servers.</li></ol>
<p style="text-align: left;">A parallel file system is not particularly useful for any of these; the only place in which their high bandwidth would be a benefit is in loading and re-loading model weights (#1 and #4). But as with hierarchical checkpointing, those I/O operations are whole-object, read-only copies that are a natural fit for object APIs. Complex directory structures and strong consistency simply aren't necessary here.</p>
<h2 style="text-align: left;">Objects are good enough, maybe better</h2>
<p style="text-align: left;">None of the steps in this model training lifecycle uniquely benefit from the capabilities that parallel file systems offer:</p>
<p style="text-align: left;"></p>
<ul style="text-align: left;"><li>Data ingestion involves hundreds of petabytes of small documents, but they are immediately packaged and indexed into large data containers. Their metadata is stored in a separate key-value store, so the directory hierarchy of a file system isn't used, and once data has been packaged and indexed, it's never modified in-place. The bandwidth requirements are modest as well since web crawling is the rate-limiting step.</li><li>Data processing is an I/O-intensive data analytics workload. Read bandwidth is critical here, but data is accessed in large transactions and most of the computation is embarrassingly parallel. This workload runs on standalone analytics clusters, so even though the read bandwidth here is rate-limiting, slower storage is not going to impact GPU utilization on training clusters in any way. This step also reduces data by 100x or more, so the write requirements are also modest.</li><li>Training requires both loading input tokens and checkpointing model weights. However, both of these workloads lean on node-local NVMe in every node to eliminate slowdowns due to noisy neighbors. Input data is staged to node-local storage only once at the beginning of a training campaign, and checkpoints are asynchronously bled out to shared storage without impacting GPU utilization.</li><li>Inferencing involves infrequent, read-only, bulk loading of model weights into GPU nodes. While key-value caches and vector databases are also used in inferencing, parallel file systems offer no particular benefit for them.</li></ul>
<p style="text-align: left;">The I/O patterns of each of these steps map nicely to object storage since they are predominantly write-once and whole-file transactions. Parallel file systems certainly can be used, and workloads will benefit from the high bandwidth they offer. However, they come with the cost of features that aren't necessary--either literal costs (in the case of appliances or proprietary software) or figurative costs (allocating people to manage the complexities of debugging a parallel file system).</p>
<p style="text-align: left;">The importance of this latter point is hard to appreciate if you've never used a supercomputer without a parallel file systems. However, I recently sat in on the validation of <a href="https://www.top500.org/system/180349/">a brand-new H200 training cluster</a> where various InfiniBand congestion and routing issues were being worked out. It wasn't until someone said "eviction" in some nontechnical context that I realized that the sporadic file system evictions during fabric instability were simply a non-issue. There was no cleanup of mount points after major fabric events because there was no persistent, fragile client-server state being maintained. I/Os between GPU nodes or nodes and storage might have failed during a rough patch, but they recovered and resumed on their own as soon as the fabric came back. Similarly, identity didn't matter, and all tests could be run as root because there was no implicit trust between the client kernel and remote storage. Removing the dependence between compute nodes, LDAP, and healthy file system mounts completely eliminates many of the challenges of standing up new clusters quickly.</p>
<h3 style="text-align: left;">An ideal AI training cluster architecture</h3>
<p style="text-align: left;">The workloads I described above form a rough outline for an AI training infrastructure which has:</p>
<p style="text-align: left;"></p>
<ol style="text-align: left;"><li><b>A bunch of GPU nodes with a strong RDMA backend like InfiniBand</b>. Each node should have at least enough node-local SSD to store a substantial amount of the input tokens to be used for training, enough space for hierarchical checkpointing, and enough I/O bandwidth to these SSDs to support draining checkpoints from partner nodes' DRAM in just a few seconds. A separate frontend network that connects to storage is also a good idea; it ensures that asynchronous checkpoint draining won't interfere with weight synchronization in the training loop.</li><li><b>A separate CPU cluster for data processing pipelines</b>. A strong backend network will benefit the deduplication step (which is critical to producing high-quality training datasets), but more emphasis should be placed on optimizing large-transaction reads from storage. Given that CPU nodes are so much cheaper than GPU nodes, separating the data processing nodes from training nodes allows you cut more corners when optimizing this CPU cluster. Keeping data processing out-of-band of actual model training means your most data-intensive step (data processing) is decoupled from your most expensive step (training).</li><li><b>A scalable object store that supports basic write-once semantics with modest I/O bandwidth at scale</b>. This matches the needs of the workloads with the price-performance of the storage system and simplifies the recovery process if the interconnect between compute and storage gets congested. It can also serve the data needs of all stages of the training pipeline: hundreds of petabytes of raw training data, hundreds of terabytes of input tokens, and tens of terabytes of model weights all have similar performance needs and can be stored on the same infrastructure with the appropriate QOS settings.</li><li><b>A pool of general-purpose compute infrastructure for hosting the raw training data indices</b>. This can also be used to support vector databases, raw context documents for RAG, and any other ancillary services required for production inferencing.</li></ol>
<p style="text-align: left;">By eschewing a high-performance parallel file system and localizing I/O performance to inside the GPU cluster with node-local NVMe, a vanilla network between the GPU cluster and the other subsystems is sufficient. Although less high-performance, these non-critical bits (ideally) have lower complexity, maintenance, and supportability as well, allowing (again, ideally) more resources to be sloshed towards supporting the high-value GPU infrastructure.</p>
<p style="text-align: left;">Incidentally, this architecture happens to be how most of the largest AI training clusters on which I work are designed.</p>
<h3 style="text-align: left;">But parallel files aren't all bad</h3>
<p style="text-align: left;">Of course, having no parallel file system presents some usability challenges if users are expecting to be able to SSH into a login node and have a complete user environment ready. The user experience for the above infrastructure works best for those who are comfortable developing software in containers and launching pods rather than developing software in vim and submitting Slurm jobs. <i>I do not advocate for throwing out parallel file systems if they're already ingrained in users' workflows!</i></p>
<p style="text-align: left;">In addition, the latest crop of modern, distributed file systems all now support multi-protocol data access. For example, <a href="https://docs.weka.io/4.0/additional-protocols/s3">WEKA</a>, <a href="https://support.vastdata.com/s/article/UUID-67c215f7-63a8-5d58-196e-5066199a6f60">VAST</a>, and <a href="https://docs.qumulo.com/administrator-guide/s3-api/configuring-using-s3-api.html">Qumulo</a>, all support S3 (object) interfaces as first-class citizens. Users who want the traditional HPC experience can play with their data using a file mount as they always have, while those who are coming in from the cloud-native side have equal access to those same data as objects. Supporting multiprotocol access to data in AI environments doesn't reduce the need to overbuild infrastructure or support stateful file mounts across all compute nodes, but it does provide an onramp for users to get comfortable moving away from the traditional HPC user experience.</p>
<p style="text-align: left;">Finally, a few of the leading-edge parallel-file-system-turned-AI-storage platforms are also shipping features that make them valuable for the deployment and inferencing part of the lifecycle. For example, WEKA has their <a href="https://www.weka.io/resources/reference-architecture/warrp-weka-ai-rag-reference-platform/">WARRP reference architecture for RAG</a>, and <a href="https://www.vastdata.com/press-releases/vast-data-unveils-vast-insightengine-with-nvidia">VAST has its InsightEngine</a>--both use the unique architectures underneath their file interfaces to accelerate vector queries far beyond what you would get from running a vector database on, say, Lustre. These so-called "AI data platforms," despite starting as parallel file systems, are spreading their relevance out to the entire LLM lifecycle, filling needs for file, object, and structured data with a single storage system.</p>
<p style="text-align: left;">This is all to say that parallel file systems aren't bad, and they aren't going anywhere. But they aren't required to train frontier models either, and as I've tried to describe above, some of the largest supercomputers on the planet are designed not to require them.</p>
<p></p>
<p></p>
<p></p>
<p></p>]]></content><author><name>Glenn K. Lockwood&apos;s Blog</name></author><category term="glennklockwood" /><summary type="html"><![CDATA[The illustrious Jeff Denworth recently posted a hot take across social media, claiming that training large language models (LLMs) doesn't require massive, expensive parallel file systems: As someone who's been working on one of the largest supercomputers on the planet--one that has no parallel file system at all--I was surprised by how many incredulous or curious responses followed. I guess supercomputers and parallel file systems are like peas and carrots in so many people's minds that the idea of being able to run a massive parallel compute job without a massive parallel file system is so unintuitive that it is unbelievable. I've given talks about how LLM training uses storage in the past, but I realized I've never written it down. So, for the benefit of humankind, let's talk about how these supercomputers without parallel file systems work. The workload Though the actual model training on giant GPU supercomputers gets all the attention, the full process of training an LLM is a little more involved. A colleague of mine at Microsoft gave a great overview of this storage-centric, end-to-end picture at SNIA SDC24; broadly, training an LLM involves the following steps: Data ingestion: This is where crawlers scrape the Internet and pull down raw html, images, videos, and other media. These raw data are indexed and shoved into a data warehouse. At scale, this can be hundreds or thousands of petabytes of data for frontier models.Data preparation: This is where the raw data is converted into tokenized data. It amounts to a huge data analytics problem that uses well-documented text and image processing pipelines that filter, deduplicate, and otherwise clean the raw garbage on the Internet using frameworks like Apache Spark. The hundreds of petabytes of input get reduced down by 10x-1000x.Model training: This is where the tokenized data is shoveled through the LLM on giant GPU clusters in little batches. As the data is processed, the model weights are updated, and those weights are checkpointed to storage. If a compute node crashes and the job fails, that checkpoint is used to restart, just like a traditional scientific HPC application. There might be fine-tuning and the like happening as part of this too, but I won't talk about that.Model deployment and inferencing: This is where the final model is copied across giant fields of inferencing servers, and a web service sits in front of it all to transform REST API requests into actual inferencing queries that run on the GPUs. This isn't training, but we'll talk about it anyway. To understand why a parallel file system offers no particular benefit to any of these steps, let's take a closer look at what's going on in each one. Data ingestion Data ingestion is a widely distributed process that involves minimal computation; you just need a lot of Internet-facing network connectivity and CPU cores to drive independent processes connecting to other people's public HTTP servers. I don't know a lot about what this process looks like, because it never relies on anything resembling a supercomputer. To the best of my knowledge, data ingestion just pulls HTML, images, or video streams from the Internet and packs them into data containers. As it is packing webpages into these files, it is building a separate index that stores metadata about the webpage (URL, encoding, date of access) and its location (the file in which the webpage's contents are stored and the byte offset within that file). Thousands of VMs might be performing these tasks completely independently, and because they do not need to synchronize with each other at any step, it can be better to distribute these scrapers around the world rather than centralize all of them in a single datacenter. While one could store each scraped HTML page in a file that's organized in a parallel file system, accessing those files would be very slow--a full crawl of all the data would require scanning hundreds of billions of little files. So instead of implementing data containers using files and the index using a file system directory tree, it's better to implement data containers on top of object stores and use a distributed key-value store for the index. The fact that scraped data is write-once (and therefore doesn't need features like file locking or read-modify-write), is a natural fit for object stores' design around object immutability. Data preparation Once raw data is indexed and saved in object stores, the first phase of computation comes into play. I've documented this data processing pipeline on my LLM training datasets page, but a lot of it amounts to running Apache Spark-like pipelines that chew through all the raw data in a trivially parallel way. These data processing pipelines are very well defined from the days when Hadoop was all the rage, and their data access patterns map well to the strengths of object stores. Each processing task might read a couple hundred megabytes of data from an object all at once, process it in-memory, then dump it back out to objects all at once. File systems offer no benefit here, because each task reads once and writes once rather than skipping around inside individual objects. There is a significant compute workload here, and there are points in the data processing pipeline where global synchronization of all tasks is required. Specifically, the process of deduplicating input data--which is a critical step to getting a high-quality model these days--requires comparing every piece of data to every other piece of data. As a result, this data preparation phase is often done in a centralized location that is adjacent the object store containing all the raw data scraped from the previous step. The clusters used for data processing can resemble traditional CPU-based supercomputers (think a system like TACC's Frontera), and in some cases, they might even have full RDMA fabrics to accelerate the all-to-all deduplication step. Critically, this data processing step is not done on the GPU nodes that actually train the model. Data processing is usually limited by I/O bandwidth to storage, and you never want your GPUs stalling out because they're waiting for data. Parallel file system vendors might tell you that the only way to avoid this GPU starvation issue is to plug every GPU node into a super-fast parallel file system, but the reality is that people just do this I/O-heavy step on completely separate supercomputers before training on GPUs ever begins. CPU nodes are significantly cheaper than GPUs, so buying cheap object storage and a cheap CPU cluster is more cost-effective than buying an expensive file system and wasting your GPU nodes on trivially parallel text processing tasks. To illustrate this, consider some normalized list prices from Azure: $1.00 gets you a 96-core general-purpose VM with 384 GB of RAM$1.65 gets you a 176-core HPC-optimized VM with NDR InfiniBand and 768 GB of RAM$22.55 gets you a 96-core, 8x H100 GPU VM with NDR InfiniBand Given that GPUs don't give you a 13x-22x speedup for data processing despite the 13x-22x the price, it makes no sense to perform this data processing on GPU nodes inline with training. One could argue that the GPUs are sitting idle while the data processing cluster is working anyway, but rest assured that AI model shops have no shortage of work to keep their GPUs busy. Data processing for the next model on a CPU cluster often happens at the same time the current model is being trained on the GPU cluster. In cases where there isn't enough work to keep both CPU and GPU clusters busy around the clock, also remember that most of this stuff happens in the cloud, and cloud providers can sell those idle CPU or GPU cycles to another customer in between training campaigns. Model training Huge, distributed training jobs are where most people would think a fast parallel file system is required for both reading input data and writing out checkpoints. After all, the need for fast checkpointing and restart were the primary driver behind the creation of parallel file systems. While parallel file systems certainly can be used for training, they are not the most cost-effective or scalable way to train across tens of thousands of GPUs. To better illustrate the reasons why this is, let's consider the processes of reading inputs and writing checkpoints separately. Reading training data Training a model on GPUs, whether it be on one or a thousand nodes, follows a simple cycle (this is a "step" in LLM training parlance) that's repeated over and over: A batch of tokenized data is loaded into GPU memoryThat data is then processed through the neural network and the model weights are adjustedAll GPUs synchronize their updated weights It's tempting to imagine the I/O load generated by step #1 as being the same as it would be for a traditional HPC job: data is read from a parallel file system into compute memory at the start of every single step: In years past, storage vendors would've insisted that this repeated, random re-reading of input data at every step requires a super-fast parallel file system to keep up. However, two factors make that untrue: The input data isn't millions of little text or image files. As described in the data ingest and data processing steps, these small files are packaged into large objects before the GPUs ever see them.Tokenized data is very dense compared to raw input, so the amount of bytes being read over the course of hundreds or thousands of steps is actually quite small. To quantify #2, consider the Llama-3 405b model, which was trained on a significant fraction of the public Internet--15.6 trillion tokens. That sounds like a lot of information until you realize that the size of a typical token is between 3 and 5 bytes depending on the tokenizer and encoding. This means that the entire 405-billion parameter Llama-3 model, which was trained using 16,000 GPUs, only had to load 60 TB of tokens from storage. That divides out to 3.75 GB of tokens processed by each GPU over the entire course of a 54-day run. When you consider how few bytes are required to train an LLM, it should become clear that the biggest I/O challenge in the performance-critical training loop isn't raw bandwidth; it's performance variability. As such, the best way to ensure that GPUs do not stall out due to read requests is to eliminate as much I/O performance variability as possible. To do this, you have to minimize the sources of contention that might arise between the storage devices and the network that connects them to the GPUs. While you can do this using sophisticated quality-of-service in both the storage servers and interconnect, there is an easier way. Just stick some local SSDs in every GPU node. This ensures that no contention will occur when loading data from storage into the GPU, because the only network between them is the PCIe on the node. In addition, using node-local NVMe allows storage capacity and storage performance to scale linearly with GPU performance. By comparison, a remote storage system (whether it be parallel file or object) won't get any bigger or faster as you add more GPUs to the training job, resulting in each GPU losing efficiency due to I/O as more GPUs are added to the training job. In practice, model training uses local SSDs like this: At the start of a training job, data is read from remote storage into the local SSDs in a distributed fashion once. Because the tokenized data is so small, many replicas of the entire dataset can be stored across the job's GPU nodes as well; for example, if you were to train Llama-3 405b on NVIDIA DGX H100 nodes, you could fit the entire training dataset (all 60 TB of it) on just three nodes since each node comes with 30 TB of local SSD. Given that the model was trained on 16,000 GPUs (2,000 nodes), that translates to storing hundreds of replicas of the entire training set. This has a few major benefits: GPUs never have to wait for shared storage to return data before they can compute. Everything they need is on the local SSDs.When a GPU node fails, its input data can be recovered from a surviving GPU node over the backend InfiniBand. After training starts, input data never has to be read from shared storage again.It's common to scale up training over time by adding more GPUs (more data-parallel domains) to the job as it stabilizes. When this happens, I/O performance scales linearly because these new GPUs never have to fight over shared storage. A reasonable critique of this approach is that data management becomes more complicated; either the training framework has to keep track of which SSDs and nodes have copies of which input data, or a distributed, client-side shared namespace like WEKA Converged Mode or CoreWeave LOTA has to sit between your application and your data. In practice though, frontier models are trained for exactly one epoch; that is, every input token is processed exactly one time to achieve optimal model quality. Because no two GPUs will ever need to read the same input token, there's never a need to copy input tokens between nodes inside the training loop.  I also acknowledge that the above description is greatly simplified; the entire node-local SSD capacity cannot be filled with input data, as space is also needed for checkpoints and other temporary data. However, the fact remains that super high-bandwidth or super high-capacity parallel file systems are not necessary for loading input tokens during training. AI training clusters are built with a ton of local SSDs to do the heavy lifting, and the input data for LLMs is small enough to fit in just a handful of GPU nodes. Writing model checkpoints Though the read workload of LLM training is modest at best, the write workload can be quite intense at scale because the probability of failure increases superlinearly with the size of the training job. However, unlike with scientific HPC jobs, the checkpoint size does not scale as a function of the job size; the checkpoint for a 405 billion-parameter model trained on 16,000 nodes is the same size as the checkpoint for that model trained on three nodes. This is a result of the fact that every training step is followed by a global synchronization which makes each data-parallel copy of the model identical. Only one copy of those model weights, which amounts to under a hundred terabytes for state-of-the-art LLMs, needs to be saved:   Kartik and Colleen Tartow at VAST wrote a quantitative breakdown of the true I/O requirements of checkpointing, and they illustrate how even a trillion-parameter model can achieve 99.7% forward progress (only 0.3% time spent checkpointing) when training across 3,072 GPUs with a modest 273 GB/s file system. A parallel file system is not required to get that level of performance; for example, HDD-based Azure Blob achieved over 1 TB/s when benchmarked with IOR for writes at scale. As with reading input tokens though, the real goal for checkpointing at scale is to remove any dependence on shared storage from the training loop entirely. And again, the best way to do this is to simply checkpoint to node-local storage. However, special care must be taken to ensure that the checkpoints don't get lost when a node crashes. In practice, LLM training is now done with asynchronous, multilevel checkpointing. This technique provides the scalability of checkpointing to node-local storage and the durability of shared storage: The key to this checkpointing process is hierarchical data synchronization: Model weights are first copied from GPU memory into the node's CPU memory after every training step. This checkpoint is governed by the CPU-GPU bandwidth (either PCIe or NVLink/Infinity Fabric), and a 500 GB checkpoint can complete in a second. The benefit of checkpointing to DRAM is that the GPU can unblock and begin computing the next step very quickly. However, this checkpoint in DRAM is not protected and will be lost if the node crashes.To protect against node crashes, the checkpoint is then asynchronously copied from CPU DRAM to a neighbor node's local SSD using RDMA. Now if a node crashes, it can restore from a checkpoint that is stored on its neighboring node's SSD via InfiniBand. Reading and writing a 500 GB checkpoint to neighboring SSDs might take ten seconds, so this asynchronous replication might be done for every tenth DRAM checkpoint.To store many checkpoints long-term, checkpoints are also asynchronously copied from node-local SSD to shared storage. This might take a minute or two per 500 GB checkpoint, so this last-level checkpoint copy might be done once every ten minutes. This hierarchical checkpointing scheme allows the GPUs to spend only a second checkpointing while being able to recover from job, node, and even cluster-level failures by tailoring the checkpoint tiering frequencies to the performance of each storage tier being used. The cost of recovering from a catastrophic failure might be re-computing up to ten minutes worth of training, but given the rarity of such events, this scheme balances the performance (and risks) of checkpointing to DRAM against hard drive prices (and suffering their performance) for a durable object store. To this latter point, the requirements of the shared storage system at the bottom of this checkpointing hierarchy are very modest: The checkpoint only needs to complete in the time between successive last-level checkpoint copies. If the 500 GB checkpoint is drained to shared storage only once every ten minutes, our shared storage only needs to deliver 1 GB/s of total bandwidth.The write pattern from node-local NVMe to shared storage is arbitrary, because it is a simple copy operation of a fully formed checkpoint file. Unlike direct-to-storage checkpoints, there are no weirdly shaped tensors being serialized into a file on the fly; rather, opaque bits are streaming from a local checkpoint file into a remote object using whatever transfer size and parallelism gives the highest write bandwidth. This combination of modest write bandwidth and simple, sequential, large-block writes is ideally suited for object stores. This isn't to say a parallel file system cannot work here, but this checkpointing scheme does not benefit from directory structure, fine-grained consistency semantics, or any of the other complexities that drive up the cost of parallel file systems. The catch, of course, is that checkpointing using these schemes can be complicated to implement. Fortunately, a growing number of training frameworks support both writing and restoring checkpoints using asynchronous and hierarchical approaches. Model developers never have to worry about interacting with specific files or objects; instead, the framework manages data locality during checkpoint and restart underneath a high-level API. Model deployment and inferencing Once a model is trained, putting it into production as an inferencing service is the final step of its lifecycle. From a storage and I/O standpoint, this is a lot more complicated than training because it marries an enterprise service delivery model (failover, load balancing, authentication, and scaling) with copies of a trained model running across HPC infrastructure. When you hear vendors talking about key-value stores, vector databases, and RAG, that is all happening at this stage. Setting aside everything but the storage attached to the GPU cluster though, the I/O requirements of inferencing are relatively straightforward: When provisioning a GPU node for inferencing, model weights must be loaded from shared storage as fast as possible.When using an LLM to search documents, a vector database is required to perform the similarity search that augments the LLM query with the relevant documents. This is the basis for RAG.Key-value caches are often used to reduce the latency for different parts of the inferencing pipeline by storing context including the conversation or frequently accessed contextual documents.As the inferencing demand evolves, different models and weights may be swapped in and out of individual GPU servers. A parallel file system is not particularly useful for any of these; the only place in which their high bandwidth would be a benefit is in loading and re-loading model weights (#1 and #4). But as with hierarchical checkpointing, those I/O operations are whole-object, read-only copies that are a natural fit for object APIs. Complex directory structures and strong consistency simply aren't necessary here. Objects are good enough, maybe better None of the steps in this model training lifecycle uniquely benefit from the capabilities that parallel file systems offer: Data ingestion involves hundreds of petabytes of small documents, but they are immediately packaged and indexed into large data containers. Their metadata is stored in a separate key-value store, so the directory hierarchy of a file system isn't used, and once data has been packaged and indexed, it's never modified in-place. The bandwidth requirements are modest as well since web crawling is the rate-limiting step.Data processing is an I/O-intensive data analytics workload. Read bandwidth is critical here, but data is accessed in large transactions and most of the computation is embarrassingly parallel. This workload runs on standalone analytics clusters, so even though the read bandwidth here is rate-limiting, slower storage is not going to impact GPU utilization on training clusters in any way. This step also reduces data by 100x or more, so the write requirements are also modest.Training requires both loading input tokens and checkpointing model weights. However, both of these workloads lean on node-local NVMe in every node to eliminate slowdowns due to noisy neighbors. Input data is staged to node-local storage only once at the beginning of a training campaign, and checkpoints are asynchronously bled out to shared storage without impacting GPU utilization.Inferencing involves infrequent, read-only, bulk loading of model weights into GPU nodes. While key-value caches and vector databases are also used in inferencing, parallel file systems offer no particular benefit for them. The I/O patterns of each of these steps map nicely to object storage since they are predominantly write-once and whole-file transactions. Parallel file systems certainly can be used, and workloads will benefit from the high bandwidth they offer. However, they come with the cost of features that aren't necessary--either literal costs (in the case of appliances or proprietary software) or figurative costs (allocating people to manage the complexities of debugging a parallel file system). The importance of this latter point is hard to appreciate if you've never used a supercomputer without a parallel file systems. However, I recently sat in on the validation of a brand-new H200 training cluster where various InfiniBand congestion and routing issues were being worked out. It wasn't until someone said "eviction" in some nontechnical context that I realized that the sporadic file system evictions during fabric instability were simply a non-issue. There was no cleanup of mount points after major fabric events because there was no persistent, fragile client-server state being maintained. I/Os between GPU nodes or nodes and storage might have failed during a rough patch, but they recovered and resumed on their own as soon as the fabric came back. Similarly, identity didn't matter, and all tests could be run as root because there was no implicit trust between the client kernel and remote storage. Removing the dependence between compute nodes, LDAP, and healthy file system mounts completely eliminates many of the challenges of standing up new clusters quickly. An ideal AI training cluster architecture The workloads I described above form a rough outline for an AI training infrastructure which has: A bunch of GPU nodes with a strong RDMA backend like InfiniBand. Each node should have at least enough node-local SSD to store a substantial amount of the input tokens to be used for training, enough space for hierarchical checkpointing, and enough I/O bandwidth to these SSDs to support draining checkpoints from partner nodes' DRAM in just a few seconds. A separate frontend network that connects to storage is also a good idea; it ensures that asynchronous checkpoint draining won't interfere with weight synchronization in the training loop.A separate CPU cluster for data processing pipelines. A strong backend network will benefit the deduplication step (which is critical to producing high-quality training datasets), but more emphasis should be placed on optimizing large-transaction reads from storage. Given that CPU nodes are so much cheaper than GPU nodes, separating the data processing nodes from training nodes allows you cut more corners when optimizing this CPU cluster. Keeping data processing out-of-band of actual model training means your most data-intensive step (data processing) is decoupled from your most expensive step (training).A scalable object store that supports basic write-once semantics with modest I/O bandwidth at scale. This matches the needs of the workloads with the price-performance of the storage system and simplifies the recovery process if the interconnect between compute and storage gets congested. It can also serve the data needs of all stages of the training pipeline: hundreds of petabytes of raw training data, hundreds of terabytes of input tokens, and tens of terabytes of model weights all have similar performance needs and can be stored on the same infrastructure with the appropriate QOS settings.A pool of general-purpose compute infrastructure for hosting the raw training data indices. This can also be used to support vector databases, raw context documents for RAG, and any other ancillary services required for production inferencing. By eschewing a high-performance parallel file system and localizing I/O performance to inside the GPU cluster with node-local NVMe, a vanilla network between the GPU cluster and the other subsystems is sufficient. Although less high-performance, these non-critical bits (ideally) have lower complexity, maintenance, and supportability as well, allowing (again, ideally) more resources to be sloshed towards supporting the high-value GPU infrastructure. Incidentally, this architecture happens to be how most of the largest AI training clusters on which I work are designed. But parallel files aren't all bad Of course, having no parallel file system presents some usability challenges if users are expecting to be able to SSH into a login node and have a complete user environment ready. The user experience for the above infrastructure works best for those who are comfortable developing software in containers and launching pods rather than developing software in vim and submitting Slurm jobs. I do not advocate for throwing out parallel file systems if they're already ingrained in users' workflows! In addition, the latest crop of modern, distributed file systems all now support multi-protocol data access. For example, WEKA, VAST, and Qumulo, all support S3 (object) interfaces as first-class citizens. Users who want the traditional HPC experience can play with their data using a file mount as they always have, while those who are coming in from the cloud-native side have equal access to those same data as objects. Supporting multiprotocol access to data in AI environments doesn't reduce the need to overbuild infrastructure or support stateful file mounts across all compute nodes, but it does provide an onramp for users to get comfortable moving away from the traditional HPC user experience. Finally, a few of the leading-edge parallel-file-system-turned-AI-storage platforms are also shipping features that make them valuable for the deployment and inferencing part of the lifecycle. For example, WEKA has their WARRP reference architecture for RAG, and VAST has its InsightEngine--both use the unique architectures underneath their file interfaces to accelerate vector queries far beyond what you would get from running a vector database on, say, Lustre. These so-called "AI data platforms," despite starting as parallel file systems, are spreading their relevance out to the entire LLM lifecycle, filling needs for file, object, and structured data with a single storage system. This is all to say that parallel file systems aren't bad, and they aren't going anywhere. But they aren't required to train frontier models either, and as I've tried to describe above, some of the largest supercomputers on the planet are designed not to require them.]]></summary></entry><entry><title type="html">Surfing the Singularity - The World is Not Flat</title><link href="https://hpc.social/personal-blog/2025/surfing-the-singularity-the-world-is-not-flat/" rel="alternate" type="text/html" title="Surfing the Singularity - The World is Not Flat" /><published>2025-01-29T18:11:00-07:00</published><updated>2025-01-29T18:11:00-07:00</updated><id>https://hpc.social/personal-blog/2025/surfing-the-singularity-the-world-is-not-flat</id><content type="html" xml:base="https://hpc.social/personal-blog/2025/surfing-the-singularity-the-world-is-not-flat/"><![CDATA[<div class="separator" style="clear: both; text-align: center;"></div>
<p>As Bill Gates recalls in his recent book-bumping interview with the Wall Street Journal, in the early innocent days of Microsoft he and his co-founder Paul Allen didn't believe in having an office in Washington, D.C.[1] They were soon to learn that was a mistake.[2] Compare and contrast with the scene in the Capital Rotunda last week for the inauguration of the new populist administration - Microsoft, Amazon, Facebook, Apple, Google, TikTok, and of course Tesla, all represented by their CEOs.[3] Microsoft's market capitalization as of this writing is now greater than the GDP of France.[4] Elon Musk's personal wealth is on par with the GDP of Denmark. Meta's platforms reach an estimated 40% of the world's population. </p>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"></td></tr><tr><td class="tr-caption" style="text-align: center;">Apple ad from yesteryear. We're now long past 1984.</td></tr></tbody></table>
<p>Consider these other inconvenient truths about global technology: that NVIDIA does not make the GPUs it designs, that most are manufactured by TSMC in Taiwan, which is about as far away from China as Cuba is from Florida. Software talent is globally distributed, and prices vary widely. There are some very good schools in some of these relatively inexpensive places - in the 2024 edition of the ACM student programming contest, MIT placed the highest among US teams, in 11th place.[5] </p>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"></td></tr><tr><td class="tr-caption" style="text-align: center;">Software salaries 2023, by country, exchange rate normalized.[6]</td></tr></tbody></table>
<p>AI programs and quantum computing initiatives are increasingly becoming nationalized as a strategic imperative. It is assumed that the country which is first to Artificial General Intelligence (AGI) will be the first to be able to use it to take control of the world. This fear is fueling an arms race in AI architectures, and the chips and the energy stations which power them. But is this a rational fear? </p>
<h3 style="text-align: left;">Unlearn What You Have Learned</h3>
<p>DeepSeek, with its deja vu inducing name and similarly eerily similar whale icon, is a new AI chatbot model wholly owned out of Hangzhou, China. And its #1 on the Apple app store, with a bullet. And yes, its quite up front that it tracks your data. There are several notable features and claims about this model: that is was trained in a fraction of the typical cost and time on a fraction of the typical hardware. That it performs about as well the OpenAI model released last month, maybe not as well on some things like straight math, but perhaps better at general writing, maybe with a bit more "personality". It shows its work - how it arrived at the answer its providing to the chat prompt. And, for the kicker from the totalitarian state, its open source up on Hugging Face.[7] </p>
<p>Shots fired. NVIDIA shares tumbled for a loss of $600B in one day in response, the largest single day loss in history. But what I'd like to know is, if this is the open source model, what's the real one like? Recent Federal governments have tried to enforce policies limiting technology exports to China, but in truth they've long had their own development programs - China has not participated in standardized HPC metric sharing and supercomputer ranking since 2017.</p>
<h3 style="text-align: left;">Red Flag on the Track</h3>
<p>Along with NVIDIA, power generation stocks were also down hard - GE Vernova down almost 20%. This does not mean the recent trend of tech companies buying nuclear power plants won't continue, or that we won't continue to hear of existing power stations giving data centers a direct hard wire bypassing the municipal grid. But clearly this open ended hunger for power - watts and GPU cycles - is not sustainable. And DeepSeek exposes that bare.</p>
<p>But make no mistake - this race is not over, its just warming up. OpenAI with their new Operators product currently defines AGI as a gaggle of collaborating AI agents, each with its own unique set of capabilities and goals. NVIDIA CEO Jansen Huang does his part in driving the GPU-dependent AI hype cycle by saying IT departments will become the new HR departments - for AI employees.[9] Goldman Sachs is telling clients to expect AI employees this year.[10] Cost avoidance will be a major driver.[11] And why not, when the same major technology companies report seemly amazing results using AI for software development? Google saves 50% on code migration time with AI! It gives *me* FOMO! [12]</p>
<p>The CEO of Anthropic predicts that by 2027 AI will be generally better than humans at almost everything.[13] Well, at some things maybe better than others. Turns out, what's the number one occupation we expect to be replaced by AI? Why, AI engineers and data scientists.</p>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"></td></tr><tr><td class="tr-caption" style="text-align: center;">Job skills impacted by generative AI, ranked. <br />Maybe I should have been a plumber.[14]</td></tr></tbody></table>
<p>This is vast uncharted territory for companies, especially for those of size, wedded to their legacy political structures and being either un-nimble or worse, fragile. People are not machines. The mistakes AI agents make are not of the same kind made by humans - current generative AI models are intentionally designed to make stuff up, to not just say "I don't know".[15] As a manager, you'll get no benefit of human insights into the truth such as from body language, though you may get to understand the "personality" and general performance characteristics of your AI employees over time. That is, until the managers are replaced by AI. But until then, what does your management interface look like? Is it perhaps similar to the IDE for a senior software engineer who manages a team of AI coders? In the AI-laced future, there will still be a place for HCI/UX designers.</p>
<h3 style="text-align: left;">Chef of the Future</h3>
<p>Finally, the November 2024 report from the National Academies on the "future of work" says "its impossible to predict exactly the nature of the coming changes in AI and all their effects on the economy and society".[16] This includes how it changes the nature of various jobs, or outright eliminates them. Continuing education will be key to a resilient workforce, and it turns out, AI might even play a role in that.</p>
<p>As shown in Washington last week, there are new business trends being driven by a rejuvenated alliance between big technology and big government, and it is therefore time for astute technology and business leaders to pay attention to both.[17] For example, tomorrow (January 30, 2025) OpenAI is holding a previously scheduled closed door meeting in Washington regarding its own current agentic technology innovations, and what they potentially imply for the US people and its government. I imagine the topic of DeepSeek will now disrupt the meeting agenda, somewhat. </p>
<p>At minimum, its a topic for a future blog. Regards. - andy</p>
<p><br /></p>
<h3 style="text-align: left;">References</h3>
<div><div><span style="font-size: x-small;">[0] Photo by AJ Colores on Unsplash, https://unsplash.com/@ajcolores</span></div>
<div><span style="font-size: x-small;">      </span></div>
<div><span style="font-size: x-small;">[1] Bill Gates interview by the Wall Street Journal, January 2025, https://www.youtube.com/watch?v=4LL-ynK_exM</span></div>
<div><span style="font-size: x-small;"><br /></span></div>
<div><span style="font-size: x-small;">[2] US vs. Microsoft, https://en.wikipedia.org/wiki/United_States_v._Microsoft_Corp</span></div>
<div><span style="font-size: x-small;"><br /></span></div>
<div><span style="font-size: x-small;">[3] https://apnews.com/article/trump-inauguration-tech-billionaires-zuckerberg-musk-wealth-0896bfc3f50d941d62cebc3074267ecd</span></div>
<div><span style="font-size: x-small;"><br /></span></div>
<div><span style="font-size: x-small;">[4] https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)</span></div>
<div><span style="font-size: x-small;"><br /></span></div>
<div><span style="font-size: x-small;">[5] https://icpc.global/worldfinals/results </span></div>
<div><span style="font-size: x-small;"><br /></span></div>
<div><span style="font-size: x-small;">[6] https://www.reddit.com/r/dataisbeautiful/comments/17a63yo/oc_2023_developer_compensation_by_country taken from the 2023 Stack Overflow developer survey.</span></div>
<div><span style="font-size: x-small;"><br /></span></div>
<div><span style="font-size: x-small;">[7] DeepSeek at Hugging Face: https://huggingface.co/organizations/deepseek-ai/activity/all</span></div>
<div><span style="font-size: x-small;"><br /></span></div>
<div><span style="font-size: x-small;">[8] OpenAI Operators: https://www.nytimes.com/2025/01/23/technology/openai-operator-launch.html</span></div>
<div><span style="font-size: x-small;"> </span></div>
<div><span style="font-size: x-small;">[9] NVIDIA CEO Jensen Huang on IT as the new HR: https://www.aol.com/finance/nvidia-jensen-huang-says-become-133641793.html</span></div>
<div><span style="font-size: x-small;"><br /></span></div>
<div><span style="font-size: x-small;">[10] Goldman Sachs on the rise of AI employees: https://it.slashdot.org/story/25/01/21/2213230/managing-ai-agents-as-employees-is-the-challenge-of-2025-says-goldman-sachs-cio</span></div>
<div><span style="font-size: x-small;"><br /></span></div>
<div><span style="font-size: x-small;">[11] https://www.msn.com/en-us/money/markets/why-cost-avoidance-became-an-ai-buzzword-for-holding-down-headcount/ar-BB1rmJSx</span></div>
<div><span style="font-size: x-small;"><br /></span></div>
<div><span style="font-size: x-small;">[12] https://developers.slashdot.org/story/25/01/17/2156235/google-reports-halving-code-migration-time-with-ai-help</span></div>
<div><span style="font-size: x-small;"><br /></span></div>
<div><span style="font-size: x-small;">[13] https://arstechnica.com/ai/2025/01/anthropic-chief-says-ai-could-surpass-almost-all-humans-at-almost-everything-shortly-after-2027/</span></div>
<div><span style="font-size: x-small;"><br /></span></div>
<div><span style="font-size: x-small;">[14] https://reports.weforum.org/docs/WEF_Future_of_Jobs_Report_2025.pdf</span></div>
<div><span style="font-size: x-small;"><br /></span></div>
<div><span style="font-size: x-small;">[15] https://slashdot.org/story/25/01/23/1645242/ai-mistakes-are-very-different-from-human-mistakes</span></div>
<div><span style="font-size: x-small;"><br /></span></div>
<div><span style="font-size: x-small;">[16] https://nap.nationalacademies.org/resource/27644/interactive/</span></div>
<div><span style="font-size: x-small;"><br /></span></div>
<div><span style="font-size: x-small;">[17] https://hbr.org/2024/11/navigating-the-new-geopolitics-of-tech</span></div>
<div><span style="font-size: x-small;"><br /></span></div>
<div><br /></div>
</div>]]></content><author><name>Surfing the Singularity</name></author><category term="surfthesing" /><summary type="html"><![CDATA[As Bill Gates recalls in his recent book-bumping interview with the Wall Street Journal, in the early innocent days of Microsoft he and his co-founder Paul Allen didn't believe in having an office in Washington, D.C.[1] They were soon to learn that was a mistake.[2] Compare and contrast with the scene in the Capital Rotunda last week for the inauguration of the new populist administration - Microsoft, Amazon, Facebook, Apple, Google, TikTok, and of course Tesla, all represented by their CEOs.[3] Microsoft's market capitalization as of this writing is now greater than the GDP of France.[4] Elon Musk's personal wealth is on par with the GDP of Denmark. Meta's platforms reach an estimated 40% of the world's population.  Apple ad from yesteryear. We're now long past 1984. Consider these other inconvenient truths about global technology: that NVIDIA does not make the GPUs it designs, that most are manufactured by TSMC in Taiwan, which is about as far away from China as Cuba is from Florida. Software talent is globally distributed, and prices vary widely. There are some very good schools in some of these relatively inexpensive places - in the 2024 edition of the ACM student programming contest, MIT placed the highest among US teams, in 11th place.[5]  Software salaries 2023, by country, exchange rate normalized.[6] AI programs and quantum computing initiatives are increasingly becoming nationalized as a strategic imperative. It is assumed that the country which is first to Artificial General Intelligence (AGI) will be the first to be able to use it to take control of the world. This fear is fueling an arms race in AI architectures, and the chips and the energy stations which power them. But is this a rational fear?  Unlearn What You Have Learned DeepSeek, with its deja vu inducing name and similarly eerily similar whale icon, is a new AI chatbot model wholly owned out of Hangzhou, China. And its #1 on the Apple app store, with a bullet. And yes, its quite up front that it tracks your data. There are several notable features and claims about this model: that is was trained in a fraction of the typical cost and time on a fraction of the typical hardware. That it performs about as well the OpenAI model released last month, maybe not as well on some things like straight math, but perhaps better at general writing, maybe with a bit more "personality". It shows its work - how it arrived at the answer its providing to the chat prompt. And, for the kicker from the totalitarian state, its open source up on Hugging Face.[7]  Shots fired. NVIDIA shares tumbled for a loss of $600B in one day in response, the largest single day loss in history. But what I'd like to know is, if this is the open source model, what's the real one like? Recent Federal governments have tried to enforce policies limiting technology exports to China, but in truth they've long had their own development programs - China has not participated in standardized HPC metric sharing and supercomputer ranking since 2017. Red Flag on the Track Along with NVIDIA, power generation stocks were also down hard - GE Vernova down almost 20%. This does not mean the recent trend of tech companies buying nuclear power plants won't continue, or that we won't continue to hear of existing power stations giving data centers a direct hard wire bypassing the municipal grid. But clearly this open ended hunger for power - watts and GPU cycles - is not sustainable. And DeepSeek exposes that bare. But make no mistake - this race is not over, its just warming up. OpenAI with their new Operators product currently defines AGI as a gaggle of collaborating AI agents, each with its own unique set of capabilities and goals. NVIDIA CEO Jansen Huang does his part in driving the GPU-dependent AI hype cycle by saying IT departments will become the new HR departments - for AI employees.[9] Goldman Sachs is telling clients to expect AI employees this year.[10] Cost avoidance will be a major driver.[11] And why not, when the same major technology companies report seemly amazing results using AI for software development? Google saves 50% on code migration time with AI! It gives *me* FOMO! [12] The CEO of Anthropic predicts that by 2027 AI will be generally better than humans at almost everything.[13] Well, at some things maybe better than others. Turns out, what's the number one occupation we expect to be replaced by AI? Why, AI engineers and data scientists. Job skills impacted by generative AI, ranked. Maybe I should have been a plumber.[14] This is vast uncharted territory for companies, especially for those of size, wedded to their legacy political structures and being either un-nimble or worse, fragile. People are not machines. The mistakes AI agents make are not of the same kind made by humans - current generative AI models are intentionally designed to make stuff up, to not just say "I don't know".[15] As a manager, you'll get no benefit of human insights into the truth such as from body language, though you may get to understand the "personality" and general performance characteristics of your AI employees over time. That is, until the managers are replaced by AI. But until then, what does your management interface look like? Is it perhaps similar to the IDE for a senior software engineer who manages a team of AI coders? In the AI-laced future, there will still be a place for HCI/UX designers. Chef of the Future Finally, the November 2024 report from the National Academies on the "future of work" says "its impossible to predict exactly the nature of the coming changes in AI and all their effects on the economy and society".[16] This includes how it changes the nature of various jobs, or outright eliminates them. Continuing education will be key to a resilient workforce, and it turns out, AI might even play a role in that. As shown in Washington last week, there are new business trends being driven by a rejuvenated alliance between big technology and big government, and it is therefore time for astute technology and business leaders to pay attention to both.[17] For example, tomorrow (January 30, 2025) OpenAI is holding a previously scheduled closed door meeting in Washington regarding its own current agentic technology innovations, and what they potentially imply for the US people and its government. I imagine the topic of DeepSeek will now disrupt the meeting agenda, somewhat.  At minimum, its a topic for a future blog. Regards. - andy References [0] Photo by AJ Colores on Unsplash, https://unsplash.com/@ajcolores        [1] Bill Gates interview by the Wall Street Journal, January 2025, https://www.youtube.com/watch?v=4LL-ynK_exM [2] US vs. Microsoft, https://en.wikipedia.org/wiki/United_States_v._Microsoft_Corp [3] https://apnews.com/article/trump-inauguration-tech-billionaires-zuckerberg-musk-wealth-0896bfc3f50d941d62cebc3074267ecd [4] https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal) [5] https://icpc.global/worldfinals/results  [6] https://www.reddit.com/r/dataisbeautiful/comments/17a63yo/oc_2023_developer_compensation_by_country taken from the 2023 Stack Overflow developer survey. [7] DeepSeek at Hugging Face: https://huggingface.co/organizations/deepseek-ai/activity/all [8] OpenAI Operators: https://www.nytimes.com/2025/01/23/technology/openai-operator-launch.html   [9] NVIDIA CEO Jensen Huang on IT as the new HR: https://www.aol.com/finance/nvidia-jensen-huang-says-become-133641793.html [10] Goldman Sachs on the rise of AI employees: https://it.slashdot.org/story/25/01/21/2213230/managing-ai-agents-as-employees-is-the-challenge-of-2025-says-goldman-sachs-cio [11] https://www.msn.com/en-us/money/markets/why-cost-avoidance-became-an-ai-buzzword-for-holding-down-headcount/ar-BB1rmJSx [12] https://developers.slashdot.org/story/25/01/17/2156235/google-reports-halving-code-migration-time-with-ai-help [13] https://arstechnica.com/ai/2025/01/anthropic-chief-says-ai-could-surpass-almost-all-humans-at-almost-everything-shortly-after-2027/ [14] https://reports.weforum.org/docs/WEF_Future_of_Jobs_Report_2025.pdf [15] https://slashdot.org/story/25/01/23/1645242/ai-mistakes-are-very-different-from-human-mistakes [16] https://nap.nationalacademies.org/resource/27644/interactive/ [17] https://hbr.org/2024/11/navigating-the-new-geopolitics-of-tech]]></summary></entry><entry><title type="html">Fine tuning AI models with InstructLab under IBM LSF</title><link href="https://hpc.social/personal-blog/2025/fine-tuning-ai-models-with-instructlab-under-ibm-lsf/" rel="alternate" type="text/html" title="Fine tuning AI models with InstructLab under IBM LSF" /><published>2025-01-06T19:36:24-07:00</published><updated>2025-01-06T19:36:24-07:00</updated><id>https://hpc.social/personal-blog/2025/fine-tuning-ai-models-with-instructlab-under-ibm-lsf</id><content type="html" xml:base="https://hpc.social/personal-blog/2025/fine-tuning-ai-models-with-instructlab-under-ibm-lsf/"><![CDATA[<p><strong>Overview</strong></p>

<p>All the best for 2025! This blog looks back on a demo which I created for <a href="https://sc24.supercomputing.org">SC24</a>
last November to demonstrate InstructLab workflows running on an <a href="https://www.ibm.com/products/hpc-workload-management">IBM LSF</a>
cluster. Let’s begin with a bit of background. I’d like to thank Michael
Spriggs, STSM, IBM LSF for his contributions to this blog.</p>

<p>When I think of tuning, what immediately comes to my mind are visions of an
expert mechanic trying to extract the most from an engine. This blog is
focused on an entirely different type of tuning, AI model tuning. Like tuning
an engine, AI model tuning can be used to ensure a better fit for a given AI
model for your business.</p>

<p>Released by IBM and Red Hat in May 2024, <a href="https://research.ibm.com/blog/instruct-lab">InstructLab</a> is an open-source project
which provides the ability to fine-tune LLMs by adding skills and knowledge,
without having to retrain the model from scratch. InstructLab can run on
resource-constrained systems such as laptops, but also supports GPUs. Much has
been written about InstructLab and this blog is not intended to provide an
in-depth look at InstructLab. Rather, the objective here is to demonstrate how
InstructLab workloads can be distributed and managed in a high-performance
computing cluster with GPUs using the IBM LSF workload scheduler. Recently, IBM
published a paper describing the infrastructure used to train the Granite family
of AI foundation models. The paper describes the Vela and Blue Vela environments
in detail. In particular, the Blue Vela environment is built on a software stack
using Red Hat Enterprise Linux, IBM LSF and Storage Scale. Learn more in the
detailed paper <a href="https://arxiv.org/abs/2407.05467">here</a>.</p>

<p>The demo workflow consists of two LSF jobs. The first job generates synthetic
data, which is used to teach the LLM new skills or knowledge. The second job,
which depends upon the successful completion of the first, is the training job,
where the new skills or knowledge are incorporated into an existing base model.
A simple LSF job dependency is used to ensure the training job only runs after
the successful completion of the synthetic data generation step.</p>

<p>The environment used is equipped with Nvidia GPUs.  InstructLab jobs will be
run with the options for GPU support, and the jobs will be submitted to LSF
with the appropriate GPU scheduling directives. Furthermore, it is assumed that
the users' $HOME directory is available on all hosts in the cluster. Note that I
require neither root access, nor a user account that is an LSF administrator, to
install and use InstructLab on the LSF cluster.</p>

<p><strong>Configuration</strong></p>

<p>The HPC cluster is configured as follows:</p>

<ul>
<li>Red Hat Enterprise Linux v8.8</li>
<li>IBM LSF v10.0.1.15</li>
<li>InstructLab v0.19.4</li>
<li>Miniforge v3 (24.9.0-0)</li>
<li>NVIDIA CUDA v12.6</li>
<li>Compute nodes are equipped with 8 x Nvidia H100 GPUs</li>
</ul>
<p><strong>Install InstructLab</strong></p>

<ol>
<li>Log in to a compute node in the LSF cluster equipped with GPUs. If ssh access
is disabled to compute nodes, then submit an interactive LSF batch job. This job
requests 8 GPUs on a single system and will set them to exclusive execution
mode.</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext">$ bsub -Is -R "span[hosts=1]" -gpu "num=8:j_exclusive=yes" bash</code></pre></div>

<ol start="2">
<li>Install and set up a Conda environment. This will enable you to install a
self-contained Conda environment for your user account with the necessary
Python version needed for InstructLab. Miniforge is installed in the default
location and the option to update the users shell profile to start the Conda
environment are selected. We assume here a shared $HOME directory.</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext">$ cd $HOME
$ curl -L -O "https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh"
$ bash Miniforge3-$(uname)-$(uname -m).sh</code></pre></div>

<ol start="3">
<li>Before proceeding, you must logout and log back in to activate the
environment. Next, a Conda environment is created with name <em>my_env</em>. Here we’ll
specify Python v3.11, which is a requirement for InstructLab.</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext">conda create --name my_env -c anaconda python=3.11
conda activate my_env</code></pre></div>

<ol start="4">
<li>Next, install InstructLab. Here, version 0.19.4 of InstructLab is specified.
This was the version of InstructLab available in the timeframe preceding the
SC24 event. Follow the installation steps in the official InstructLab
documentation <a href="https://github.com/instructlab/instructlab?tab=readme-ov-file#-installing-ilab">here</a>.</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext">$ pip install instructlab==0.19.4</code></pre></div>

<ol start="5">
<li>Next, perform the installation of InstructLab with Nvidia CUDA support. This
is required for InstructLab to utilize the GPUs. Without this step, InstructLab
will run on the CPUs. Note that CUDA v12.6 is installed on the system and the
variables set below reflect this.</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext">$ export CMAKE_ARGS="-DLLAMA_CUBLAS=on -DCUDA_PATH=/usr/local/cuda-12.6 -DCUDAToolkit_ROOT=/usr/local/cuda-12.6 -DCUDAToolkit_INCLUDE_DIR=/usr/local/cuda-12/include -DCUDAToolkit_LIBRARY_DIR=/usr/local/cuda-12.6/lib64"
$ export PATH=/usr/local/cuda-12.6/bin:$PATH
$ pip cache remove llama_cpp_python
$ CMAKE_ARGS="-DLLAMA_CUDA=on -DLLAMA_NATIVE=off" pip install 'instructlab[cuda]'
$ pip install vllm@git+https://github.com/opendatahub-io/vllm@v0.6.2</code></pre></div>

<p><strong>Configure InstructLab</strong></p>

<ol>
<li>With the installation of InstructLab complete, the next step is to run the
initialization. This will setup paths to models, taxonomy repo as well as the
GPU configuration.</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext">$ ilab config init</code></pre></div>

<ol start="2">
<li>By default InstructLab stores models, training checkpoints and other files
within <em>~/.cache</em> and <em>~/.local/share/instructlab</em>. If you have limited storage
capacity available in $HOME, then you may opt to disable training checkpoint
files. This can be done by setting the following option in <em>~/.config/instructlab/config.yaml</em> as follows.</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext">train:

  checkpoint_at_epoch: false</code></pre></div>

<ol start="3">
<li>Next, we download the required models. The ilab model list command can be
used to list the models which are available. Note that a <a href="https://huggingface.co">HuggingFace</a> token is
required to download certain models. Please set HF_TOKEN in the environment
with the appropriate token.</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext">$ export HF_TOKEN=&lt;HuggingFace token&gt;
$ ilab model download
$ ilab model download --repository=instructlab/granite-7b-lab
$ ilab model list

+--------------------------------------+---------------------+---------+

| Model Name                           | Last Modified       | Size    |

+--------------------------------------+---------------------+---------+
| instructlab/granite-7b-lab           | 2024-12-27 20:37:29 | 12.6 GB |
| mistral-7b-instruct-v0.2.Q4_K_M.gguf | 2024-12-27 16:55:46 | 4.1 GB  |
| merlinite-7b-lab-Q4_K_M.gguf         | 2024-12-27 16:48:39 | 4.1 GB  |
+--------------------------------------+---------------------+---------+</code></pre></div>

<p><strong>Generate synthetic data &amp; AI model training</strong></p>

<p>Next, is the synthetic data generation step, which will be executed on GPUs.
This step is a prerequisite to teaching the LLM new skills/knowledge via
training.</p>

<ol>
<li>
<p>Here we use example knowledge from the InstructLab github about Taylor Swift
fans, who are known as “Swifties”. This is timely because Taylor Swift recently
wrapped up 6 concerts in Toronto, Canada, where I happen to be based. Copy
attribution.txt and qna.yaml from the following <a href="https://github.com/mairin/taxonomy/tree/swifties/knowledge/arts/music/fandom/swifties">location</a>.</p>

</li>
<li>
<p>By default, the InstructLab taxonomy is found in <em>~/.local/share/instructlab/taxonomy</em>. Here we create the directories fandom/swifties under <em>~/.local/share/instructlab/taxonomy/knowledge/arts/fandom</em> and copy the files from step 1 into
this location.</p>

</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext">$ mkdir -p ~/.local/share/instructlab/taxonomy/knowledge/arts/fandom/swifties
$ cp &lt;path_to&gt;/attribution.txt ~/.local/share/instructlab/taxonomy/knowledge/arts/fandom/swifties
$ cp &lt;path_to&gt;/qna.yaml ~/.local/share/instructlab/taxonomy/knowledge/arts/fandom/swifties</code></pre></div>

<ol start="3">
<li>With the Swifties taxonomy in place, check for any syntax errors with the
command <em>ilab taxonomy diff</em>. It should report that the taxonomy is valid if
there are no syntax errors.</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext">$ ilab taxonomy diff
knowledge/arts/fandom/swifties/qna.yaml
Taxonomy in /u/gsamu/.local/share/instructlab/taxonomy is valid :)</code></pre></div>

<ol start="4">
<li>With the taxonomy in place and having confirmed that the syntax is valid,
it’s now time to run the synthetic data generation job through LSF. Here we will
request 8 GPUs on a single server in exclusive execution mode. For the
InstructLab ilab command, specify the <em>&ndash;gpus 8 and &ndash;pipeline full</em> options.
Standard output is written to the $HOME/job-output with filename specification
&lt;LSF_JOBID&gt;.out. The $HOME/job-output directory must already exist.</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext">$ mkdir -p $HOME/job-output
$ bsub -o $HOME/job-output/%J.out -R "span[hosts=1]" -gpu "num=8:j_exclusive=yes" ilab data generate --pipeline full --gpus 8
Job &lt;1131&gt; is submitted to default queue &lt;normal&gt;.</code></pre></div>

<ol start="5">
<li>During job execution, the LSF <em>bpeek</em> command can be used to monitor the job
standard output.</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext">$ bpeek -f 1131 
&lt;&lt; output from stdout &gt;&gt;
INFO 2025-01-02 09:51:29,503 numexpr.utils:146: Note: detected 96 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
INFO 2025-01-02 09:51:29,504 numexpr.utils:149: Note: NumExpr detected 96 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
INFO 2025-01-02 09:51:29,504 numexpr.utils:162: NumExpr defaulting to 16 threads.
INFO 2025-01-02 09:51:30,038 datasets:59: PyTorch version 2.3.1 available.
INFO 2025-01-02 09:51:31,226 instructlab.model.backends.llama_cpp💯 Trying to connect to model server at http://127.0.0.1:8000/v1
WARNING 2025-01-02 09:51:56,356 instructlab.data.generate:270: Disabling SDG batching - unsupported with llama.cpp serving
Generating synthetic data using 'full' pipeline, '/u/gsamu/.cache/instructlab/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf' model, '/u/gsamu/.local/share/instructlab/taxonomy' taxonomy, against http://127.0.0.1:55779/v1 server
INFO 2025-01-02 09:51:56,861 instructlab.sdg.generate_data:356: Synthesizing new instructions. If you aren't satisfied with the generated instructions, interrupt training (Ctrl-C) and try adjusting your YAML files. Adding more examples may help.
INFO 2025-01-02 09:51:56,872 instructlab.sdg.pipeline:153: Running pipeline single-threaded
INFO 2025-01-02 09:51:56,872 instructlab.sdg.pipeline:197: Running block: duplicate_document_col
INFO 2025-01-02 09:51:56,872 instructlab.sdg.pipeline:198: Dataset({
    features: ['icl_document', 'document', 'document_outline', 'domain', 'icl_query_1', 'icl_query_2', 'icl_query_3', 'icl_response_1', 'icl_response_2', 'icl_response_3'],
    num_rows: 35
})
INFO 2025-01-02 09:51:58,286 instructlab.sdg.llmblock:51: LLM server supports batched inputs: False
INFO 2025-01-02 09:51:58,286 instructlab.sdg.pipeline:197: Running block: gen_spellcheck
INFO 2025-01-02 09:51:58,286 instructlab.sdg.pipeline:198: Dataset({
    features: ['icl_document', 'document', 'document_outline', 'domain', 'icl_query_1', 'icl_query_2', 'icl_query_3', 'icl_response_1', 'icl_response_2', 'icl_response_3', 'base_document'],
    num_rows: 35
})
/u/gsamu/miniforge3/envs/my_env/lib/python3.11/site-packages/llama_cpp/llama.py:1054: RuntimeWarning: Detected duplicate leading "&lt;s&gt;" in prompt, this will likely reduce response quality, consider removing it...
  warnings.warn(
INFO 2025-01-02 09:57:42,264 instructlab.sdg.pipeline:197: Running block: flatten_auxiliary_columns
INFO 2025-01-02 09:57:42,264 instructlab.sdg.pipeline:198: Dataset({
    features: ['icl_document', 'document', 'document_outline', 'domain', 'icl_query_1', 'icl_query_2', 'icl_query_3', 'icl_response_1', 'icl_response_2', 'icl_response_3', 'base_document', 'spellcheck'],
    num_rows: 35
})
INFO 2025-01-02 09:57:42,279 instructlab.sdg.pipeline:197: Running block: rename_to_document_column
INFO 2025-01-02 09:57:42,279 instructlab.sdg.pipeline:198: Dataset({
    features: ['icl_document', 'document', 'document_outline', 'domain', 'icl_query_1', 'icl_query_2', 'icl_query_3', 'icl_response_1', 'icl_response_2', 'icl_response_3', 'dataset_type', 'corrected_document'],
    num_rows: 70
})
INFO 2025-01-02 09:57:42,282 instructlab.sdg.pipeline:197: Running block: gen_knowledge
INFO 2025-01-02 09:57:42,282 instructlab.sdg.pipeline:198: Dataset({
    features: ['icl_document', 'raw_document', 'document_outline', 'domain', 'icl_query_1', 'icl_query_2', 'icl_query_3', 'icl_response_1', 'icl_response_2', 'icl_response_3', 'dataset_type', 'document'],
    num_rows: 70
})
…
…</code></pre></div>

<ol start="6">
<li>During the runtime of the job, it’s possible to view GPU related metrics
using the LSF <em>lsload</em> and <em>bhosts</em> commands. First, we need to identify the host
where the job has been dispatched to using the LSF bjobs command. In this case
the job was dispatched to host <em>p1-r01-n4</em>. Note that details GPU accounting
metrics are available once the job runs to completion.</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext">$ bjobs -w
JOBID   USER    STAT  QUEUE      FROM_HOST   EXEC_HOST   JOB_NAME   SUBMIT_TIME
1131    gsamu   RUN   normal     rmf-login-1 p1-r01-n4   ilab data generate --pipeline full --gpus 8 Jan  2 14:51
$ lsload -w -gpu p1-r01-n4
HOST_NAME                 status ngpus gpu_shared_avg_mut gpu_shared_avg_ut ngpus_physical
p1-r01-n4                     ok     8                 2%                7%              8
$ bhosts -w -gpu p1-r01-n4
HOST_NAME            GPU_ID                MODEL     MUSED      MRSV  NJOBS    RUN   SUSP    RSV 
p1-r01-n4                 0   NVIDIAH10080GBHBM3        2G        0G      1      1      0      0
                          1   NVIDIAH10080GBHBM3        2G        0G      1      1      0      0
                          2   NVIDIAH10080GBHBM3        2G        0G      1      1      0      0
                          3   NVIDIAH10080GBHBM3        2G        0G      1      1      0      0
                          4   NVIDIAH10080GBHBM3        2G        0G      1      1      0      0
                          5   NVIDIAH10080GBHBM3        2G        0G      1      1      0      0
                          6   NVIDIAH10080GBHBM3        2G        0G      1      1      0      0
                          7   NVIDIAH10080GBHBM3        2G        0G      1      1      0      0</code></pre></div>

<ol start="7">
<li>After job completion, it’s possible to view details about the job including
GPU utilization which LSF collects by leveraging NVIDIA DCGM. These metrics are
available upon job completion using both the LSF <em>bhist</em> and <em>bjobs</em> commands.</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext">$ bhist -l -gpu 1131

Job &lt;1131&gt;, User &lt;gsamu&gt;, Project &lt;default&gt;, Command &lt;ilab data generate --pipe
                          line full --gpus 8&gt;
Thu Jan  2 14:51:23 2025: Submitted from host &lt;rmf-login-1&gt;, to Queue &lt;normal&gt;,
                           CWD &lt;$HOME&gt;, Output File &lt;/u/gsamu/job-output/%J.out
                          &gt;, Requested Resources &lt;span[hosts=1]&gt;, Requested GPU
                           &lt;num=8:j_exclusive=yes&gt;;
Thu Jan  2 14:51:24 2025: Dispatched 1 Task(s) on Host(s) &lt;p1-r01-n4&gt;, Allocate
                          d 1 Slot(s) on Host(s) &lt;p1-r01-n4&gt;, Effective RES_REQ
                           &lt;select[((ngpus&gt;0)) &amp;&amp; (type == local)] order[r15s:p
                          g] rusage[ngpus_physical=8.00] span[hosts=1] &gt;;
Thu Jan  2 14:51:25 2025: Starting (Pid 3095851);
Thu Jan  2 14:51:25 2025: External Message "p1-r01-n4:gpus=0,1,2,3,4,5,6,7;EFFE
                          CTIVE GPU REQ: num=8:mode=shared:mps=no:j_exclusive=y
                          es:gvendor=nvidia;" was posted from "gsamu" to messag
                          e box 0;
Thu Jan  2 14:51:26 2025: Running with execution home &lt;/u/gsamu&gt;, Execution CWD
                           &lt;/u/gsamu&gt;, Execution Pid &lt;3095851&gt;;
Thu Jan  2 16:08:05 2025: Done successfully. The CPU time used is 4624.0 second
                          s;
                          HOST: p1-r01-n4; CPU_TIME: 4624 seconds              
                                          GPU ID: 0
                                  Total Execution Time: 4597 seconds
                                  Energy Consumed: 579704 Joules
                                  SM Utilization (%): Avg 9, Max 15, Min 0
                                  Memory Utilization (%): Avg 2, Max 100, Min 0
                                  Max GPU Memory Used: 1956642816 bytes

                              GPU ID: 1
                                  Total Execution Time: 4597 seconds
                                  Energy Consumed: 503956 Joules
                                  SM Utilization (%): Avg 7, Max 11, Min 0
                                  Memory Utilization (%): Avg 2, Max 5, Min 0
                                  Max GPU Memory Used: 1767899136 bytes

                              GPU ID: 2
                                  Total Execution Time: 4597 seconds
                                  Energy Consumed: 501754 Joules
                                  SM Utilization (%): Avg 7, Max 11, Min 0
                                  Memory Utilization (%): Avg 2, Max 5, Min 0
                                  Max GPU Memory Used: 1784676352 bytes

                              GPU ID: 3
                                  Total Execution Time: 4597 seconds
                                  Energy Consumed: 525195 Joules
                                  SM Utilization (%): Avg 7, Max 11, Min 0
                                  Memory Utilization (%): Avg 2, Max 54, Min 0
                                  Max GPU Memory Used: 1767899136 bytes

                              GPU ID: 4
                                  Total Execution Time: 4597 seconds
                                  Energy Consumed: 525331 Joules
                                  SM Utilization (%): Avg 7, Max 12, Min 0
                                  Memory Utilization (%): Avg 2, Max 5, Min 0
                                  Max GPU Memory Used: 1767899136 bytes

                              GPU ID: 5
                                  Total Execution Time: 4597 seconds
                                  Energy Consumed: 502416 Joules
                                  SM Utilization (%): Avg 7, Max 11, Min 0
                                  Memory Utilization (%): Avg 2, Max 5, Min 0
                                  Max GPU Memory Used: 1784676352 bytes

                              GPU ID: 6
                                  Total Execution Time: 4597 seconds
                                  Energy Consumed: 508720 Joules
                                  SM Utilization (%): Avg 7, Max 12, Min 0
                                  Memory Utilization (%): Avg 2, Max 5, Min 0
                                  Max GPU Memory Used: 1784676352 bytes

                              GPU ID: 7
                                  Total Execution Time: 4597 seconds
                                  Energy Consumed: 491041 Joules
                                  SM Utilization (%): Avg 6, Max 12, Min 0
                                  Memory Utilization (%): Avg 2, Max 4, Min 0
                                  Max GPU Memory Used: 1933574144 bytes

GPU Energy Consumed: 4138117.000000 Joules

Thu Jan  2 16:08:05 2025: Post job process done successfully;


GPU_ALLOCATION:
 HOST             TASK GPU_ID  GI_PLACEMENT/SIZE    CI_PLACEMENT/SIZE    MODEL        MTOTAL  FACTOR MRSV    SOCKET NVLINK/XGMI                      
 p1-r01-n4        0    0       -                    -                    NVIDIAH10080 80G     9.0    0G      0      -                               
                  0    1       -                    -                    NVIDIAH10080 80G     9.0    0G      0      -                               
                  0    2       -                    -                    NVIDIAH10080 80G     9.0    0G      0      -                               
                  0    3       -                    -                    NVIDIAH10080 80G     9.0    0G      0      -                               
                  0    4       -                    -                    NVIDIAH10080 80G     9.0    0G      1      -                               
                  0    5       -                    -                    NVIDIAH10080 80G     9.0    0G      1      -                               
                  0    6       -                    -                    NVIDIAH10080 80G     9.0    0G      1      -                               
                  0    7       -                    -                    NVIDIAH10080 80G     9.0    0G      1      -                               

MEMORY USAGE:
MAX MEM: 2 Gbytes;  AVG MEM: 1 Gbytes; MEM Efficiency: 0.00%

CPU USAGE:
CPU PEAK: 1.69 ;  CPU PEAK DURATION: 52 second(s)
CPU AVERAGE EFFICIENCY: 100.69% ;  CPU PEAK EFFICIENCY: 169.23%

Summary of time in seconds spent in various states by  Thu Jan  2 16:08:05 2025
  PEND     PSUSP    RUN      USUSP    SSUSP    UNKWN    TOTAL
  1        0        4601     0        0        0        4602 </code></pre></div>

<ol start="8">
<li>
<p>When the synthetic data generation job completes, it’s output can be viewed
at <em>~/job-output/<!-- raw HTML omitted -->.out</em>. The synthetic data sets will comprise files in
the directory <em>~/.local/share/instructlab/datasets</em>. These files will be named
*skills_train_msgs_*.jsonl* and *knowledge_train_msgs_*.jsonl*.</p>

</li>
<li>
<p>With the synthetic data generation step complete, it’s now time to run the
training. We first set 2 environment variables to point to the following
files:  <em>~/.local/share/instructlab/datasets/knowledge_train_msgs_2025-01-02T09_51_56.jsonl</em>  and <em>~./.local/share/instructlab/datasets/skills_train_msgs_2025-01-02T09_51_56.jsonl</em>.</p>

</li>
</ol>
<p>Afterward, we submit the training job to LSF requesting 8 GPUs and with ilab
options <em>&ndash;pipeline accelerated</em>, <em>&ndash;gpus 8</em>, <em>&ndash;device cuda</em> and
<em>&ndash;data-path</em> pointing to the two above data files that were produced in the
synthetic data generation step.</p>

<div class="highlight"><pre><code class="language-plaintext">$ export SKILLS_PATH=/u/gsamu/.local/share/instructlab/datasets/skills_train_msgs_2025-01-02T09_51_56.jsonl
$ export KNOWLEDGE_PATH=/u/gsamu/.local/share/instructlab/datasets/knowledge_train_msgs_2025-01-02T09_51_56.jsonl
$ bsub -o $HOME/job-output/%J.out -R "span[hosts=1]" -gpu "num=8:j_exclusive=yes" ilab model train --pipeline accelerated --data-path $SKILLS_PATH --data-path $KNOWLEDGE_PATH --device cuda --gpus 8
Job &lt;1135&gt; is submitted to default queue &lt;normal&gt;.</code></pre></div>

<ol start="10">
<li>During job execution, the LSF <em>bpeek</em> command can be used to monitor the
job standard output.</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext">$ bpeek -f 1135
&lt;&lt; output from stdout &gt;&gt;
LoRA is disabled (rank=0), ignoring all additional LoRA args
[2025-01-02 12:52:04,359] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
INFO 2025-01-02 12:52:09,061 numexpr.utils:146: Note: detected 96 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
INFO 2025-01-02 12:52:09,061 numexpr.utils:149: Note: NumExpr detected 96 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
INFO 2025-01-02 12:52:09,061 numexpr.utils:162: NumExpr defaulting to 16 threads.
INFO 2025-01-02 12:52:09,304 datasets:59: PyTorch version 2.3.1 available.
You are using the default legacy behaviour of the &lt;class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'&gt;. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
INFO 2025-01-02 12:52:09,653 root:617: Special tokens: eos: [32000], pad: [32001], bos: [32005], system: [32004], user: [32002], assistant: [32003]
INFO 2025-01-02 12:52:09,923 root:617: number of dropped samples: 0 -- out of 641
 data arguments are:
{"data_path":"/u/gsamu/.local/share/instructlab/datasets/knowledge_train_msgs_2025-01-02T09_51_56.jsonl","data_output_path":"/u/gsamu/.local/share/instructlab/internal","max_seq_len":4096,"model_path":"/u/gsamu/.cache/instructlab/models/instructlab/granite-7b-lab","chat_tmpl_path":"/u/gsamu/miniforge3/envs/my_env/lib/python3.11/site-packages/instructlab/training/chat_templates/ibm_generic_tmpl.py","num_cpu_procs":16}
tokenizing the dataset with /u/gsamu/.cache/instructlab/models/instructlab/granite-7b-lab tokenizer...
ten largest length percentiles:
quantile 90th: 1459.0
quantile 91th: 1466.0
quantile 92th: 1469.6000000000001
quantile 93th: 1478.2
quantile 94th: 1483.0
quantile 95th: 1488.0
quantile 96th: 1497.1999999999998
quantile 97th: 1516.5999999999997
quantile 98th: 1540.6000000000001
quantile 99th: 1656.0000000000016
quantile 100th: 2578.0

at 4096 max sequence length, the number of samples to be dropped is 0
(0.00% of total)
quantile 0th: 368.0
quantile 1th: 393.0
quantile 2th: 411.2
quantile 3th: 421.2
quantile 4th: 427.2
quantile 5th: 442.0
quantile 6th: 604.4
quantile 7th: 631.8
quantile 8th: 653.8000000000001
quantile 9th: 679.8
quantile 10th: 742.0
at 20 min sequence length, the number of samples to be dropped is 0
checking the validity of the samples...
Categorizing training data type...
unmasking the appropriate message content...
 Samples Previews...
…
…</code></pre></div>

<ol start="11">
<li>During the runtime of the training job, we can observe some GPU utilization
information using the LSF lsload and bhosts commands.  First we need to identify
the server on which the training job is running. This is done using the bjobs
command and checking for the execution host of the job.</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext">$ bjobs -w
JOBID   USER    STAT  QUEUE      FROM_HOST   EXEC_HOST   JOB_NAME   SUBMIT_TIME
1135    gsamu   RUN   normal     rmf-login-1 p1-r01-n1   ilab model train --pipeline accelerated --data-path /u/gsamu/.local/share/instructlab/datasets/skills_train_msgs_2025-01-02T09_51_56.jsonl --data-path /u/gsamu/.local/share/instructlab/datasets/knowledge_train_msgs_2025-01-02T09_51_56.jsonl --device cuda --gpus 8 Jan  2 17:51
$ lsload -w -gpu p1-r01-n1
HOST_NAME                 status ngpus gpu_shared_avg_mut gpu_shared_avg_ut ngpus_physical
p1-r01-n1                     ok     8                 0%               22%              8
$ bhosts -w -gpu p1-r01-n1
HOST_NAME            GPU_ID                MODEL     MUSED      MRSV  NJOBS    RUN   SUSP    RSV 
p1-r01-n1                 0   NVIDIAH10080GBHBM3       10G        0G      1      1      0      0
                          1   NVIDIAH10080GBHBM3       10G        0G      1      1      0      0
                          2   NVIDIAH10080GBHBM3       10G        0G      1      1      0      0
                          3   NVIDIAH10080GBHBM3       10G        0G      1      1      0      0
                          4   NVIDIAH10080GBHBM3       10G        0G      1      1      0      0
                          5   NVIDIAH10080GBHBM3       10G        0G      1      1      0      0
                          6   NVIDIAH10080GBHBM3       10G        0G      1      1      0      0
                          7   NVIDIAH10080GBHBM3       10G        0G      1      1      0      0</code></pre></div>

<ol start="12">
<li>Once the job is complete, detailed GPU accounting can again be viewed using
the LSF <em>bhist</em> command as follows below.</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext">$ bhist -l -gpu 1135

Job &lt;1135&gt;, User &lt;gsamu&gt;, Project &lt;default&gt;, Command &lt;ilab model train --pipeli
                          ne accelerated --data-path /u/gsamu/.local/share/inst
                          ructlab/datasets/skills_train_msgs_2025-01-02T09_51_5
                          6.jsonl --data-path /u/gsamu/.local/share/instructlab
                          /datasets/knowledge_train_msgs_2025-01-02T09_51_56.js
                          onl --device cuda --gpus 8&gt;
Thu Jan  2 17:51:48 2025: Submitted from host &lt;rmf-login-1&gt;, to Queue &lt;normal&gt;,
                           CWD &lt;$HOME/.local/share/instructlab/checkpoints&gt;, Ou
                          tput File &lt;/u/gsamu/job-output/%J.out&gt;, Requested Res
                          ources &lt;span[hosts=1]&gt;, Requested GPU &lt;num=8:j_exclus
                          ive=yes&gt;;
Thu Jan  2 17:51:48 2025: Dispatched 1 Task(s) on Host(s) &lt;p1-r01-n1&gt;, Allocate
                          d 1 Slot(s) on Host(s) &lt;p1-r01-n1&gt;, Effective RES_REQ
                           &lt;select[((ngpus&gt;0)) &amp;&amp; (type == local)] order[r15s:p
                          g] rusage[ngpus_physical=8.00] span[hosts=1] &gt;;
Thu Jan  2 17:51:49 2025: Starting (Pid 3462241);
Thu Jan  2 17:51:49 2025: Running with execution home &lt;/u/gsamu&gt;, Execution CWD
                           &lt;/u/gsamu/.local/share/instructlab/checkpoints&gt;, Exe
                          cution Pid &lt;3462241&gt;;
Thu Jan  2 17:51:49 2025: External Message "p1-r01-n1:gpus=0,1,2,3,4,5,6,7;EFFE
                          CTIVE GPU REQ: num=8:mode=shared:mps=no:j_exclusive=y
                          es:gvendor=nvidia;" was posted from "gsamu" to messag
                          e box 0;
Thu Jan  2 17:57:56 2025: Done successfully. The CPU time used is 3024.0 second
                          s;
                          HOST: p1-r01-n1; CPU_TIME: 3024 seconds              
                                          GPU ID: 0
                                  Total Execution Time: 365 seconds
                                  Energy Consumed: 98890 Joules
                                  SM Utilization (%): Avg 20, Max 100, Min 0
                                  Memory Utilization (%): Avg 9, Max 62, Min 0
                                  Max GPU Memory Used: 53022294016 bytes

                              GPU ID: 1
                                  Total Execution Time: 365 seconds
                                  Energy Consumed: 97697 Joules
                                  SM Utilization (%): Avg 53, Max 100, Min 0
                                  Memory Utilization (%): Avg 9, Max 58, Min 0
                                  Max GPU Memory Used: 53087305728 bytes

                              GPU ID: 2
                                  Total Execution Time: 365 seconds
                                  Energy Consumed: 94820 Joules
                                  SM Utilization (%): Avg 53, Max 100, Min 0
                                  Memory Utilization (%): Avg 9, Max 62, Min 0
                                  Max GPU Memory Used: 53221523456 bytes

                              GPU ID: 3
                                  Total Execution Time: 365 seconds
                                  Energy Consumed: 98014 Joules
                                  SM Utilization (%): Avg 53, Max 100, Min 0
                                  Memory Utilization (%): Avg 9, Max 59, Min 0
                                  Max GPU Memory Used: 53041168384 bytes

                              GPU ID: 4
                                  Total Execution Time: 365 seconds
                                  Energy Consumed: 99246 Joules
                                  SM Utilization (%): Avg 53, Max 100, Min 0
                                  Memory Utilization (%): Avg 9, Max 60, Min 0
                                  Max GPU Memory Used: 53045362688 bytes

                              GPU ID: 5
                                  Total Execution Time: 365 seconds
                                  Energy Consumed: 94952 Joules
                                  SM Utilization (%): Avg 53, Max 100, Min 0
                                  Memory Utilization (%): Avg 9, Max 65, Min 0
                                  Max GPU Memory Used: 53047459840 bytes

                              GPU ID: 6
                                  Total Execution Time: 365 seconds
                                  Energy Consumed: 98227 Joules
                                  SM Utilization (%): Avg 53, Max 100, Min 0
                                  Memory Utilization (%): Avg 9, Max 63, Min 0
                                  Max GPU Memory Used: 53127151616 bytes

                              GPU ID: 7
                                  Total Execution Time: 365 seconds
                                  Energy Consumed: 94582 Joules
                                  SM Utilization (%): Avg 52, Max 100, Min 0
                                  Memory Utilization (%): Avg 9, Max 65, Min 0
                                  Max GPU Memory Used: 53481570304 bytes

GPU Energy Consumed: 776428.000000 Joules

Thu Jan  2 17:57:56 2025: Post job process done successfully;


GPU_ALLOCATION:
 HOST             TASK GPU_ID  GI_PLACEMENT/SIZE    CI_PLACEMENT/SIZE    MODEL        MTOTAL  FACTOR MRSV    SOCKET NVLINK/XGMI                      
 p1-r01-n1        0    0       -                    -                    NVIDIAH10080 80G     9.0    0G      0      -                               
                  0    1       -                    -                    NVIDIAH10080 80G     9.0    0G      0      -                               
                  0    2       -                    -                    NVIDIAH10080 80G     9.0    0G      0      -                               
                  0    3       -                    -                    NVIDIAH10080 80G     9.0    0G      0      -                               
                  0    4       -                    -                    NVIDIAH10080 80G     9.0    0G      1      -                               
                  0    5       -                    -                    NVIDIAH10080 80G     9.0    0G      1      -                               
                  0    6       -                    -                    NVIDIAH10080 80G     9.0    0G      1      -                               
                  0    7       -                    -                    NVIDIAH10080 80G     9.0    0G      1      -                               

MEMORY USAGE:
MAX MEM: 104 Gbytes;  AVG MEM: 16 Gbytes; MEM Efficiency: 0.00%

CPU USAGE:
CPU PEAK: 17.86 ;  CPU PEAK DURATION: 49 second(s)
CPU AVERAGE EFFICIENCY: 856.60% ;  CPU PEAK EFFICIENCY: 1785.71%

Summary of time in seconds spent in various states by  Thu Jan  2 17:57:56 2025
  PEND     PSUSP    RUN      USUSP    SSUSP    UNKWN    TOTAL
  0        0        368      0        0        0        368         </code></pre></div>

<ol start="13">
<li>Finally, with the model successfully trained, let’s chat with the new model
to check the result. Here’s we’ll pose it Swiftie specific questions. Note that
the output from the training is written to <em>~/.local/share/instructlab/checkpoints/hf_format</em>. We’ll take the model from the latest checkpoint directory that was
created. Here again, we launch the model chat job via LSF as an interactive
batch job (i.e. <em>bsub -Is</em>).</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext">$ grep hf_format 1135.out
Model saved in /u/gsamu/.local/share/instructlab/checkpoints/hf_format/samples_886
Model saved in /u/gsamu/.local/share/instructlab/checkpoints/hf_format/samples_1776
Model saved in /u/gsamu/.local/share/instructlab/checkpoints/hf_format/samples_2658
Model saved in /u/gsamu/.local/share/instructlab/checkpoints/hf_format/samples_3546
Model saved in /u/gsamu/.local/share/instructlab/checkpoints/hf_format/samples_4435</code></pre></div>

<div class="highlight"><pre><code class="language-plaintext">$ bsub -Is -R "span[hosts=1]" -gpu "num=8:j_exclusive=yes" ilab model chat --model /u/gsamu/.local/share/instructlab/checkpoints/hf_format/samples_4435
Job &lt;1146&gt; is submitted to default queue &lt;interactive&gt;.
&lt;&lt;Waiting for dispatch ...&gt;&gt;
&lt;&lt;Starting on p1-r01-n2&gt;&gt;
INFO 2025-01-02 15:06:07,600 instructlab.model.backends.vllm:105: Trying to connect to model server at http://127.0.0.1:8000/v1
INFO 2025-01-02 15:06:08,876 instructlab.model.backends.vllm:308: vLLM starting up on pid 3744375 at http://127.0.0.1:41531/v1
INFO 2025-01-02 15:06:08,876 instructlab.model.backends.vllm:114: Starting a temporary vLLM server at http://127.0.0.1:41531/v1
INFO 2025-01-02 15:06:08,876 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 1/120
INFO 2025-01-02 15:06:12,244 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 2/120
INFO 2025-01-02 15:06:15,614 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 3/120
INFO 2025-01-02 15:06:18,801 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 4/120
INFO 2025-01-02 15:06:21,952 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 5/120
INFO 2025-01-02 15:06:25,391 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 6/120
INFO 2025-01-02 15:06:28,638 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 7/120
INFO 2025-01-02 15:06:32,103 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 8/120
INFO 2025-01-02 15:06:35,296 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 9/120
INFO 2025-01-02 15:06:38,616 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 10/120
INFO 2025-01-02 15:06:42,015 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 11/120
INFO 2025-01-02 15:06:45,435 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 12/120
INFO 2025-01-02 15:06:48,679 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 13/120
INFO 2025-01-02 15:06:52,025 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 14/120
INFO 2025-01-02 15:06:55,317 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 15/120
INFO 2025-01-02 15:06:58,604 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 16/120
INFO 2025-01-02 15:07:01,927 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 17/120
INFO 2025-01-02 15:07:05,287 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 18/120
INFO 2025-01-02 15:07:08,763 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 19/120
INFO 2025-01-02 15:07:12,131 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 20/120
INFO 2025-01-02 15:07:15,476 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 21/120
INFO 2025-01-02 15:07:18,881 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 22/120
INFO 2025-01-02 15:07:22,203 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 23/120
INFO 2025-01-02 15:07:25,599 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 24/120
INFO 2025-01-02 15:07:28,991 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 25/120
INFO 2025-01-02 15:07:32,234 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 26/120
INFO 2025-01-02 15:07:35,714 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 27/120
INFO 2025-01-02 15:07:38,974 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 28/120
INFO 2025-01-02 15:07:42,265 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 29/120
INFO 2025-01-02 15:07:45,582 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 30/120
INFO 2025-01-02 15:07:45,586 instructlab.model.backends.vllm:136: vLLM engine successfully started at http://127.0.0.1:41531/v1
╭────────────────────────────────────────────────────────────── system ──────────────────────────────────────────────────────────────╮
│ Welcome to InstructLab Chat w/ SAMPLES_4435 (type /h for help)                                                                     │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
&gt;&gt;&gt; Tell me everything you know about Swifties.                                                                           [S][default]
╭─────────────────────────────────────────────────────────── samples_4435 ───────────────────────────────────────────────────────────╮
│ Swifties are the fandom of the American singer-songwriter Taylor Swift.                                                            │
│ Regarded by journalists as one of the largest, most devoted, and influential fan bases, Swifties are known for their high levels   │
│ of participation, creativity, community, fanaticism, and cultural impact on the music industry and popular culture. They are a     │
│ subject of widespread coverage in the mainstream media.                                                                            │
│                                                                                                                                    │
│ Critics have opined that Swift has redefined artist-fan relationships by establishing an intimate connection with Swifties. She    │
│ has frequently engaged with, helped, credited, and prioritized her fans, who have offered unprecedented support and interest in    │
│ her works irrespective of her wavering reception in the media. They continued to support Swift through her genre transitions,      │
│ unexpected artistic pivots, and her highly publicized controversies such as the 2019 masters dispute, while instigating the        │
│ political scrutiny of Ticketmaster that led to implementation of various laws and stimulated economic growth with the Eras Tour.   │
│ Swift's releases, promotional efforts, and fashion have garnered attention for incorporating Easter eggs and clues that are        │
...
...</code></pre></div>

<p><strong>Conclusions</strong></p>

<p>We’ve demonstrated a simple InstructLab workflow that is scheduled by IBM LSF
in a compute cluster equipped with GPUs.  As part of this example, LSF GPU
scheduling and accounting for GPU workloads was highlighted. For organizations
looking to productionize InstructLab and where there is a pool of GPU equipped
compute resources, LSF provides an ideal way to manage demand from a user
community looking to run these intensive workloads.</p>

<p>At the recent SC24 event, the demonstration went beyond what is shown in this
blog. It incorporated single click job submission via LSF Application Center
using a custom template that was created for InstructLab to submits both the
synthetic data generation job, as well the training job with a single click.
The demo environment was on IBM Cloud using instances equipped with Nvidia GPUs.
The compute instances were automatically scaled up and down by the LSF resource
connector. This will be the topic for a future blog.</p>]]></content><author><name>Ramblings of a supercomputing enthusiast.</name></author><category term="gaborsamu" /><summary type="html"><![CDATA[Overview]]></summary></entry><entry><title type="html">Surfing the Singularity - “Please hold for the next available agent”</title><link href="https://hpc.social/personal-blog/2025/surfing-the-singularity-please-hold-for-the-next-available-agent/" rel="alternate" type="text/html" title="Surfing the Singularity - “Please hold for the next available agent”" /><published>2025-01-03T17:00:00-07:00</published><updated>2025-01-03T17:00:00-07:00</updated><id>https://hpc.social/personal-blog/2025/surfing-the-singularity-please-hold-for-the-next-available-agent-</id><content type="html" xml:base="https://hpc.social/personal-blog/2025/surfing-the-singularity-please-hold-for-the-next-available-agent/"><![CDATA[<div class="separator" style="clear: both; text-align: center;"></div>
<p><br />&lt;div style="text-align: left;"&gt;<br />&lt;/div&gt;</p>
<div><span style="font-family: verdana;">Our imaginations, having been so stimulated by the "innovation trigger" of early interactions with ChatGPT and its LLM kin, having experienced the illusion of the algorithm reading your mind, we have now firmly entered into the period of inflated expectations. Any day now we expect a knock on the door to be informed by some HAL Junior that not only are we now out of a job, we've also got 20 minutes to evacuate the premise before its bulldozed to make way for another solar farm and data center. AGI is only just one product announcement away, or maybe two, but certainly three at most... </span></div>
<div><h3 style="text-align: left;"><span style="font-family: verdana;">Nose Deep </span></h3></div>
<div><span style="font-family: verdana;">There is a strong desire on the part of companies trafficking in AI to generate not just chatbot hallucinations but also customers for real business use cases, meaning revenue, and now. To do that we're going to need hardware, fast, lots of it, and gigajoules to power it. So AWS buys a new data center in PA adjacent to a 2.5GW nuclear power plant.[1] Not to be outdone Microsoft re-revs up Three Mile Island (albeit with a catchy rebranding laughable by 1970's standards), with 100% of the power going to their regional AI data centers.[2] </span></div>
<div><span style="font-family: verdana;"><br /></span><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://media.datacenterdynamics.com/media/images/Constellation_Three_Mile_Island.width-358.png" style="margin-left: auto; margin-right: auto;"><img border="0" height="280" src="https://media.datacenterdynamics.com/media/images/Constellation_Three_Mile_Island.width-358.png" width="400" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;"><span style="font-family: verdana;">Three Mile Island nuclear power plant, aka the "Crane Clean Energy Center".<br /></span>  </td></tr></tbody></table></div>
<div><span style="font-family: verdana;">After vigorous expectations the trough of disillusionment will soon follow. Already Microsoft hints that demand for AI-oriented chips is waning.[3] Practical, as you'll have a hard time getting them anyway - the data-center grade GPU chips on which AI computation rely are in short supply - NVIDIA via their TSMC outsource manufacturing partner is fully booked for Blackwell GPU orders for the next 12 months.[4] AWS has recently announced to customers (like me) new limitations on availability of certain NVIDIA GPU instances. (Consider also that AI competes with crypto for these scarce GPUs.) Intel suggests it will ship mass quantities of chips for AI-ready PCs and other mobile devices in 2025, but the stock traders are not yet buying it, with the stock currently fallen over 50% year-over-year. In the end, and as evidenced by the long term investments, we of course expect the march of techno-progress to continue, but in the short run, aligning expectations with reality may remain a challenge.</span></div>
<div><span style="font-family: verdana;"><br /></span></div>
<div><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"></td></tr><tr><td class="tr-caption" style="text-align: center;"><div style="font-family: verdana;">The August 2024 Gartner Hype Cycle for Emerging Technologies. </div>
<div style="font-family: verdana;">Generative AI - weee! [5]</div>
</td></tr></tbody></table></div>
<div><span style="font-family: verdana;"><div style="text-align: center;"><br /></div>
What does OpenAI say about all this? First, the desire to be non-profit has bumped up against the realities of scaling up the models. Will they continue to scale up, yielding better and deeper performance on the road to artificial general intelligence simply by scaling up, or will they hit a theoretical wall?[6] Sam Altman says succinctly: "there is no wall".[7] The nuclear-powered race is on, be it sustainable or not.</span></div>
<div><span style="font-family: verdana;"><br /></span><h3 style="text-align: left;"><span style="font-family: verdana;">"Your wait time is now less than..."</span></h3></div>
<div><span style="font-family: verdana;">But as we argued in the last blog [8], we don't need dystopia-inducing super-human AGI in order to make productive and disruptive use of artificial intelligence technologies - a domain-tuned artificial capable intelligence (ACI) is enough.[9] Or a collaborating set of them. </span></div>
<div><span style="font-family: verdana;"><br />OpenAI's strategic product roadmap is more than a little vague [10], but in theory after chatbots capable of basic reasoning comes the age of agents - think: allowing Alexa to auto-restock your pantry via a hotline to Bezos when it overhears you say you're low on sugar. Such "AI" does such a good job doing basic thinks like, oh I dunno, controlling the lights in your home now, what could go wrong?! Truth is, today's LLMs perform only so-so on standardized benchmarks, and while they improve all the time [11], the current state of the art is not yet ready to be trusted and at times seems like snake oil.[12]</span></div>
<div><span style="font-family: verdana;"><br />Today's agents tend to be domain-specific and tailored to narrow purpose - Salesforce.com agents for common customer interactions, ServiceNow agents helping the human agent perform repetitive or summary tasks in handling case loads, but not replacing the human.[13,14] Google Gemini can add events to your calendar, help you plan travel, but is not yet trusted to actually borrow your credit card and book it. Keeping the human-in-the-loop will remain for now, as a stepping stone to full automation.</span></div>
<div><span style="font-family: verdana;"><br />If you visit agent marketplaces like Agent.ai or SwarmZero.ai, you'll see on the order of hundreds of agents available to handle what are largely small, mundane, and repetitive tasks. There are similar domain agent marketplaces on OpenAI's site, Anthropic's, GitHub, Hugging Face, and more. Let's go along with the current norm and define "assistants" as gaggles of agents loosely collaborating to accomplish more complex tasks, perhaps as part of a hybrid AI-human team or for some cases ultimately on behalf of the entire organization, and yet, still not requiring full-on AGI. (Consider what just one techno-savvy entrepreneur with a diverse collection of AI auto-orgs might do.)</span></div>
<div><span style="font-family: verdana;"><br />The missing elements are reliable agent accuracy, which yields trust, and the hardware and power to run it all. Trust, unfortunately in the near term, may play second fiddle to profit, as the AI snake oil is sold to companies and governments and ultimately end users, most of whom barely understand it.</span></div>
<div><span style="font-family: verdana;"><br />In fact, the scientists themselves barely understand it. The deep learning networks that power today's LLMs are generally black boxes, layers upon layers of neural networks, numeric weights and matrix computations, where its pretty difficult to tell where any given word, image fragment, or concept is held in the vast space of the model, and how with various feed-forward and back-propagation processes in the network it is used in computing responses.</span></div>
<div><span style="font-family: verdana;"><br /></span></div>
<div><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://miro.medium.com/v2/0*-8c-MXmNvcvTLdHH.png" style="margin-left: auto; margin-right: auto;"><img border="0" height="360" src="https://miro.medium.com/v2/0*-8c-MXmNvcvTLdHH.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;"><span style="font-family: verdana; text-align: left;">A GPT model formed by combining successive attention and neural net layers. Input comes in at the left, and its black boxes all the way down.[15]</span></td></tr></tbody></table><div class="separator" style="clear: both; text-align: center;"> <span>  </span></div>
<span style="font-family: verdana;"><br /></span></div>
<div><span style="font-family: verdana;">Black box or not, as Sam Altman says, deep learning just works.[16] Sort of - AGI is unlikely without strong ontological and reasoning abilities and a tactile understanding of the physical world.[17] And deep learning itself is not without its problems. If the training data is biased, so will be the results. Trainers have to be alert to overfitting the model to the training data in a way that makes the model ineffective on new data. And implementors need better tools which help introspect and observe the model to provide verification, to illuminate the black box. Until then, any technology which cannot be understood is indistinguishable from magic.</span></div>
<div><span style="font-family: verdana;"><br /></span><h3 style="text-align: left;"><span style="font-family: verdana;">Hell-o Operator</span></h3></div>
<div><span style="font-family: verdana;">AI is a broad term, encompassing many technologies, machine learning being just one of them, and deep learning based on neural networks being an even further niche. In many ways, given the black box nature of the solution, AI has become a substitute word for "automation", and/or "program", or "algorithm". And the ill-defined AI landscape is moving fast. Twelve months ago the buzz was about the emergence of the "prompt engineer" role in lieu of computer programmers, and today, not so much. Instead we now have thin but actionable (i.e. product-oriented) definitions like "agent" and "assistant" and a new suite of tools and cute icons to put on enterprise architecture diagrams. This is not to even mention the human and organizational impact of new agent-based workflows characterized by iterative, non-waterfall business processes - not something well understood or appreciated outside of software engineering circles.</span></div>
<div><span style="font-family: verdana;"><br />In this turbulent time, with vendors leapfrogging each other's capabilities and performance, there is no and cannot be any real standardization, no agreed abstractions on which to base a unifying orchestration layer. Move fast and break things, fix them later if they live long enough. Let the prototype knowingly become the short-lived product, and iterate, maybe. Think: sqrt of web time. Think: ChatGPT + IFTTT.[18] That is not an enterprise IT solution, nor one manageable for most individuals. That is a fine mess.</span></div>
<div><span style="font-family: verdana;"><br />Thankfully, we'll soon have AI assistants to fix it for us. </span></div>
<div><span style="font-family: verdana;"><br /></span></div>
<div><span style="font-family: verdana;">- andy <span style="font-size: 16px;">(linkedin: andygallojr)</span><br /><br /><br /></span><h3 style="text-align: left;"><span style="font-family: verdana;">References</span></h3><span style="font-family: verdana;">[1] <a href="https://www.datacenterdynamics.com/en/news/aws-acquires-talens-nuclear-data-center-campus-in-pennsylvania/">https://www.datacenterdynamics.com/en/news/aws-acquires-talens-nuclear-data-center-campus-in-pennsylvania/</a><br />[2] <a href="https://www.datacenterdynamics.com/en/news/three-mile-island-nuclear-power-plant-to-return-as-microsoft-signs-20-year-835mw-ai-data-center-ppa/">https://www.datacenterdynamics.com/en/news/three-mile-island-nuclear-power-plant-to-return-as-microsoft-signs-20-year-835mw-ai-data-center-ppa/</a></span></div>
<div><span style="font-family: verdana;">Readers unfamiliar with the nuclear accident at Three Mile Island in 1979 can read the summary here: <a href="https://en.wikipedia.org/wiki/Three_Mile_Island_accident">https://en.wikipedia.org/wiki/Three_Mile_Island_accident</a></span></div>
<div><span style="font-family: verdana;">[3] <a href="https://finance.yahoo.com/news/nvidia-stocks-correction-accelerated-since-020804144.html">https://finance.yahoo.com/news/nvidia-stocks-correction-accelerated-since-020804144.html</a><br />[4] <a href="https://www.smbom.com/news/14253">https://www.smbom.com/news/14253</a><br />[5] Gartner Hype Cycle for Emerging Technologies, August 2024, <a href="https://emt.gartnerweb.com/ngw/globalassets/en/newsroom/images/graphs/august_2024_ethc.png">https://emt.gartnerweb.com/ngw/globalassets/en/newsroom/images/graphs/august_2024_ethc.png</a><br />[6] "The Computational Limits of Deep Learning", <a href="https://arxiv.org/pdf/2007.05558">https://arxiv.org/pdf/2007.05558</a><br />[7] Sam Altman on X: "there is no wall", <a href="https://x.com/sama/status/1856941766915641580">https://x.com/sama/status/1856941766915641580</a></span></div>
<div><span style="font-family: verdana;">[8] Surfing the Singularity blog, <a href="https://surfthesing.blogspot.com/2024/12/surfing-singularity-coming-wave-book.html">https://surfthesing.blogspot.com/2024/12/surfing-singularity-coming-wave-book.html</a></span></div>
<div><span style="font-family: verdana;">[9] <span style="background-color: black; font-size: 15px;">"The Coming Wave", </span><span style="background-color: black; font-size: 15px;">M. Suleyman, Crown Pub., 2023</span><br />[10] <a href="https://www.theneurondaily.com/p/openais-leaked-agi-roadmap">https://www.theneurondaily.com/p/openais-leaked-agi-roadmap</a><br />[11] 12 Days of OpenAI, Day 12: <a href="https://www.youtube.com/watch?v=SKBG1sqdyIU">https://www.youtube.com/watch?v=SKBG1sqdyIU</a> <br />[12] "AI Snake Oil", Narayanan &amp; Kapoor, Princeton U. Press, 2024<br />[13] <a href="https://www.salesforce.com/news/stories/einstein-sales-agents-announcement">https://www.salesforce.com/news/stories/einstein-sales-agents-announcement</a><br />[14] <a href="https://www.servicenow.com/standard/resource-center/data-sheet/ds-virtual-agent.html">https://www.servicenow.com/standard/resource-center/data-sheet/ds-virtual-agent.html</a><br />[15] <a href="https://miro.medium.com/v2/0*-8c-MXmNvcvTLdHH.png">https://miro.medium.com/v2/0*-8c-MXmNvcvTLdHH.png</a> We recommend the following video for those not familiar with this architecture:  <a href="https://youtu.be/KJtZARuO3JY?si=Muq2xRdSTaa9LMXb">https://youtu.be/KJtZARuO3JY?si=Muq2xRdSTaa9LMXb</a><br />[16] <a href="https://ia.samaltman.com/">https://ia.samaltman.com/</a><br />[17] Yann LeCun on Lex Fridman podcast, <a href="https://www.youtube.com/watch?v=5t1vTLU7s40">https://www.youtube.com/watch?v=5t1vTLU7s40</a><br />[18] <a href="https://ifttt.com/chatgpt">https://ifttt.com/chatgpt</a><br /><br /></span></div>]]></content><author><name>Surfing the Singularity</name></author><category term="surfthesing" /><summary type="html"><![CDATA[&lt;div style="text-align: left;"&gt;&lt;/div&gt; Our imaginations, having been so stimulated by the "innovation trigger" of early interactions with ChatGPT and its LLM kin, having experienced the illusion of the algorithm reading your mind, we have now firmly entered into the period of inflated expectations. Any day now we expect a knock on the door to be informed by some HAL Junior that not only are we now out of a job, we've also got 20 minutes to evacuate the premise before its bulldozed to make way for another solar farm and data center. AGI is only just one product announcement away, or maybe two, but certainly three at most...  Nose Deep  There is a strong desire on the part of companies trafficking in AI to generate not just chatbot hallucinations but also customers for real business use cases, meaning revenue, and now. To do that we're going to need hardware, fast, lots of it, and gigajoules to power it. So AWS buys a new data center in PA adjacent to a 2.5GW nuclear power plant.[1] Not to be outdone Microsoft re-revs up Three Mile Island (albeit with a catchy rebranding laughable by 1970's standards), with 100% of the power going to their regional AI data centers.[2]  Three Mile Island nuclear power plant, aka the "Crane Clean Energy Center".   After vigorous expectations the trough of disillusionment will soon follow. Already Microsoft hints that demand for AI-oriented chips is waning.[3] Practical, as you'll have a hard time getting them anyway - the data-center grade GPU chips on which AI computation rely are in short supply - NVIDIA via their TSMC outsource manufacturing partner is fully booked for Blackwell GPU orders for the next 12 months.[4] AWS has recently announced to customers (like me) new limitations on availability of certain NVIDIA GPU instances. (Consider also that AI competes with crypto for these scarce GPUs.) Intel suggests it will ship mass quantities of chips for AI-ready PCs and other mobile devices in 2025, but the stock traders are not yet buying it, with the stock currently fallen over 50% year-over-year. In the end, and as evidenced by the long term investments, we of course expect the march of techno-progress to continue, but in the short run, aligning expectations with reality may remain a challenge. The August 2024 Gartner Hype Cycle for Emerging Technologies.  Generative AI - weee! [5] What does OpenAI say about all this? First, the desire to be non-profit has bumped up against the realities of scaling up the models. Will they continue to scale up, yielding better and deeper performance on the road to artificial general intelligence simply by scaling up, or will they hit a theoretical wall?[6] Sam Altman says succinctly: "there is no wall".[7] The nuclear-powered race is on, be it sustainable or not. "Your wait time is now less than..." But as we argued in the last blog [8], we don't need dystopia-inducing super-human AGI in order to make productive and disruptive use of artificial intelligence technologies - a domain-tuned artificial capable intelligence (ACI) is enough.[9] Or a collaborating set of them.  OpenAI's strategic product roadmap is more than a little vague [10], but in theory after chatbots capable of basic reasoning comes the age of agents - think: allowing Alexa to auto-restock your pantry via a hotline to Bezos when it overhears you say you're low on sugar. Such "AI" does such a good job doing basic thinks like, oh I dunno, controlling the lights in your home now, what could go wrong?! Truth is, today's LLMs perform only so-so on standardized benchmarks, and while they improve all the time [11], the current state of the art is not yet ready to be trusted and at times seems like snake oil.[12] Today's agents tend to be domain-specific and tailored to narrow purpose - Salesforce.com agents for common customer interactions, ServiceNow agents helping the human agent perform repetitive or summary tasks in handling case loads, but not replacing the human.[13,14] Google Gemini can add events to your calendar, help you plan travel, but is not yet trusted to actually borrow your credit card and book it. Keeping the human-in-the-loop will remain for now, as a stepping stone to full automation. If you visit agent marketplaces like Agent.ai or SwarmZero.ai, you'll see on the order of hundreds of agents available to handle what are largely small, mundane, and repetitive tasks. There are similar domain agent marketplaces on OpenAI's site, Anthropic's, GitHub, Hugging Face, and more. Let's go along with the current norm and define "assistants" as gaggles of agents loosely collaborating to accomplish more complex tasks, perhaps as part of a hybrid AI-human team or for some cases ultimately on behalf of the entire organization, and yet, still not requiring full-on AGI. (Consider what just one techno-savvy entrepreneur with a diverse collection of AI auto-orgs might do.) The missing elements are reliable agent accuracy, which yields trust, and the hardware and power to run it all. Trust, unfortunately in the near term, may play second fiddle to profit, as the AI snake oil is sold to companies and governments and ultimately end users, most of whom barely understand it. In fact, the scientists themselves barely understand it. The deep learning networks that power today's LLMs are generally black boxes, layers upon layers of neural networks, numeric weights and matrix computations, where its pretty difficult to tell where any given word, image fragment, or concept is held in the vast space of the model, and how with various feed-forward and back-propagation processes in the network it is used in computing responses. A GPT model formed by combining successive attention and neural net layers. Input comes in at the left, and its black boxes all the way down.[15]    Black box or not, as Sam Altman says, deep learning just works.[16] Sort of - AGI is unlikely without strong ontological and reasoning abilities and a tactile understanding of the physical world.[17] And deep learning itself is not without its problems. If the training data is biased, so will be the results. Trainers have to be alert to overfitting the model to the training data in a way that makes the model ineffective on new data. And implementors need better tools which help introspect and observe the model to provide verification, to illuminate the black box. Until then, any technology which cannot be understood is indistinguishable from magic. Hell-o Operator AI is a broad term, encompassing many technologies, machine learning being just one of them, and deep learning based on neural networks being an even further niche. In many ways, given the black box nature of the solution, AI has become a substitute word for "automation", and/or "program", or "algorithm". And the ill-defined AI landscape is moving fast. Twelve months ago the buzz was about the emergence of the "prompt engineer" role in lieu of computer programmers, and today, not so much. Instead we now have thin but actionable (i.e. product-oriented) definitions like "agent" and "assistant" and a new suite of tools and cute icons to put on enterprise architecture diagrams. This is not to even mention the human and organizational impact of new agent-based workflows characterized by iterative, non-waterfall business processes - not something well understood or appreciated outside of software engineering circles. In this turbulent time, with vendors leapfrogging each other's capabilities and performance, there is no and cannot be any real standardization, no agreed abstractions on which to base a unifying orchestration layer. Move fast and break things, fix them later if they live long enough. Let the prototype knowingly become the short-lived product, and iterate, maybe. Think: sqrt of web time. Think: ChatGPT + IFTTT.[18] That is not an enterprise IT solution, nor one manageable for most individuals. That is a fine mess. Thankfully, we'll soon have AI assistants to fix it for us.  - andy (linkedin: andygallojr)References[1] https://www.datacenterdynamics.com/en/news/aws-acquires-talens-nuclear-data-center-campus-in-pennsylvania/[2] https://www.datacenterdynamics.com/en/news/three-mile-island-nuclear-power-plant-to-return-as-microsoft-signs-20-year-835mw-ai-data-center-ppa/ Readers unfamiliar with the nuclear accident at Three Mile Island in 1979 can read the summary here: https://en.wikipedia.org/wiki/Three_Mile_Island_accident [3] https://finance.yahoo.com/news/nvidia-stocks-correction-accelerated-since-020804144.html[4] https://www.smbom.com/news/14253[5] Gartner Hype Cycle for Emerging Technologies, August 2024, https://emt.gartnerweb.com/ngw/globalassets/en/newsroom/images/graphs/august_2024_ethc.png[6] "The Computational Limits of Deep Learning", https://arxiv.org/pdf/2007.05558[7] Sam Altman on X: "there is no wall", https://x.com/sama/status/1856941766915641580 [8] Surfing the Singularity blog, https://surfthesing.blogspot.com/2024/12/surfing-singularity-coming-wave-book.html [9] "The Coming Wave", M. Suleyman, Crown Pub., 2023[10] https://www.theneurondaily.com/p/openais-leaked-agi-roadmap[11] 12 Days of OpenAI, Day 12: https://www.youtube.com/watch?v=SKBG1sqdyIU [12] "AI Snake Oil", Narayanan &amp; Kapoor, Princeton U. Press, 2024[13] https://www.salesforce.com/news/stories/einstein-sales-agents-announcement[14] https://www.servicenow.com/standard/resource-center/data-sheet/ds-virtual-agent.html[15] https://miro.medium.com/v2/0*-8c-MXmNvcvTLdHH.png We recommend the following video for those not familiar with this architecture:  https://youtu.be/KJtZARuO3JY?si=Muq2xRdSTaa9LMXb[16] https://ia.samaltman.com/[17] Yann LeCun on Lex Fridman podcast, https://www.youtube.com/watch?v=5t1vTLU7s40[18] https://ifttt.com/chatgpt]]></summary></entry></feed>