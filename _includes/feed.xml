<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en-US"><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="https://hpc.social/personal-blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://hpc.social/personal-blog/" rel="alternate" type="text/html" hreflang="en-US" /><updated>2026-01-11T20:59:24-07:00</updated><id>https://hpc.social/personal-blog/feed.xml</id><title type="html">hpc.social - Aggregated Personal Blog</title><subtitle>Shared personal experiences and stories</subtitle><author><name>hpc.social</name><email>info@hpc.social</email></author><entry><title type="html">Latency-critical Linux task scheduling for gaming</title><link href="https://hpc.social/personal-blog/2026/latency-critical-linux-task-scheduling-for-gaming/" rel="alternate" type="text/html" title="Latency-critical Linux task scheduling for gaming" /><published>2026-01-10T17:26:29-07:00</published><updated>2026-01-10T17:26:29-07:00</updated><id>https://hpc.social/personal-blog/2026/latency-critical-linux-task-scheduling-for-gaming</id><content type="html" xml:base="https://hpc.social/personal-blog/2026/latency-critical-linux-task-scheduling-for-gaming/"><![CDATA[<p><em><a href="https://lwn.net/Articles/1051430/">LWN</a></em> has an excellent article up on the ‚Äúlatency-criticality aware virtual deadline‚Äù (LAVD) scheduler, from a talk at the <em>Linux Plumbers Conference</em> in December.</p>

<p>In particular, I appreciate the detailed discussion of using different profilers and performance-analysis tools at different levels to determine how to optimize scheduling to improve two key goals: providing high average FPS while keeping 99th-percentile FPS as low as possible, e.g. to prevent UI stuttering. Optimizing for battery usage is also important, as the Steam Deck was one of the main targets for this work.</p>

<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p>The key finding that came out of his analysis is perhaps somewhat obvious: a single high-level action, such as moving a character on-screen and emitting a sound based on a key-press event, requires that many tasks work together. Some of the tasks are threads in the game process, but others are not because they are in the game engine, kernel, and device drivers; there are often 20 or 30 tasks in a chain that all need to collaborate. Finding tasks with a high waker or wakee frequency and prioritizing them is the basis of the LAVD scheduling policy.</p>

</blockquote>

<p>As always with <em>LWN</em> there‚Äôs good coverage not only of the talk itself, but also the Q&amp;A following the session and ideas from the audience on tooling and other improvements.</p>

<p><em><a href="https://www.phoronix.com/news/Meta-SCX-LAVD-Steam-Deck-Server">Phoronix</a></em> also covered a different talk from the same conference (I think) on how Meta is using the LAVD scheduler as the basis for a new default scheduler used on their fleet. </p>

<p>I haven‚Äôt had a chance to watch this talk yet (<a href="https://youtu.be/KFItEHbFEwg?si=62Hsyr9ydHcOVu9b">video</a> linked from the article) but I‚Äôm very interested in the idea that the same concepts might be useful to a hyper scaler as well as a device like a Steam Deck.</p>]]></content><author><name>Thinking Out Loud</name></author><category term="ajdecon" /><summary type="html"><![CDATA[LWN has an excellent article up on the ‚Äúlatency-criticality aware virtual deadline‚Äù (LAVD) scheduler, from a talk at the Linux Plumbers Conference in December.]]></summary></entry><entry><title type="html">Orchestrating Hybrid Quantum‚ÄìClassical Workflows with IBM LSF- Inside the SQD Workflow Demo at SC25</title><link href="https://hpc.social/personal-blog/2026/orchestrating-hybrid-quantum-classical-workflows-with-ibm-lsf-inside-the-sqd-workflow-demo-at-sc25/" rel="alternate" type="text/html" title="Orchestrating Hybrid Quantum‚ÄìClassical Workflows with IBM LSF- Inside the SQD Workflow Demo at SC25" /><published>2026-01-08T14:22:59-07:00</published><updated>2026-01-08T14:22:59-07:00</updated><id>https://hpc.social/personal-blog/2026/orchestrating-hybrid-quantum-classical-workflows-with-ibm-lsf-inside-the-sqd-workflow-demo-at-sc25</id><content type="html" xml:base="https://hpc.social/personal-blog/2026/orchestrating-hybrid-quantum-classical-workflows-with-ibm-lsf-inside-the-sqd-workflow-demo-at-sc25/"><![CDATA[<p>As we enter 2026, it seems that SC25 is far off in our rearview mirror. But it&rsquo;s only been a bit over a month since the HPC world converged on St. Louis, Missouri for the annual <a href="https://sc25.supercomputing.org/">Supercomputing 2025</a> (SC25) event. SC25 signaled one emerging trend: the exploration of hybrid workflows combining quantum and classical computing, offering a look at how these technologies can work synergistically over time. This was indeed the main topic of the 1st Annual Workshop on Large-Scale Quantum-Classical Computing, a workshop which I found to be very insightful.</p>

<p>At the IBM booth, we showcased how <a href="https://www.ibm.com/products/hpc-workload-management">IBM LSF</a> can schedule and orchestrate a hybrid quantum‚Äìclassical workflow across IBM Quantum systems and classical x86 compute.  The demo featured the Sample-based Quantum Diagonalization (SQD) workflow, to estimate the ground-state energy of a Hamiltonian representing a molecular system. SQD is part of the <a href="https://quantum.cloud.ibm.com/docs/en/guides/qiskit-addons-sqd">IBM Qiskit add-ons</a>.</p>

<p>Before diving into the details on what was demonstrated at SC25, and how LSF was used to manage the workflow, I would like to acknowledge that this work was supported by the Hartree Center for Digital Innovation, a collaboration between UKRI-STFC and IBM. The demonstration was created in close collaboration with Vadim Elisseev and Ritesh Krishna from IBM Research, alongside G√°bor Samu and Michael Spriggs from IBM. Additionally, this post does not aim to provide an in-depth look at SQD itself. Rather the focus is on how LSF can manage hybrid quantum-classical workflows across a heterogeneous environment comprised of both quantum and classical resources.</p>

<p><strong>Hybrid workflows are not new</strong></p>

<p>For three decades, we have seen the use of accelerators in HPC to drive performance‚Äîfrom GPUs to FPGAs and other specialized architectures. Effective scheduling of tasks in these heterogeneous environments has always been a key consideration for efficiency, scalability‚Äîand to maximize the ROI in commercial HPC environments. As resource topologies grow more complex, scheduling must account for characteristics such as connectivity, latency, and dependency constraints across increasingly diverse infrastructures. Quantum Processors (QPUs) are now making their appearance as complementary resources within HPC workflows, aim at challenges such as specific optimization problems, many-body physics and quantum chemistry.</p>

<p><strong>Demo details</strong></p>

<p>The IBM LSF cluster was deployed on IBM Cloud using the LSF Deployable Architecture, which rapidly deploys and configures a ready-to-use HPC environment. IBM Research provided integration components for LSF in the form of esub and jobstarter scripts. These scripts enable LSF to query the cloud-based IBM Quantum Platform to determine which QPUs are available for a given user account and meet the qubit requirements specified at job submission. The list of eligible QPUs is then sorted by queue length, and the system with the shortest queue is selected as the target for the quantum circuit. These integration scripts (esub and jobstarter) are intended to be made open source at a later time.</p>

<p>The LSF environment was deployed on IBM Cloud using the <a href="https://cloud.ibm.com/catalog/architecture/deploy-arch-ibm-hpc-lsf-1444e20a-af22-40d1-af98-c880918849cb-global">LSF Deployable Architecture</a> v3.1.0:</p>

<ul>
<li>LSF 10.1.0.15</li>
<li>RHEL 8.10</li>
<li>IBM Cloud profile bx2-16x64 (compute hosts)</li>
</ul>
<p>The IBM Qiskit package versions used:</p>

<ul>
<li>qiskit v2.2.1</li>
<li>qiskit-addon-sqd v0.12.0</li>
<li>qiskit-ibm-runtime v0.43.0</li>
</ul>
<p>The SQD Python program is available as part of the IBM Qiskit Add-ons (see details here). For this demonstration, the original monolithic SQD script was refactored into four smaller Python programs‚Äîeach representing a distinct step in the workflow. These steps map directly to LSF jobs, enabling orchestration of the workflow across the quantum and classical HPC resources as shown in the architecture diagram (Figure 1):</p>

<ul>
<li><strong>Stage 1</strong> map the inputs to a quantum problem.</li>
<li><strong>Stage 2</strong> optimizes the problem for quantum hardware execution‚Äîthis is where the circuit is transpiled and optimized for the target QPU</li>
<li><strong>Stage 3</strong> executes the circuit on the QPU using Qiskit primitives</li>
<li><strong>Stage 4</strong> performs post-processing and returns the result in the desired classical format</li>
</ul>
<p><figure><img src="https://www.gaborsamu.com/images/figure1_lsfqc.png" />
</figure>

<em>Figure 1 LSF hybrid quantum-classical workflow demo (Vadim Elisseev, IBM Research)</em></p>

<p>For this demonstration, we used IBM LSF Application Center‚Äîa web-based interface for job submission and management. LSF Application Center supports application templates, which simplify job submission by providing predefined forms. Templates were created for both the SQD workflow and the Jupyter Notebook application, which is used to visualize the workflow results.</p>

<p><strong>Demo execution steps</strong></p>

<ul>
<li>We start by using the SQD template to submit an instance of the SQD workflow (Figure 2) which is used to calculate an approximate ground-energy state of the nitrogen molecule (N2). The submission form is customized to let users specify the script for each step of the workflow and specify the desired number of qubits required on the QPU for the quantum circuit. This parameter is used by LSF to select the appropriate quantum system from the available resources. Note that jobs are submitted to LSF with a done dependency condition, ensuring that each stage runs only after the previous one completes successfully. Stage 2 begins after Stage 1, Stage 3 follows Stage 2, and Stage 4 executes once Stage 3 has finished</li>
</ul>
<p><figure><img src="https://www.gaborsamu.com/images/figure2a_lsfqc.png" />
</figure>

<em>Figure 2 LSF Application Center SQD submission form</em></p>

<ul>
<li>Next, we submit an instance of the Jupyter Notebook to monitor the workflow initiated in Step 1. This notebook is designed for this demonstration to visualize the status of each workflow step, displaying results as they successfully complete. Figure 3 shows the Jupyter submission form.</li>
</ul>
<p><figure><img src="https://www.gaborsamu.com/images/figure3a_lsfqc.png" />
</figure>

<em>Figure 3 LSF Application Center Jupyter Notebook submission form</em></p>

<ul>
<li>The Workload view in the LSF Application Center can be used to monitor the progress of each job within the workflow. Additionally, the Jupyter Notebook instance can be accessed here via the provided hyperlink. Figure 4 shows the workload view in LSF Application Center. This shows a list of jobs in the LSF system.</li>
</ul>
<p><figure><img src="https://www.gaborsamu.com/images/figure4a_lsfqc.png" />
</figure>

<em>Figure 4 LSF Application Center workload view</em></p>

<ul>
<li>As each stage of the SQD workflow completes, the Jupyter Notebook displays the corresponding output in new browser tabs. This includes qubit coupling maps for the QPUs available on the IBM Quantum Platform for the specific account, a diagram of the circuit mapped to the selected QPU, readings from the QPU, and a plot of the estimated ground-state energy of the N2 molecule.</li>
</ul>
<p><figure><img src="https://www.gaborsamu.com/images/figure5_lsfqc.png" />
</figure>

<em>Figure 5 Output from each step of the SQD workflow (Vadim Elisseev, IBM Research)</em></p>

<ul>
<li>Given that demo environment was built using the LSF Deployable Architecture, IBM Cloud Monitoring is automatically configured. It provides a dashboard for the underlying cloud infrastructure, including detailed hardware metrics. In addition, an LSF Dashboard is available through IBM Cloud Monitoring, showing overall cluster metrics such as total jobs, job status, and queue distribution, along with scheduler performance trends over time. IBM Cloud Monitoring infrastructure view and LSF dashboard are shown in Figure 5.</li>
</ul>
<p><figure><img src="https://www.gaborsamu.com/images/figure6_lsfqc.png" />
</figure>

<em>Figure 6 IBM Cloud Monitoring: Infrastructure view, and LSF dashboard</em></p>

<p>A video recording of the end-to-end demonstration can be found <a href="https://community.ibm.com/community/user/viewdocument/demonstration-of-managing-hybrid-qu?CommunityKey=74d589b7-7276-4d70-acf5-0fc26430c6c0&amp;tab=librarydocuments">here</a>.</p>

<p><strong>Conclusions</strong></p>

<p>This demo marked a milestone by demonstrating that IBM Spectrum LSF can seamlessly orchestrate quantum and classical compute resources for a unified workflow. This example demonstrates a practical approach to integrating quantum capabilities into an existing HPC environment running IBM LSF.</p>

<p>This capability lays the foundation for hybrid computing pipelines that integrate emerging quantum hardware into established HPC environments. As organizations adopt these architectures and tools mature, we can expect production-grade workflows tackling complex problems across domains. The future of HPC is not a choice between classical or quantum‚Äîit is their convergence, working together to unlock new computational possibilities.</p>

<p>The topic of scheduling for hybrid quantum-classical environments will be the subject of an upcoming paper &ldquo;On Topological Aspects of Workflows Scheduling on Hybrid Quantum - High Performance Computing Systems&rdquo; by Vadim Elisseev, Ritesh Krishna, Vasileios Kalantzis, M. Emre Sahin and G√°bor Samu.</p>]]></content><author><name>Ramblings of a supercomputing enthusiast.</name></author><category term="gaborsamu" /><summary type="html"><![CDATA[As we enter 2026, it seems that SC25 is far off in our rearview mirror. But it&rsquo;s only been a bit over a month since the HPC world converged on St. Louis, Missouri for the annual Supercomputing 2025 (SC25) event. SC25 signaled one emerging trend: the exploration of hybrid workflows combining quantum and classical computing, offering a look at how these technologies can work synergistically over time. This was indeed the main topic of the 1st Annual Workshop on Large-Scale Quantum-Classical Computing, a workshop which I found to be very insightful.]]></summary></entry><entry><title type="html">‚ÄúMy Cousin Vinny‚Äù as an LLM benchmark</title><link href="https://hpc.social/personal-blog/2026/my-cousin-vinny-as-an-llm-benchmark/" rel="alternate" type="text/html" title="‚ÄúMy Cousin Vinny‚Äù as an LLM benchmark" /><published>2026-01-05T04:04:31-07:00</published><updated>2026-01-05T04:04:31-07:00</updated><id>https://hpc.social/personal-blog/2026/-my-cousin-vinny-as-an-llm-benchmark</id><content type="html" xml:base="https://hpc.social/personal-blog/2026/my-cousin-vinny-as-an-llm-benchmark/"><![CDATA[<p>Mike Caulfield wrote a <a href="https://mikecaulfield.substack.com/p/notes-towards-a-narrative-llm-benchmark">very thorough and quite entertaining article</a> about posing the following question to ChatGPT:</p>

<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p>What were Marisa Tomei‚Äôs most famous quotes from My Cousin Vinny and what was the context?</p>

</blockquote>

<p>Depending on the model selected, the answers to this varied from hilariously wrong, to plausible-but-flawed, to accurate. </p>

<p>Interestingly, substantial test-time compute (‚Äúthinking time‚Äù) seems to be necessary to do a good job here, despite the easy availability online of famous quotes, plot summaries, and even the script. While the fast-response models available for free were prone to hallucinate. </p>

<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p>At the same time I was struck just how&nbsp;<em>much</em> reasoning time needed to be expended to get this task right. It‚Äôs possible that&nbsp;<em>My Cousin Vinny</em>&nbsp;is uniquely hard to parse, but I don‚Äôt think that is the case. I‚Äôve tried this with a half dozen other films and the pattern seems to hold. If it‚Äôs true that a significant amount of similar film contextualization tasks are solvable with test-time compute but require extensive compute to get it right, it seems to me this could be the basis of a number of useful benchmarks.</p>

</blockquote>

<p>The <a href="https://mikecaulfield.substack.com/p/notes-towards-a-narrative-llm-benchmark">full article</a> is well-worth reading, and not only because it discusses <em>My Cousin Vinny</em> in substantial detail (great movie).</p>]]></content><author><name>Thinking Out Loud</name></author><category term="ajdecon" /><summary type="html"><![CDATA[Mike Caulfield wrote a very thorough and quite entertaining article about posing the following question to ChatGPT:]]></summary></entry><entry><title type="html">Podcasts and blogs I‚Äôm following in early 2026</title><link href="https://hpc.social/personal-blog/2026/podcasts-and-blogs-i-m-following-in-early-2026/" rel="alternate" type="text/html" title="Podcasts and blogs I‚Äôm following in early 2026" /><published>2026-01-02T19:13:35-07:00</published><updated>2026-01-02T19:13:35-07:00</updated><id>https://hpc.social/personal-blog/2026/podcasts-and-blogs-i-m-following-in-early-2026</id><content type="html" xml:base="https://hpc.social/personal-blog/2026/podcasts-and-blogs-i-m-following-in-early-2026/"><![CDATA[<p>As part of the new year, I&#8217;m going through my feed readers for podcasts and blogs. This is mostly a cleanup exercise to remove sources that I regularly skip, but I&#8217;m also adding in a few feeds for sites that I find myself regularly clicking on in social media. As part of this, I figured I&#8217;d share the sources that made the cut to stick around.</p>

<p><span id="more-377"></span></p>

<p>You&#8217;ll notice that there are a lot of podcasts in this post! With two golden doodles in the family, I spend a lot of time on dog walks, not to mention doing chores around the house. Because of this, it&#8217;s often a lot easier for me to listen to content than read it, and indeed I often find myself feeding long-form text articles into <a href="https://elevenreader.io/">ElevenReader</a> so that I can listen to those items too.</p>

<p><strong>Nerd notes:</strong></p>

<ul class="wp-block-list">
<li>I self-host <a href="https://freshrss.org/index.html">FreshRSS</a> to aggregate written blog feeds and read them using <a href="https://reederapp.com/">Reeder</a>. FreshRSS is hosted on a private VPS that I access via <a href="https://tailscale.com/">Tailscale</a> on my various devices, because I&#8217;m really the only person that needs to access it.</li>



<li>I listen to podcasts via <a href="https://overcast.fm/">Overcast</a>, which I prefer for its audio features to the default Apple Podcasts app.</li>



<li>This is not 100% complete as there are some blogs I follow purely through the Patreon site, and I haven‚Äôt (yet) taken the time to go through that and add them to this list.</li>



<li>There are a few NSFW items left out intentionally, as my voice on this blog is at least semi-professional <img alt="üòâ" class="wp-smiley" src="https://s.w.org/images/core/emoji/17.0.2/72x72/1f609.png" style="height: 1em;" /> </li>
</ul>

<h2 class="wp-block-heading">Computing-related</h2>

<p><strong>Podcasts:</strong></p>

<ul class="wp-block-list">
<li><em><a href="https://oxide-and-friends.transistor.fm/">Oxide and Friends</a></em> is a weekly live show recorded by Bryan Cantrill, Adam Leventhal, and friends from the <a href="https://oxide.computer">Oxide Computer Company</a>. Despite being a &#8220;corporate&#8221; podcast, it generally has the vibe of &#8220;Car Talk for Computers&#8221; and can often dig into really interesting computer systems topics, and even computer and industry history. Some of the episodes get into specifics of the Oxide product, which may be interesting or skip-able depending on your interests.</li>



<li><em><a href="https://www.fafo.fm/">Fork Around and Find Out</a></em> is a resurrection of the <em>Ship It!</em> podcast that used to be part of the Changelog network, focused on production systems, on-call, and large-scale engineering. The updates are a bit irregular but Justin and Autumn are enjoyable hosts to listen to.</li>



<li><em><a href="https://www.thisisfinepod.com/">This is Fine!</a></em> is a podcast on resilience engineering from Colette Alexander and Clint Byrum. It sometimes picks up topics from discussions in the <a href="https://resilienceinsoftware.org/">Resilience in Software Foundation Slack</a> instance and is almost always a fun listen.</li>



<li><em><a href="https://randsinrepose.com/the-important-thing/">The Important Thing</a> </em>is an occasional discussion podcast between <a href="https://randsinrepose.com/">Michael Lopp</a> and <a href="https://troxell.com/">Lyle Troxell</a>. Often very random, with widely varying episode lengths, it&#8217;s nicely chatty and a great listen during dog walks.</li>



<li><em><a href="https://signalsandthreads.com/">Signals and Threads</a></em>, from Jane Street, is another occasional podcast but often gets very deep into interesting software topics such as performance analysis, state machine replication, and memory management &#8212; with a focus on low-latency trading systems that have interesting constraints. </li>



<li><a href="https://changelog.com/podcast"><em>The Changelog</em></a> is a long-standing general software news podcast. I dip in and out of this one based on the topic being covered, and often like their &#8220;Changelog and Friends&#8221; chatty episodes more than the news or interview episodes.</li>



<li><em><a href="https://comparchpodcast.podbean.com/">The Compute Architecture Podcast</a></em> updates very infrequently but often has interesting, hardware- or systems-focused interviews.</li>
</ul>

<p><strong>Personal blogs:</strong></p>

<ul class="wp-block-list">
<li><a href="https://charity.wtf/">Charity Majors</a> is a long-time follow of mine for her technical work on observability, her practical SRE/sysadmin mentality, and her useful perspectives on engineering management.</li>



<li><a href="https://www.brendangregg.com/blog/">Brendan Gregg</a> posts infrequently, but will publish fascinating deep dives on performance engineering that are always worth reading.</li>



<li><a href="https://utcc.utoronto.ca/~cks/space/blog/">Chris Siebenmann</a> is a sysadmin at the University of Toronto and a prolific blogger about nuts-and-bolts Linux admin topics. He publishes a <em>ton</em> so I dip in and out of his feed, but always keep it in my feed reader.</li>



<li><a href="https://www.drcathicks.com/blog">Cat Hicks</a> does psychological research on software teams and I always learn a ton from her writing.</li>



<li><a href="https://soatok.blog/b/">Soatok</a> writes excellent, interesting, and opinionated articles on security and cryptography topics.</li>



<li><a href="https://ferd.ca/">Fred Hebert</a> is an SRE with a strong interest in resilience engineering who frequently discusses interesting academic research on resilience and human factors.</li>



<li><a href="https://blog.glennklockwood.com/">Glenn Lockwood</a> is an HPC engineer who I&#8217;ve known online for a long time, and who came into the field from materials science in a similar manner to me. He&#8217;s worked at SDSC, NERSC, Microsoft, and VAST, and his annual recap of the SuperComputing conference is worth reading every year. (So is the rest of his blog!)</li>



<li><a href="https://www.seangoedecke.com/">Sean Goedecke</a> is a software engineer at Github who writes interesting work on AI and on the dynamics of large companies.</li>



<li><a href="https://simonwillison.net/">Simon Willison</a> is one of the most essential bloggers on AI and LLMs today, not to mention incredibly prolific. His style of writing short posts on whatever he&#8217;s thinking about is one I hope to emulate more often here!</li>



<li>Rachel Kroll, aka <a href="https://rachelbythebay.com/w/">Rachel By the Bay</a>, is a long-time sysadmin/SRE who writes on detailed sysadmin and software engineering topics in an often-ironic fashion.</li>



<li><a href="https://xeiaso.net/blog/">Xe Iaso</a> is a software engineer and author of the <a href="https://anubis.techaro.lol/">Anubis</a> Web AI Firewall tool. Xer blog covers a wide variety of software, systems, and AI work.<br /></li>
</ul>

<p><strong>Technical blogs and industry news:</strong></p>

<ul class="wp-block-list">
<li><em><a href="https://lwn.net/">LWN</a></em> is the definitive source for Linux and free software news, and is supported by the community via subscriptions. You should subscribe!</li>



<li><em><a href="https://chipsandcheese.com/">Chips and Cheese</a></em> does really interesting deep dives into chip architecture and performance, often focused on newer products but occasionally digging into older hardware.</li>



<li><a href="https://semianalysis.com/"><em>SemiAnalysis</em></a> is at this point one of the most essential news sources for the semiconducting industry, and one of the few paid sources I follow.</li>



<li><em><a href="https://jepsen.io/blog">Jepsen</a></em> performs detailed analyses of distributed systems reliability and consistency by <a href="https://aphyr.com/">Kyle Kingsbury</a>, both as consulting engagements and for the community. Read all of these, they&#8217;re excellent!</li>



<li><em><a href="https://semiengineering.com/">Semiconductor Engineering</a></em> is one of the long-standing industry news sites. I don&#8217;t read a ton of this but I do keep an eye on the feed for interesting headlines.</li>



<li>Similarly, <em><a href="https://www.datacenterdynamics.com/en/">Data Center Dynamics</a></em> is one of the standard industry news sites for data centers.</li>
</ul>

<h2 class="wp-block-heading">News and Politics</h2>

<p><strong>Podcasts:</strong></p>

<ul class="wp-block-list">
<li><em><a href="https://www.lawfaremedia.org/podcasts-multimedia/podcast">The Lawfare Podcast</a></em> covers a really wide variety of national security law topics. My only regular listen is their <em><a href="https://www.lawfaremedia.org/podcasts-multimedia/podcast/rational-security">Rational Security</a></em> episodes which provide a weekly roundup of relevant news in an informal discussion format, but I dip in and out of the others.</li>



<li><em><a href="https://www.bloomberg.com/podcasts/series/money-stuff">Money Stuff</a></em> is a fun weekly podcast from Matt Levine and Katie Greifeld of Bloomberg News, who discuss weekly financial news from a very nerdy perspective. I&#8217;m not generally a huge finance person, but I like that this podcast allows me to listen in to people geeking out about the topic.</li>



<li><em><a href="https://www.economist.com/the-world-in-brief">The World in Brief</a></em> from the Economist is their daily quick summary of the news. I have very mixed feelings about the Economist in general &#8212; as with many British sources, they platform far too much transphobia &#8212; but I have yet to find a better substitute for &#8220;quick morning summary of the news&#8221;. At least, nothing else that doesn&#8217;t make me want to throw my phone at a wall.</li>
</ul>

<p><strong>Blogs and News:</strong></p>

<ul class="wp-block-list">
<li><em><a href="https://restofworld.org/">Rest of World</a></em> covers tech industry news with a focus on impacts outside the West, and often has really interesting coverage from a different angle.</li>



<li><em><a href="https://www.liberalcurrents.com/">Liberal Currents</a></em> is a political blog focused on liberalism, both in current events and as a political philosophy, and has published a lot of excellent pieces since I started reading it in 2025.</li>
</ul>

<h2 class="wp-block-heading">Miscellaneous</h2>

<p><strong>Podcasts:</strong></p>

<ul class="wp-block-list">
<li><a href="https://www.armscontrolwonk.com/archive/author/podcast/"><em>Arms Control Wonk</em></a> continues to be a good listen, though it&#8217;s updated something sporadically the past few years. The coverage of nuclear weapons, missiles and other delivery systems, and current events around arms control (or lack thereof) is very good. If you sponsor them via Patreon, their Slack instance is also a fascinating discussion forum, though I only dip in and out of it occasionally.</li>



<li><em><a href="https://culturestudypod.substack.com/">The Culture Study Podcast</a></em> by Anne Helen Petersen features conversations between Anne and a guest and focuses on listener Q&amp;A. It often covers culture topics that I otherwise don&#8217;t get much of through other feeds. For example, recent episodes have talked about anything from birding to K-pop to the anatomy of cultural panics. I don&#8217;t listen to every episode, but they&#8217;re often quite fun.</li>



<li><em><a href="https://www.liberalcurrents.com/neonliberalism/">Neon Liberalism</a></em> is a regular podcast from <a href="https://www.liberalcurrents.com/">Liberal Currents</a>. I might put this in the News category except that it often digs into political topics from a historical or theoretical perspective rather than just focusing on current events.</li>



<li>Similarly, <em><a href="https://open.spotify.com/show/29wW6zsYyYuelcFJcyHOmv">Reimagining Liberty</a></em> from Aaron Ross Powell digs into political theory and current events from the perspective of Powell&#8217;s particular strain of libertarian-ism, which is much more in conversation with modern liberalism and anarchism vs more right-wing strains.</li>
</ul>

<p><strong>Personal blogs:</strong></p>

<ul class="wp-block-list">
<li><a href="https://www.funraniumlabs.com/">Phil Broughton</a> is a health physicist at UC Berkeley who has worked in classified nuclear work at LLNL as well as spending a year in Alaska, and has a wealth of fascinating and hilarious stories.</li>



<li><a href="https://www.goodreads.com/author/show/16094.Lois_McMaster_Bujold/blog">Lois McMaster Bujold</a> is one of my favorite science fiction and fantasy authors. While she&#8217;s semi-retired, she still writes occasional novellas following Penric, a sorcerer in her World of the Five Gods, which I really love. Her blog helpfully announces new stories!</li>



<li><a href="https://whatever.scalzi.com/">John Scalzi</a> is another favorite author, and also has an excellent blog called <em>Whatever</em>.</li>



<li>Bret Devereaux writes <em><a href="https://acoup.blog/">A Collection of Unmitigated Pedantry</a></em> about history, the military, and pop culture. If you&#8217;re interested in an extensive deep dive into the military missteps made Saruman in <em>The Two Towers</em>, this is the blog for you!</li>



<li><em><a href="https://www.bitsaboutmoney.com/">Bits About Money</a></em> is Patrick McKenzie&#8217;s blog about finance, and each entry tends to be a highly-nerdy deep dive about how some interesting corner of the financial system works.</li>
</ul>

<p><strong>Webcomics:</strong></p>

<ul class="wp-block-list">
<li><em><a href="https://www.giantitp.com/comics/oots.html">The Order of the Stick</a></em> is a long-running D&amp;D stick-figure comic that I have been reading for longer than I can really say. I highly recommend it, though I&#8217;ll warn you that with an archive of &gt;1,300 comics (and growing!) you are likely to lose a lot of time this way.</li>



<li><em><a href="https://questionablecontent.net/">Questionable Content</a></em> is a slice-of-life comic about a coffee shop&#8230; with robots, super-intelligent AIs, stupid dick jokes, and more. Also has a long archive to dig through.</li>



<li><em><a href="https://www.girlgeniusonline.com/">Girl Genius</a></em> is a long-running online comic book about &#8220;Adventure, Romance, and MAD SCIENCE!&#8221; and thoroughly excellent.</li>
</ul>]]></content><author><name>Thinking Out Loud</name></author><category term="ajdecon" /><summary type="html"><![CDATA[As part of the new year, I&#8217;m going through my feed readers for podcasts and blogs. This is mostly a cleanup exercise to remove sources that I regularly skip, but I&#8217;m also adding in a few feeds for sites that I find myself regularly clicking on in social media. As part of this, I figured I&#8217;d share the sources that made the cut to stick around.]]></summary></entry><entry><title type="html">On Friday deploys</title><link href="https://hpc.social/personal-blog/2025/on-friday-deploys/" rel="alternate" type="text/html" title="On Friday deploys" /><published>2025-12-30T20:58:00-07:00</published><updated>2025-12-30T20:58:00-07:00</updated><id>https://hpc.social/personal-blog/2025/on-friday-deploys</id><content type="html" xml:base="https://hpc.social/personal-blog/2025/on-friday-deploys/"><![CDATA[<p><a href="https://charity.wtf/2025/12/24/on-friday-deploys-sometimes-that-puppy-needs-murdering-xpost/">This post</a> from Charity Majors on Friday deploys is well worth reading. </p>

<p>In the past I‚Äôve seen her comment on how deployments should be carried out fearlessly regardless of when, and I‚Äôve often felt like saying ‚Äúyeah, well, ‚Ä¶‚Äù. Because of course I agree with that as a goal, but many real-world orgs and conditions make it challenging.</p>

<p>This most recent post talks about the situations when those freezes <em>can</em> make sense, even if they‚Äôre not ideal. And in particular I like the discussion about what really needs to be frozen is not deploys, but merges:</p>

<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p>To a developer, ideally, the act of merging their changes back to main and those changes being deployed to production should feel like one singular atomic action, the faster the better, the less variance the better. You merge, it goes right out. You don‚Äôt want it to go out, you better not merge.</p>




<p>The worst of both worlds is when you let devs keep merging diffs, checking items off their todo lists, closing out tasks, for days or weeks. All these changes build up like a snowdrift over a pile of grenades. You aren‚Äôt going to find the grenades til you plow into the snowdrift on January 5th, and then you‚Äôll find them with your face. Congrats!</p>

</blockquote>]]></content><author><name>Thinking Out Loud</name></author><category term="ajdecon" /><summary type="html"><![CDATA[This post from Charity Majors on Friday deploys is well worth reading.]]></summary></entry><entry><title type="html">Why generic software design advice is often useless</title><link href="https://hpc.social/personal-blog/2025/why-generic-software-design-advice-is-often-useless/" rel="alternate" type="text/html" title="Why generic software design advice is often useless" /><published>2025-12-30T03:33:15-07:00</published><updated>2025-12-30T03:33:15-07:00</updated><id>https://hpc.social/personal-blog/2025/why-generic-software-design-advice-is-often-useless</id><content type="html" xml:base="https://hpc.social/personal-blog/2025/why-generic-software-design-advice-is-often-useless/"><![CDATA[<p>In <em><a href="https://www.seangoedecke.com/you-cant-design-software-you-dont-work-on/">You can&#8217;t design software you don&#8217;t work on</a>, </em>Sean Goedecke discusses why generic advice on the design of software systems is often unhelpful.</p>

<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p><strong>When you‚Äôre doing real work, concrete factors dominate generic factors</strong>. Having a clear understanding of what the code looks like right now is far, far more important than having a good grasp on general design patterns or principles.</p>

</blockquote>

<p>This tracks with my experience not just of software systems, but also systems with a hardware component (eg ML training clusters) or a facility component (eg datacenters). The specifics of your system absolutely dominate any general design guidance.</p>

<p>As the manager of a team that publishes reference architectures, I do think that it‚Äôs helpful to clearly understand where your specific design differs from generic advice. If you‚Äôre going off the beaten path, you should k<em>now </em>you‚Äôre doing that! And be able to plan for any additional validation involved in doing that.</p>

<p>But relatedly, this is part of why I think that any generic advice should be based on some actually existing system. If you are telling someone they should follow a given principle, you should be able to point to an implementation that <em>does</em> follow that principle. </p>

<p>Or else you‚Äôre just speculating into the void. Which admittedly can be <em>fun</em> but is not nearly as valuable as speaking from experience.</p>]]></content><author><name>Thinking Out Loud</name></author><category term="ajdecon" /><summary type="html"><![CDATA[In You can&#8217;t design software you don&#8217;t work on, Sean Goedecke discusses why generic advice on the design of software systems is often unhelpful.]]></summary></entry><entry><title type="html">Large software systems</title><link href="https://hpc.social/personal-blog/2025/large-software-systems/" rel="alternate" type="text/html" title="Large software systems" /><published>2025-12-28T19:50:27-07:00</published><updated>2025-12-28T19:50:27-07:00</updated><id>https://hpc.social/personal-blog/2025/large-software-systems</id><content type="html" xml:base="https://hpc.social/personal-blog/2025/large-software-systems/"><![CDATA[<p>In <em><a href="https://seangoedecke.com/nobody-knows-how-software-products-work/">Nobody understands how large software products work</a></em>, Sean Goedecke makes a number of good points about how difficult it is to really grasp large software systems.</p>

<p>In particular, some features impact every part of the system in unforeseen ways:</p>

<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p>Why are these features complicated? Because&nbsp;<strong>they affect every single other feature you build</strong>. If you add organizations and policy controls, you must build a policy control for every new feature you add. If you localize your product, you must include translations for every new feature. And so on. Eventually you‚Äôre in a position where you‚Äôre trying to figure out whether a self-hosted enterprise customer in the EU is entitled to access a particular feature, and&nbsp;<em>nobody knows</em>&nbsp;&#8211; you have to go and read through the code or do some experimenting to figure it out.</p>

</blockquote>

<p>Sean also points out that eventually the code itself has to be the source of truth, and debugging requires deep investigation of the continually-changing system.</p>

<p>I‚Äôve seen this happen in a bunch of different orgs, and it does seem to be true, especially for products with a large number of collaborating teams. I would add that in addition to the code itself, you often need to have conversations with the relevant teams to discern intent and history. Documentation only goes so far, eventually you need talk to people.</p>]]></content><author><name>Thinking Out Loud</name></author><category term="ajdecon" /><summary type="html"><![CDATA[In Nobody understands how large software products work, Sean Goedecke makes a number of good points about how difficult it is to really grasp large software systems.]]></summary></entry><entry><title type="html">SC‚Äô25 recap</title><link href="https://hpc.social/personal-blog/2025/sc-25-recap/" rel="alternate" type="text/html" title="SC‚Äô25 recap" /><published>2025-12-01T14:34:00-07:00</published><updated>2025-12-01T14:34:00-07:00</updated><id>https://hpc.social/personal-blog/2025/sc-25-recap</id><content type="html" xml:base="https://hpc.social/personal-blog/2025/sc-25-recap/"><![CDATA[<p>
  The annual SC conference was held last week, drawing over
  <a href="https://www.hpcwire.com/2025/11/19/sc25-observations-more-pumps-than-processors/">16,000 registrants and 560 exhibitors</a>
  to in St. Louis, Missouri to talk about high-performance computing, artificial
  intelligence, infrastructure, and science. It was my tenth time attending
  in-person (12th overall), and as is always the case, it was a great week to
  reconnect with colleagues, hear what people are worrying about, and get a
  finger on the pulse of the now-rapidly changing HPC industry.
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">Outside the SC'25 convention center on the only clear day of the week.</figcaption>
</figure>
</div>
<p>Although every SC I've attended always felt a little different from the
  previous year, this one felt quite different. Part of that results from my own
  personal circumstances: this is the first year I attended as an employee of
  VAST Data, and so the people with whom I met and the technical problems to
  which I paid attention were certainly biased towards those most relevant to my
  work. But the backdrop of the whole conference has also shifted. It's been
  three SC conferences since ChatGPT came out, and it's now undeniable that AI
  isn't simply on the horizon; it's shaping the field of HPC and scientific
  computing. What used to be an argument of "<a href="https://blog.glennklockwood.com/2024/05/isc24-recap.html#section11">us vs. them</a>" is now more like "them (and us?)"<span></span></p>
<p></p>
<p>
  As has become tradition, I'm sharing some of my thoughts from the week with
  the world in the hopes that someone finds this interesting and insightful.
  I've roughly organized them into two areas big themes and the exhibition hall.
</p>
<ul style="text-align: left;">
<li>
<a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#big-themes">Big themes</a>
<ul>
<li>
<a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#theme-1-the-big-number-is-losing-its-shine">Theme 1: The big number is losing its shine</a>
<ul>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#top500">Top500</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#the-gordon-bell-prize">The Gordon Bell Prize</a></li>
<li>
<a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#fixing-problems-caused-by-the-big-number">Fixing problems caused by the big number</a>
</li>
</ul>
</li>
<li>
<a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#theme-2-hpc-policy-is-becoming-ai-policy">Theme 2: HPC policy is becoming AI policy</a>
</li>
<li>
<a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#theme-3-ai-discourse-is-growing-up">Theme 3: AI discourse is growing up</a>
<ul>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#agentic-workflows">Agentic workflows</a></li>
<li>
<a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#data-and-agentcentric-service-infrastructure">Data and agent-centric service infrastructure</a>
</li>
</ul>
</li>
</ul>
</li>
<li>
<a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#the-exhibit-hall">The exhibit hall</a>
<ul>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#by-the-numbers">By the numbers</a></li>
<li>
<a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#interesting-new-technology">Interesting new technology</a>
<ul>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#dell-ir700">Dell IR700</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#hpe-cray-gx5000">HPE Cray GX5000</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="big-themes">Big themes</h2>
<p>
  HPC has always been at the center of a tension between keeping things the same
  (supercomputers are the most stable the day they are turned off) and pushing
  the technological envelope (which is the fastest way to unlock new discovery).
  The desire to push the envelope has always been a "pull" towards the future;
  researchers first led with kooky ideas (like DAOS and Kokkos), and as those
  ideas turn from research into production, they make new technologies (like
  all-flash and AMD GPUs) accessible to scientists.
</p>
<p>
  What hasn't historically happened, though, is a strong "push" towards the
  future. Scientific HPC centers push themselves to justify building the next
  big supercomputer, but it's been a given that there will always be another big
  machine, so this push has been internal and gentle. Combined with the
  not-so-urgent pull of HPC researchers, every center has gotten a new machine
  every five years or so.
</p>
<p>
  This is the year where it became clear to me that AI is now exerting a strong
  push on the HPC industry--a shove even, forcing HPC centers around the world
  to align themselves on an AI mission if they want to survive. All the
  big-money HPC systems being announced this year are clearly being positioned
  as AI-first and AI-motivated, and these announcements are going well beyond
  simply peppering "AI" throughout the press release and otherwise acting as if
  it was business-as-usual. This is the first SC where I saw scientists,
  architects, and decision-makers being being forced to confront real tradeoffs
  favor either HPC or AI, and they are beginning to choose AI.
</p>
<p>
  This push-and-pull on HPC towards the future manifested in three big themes.
</p>
<h3 id="theme-1-the-big-number-is-losing-its-shine">
  Theme 1: The big number is losing its shine
</h3>
<p>
  HPC has long organized itself around treating the big machine and the big
  number as its top priority, and this is why the two largest HPC conferences of
  the year honor the semiannual release of the Top500 list on their main stage.
  However, this year felt like the first time that one number (that somehow
  reflects "performance") dominated the conversation. Instead, the discourse was
  more diffuse and discussed "performance and x" or "the supercomputer and x."
</p>
<h4 id="top500">Top500</h4>
<p>
  The place where this was most evident to me was at the
  <a href="https://sc25.conference-program.com/presentation/?id=bof117&amp;sess=sess409">Top500 BOF</a>, where the latest list was unveiled.
</p>
<p>
  The biggest announcement was that Europe now has its first benchmark-confirmed
  exascale system in JUPITER, which ran a full-system HPL at
  <a href="https://mastodon.social/@andih/115566907716591104">1,000,184 TFLOPS</a>
  for two hours and seven minutes. However, JUPITER didn't get any stage time at
  the BOF since, like Aurora, it actually debuted on a previous list with a
  sub-exascale run. This run pushed it over the finish exascale finish line, but
  if the Top500 list metadata is to be believed, the run used 100% of JUPITER's
  5,884 nodes to break the barrier--a feat that is unlikely to be reproduced on
  any production applications, since it is rare to have zero failed nodes in any
  large-scale production environment.
</p>
<p>
  So, while there was little fanfare for Europe in breaking the exaflops barrier
  with its new big machine and big number, there were some big
  announcements--one overt, and others more muted.
</p>
<p>
  The biggest news was that <strong>the Top500 list is changing hands</strong>.
  Whereas it has historically been controlled by three people--Jack Dongarra,
  Horst Simon, and Erich Strohmaier--it will be transitioning to be
  community-controlled under the stewardship of ACM SIGHPC. Dongarra, Simon, and
  Strohmaier will still be on the steering committee under the ACM stewardship,
  but this new governance structure opens the doors for new ideas to breathe new
  life into the way systems are ranked and, more broadly, how "performance" is
  meant to be interpreted from Rmax.
</p>
<p>
  At present, the list (and related lists) are bound by rules that, in the
  present day of reduced-precision accelerators, make little sense. For example,
  using the Ozaki scheme within the LU decomposition is not allowed by Top500
  despite the fact that it can produce the same answer with the same numerical
  accuracy much faster than hardware FP64. And while the HPL-MxP benchmark does
  allow solving the same problem using more creative methods, Strohmaier
  highlighted a problem there too: it never dictated how to deal with multiple
  levels of mixed precision until AIST broke the rankings. AIST ran HPL-MxP at
  both 16-bit and 8-bit precisions, resulting in their ABCI 3.0 system
  simultaneously ranking at #6 and #10.
</p>
<p>
  These sorts of issues make it easy to question the value of leaderboards like
  Top500 or HPL-MxP, as their definition of "performance" becomes increasingly
  further divorced from how large supercomputers are really used. The past few
  years have shown that there hasn't been the time or energy to get ahead of
  these ambiguities amongst the three men maintaining the list, so transitioning
  it to ACM will hopefully be a positive move that will give the list a chance
  to be revitalized.
</p>
<p>
  To their credit,
  <strong>the incipient stagnation of the Top500 list</strong> was called out by
  Strohmaier during his analysis of the list, acknowledging that "growth has
  tremendously slowed down compared to what it used to be" and "we don't have
  proof of what is actually the reason for that:"
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">All the key highlights of this SC's Top500 list.</figcaption>
</figure>
</div>
<p>China has stopped submitting, the AI and hyperscale providers really never
  started submitting, and retired systems are being thrown off the list long
  before they fall off the bottom. To me, this was a tacit acknowledgment that
  the list does not have a bright future out to 2030 unless it is modernized to
  be relevant to the way in which today's largest systems are actually being
  used--which is not DGEMM.
</p>
<p>
  The final surprising acknowledgment during Strohmaier's talk was that
  <strong>the list is trailing the state of the art in hardware</strong> by
  quite a bit. He pointed out that Blackwell systems are only now starting to
  appear even though they've been shipping in volume for the better part of a
  year. While he hypothesized that there is "uneasiness" about Blackwell in an
  HPC context, the reality is that there are no Blackwells for HPC until the
  Blackwell orders for hyperscale AI have been fulfilled. HPC is second in line,
  and even then, the only Blackwells I could find on this year's Top500 list
  were NVL8 configurations--not the NVL72 configurations that have been filling
  up hyperscale datacenters like
  <a href="https://glennklockwood.com/garden/systems/Fairwater">Fairwater</a>.
</p>
<p>
  Strohmaier pointed out that Blackwell, by virtue of its HBM3e (vs. Hopper's
  HBM3), is showing up higher on the HPCG list (which is a memory bandwidth
  test) than on Top500 (which is an FP64 FLOPS test). He phrased this as
  evidence that "not everything is bad for the HPC community," but I would have
  phrased my conclusion a little differently:
</p>
<ol type="1">
<li>
    Blackwell is actually great for HPC, because most real workloads are
    memory-bandwidth bound, not FLOPS bound. The fact that B200 offers similar
    FP64 FLOPS at higher memory bandwidth means that real applications will get
    higher effective use of those FP64 FLOPS.
  </li>
<li>
    Despite the above, Blackwell doesn't perform well on Top500 because HPL
    doesn't reflect the reality that memory bandwidth is important. It follows
    that HPL doesn't reflect the reality of real HPC applications. A Blackwell
    system can be significantly better for real HPC applications than a
    comparably sized Hopper system even though it may rank lower than Hopper on
    Top500.
  </li>
<li>
    Blackwell isn't showing up in volume now because the HPC community is second
    in line. The HPC community isn't uneasy as much as it is completely locked
    out. The first NVIDIA-based exascale system debuted in November 2025 despite
    its GPU being three years old, suggesting that if big Blackwell systems ever
    appear on Top500, it'll happen in 2026-2027.
  </li>
</ol>
<p>
  All of this is a roundabout way of showing that the big number--in this case,
  the HPL score--no longer leads meaningful conversation around how useful a
  system is for science.
</p>
<h4 id="the-gordon-bell-prize">The Gordon Bell Prize</h4>
<p>
  Another major indicator of the changing tide away from the big number was the
  work that won this year's
  <a href="https://awards.acm.org/bell">Gordon Bell Prize</a>. The winning
  paper, titled "<a href="https://arxiv.org/html/2504.16344v2">Real-time Bayesian inference at extreme scale: A digital twin for tsunami
    early warning applied to the Cascadia subduction zone</a>," wasn't the typical case of running a huge simulation for a few hours and
  reporting some result. Rather, it described a four-step workflow that
  culminates in the desired insight popping out of a computation that runs
  across only 128 nodes and completes in less than 0.2 seconds. Furthermore, the
  hero run part could be decomposed into trivially parallel components, allowing
  the bulk of the computation to be geographically distributed across HPC
  centers or GPUs spread across on-prem and cloud providers.
</p>
<p>
  My understanding of the work is that there was a massive "offline" computation
  to precompute a few key matrices (Phase 1) followed by two shorter offline
  steps that turn those matrices into the core of the digital twin. The last
  step, which was "online" and designed to be computed in real-time, could then
  take this core and solve the input problem with extremely low latency. This
  workflow front-loads a hero run in such a way that, if an earthquake were to
  occur, the risk of tsunami could be calculated in less than a second using
  only modest compute resources and the precomputed core.
</p>
<p>
  The authors eschewed methods that generated tons of FLOPS in favor of methods
  that were less FLOPS-efficient but got to the answer faster. In the authors'
  own words:
</p>
<blockquote>
<p>
    As shown in Fig.¬†<a href="https://arxiv.org/html/2504.16344v2#S7.F7">7</a>, higher FLOP/s does not necessarily lead to faster time-to-solution. On
    MI300A nodes of¬†<em>El Capitan</em>, the best-performing
    implementation, Fused PA, achieves a lower percentage (5.2%) of theoretical
    peak FLOP/s than Fused MF (5.5%) but is faster.
  </p>
</blockquote>
<p>
  Interestingly, the hero computation here was embarrassingly parallel(ish) as
  well; in their demonstration run, the hero run (Phase 1) was broken into 621
  independent calculations each requiring 128 nodes (512 A100 GPUs) for about an
  hour. Because they are independent, these tasks could be parallelized across
  multiple HPC centers as well, and my understanding of the data volumes
  involved are modest; Phase 1 would require a single shared copy of the input
  mesh (a hundred GiB?) per HPC center, and each of the 621 tasks would output
  around 8 GiB which would have to be copied back.
</p>
<p>
  While I don't understand the mathematics behind this work, the paper took what
  would've been a huge exascale-class mathematical problem ("10 years on a
  sustained 1 EFLOP/s machine") and reformulated it into a workflow that solves
  the problem faster and more usefully. Instead of brute-forcing the problem
  with a big supercomputer, they split it into separate offline and online
  parts, and this naturally allowed the most computationally expensive part to
  be geographically distributable.
</p>
<p>
  This work surrendered the need for a single big machine, and it didn't produce
  a big-number result. But it did win the Gordon Bell Prize, again signaling
  that the HPC community is beginning to look beyond performance-only and think
  about awarding innovation according to outcomes, not just FLOPS.
</p>
<p>
  The talk for this paper can be viewed
  <a href="https://sc25.conference-program.com/presentation/?id=gb106&amp;sess=sess577">here in the SC25 Digital Experience</a>.
</p>
<h4 id="fixing-problems-caused-by-the-big-number">
  Fixing problems caused by the big number
</h4>
<p>
  Most of my perception around the HPC community beginning to de-emphasize the
  singular big machine or big number arose from organic interactions I had with
  colleagues and customers though. It's hard to summarize how these
  conversations went, but the
  <a href="https://sc25.conference-program.com/presentation/?id=bof197&amp;sess=sess439">Lustre Community BoF</a>
  is a good example of what I saw elsewhere.
</p>
<p>
  Lustre has long been the gold standard in high-performance parallel I/O in the
  HPC community because it was designed from day one to deliver high bandwidth
  above all else. As a result, Lustre already has the big number solved in many
  ways, and events like the Lustre BOF are a great case study in what it looks
  like for a performance-first technology to be pushed into adapting to deliver
  more than just a big number.
</p>
<p>
  First, the ever-innovative St√©phane Thiell from Stanford discussed the process
  and tooling he developed to enable online capacity expansion of a Lustre file
  system. The basis for it was a distributed, fault-tolerant tool he developed
  that uses redis, lfs find, and lfs migrate to manage the state of file
  migrations across Lustre servers as the file system is rebalanced. While a
  part of me thought this was a great tool that would be super helpful for many
  others, another part of me was kind of horrified.
</p>
<p>
  Maybe I've been spoiled by working in hyperscale and AI these past three
  years, but online capacity expansion and rebalancing is a built-in capability
  of all distributed storage systems these days. All the major cloud object
  stores do this, as do all modern parallel file systems including Quobyte,
  VAST, and WEKA. Of course, none of these modern systems are as efficient (on a
  per-CPU core or per-SSD basis) as Lustre at delivering peak performance. But
  St√©phane's talk made me realize the price that's paid for this great
  performance.
</p>
<p>
  Andreas Dilger and others went on to talk about Lustre futures, and as they
  were speaking, I noticed that nobody was talking about performance
  improvements to Lustre. Rather, feature development was focused on catching up
  in every other dimension--data governance, reliability, manageability, and
  others. For example, Andreas talked a bit about the upcoming "multi-tenancy"
  features coming to Lustre:
</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">It's a lot of work to retrofit multitenancy into a performance-first file system.</figcaption>
</figure>
</div>

<p>I put ‚Äúmulti-tenancy‚Äù in quotes because these changes really represent
trying to back into a security posture that is fundamentally different from the
one that Lustre was designed around. In the pursuit of performance, Lustre (as
with most other HPC technologies) was designed assuming that security was
someone else‚Äôs problem. By the time someone could log into a system that could
mount a Lustre file system, they had already been authenticated, and it was up
to the OS on each compute node to authorize any interactions with Lustre itself.
This is the ‚Äúimplicit trust‚Äù model.</p>
<p></p>
<p>
  The problem, of course, is that the rest of the world has adopted a "zero
  trust" model which makes many things (except performance!) generally easier.
  Compliance is easier when the system assumes that everything is encrypted as a
  default and key management can be delegated to a third party. Because Lustre
  didn't do this from the outset, it is going through this process of
  retrofitting encryption in various places and using a mixture of nodemaps,
  UID/GID maps, and shared secrets to patch over all the places where trust was
  fundamentally implicit.
</p>
<p>
  Later on in the BOF, panelists acknowledged (some half-heartedly) that
  manageability of Lustre was a barrier. One panelist admitted that it took five
  years of work to almost get to the point where a Lustre update can be done
  without crashing applications. Another panelist said that multitenancy in
  Lustre is easy <em>if you follow a million steps</em>, and that his company
  was developing script-based ways to simplify this. While the idea of using
  scripts to simplify operations is not bad, from a secure supply chain
  standpoint, relying on third-party bash scripts to enable features required
  for legal compliance is horrifying.
</p>
<p>
  I don't mean to pick on Lustre alone here; other HPC technologies such as
  InfiniBand, Slurm, and DAOS are facing the same reality: retrofitting modern
  requirements like security and manageability into architectures that
  prioritized performance and scalability over everything else are now going
  through similar contortions to meet modern requirements around data
  governance. For those HPC centers who do not have to worry about compliance
  (which is most of open-science computing), these technologies will continue to
  be just fine.
</p>
<p>
  However, the
  <a href="https://blocksandfiles.com/2025/11/18/vast-data-dell-versity-and-spectra-logic-are-shining-storage-stars-on-taccs-horizon/">successes of these modern file systems</a>
<a href="https://blocksandfiles.com/2025/07/04/vast-doudna-supercomputer-storage/">across leading HPC centers</a>
  and the proliferation of alternative technologies such as
  <a href="https://nrp.ai">Kubernetes-based HPC</a> and
  <a href="https://blogs.microsoft.com/blog/2025/11/12/infinite-scale-the-architecture-behind-the-azure-ai-superfactory/?utm_source=chatgpt.com">MRC over Ethernet</a>
  tells me that HPC coming around to the idea that marginal increases in
  performance are no longer worth missing out on factors that weigh heavily on
  day-to-day operations like manageability, reliability, and flexibility.
</p>
<h3 id="theme-2-hpc-policy-is-becoming-ai-policy">
  Theme 2: HPC policy is becoming AI policy
</h3>
<p>
  Some of the biggest news at SC was not actually showcased at the conference
  despite being what many people wanted to talk about in side conversations: HPC
  policy is rapidly becoming AI policy, resulting in a slew of huge (but poorly
  defined) "public-private partnerships."
</p>
<p>
  As a bit of background, the Oak Ridge Leadership Computing facility announced
  its next system, Discovery, in late October--this was the result of a
  "typical" supercomputer procurement process that
  <a href="https://www.nextplatform.com/2023/10/02/the-first-peeks-at-the-doe-post-exascale-supercomputers/">first came into the public eye in 2023</a>. However, the Discovery announcement also included mention of a smaller
  system, Lux, which will "<a href="https://www.olcf.ornl.gov/2025/10/27/ornl-amd-and-hpe-to-deliver-does-newest-ai-supercomputers-discovery-and-lux/">leverage the Oracle Cloud Infrastructure (OCI)</a>" (whatever that means) to provide earlier access to AMD MI355X GPUs ahead of
  Discovery's full-scale deployment.
</p>
<p>
  Then, two days later, Argonne National Laboratory announced a
  <a href="https://www.energy.gov/articles/energy-department-announces-new-partnership-nvidia-and-oracle-build-largest-doe-ai">similar arrangement with Oracle Cloud and NVIDIA</a>
  to deliver a small (Lux-sized) GPU supercomputer named Equinox, followed by a
  much-larger 100,000-GPU supercomputer named Solstice. Neither Equinox nor
  Solstice are attached to a "typical" supercomputer procurement; the follow-on
  to Aurora, to be named
  <a href="https://intro-hpc-bootcamp.alcf.anl.gov/sites/hpc/files/2025-09/WelcomeToHPC_Papka.pdf">Helios</a>, is
  <a href="https://www.alcf.anl.gov/draft-technical-requirements-alcf-4-system">still in planning</a>
  and will be deployed in 2028. This strongly suggests that, whatever
  "public-private partnership" means to the DOE, it is not the same as the
  typical leadership computing systems; it is its own AI-centric program.
</p>
<p>
  At SC itself, Evangelos Floros (EuroHPC's head of infrastructure) also
  mentioned the "need for public-private partnerships" to realize EuroHPC's goal
  of building "AI Gigafactories" with "100,000 advanced AI processors" across
  Europe.
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">"Need for public-private partnerships" to fund AI factories is recognized by EuroHPC too.</figcaption>
</figure>
</div>
<p>Again, what exactly this "public-private partnership" model entails in Europe
  was never really defined.
</p>
<p>
  What was clear is that both American and European efforts are declaring the
  need to build massive (100K+ GPU) supercomputers for AI, the traditional HPC
  centers will be the public stewards of them, and "public-private partnerships"
  are the only way to realize them since governments alone cannot foot the bill.
</p>
<p>
  The Top500 BOF also included a short, awkward talk by Rick Stevens titled "The
  DOE AI Initiatives" that amounted to Stevens saying he had nothing to say.
  What really happened, I suspect, is that DOE's new "<a href="https://genesis.energy.gov">Genesis Mission</a>," which was announced the week <em>after</em> the SC conference, was a week
  late and therefore couldn't be discussed as originally planned. If Stevens had
  been able to describe the Genesis Mission, though, I'm sure he would've also
  described "public-private partnership" as a key aspect, since the same
  language is used in the
  <a href="https://www.whitehouse.gov/presidential-actions/2025/11/launching-the-genesis-mission/">Executive Order that established Genesis</a>. And I'm sure his description would've been no clearer about what this
  really means than what EuroHPC or the OCI/DOE descriptions have stated.
</p>
<p>
  Most revealing was my observation that, even outside of the proper conference
  program, nobody really knew what any of this meant. I talked to plenty of my
  colleagues from both government HPC and hyperscale cloud organizations, and
  the only consistent message was that there aren't many concrete facts backing
  up the the press releases right now. It appears that these partnerships were
  brokered far outside the usual channels that large supercomputer procurements
  are normally done, and the people in charge of actually delivering on the
  promises of the press releases are still figuring out what is possible.
</p>
<p>
  Connecting the dots between Lux/Equinox/Solstice, Genesis, and a recent
  <a href="https://www.energy.gov/sites/default/files/2025-04/RFI%20to%20Inform%20Public%20Bids%20to%20Construct%20AI%20Infrastructure%20%28website%20copy%29.pdf">RFI</a>
  and
  <a href="https://sam.gov/workspace/contract/opp/7864e8f4d61f42dc811ba095a41c8368/view">RFP</a>
  from DOE to allow
  <a href="https://www.energy.gov/articles/doe-announces-site-selection-ai-data-center-and-energy-infrastructure-development-federal">hyperscalers to build AI factories on federal land</a>, it appears that what is happening is...
</p>
<ul>
<li>
    The DOE has a bunch of land that is adjacent to the National Labs that is
    undeveloped but has the infrastructure to support massive AI factories.
    Specifically named is a 110-acre parcel at Argonne that can accommodate up
    to 1 GW "AI data park," and a 100-acre parcel at Oak Ridge with up to 800
    MW. These details were disclosed in
    <a href="https://www.energy.gov/sites/default/files/2025-04/RFI%20to%20Inform%20Public%20Bids%20to%20Construct%20AI%20Infrastructure%20%28website%20copy%29.pdf">an RFI they issued earlier in the spring</a>.
  </li>
<li>
    The
    <a href="https://www.energy.gov/articles/energy-department-announces-new-partnership-nvidia-and-oracle-build-largest-doe-ai">Solstice press release</a>
    specifically said that DOE envisions "shared investments and shared
    computing power between government and industry." Given the RFI/RFP were
    about land leases, these public-private partnerships may involve splitting
    the costs of space/power/cooling (the land and infrastructure being leased)
    and the capital/operations (the supercomputer cloud services being built)
    between the Labs and Oracle.
  </li>
</ul>
<p>
  A potential model for operations is that cloud providers are allowed to build
  and operate commercial AI cloud services adjacent to the DOE HPC facilities in
  exchange for the DOE Genesis Mission being entitled to some of those AI cloud
  capabilities. Exactly how much supercomputing resources hyperscalers like OCI
  would give to DOE, and exactly how much it would cost the DOE Labs to serve as
  landlords, is probably still undefined. But seeing as how power is the single
  biggest limiter in AI these days, I expect this model will only spread costs
  around, not actually lower them.
</p>
<p>
  If this is indeed how Genesis plays out, this would establish a bizarre new
  way for the government to acquire HPC (or AI) capabilities that completely
  sidesteps the standard procurement model. Instead of plunking down a hundred
  million dollars a year to finance a new leadership supercomputer, we might be
  moving into a world where the Labs plunk down a hundred million dollars a year
  to cover the costs of power, space, and cooling for a cloud provider. And
  instead of owning a leadership supercomputer, these national HPC facilities
  wind up consuming HPC (well, AI) resources from cloud providers--hopefully at
  a cost that reflects the fact that the cloud providers are also profiting from
  cycles being sold off of these machines to commercial AI customers.
</p>
<p>
  But again, this is all speculation based on the consistencies I heard
  throughout the conference and the experience I had trying to build these sorts
  of partnership with the HPC community while I worked at Microsoft. I may be
  right, or I may be wildly wrong. There are probably only a handful of people
  in the world with a clear idea of what these partnerships are meant to look
  like right now, and they are all way above the heads of the people at the HPC
  centers who will be tasked with executing on the vision.
</p>
<p>
  Selfishly, I am also left with a bit of heartburn over all of this news. I put
  a lot of personal time and energy into giving the HPC community the
  information it needed to feel comfortable about partnering with hyperscale AI
  infrastructure providers while I was at Microsoft, and it often felt like a
  Sisyphean task. Within months of me giving up and moving on from my career at
  a cloud provider, seeing a complete reversal of policy from the leadership HPC
  folks--and to see the "other guy" in pole position--is a bit of a slap in the
  face.
</p>
<p>
  I also couldn't help but notice that the cloud provider in all the headlines
  in the US didn't seem to demonstrate a very strong and unified presence at SC
  this year. Comically, they didn't even use their own brand's colors for their
  booth on the exhibit floor. And the color scheme they did use left no room for
  Oak Ridge's Lux system, which will be AMD-based, to be showcased.
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">Oracle's booth at SC25. Their brand color is red, not green. Or so I thought.</figcaption>
</figure>
</div>
<p>Though I may have read too much into this, it feels like these public-private
  partnerships are not necessarily composed of equal partners with equal levels
  of commitment.
</p>
<p>
  More broadly, I left the conference concerned that the discourse happening
  around these cloud-HPC/AI integrations--at least in the US--appears to have
  regressed compared to where it was when I worked at Microsoft. Many of the
  things we had to figure out years ago (cybersecurity models, impacts on jobs
  at the HPC centers) seem to have reset to zero. And sidestepping the
  procurement processes for leadership computing to enable these public-private
  partnerships will either require significant new funding (of which Genesis
  provides none; the executive order as-written appears to recolor existing
  money) or robbing Peter (the budget funding the next generation of leadership
  HPCs) to pay Paul (the cloud providers serving up compute resources for AI).
  As a result, I can envision a future where all of the money that used to fund
  leadership computing for science becomes money to fund commercial AI
  factories, resulting in a slow evaporation of the LCFs as their HPC
  capabilities shrink in size and relevance.
</p>
<p>
  Though there's lots more to be said on this topic, it's all based on
  conjecture. So, maybe the best thing to do is quietly wait and see.
</p>
<h3 id="theme-3-ai-discourse-is-growing-up">
  Theme 3: AI discourse is growing up
</h3>
<p>
  This was the first SC where it felt like the discourse around AI's role in the
  future of scientific computing actually carried some substance. Whereas
  previous years saw talk that mostly revolved around basic ideas like "do LLMs
  hallucinate too much?" or "can ChatGPT write MPI code?," I sat in on a number
  of interesting talks and conversations that skipped the question of "is AI
  useful?" and went straight to "this is how AI is proving useful to us."
</p>
<p>
  Maybe it's related to the previous theme: HPC money is becoming AI money, so
  AI research is becoming required to stay afloat. Or maybe it's because 2025
  has been the year of agentic AI, and agents allow LLMs to be integrated much
  more surgically into complex workflows. Or maybe confirmation bias led me to
  sit in sessions and talk with people who are at the frontier of applying AI to
  scientific discovery. Whatever the case may be, I was glad to hear so much
  discussion from researchers around the importance of all the connective tissue
  required to operationalize AI in scientific computing.
</p>
<h4 id="agentic-workflows">Agentic workflows</h4>
<p>
  A great example of this was the
  <a href="https://aiexscale.github.io">1st International Symposium on Artificial Intelligence and Extreme-Scale
    Workflows</a>, which happened on Friday. One of the invited speakers, Dr. Katrin Heitmann,
  connected a lot of dots in my head with a talk she gave on how massive-scale,
  physics-based simulation workflows can benefit from agentic AI.
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">Heitmann's vision on how agentic approaches can augment (but not replace) humans in complex scientific workflows.</figcaption>
</figure>
</div>
<p>The crux of the challenge faced by most massive-scale simulation (like
  <a href="https://cpac.hep.anl.gov/projects/hacc/">HACC</a>, the cosmology code
  for which she is famous) is that they generate massive amounts of data. The
  <a href="https://www.anl.gov/cels/article/simulating-the-cosmos-frontiere-sets-new-record-with-trillionparticle-universe-model">most recent HACC run</a>
  generated hundreds of terabytes of compressed data per checkpoint and over a
  hundred petabytes of data in the end; this cosmological simulation serves as a
  reference dataset from which downstream cosmological research can draw when
  exploring targeted questions. The challenge, of course, is finding relevant
  pieces of the simulated universe from amidst a hundred petabytes of raw data.
</p>
<p>
  Dr. Heitmann's premise is that agents and tools have very specific scopes and
  capabilities, and researchers have control over which of these tools they wish
  to use. However, they can hand off these tools to an agentic workflow to let
  it autonomously sift through all of the data, looking for specific features
  within the simulated universe that are relevant. A specific example she gave
  was the process of examining 500 million galaxy clusters; with an agentic,
  AI-driven approach, a postdoc was able to interactively sift through these
  objects without examining each one individually. For truly interesting
  objects, a separate agent could go search the literature and provide an
  explanation as to why it may be interesting, absolving the postdoc from having
  to make round trips between the dataset and external literature.
</p>
<p>
  That all said, it was clear from this talk (and others) that integrating
  agentic AI into scientific inquiry is still in its early days. But what I
  appreciated about this talk (and the entire workshop) is that it sidestepped
  pedestrian questions about trustworthiness by acknowledging that the goal
  isn't full autonomy, but rather, enabling researchers to do things faster.
  There is still a human at the start and the end of the workflow just as there
  always has been, but agents can reduce the number of times a human must be in
  the loop.
</p>
<h4 id="data-and-agentcentric-service-infrastructure">
  Data and agent-centric service infrastructure
</h4>
<p>
  Even when AI wasn't the main topic of discussion, it was clear to me at this
  SC that AI is influencing the way researchers are thinking about the
  infrastructure surrounding supercomputers. A great example of this was the
  keynote at the <a href="https://www.pdsw.org/index.shtml">PDSW workshop</a>,
  given by the ever-insightful
  <a href="https://sc25.conference-program.com/presentation/?id=misc185&amp;sess=sess202">Dr. Rob Ross, where he offered a retrospective on the work his team has
    done over the last two decades</a>, what he felt they got right, what they missed, and what's ahead.
</p>
<p>
  Towards the end of his presentation, he made the case that "science is
  increasingly multi-modal." But rather than talk about multimodality in the AI
  sense, he was emphasizing that there's more to scientific computing than
  performance:
</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">Domain science, provenance, search, and resilience are equal partners to performance in scientific computing.</figcaption>
</figure>
</div>

<p>Taken at face value, this slide positions performance on equal footing
with domain science, provenance, findability, and his argument was that we‚Äôve
moved beyond the world where the only storage problem that HPC faces is
checkpointing. Just as Dr. Heitmann would say on Friday, Dr. Ross‚Äô argument was
that the increasing volume of scientific data coming out of both exascale
simulation and scientific instruments is driving the field towards more
automation. And with automation comes a greater need to understand data
provenance‚Äìafter all, if automation produces a surprising result, a human
ultimately has to go back and understand exactly how the automation generated
that result.</p>
<p></p>
<p>
  He also point out that in this coming world of automation-by-necessity,
  infrastructure itself might have to be rethought. After all, traditional
  technologies like parallel file systems were designed to make the lives of
  human researchers easier; when the primary consumer of data becomes AI agents,
  not humans, there may be better ways to organize and expose data than through
  files and directories. A human might repeatedly cd and ls to find a specific
  dataset on a file system, whereas an agent use a query a flat index to find
  the same data in a single step.
</p>
<p>
  At the end of the same PDSW workshop, I was fortunate enough to contribute to
  <a href="https://sc25.conference-program.com/presentation/?id=miscp112&amp;sess=sess202">a panel</a>
  where many of these same themes--how will data systems change as AI plays a
  greater role in scientific discovery--were discussed. Although we touched on a
  lot of topics, what stuck with me was a general acknowledgment that, while HPC
  has always talked about data management and provenance as being important,
  they were always treated as a "nice to have" rather than a "must have."
  However, as was echoed across many presentations (including the two I
  described above), governance and provenance are now becoming non-negotiable as
  larger datasets drive us towards AI-driven automation.
</p>
<p>
  Regardless of what you think about AI's ability to accelerate scientific
  discovery, I left SC with the feeling that AI is forcing the HPC community to
  grow up with regards to how seriously it takes data management:
</p>
<ul>
<li>
    The size and velocity of datasets generated by simulation or experiment is
    growing beyond any single person's ability to analyze it by hand. The
    complexity of these data are also making it harder to develop
    herustics-based or analytical approaches to combing through all of it.
  </li>
<li>
    The best path forward to understanding these data is through AI (via
    purpose-built models for analysis) or AI-driven data exploration (via
    autonomous, agentic workflows).
  </li>
<li>
    Automation or autonomous workflows will always act under authority delegated
    to them by human researchers, meaning there is a growing need to be able to
    inspect how these workflows arrived at the conclusions they generate.
  </li>
<li>
    Understanding how an answer was achieved requires significantly better data
    management features such as governance, provenance, and auditability. A
    result is ultimately only useful if a human can trust it, and that trust
    comes from understanding which data informed that conclusion, how that data
    was created, and how it was modified over time.
  </li>
</ul>
<p>
  Put differently, checkpointing was the main concern of I/O research because
  I/O performance was the first scalability issue that scientific computing ran
  into as supercomputers and scientific instruments got bigger. However, we're
  now at a point where issues ancillary to performance have reached the limits
  of scalability. Dr. Ross's multi-modal slide indicate that provenance,
  indices/search, and resilience are some examples of these new barriers, but
  there are plenty more as well.
</p>
<p>
  In a sense, this theme is the opposite side of the same coin as the first
  theme I discussed--that the big number is losing its shine. The hardest
  questions going forward aren't the obvious ones about scaling performance;
  they are about scaling everything else to keep up. AI seems to be the
  technology that has cleared a path to these data management hurdles, but the
  benefits of adopting strong data management practices and systems will extend
  far beyond the reach of just enabling AI-based automation.
</p>
<h2 id="the-exhibit-hall">The exhibit hall</h2>
<p>
  The exhibit hall has long been one of my favorite parts of attending SC
  because it's a great way to get a feeling for what technologies and vendors
  are hot, where the innovation is trending, and what sorts of commercial
  problems are worth solving. Every year I feel like I have less and less time
  to walk the exhibit hall though, and the layout and composition of this year's
  exhibition meant I only saw a small fraction of what I wanted to see in the
  few days it was up.
</p>
<p>
  The most common comment I heard about the exhibit this year is captured in
  Doug Eadline's article,
  <a href="https://www.hpcwire.com/2025/11/26/sc25-observations-more-pumps-than-processors/">SC25 Observations: More Pumps than Processors</a>
  (which is well worth the read!). The same commentary was repeated throughout
  the OCP conference in October as well, suggesting that there is a lot of money
  to be made (or at least the prospect of money) in helping datacenters get
  outfitted for the liquid cooling demanded by the next generation of
  large-scale GPU infrastructure. However, I found the overwhelming amount of
  space devoted to liquid cooling companies acutely problematic at SC25 this
  year for two reasons:
</p>
<ol type="1">
<li>
<strong>Most SC attendees have nothing to do with liquid cooling</strong>. A
    colleague of mine who operates supercomputers for the energy sector asked
    one of these big liquid cooling vendors what he could do to actually engage
    with them. After all, he doesn't buy liquid cooling infrastructure; he buys
    whole supercomputers that come with heat exchangers and CDUs that are
    integrated into the solution. The vendor had no good answer, because the
    reality is that the typical supercomputer user or buyer has no say over what
    piping, coolant, or exchangers are used inside the machine itself. The whole
    point of buying an integrated supercomputer is to not have to deal with that
    level of details.
  </li>
<li>
<strong>These liquid cooling vendors soaked up a ton of floor space</strong>. A few of these physical infrastructure providers had massive (50x50)
    booths sprinkled across the exhibit hall. Combined with the fact that the
    average SC attendee has nothing to do with liquid cooling meant that the
    booths that were more likely to be relevant to a typical attendee were much
    further apart than they had to be.
  </li>
</ol>
<p>
  The end result was that the exhibit hall was absolutely gargantuan and yet
  information-sparse. In fact, this year saw a secondary exhibit hall in the old
  football stadium serve as overflow space, because the entire primary exhibit
  hall was full. What's worse is that this overflow space was (as best as I
  could tell) completely disconnected from the main hall, and the only time I
  ever saw it was from the dining area used to serve lunch for the tutorials.
</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">The exhibit hall's overflow space being set up in the former football stadium.</figcaption>
</figure>
</div>

<p>I would‚Äôve been furious if if I had been stuck with a booth in this
overflow space, because I can‚Äôt imagine the foot traffic in there was very high.
I personally couldn‚Äôt even find the entrance to this second exhibition area in
the few hours I had to look for it.</p>
<p></p>
<p>
  I can't help but think the SC organizers leaned far too much into booking up
  as much space (and therefore exhibitor dollars) as possible without thinking
  about the dilutive effects of having such a massive vendor count. Some vendors
  definitely benefitted from having a good location near one of the hall
  entrances, but I also heard a nontrivial amount of grumbling around how little
  traffic there was at some of the big booths. It wouldn't surprise me if there
  was a contraction of the HPC mainstays at SC26.
</p>
<h3 id="by-the-numbers">By the numbers</h3>
<p>
  Rather than rely solely on anecdotes though, it's also fun to take a
  quantitative look at the changes in exhibitors relative to last year. Since I
  spent the time figuring out how to generate tree maps for my SC24 recap last
  year, I figured I should re-run the same analysis to compare SC25 to SC24.
</p>
<p>
  Of the biggest booths who were exhibiting for the first time this year, it
  should be no surprise that the two biggest new entrants were Danfoss (liquid
  cooling infrastructure) and Mitsubishi Heavy Industries (gas turbines and
  other large-scale infrastructure):
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>
 <figcaption class="image-caption">New exhibitors with the largest booths.</figcaption></figure>
</div>
<p>Of the other top new exhibitors, some (Solidigm, Sandisk, C-DAC, MinIO, and
  University of Missouri Quantum Innovation Center) were quite relevant to the
  typical SC attendee. Arm was also back after having skipped SC24. But there
  were scores of new exhibitors whose services and products seem much more
  relevant to very niche aspects of physical datacenter infrastructure.
</p>
<p>
  Of the exhibitors who didn't show up to SC25 but had big booths at SC24, there
  was a diverse mix of markets:
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">Vendors who didn't show up to SC'25 but had big booths at SC'24.</figcaption>
</figure>
</div>
<p>Sadly, higher ed and government popped up on this list (see
  <a href="https://www.hpcwire.com/2025/11/26/sc25-observations-more-pumps-than-processors/">Doug Eadline's take on this for more</a>). A bunch of datacenter infrastructure providers also vanished, including
  Valvoline and Boundary Electric; this suggests that some of the top new
  vendors of this year (Danfoss, Mitsubishi) may similarly vanish entirely next
  year after realizing that SC isn't really their crowd. But I was also
  surprised to see some big names in AI vanish; Iris Energy (IREN) is a GPU
  cloud provider that just inked a multi-billion dollar deal with Microsoft;
  Ingrasys manufactures much of the world's GB200 NVL72 infrastructure; Groq,
  Sambanova, and SambaNova also inexplicably vanished.
</p>
<p>
  Perhaps more interesting are the top growers; these vendors exhibited both
  last year and this year, but went significantly larger on their booth sizes:
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">Biggest increases in booth size at SC'25 vs. SC'24.</figcaption>
</figure>
</div>
<p>Legrand, which provides datacenter infrastructure bits, likely grew as a
  result of it acquiring USystems and merging USystems' booth with Legrand's
  booth this year. The other big booth expansions are mostly household names
  though; Gates, EBARA, and GRC are cooling vendors that the typical SC attendee
  can't do much with, but the others are organizations with whom a researcher or
  HPC datacenter operator might actually talk to.
</p>
<p>
  Finally, the top contractions in booth space are a mix of service providers,
  HPC facilities or research centers, and component suppliers:
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">Biggest decreases in booth size at SC'25 vs. SC'24.</figcaption>
</figure>
</div>
<p>Of the biggest vendors who downsized, Carahsoft is a component reseller and
  service provider, Stulz is a liquid cooling company, HLRS is a German
  supercomputer center, and Viridien is an HPC services company that primarily
  serves the energy sector. It is surprising to see AWS shrink while Microsoft
  grew, and it is doubly surprising to see Oracle shrink when it's at the center
  of the biggest HPC deployment news of the season. Given that these booth sizes
  are chosen a year in advance, this may speak to how unexpected the turn of
  events were that resulted in Oracle carrying the cloud services end of DOE's
  big public-private partnerships.
</p>
<h3 id="interesting-new-technology">Interesting new technology</h3>
<p>
  For reasons I'll discuss later, I didn't have much time to walk the exhibit
  hall floor. Combined with the fact that everything was so spread out and
  diffuse, I just didn't get a great sense of what interesting new technology
  was being introduced this year beyond what tended to stick out. And amidst all
  the giant CDUs and liquid cooling infrastructure, it was hard for anything to
  stick out except really big compute cabinets.
</p>
<h4 id="dell-ir700">Dell IR700</h4>
<p>
  Dell's booth had a fully loaded IR7000 rack on display (as they did at
  <a href="https://blog.glennklockwood.com/2025/03/gtc-2025-recap.html#dells-480-kw-ir7000">GTC earlier in the year</a>) with 36 GB200 NVL4 sleds. At 50OU high (almost eight feet tall), this thing
  is physically huge:
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">Dell's 50OU IR7000 rack, fully loaded. This is what TACC Horizon will be built from.</figcaption>
</figure>
</div>
<p>Unlike the version they had on display at GTC though, this one had both the
  front door and a full rear-door heat exchanger installed:
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">HUGE rear-door heat exchanger on the back of the Dell IR7000 rack.</figcaption>
</figure>
</div>
<p>What's notable about this platform is that we now know that it is the basis
  for both
  <a href="https://tacc.utexas.edu/systems/horizon/">TACC's upcoming Horizon system</a>
  (which will have
  <a href="https://glennklockwood.com/garden/systems/Horizon">28 of these fully loaded racks</a>) and
  <a href="https://www.nersc.gov/what-we-do/computing-for-science/doudna-system">NERSC's upcoming Doudna system</a>
  (which will have Vera Rubin rather than Blackwell). This rack was nominally
  designed for hyperscale AI and is the basis for Dell's GB200 NVL72 (XE9712)
  deployments at places like CoreWeave and xAI, which means that it'll be
  thoroughly tested at scale long before TACC or NERSC have it up and running.
  This is the opposite of what has historically happened: before AI, it was
  usually government HPC that had to debug new rack-scale architectures before
  industry would touch it.
</p>
<h4 id="hpe-cray-gx5000">HPE Cray GX5000</h4>
<p>
  However, government HPC will still have a chance to debug a new supercomputing
  platform in the recently announced
  <a href="https://glennklockwood.com/garden/Cray-GX">Cray GX</a> (formally
  called "the HPE Cray Supercomputing GX platform"), which is the successor to
  the current Cray EX platform. This is the platform that the
  <a href="https://glennklockwood.com/garden/systems/Discovery">Discovery supercomputer</a>
  at OLCF will use, and HPE had a CPU-only blade (<a href="https://glennklockwood.com/garden/nodes/Cray-GX250">Cray GX250</a>) and a rack mockup on display at SC:
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">HPE's new GX blade form factor. This one appears to be the GX250, the 8-socket CPU-only blade.</figcaption>
</figure>
</div>
<p>It's hard to tell the size of this blade from the photo, but if you look at
  the relative size of the CPU socket and the DIMM slots, you can get a sense of
  how physically massive it is--it's like a coffee table. It also isn't
  perfectly rectangular; Cray decided to put this unusual protrusion on the
  front of the blades which is where the four NICs and eight E1.S SSDs are
  housed:
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">A look at the side of the Cray GX blade's "nose" showing the side-mounted NIC ports.</figcaption>
</figure>
</div>
<p>This nose(?) adds more surface area to the front of the rack, and it makes
  more sense when you see a rack full of these nodes. HPE had a full GX5000 rack
  with mocked-up cardboard nodes in their booth as well:
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">Fully loaded GX5000 rack. The nodes were cardboard, but pretty nice cardboard.</figcaption>
</figure>
</div>
<p>By having the NIC ports (which are Slingshot 400) face the sides of the rack
  rather than stick out the front, the bend radius of all that copper doesn't
  have to be quite as dramatic to route it along the sides of these node noses.
  And unlike previous Cray designs, there's also no midplane or backplane that
  connect the nodes in a rack to the rack-local switches; everything connects
  through discrete copper or optical cables.
</p>
<p>
  At the center of the rack is a liquid-cooled switch chassis, and each rack can
  support either 8-, 16-, or 32-switch configurations. Each switch is a 64-port
  Slingshot 400 switch, and I think the premise is that a single GX5000 rack is
  always exactly one dragonfly group. If you want a smaller group, you use a
  switch chassis with fewer switches.
</p>
<p>
  Interestingly, this GX will also support non-Slingshot Ethernet and XDR
  InfiniBand switches. Given that both XDR InfiniBand and 800G Ethernet are
  shipping today and have twice the bandwidth that Slingshot 400 will have when
  it starts shipping in a year, perhaps the Slingshot 400 option is just a
  stopgap until HPE's investments in Ultra Ethernet result in a product. The
  lack of a network backplane in the rack also makes it easier for the rack to
  accommodate the non-dragonfly topologies that would be required for InfiniBand
  or Ethernet.
</p>
<p>
  The rear of the rack is remarkably unremarkable in that it simply contains a
  rear bus bar and the liquid cooling manifolds and mates. In this sense, the
  rack looks very
  <a href="https://www.opencompute.org/documents/open-rack-base-specification-version-3-pdf">OCP-like</a>; the boring stuff is in the back, everything exciting is serviced from the
  front, and the rack itself is passive plumbing. Like any OCP ORv3 rack, power
  shelves slot in just as server blades do, and they use the same liquid cooling
  infrastructure as the rest of the rack. They power the bus bar, and the blades
  and switches draw from the same bus bar.
</p>
<p>
  Compared to an ORv3 rack though, these GX racks are wider and shorter. The
  width probably offers more flexibility for future NVIDIA or AMD GPU boards,
  but I was surprised that Cray didn't go ultra tall like Dell's 50OU IR7000. I
  was also surprised to hear that Cray is launching GX with a 400 kW cabinet
  design; power appears to already be a limiting factor in the nodes launching
  with GX. A single 400 kW GX rack can support
</p>
<ul>
<li>40 CPU-only blades (81,920 cores of Venice)</li>
<li>28 AMD Venice+MI430X blades (112 GPUs)</li>
<li>24 NVIDIA Vera+Rubin blades (192 GPUs)</li>
</ul>
<p>
  For reference, the demo GX5000 rack pictured above had only 29 blades and 16
  switches. I assume that fitting 40 blades into the rack requires using the
  smallest dragonfly group possible.
</p>
<p>
  On the cooling front, the GX5000 rack will launch with support for the same
  1.6 MW CDUs as the current Cray EX platform. I heard talk of a neat sidecar
  CDU option as well, but the person with whom I spoke at the HPE booth said
  that would come a little later.
</p>
<p>
  Overall, I was surprised by how un-exotic the new Cray GX platform is compared
  to what the AI world has been doing with ORv3 racks. The fact that Cray and
  Dell's designs are more similar than different suggests that the HPC/AI world
  is converging on a place where the future is uncertain, and flexibility is
  more important that highly engineered racks that optimize for very specific
  nodes and networks. It also suggests that the real value of buying Cray is
  higher up the stack; liquid cooling, power delivery, and rack integration is
  becoming commoditized thanks to AI.
</p>
<p>
  I was also surprised that Cray's next-generation design is not obviously
  superior to what the hyperscale community is designing. Whereas the GX rack
  caps out at 400 kW, Dell's will allegedly scale up to 480 kW. That said,
  today's IR7000 racks shipping for Horizon are only 215 kW (for GPU racks) and
  100 kW (for CPU-only racks) according to a talk given by Dan Stanzione:
</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">The physical configuration of TACC's upcoming Horizon supercomputer.</figcaption>
</figure>
</div>
<p>So until the final specifications for the Rubin GPU are released, I suspect we
  won't know whether Cray still leads the pack in terms of compute density, or
  if Dell made the better bet by aligning its supercomputing platform on a
  standard OCP rack design.
</p>]]></content><author><name>Glenn K. Lockwood&apos;s Blog</name></author><category term="glennklockwood" /><summary type="html"><![CDATA[The annual SC conference was held last week, drawing over 16,000 registrants and 560 exhibitors to in St. Louis, Missouri to talk about high-performance computing, artificial intelligence, infrastructure, and science. It was my tenth time attending in-person (12th overall), and as is always the case, it was a great week to reconnect with colleagues, hear what people are worrying about, and get a finger on the pulse of the now-rapidly changing HPC industry.]]></summary></entry><entry><title type="html">The trap of prioritizing impact</title><link href="https://hpc.social/personal-blog/2025/the-trap-of-prioritizing-impact/" rel="alternate" type="text/html" title="The trap of prioritizing impact" /><published>2025-09-20T14:46:41-06:00</published><updated>2025-09-20T14:46:41-06:00</updated><id>https://hpc.social/personal-blog/2025/the-trap-of-prioritizing-impact</id><content type="html" xml:base="https://hpc.social/personal-blog/2025/the-trap-of-prioritizing-impact/"><![CDATA[<p>(I wrote this originally as a comment in <a href="https://randsinrepose.com/welcome-to-rands-leadership-slack/">RLS </a>in response to a staff-level engineer who was frustrated at how little they got to code anymore, and it resonated with enough folks that maybe it‚Äôs worth sharing here!)</p>

<p>There‚Äôs a trap I‚Äôve seen a lot of staff+ folks fall into where they over-prioritize the idea that they should always be doing ‚Äúthe right, most effective thing for the company‚Äù. When I see engineers complain that they don‚Äôt get to code enough, I often suspect they‚Äôve fallen prey to this.</p>

<p>I say <strong><em>that‚Äôs a trap</em></strong>! because I see people do this at the expense of their own job satisfaction and growth, which is bad for both them and (eventually) for the company which is likely to lose them.</p>

<p>I don‚Äôt blame people for falling into this trap, it‚Äôs what we‚Äôre rewarded for. I‚Äôve fallen into it! I have stopped doing technical work I cared about, prioritized #impact, and fought fires wherever they arose. I have spent all my time mentoring and teaching and none coding. The result was often grateful colleagues, but also burnout and leaving jobs I otherwise liked.</p>

<p>Whereas when I‚Äôve allowed myself to be like 30% selfish ‚Äî picking some of my work because it was fun and technical, even when doing so was not the ‚Äúmost impactful‚Äù thing I could do ‚Äî I was happier, learned more, and stayed in roles longer.</p>

<p>An example: I worked on a team that was doing capacity planning poorly and was buying too much hardware. (On-prem, physical hardware.) I could have solved the problem with a spreadsheet, but that was boring and made my soul hurt.</p>

<p>What I did instead was dig into how our container scheduling platform worked, and wrote a nifty little CLI tool that would look at the team‚Äôs configured workloads and spit out a capacity requirement calculation. It took about three times as long as the spreadsheet would have, but it was fun and accomplished the same goal and gave me some experience in the container platform. And it wasn‚Äôt that much of a time sink.</p>

<p>Was that better for the company? No idea. I hope it was ‚Äî I hear the tool is still maintained and no one has replaced it with a spreadsheet yet! But that‚Äôs a happy accident.</p>

<p>Was it better for me? Absolutely! It was a bit selfish, but it made an otherwise tedious task more fun and I learned some useful tricks.</p>

<p>So ‚Äî if you wish you had more time to code‚Ä¶ go code a bit more. Don‚Äôt let the idea of being more effective guilt you into giving it up. Your career is your career and you should enjoy it.</p>]]></content><author><name>Thinking Out Loud</name></author><category term="ajdecon" /><summary type="html"><![CDATA[(I wrote this originally as a comment in RLS in response to a staff-level engineer who was frustrated at how little they got to code anymore, and it resonated with enough folks that maybe it‚Äôs worth sharing here!)]]></summary></entry><entry><title type="html">Lessons learned from three years in cloud supercomputing</title><link href="https://hpc.social/personal-blog/2025/lessons-learned-from-three-years-in-cloud-supercomputing/" rel="alternate" type="text/html" title="Lessons learned from three years in cloud supercomputing" /><published>2025-07-11T05:26:00-06:00</published><updated>2025-07-11T05:26:00-06:00</updated><id>https://hpc.social/personal-blog/2025/lessons-learned-from-three-years-in-cloud-supercomputing</id><content type="html" xml:base="https://hpc.social/personal-blog/2025/lessons-learned-from-three-years-in-cloud-supercomputing/"><![CDATA[<p>I recently decided to leave Microsoft after having spent just over three years there, first as a storage product manager, then as a compute engineer. Although I touched many parts of Azure's infrastructure during that time, everything I did was at the intersection of large-scale supercomputing and hyperscale cloud. There was no shortage of interesting systems to figure out and problems to solve, but as I began to wrap my arms around the totality of hyperscale AI training in the cloud, I also began to see the grand challenges that lay ahead.</p>
<div class="separator" style="clear: both; text-align: center;">
<figure>

<figcaption class="image-caption">Outside Microsoft's Silicon Valley Campus minutes after I was escorted off the premises.</figcaption>
</figure>
</div>
<p>Although many of those challenges would probably be fun and exciting to tackle, the more I learned, the more I found myself asking the same two questions: what did I want to do with the rest of my career, and was the path I was following going in the right direction? I spent a lot of time thinking about this, and my decision to leave Microsoft ultimately reflects the answer at which I arrived. But rather than indulge myself by recounting my introspection, I thought I would share some of the things that I learned while at Microsoft in the hopes that others find value in my experience.</p>
<p>To that end, I've split this post into two sections:</p>
<ol type="1">
<li>Things I've observed about <a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#hpc"><strong>HPC and technology trends</strong></a> from the perspective of a cloud/hyperscale/AI practitioner and provider, and</li>
<li>Things I've realized about <a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#career"><strong>jobs and careers</strong></a> from the perspective of someone who's now worked in <a href="https://www.sdsc.edu/">academia</a>, a <a href="https://www.cnbc.com/2019/09/12/10x-genomics-txg-biotech-start-up-surges-in-ipo-debut.html">successful startup</a>, <a href="https://www.nersc.gov/">government</a>, and now <a href="https://www.microsoft.com/">Big Tech</a> and is about halfway through his career</li>
</ol>
<p>I consider this to be the concluding chapter of a three-part series that began with¬†<a href="https://blog.glennklockwood.com/2022/05/life-and-leaving-nersc.html">Life and leaving NERSC</a>¬†and continued with¬†<a href="https://blog.glennklockwood.com/2024/08/how-has-life-after-leaving-labs-been.html">How has life after leaving the Labs been going</a>.</p>
<p>Also, please note that I authored this the day after my employment at Microsoft ended, and I was not beholden to any company or organization at the time of writing. <i>The views expressed below are mine alone</i>.</p>
<!--<ul><ul><li><a href="#hpc">HPC</a><ul><li><a href="#hpc-wants-to-be-like-the-cloud-not-in-it">HPC wants to be like the cloud, not in it</a></li><li><a href="#cloud-is-expensive-but-not-for-the-reasons-most-think">Cloud is expensive, but not for the reasons most think</a></li><li><a href="#although-sometimes-it-is">...Although sometimes it is</a></li><li><a href="#influencing-the-cloud-is-hard">Influencing the cloud is hard</a></li></ul></li><li><a href="#career">Career</a><ul><li><a href="#people-matter">People matter</a></li><li><a href="#company-culture-matters-too">Company culture matters, too</a></li><li><a href="#being-good-at-things-isnt-always-a-job">Being good at things isn't always a job</a></li><li><a href="#you-dont-have-to-be-your-employer">You don't have to be your employer</a></li><li><a href="#happiness-sometimes-costs-money">Happiness sometimes costs money</a></li></ul></li><li><a href="#final-thoughts">Final thoughts</a></li></ul></ul>-->
<h2 id="hpc">HPC</h2>
<p>Everything I did at Microsoft touched supercomputers in one way or another, and my day job was exclusively supporting Microsoft's largest AI training supercomputers. Despite that, I did a lot of moonlighting in support of Azure's Federal business, and this is how I justified giving talks at events like like <a href="https://sites.google.com/lbl.gov/nersc50-nug/home">NERSC@50</a>, <a href="https://sc24.supercomputing.org">SC</a>, and <a href="https://www.glennklockwood.com/garden/Salishan">Salishan</a> in my last year. It's also what let me straddle both worlds: I had a rare, first-hand knowledge of how the <a href="https://www.glennklockwood.com/garden/systems/Eagle">de facto largest supercomputers in the world</a> were built and used, and I had a front-row seat for how leaders in the traditional supercomputing world perceived (and sometimes misunderstood) what we were doing in the cloud.</p>
<p>Before I get into specific observations though, I should clarify some nomenclature that I will use throughout:</p>
<ul>
<li><strong>Supercomputers</strong> are the piles of compute nodes with a high-speed interconnect that are designed to solve one big problem in parallel. This is a generic term to describe the instrument, not its workload.</li>
<li><strong>HPC</strong>, <strong>traditional HPC</strong>, <strong>modsim</strong>, and <strong>scientific computing</strong> all refer to the ecosystem built around using something like MPI to solve a problem rooted in some type of science. Every big supercomputer run by DOE, procured through EuroHPC, and sited at the world-famous, government-funded supercomputer centers falls into this category.</li>
<li><strong>Cloud</strong>, <strong>hyperscale</strong>, and <strong>AI training</strong> all refer to the ecosystem built to train large language models. The supercomputers are run by hyperscale companies like Microsoft, Amazon, or Meta whose backgrounds have not historically been in the world of supercomputing.</li>
</ul>
<p>I realize that these are not very precise, but they're the easiest way to contrast what I learned inside Microsoft (a hyperscale cloud) with the world I came from prior (traditional HPC).</p>
<h3 id="hpc-wants-to-be-like-the-cloud-not-in-it">HPC wants to be like the cloud, not in it</h3>
<p>When I left NERSC in May 2022, <a href="https://blog.glennklockwood.com/2022/05/life-and-leaving-nersc.html">I speculated that the future of large-scale supercomputer centers</a> would be follow one of two paths:</p>
<ol type="1">
<li>They develop and squish cloud technologies into their supercomputers to make them more cloud-like, or</li>
<li>They abandon the idea of buying individual systems and instead enter into long-term relationships where flagship HPC systems are colocated inside cloud datacenters sited in places with low-cost, low-carbon power.</li>
</ol>
<p>I was hoping that the desire to continue building systems after passing the exascale milestone would make the next click-stop follow path #2, but early indications (across the global HPC landscape) are that the community has chosen path #1.</p>
<p>HPC centers around the world are embracing the idea of cloudifying on-prem supercomputers by adding virtualization, containerization, and integration with other services to enable complex workflows. And as a part of that, they're reinventing many of the technology integrations that have always been first-class citizens in cloud: CSCS added capabilities to create <a href="https://www.cscs.ch/publications/news/2024/new-research-infrastructure-alps-supercomputer-inaugurated">"versatile software-defined clusters" on their latest Cray system, Alps</a>. NERSC's next system, Doudna, is envisioned to allow its users to "<a href="https://www.vastdata.com/sharedeverything/how-nersc-is-rewriting-the-role-of-the-supercomputer">move from programming the supercomputer to programming the datacenter</a>." But none of these systems are actually using commercial cloud services in non-trivial ways.</p>
<p>In the year or two that followed ChatGPT, the notion of large-scale supercomputers in the cloud was a green field, and cloud providers were open to chasing all sorts of silly ideas. This made it the ideal time for the leadership HPC computing community to get a seat at the hyperscale table. Although their budgets couldn't compete with AI, HPC centers could've drafted on the investments of AI buildout and offered the societal impacts of using GPUs for science as a nice complement to the societal impacts of using GPUs for AI training.</p>
<p>Much to my dismay, though, that window of opportunity was spent decrying the investment in hyperscale and AI rather than trying to exploit it; that window was the year of "<a href="https://blog.glennklockwood.com/2024/05/isc24-recap.html#section11">us versus them</a>." And unfortunately, that window has essentially closed as accountants and CFOs have now sharpened their pencils and are searching for returns on the investments made in GPU infrastructure. The intrinsic value of supercomputing infrastructure in the cloud has been reduced to the point where <a href="https://www.theregister.com/2024/10/31/microsoft_q1_fy_2025/">Microsoft's CEO outright said they were turning away customers who just wanted to pay for GPU clusters</a>, because higher-quality revenue could be made from inferencing services that use those same GPUs.</p>
<p>So even if the HPC community woke up tomorrow and realized the long-term benefits of partnering with commercial clouds (instead of trying to copy them), I don't think cloud providers would respond with the same enthusiasm to meet in the middle now as they would have a year or two ago. I don't think this was a deliberate decision on behalf of the cloud providers, and they may not even fully realize this change. But the future of hyperscale supercomputing is rapidly crystallizing, and because HPC wasn't present in the solution, there's no room for it in the final structure.</p>
<h3 id="cloud-is-expensive-but-not-for-the-reasons-most-think">Cloud is expensive, but not for the reasons most think</h3>
<p>It's been easy to write off the cloud as too expensive for HPC, and most people do silly math based on public list prices for VMs to justify their position. The narrative usually goes something like, "<a href="https://info.ornl.gov/sites/publications/Files/Pub202373.pdf">if a single GPU VM costs $40/hr, then running 10,000 of them for five years will cost 17X more than our on-prem supercomputer!</a>" That's not how it works, and nobody pays that price. That $40/hr is the maximum possible price, and it includes the cost to the cloud provider of keeping nodes idle in the event that someone shows up and suddenly wants to use one on-demand.</p>
<p>But even if you cut out all the profit for the cloud provider and just look at the cost of the physical infrastructure, building a supercomputer in the cloud is just more expensive than putting a bunch of whitebox nodes into a traditional HPC datacenter. There's a couple reasons for this, and here are a couple in no particular order:</p>
<p><strong>High availability</strong>: Every cloud datacenter has redundant power, and most of them have <em>very</em> redundant power. This is provisioned independently of whatever goes inside of that datacenter, so when you deploy a 10 MW supercomputer inside a 10 MW cloud datacenter, that comes with at least 10 MW of backup diesel generators, UPSes, and the electrical infrastructure. HPC workloads don't really need this, but it's hard to deploy HPC in the cloud without a ton of generators and UPSes coming along for the ride. This is changing with AI-specific cloud datacenters now being built, but these AI datacenters still have way more redundant power than a typical on-prem HPC datacenter. Building a cloud datacenter with the minimal redundancy that a traditional HPC datacenter has would mean that facility couldn't ever be used for anything but HPC, and that would undercut the overall flexibility upon which cloud economics are built.</p>
<p><strong>Cloud-side infrastructure</strong>: Every compute node has to be attached to the frontend cloud network in addition to a backend high-speed network like InfiniBand, unlike a traditional supercomputer where nodes are only attached to one high-speed network. While the cost of the smart NIC in each node is just a couple hundred dollars, every cloud supercomputer has to have a complete frontend network built out to support every single compute node--that's a ton of switches, routers, and fiber that must be properly provisioned all the way up to the cloud region in which those nodes are deployed. This frontend network is what enables all the cool cloud features on every node (full SDN, integration with other cloud services, etc), but these features aren't generally worth their cost when running meat-and-potatoes HPC workloads like MPI jobs by themselves. Their value only really shines through when executing complex workflows that, for example, couple an MPI job with stateful services and globally accessible data sharing with fine-grained access controls, all fully automated through programmable APIs and full RBAC.</p>
<p><strong>AI-optimized system architecture</strong>: AI-optimized GPU supercomputers contain a bunch of components that your typical Cray or Eviden simply wouldn't have. I wrote about the <a href="https://www.glennklockwood.com/garden/differences-between-AI-and-HPC">differences between AI and HPC supercomputers elsewhere</a>, but in brief, AI workloads specifically benefit from having tens of terabytes of local SSDs and all-optical (no copper) RDMA fabrics. These add to the COGS (cost of goods sold) of an AI-optimized supercomputer, meaning that that a supercomputer with a thousand GPUs designed for AI is going to be more expensive than one designed for scientific computing no matter where it's deployed. And cloud providers are all optimizing their supercomputers for AI.</p>
<p>There's a bunch of other cloud "stuff" that is required as well; every cloud region has a first footprint which is a LOT of general-purpose servers and storage that is required to support the basic cloud control plane. Before any user-facing cloud resources (including supercomputers) can be deployed, there has to be tens or hundreds of racks of this cloud "stuff" that is up and running. And although the cost of that first footprint is amortized over many customers in larger or older cloud regions, larger single-use infrastructures (like supercomputers) carry a proportionally larger fraction of the cost to deploy the first footprint.</p>
<p>So when you look at the cost of running a single compute node in a cloud supercomputer, there are a bunch of extra ingredients baked in that you wouldn't get by just signing a check over to an OEM:</p>
<ul>
<li>a high availability SLA, afforded in part by all those generators and UPSes</li>
<li>slick cloud service integrations, privacy features, virtual networking, afforded by that frontend cloud network</li>
<li>better performance for AI training or inferencing workloads, afforded by extra SSDs and all-optical interconnects</li>
<li>a bunch of other typical TCO stuff--the power consumed by the node, the opportunity cost of free floor tiles in your datacenter, and the engineers and technicians that keep it all running</li>
</ul>
<p>Ultimately, someone needs to pay for all of these extra ingredients. Cloud providers <em>could</em> just eat the costs themselves and sell the supercomputing service at a price comparable to what a customer would pay for an on-prem supercomputer--and sometimes they do. But this dilutes the profitability of the deal, and it increases the risks of the cloud provider losing money if unexpected issues arise during execution. Losing money is objectively bad business, so it's usually cloud customers who are paying for all these extra capabilities regardless of if they use them or not.</p>
<p>So if all you want to do is run big MPI jobs, and you have no use for the extra availability, cloud integrations, privacy and security, and programmable infrastructure, sure--the price per-node is going to be higher in the cloud than on-prem. You're paying for a bunch of features that you don't need.</p>
<h3 id="although-sometimes-it-is">...Although sometimes it is</h3>
<p>Sometimes buying a supercomputer in the cloud is straight up more expensive because of the value it provides though. For example, I remember a case where a large AI company needed to train a big LLM on many thousands of GPUs, so they signed an agreement which gave them exclusive access to a cloud supercomputer that strongly resembled a specific GPU system in the DOE complex. Because I used to work in the DOE, I knew how much DOE paid to buy their GPU cluster, and I also knew that three years of maintenance was included in that cost.</p>
<p>What amazed me is what this AI company was willing to pay (roughly) the same price that DOE paid for their on-prem supercomputer, but in exchange, get exclusive access to a comparably capable cloud supercomputer (same GPUs model, similar GPU count, similar interconnect) for <em>one year only</em>. Put differently, being able to use a big, cutting-edge GPU cluster was worth up to 3x more to this AI company than it was to the DOE.</p>
<p>While it may sound like I'm spilling secrets here, the reality is that anyone working for a cloud provider wouldn't be able to tell which AI deal I was describing here--they all look like this, and they're all willing to spend significantly more than the HPC community for the same compute capability. This gives you a sense of the real value that AI companies place on all the benefits that cloud-based supercomputers can provide.</p>
<p>This isn't all bad for HPC, though. Every fat deal with an AI company means that there can be another deal with an HPC center that has slim margins. For example, let's say an AI company is willing to pay a billion dollars for a supercomputer whose TCO is only $330M--that means the cloud provider gets 67% margin. If the cloud provider's overall margin target is 50%, that means it can sell an identical supercomputer to an HPC customer at zero profit (for $330M) and still walk away happy. Thus, it is possible for the price of a supercomputer for HPC to be subsidized by all the money that the AI industry is throwing into supercomputing. Whether or not a cloud provider ever cuts deals like this is a business decision though--and as I said earlier, I don't think they're as open to silly ideas now as they used to be.</p>
<p>The real hurdle that I was never able to overcome out, though, is a result of the fact that there is finite expertise in HPC and AI in the world. HPC-AI is ultimately a zero-sum game, and every hour spent working with an HPC customer is usually an hour that isn't being spent working with a much more profitable AI customer. I constantly ran into this problem working in hyperscale AI; my full-time job was to deal with AI customers, but I enjoyed interacting with HPC customers too. As a result, I had to do a lot of my the HPC-specific work (preparing conference presentations, for example) on nights, weekends, and vacations. It was just hard to tell people that I couldn't help improve job uptime on a massive training run because I was preparing a talk for a workshop that, frankly, might be openly hostile to my message.</p>
<h3 id="influencing-the-cloud-is-hard">Influencing the cloud is hard</h3>
<p>Because the difference in investment is so big between HPC and AI, many of the carrots that the HPC community has traditionally dangled in front of HPC vendors aren't very enticing to the hyperscale AI community. For example, both US and European HPC programs have relied heavily on non-recurring engineering (NRE) contracts with industry partners to incentivize the creation of products that are well-suited for scientific computing; <a href="https://www.energy.gov/articles/department-energy-awards-six-research-contracts-totaling-258-million-accelerate-us">PathFoward</a> and <a href="https://research-and-innovation.ec.europa.eu/funding/funding-opportunities/funding-programmes-and-open-calls/horizon-2020_en">Horizon 2020</a> both come to mind as well-funded, successful efforts on this front.</p>
<p>However, HPC is the only customer community that really tries to do this, and it echoes a time when the HPC community was at the forefront of scale and innovation. Nowadays, the prospect of accepting $1M/year NRE contract to implement XYZ is completely unappetizing to a hyperscaler; it would probably cost more than $1M/year just to figure out how a company with <a href="https://www.microsoft.com/investor/reports/ar24/">$250 billion in annual revenue</a> can handle such an unusual type of contract and payment. Add to to this the weird intellectual property rules (like disentangling a <a href="https://www.energy.gov/gc/articles/advance-patent-waiver-wa2017-007?utm_source=chatgpt.com">40% cost sharing advance waiver</a> for a tiny project within a multi-billion-dollar business), and it can become a corporate quagmire to go anywhere near NRE projects. Companies with well-insulated HPC silos can probably manage this better, but part of hyperscale economics is that everything overlaps with everything else as much as possible across supercomputing, general-purpose computing, hardware, and software.</p>
<p>As a result of this, I really struggled to understand how a $20M/year service contract and a $1M/year NRE contract is materially different from a $21M/year service contract in the cloud world. For most (non-HPC) cloud customers, the RFP comes in saying "we need XYZ" and some product manager notes customer demand for XYZ. If the demand is large enough, the feature winds up on roadmap, and the cloud provider develops it as a part of regular business. If there is no other demand, then an NRE contract isn't really going to change that; maintaining feature XYZ long-term will cost far more than a couple million dollars, so implementing it would be a bad decision. This isn't unique to cloud, for what it's worth; while there are some successful HPC NRE stories, there are far more NRE-originated products that had no product-market fit and were <a href="https://cug.org/proceedings/cug2016_proceedings/includes/files/pap105s2-file1.pdf">simply abandoned</a> after the associated supercomputer was retired.</p>
<p>As best as I can tell, NRE has become a way for big HPC customers to maintain the illusion that they are influencing hyperscalers. A hyperscaler could propose some NRE, and an HPC buyer could fund it, and there could be weekly meetings where the two get together and pretend like they're collaborating and codesigning. The hyperscaler could write milestone reports, and they could attend quarterly business reviews with the customer. But this feels like an act. You simply can't move a $250B/year company that isn't solely organized around supercomputing with the lure of a couple million a year.</p>
<p>This is not to say that NRE and codesign have no purpose in HPC! I'm sure component vendors (GPUs, networking, and the like) can make minor tweaks that offer big upside for the HPC community. But I learned that, as in several other dimensions, the HPC community is being pushed towards buying whatever is already on the truck, and NRE isn't going to have the impact that it once did.</p>
<h2 id="career">Career</h2>
<p>In addition to learning about how the hyperscale supercomputer world works in practice, my time at Microsoft exposed me to a segment of the supercomputing community that I didn't know existed: junior software engineers who were unwittingly thrown into the deep end of HPC straight out of college and were desperate to find their footing in both the technology and their careers overall. Maybe the most impactful work I did in the past three years was not technical at all, but instead came through some internal talks I gave on my professional journey in HPC and the one-on-one conversations that followed.</p>
<p>Since I've gotten such positive feedback when I talk and write about this aspect of HPC, I'll also share some things I've learned about choosing the right employer and job during my time at Microsoft.</p>
<h3 id="people-matter">People matter</h3>
<p>I learned that the right team matters more than the right job. It is profoundly important to me that I get to work with people with the same level of passion and curiosity, even if we are working on different problems.</p>
<p>In retrospect, I realize that I have been very lucky that my career has progressed through organizations that were packed to the gills with people with whom I shared values. They wanted to go to conferences to share their work, they wanted to hear about how others are solving similar challenges, and they weren't afraid to present (and challenge) new ideas. As I learned over the last three years though, I think these traits are acutely concentrated in the HPC world since HPC itself originated from academia and a culture of independence and self-direction. They certainly aren't universal to all workplaces.</p>
<p>To be clear, I am not saying that my coworkers at Microsoft weren't passionate or curious. But I did learn that, at big tech companies, you can have a perfectly successful career by keeping your head down and cranking away at the tasks given to you. If the work changes one day, it's actually a virtue to be able to walk away from the old project and turn your complete attention to a new one. Did the company just <a href="https://www.theverge.com/2024/10/1/24259369/microsoft-hololens-2-discontinuation-support">cancel the product you've been working on</a>? No problem. If you were good at writing code for Windows update, you'll probably be just fine at coordinating planned maintenances for supercomputers. A colleague of mine called these people "survivors," because they will do the best they can with whatever they're given.</p>
<p>While this agility is great if you love programming, it can also engender numbness and dispassion for any specific application area. If a "survivor" can just as easily program for HoloLens as they can for GPU telemetry, they also likely don't really <em>care</em> about either HoloLens or GPUs. This isn't a bad thing, and I am certainly not passing judgment on people who don't care about GPUs. But it does mean that it's harder for someone who really cares about GPUs to connect with a teammate who really doesn't. And this has many knock-on effects in day-to-day work; it's only natural for people who share common values to help each other out, while relative strangers are less likely to go that extra mile. Finding that common ground to promote "some person on team X" to "my trusted colleague on team X" is that much harder.</p>
<p>This difficulty in finding my community amidst all the survivors is what led me to look outside of my company to find my people. I went to events like the <a href="https://www.olcf.ornl.gov/tag/smoky-mountain-conference/">Smoky Mountains Conference</a> and <a href="https://sites.google.com/lbl.gov/nersc50-nug/home">NERSC@50</a> and took the stage to literally beg the HPC community to give me a reason to work with them. By the letter of my job description, I was never supposed to be on stage; I was supposed to spending all my time behind my desk, thinking about the reliability of our biggest supercomputers. But I liked working with the people in the HPC community, and I liked working with our HPC sales organization, because we all shared common values; we were passionate about HPC and the mission of advancing scientific computing. So, I wound up spending a lot of time working on simple things with HPC folks and not enough time doing my day job.</p>
<h3 id="company-culture-matters-too">Company culture matters, too</h3>
<p>In an organization where individuals don't often share a lot of common ground, I learned that it's incumbent upon everyone to make a deliberate effort to maintain a culture of working together and helping each other out. A positive workplace culture won't happen by itself across a massive organization. To this end, Satya has a bunch of corporate culture mantras that are often repeated to keep reminding people of the way employees should treat each other.</p>
<p>For example, he has a mantra of "<a href="https://www.msn.com/en-us/money/other/how-satya-nadella-created-a-learn-it-all-culture-at-microsoft-to-help-it-become-a-3-trillion-powerhouse/ar-BB1qWoRY">be a learn-it-all, not a know-it-all</a>." But I found that many people struggled to really understand how to do this in practice; when confronted with a tough problem ("your database keeps timing out when we point a thousand nodes at it at once"), it's often too easy to just be a know-it-all ("nobody else does that, so you are doing it wrong") rather than a learn-it-all ("why are you doing it that way?"). And the older a company is, the harder it is for decades-long veterans to maintain openness to new challenges in the silo they've built around themselves.</p>
<p>I've worked with HPC users for long enough to know that this attitude is pervasive anywhere you put a bunch of smart people with different perspectives into a room. However, it wasn't until I came to Microsoft that I learned that there's something to be gained by explicitly and repeatedly reminding people that they should strive to understand at least as much as they try to explain. Should I ever find myself in a leadership position, this is definitely a mantra I will carry with me and repeat to others, and I will credit my time at Microsoft with appreciating how to really live this mentality, not just parrot it.</p>
<h3 id="being-good-at-things-isnt-always-a-job">Being good at things isn't always a job</h3>
<p>People tell me that I'm pretty good at a bunch of stuff: figuring out how technologies work, explaining complex concepts in understandable ways, and taking a critical look at data and figuring out what's missing. And I enjoy doing these things; this is why I post to <a href="https://blog.glennklockwood.com/">my blog</a>, maintain <a href="https://www.glennklockwood.com/garden/">my digital garden</a>, and love <a href="https://www.youtube.com/playlist?list=PLtPey-3r1oZS0S5pPcWq-L4yrT9-R0gIm">getting on stage and giving presentations</a>. But people also say that, because I'm good at these things, there'd be no shortage of opportunities for me in the HPC industry should I ever go looking.</p>
<p>However, I've learned that a <em>job</em> has to be an amalgamation of <em>responsibilities</em> that create value, and connecting "things I'm good at" with "things that need to be done" is not always straightforward. For example, if I am <em>good at</em> learning things and share what I learned with others, what kind of jobs actually turn that into a <em>responsibility</em>?</p>
<ul>
<li><strong>Developers</strong> don't really do this at all. Their job is really to keep those git commits coming. Sometimes this requires learning new things, but writing blog posts or giving talks is not in the job description, so they don't count for much on performance reviews.</li>
<li><strong>Product managers</strong> do a little of this. I had to learn a few things and then repeat them a lot when I was a PM. Over and over. To customers, to executives, to partner teams. It was 5% learning and 95% sharing.</li>
<li><strong>Salespeople</strong> also do a little of this. They have to stay current on customer needs and product features, then repeat them a lot.</li>
<li><strong>System architects</strong> do a fair amount of this. I had to learn about what technologies are on the horizon, figure out how to piece them into an idea that could be implemented, then explain why it'd all be a good idea to others.</li>
<li><strong>Educators</strong> do a lot of this. The technology industry is always moving, so learning is required to stay up to date. They also get to be selective about the ideas worth sharing and downplay the rest.</li>
</ul>
<p>Each one of these roles has its own downsides too; for example, product managers and salespeople often have to nag people a lot, which I don't think anyone likes. And many of these roles require sharing knowledge with people who really don't want to hear it. After all, what customer is eager to talk to every salesperson who comes in the door?</p>
<p>Trying to find the ideal job is not just a matter of being good at many things; it's a matter of finding specific jobs that contain a maximal number of things you're good at and a minimal number of things you don't want to do. It's an NP-hard problem, and I've come to realize that the only way to solve it is through trial-and-error. I'm sure some people get lucky and figure out the optimal path on their first try, but for the rest of us, the only way to approach the optimal path is to continuously reflect and not longer on a known-suboptimal path for any longer than is necessary.</p>
<p>I've given up on trying to find the perfect job, because I've learned that it probably doesn't exist. I'm good at some things, I'm bad at some things; I enjoy some responsibilities, and I dislike some responsibilities. As with every other job I've had, I learned a lot about all four of these categories during my time at Microsoft, and my choice of next step has been informed by that. I don't expect it to be perfect, but I have high hopes that it will be a step in the right direction.</p>
<h3 id="you-dont-have-to-be-your-employer">You don't have to be your employer</h3>
<p>When I left the government for a corporate job, one of my biggest worries was losing credibility with peers whose opinions I respected. It's easy to dismiss the viewpoint of someone at a large vendor with a rationalization like, "of course they'd say that; it's their job," but I learned that the HPC community isn't so reductive. People are smart, and most were willing to engage with the quality of my ideas before checking the affiliation on my conference badge.</p>
<p>The trick, of course, was finding ways to share ideas in a way that didn't upset my corporate overlords but had substantive value to my audience. I think I figured this out, and in short, I found that leading with honesty and precision works best. The HPC community was built on sharing experiences and learnings about what does and doesn't work, so embracing that--rather than name-dropping products and making hyperbolic claims--seemed to keep me getting invited back to the HPC conferences and workshops that I wanted to attend.</p>
<p>I wasn't completely intentional in building whatever credibility I've gained over the last three years, but I was intentional in avoiding work that would clearly compromise it. I never want to be accused of misrepresenting the limits of my understanding, so I will never present a slide containing statements or plots that I can't substantiate. I also never want to be accused of misrepresenting the truth, so I am as forthright as possible in disclosing when I do (or don't) have an incentive to say something.</p>
<p>Because I stayed true to myself, I think I was the same person at Microsoft as I was at NERSC or SDSC. That continuity helped my peers quickly recalibrate after I became a vendor, and I think this helped me do more than if I had gone all-in on the role of a cloud spokesperson. Of course, there were times when I had to take on an employer-specific persona, but that's just business, and I've found that peers recognize that this is just a part of the game that we all must play.</p>
<p>The result of all this wasn't clear to me until after I started telling people I was leaving Microsoft. There are a bunch of HPC-specific projects I undertook on the side (e.g., reviewing and advising on research, serving on panels), and I started notifying people that I would have to find other Microsoft engineers to take over these obligations since I was leaving. Much to my surprise though, everyone responded the same way: the request to have me help was specifically to me, not my employer. Short of any conflicts of interest, they didn't care who employed me and valued my contributions regardless of who was signing my paychecks.</p>
<p>So, after three years working for an HPC vendor, I have learned that most people won't define you by your employer as long as you don't define yourself by your employer. It is possible to work for a company that sells HPC and still maintain your own identity as a person, but it requires thoughtful effort and a supportive (or indifferent!) employer. If you act like a company shill, you will be regarded as one, but not many jobs in industry actually <em>require</em> that to fulfill your responsibilities.</p>
<h3 id="happiness-sometimes-costs-money">Happiness sometimes costs money</h3>
<p>I think most people would agree that, while money can't buy happiness, it certainly helps. What I didn't realize until recently, though, is a reciprocal truth: sometimes happiness costs money.</p>
<p>A year ago, I wrote about <a href="https://blog.glennklockwood.com/2024/08/how-has-life-after-leaving-labs-been.html#pay-good">how the pay in industry compares to working at the national labs</a>, and I described how my golden handcuffs were structured. An optimist might say that these vesting schedules are a way to keep a happy employee from being lured away, but I think it's equally common that these are truly handcuffs. They are a constant reminder that, even in the darkest of days, there is a six-figure reason to grit one's teeth and persevere.</p>
<p>I've come to realize that there is an adverse correlation between a few factors:</p>
<ul>
<li>Smaller organizations offer more flexibility to mold a job around your preferences, because there is more work scope spread across fewer people.</li>
<li>Larger organizations can afford to offer larger total compensation, but flexibility is limited to the scope of any single team.</li>
</ul>
<p>I kind of thought about it like this:</p>
<div class="separator" style="clear: both; text-align: center;"></div>
<p>When I realized that I should explore other paths, I had to determine where in this continuum I wanted to wind up: do I care more about a fat paycheck, or do I care more about enjoying my day-to-day responsibilities? And once offers started coming in, exactly how much of a pay cut was I willing to take in exchange for the flexibility that I would receive?</p>
<p>By the time I handed in my resignation at Microsoft, I knew exactly how much this happiness was worth to me. Alternatively, I found out how much opportunity cost I was willing to pay for the ability (hopefully!) to reconnect with my day-to-day work. The calculus was an interesting exercise involving a bunch of Monte Carlo simulation which I won't detail here, but as it turns out, I was willing to pay a lot of money for the chance to do something that aligned more completely with what I wanted to do with the rest of my career. In the end, I gave up hundreds of thousands in unvested stock, and I am taking a six-figure pay cut in annual base pay when I start my next job. For me, though, this was a fair price to pay.</p>
<h2 id="final-thoughts">Final thoughts</h2>
<p>After three years in the world of hyperscale supercomputing, I have come away with two major learnings that now shape how I think about the future.</p>
<p>On the technical front, I think the HPC community has chosen to keep going its own way and reinvent the cloud rather than work meaningfully with hyperscale cloud providers. There was a brief window of opportunity where <a href="https://idiomorigins.org/origin/if-the-mountain-wont-come-to-muhammad-then-muhammed-must-go-to-the-mountain">the mountain may have actually come to Muhammed</a>, and the trajectory of scientific computing could have fundamentally changed to align with the growth trajectory of hyperscale AI. However, I don't think the HPC community was ready to take a big swing during those early days post-ChatGPT or do an earnest assessment of what that future could've looked like. I also worry that the window has closed, and the HPC community never even realized what was on the table.</p>
<p>On the career front, I've realized that success is multidimensional. Money is one axis, but so are impact, people, and purpose. The relative importance of each is not always obvious either; they only became clearer to me as I tried different jobs across the space. I've found that the ability to work with like-minded people and the opportunity to learn and share are the most important dimensions to me, but also I recognize that I am privileged in others. Finding stacks of money can be easy for those who work in AI, but there are no shortcuts to building (and retaining!) teams of great people. Anyone who can do the latter well should not be undervalued.</p>
<p>There's a lot more that I didn't have time to organize and write, but I have every intention of continuing to be myself, regardless of where I work, in the future. I will keep writing, posting, and talking about what I'm learning in supercomputing whenever I can. And along those lines, I hope that writing all this out helps others figure out what's important to them and where they want to go.</p>]]></content><author><name>Glenn K. Lockwood&apos;s Blog</name></author><category term="glennklockwood" /><summary type="html"><![CDATA[I recently decided to leave Microsoft after having spent just over three years there, first as a storage product manager, then as a compute engineer. Although I touched many parts of Azure's infrastructure during that time, everything I did was at the intersection of large-scale supercomputing and hyperscale cloud. There was no shortage of interesting systems to figure out and problems to solve, but as I began to wrap my arms around the totality of hyperscale AI training in the cloud, I also began to see the grand challenges that lay ahead.]]></summary></entry></feed>