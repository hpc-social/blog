<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en-US"><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="https://hpc.social/personal-blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://hpc.social/personal-blog/" rel="alternate" type="text/html" hreflang="en-US" /><updated>2025-01-16T19:56:12-07:00</updated><id>https://hpc.social/personal-blog/feed.xml</id><title type="html">hpc.social - Aggregated Personal Blog</title><subtitle>Shared personal experiences and stories</subtitle><author><name>hpc.social</name><email>info@hpc.social</email></author><entry><title type="html">Fine tuning AI models with InstructLab under IBM LSF</title><link href="https://hpc.social/personal-blog/2025/fine-tuning-ai-models-with-instructlab-under-ibm-lsf/" rel="alternate" type="text/html" title="Fine tuning AI models with InstructLab under IBM LSF" /><published>2025-01-06T19:36:24-07:00</published><updated>2025-01-06T19:36:24-07:00</updated><id>https://hpc.social/personal-blog/2025/fine-tuning-ai-models-with-instructlab-under-ibm-lsf</id><content type="html" xml:base="https://hpc.social/personal-blog/2025/fine-tuning-ai-models-with-instructlab-under-ibm-lsf/"><![CDATA[<p><strong>Overview</strong></p>

<p>All the best for 2025! This blog looks back on a demo which I created for <a href="https://sc24.supercomputing.org">SC24</a>
last November to demonstrate InstructLab workflows running on an <a href="https://www.ibm.com/products/hpc-workload-management">IBM LSF</a>
cluster. Let’s begin with a bit of background. I’d like to thank Michael
Spriggs, STSM, IBM LSF for his contributions to this blog.</p>

<p>When I think of tuning, what immediately comes to my mind are visions of an
expert mechanic trying to extract the most from an engine. This blog is
focused on an entirely different type of tuning, AI model tuning. Like tuning
an engine, AI model tuning can be used to ensure a better fit for a given AI
model for your business.</p>

<p>Released by IBM and Red Hat in May 2024, <a href="https://research.ibm.com/blog/instruct-lab">InstructLab</a> is an open-source project
which provides the ability to fine-tune LLMs by adding skills and knowledge,
without having to retrain the model from scratch. InstructLab can run on
resource-constrained systems such as laptops, but also supports GPUs. Much has
been written about InstructLab and this blog is not intended to provide an
in-depth look at InstructLab. Rather, the objective here is to demonstrate how
InstructLab workloads can be distributed and managed in a high-performance
computing cluster with GPUs using the IBM LSF workload scheduler. Recently, IBM
published a paper describing the infrastructure used to train the Granite family
of AI foundation models. The paper describes the Vela and Blue Vela environments
in detail. In particular, the Blue Vela environment is built on a software stack
using Red Hat Enterprise Linux, IBM LSF and Storage Scale. Learn more in the
detailed paper <a href="https://arxiv.org/abs/2407.05467">here</a>.</p>

<p>The demo workflow consists of two LSF jobs. The first job generates synthetic
data, which is used to teach the LLM new skills or knowledge. The second job,
which depends upon the successful completion of the first, is the training job,
where the new skills or knowledge are incorporated into an existing base model.
A simple LSF job dependency is used to ensure the training job only runs after
the successful completion of the synthetic data generation step.</p>

<p>The environment used is equipped with Nvidia GPUs.  InstructLab jobs will be
run with the options for GPU support, and the jobs will be submitted to LSF
with the appropriate GPU scheduling directives. Furthermore, it is assumed that
the users' $HOME directory is available on all hosts in the cluster. Note that I
require neither root access, nor a user account that is an LSF administrator, to
install and use InstructLab on the LSF cluster.</p>

<p><strong>Configuration</strong></p>

<p>The HPC cluster is configured as follows:</p>

<ul>
<li>Red Hat Enterprise Linux v8.8</li>
<li>IBM LSF v10.0.1.15</li>
<li>InstructLab v0.19.4</li>
<li>Miniforge v3 (24.9.0-0)</li>
<li>NVIDIA CUDA v12.6</li>
<li>Compute nodes are equipped with 8 x Nvidia H100 GPUs</li>
</ul>
<p><strong>Install InstructLab</strong></p>

<ol>
<li>Log in to a compute node in the LSF cluster equipped with GPUs. If ssh access
is disabled to compute nodes, then submit an interactive LSF batch job. This job
requests 8 GPUs on a single system and will set them to exclusive execution
mode.</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext">$ bsub -Is -R "span[hosts=1]" -gpu "num=8:j_exclusive=yes" bash</code></pre></div>

<ol start="2">
<li>Install and set up a Conda environment. This will enable you to install a
self-contained Conda environment for your user account with the necessary
Python version needed for InstructLab. Miniforge is installed in the default
location and the option to update the users shell profile to start the Conda
environment are selected. We assume here a shared $HOME directory.</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext">$ cd $HOME
$ curl -L -O "https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh"
$ bash Miniforge3-$(uname)-$(uname -m).sh</code></pre></div>

<ol start="3">
<li>Before proceeding, you must logout and log back in to activate the
environment. Next, a Conda environment is created with name <em>my_env</em>. Here we’ll
specify Python v3.11, which is a requirement for InstructLab.</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext">conda create --name my_env -c anaconda python=3.11
conda activate my_env</code></pre></div>

<ol start="4">
<li>Next, install InstructLab. Here, version 0.19.4 of InstructLab is specified.
This was the version of InstructLab available in the timeframe preceding the
SC24 event. Follow the installation steps in the official InstructLab
documentation <a href="https://github.com/instructlab/instructlab?tab=readme-ov-file#-installing-ilab">here</a>.</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext">$ pip install instructlab==0.19.4</code></pre></div>

<ol start="5">
<li>Next, perform the installation of InstructLab with Nvidia CUDA support. This
is required for InstructLab to utilize the GPUs. Without this step, InstructLab
will run on the CPUs. Note that CUDA v12.6 is installed on the system and the
variables set below reflect this.</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext">$ export CMAKE_ARGS="-DLLAMA_CUBLAS=on -DCUDA_PATH=/usr/local/cuda-12.6 -DCUDAToolkit_ROOT=/usr/local/cuda-12.6 -DCUDAToolkit_INCLUDE_DIR=/usr/local/cuda-12/include -DCUDAToolkit_LIBRARY_DIR=/usr/local/cuda-12.6/lib64"
$ export PATH=/usr/local/cuda-12.6/bin:$PATH
$ pip cache remove llama_cpp_python
$ CMAKE_ARGS="-DLLAMA_CUDA=on -DLLAMA_NATIVE=off" pip install 'instructlab[cuda]'
$ pip install vllm@git+https://github.com/opendatahub-io/vllm@v0.6.2</code></pre></div>

<p><strong>Configure InstructLab</strong></p>

<ol>
<li>With the installation of InstructLab complete, the next step is to run the
initialization. This will setup paths to models, taxonomy repo as well as the
GPU configuration.</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext">$ ilab config init</code></pre></div>

<ol start="2">
<li>By default InstructLab stores models, training checkpoints and other files
within <em>~/.cache</em> and <em>~/.local/share/instructlab</em>. If you have limited storage
capacity available in $HOME, then you may opt to disable training checkpoint
files. This can be done by setting the following option in <em>~/.config/instructlab/config.yaml</em> as follows.</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext">train:

  checkpoint_at_epoch: false</code></pre></div>

<ol start="3">
<li>Next, we download the required models. The ilab model list command can be
used to list the models which are available. Note that a <a href="https://huggingface.co">HuggingFace</a> token is
required to download certain models. Please set HF_TOKEN in the environment
with the appropriate token.</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext">$ export HF_TOKEN=&lt;HuggingFace token&gt;
$ ilab model download
$ ilab model download --repository=instructlab/granite-7b-lab
$ ilab model list

+--------------------------------------+---------------------+---------+

| Model Name                           | Last Modified       | Size    |

+--------------------------------------+---------------------+---------+
| instructlab/granite-7b-lab           | 2024-12-27 20:37:29 | 12.6 GB |
| mistral-7b-instruct-v0.2.Q4_K_M.gguf | 2024-12-27 16:55:46 | 4.1 GB  |
| merlinite-7b-lab-Q4_K_M.gguf         | 2024-12-27 16:48:39 | 4.1 GB  |
+--------------------------------------+---------------------+---------+</code></pre></div>

<p><strong>Generate synthetic data &amp; AI model training</strong></p>

<p>Next, is the synthetic data generation step, which will be executed on GPUs.
This step is a prerequisite to teaching the LLM new skills/knowledge via
training.</p>

<ol>
<li>
<p>Here we use example knowledge from the InstructLab github about Taylor Swift
fans, who are known as “Swifties”. This is timely because Taylor Swift recently
wrapped up 6 concerts in Toronto, Canada, where I happen to be based. Copy
attribution.txt and qna.yaml from the following <a href="https://github.com/mairin/taxonomy/tree/swifties/knowledge/arts/music/fandom/swifties">location</a>.</p>

</li>
<li>
<p>By default, the InstructLab taxonomy is found in <em>~/.local/share/instructlab/taxonomy</em>. Here we create the directories fandom/swifties under <em>~/.local/share/instructlab/taxonomy/knowledge/arts/fandom</em> and copy the files from step 1 into
this location.</p>

</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext">$ mkdir -p ~/.local/share/instructlab/taxonomy/knowledge/arts/fandom/swifties
$ cp &lt;path_to&gt;/attribution.txt ~/.local/share/instructlab/taxonomy/knowledge/arts/fandom/swifties
$ cp &lt;path_to&gt;/qna.yaml ~/.local/share/instructlab/taxonomy/knowledge/arts/fandom/swifties</code></pre></div>

<ol start="3">
<li>With the Swifties taxonomy in place, check for any syntax errors with the
command <em>ilab taxonomy diff</em>. It should report that the taxonomy is valid if
there are no syntax errors.</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext">$ ilab taxonomy diff
knowledge/arts/fandom/swifties/qna.yaml
Taxonomy in /u/gsamu/.local/share/instructlab/taxonomy is valid :)</code></pre></div>

<ol start="4">
<li>With the taxonomy in place and having confirmed that the syntax is valid,
it’s now time to run the synthetic data generation job through LSF. Here we will
request 8 GPUs on a single server in exclusive execution mode. For the
InstructLab ilab command, specify the <em>&ndash;gpus 8 and &ndash;pipeline full</em> options.
Standard output is written to the $HOME/job-output with filename specification
&lt;LSF_JOBID&gt;.out. The $HOME/job-output directory must already exist.</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext">$ mkdir -p $HOME/job-output
$ bsub -o $HOME/job-output/%J.out -R "span[hosts=1]" -gpu "num=8:j_exclusive=yes" ilab data generate --pipeline full --gpus 8
Job &lt;1131&gt; is submitted to default queue &lt;normal&gt;.</code></pre></div>

<ol start="5">
<li>During job execution, the LSF <em>bpeek</em> command can be used to monitor the job
standard output.</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext">$ bpeek -f 1131 
&lt;&lt; output from stdout &gt;&gt;
INFO 2025-01-02 09:51:29,503 numexpr.utils:146: Note: detected 96 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
INFO 2025-01-02 09:51:29,504 numexpr.utils:149: Note: NumExpr detected 96 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
INFO 2025-01-02 09:51:29,504 numexpr.utils:162: NumExpr defaulting to 16 threads.
INFO 2025-01-02 09:51:30,038 datasets:59: PyTorch version 2.3.1 available.
INFO 2025-01-02 09:51:31,226 instructlab.model.backends.llama_cpp💯 Trying to connect to model server at http://127.0.0.1:8000/v1
WARNING 2025-01-02 09:51:56,356 instructlab.data.generate:270: Disabling SDG batching - unsupported with llama.cpp serving
Generating synthetic data using 'full' pipeline, '/u/gsamu/.cache/instructlab/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf' model, '/u/gsamu/.local/share/instructlab/taxonomy' taxonomy, against http://127.0.0.1:55779/v1 server
INFO 2025-01-02 09:51:56,861 instructlab.sdg.generate_data:356: Synthesizing new instructions. If you aren't satisfied with the generated instructions, interrupt training (Ctrl-C) and try adjusting your YAML files. Adding more examples may help.
INFO 2025-01-02 09:51:56,872 instructlab.sdg.pipeline:153: Running pipeline single-threaded
INFO 2025-01-02 09:51:56,872 instructlab.sdg.pipeline:197: Running block: duplicate_document_col
INFO 2025-01-02 09:51:56,872 instructlab.sdg.pipeline:198: Dataset({
    features: ['icl_document', 'document', 'document_outline', 'domain', 'icl_query_1', 'icl_query_2', 'icl_query_3', 'icl_response_1', 'icl_response_2', 'icl_response_3'],
    num_rows: 35
})
INFO 2025-01-02 09:51:58,286 instructlab.sdg.llmblock:51: LLM server supports batched inputs: False
INFO 2025-01-02 09:51:58,286 instructlab.sdg.pipeline:197: Running block: gen_spellcheck
INFO 2025-01-02 09:51:58,286 instructlab.sdg.pipeline:198: Dataset({
    features: ['icl_document', 'document', 'document_outline', 'domain', 'icl_query_1', 'icl_query_2', 'icl_query_3', 'icl_response_1', 'icl_response_2', 'icl_response_3', 'base_document'],
    num_rows: 35
})
/u/gsamu/miniforge3/envs/my_env/lib/python3.11/site-packages/llama_cpp/llama.py:1054: RuntimeWarning: Detected duplicate leading "&lt;s&gt;" in prompt, this will likely reduce response quality, consider removing it...
  warnings.warn(
INFO 2025-01-02 09:57:42,264 instructlab.sdg.pipeline:197: Running block: flatten_auxiliary_columns
INFO 2025-01-02 09:57:42,264 instructlab.sdg.pipeline:198: Dataset({
    features: ['icl_document', 'document', 'document_outline', 'domain', 'icl_query_1', 'icl_query_2', 'icl_query_3', 'icl_response_1', 'icl_response_2', 'icl_response_3', 'base_document', 'spellcheck'],
    num_rows: 35
})
INFO 2025-01-02 09:57:42,279 instructlab.sdg.pipeline:197: Running block: rename_to_document_column
INFO 2025-01-02 09:57:42,279 instructlab.sdg.pipeline:198: Dataset({
    features: ['icl_document', 'document', 'document_outline', 'domain', 'icl_query_1', 'icl_query_2', 'icl_query_3', 'icl_response_1', 'icl_response_2', 'icl_response_3', 'dataset_type', 'corrected_document'],
    num_rows: 70
})
INFO 2025-01-02 09:57:42,282 instructlab.sdg.pipeline:197: Running block: gen_knowledge
INFO 2025-01-02 09:57:42,282 instructlab.sdg.pipeline:198: Dataset({
    features: ['icl_document', 'raw_document', 'document_outline', 'domain', 'icl_query_1', 'icl_query_2', 'icl_query_3', 'icl_response_1', 'icl_response_2', 'icl_response_3', 'dataset_type', 'document'],
    num_rows: 70
})
…
…</code></pre></div>

<ol start="6">
<li>During the runtime of the job, it’s possible to view GPU related metrics
using the LSF <em>lsload</em> and <em>bhosts</em> commands. First, we need to identify the host
where the job has been dispatched to using the LSF bjobs command. In this case
the job was dispatched to host <em>p1-r01-n4</em>. Note that details GPU accounting
metrics are available once the job runs to completion.</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext">$ bjobs -w
JOBID   USER    STAT  QUEUE      FROM_HOST   EXEC_HOST   JOB_NAME   SUBMIT_TIME
1131    gsamu   RUN   normal     rmf-login-1 p1-r01-n4   ilab data generate --pipeline full --gpus 8 Jan  2 14:51
$ lsload -w -gpu p1-r01-n4
HOST_NAME                 status ngpus gpu_shared_avg_mut gpu_shared_avg_ut ngpus_physical
p1-r01-n4                     ok     8                 2%                7%              8
$ bhosts -w -gpu p1-r01-n4
HOST_NAME            GPU_ID                MODEL     MUSED      MRSV  NJOBS    RUN   SUSP    RSV 
p1-r01-n4                 0   NVIDIAH10080GBHBM3        2G        0G      1      1      0      0
                          1   NVIDIAH10080GBHBM3        2G        0G      1      1      0      0
                          2   NVIDIAH10080GBHBM3        2G        0G      1      1      0      0
                          3   NVIDIAH10080GBHBM3        2G        0G      1      1      0      0
                          4   NVIDIAH10080GBHBM3        2G        0G      1      1      0      0
                          5   NVIDIAH10080GBHBM3        2G        0G      1      1      0      0
                          6   NVIDIAH10080GBHBM3        2G        0G      1      1      0      0
                          7   NVIDIAH10080GBHBM3        2G        0G      1      1      0      0</code></pre></div>

<ol start="7">
<li>After job completion, it’s possible to view details about the job including
GPU utilization which LSF collects by leveraging NVIDIA DCGM. These metrics are
available upon job completion using both the LSF <em>bhist</em> and <em>bjobs</em> commands.</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext">$ bhist -l -gpu 1131

Job &lt;1131&gt;, User &lt;gsamu&gt;, Project &lt;default&gt;, Command &lt;ilab data generate --pipe
                          line full --gpus 8&gt;
Thu Jan  2 14:51:23 2025: Submitted from host &lt;rmf-login-1&gt;, to Queue &lt;normal&gt;,
                           CWD &lt;$HOME&gt;, Output File &lt;/u/gsamu/job-output/%J.out
                          &gt;, Requested Resources &lt;span[hosts=1]&gt;, Requested GPU
                           &lt;num=8:j_exclusive=yes&gt;;
Thu Jan  2 14:51:24 2025: Dispatched 1 Task(s) on Host(s) &lt;p1-r01-n4&gt;, Allocate
                          d 1 Slot(s) on Host(s) &lt;p1-r01-n4&gt;, Effective RES_REQ
                           &lt;select[((ngpus&gt;0)) &amp;&amp; (type == local)] order[r15s:p
                          g] rusage[ngpus_physical=8.00] span[hosts=1] &gt;;
Thu Jan  2 14:51:25 2025: Starting (Pid 3095851);
Thu Jan  2 14:51:25 2025: External Message "p1-r01-n4:gpus=0,1,2,3,4,5,6,7;EFFE
                          CTIVE GPU REQ: num=8:mode=shared:mps=no:j_exclusive=y
                          es:gvendor=nvidia;" was posted from "gsamu" to messag
                          e box 0;
Thu Jan  2 14:51:26 2025: Running with execution home &lt;/u/gsamu&gt;, Execution CWD
                           &lt;/u/gsamu&gt;, Execution Pid &lt;3095851&gt;;
Thu Jan  2 16:08:05 2025: Done successfully. The CPU time used is 4624.0 second
                          s;
                          HOST: p1-r01-n4; CPU_TIME: 4624 seconds              
                                          GPU ID: 0
                                  Total Execution Time: 4597 seconds
                                  Energy Consumed: 579704 Joules
                                  SM Utilization (%): Avg 9, Max 15, Min 0
                                  Memory Utilization (%): Avg 2, Max 100, Min 0
                                  Max GPU Memory Used: 1956642816 bytes

                              GPU ID: 1
                                  Total Execution Time: 4597 seconds
                                  Energy Consumed: 503956 Joules
                                  SM Utilization (%): Avg 7, Max 11, Min 0
                                  Memory Utilization (%): Avg 2, Max 5, Min 0
                                  Max GPU Memory Used: 1767899136 bytes

                              GPU ID: 2
                                  Total Execution Time: 4597 seconds
                                  Energy Consumed: 501754 Joules
                                  SM Utilization (%): Avg 7, Max 11, Min 0
                                  Memory Utilization (%): Avg 2, Max 5, Min 0
                                  Max GPU Memory Used: 1784676352 bytes

                              GPU ID: 3
                                  Total Execution Time: 4597 seconds
                                  Energy Consumed: 525195 Joules
                                  SM Utilization (%): Avg 7, Max 11, Min 0
                                  Memory Utilization (%): Avg 2, Max 54, Min 0
                                  Max GPU Memory Used: 1767899136 bytes

                              GPU ID: 4
                                  Total Execution Time: 4597 seconds
                                  Energy Consumed: 525331 Joules
                                  SM Utilization (%): Avg 7, Max 12, Min 0
                                  Memory Utilization (%): Avg 2, Max 5, Min 0
                                  Max GPU Memory Used: 1767899136 bytes

                              GPU ID: 5
                                  Total Execution Time: 4597 seconds
                                  Energy Consumed: 502416 Joules
                                  SM Utilization (%): Avg 7, Max 11, Min 0
                                  Memory Utilization (%): Avg 2, Max 5, Min 0
                                  Max GPU Memory Used: 1784676352 bytes

                              GPU ID: 6
                                  Total Execution Time: 4597 seconds
                                  Energy Consumed: 508720 Joules
                                  SM Utilization (%): Avg 7, Max 12, Min 0
                                  Memory Utilization (%): Avg 2, Max 5, Min 0
                                  Max GPU Memory Used: 1784676352 bytes

                              GPU ID: 7
                                  Total Execution Time: 4597 seconds
                                  Energy Consumed: 491041 Joules
                                  SM Utilization (%): Avg 6, Max 12, Min 0
                                  Memory Utilization (%): Avg 2, Max 4, Min 0
                                  Max GPU Memory Used: 1933574144 bytes

GPU Energy Consumed: 4138117.000000 Joules

Thu Jan  2 16:08:05 2025: Post job process done successfully;


GPU_ALLOCATION:
 HOST             TASK GPU_ID  GI_PLACEMENT/SIZE    CI_PLACEMENT/SIZE    MODEL        MTOTAL  FACTOR MRSV    SOCKET NVLINK/XGMI                      
 p1-r01-n4        0    0       -                    -                    NVIDIAH10080 80G     9.0    0G      0      -                               
                  0    1       -                    -                    NVIDIAH10080 80G     9.0    0G      0      -                               
                  0    2       -                    -                    NVIDIAH10080 80G     9.0    0G      0      -                               
                  0    3       -                    -                    NVIDIAH10080 80G     9.0    0G      0      -                               
                  0    4       -                    -                    NVIDIAH10080 80G     9.0    0G      1      -                               
                  0    5       -                    -                    NVIDIAH10080 80G     9.0    0G      1      -                               
                  0    6       -                    -                    NVIDIAH10080 80G     9.0    0G      1      -                               
                  0    7       -                    -                    NVIDIAH10080 80G     9.0    0G      1      -                               

MEMORY USAGE:
MAX MEM: 2 Gbytes;  AVG MEM: 1 Gbytes; MEM Efficiency: 0.00%

CPU USAGE:
CPU PEAK: 1.69 ;  CPU PEAK DURATION: 52 second(s)
CPU AVERAGE EFFICIENCY: 100.69% ;  CPU PEAK EFFICIENCY: 169.23%

Summary of time in seconds spent in various states by  Thu Jan  2 16:08:05 2025
  PEND     PSUSP    RUN      USUSP    SSUSP    UNKWN    TOTAL
  1        0        4601     0        0        0        4602 </code></pre></div>

<ol start="8">
<li>
<p>When the synthetic data generation job completes, it’s output can be viewed
at <em>~/job-output/<!-- raw HTML omitted -->.out</em>. The synthetic data sets will comprise files in
the directory <em>~/.local/share/instructlab/datasets</em>. These files will be named
*skills_train_msgs_*.jsonl* and *knowledge_train_msgs_*.jsonl*.</p>

</li>
<li>
<p>With the synthetic data generation step complete, it’s now time to run the
training. We first set 2 environment variables to point to the following
files:  <em>~/.local/share/instructlab/datasets/knowledge_train_msgs_2025-01-02T09_51_56.jsonl</em>  and <em>~./.local/share/instructlab/datasets/skills_train_msgs_2025-01-02T09_51_56.jsonl</em>.</p>

</li>
</ol>
<p>Afterward, we submit the training job to LSF requesting 8 GPUs and with ilab
options <em>&ndash;pipeline accelerated</em>, <em>&ndash;gpus 8</em>, <em>&ndash;device cuda</em> and
<em>&ndash;data-path</em> pointing to the two above data files that were produced in the
synthetic data generation step.</p>

<div class="highlight"><pre><code class="language-plaintext">$ export SKILLS_PATH=/u/gsamu/.local/share/instructlab/datasets/skills_train_msgs_2025-01-02T09_51_56.jsonl
$ export KNOWLEDGE_PATH=/u/gsamu/.local/share/instructlab/datasets/knowledge_train_msgs_2025-01-02T09_51_56.jsonl
$ bsub -o $HOME/job-output/%J.out -R "span[hosts=1]" -gpu "num=8:j_exclusive=yes" ilab model train --pipeline accelerated --data-path $SKILLS_PATH --data-path $KNOWLEDGE_PATH --device cuda --gpus 8
Job &lt;1135&gt; is submitted to default queue &lt;normal&gt;.</code></pre></div>

<ol start="10">
<li>During job execution, the LSF <em>bpeek</em> command can be used to monitor the
job standard output.</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext">$ bpeek -f 1135
&lt;&lt; output from stdout &gt;&gt;
LoRA is disabled (rank=0), ignoring all additional LoRA args
[2025-01-02 12:52:04,359] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
INFO 2025-01-02 12:52:09,061 numexpr.utils:146: Note: detected 96 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
INFO 2025-01-02 12:52:09,061 numexpr.utils:149: Note: NumExpr detected 96 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
INFO 2025-01-02 12:52:09,061 numexpr.utils:162: NumExpr defaulting to 16 threads.
INFO 2025-01-02 12:52:09,304 datasets:59: PyTorch version 2.3.1 available.
You are using the default legacy behaviour of the &lt;class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'&gt;. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
INFO 2025-01-02 12:52:09,653 root:617: Special tokens: eos: [32000], pad: [32001], bos: [32005], system: [32004], user: [32002], assistant: [32003]
INFO 2025-01-02 12:52:09,923 root:617: number of dropped samples: 0 -- out of 641
 data arguments are:
{"data_path":"/u/gsamu/.local/share/instructlab/datasets/knowledge_train_msgs_2025-01-02T09_51_56.jsonl","data_output_path":"/u/gsamu/.local/share/instructlab/internal","max_seq_len":4096,"model_path":"/u/gsamu/.cache/instructlab/models/instructlab/granite-7b-lab","chat_tmpl_path":"/u/gsamu/miniforge3/envs/my_env/lib/python3.11/site-packages/instructlab/training/chat_templates/ibm_generic_tmpl.py","num_cpu_procs":16}
tokenizing the dataset with /u/gsamu/.cache/instructlab/models/instructlab/granite-7b-lab tokenizer...
ten largest length percentiles:
quantile 90th: 1459.0
quantile 91th: 1466.0
quantile 92th: 1469.6000000000001
quantile 93th: 1478.2
quantile 94th: 1483.0
quantile 95th: 1488.0
quantile 96th: 1497.1999999999998
quantile 97th: 1516.5999999999997
quantile 98th: 1540.6000000000001
quantile 99th: 1656.0000000000016
quantile 100th: 2578.0

at 4096 max sequence length, the number of samples to be dropped is 0
(0.00% of total)
quantile 0th: 368.0
quantile 1th: 393.0
quantile 2th: 411.2
quantile 3th: 421.2
quantile 4th: 427.2
quantile 5th: 442.0
quantile 6th: 604.4
quantile 7th: 631.8
quantile 8th: 653.8000000000001
quantile 9th: 679.8
quantile 10th: 742.0
at 20 min sequence length, the number of samples to be dropped is 0
checking the validity of the samples...
Categorizing training data type...
unmasking the appropriate message content...
 Samples Previews...
…
…</code></pre></div>

<ol start="11">
<li>During the runtime of the training job, we can observe some GPU utilization
information using the LSF lsload and bhosts commands.  First we need to identify
the server on which the training job is running. This is done using the bjobs
command and checking for the execution host of the job.</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext">$ bjobs -w
JOBID   USER    STAT  QUEUE      FROM_HOST   EXEC_HOST   JOB_NAME   SUBMIT_TIME
1135    gsamu   RUN   normal     rmf-login-1 p1-r01-n1   ilab model train --pipeline accelerated --data-path /u/gsamu/.local/share/instructlab/datasets/skills_train_msgs_2025-01-02T09_51_56.jsonl --data-path /u/gsamu/.local/share/instructlab/datasets/knowledge_train_msgs_2025-01-02T09_51_56.jsonl --device cuda --gpus 8 Jan  2 17:51
$ lsload -w -gpu p1-r01-n1
HOST_NAME                 status ngpus gpu_shared_avg_mut gpu_shared_avg_ut ngpus_physical
p1-r01-n1                     ok     8                 0%               22%              8
$ bhosts -w -gpu p1-r01-n1
HOST_NAME            GPU_ID                MODEL     MUSED      MRSV  NJOBS    RUN   SUSP    RSV 
p1-r01-n1                 0   NVIDIAH10080GBHBM3       10G        0G      1      1      0      0
                          1   NVIDIAH10080GBHBM3       10G        0G      1      1      0      0
                          2   NVIDIAH10080GBHBM3       10G        0G      1      1      0      0
                          3   NVIDIAH10080GBHBM3       10G        0G      1      1      0      0
                          4   NVIDIAH10080GBHBM3       10G        0G      1      1      0      0
                          5   NVIDIAH10080GBHBM3       10G        0G      1      1      0      0
                          6   NVIDIAH10080GBHBM3       10G        0G      1      1      0      0
                          7   NVIDIAH10080GBHBM3       10G        0G      1      1      0      0</code></pre></div>

<ol start="12">
<li>Once the job is complete, detailed GPU accounting can again be viewed using
the LSF <em>bhist</em> command as follows below.</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext">$ bhist -l -gpu 1135

Job &lt;1135&gt;, User &lt;gsamu&gt;, Project &lt;default&gt;, Command &lt;ilab model train --pipeli
                          ne accelerated --data-path /u/gsamu/.local/share/inst
                          ructlab/datasets/skills_train_msgs_2025-01-02T09_51_5
                          6.jsonl --data-path /u/gsamu/.local/share/instructlab
                          /datasets/knowledge_train_msgs_2025-01-02T09_51_56.js
                          onl --device cuda --gpus 8&gt;
Thu Jan  2 17:51:48 2025: Submitted from host &lt;rmf-login-1&gt;, to Queue &lt;normal&gt;,
                           CWD &lt;$HOME/.local/share/instructlab/checkpoints&gt;, Ou
                          tput File &lt;/u/gsamu/job-output/%J.out&gt;, Requested Res
                          ources &lt;span[hosts=1]&gt;, Requested GPU &lt;num=8:j_exclus
                          ive=yes&gt;;
Thu Jan  2 17:51:48 2025: Dispatched 1 Task(s) on Host(s) &lt;p1-r01-n1&gt;, Allocate
                          d 1 Slot(s) on Host(s) &lt;p1-r01-n1&gt;, Effective RES_REQ
                           &lt;select[((ngpus&gt;0)) &amp;&amp; (type == local)] order[r15s:p
                          g] rusage[ngpus_physical=8.00] span[hosts=1] &gt;;
Thu Jan  2 17:51:49 2025: Starting (Pid 3462241);
Thu Jan  2 17:51:49 2025: Running with execution home &lt;/u/gsamu&gt;, Execution CWD
                           &lt;/u/gsamu/.local/share/instructlab/checkpoints&gt;, Exe
                          cution Pid &lt;3462241&gt;;
Thu Jan  2 17:51:49 2025: External Message "p1-r01-n1:gpus=0,1,2,3,4,5,6,7;EFFE
                          CTIVE GPU REQ: num=8:mode=shared:mps=no:j_exclusive=y
                          es:gvendor=nvidia;" was posted from "gsamu" to messag
                          e box 0;
Thu Jan  2 17:57:56 2025: Done successfully. The CPU time used is 3024.0 second
                          s;
                          HOST: p1-r01-n1; CPU_TIME: 3024 seconds              
                                          GPU ID: 0
                                  Total Execution Time: 365 seconds
                                  Energy Consumed: 98890 Joules
                                  SM Utilization (%): Avg 20, Max 100, Min 0
                                  Memory Utilization (%): Avg 9, Max 62, Min 0
                                  Max GPU Memory Used: 53022294016 bytes

                              GPU ID: 1
                                  Total Execution Time: 365 seconds
                                  Energy Consumed: 97697 Joules
                                  SM Utilization (%): Avg 53, Max 100, Min 0
                                  Memory Utilization (%): Avg 9, Max 58, Min 0
                                  Max GPU Memory Used: 53087305728 bytes

                              GPU ID: 2
                                  Total Execution Time: 365 seconds
                                  Energy Consumed: 94820 Joules
                                  SM Utilization (%): Avg 53, Max 100, Min 0
                                  Memory Utilization (%): Avg 9, Max 62, Min 0
                                  Max GPU Memory Used: 53221523456 bytes

                              GPU ID: 3
                                  Total Execution Time: 365 seconds
                                  Energy Consumed: 98014 Joules
                                  SM Utilization (%): Avg 53, Max 100, Min 0
                                  Memory Utilization (%): Avg 9, Max 59, Min 0
                                  Max GPU Memory Used: 53041168384 bytes

                              GPU ID: 4
                                  Total Execution Time: 365 seconds
                                  Energy Consumed: 99246 Joules
                                  SM Utilization (%): Avg 53, Max 100, Min 0
                                  Memory Utilization (%): Avg 9, Max 60, Min 0
                                  Max GPU Memory Used: 53045362688 bytes

                              GPU ID: 5
                                  Total Execution Time: 365 seconds
                                  Energy Consumed: 94952 Joules
                                  SM Utilization (%): Avg 53, Max 100, Min 0
                                  Memory Utilization (%): Avg 9, Max 65, Min 0
                                  Max GPU Memory Used: 53047459840 bytes

                              GPU ID: 6
                                  Total Execution Time: 365 seconds
                                  Energy Consumed: 98227 Joules
                                  SM Utilization (%): Avg 53, Max 100, Min 0
                                  Memory Utilization (%): Avg 9, Max 63, Min 0
                                  Max GPU Memory Used: 53127151616 bytes

                              GPU ID: 7
                                  Total Execution Time: 365 seconds
                                  Energy Consumed: 94582 Joules
                                  SM Utilization (%): Avg 52, Max 100, Min 0
                                  Memory Utilization (%): Avg 9, Max 65, Min 0
                                  Max GPU Memory Used: 53481570304 bytes

GPU Energy Consumed: 776428.000000 Joules

Thu Jan  2 17:57:56 2025: Post job process done successfully;


GPU_ALLOCATION:
 HOST             TASK GPU_ID  GI_PLACEMENT/SIZE    CI_PLACEMENT/SIZE    MODEL        MTOTAL  FACTOR MRSV    SOCKET NVLINK/XGMI                      
 p1-r01-n1        0    0       -                    -                    NVIDIAH10080 80G     9.0    0G      0      -                               
                  0    1       -                    -                    NVIDIAH10080 80G     9.0    0G      0      -                               
                  0    2       -                    -                    NVIDIAH10080 80G     9.0    0G      0      -                               
                  0    3       -                    -                    NVIDIAH10080 80G     9.0    0G      0      -                               
                  0    4       -                    -                    NVIDIAH10080 80G     9.0    0G      1      -                               
                  0    5       -                    -                    NVIDIAH10080 80G     9.0    0G      1      -                               
                  0    6       -                    -                    NVIDIAH10080 80G     9.0    0G      1      -                               
                  0    7       -                    -                    NVIDIAH10080 80G     9.0    0G      1      -                               

MEMORY USAGE:
MAX MEM: 104 Gbytes;  AVG MEM: 16 Gbytes; MEM Efficiency: 0.00%

CPU USAGE:
CPU PEAK: 17.86 ;  CPU PEAK DURATION: 49 second(s)
CPU AVERAGE EFFICIENCY: 856.60% ;  CPU PEAK EFFICIENCY: 1785.71%

Summary of time in seconds spent in various states by  Thu Jan  2 17:57:56 2025
  PEND     PSUSP    RUN      USUSP    SSUSP    UNKWN    TOTAL
  0        0        368      0        0        0        368         </code></pre></div>

<ol start="13">
<li>Finally, with the model successfully trained, let’s chat with the new model
to check the result. Here’s we’ll pose it Swiftie specific questions. Note that
the output from the training is written to <em>~/.local/share/instructlab/checkpoints/hf_format</em>. We’ll take the model from the latest checkpoint directory that was
created. Here again, we launch the model chat job via LSF as an interactive
batch job (i.e. <em>bsub -Is</em>).</li>
</ol>
<div class="highlight"><pre><code class="language-plaintext">$ grep hf_format 1135.out
Model saved in /u/gsamu/.local/share/instructlab/checkpoints/hf_format/samples_886
Model saved in /u/gsamu/.local/share/instructlab/checkpoints/hf_format/samples_1776
Model saved in /u/gsamu/.local/share/instructlab/checkpoints/hf_format/samples_2658
Model saved in /u/gsamu/.local/share/instructlab/checkpoints/hf_format/samples_3546
Model saved in /u/gsamu/.local/share/instructlab/checkpoints/hf_format/samples_4435</code></pre></div>

<div class="highlight"><pre><code class="language-plaintext">$ bsub -Is -R "span[hosts=1]" -gpu "num=8:j_exclusive=yes" ilab model chat --model /u/gsamu/.local/share/instructlab/checkpoints/hf_format/samples_4435
Job &lt;1146&gt; is submitted to default queue &lt;interactive&gt;.
&lt;&lt;Waiting for dispatch ...&gt;&gt;
&lt;&lt;Starting on p1-r01-n2&gt;&gt;
INFO 2025-01-02 15:06:07,600 instructlab.model.backends.vllm:105: Trying to connect to model server at http://127.0.0.1:8000/v1
INFO 2025-01-02 15:06:08,876 instructlab.model.backends.vllm:308: vLLM starting up on pid 3744375 at http://127.0.0.1:41531/v1
INFO 2025-01-02 15:06:08,876 instructlab.model.backends.vllm:114: Starting a temporary vLLM server at http://127.0.0.1:41531/v1
INFO 2025-01-02 15:06:08,876 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 1/120
INFO 2025-01-02 15:06:12,244 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 2/120
INFO 2025-01-02 15:06:15,614 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 3/120
INFO 2025-01-02 15:06:18,801 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 4/120
INFO 2025-01-02 15:06:21,952 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 5/120
INFO 2025-01-02 15:06:25,391 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 6/120
INFO 2025-01-02 15:06:28,638 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 7/120
INFO 2025-01-02 15:06:32,103 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 8/120
INFO 2025-01-02 15:06:35,296 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 9/120
INFO 2025-01-02 15:06:38,616 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 10/120
INFO 2025-01-02 15:06:42,015 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 11/120
INFO 2025-01-02 15:06:45,435 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 12/120
INFO 2025-01-02 15:06:48,679 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 13/120
INFO 2025-01-02 15:06:52,025 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 14/120
INFO 2025-01-02 15:06:55,317 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 15/120
INFO 2025-01-02 15:06:58,604 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 16/120
INFO 2025-01-02 15:07:01,927 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 17/120
INFO 2025-01-02 15:07:05,287 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 18/120
INFO 2025-01-02 15:07:08,763 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 19/120
INFO 2025-01-02 15:07:12,131 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 20/120
INFO 2025-01-02 15:07:15,476 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 21/120
INFO 2025-01-02 15:07:18,881 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 22/120
INFO 2025-01-02 15:07:22,203 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 23/120
INFO 2025-01-02 15:07:25,599 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 24/120
INFO 2025-01-02 15:07:28,991 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 25/120
INFO 2025-01-02 15:07:32,234 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 26/120
INFO 2025-01-02 15:07:35,714 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 27/120
INFO 2025-01-02 15:07:38,974 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 28/120
INFO 2025-01-02 15:07:42,265 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 29/120
INFO 2025-01-02 15:07:45,582 instructlab.model.backends.vllm:129: Waiting for the vLLM server to start at http://127.0.0.1:41531/v1, this might take a moment... Attempt: 30/120
INFO 2025-01-02 15:07:45,586 instructlab.model.backends.vllm:136: vLLM engine successfully started at http://127.0.0.1:41531/v1
╭────────────────────────────────────────────────────────────── system ──────────────────────────────────────────────────────────────╮
│ Welcome to InstructLab Chat w/ SAMPLES_4435 (type /h for help)                                                                     │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
&gt;&gt;&gt; Tell me everything you know about Swifties.                                                                           [S][default]
╭─────────────────────────────────────────────────────────── samples_4435 ───────────────────────────────────────────────────────────╮
│ Swifties are the fandom of the American singer-songwriter Taylor Swift.                                                            │
│ Regarded by journalists as one of the largest, most devoted, and influential fan bases, Swifties are known for their high levels   │
│ of participation, creativity, community, fanaticism, and cultural impact on the music industry and popular culture. They are a     │
│ subject of widespread coverage in the mainstream media.                                                                            │
│                                                                                                                                    │
│ Critics have opined that Swift has redefined artist-fan relationships by establishing an intimate connection with Swifties. She    │
│ has frequently engaged with, helped, credited, and prioritized her fans, who have offered unprecedented support and interest in    │
│ her works irrespective of her wavering reception in the media. They continued to support Swift through her genre transitions,      │
│ unexpected artistic pivots, and her highly publicized controversies such as the 2019 masters dispute, while instigating the        │
│ political scrutiny of Ticketmaster that led to implementation of various laws and stimulated economic growth with the Eras Tour.   │
│ Swift's releases, promotional efforts, and fashion have garnered attention for incorporating Easter eggs and clues that are        │
...
...</code></pre></div>

<p><strong>Conclusions</strong></p>

<p>We’ve demonstrated a simple InstructLab workflow that is scheduled by IBM LSF
in a compute cluster equipped with GPUs.  As part of this example, LSF GPU
scheduling and accounting for GPU workloads was highlighted. For organizations
looking to productionize InstructLab and where there is a pool of GPU equipped
compute resources, LSF provides an ideal way to manage demand from a user
community looking to run these intensive workloads.</p>

<p>At the recent SC24 event, the demonstration went beyond what is shown in this
blog. It incorporated single click job submission via LSF Application Center
using a custom template that was created for InstructLab to submits both the
synthetic data generation job, as well the training job with a single click.
The demo environment was on IBM Cloud using instances equipped with Nvidia GPUs.
The compute instances were automatically scaled up and down by the LSF resource
connector. This will be the topic for a future blog.</p>]]></content><author><name>Ramblings of a supercomputing enthusiast.</name></author><category term="gaborsamu" /><summary type="html"><![CDATA[Overview]]></summary></entry><entry><title type="html">Surfing the Singularity - “Please hold for the next available agent”</title><link href="https://hpc.social/personal-blog/2025/surfing-the-singularity-please-hold-for-the-next-available-agent/" rel="alternate" type="text/html" title="Surfing the Singularity - “Please hold for the next available agent”" /><published>2025-01-03T17:00:00-07:00</published><updated>2025-01-03T17:00:00-07:00</updated><id>https://hpc.social/personal-blog/2025/surfing-the-singularity-please-hold-for-the-next-available-agent-</id><content type="html" xml:base="https://hpc.social/personal-blog/2025/surfing-the-singularity-please-hold-for-the-next-available-agent/"><![CDATA[<div class="separator" style="clear: both; text-align: center;"></div>
<p><br />&lt;div style="text-align: left;"&gt;<br />&lt;/div&gt;</p>
<div><span style="font-family: verdana;">Our imaginations, having been so stimulated by the "innovation trigger" of early interactions with ChatGPT and its LLM kin, having experienced the illusion of the algorithm reading your mind, we have now firmly entered into the period of inflated expectations. Any day now we expect a knock on the door to be informed by some HAL Junior that not only are we now out of a job, we've also got 20 minutes to evacuate the premise before its bulldozed to make way for another solar farm and data center. AGI is only just one product announcement away, or maybe two, but certainly three at most... </span></div>
<div><h3 style="text-align: left;"><span style="font-family: verdana;">Nose Deep </span></h3></div>
<div><span style="font-family: verdana;">There is a strong desire on the part of companies trafficking in AI to generate not just chatbot hallucinations but also customers for real business use cases, meaning revenue, and now. To do that we're going to need hardware, fast, lots of it, and gigajoules to power it. So AWS buys a new data center in PA adjacent to a 2.5GW nuclear power plant.[1] Not to be outdone Microsoft re-revs up Three Mile Island (albeit with a catchy rebranding laughable by 1970's standards), with 100% of the power going to their regional AI data centers.[2] </span></div>
<div><span style="font-family: verdana;"><br /></span><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://media.datacenterdynamics.com/media/images/Constellation_Three_Mile_Island.width-358.png" style="margin-left: auto; margin-right: auto;"><img border="0" height="280" src="https://media.datacenterdynamics.com/media/images/Constellation_Three_Mile_Island.width-358.png" width="400" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;"><span style="font-family: verdana;">Three Mile Island nuclear power plant, aka the "Crane Clean Energy Center".<br /></span>  </td></tr></tbody></table></div>
<div><span style="font-family: verdana;">After vigorous expectations the trough of disillusionment will soon follow. Already Microsoft hints that demand for AI-oriented chips is waning.[3] Practical, as you'll have a hard time getting them anyway - the data-center grade GPU chips on which AI computation rely are in short supply - NVIDIA via their TSMC outsource manufacturing partner is fully booked for Blackwell GPU orders for the next 12 months.[4] AWS has recently announced to customers (like me) new limitations on availability of certain NVIDIA GPU instances. (Consider also that AI competes with crypto for these scarce GPUs.) Intel suggests it will ship mass quantities of chips for AI-ready PCs and other mobile devices in 2025, but the stock traders are not yet buying it, with the stock currently fallen over 50% year-over-year. In the end, and as evidenced by the long term investments, we of course expect the march of techno-progress to continue, but in the short run, aligning expectations with reality may remain a challenge.</span></div>
<div><span style="font-family: verdana;"><br /></span></div>
<div><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"></td></tr><tr><td class="tr-caption" style="text-align: center;"><div style="font-family: verdana;">The August 2024 Gartner Hype Cycle for Emerging Technologies. </div>
<div style="font-family: verdana;">Generative AI - weee! [5]</div>
</td></tr></tbody></table></div>
<div><span style="font-family: verdana;"><div style="text-align: center;"><br /></div>
What does OpenAI say about all this? First, the desire to be non-profit has bumped up against the realities of scaling up the models. Will they continue to scale up, yielding better and deeper performance on the road to artificial general intelligence simply by scaling up, or will they hit a theoretical wall?[6] Sam Altman says succinctly: "there is no wall".[7] The nuclear-powered race is on, be it sustainable or not.</span></div>
<div><span style="font-family: verdana;"><br /></span><h3 style="text-align: left;"><span style="font-family: verdana;">"Your wait time is now less than..."</span></h3></div>
<div><span style="font-family: verdana;">But as we argued in the last blog [8], we don't need dystopia-inducing super-human AGI in order to make productive and disruptive use of artificial intelligence technologies - a domain-tuned artificial capable intelligence (ACI) is enough.[9] Or a collaborating set of them. </span></div>
<div><span style="font-family: verdana;"><br />OpenAI's strategic product roadmap is more than a little vague [10], but in theory after chatbots capable of basic reasoning comes the age of agents - think: allowing Alexa to auto-restock your pantry via a hotline to Bezos when it overhears you say you're low on sugar. Such "AI" does such a good job doing basic thinks like, oh I dunno, controlling the lights in your home now, what could go wrong?! Truth is, today's LLMs perform only so-so on standardized benchmarks, and while they improve all the time [11], the current state of the art is not yet ready to be trusted and at times seems like snake oil.[12]</span></div>
<div><span style="font-family: verdana;"><br />Today's agents tend to be domain-specific and tailored to narrow purpose - Salesforce.com agents for common customer interactions, ServiceNow agents helping the human agent perform repetitive or summary tasks in handling case loads, but not replacing the human.[13,14] Google Gemini can add events to your calendar, help you plan travel, but is not yet trusted to actually borrow your credit card and book it. Keeping the human-in-the-loop will remain for now, as a stepping stone to full automation.</span></div>
<div><span style="font-family: verdana;"><br />If you visit agent marketplaces like Agent.ai or SwarmZero.ai, you'll see on the order of hundreds of agents available to handle what are largely small, mundane, and repetitive tasks. There are similar domain agent marketplaces on OpenAI's site, Anthropic's, GitHub, Hugging Face, and more. Let's go along with the current norm and define "assistants" as gaggles of agents loosely collaborating to accomplish more complex tasks, perhaps as part of a hybrid AI-human team or for some cases ultimately on behalf of the entire organization, and yet, still not requiring full-on AGI. (Consider what just one techno-savvy entrepreneur with a diverse collection of AI auto-orgs might do.)</span></div>
<div><span style="font-family: verdana;"><br />The missing elements are reliable agent accuracy, which yields trust, and the hardware and power to run it all. Trust, unfortunately in the near term, may play second fiddle to profit, as the AI snake oil is sold to companies and governments and ultimately end users, most of whom barely understand it.</span></div>
<div><span style="font-family: verdana;"><br />In fact, the scientists themselves barely understand it. The deep learning networks that power today's LLMs are generally black boxes, layers upon layers of neural networks, numeric weights and matrix computations, where its pretty difficult to tell where any given word, image fragment, or concept is held in the vast space of the model, and how with various feed-forward and back-propagation processes in the network it is used in computing responses.</span></div>
<div><span style="font-family: verdana;"><br /></span></div>
<div><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://miro.medium.com/v2/0*-8c-MXmNvcvTLdHH.png" style="margin-left: auto; margin-right: auto;"><img border="0" height="360" src="https://miro.medium.com/v2/0*-8c-MXmNvcvTLdHH.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;"><span style="font-family: verdana; text-align: left;">A GPT model formed by combining successive attention and neural net layers. Input comes in at the left, and its black boxes all the way down.[15]</span></td></tr></tbody></table><div class="separator" style="clear: both; text-align: center;"> <span>  </span></div>
<span style="font-family: verdana;"><br /></span></div>
<div><span style="font-family: verdana;">Black box or not, as Sam Altman says, deep learning just works.[16] Sort of - AGI is unlikely without strong ontological and reasoning abilities and a tactile understanding of the physical world.[17] And deep learning itself is not without its problems. If the training data is biased, so will be the results. Trainers have to be alert to overfitting the model to the training data in a way that makes the model ineffective on new data. And implementors need better tools which help introspect and observe the model to provide verification, to illuminate the black box. Until then, any technology which cannot be understood is indistinguishable from magic.</span></div>
<div><span style="font-family: verdana;"><br /></span><h3 style="text-align: left;"><span style="font-family: verdana;">Hell-o Operator</span></h3></div>
<div><span style="font-family: verdana;">AI is a broad term, encompassing many technologies, machine learning being just one of them, and deep learning based on neural networks being an even further niche. In many ways, given the black box nature of the solution, AI has become a substitute word for "automation", and/or "program", or "algorithm". And the ill-defined AI landscape is moving fast. Twelve months ago the buzz was about the emergence of the "prompt engineer" role in lieu of computer programmers, and today, not so much. Instead we now have thin but actionable (i.e. product-oriented) definitions like "agent" and "assistant" and a new suite of tools and cute icons to put on enterprise architecture diagrams. This is not to even mention the human and organizational impact of new agent-based workflows characterized by iterative, non-waterfall business processes - not something well understood or appreciated outside of software engineering circles.</span></div>
<div><span style="font-family: verdana;"><br />In this turbulent time, with vendors leapfrogging each other's capabilities and performance, there is no and cannot be any real standardization, no agreed abstractions on which to base a unifying orchestration layer. Move fast and break things, fix them later if they live long enough. Let the prototype knowingly become the short-lived product, and iterate, maybe. Think: sqrt of web time. Think: ChatGPT + IFTTT.[18] That is not an enterprise IT solution, nor one manageable for most individuals. That is a fine mess.</span></div>
<div><span style="font-family: verdana;"><br />Thankfully, we'll soon have AI assistants to fix it for us. </span></div>
<div><span style="font-family: verdana;"><br /></span></div>
<div><span style="font-family: verdana;">- andy <span style="font-size: 16px;">(linkedin: andygallojr)</span><br /><br /><br /></span><h3 style="text-align: left;"><span style="font-family: verdana;">References</span></h3><span style="font-family: verdana;">[1] <a href="https://www.datacenterdynamics.com/en/news/aws-acquires-talens-nuclear-data-center-campus-in-pennsylvania/">https://www.datacenterdynamics.com/en/news/aws-acquires-talens-nuclear-data-center-campus-in-pennsylvania/</a><br />[2] <a href="https://www.datacenterdynamics.com/en/news/three-mile-island-nuclear-power-plant-to-return-as-microsoft-signs-20-year-835mw-ai-data-center-ppa/">https://www.datacenterdynamics.com/en/news/three-mile-island-nuclear-power-plant-to-return-as-microsoft-signs-20-year-835mw-ai-data-center-ppa/</a></span></div>
<div><span style="font-family: verdana;">Readers unfamiliar with the nuclear accident at Three Mile Island in 1979 can read the summary here: <a href="https://en.wikipedia.org/wiki/Three_Mile_Island_accident">https://en.wikipedia.org/wiki/Three_Mile_Island_accident</a></span></div>
<div><span style="font-family: verdana;">[3] <a href="https://finance.yahoo.com/news/nvidia-stocks-correction-accelerated-since-020804144.html">https://finance.yahoo.com/news/nvidia-stocks-correction-accelerated-since-020804144.html</a><br />[4] <a href="https://www.smbom.com/news/14253">https://www.smbom.com/news/14253</a><br />[5] Gartner Hype Cycle for Emerging Technologies, August 2024, <a href="https://emt.gartnerweb.com/ngw/globalassets/en/newsroom/images/graphs/august_2024_ethc.png">https://emt.gartnerweb.com/ngw/globalassets/en/newsroom/images/graphs/august_2024_ethc.png</a><br />[6] "The Computational Limits of Deep Learning", <a href="https://arxiv.org/pdf/2007.05558">https://arxiv.org/pdf/2007.05558</a><br />[7] Sam Altman on X: "there is no wall", <a href="https://x.com/sama/status/1856941766915641580">https://x.com/sama/status/1856941766915641580</a></span></div>
<div><span style="font-family: verdana;">[8] Surfing the Singularity blog, <a href="https://surfthesing.blogspot.com/2024/12/surfing-singularity-coming-wave-book.html">https://surfthesing.blogspot.com/2024/12/surfing-singularity-coming-wave-book.html</a></span></div>
<div><span style="font-family: verdana;">[9] <span style="background-color: black; font-size: 15px;">"The Coming Wave", </span><span style="background-color: black; font-size: 15px;">M. Suleyman, Crown Pub., 2023</span><br />[10] <a href="https://www.theneurondaily.com/p/openais-leaked-agi-roadmap">https://www.theneurondaily.com/p/openais-leaked-agi-roadmap</a><br />[11] 12 Days of OpenAI, Day 12: <a href="https://www.youtube.com/watch?v=SKBG1sqdyIU">https://www.youtube.com/watch?v=SKBG1sqdyIU</a> <br />[12] "AI Snake Oil", Narayanan &amp; Kapoor, Princeton U. Press, 2024<br />[13] <a href="https://www.salesforce.com/news/stories/einstein-sales-agents-announcement">https://www.salesforce.com/news/stories/einstein-sales-agents-announcement</a><br />[14] <a href="https://www.servicenow.com/standard/resource-center/data-sheet/ds-virtual-agent.html">https://www.servicenow.com/standard/resource-center/data-sheet/ds-virtual-agent.html</a><br />[15] <a href="https://miro.medium.com/v2/0*-8c-MXmNvcvTLdHH.png">https://miro.medium.com/v2/0*-8c-MXmNvcvTLdHH.png</a> We recommend the following video for those not familiar with this architecture:  <a href="https://youtu.be/KJtZARuO3JY?si=Muq2xRdSTaa9LMXb">https://youtu.be/KJtZARuO3JY?si=Muq2xRdSTaa9LMXb</a><br />[16] <a href="https://ia.samaltman.com/">https://ia.samaltman.com/</a><br />[17] Yann LeCun on Lex Fridman podcast, <a href="https://www.youtube.com/watch?v=5t1vTLU7s40">https://www.youtube.com/watch?v=5t1vTLU7s40</a><br />[18] <a href="https://ifttt.com/chatgpt">https://ifttt.com/chatgpt</a><br /><br /></span></div>]]></content><author><name>Surfing the Singularity</name></author><category term="surfthesing" /><summary type="html"><![CDATA[&lt;div style="text-align: left;"&gt;&lt;/div&gt; Our imaginations, having been so stimulated by the "innovation trigger" of early interactions with ChatGPT and its LLM kin, having experienced the illusion of the algorithm reading your mind, we have now firmly entered into the period of inflated expectations. Any day now we expect a knock on the door to be informed by some HAL Junior that not only are we now out of a job, we've also got 20 minutes to evacuate the premise before its bulldozed to make way for another solar farm and data center. AGI is only just one product announcement away, or maybe two, but certainly three at most...  Nose Deep  There is a strong desire on the part of companies trafficking in AI to generate not just chatbot hallucinations but also customers for real business use cases, meaning revenue, and now. To do that we're going to need hardware, fast, lots of it, and gigajoules to power it. So AWS buys a new data center in PA adjacent to a 2.5GW nuclear power plant.[1] Not to be outdone Microsoft re-revs up Three Mile Island (albeit with a catchy rebranding laughable by 1970's standards), with 100% of the power going to their regional AI data centers.[2]  Three Mile Island nuclear power plant, aka the "Crane Clean Energy Center".   After vigorous expectations the trough of disillusionment will soon follow. Already Microsoft hints that demand for AI-oriented chips is waning.[3] Practical, as you'll have a hard time getting them anyway - the data-center grade GPU chips on which AI computation rely are in short supply - NVIDIA via their TSMC outsource manufacturing partner is fully booked for Blackwell GPU orders for the next 12 months.[4] AWS has recently announced to customers (like me) new limitations on availability of certain NVIDIA GPU instances. (Consider also that AI competes with crypto for these scarce GPUs.) Intel suggests it will ship mass quantities of chips for AI-ready PCs and other mobile devices in 2025, but the stock traders are not yet buying it, with the stock currently fallen over 50% year-over-year. In the end, and as evidenced by the long term investments, we of course expect the march of techno-progress to continue, but in the short run, aligning expectations with reality may remain a challenge. The August 2024 Gartner Hype Cycle for Emerging Technologies.  Generative AI - weee! [5] What does OpenAI say about all this? First, the desire to be non-profit has bumped up against the realities of scaling up the models. Will they continue to scale up, yielding better and deeper performance on the road to artificial general intelligence simply by scaling up, or will they hit a theoretical wall?[6] Sam Altman says succinctly: "there is no wall".[7] The nuclear-powered race is on, be it sustainable or not. "Your wait time is now less than..." But as we argued in the last blog [8], we don't need dystopia-inducing super-human AGI in order to make productive and disruptive use of artificial intelligence technologies - a domain-tuned artificial capable intelligence (ACI) is enough.[9] Or a collaborating set of them.  OpenAI's strategic product roadmap is more than a little vague [10], but in theory after chatbots capable of basic reasoning comes the age of agents - think: allowing Alexa to auto-restock your pantry via a hotline to Bezos when it overhears you say you're low on sugar. Such "AI" does such a good job doing basic thinks like, oh I dunno, controlling the lights in your home now, what could go wrong?! Truth is, today's LLMs perform only so-so on standardized benchmarks, and while they improve all the time [11], the current state of the art is not yet ready to be trusted and at times seems like snake oil.[12] Today's agents tend to be domain-specific and tailored to narrow purpose - Salesforce.com agents for common customer interactions, ServiceNow agents helping the human agent perform repetitive or summary tasks in handling case loads, but not replacing the human.[13,14] Google Gemini can add events to your calendar, help you plan travel, but is not yet trusted to actually borrow your credit card and book it. Keeping the human-in-the-loop will remain for now, as a stepping stone to full automation. If you visit agent marketplaces like Agent.ai or SwarmZero.ai, you'll see on the order of hundreds of agents available to handle what are largely small, mundane, and repetitive tasks. There are similar domain agent marketplaces on OpenAI's site, Anthropic's, GitHub, Hugging Face, and more. Let's go along with the current norm and define "assistants" as gaggles of agents loosely collaborating to accomplish more complex tasks, perhaps as part of a hybrid AI-human team or for some cases ultimately on behalf of the entire organization, and yet, still not requiring full-on AGI. (Consider what just one techno-savvy entrepreneur with a diverse collection of AI auto-orgs might do.) The missing elements are reliable agent accuracy, which yields trust, and the hardware and power to run it all. Trust, unfortunately in the near term, may play second fiddle to profit, as the AI snake oil is sold to companies and governments and ultimately end users, most of whom barely understand it. In fact, the scientists themselves barely understand it. The deep learning networks that power today's LLMs are generally black boxes, layers upon layers of neural networks, numeric weights and matrix computations, where its pretty difficult to tell where any given word, image fragment, or concept is held in the vast space of the model, and how with various feed-forward and back-propagation processes in the network it is used in computing responses. A GPT model formed by combining successive attention and neural net layers. Input comes in at the left, and its black boxes all the way down.[15]    Black box or not, as Sam Altman says, deep learning just works.[16] Sort of - AGI is unlikely without strong ontological and reasoning abilities and a tactile understanding of the physical world.[17] And deep learning itself is not without its problems. If the training data is biased, so will be the results. Trainers have to be alert to overfitting the model to the training data in a way that makes the model ineffective on new data. And implementors need better tools which help introspect and observe the model to provide verification, to illuminate the black box. Until then, any technology which cannot be understood is indistinguishable from magic. Hell-o Operator AI is a broad term, encompassing many technologies, machine learning being just one of them, and deep learning based on neural networks being an even further niche. In many ways, given the black box nature of the solution, AI has become a substitute word for "automation", and/or "program", or "algorithm". And the ill-defined AI landscape is moving fast. Twelve months ago the buzz was about the emergence of the "prompt engineer" role in lieu of computer programmers, and today, not so much. Instead we now have thin but actionable (i.e. product-oriented) definitions like "agent" and "assistant" and a new suite of tools and cute icons to put on enterprise architecture diagrams. This is not to even mention the human and organizational impact of new agent-based workflows characterized by iterative, non-waterfall business processes - not something well understood or appreciated outside of software engineering circles. In this turbulent time, with vendors leapfrogging each other's capabilities and performance, there is no and cannot be any real standardization, no agreed abstractions on which to base a unifying orchestration layer. Move fast and break things, fix them later if they live long enough. Let the prototype knowingly become the short-lived product, and iterate, maybe. Think: sqrt of web time. Think: ChatGPT + IFTTT.[18] That is not an enterprise IT solution, nor one manageable for most individuals. That is a fine mess. Thankfully, we'll soon have AI assistants to fix it for us.  - andy (linkedin: andygallojr)References[1] https://www.datacenterdynamics.com/en/news/aws-acquires-talens-nuclear-data-center-campus-in-pennsylvania/[2] https://www.datacenterdynamics.com/en/news/three-mile-island-nuclear-power-plant-to-return-as-microsoft-signs-20-year-835mw-ai-data-center-ppa/ Readers unfamiliar with the nuclear accident at Three Mile Island in 1979 can read the summary here: https://en.wikipedia.org/wiki/Three_Mile_Island_accident [3] https://finance.yahoo.com/news/nvidia-stocks-correction-accelerated-since-020804144.html[4] https://www.smbom.com/news/14253[5] Gartner Hype Cycle for Emerging Technologies, August 2024, https://emt.gartnerweb.com/ngw/globalassets/en/newsroom/images/graphs/august_2024_ethc.png[6] "The Computational Limits of Deep Learning", https://arxiv.org/pdf/2007.05558[7] Sam Altman on X: "there is no wall", https://x.com/sama/status/1856941766915641580 [8] Surfing the Singularity blog, https://surfthesing.blogspot.com/2024/12/surfing-singularity-coming-wave-book.html [9] "The Coming Wave", M. Suleyman, Crown Pub., 2023[10] https://www.theneurondaily.com/p/openais-leaked-agi-roadmap[11] 12 Days of OpenAI, Day 12: https://www.youtube.com/watch?v=SKBG1sqdyIU [12] "AI Snake Oil", Narayanan &amp; Kapoor, Princeton U. Press, 2024[13] https://www.salesforce.com/news/stories/einstein-sales-agents-announcement[14] https://www.servicenow.com/standard/resource-center/data-sheet/ds-virtual-agent.html[15] https://miro.medium.com/v2/0*-8c-MXmNvcvTLdHH.png We recommend the following video for those not familiar with this architecture:  https://youtu.be/KJtZARuO3JY?si=Muq2xRdSTaa9LMXb[16] https://ia.samaltman.com/[17] Yann LeCun on Lex Fridman podcast, https://www.youtube.com/watch?v=5t1vTLU7s40[18] https://ifttt.com/chatgpt]]></summary></entry><entry><title type="html">Surfing the Singularity - “The Coming Wave” (a book report)</title><link href="https://hpc.social/personal-blog/2024/surfing-the-singularity-the-coming-wave-a-book-report/" rel="alternate" type="text/html" title="Surfing the Singularity - “The Coming Wave” (a book report)" /><published>2024-12-18T17:00:00-07:00</published><updated>2024-12-18T17:00:00-07:00</updated><id>https://hpc.social/personal-blog/2024/surfing-the-singularity-the-coming-wave-a-book-report-</id><content type="html" xml:base="https://hpc.social/personal-blog/2024/surfing-the-singularity-the-coming-wave-a-book-report/"><![CDATA[<p><span style="font-family: verdana;">Mustapha Suleyman knows a thing or two about AI.  Originally co-founder of DeepMind, a company and IP eventually acquired by Google, Mr. Suleyman is now CEO of AI at Microsoft. In this latest "Surfing the Singularity" blog installment, we'll review his recent book "The Coming Wave". Hang ten!</span></p>
<p><span style="font-family: verdana;"><br /></span></p>
<p><span style="font-family: verdana; font-size: x-large;">Go Where You Wanna Go</span></p>
<p><span style="font-family: verdana;">As a game, Go is notorious for its huge array of potential moves, exponentially more complex than chess for example, where computer models beat the best chess player way back in 1997. In 2016, DeepMind's model AlphaGo beat the best Go player in world after being trained the better part of a year with reinforced machine learning on a data set of human Go games and computer-vs-computer play. The following year, DeepMind's AlphaZero exceeded that performance in just a few days of training computation without ever being shown a single Go game, just having been described the rules of the game.[1]   </span></p>
<p><span style="font-family: verdana;"></span></p>
<div class="separator" style="clear: both; text-align: center;"></div>
<p><br />&lt;div style="text-align: center;"&gt;Alas, born at the wrong time.&lt;/div&gt;</p>
<p></p>
<p><span style="font-family: verdana;">In his Bill Gates-recommended [2] book "The Coming Wave", Mr. Suleyman's dystopian thesis is this: that the combination of AI, synthetic biology, and a host of other general purpose technologies such as robotics and additive manufacturing will combine into a major technological wave which will wash over the human race and alter it in unprecedented ways. Much as in past waves - the harnessing of fire, the wheel, the printing press, the combustion engine - each set off dramatic and often cataclysmic societal change the likes of which was certainly not obvious or expected by the "engineers" which developed the tooling. Call it the "rule of unintended consequences". The author supposes there have been about two dozen such waves over human history, and as expected in these times, the rate of arrival of transformational technologies is accelerating.</span></p>
<p><span style="font-family: verdana;">Take the printing press. Originally in 1440 there is but one device, the lab prototype. Fifty years later there are 1,000 printing presses in Europe. From producing just a few thousand hand-copied manuscripts a year, the bookmakers now produced millions. Demand for books soars, cost per unit drops, adoption deepens. What was the impact of this new information proliferation in the society? As Suleyman writes: "Gutenberg just wanted to make money printing Bibles. Yet his press catalyzed the Scientific Revolution and the Reformation, and so became the greatest threat to the Catholic Church since its establishment." And in spite of the efforts of certain Byzantine lords to control the press, proliferation of the technology was and is the default, driven by FOMO, at least.</span></p>
<p><span style="font-family: verdana;"><br /></span></p>
<p><span style="font-family: verdana; font-size: x-large;">Straight Outta Coruscant</span></p>
<p><span style="font-family: verdana;">The mass-scale rollout of AI is already underway, hand-in-hand with surveillance devices at the edge, high speed networking, nearly bottomless storage, and high performance computing on demand to make sense of it all. All so "The Algorithm" can feed you tailored news (and ads) with your morning coffee. And more, much more. Large Language Models (LLMs), trained on the corpus of human written and other creative output can now generate helpful suggestions in a variety of useful contexts (such as blog writing). And as I wrote about in my last blog [3], LLMs are useful code assistants too, although here current state of the art is about a 50% success rate on senior-level software development tasks. So yes, there is room to grow, but in line with the acceleration of the rate of change, we expect that gap to be closed in short time.</span></p>
<p><span style="font-family: verdana;">What then? A whole host of human-centric but generally rule-oriented tasks - think: back-office work in the finance and insurance industry - will become fair game for AI augmentation, meaning, human replacement. We see the rise of autonomous vehicles - think: bus and cab drivers, mail and package delivery, pilots. Air traffic controllers. Call centers. Medical radiology readers. Not one of these applications requires a super "artificial general intelligence" (AGI), simply a good model tailored to a specific task set, aka "artificial capable intelligence" (ACI). This is nearly all line-of-sight to market.</span></p>
<p><span style="font-family: verdana;">What then is not here now? The author spends a good amount of time discussing the rise and impact of artificial and synthetic biology, CRISPR gene hacking and the like. Not being personally equipped to analyze such biotechnologies, I'm simply going to leave that one to the reader, suggest its some heady stuff, but otherwise stay in the domain of the electro-mechanical. But even with this scope limitation, what is the wider wave?</span></p>
<p></p>
<div class="separator" style="clear: both; text-align: center;"></div>
<p><span style="font-family: verdana;">&lt;p&gt;<span style="font-family: verdana;"><br /></span>&lt;/p&gt;
Consider the rise of the bots, farm bots. GPS-guided autonomous tractors (already a thing).</span><span style="font-family: verdana;">[4]</span><span style="font-family: verdana;"> These robots don’t look like C3PO tending the moisture reapers on Tatooine - these robots look like farm equipment and are painted John Deere green. Amazon and Walmart distribution warehouses are already highly automated - combined with autonomous vehicles and AI-driven back-office work, how many employees do we think Amazon will need in 2 years? In 5 years? They currently employ 1.5 million people and reduced headcount 5% in the last 2 years while growing revenues over 20%.[5]</span>&lt;p&gt;&lt;/p&gt;</p>
<p><span style="font-family: verdana;"><br /></span></p>
<p><span style="font-family: verdana;"><span style="font-size: x-large;">Mind the Gap</span></span></p>
<p><span style="font-family: verdana;">And while your local Joe the Plumber [6] may continue to have a job visiting homes for some time to come, the use of single-task robotic automation in construction, especially new commercial construction, and property maintenance is on the rise. Concrete and paint-spraying robots. High rise window washers. Roofers, and general laborers to move material around a job site - bots - flying, floating, swimming, walking, drilling, boring bots. And more factory bots too, using traditional and additive techniques, to print their own parts. Bots that make bots. And an unemployment office at the Department of Labor run by AI.[7]</span></p>
<p><span style="font-family: verdana;">What about joining up with Uncle Sam, see the world, serve your country? Drone warfare in Ukraine has shown the folly of the massing of expensively equipped troops, and in the Red Sea the risks associated with large and high priced floating projections of power. Hypersonic weapons, beyond the capabilities of a human in-the-loop system to thwart. The result is asymmetric bot-on-bot warfare, beyond the battlefield, beyond borders. What are we to do with legions of technically unemployed, if they are not even useful for cannon fodder? And what are we to do with the State, if it cannot provide a system which benefits the population, which can keep it protected from proliferating technological threats? With information, wealth, and power centralized in the hands of a self-selected few, is it pitchforks and torches to the barricades then?[8]</span></p>
<p><span style="font-family: verdana;"></span></p>
<div class="separator" style="clear: both; text-align: center;"><span style="font-family: verdana;"></span></div>
<p></p>
<p><span style="font-family: verdana;">While its clear and no surprise the coming wave will benefit those with technical and financial authority, there is a chance of a boomerang effect which will result in forces in the opposite direction. The tooling, including the availability of sophisticated AI models and the means to run them, is being democratized. While increasing in capability, the cost of military-grade drones has decreased orders of magnitude in the last decade.[9] Rabble-rousing AI deepfakes proliferate. As Mr. Suleyman says, "anyone motivated to sow instability now has an easier time of it", not just state actors, agents, or oligarchs, but anyone with a few thousand dollars and an axe to grind. And considering the examples from recent 21st century past, if a rogue actor were to leverage the technology for nefarious purposes (think: 9/11 and the Patriot Act), there would surely be immediate call by the population for protection, likely but perhaps not exclusively by the State, backed by pervasive security surveillance. And this time, the means to fully execute on that wish exists. </span></p>
<p><span style="font-family: verdana;"><br /></span></p>
<p><span style="font-family: verdana;"><span style="font-size: x-large;">The China Syndrome</span></span></p>
<p><span style="font-family: verdana;"><span style="font-size: x-large;"></span></span></p>
<div class="separator" style="clear: both; text-align: center;"><span style="font-size: x-large;"></span></div>
<p><span style="font-family: verdana;">&lt;p style="text-align: center;"&gt;<span style="font-family: verdana;">”</span><span color="rgba(0, 0, 0, 0.87)" face="Roboto, Helvetica, Arial, sans-serif" style="background-color: white; font-size: 16px; letter-spacing: 0.5px;">The system works! That’s not the problem!”</span>&lt;/p&gt;
</span><span style="font-family: verdana;">&lt;div style="text-align: left;"&gt;It is a coming wave of contradictions and competing forces, and it sounds disruptive and quite unpleasant to say the least, perhaps even a human catastrophe. And besides avoiding the topic of bioengineering, we also haven’t yet discussed what happens when we actually do get to superhuman generalized AI - we’re still talking here about relatively dumb AI with human actors in charge, in theory.&lt;/div&gt;
</span>&lt;p&gt;&lt;/p&gt;</p>
<p><span style="font-family: verdana;">The author Mr. Suleyman concludes that the containment of this new technology - this artificial intelligence backed by autonomous mobility - a containment which has rarely if ever been possible (nukes being maybe the sole exception), must be done successfully, and urgently. Its a good sentiment, albeit one which may be too optimistic, even blindly. Can the march of this autonomous AI "progress" with its obvious and as yet to be seen additional consequences be stopped? I would argue, and the author would likely in the final analysis have to admit, that it cannot.</span></p>
<p><span style="font-family: verdana;">What to do about it? Maybe we should give serious thought to the existential question of what it actually means to be human.[10] Or, alternatively, as Timothy Leary said...[11]</span></p>
<p><span style="font-family: verdana;">Until next time.  </span><span style="font-family: verdana;">- andy</span></p>
<p><span style="font-family: verdana;"><br /></span></p>
<p><br /></p>
<p><span style="font-family: verdana; font-size: x-large;">References &amp; Amusements</span></p>
<p><span style="font-family: verdana;">[1] "The Coming Wave", </span><span style="font-family: verdana;">Mustapha Suleyman, Crown Pub., 2023</span></p>
<p><span style="font-family: verdana;">[2] Bill Gates blog, https://www.gatesnotes.com/holiday-books-2024</span></p>
<p><span style="font-family: verdana;">[3] "Surfing the Singularity: Super Grover!", </span><span style="font-family: verdana;">https://surfthesing.blogspot.com/2024/12/surfing-singularity-super-grover.html</span></p>
<p><span style="font-family: verdana;">[4] "John Deere Robot Planter"</span><span style="color: #020203;"><span style="font-family: verdana;">, </span></span><span style="font-family: verdana;">https://www.cnet.com/tech/john-deere-robot-planter-the-future-of-farming-looks-like-fewer-chemicals/</span></p>
<p><span style="font-family: verdana;">[5] https://www.statista.com/statistics/234488/number-of-amazon-employees/ and https://www.statista.com/statistics/266282/annual-net-revenue-of-amazoncom/</span></p>
<p><span style="font-family: verdana;">[6] https://www.nytimes.com/2023/08/28/us/politics/samuel-wurzelbacher-joe-the-plumber-dead.html</span></p>
<p><span style="font-family: verdana;">[7] https://www.dol.gov/agencies/oasam/centers-offices/ocio/ai-inventory</span></p>
<p><span style="font-family: verdana;">[8] https://www.stlouisfed.org/community-development-research/the-state-of-us-wealth-inequality</span></p>
<p><span style="font-family: verdana;">[9] https://www.technologyreview.com/2023/01/30/1067348/mass-market-military-drones-have-changed-the-way-wars-are-fought/</span></p>
<p><span style="font-family: verdana;">[10] https://www.organism.earth/library/document/unapologetically-human</span></p>
<p><span style="font-family: verdana;">[11] https://www.youtube.com/watch?v=IPSzTBP5PAU</span></p>
<p><span style="font-family: verdana;"><br /></span></p>
<p><br /></p>]]></content><author><name>Surfing the Singularity</name></author><category term="surfthesing" /><summary type="html"><![CDATA[Mustapha Suleyman knows a thing or two about AI.  Originally co-founder of DeepMind, a company and IP eventually acquired by Google, Mr. Suleyman is now CEO of AI at Microsoft. In this latest "Surfing the Singularity" blog installment, we'll review his recent book "The Coming Wave". Hang ten! Go Where You Wanna Go As a game, Go is notorious for its huge array of potential moves, exponentially more complex than chess for example, where computer models beat the best chess player way back in 1997. In 2016, DeepMind's model AlphaGo beat the best Go player in world after being trained the better part of a year with reinforced machine learning on a data set of human Go games and computer-vs-computer play. The following year, DeepMind's AlphaZero exceeded that performance in just a few days of training computation without ever being shown a single Go game, just having been described the rules of the game.[1]    &lt;div style="text-align: center;"&gt;Alas, born at the wrong time.&lt;/div&gt; In his Bill Gates-recommended [2] book "The Coming Wave", Mr. Suleyman's dystopian thesis is this: that the combination of AI, synthetic biology, and a host of other general purpose technologies such as robotics and additive manufacturing will combine into a major technological wave which will wash over the human race and alter it in unprecedented ways. Much as in past waves - the harnessing of fire, the wheel, the printing press, the combustion engine - each set off dramatic and often cataclysmic societal change the likes of which was certainly not obvious or expected by the "engineers" which developed the tooling. Call it the "rule of unintended consequences". The author supposes there have been about two dozen such waves over human history, and as expected in these times, the rate of arrival of transformational technologies is accelerating. Take the printing press. Originally in 1440 there is but one device, the lab prototype. Fifty years later there are 1,000 printing presses in Europe. From producing just a few thousand hand-copied manuscripts a year, the bookmakers now produced millions. Demand for books soars, cost per unit drops, adoption deepens. What was the impact of this new information proliferation in the society? As Suleyman writes: "Gutenberg just wanted to make money printing Bibles. Yet his press catalyzed the Scientific Revolution and the Reformation, and so became the greatest threat to the Catholic Church since its establishment." And in spite of the efforts of certain Byzantine lords to control the press, proliferation of the technology was and is the default, driven by FOMO, at least. Straight Outta Coruscant The mass-scale rollout of AI is already underway, hand-in-hand with surveillance devices at the edge, high speed networking, nearly bottomless storage, and high performance computing on demand to make sense of it all. All so "The Algorithm" can feed you tailored news (and ads) with your morning coffee. And more, much more. Large Language Models (LLMs), trained on the corpus of human written and other creative output can now generate helpful suggestions in a variety of useful contexts (such as blog writing). And as I wrote about in my last blog [3], LLMs are useful code assistants too, although here current state of the art is about a 50% success rate on senior-level software development tasks. So yes, there is room to grow, but in line with the acceleration of the rate of change, we expect that gap to be closed in short time. What then? A whole host of human-centric but generally rule-oriented tasks - think: back-office work in the finance and insurance industry - will become fair game for AI augmentation, meaning, human replacement. We see the rise of autonomous vehicles - think: bus and cab drivers, mail and package delivery, pilots. Air traffic controllers. Call centers. Medical radiology readers. Not one of these applications requires a super "artificial general intelligence" (AGI), simply a good model tailored to a specific task set, aka "artificial capable intelligence" (ACI). This is nearly all line-of-sight to market. What then is not here now? The author spends a good amount of time discussing the rise and impact of artificial and synthetic biology, CRISPR gene hacking and the like. Not being personally equipped to analyze such biotechnologies, I'm simply going to leave that one to the reader, suggest its some heady stuff, but otherwise stay in the domain of the electro-mechanical. But even with this scope limitation, what is the wider wave? &lt;p&gt;&lt;/p&gt; Consider the rise of the bots, farm bots. GPS-guided autonomous tractors (already a thing).[4] These robots don’t look like C3PO tending the moisture reapers on Tatooine - these robots look like farm equipment and are painted John Deere green. Amazon and Walmart distribution warehouses are already highly automated - combined with autonomous vehicles and AI-driven back-office work, how many employees do we think Amazon will need in 2 years? In 5 years? They currently employ 1.5 million people and reduced headcount 5% in the last 2 years while growing revenues over 20%.[5]&lt;p&gt;&lt;/p&gt; Mind the Gap And while your local Joe the Plumber [6] may continue to have a job visiting homes for some time to come, the use of single-task robotic automation in construction, especially new commercial construction, and property maintenance is on the rise. Concrete and paint-spraying robots. High rise window washers. Roofers, and general laborers to move material around a job site - bots - flying, floating, swimming, walking, drilling, boring bots. And more factory bots too, using traditional and additive techniques, to print their own parts. Bots that make bots. And an unemployment office at the Department of Labor run by AI.[7] What about joining up with Uncle Sam, see the world, serve your country? Drone warfare in Ukraine has shown the folly of the massing of expensively equipped troops, and in the Red Sea the risks associated with large and high priced floating projections of power. Hypersonic weapons, beyond the capabilities of a human in-the-loop system to thwart. The result is asymmetric bot-on-bot warfare, beyond the battlefield, beyond borders. What are we to do with legions of technically unemployed, if they are not even useful for cannon fodder? And what are we to do with the State, if it cannot provide a system which benefits the population, which can keep it protected from proliferating technological threats? With information, wealth, and power centralized in the hands of a self-selected few, is it pitchforks and torches to the barricades then?[8] While its clear and no surprise the coming wave will benefit those with technical and financial authority, there is a chance of a boomerang effect which will result in forces in the opposite direction. The tooling, including the availability of sophisticated AI models and the means to run them, is being democratized. While increasing in capability, the cost of military-grade drones has decreased orders of magnitude in the last decade.[9] Rabble-rousing AI deepfakes proliferate. As Mr. Suleyman says, "anyone motivated to sow instability now has an easier time of it", not just state actors, agents, or oligarchs, but anyone with a few thousand dollars and an axe to grind. And considering the examples from recent 21st century past, if a rogue actor were to leverage the technology for nefarious purposes (think: 9/11 and the Patriot Act), there would surely be immediate call by the population for protection, likely but perhaps not exclusively by the State, backed by pervasive security surveillance. And this time, the means to fully execute on that wish exists.  The China Syndrome &lt;p style="text-align: center;"&gt;”The system works! That’s not the problem!”&lt;/p&gt; &lt;div style="text-align: left;"&gt;It is a coming wave of contradictions and competing forces, and it sounds disruptive and quite unpleasant to say the least, perhaps even a human catastrophe. And besides avoiding the topic of bioengineering, we also haven’t yet discussed what happens when we actually do get to superhuman generalized AI - we’re still talking here about relatively dumb AI with human actors in charge, in theory.&lt;/div&gt; &lt;p&gt;&lt;/p&gt; The author Mr. Suleyman concludes that the containment of this new technology - this artificial intelligence backed by autonomous mobility - a containment which has rarely if ever been possible (nukes being maybe the sole exception), must be done successfully, and urgently. Its a good sentiment, albeit one which may be too optimistic, even blindly. Can the march of this autonomous AI "progress" with its obvious and as yet to be seen additional consequences be stopped? I would argue, and the author would likely in the final analysis have to admit, that it cannot. What to do about it? Maybe we should give serious thought to the existential question of what it actually means to be human.[10] Or, alternatively, as Timothy Leary said...[11] Until next time.  - andy References &amp; Amusements [1] "The Coming Wave", Mustapha Suleyman, Crown Pub., 2023 [2] Bill Gates blog, https://www.gatesnotes.com/holiday-books-2024 [3] "Surfing the Singularity: Super Grover!", https://surfthesing.blogspot.com/2024/12/surfing-singularity-super-grover.html [4] "John Deere Robot Planter", https://www.cnet.com/tech/john-deere-robot-planter-the-future-of-farming-looks-like-fewer-chemicals/ [5] https://www.statista.com/statistics/234488/number-of-amazon-employees/ and https://www.statista.com/statistics/266282/annual-net-revenue-of-amazoncom/ [6] https://www.nytimes.com/2023/08/28/us/politics/samuel-wurzelbacher-joe-the-plumber-dead.html [7] https://www.dol.gov/agencies/oasam/centers-offices/ocio/ai-inventory [8] https://www.stlouisfed.org/community-development-research/the-state-of-us-wealth-inequality [9] https://www.technologyreview.com/2023/01/30/1067348/mass-market-military-drones-have-changed-the-way-wars-are-fought/ [10] https://www.organism.earth/library/document/unapologetically-human [11] https://www.youtube.com/watch?v=IPSzTBP5PAU]]></summary></entry><entry><title type="html">Surfing the Singularity - Super Grover!</title><link href="https://hpc.social/personal-blog/2024/surfing-the-singularity-super-grover/" rel="alternate" type="text/html" title="Surfing the Singularity - Super Grover!" /><published>2024-12-07T17:00:00-07:00</published><updated>2024-12-07T17:00:00-07:00</updated><id>https://hpc.social/personal-blog/2024/surfing-the-singularity-super-grover-</id><content type="html" xml:base="https://hpc.social/personal-blog/2024/surfing-the-singularity-super-grover/"><![CDATA[<header class="pt4"><p style="line-height: 1.2; text-align: left;"><span style="font-weight: normal;"><span style="font-family: verdana;"><span color="rgba(255, 255, 255, 0.9)" face="-apple-system, system-ui, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, &quot;Helvetica Neue&quot;, &quot;Fira Sans&quot;, Ubuntu, Oxygen, &quot;Oxygen Sans&quot;, Cantarell, &quot;Droid Sans&quot;, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;, &quot;Lucida Grande&quot;, Helvetica, Arial, sans-serif" style="background-color: #1b1f23; font-size: 16px;">Hello and happy holidays to all. In this blog installment I'll report back from SuperComputing 2024, offer up a programmer-friendly view of the quantum computing space with a code tour of Grover's algorithm, and share some of my own thoughts on using the latest crop of AI programmer assistant tools.</span><span class="white-space-pre" color="rgba(255, 255, 255, 0.9)" face="var(--artdeco-reset-typography-font-family-sans)"> </span></span></span></p>
</header>
<div class="relative reader__grid"><div><div><div class="reader-article-content reader-article-content--content-blocks" dir="ltr"><div class="reader-content-blocks-container" tabindex="0"><div class="reader-image-block reader-image-block--resize"><figure class="reader-image-block__figure"><div class="ivm-image-view-model"><div class="ivm-view-attr__img-wrapper"><img alt="" class="ivm-view-attr__img--centered reader-image-block__img evi-image lazy-image ember-view" id="ember869" src="https://media.licdn.com/dms/image/v2/D4E12AQEXGYm_FH40rg/article-inline_image-shrink_1500_2232/article-inline_image-shrink_1500_2232/0/1733613595470?e=1740009600&amp;v=beta&amp;t=27x_IfTeyWu00AhmowXKRB402xwRZI5SLAg3MUm9UGc" /></div>
</div>
<figcaption class="reader-image-block__figure-image-caption display-block full-width text-body-small-open t-sans text-align-center t-black--light"><span style="font-family: verdana;">(Sadly, not this Grover.)</span></figcaption></figure></div>
<p class="ember-view reader-text-block__paragraph" id="ember870"><span style="font-family: verdana;">It was a pleasant SC24 high performance computing (HPC) conference in November. Having attended in past either in-person (Atlanta this year) or virtual, this year I chose virtual again. The big loss was being unable to troll the enormous vendor hall, but otherwise, webcasts make it much easier to be in two places at one time or to skim topics of passing interest.[1]<span class="white-space-pre" face="var(--artdeco-reset-typography-font-family-sans)"> </span></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember871"><span style="font-family: verdana;">There's a new top HPC machine (that we know of) - El Capitan, and its powered by AMD, containing about a million CPU cores and about 10 million GPU cores.[2] Molecular dynamics papers presented using a GPU-accelerated exascale computer reminds that, quantum aside for a moment, the real work is still being done in the classical world.[3] NVIDIA showcased the growing fusion of AI and HPC with their "superchip" designs - incorporating a CPU and a GPU on the same chip.[4] And why not, the money keeps flowing, the current outgoing federal administration now locking in the CHIPS Act funding before the end of the term.[5]</span></p>
<p class="ember-view reader-text-block__paragraph" id="ember872"><br /></p>
<div class="reader-image-block reader-image-block--full-width"><figure class="reader-image-block__figure"><div class="ivm-image-view-model"><div class="ivm-view-attr__img-wrapper"><img alt="" class="ivm-view-attr__img--centered reader-image-block__img evi-image lazy-image ember-view" id="ember873" src="https://media.licdn.com/dms/image/v2/D4E12AQE0Sq5LcwFEAQ/article-inline_image-shrink_1500_2232/article-inline_image-shrink_1500_2232/0/1733610500704?e=1740009600&amp;v=beta&amp;t=Dr2oQXKZQk3AXM8R_LhkmyXIRrnOeaejjn8q97YVy_w" /></div>
</div>
<figcaption class="reader-image-block__figure-image-caption display-block full-width text-body-small-open t-sans text-align-center t-black--light"><span style="font-family: verdana;">NVIDIA's Grace Hopper architecture [6]</span></figcaption></figure></div>
<p class="ember-view reader-text-block__paragraph" id="ember874"><span style="font-family: verdana;">But beyond the incremental improvements in compute, storage, cooling, power consumption and the like, it seemed to me, through my remote goggles, that the real action was happening on the sidelines of SC24, in the quantum computing space.</span></p>
<p class="ember-view reader-text-block__paragraph" id="ember874"><span style="font-family: verdana;"><br /></span></p>
<h2><span style="font-family: verdana;">Quantum Chip on Shoulder<span class="white-space-pre" face="var(--artdeco-reset-typography-font-family-sans)"> </span></span></h2><p class="ember-view reader-text-block__paragraph" id="ember876"><span style="font-family: verdana;">There's significant skepticism of quantum computing from the HPC community at the moment. Quantum computers today are toys in comparison to HPC, and stakeholders in HPC and classical computing (which would include myself) might wonder aloud "what's the point?" For some applications (like the fluid dynamics apps my company uses), quantum utility is perhaps still a decade away.[7]<span class="white-space-pre" face="var(--artdeco-reset-typography-font-family-sans)"> </span></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember877"><span style="font-family: verdana;">But the groundwork is being laid today, and when we understand that there are useful problems which can be solved on quantum computers, and only on quantum computers, we might at least allow the playing to continue. And we might be surprised at how fast quantum computing is progressing, and also just churning. Even those in the industry are hedging their bets - on which qubit technologies and which companies will be the winners - and as such are changing partners on a regular basis [8,9,10], although this sometimes means needing to unproductively reinvent the wheel (how many Python quantum circuit libraries do we need?)[11]</span></p>
<p class="ember-view reader-text-block__paragraph" id="ember878"><span style="font-family: verdana;">A couple of new announcements from Big Blue caught my eye. First, an early demonstrator of incorporating an IBM quantum computer into an HPC data center, unifying the resource scheduling, was shown at RPI with their AiMOS cluster and their first-in-the-nation academic installation.[12] The second, and more important, was the paper in Nature demonstrating the union of two quantum computers via classical networks, providing another avenue for scaling up hybrid quantum computing.[13]<span class="white-space-pre" face="var(--artdeco-reset-typography-font-family-sans)"> </span></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember879"><span style="font-family: verdana;">But today's QPUs are still noisy, fragile, small, expensive, and scarce. Maturity is still a ways off. QPUs are not fungible - to be successful in executing an application on a quantum computer, we must understand the error profile of that specific device! Not just that brand of quantum computer product, but this instance of that product! We need hands-on examples to grow the personal and team experience with quantum while we await stabilized hardware and the productivity-enabling software abstractions which can only come after industry maturity, and these early (head-butting) experiences.<span class="white-space-pre" face="var(--artdeco-reset-typography-font-family-sans)"> </span></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember879"><span style="font-family: verdana;"><span class="white-space-pre" face="var(--artdeco-reset-typography-font-family-sans)"><br /></span></span></p>
<h2><span style="font-family: verdana;">Making Your Quantum Bones</span></h2><p class="ember-view reader-text-block__paragraph" id="ember881"><span style="font-family: verdana;">Today's education in and around quantum computing is still focused on the experimental audience. We are still doing experiments about quantum more than experiments with quantum. The educational material which does exist, and there is an increasing amount, is focused on a community which is very comfortable with quantum physics and its associated mathematics. Most people, myself included, do not fit this description. As a computer science graduate, adjunct prof, and software engineer by trade, I want to see higher level programming abstractions, not those centered around qubits and gates which clearly does not scale for large programs. We will be waiting a while. So in the meantime, we need to see some examples using today's technology and syntax but which are more suited for the Comp Sci student audience, to help begin to bridge that gap.<span class="white-space-pre" face="var(--artdeco-reset-typography-font-family-sans)"> </span></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember882"><span style="font-family: verdana;">If we visit the Algorithm Zoo [14], a collection of quantum algorithms which show a computational advantage over similar classical approaches, we find some things (but not many) which might look familiar to a CS undergrad, but the implementations, when they exist, are often broken. In this current early phase of the quantum era, vendors are playing free and loose with their SDKs, and releases with significant object model refactorings and breaking changes are the norm.<span class="white-space-pre" face="var(--artdeco-reset-typography-font-family-sans)"> </span></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember883"><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://github.com/agallojr/research-notes/blob/02253900f33d784402f0cd0b3ed4d9d360544605/quantum/src/qiskit/pattern_match.py" target="_self"><span style="font-family: verdana;">pattern_match.py</span></a></p>
<p class="ember-view reader-text-block__paragraph" id="ember884"><span style="font-family: verdana;">So I offer here an example of a quantum program which itself likely has a short shelf life. You can find it here.[15, &amp; above] It shows a cooked up example of using Grover's algorithm, which is a quantum algorithm for search in unstructured (e.g. unsorted) data. Classically, "cuz Murphy's Law", you might need to walk the entire dataset to find the item of interest (or show its not there) - we call this an O(n) algorithm. Using Grover's algorithm, you can do the search with a strong probability of success in O(sqrt(n)) time - a quadratic speedup, and one worth pursuing for many applications. Note we are only saying we can perform the search with high probability - this is quantum computing, and everything is a probability. And while the example shown isn't necessarily the most common application of Grover, understanding Grover is worthwhile, as it appears as a sub-procedure in many other quantum algorithms.</span></p>
<p class="ember-view reader-text-block__paragraph" id="ember885"><span style="font-family: verdana;">Here are a few key points to take away, even if you don't take the time to look at the documented code example:</span></p>
<p class="ember-view reader-text-block__paragraph" id="ember886"></p>
<ul><li><span style="font-family: verdana;">We're going to store a small set of strings - binary strings of 1's and 0's representing a small dataset - into a quantum system of qubits.<span class="white-space-pre" face="var(--artdeco-reset-typography-font-family-sans)"> </span></span></li><li><span style="font-family: verdana;">Since our strings are chosen to be 16 binary digits long, we will use 16 qubits. This is a large enough number of qubits to show some non-trivial problems, but not so many as to not be runnable on a simulator on your laptop. (The performance does not scale linearly with the number of qubits.)</span></li><li><span style="font-family: verdana;">A system of n qubits contains 2**n possible states (combinations of 0's and 1's). That's a lot. We will have far fewer state strings in our dataset - just a handful for this experiment.</span></li><li><span style="font-family: verdana;">After initializing the qubits, we will mark each state in the quantum system which corresponds to a binary string in our dataset. To do this we will use the phase of the qubit, which is an extra useful lever you do not find in classical bits (among other unique quantum advantages).</span></li></ul><p style="color: black;"></p>
<div class="reader-image-block reader-image-block--full-width"><figure class="reader-image-block__figure"><div class="ivm-image-view-model"><div class="ivm-view-attr__img-wrapper"><img alt="" class="ivm-view-attr__img--centered reader-image-block__img evi-image lazy-image ember-view" id="ember887" src="https://media.licdn.com/dms/image/v2/D4E12AQH3qvl7gip82A/article-inline_image-shrink_1500_2232/article-inline_image-shrink_1500_2232/0/1733610781858?e=1740009600&amp;v=beta&amp;t=5rAwLjF3EHyEDVvd1GHOAO65F-dKDYWy8gCvJ27vCF4" /></div>
</div>
<figcaption class="reader-image-block__figure-image-caption display-block full-width text-body-small-open t-sans text-align-center t-black--light"><span style="font-family: verdana;">Three qubits in superposition.[16]</span></figcaption></figure></div>
<div class="reader-image-block reader-image-block--full-width"><figure class="reader-image-block__figure"><div class="ivm-image-view-model"><div class="ivm-view-attr__img-wrapper"><img alt="" class="ivm-view-attr__img--centered reader-image-block__img evi-image lazy-image ember-view" id="ember888" src="https://media.licdn.com/dms/image/v2/D4E12AQGrCHC-cx2zUg/article-inline_image-shrink_1500_2232/article-inline_image-shrink_1500_2232/0/1733610877320?e=1740009600&amp;v=beta&amp;t=lHNGjay8jH78KrwCl9WJwEIHqXuIj4LIX6gviS9nVSM" /></div>
</div>
<figcaption class="reader-image-block__figure-image-caption display-block full-width text-body-small-open t-sans text-align-center t-black--light"><span style="font-family: verdana;">Marking one of the states by flipping its phase.[16]</span></figcaption></figure></div>
<p class="ember-view reader-text-block__paragraph" id="ember889"></p>
<ul><li><span style="font-family: verdana;">Using Grover's algorithm, we will amplify the probability of finding the marked states relative to other background states. I.e., the signal is separated from the noise. To do this we iteratively apply an oracle quantum circuit to the initial system.</span></li></ul><p style="color: black;"></p>
<div class="reader-image-block reader-image-block--full-width"><figure class="reader-image-block__figure"><div class="ivm-image-view-model"><div class="ivm-view-attr__img-wrapper"><img alt="" class="ivm-view-attr__img--centered reader-image-block__img evi-image lazy-image ember-view" id="ember890" src="https://media.licdn.com/dms/image/v2/D4E12AQHu1UQVg1dbqQ/article-inline_image-shrink_1500_2232/article-inline_image-shrink_1500_2232/0/1733610938287?e=1740009600&amp;v=beta&amp;t=-vmo3-9uS3FY-kCQiudQ-Ui0eEUyxl0Ao7k45IwSOMg" /></div>
</div>
<figcaption class="reader-image-block__figure-image-caption display-block full-width text-body-small-open t-sans text-align-center t-black--light"><span style="font-family: verdana;">The states, with our target amplified, after some number of iterations of Grover's algorithm.[16]</span></figcaption></figure></div>
<p class="ember-view reader-text-block__paragraph" id="ember891"></p>
<ul><li><span style="font-family: verdana;">Note that we use the same 16 qubits to encode all of the 16-digit strings in the dataset. Try that with a classical computer!</span></li><li><span style="font-family: verdana;">We then use Grover's algorithm again using a target string, applying the oracle against the now prepared quantum system, and returning the result that the target either is or is not in the dataset.</span></li></ul><p style="color: black;"></p>
<p class="ember-view reader-text-block__paragraph" id="ember892"><span style="font-family: verdana;">Hopefully this gives some flavor of what its like to program a quantum computer using an example most classical programmers today can understand. There are places where the code can potentially be improved - I welcome your input in the comments section below.</span></p>
<p class="ember-view reader-text-block__paragraph" id="ember892"><span style="font-family: verdana;"><br /></span></p>
<h2><span style="font-family: verdana;">AI AI, Oh.<span class="white-space-pre" face="var(--artdeco-reset-typography-font-family-sans)"> </span></span></h2><p class="ember-view reader-text-block__paragraph" id="ember894"><span style="font-family: verdana;">During these quantum coding explorations, during other coding work, and while writing documents (like this one), I've also been experimenting with a sequence of so-called "AI assistants". Starting with Copilot as a plugin for the popular VS Code IDE, I quickly switched to the Codeium plugin which performed better for my purposes, mostly because it took more of my code into context while making suggestions.[17] Since, the various vendors have leap-frogged each other in features and quality of results, and so while this tool comparison may be stale, its also the case that any new comparison would soon itself become stale.<span class="white-space-pre" face="var(--artdeco-reset-typography-font-family-sans)"> </span></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember895"><span style="font-family: verdana;">While the VS Code IDE provides hooks for plugins, some vendors, most notably Cursor [18] took the approach that to provide a true AI assistant (AIA) for a modern software developer you needed plugin hooks in many places, and control over things like the rendering of change sets. So they forked the VS Code IDE entirely. And they weren't the only ones to do this - Codeium now also has its own IDE called Windsurf - this is what I'm currently using. This can't be end state for the discussion and more leap-frogging is expected, so watch this space. I've now seen cases where people are putting "Cursor" on their resume - not a good idea to commit to that, but it is a good idea to start using one of these modern tools in your work, and to better understand how to adopt your work practices in light of their potential.</span></p>
<p class="ember-view reader-text-block__paragraph" id="ember896"><span style="font-family: verdana;">In other words, to experiment while these AIA tools improve. The head of Anthropic, a major AI player and the maker of the model driving my current IDE, is proud to say their model reaches 50% success rates on a popular software engineering metric.[19,20] Do<span class="white-space-pre" face="var(--artdeco-reset-typography-font-family-sans)"> </span><span face="var(--artdeco-reset-typography-font-family-sans)">you</span><span class="white-space-pre" face="var(--artdeco-reset-typography-font-family-sans)"> </span>want to pair program with an assistant which is only right 50% of the time? I can tell you that its got its ups and downs.</span></p>
<p class="ember-view reader-text-block__paragraph" id="ember897"><span style="font-family: verdana;">On the positive, for mundane tasks, it can do a pretty good job. Questions like "how do you do X in Python?" which you might have previously taken to Stack Overflow can now be answered in the IDE with custom-made code samples. You can turn your question into an instruction:<span class="white-space-pre" face="var(--artdeco-reset-typography-font-family-sans)"> </span></span></p>
<blockquote class="ember-view reader-text-block__blockquote" id="ember898"><span style="color: #b6d7a8; font-family: courier;"><span face="var(--artdeco-reset-typography-font-family-sans)">Me</span>: modify this code to take the target string as a command line argument and default to the current target string if none is provided</span></blockquote><blockquote class="ember-view reader-text-block__blockquote" id="ember899"><span style="color: #cc0000; font-family: courier;"><span face="var(--artdeco-reset-typography-font-family-sans)">AIA</span>: I've modified the code to...</span></blockquote><p class="ember-view reader-text-block__paragraph" id="ember900"><span style="font-family: verdana;">And it will describe in words and code a working solution, albeit in my case one I found to be slightly over-engineered:</span></p>
<blockquote class="ember-view reader-text-block__blockquote" id="ember901"><span style="color: #b6d7a8; font-family: courier;"><span face="var(--artdeco-reset-typography-font-family-sans)">Me</span>: use sys to determine if a single arg was provided</span></blockquote><blockquote class="ember-view reader-text-block__blockquote" id="ember902"><span style="color: #cc0000; font-family: courier;"><span face="var(--artdeco-reset-typography-font-family-sans)">AIA</span>: I'll modify the code to use sys.argv for a simpler command line argument handling...</span></blockquote><p class="ember-view reader-text-block__paragraph" id="ember903"><span style="font-family: verdana;">In another interaction, we debated if an AIA-proposed solution was mathematically efficient, at one point:</span></p>
<blockquote class="ember-view reader-text-block__blockquote" id="ember904"><span style="color: #cc0000; font-family: courier;"><span face="var(--artdeco-reset-typography-font-family-sans)">AIA</span>: Would you like me to: 1) Implement a simpler but less efficient version that demonstrates the concept? 2) Or explore how to properly implement the quantum multiplexing needed for efficient database loading?</span></blockquote><blockquote class="ember-view reader-text-block__blockquote" id="ember905"><span style="color: #b6d7a8; font-family: courier;"><span face="var(--artdeco-reset-typography-font-family-sans)">Me</span>: yes, but make it in a new file</span></blockquote><p class="ember-view reader-text-block__paragraph" id="ember906"><span style="font-family: verdana;">And it did. The main reason for asking for a new file rather than change in place was that the AIA is still often wrong! And reviewing a large amount of inline change is hard for the human eye and mind.</span></p>
<blockquote class="ember-view reader-text-block__blockquote" id="ember907"><span style="color: #b6d7a8; font-family: courier;"><span face="var(--artdeco-reset-typography-font-family-sans)">Me</span>: but now your proposed solution doesn't load the database or even look at it!</span></blockquote><blockquote class="ember-view reader-text-block__blockquote" id="ember908"><span style="color: #cc0000; font-family: courier;"><span face="var(--artdeco-reset-typography-font-family-sans)">AIA</span>: Ah, you're absolutely right again!<span class="white-space-pre" face="var(--artdeco-reset-typography-font-family-sans)"> </span></span></blockquote><p class="ember-view reader-text-block__paragraph" id="ember909"><span style="font-family: verdana;">And it then goes on to suggest further changes, which you might wonder why those changes weren't proposed in the first place? Keep in mind, its only (proudly) right 50% of the time. It is, as we say, "generative AI" after all - it doesn't so much "hallucinate" - it is designed to make s***... I mean, "stuff" up. (The idea that this GPT-based technology such as it is would be even capable of producing generalized AI (AGI) is an open question, indeed.)</span></p>
<p class="ember-view reader-text-block__paragraph" id="ember910"><span style="font-family: verdana;">But these tools can still be useful, not just for single-line "tab" completions, but now as we see here, in higher level conversations with the programmer. It can help articulate requirements, and write test cases, and help drive CI/CD pipelines. And it will improve in scope and accuracy, and custom AIA models tuned for specific programming domains (e.g. quantum) already exist.[21]<span class="white-space-pre" face="var(--artdeco-reset-typography-font-family-sans)"> </span></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember911"><span style="font-family: verdana;">This is truly the death and rebirth of computer programming, as we have come to experience it. In 1975, IBM's Fred Brooks published the seminal book "The Mythical Man-Month" which described, among other things, the software team which one would want to wrap around a senior technical engineer - a team of as many as 10 specialized professionals to handle the documentation, testing, business communications, and more common technical tasks so your senior contributor ("the surgeon") can focus on great ideas and great architecture.[22] But however, in today's DevOps culture, where we expect our senior engineers to be "full stack", to do it all, to play all roles, the AI tooling brings back some sanity and reminds that there are tasks best left delegated.</span></p>
<p class="ember-view reader-text-block__paragraph" id="ember911"><span style="font-family: verdana;"><br /></span></p>
<h2><span style="font-family: verdana;">2025</span></h2><p class="ember-view reader-text-block__paragraph" id="ember913"><span style="font-family: verdana;">It's that time in the calendar when everyone offers up their forecasts for the coming year. I'll not wade into that. My only prediction is this - that in 2025, the rate of change in key emerging and difficult to humanly understand (nearly black-box) technologies like AI and quantum computing will continue to accelerate, in many cases, beyond our ability to comprehend or predict. This is the so-called singularity, and it's evolving and emerging during a period also marked by political, military, and economic upheaval.<span class="white-space-pre" face="var(--artdeco-reset-typography-font-family-sans)"> </span></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember914"><span style="font-family: verdana;">Surf's up. Happy New Year. - andy</span></p>
<p class="ember-view reader-text-block__paragraph" id="ember915"><br /></p>
<h2><span style="font-family: verdana;">References</span></h2><p class="ember-view reader-text-block__paragraph" id="ember917"><span style="font-family: verdana;">[0] Photo by Ben Wicks on Unsplash</span></p>
<p class="ember-view reader-text-block__paragraph" id="ember918"><span style="font-family: verdana;">[1] SC24 schedule:<span class="white-space-pre" face="var(--artdeco-reset-typography-font-family-sans)"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://sc24.conference-program.com/" target="_self">https://sc24.conference-program.com/</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember919"><span style="font-family: verdana;">[2] El Capitan hardware overview:<span class="white-space-pre" face="var(--artdeco-reset-typography-font-family-sans)"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://hpc.llnl.gov/documentation/user-guides/using-el-capitan-systems/hardware-overview" target="_self">https://hpc.llnl.gov/documentation/user-guides/using-el-capitan-systems/hardware-overview</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember920"><span style="font-family: verdana;">[3] "Breaking the Million-Electron and 1 EFLOP/s Barriers: Biomolecular-Scale Ab Initio Molecular Dynamics Using MP2 Potentials",<span class="white-space-pre" face="var(--artdeco-reset-typography-font-family-sans)"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://dl.acm.org/doi/pdf/10.1109/SC41406.2024.00015" target="_self">https://dl.acm.org/doi/pdf/10.1109/SC41406.2024.00015</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember921"><span style="font-family: verdana;">[4] NVIDIA SC24 superchip press release:<span class="white-space-pre" face="var(--artdeco-reset-typography-font-family-sans)"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://www.datacenterdynamics.com/en/news/nvidia-announces-new-gb200-nvl4-superchip-at-sc24-but-says-theres-still-value-to-be-found-in-grace-hopper/" target="_self">https://www.datacenterdynamics.com/en/news/nvidia-announces-new-gb200-nvl4-superchip-at-sc24-but-says-theres-still-value-to-be-found-in-grace-hopper/</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember922"><span style="font-family: verdana;">[5] Tracking CHIPS Act funding:<span class="white-space-pre" face="var(--artdeco-reset-typography-font-family-sans)"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://www.semiconductors.org/chips-incentives-awards/" target="_self">https://www.semiconductors.org/chips-incentives-awards/</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember923"><span style="font-family: verdana;">[6] NVIDIA Grace Hopper architecture:<span class="white-space-pre" face="var(--artdeco-reset-typography-font-family-sans)"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://developer-blogs.nvidia.com/wp-content/uploads/2022/11/grace-hopper-overview.png" target="_self">https://developer-blogs.nvidia.com/wp-content/uploads/2022/11/grace-hopper-overview.png</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember924"><span style="font-family: verdana;">[7] "Exploring quantum use cases for the aerospace industry", IBM white paper,<span class="white-space-pre" face="var(--artdeco-reset-typography-font-family-sans)"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://www.ibm.com/thought-leadership/institute-business-value/en-us/report/quantum-aerospace" target="_self">https://www.ibm.com/thought-leadership/institute-business-value/en-us/report/quantum-aerospace</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember925"><span style="font-family: verdana;">[8] IonQ with NVIDIA SC24 press release:<span class="white-space-pre" face="var(--artdeco-reset-typography-font-family-sans)"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://ionq.com/news/ionq-to-advance-hybrid-quantum-computing-with-new-chemistry-application-and" target="_self">https://ionq.com/news/ionq-to-advance-hybrid-quantum-computing-with-new-chemistry-application-and</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember926"><span style="font-family: verdana;">[9] Microsoft and Atom quantum press releease:<span class="white-space-pre" face="var(--artdeco-reset-typography-font-family-sans)"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://azure.microsoft.com/en-us/blog/quantum/2024/11/19/microsoft-and-atom-computing-offer-a-commercial-quantum-machine-with-the-largest-number-of-entangled-logical-qubits-on-record/" target="_self">https://azure.microsoft.com/en-us/blog/quantum/2024/11/19/microsoft-and-atom-computing-offer-a-commercial-quantum-machine-with-the-largest-number-of-entangled-logical-qubits-on-record/</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember927"><span style="font-family: verdana;">[10] Alice &amp; Bob logical qubit lib press release :<span class="white-space-pre" face="var(--artdeco-reset-typography-font-family-sans)"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://alice-bob.com/newsroom/logical-qubit-emulator-felis-quantum-cloud-alice-bob/" target="_self">https://alice-bob.com/newsroom/logical-qubit-emulator-felis-quantum-cloud-alice-bob/</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember928"><span style="font-family: verdana;">[11] Quantinuum stack press release:<span class="white-space-pre" face="var(--artdeco-reset-typography-font-family-sans)"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://www.quantinuum.com/blog/announcing-the-launch-of-quantinuum-nexus-our-all-in-one-quantum-computing-platform" target="_self">https://www.quantinuum.com/blog/announcing-the-launch-of-quantinuum-nexus-our-all-in-one-quantum-computing-platform</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember929"><span style="font-family: verdana;">[12] RPI's experiments with HPC and quantum co-scheduling:<span class="white-space-pre" face="var(--artdeco-reset-typography-font-family-sans)"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://www.ibm.com/quantum/blog/supercomputing-24" target="_self">https://www.ibm.com/quantum/blog/supercomputing-24</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember930"><span style="font-family: verdana;">[13] "Combining quantum processors with real-time classical communication", Nature, Nov 2024,<span class="white-space-pre" face="var(--artdeco-reset-typography-font-family-sans)"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://www.nature.com/articles/s41586-024-08178-2" target="_self">https://www.nature.com/articles/s41586-024-08178-2</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember931"><span style="font-family: verdana;">[14] Algorithm Zoo:<span class="white-space-pre" face="var(--artdeco-reset-typography-font-family-sans)"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://quantumalgorithmzoo.org/" target="_self">https://quantumalgorithmzoo.org/</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember932"><span style="font-family: verdana;">[15] Pattern match example code:<span class="white-space-pre" face="var(--artdeco-reset-typography-font-family-sans)"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://github.com/agallojr/research-notes/blob/02253900f33d784402f0cd0b3ed4d9d360544605/quantum/src/qiskit/pattern_match.py" target="_self">https://github.com/agallojr/research-notes/blob/02253900f33d784402f0cd0b3ed4d9d360544605/quantum/src/qiskit/pattern_match.py</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember933"><span style="font-family: verdana;">[16] "QC — Grover’s algorithm", J. Hui,<span class="white-space-pre" face="var(--artdeco-reset-typography-font-family-sans)"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://jonathan-hui.medium.com/qc-grovers-algorithm-cd81e61cf248" target="_self">https://jonathan-hui.medium.com/qc-grovers-algorithm-cd81e61cf248</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember934"><span style="font-family: verdana;">[17] Codeium Windsurf IDE:<span class="white-space-pre" face="var(--artdeco-reset-typography-font-family-sans)"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://codeium.com/windsurf" target="_self">https://codeium.com/windsurf</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember935"><span style="font-family: verdana;">[18] Cursor IDE:<span class="white-space-pre" face="var(--artdeco-reset-typography-font-family-sans)"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://www.cursor.com/" target="_self">https://www.cursor.com/</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember936"><span style="font-family: verdana;">[19] Dario Amodei, CEO Anthropic, on Lex Fridman podcast:<span class="white-space-pre" face="var(--artdeco-reset-typography-font-family-sans)"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://www.youtube.com/watch?v=ugvHCXCOmm4&amp;t=20s&amp;pp=ygULbGV4IGZyaWRtYW4%3D" target="_self">https://www.youtube.com/watch?v=ugvHCXCOmm4&amp;t=20s&amp;pp=ygULbGV4IGZyaWRtYW4%3D</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember937"><span style="font-family: verdana;">[20] SWE-bench:<span class="white-space-pre" face="var(--artdeco-reset-typography-font-family-sans)"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://www.swebench.com/" target="_self">https://www.swebench.com/</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember938"><span style="font-family: verdana;">[21] IBM Qiskit Code Assistant:<span class="white-space-pre" face="var(--artdeco-reset-typography-font-family-sans)"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://www.ibm.com/quantum/blog/qiskit-code-assistant" target="_self">https://www.ibm.com/quantum/blog/qiskit-code-assistant</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember939"><span style="font-family: verdana;">[22] "The Mythical Man-Month", Fred Brooks, 1975:<span class="white-space-pre" face="var(--artdeco-reset-typography-font-family-sans)"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://web.eecs.umich.edu/~weimerw/2018-481/readings/mythical-man-month.pdf" target="_self">https://web.eecs.umich.edu/~weimerw/2018-481/readings/mythical-man-month.pdf</a></span></p>
<br class="Apple-interchange-newline" /></div>
</div>
</div>
</div>
</div>]]></content><author><name>Surfing the Singularity</name></author><category term="surfthesing" /><summary type="html"><![CDATA[Hello and happy holidays to all. In this blog installment I'll report back from SuperComputing 2024, offer up a programmer-friendly view of the quantum computing space with a code tour of Grover's algorithm, and share some of my own thoughts on using the latest crop of AI programmer assistant tools. (Sadly, not this Grover.) It was a pleasant SC24 high performance computing (HPC) conference in November. Having attended in past either in-person (Atlanta this year) or virtual, this year I chose virtual again. The big loss was being unable to troll the enormous vendor hall, but otherwise, webcasts make it much easier to be in two places at one time or to skim topics of passing interest.[1] There's a new top HPC machine (that we know of) - El Capitan, and its powered by AMD, containing about a million CPU cores and about 10 million GPU cores.[2] Molecular dynamics papers presented using a GPU-accelerated exascale computer reminds that, quantum aside for a moment, the real work is still being done in the classical world.[3] NVIDIA showcased the growing fusion of AI and HPC with their "superchip" designs - incorporating a CPU and a GPU on the same chip.[4] And why not, the money keeps flowing, the current outgoing federal administration now locking in the CHIPS Act funding before the end of the term.[5] NVIDIA's Grace Hopper architecture [6] But beyond the incremental improvements in compute, storage, cooling, power consumption and the like, it seemed to me, through my remote goggles, that the real action was happening on the sidelines of SC24, in the quantum computing space. Quantum Chip on Shoulder There's significant skepticism of quantum computing from the HPC community at the moment. Quantum computers today are toys in comparison to HPC, and stakeholders in HPC and classical computing (which would include myself) might wonder aloud "what's the point?" For some applications (like the fluid dynamics apps my company uses), quantum utility is perhaps still a decade away.[7] But the groundwork is being laid today, and when we understand that there are useful problems which can be solved on quantum computers, and only on quantum computers, we might at least allow the playing to continue. And we might be surprised at how fast quantum computing is progressing, and also just churning. Even those in the industry are hedging their bets - on which qubit technologies and which companies will be the winners - and as such are changing partners on a regular basis [8,9,10], although this sometimes means needing to unproductively reinvent the wheel (how many Python quantum circuit libraries do we need?)[11] A couple of new announcements from Big Blue caught my eye. First, an early demonstrator of incorporating an IBM quantum computer into an HPC data center, unifying the resource scheduling, was shown at RPI with their AiMOS cluster and their first-in-the-nation academic installation.[12] The second, and more important, was the paper in Nature demonstrating the union of two quantum computers via classical networks, providing another avenue for scaling up hybrid quantum computing.[13] But today's QPUs are still noisy, fragile, small, expensive, and scarce. Maturity is still a ways off. QPUs are not fungible - to be successful in executing an application on a quantum computer, we must understand the error profile of that specific device! Not just that brand of quantum computer product, but this instance of that product! We need hands-on examples to grow the personal and team experience with quantum while we await stabilized hardware and the productivity-enabling software abstractions which can only come after industry maturity, and these early (head-butting) experiences. Making Your Quantum BonesToday's education in and around quantum computing is still focused on the experimental audience. We are still doing experiments about quantum more than experiments with quantum. The educational material which does exist, and there is an increasing amount, is focused on a community which is very comfortable with quantum physics and its associated mathematics. Most people, myself included, do not fit this description. As a computer science graduate, adjunct prof, and software engineer by trade, I want to see higher level programming abstractions, not those centered around qubits and gates which clearly does not scale for large programs. We will be waiting a while. So in the meantime, we need to see some examples using today's technology and syntax but which are more suited for the Comp Sci student audience, to help begin to bridge that gap. If we visit the Algorithm Zoo [14], a collection of quantum algorithms which show a computational advantage over similar classical approaches, we find some things (but not many) which might look familiar to a CS undergrad, but the implementations, when they exist, are often broken. In this current early phase of the quantum era, vendors are playing free and loose with their SDKs, and releases with significant object model refactorings and breaking changes are the norm. pattern_match.py So I offer here an example of a quantum program which itself likely has a short shelf life. You can find it here.[15, &amp; above] It shows a cooked up example of using Grover's algorithm, which is a quantum algorithm for search in unstructured (e.g. unsorted) data. Classically, "cuz Murphy's Law", you might need to walk the entire dataset to find the item of interest (or show its not there) - we call this an O(n) algorithm. Using Grover's algorithm, you can do the search with a strong probability of success in O(sqrt(n)) time - a quadratic speedup, and one worth pursuing for many applications. Note we are only saying we can perform the search with high probability - this is quantum computing, and everything is a probability. And while the example shown isn't necessarily the most common application of Grover, understanding Grover is worthwhile, as it appears as a sub-procedure in many other quantum algorithms. Here are a few key points to take away, even if you don't take the time to look at the documented code example: We're going to store a small set of strings - binary strings of 1's and 0's representing a small dataset - into a quantum system of qubits. Since our strings are chosen to be 16 binary digits long, we will use 16 qubits. This is a large enough number of qubits to show some non-trivial problems, but not so many as to not be runnable on a simulator on your laptop. (The performance does not scale linearly with the number of qubits.)A system of n qubits contains 2**n possible states (combinations of 0's and 1's). That's a lot. We will have far fewer state strings in our dataset - just a handful for this experiment.After initializing the qubits, we will mark each state in the quantum system which corresponds to a binary string in our dataset. To do this we will use the phase of the qubit, which is an extra useful lever you do not find in classical bits (among other unique quantum advantages). Three qubits in superposition.[16] Marking one of the states by flipping its phase.[16] Using Grover's algorithm, we will amplify the probability of finding the marked states relative to other background states. I.e., the signal is separated from the noise. To do this we iteratively apply an oracle quantum circuit to the initial system. The states, with our target amplified, after some number of iterations of Grover's algorithm.[16] Note that we use the same 16 qubits to encode all of the 16-digit strings in the dataset. Try that with a classical computer!We then use Grover's algorithm again using a target string, applying the oracle against the now prepared quantum system, and returning the result that the target either is or is not in the dataset. Hopefully this gives some flavor of what its like to program a quantum computer using an example most classical programmers today can understand. There are places where the code can potentially be improved - I welcome your input in the comments section below. AI AI, Oh. During these quantum coding explorations, during other coding work, and while writing documents (like this one), I've also been experimenting with a sequence of so-called "AI assistants". Starting with Copilot as a plugin for the popular VS Code IDE, I quickly switched to the Codeium plugin which performed better for my purposes, mostly because it took more of my code into context while making suggestions.[17] Since, the various vendors have leap-frogged each other in features and quality of results, and so while this tool comparison may be stale, its also the case that any new comparison would soon itself become stale. While the VS Code IDE provides hooks for plugins, some vendors, most notably Cursor [18] took the approach that to provide a true AI assistant (AIA) for a modern software developer you needed plugin hooks in many places, and control over things like the rendering of change sets. So they forked the VS Code IDE entirely. And they weren't the only ones to do this - Codeium now also has its own IDE called Windsurf - this is what I'm currently using. This can't be end state for the discussion and more leap-frogging is expected, so watch this space. I've now seen cases where people are putting "Cursor" on their resume - not a good idea to commit to that, but it is a good idea to start using one of these modern tools in your work, and to better understand how to adopt your work practices in light of their potential. In other words, to experiment while these AIA tools improve. The head of Anthropic, a major AI player and the maker of the model driving my current IDE, is proud to say their model reaches 50% success rates on a popular software engineering metric.[19,20] Do you want to pair program with an assistant which is only right 50% of the time? I can tell you that its got its ups and downs. On the positive, for mundane tasks, it can do a pretty good job. Questions like "how do you do X in Python?" which you might have previously taken to Stack Overflow can now be answered in the IDE with custom-made code samples. You can turn your question into an instruction: Me: modify this code to take the target string as a command line argument and default to the current target string if none is providedAIA: I've modified the code to...And it will describe in words and code a working solution, albeit in my case one I found to be slightly over-engineered: Me: use sys to determine if a single arg was providedAIA: I'll modify the code to use sys.argv for a simpler command line argument handling...In another interaction, we debated if an AIA-proposed solution was mathematically efficient, at one point: AIA: Would you like me to: 1) Implement a simpler but less efficient version that demonstrates the concept? 2) Or explore how to properly implement the quantum multiplexing needed for efficient database loading?Me: yes, but make it in a new fileAnd it did. The main reason for asking for a new file rather than change in place was that the AIA is still often wrong! And reviewing a large amount of inline change is hard for the human eye and mind. Me: but now your proposed solution doesn't load the database or even look at it!AIA: Ah, you're absolutely right again! And it then goes on to suggest further changes, which you might wonder why those changes weren't proposed in the first place? Keep in mind, its only (proudly) right 50% of the time. It is, as we say, "generative AI" after all - it doesn't so much "hallucinate" - it is designed to make s***... I mean, "stuff" up. (The idea that this GPT-based technology such as it is would be even capable of producing generalized AI (AGI) is an open question, indeed.) But these tools can still be useful, not just for single-line "tab" completions, but now as we see here, in higher level conversations with the programmer. It can help articulate requirements, and write test cases, and help drive CI/CD pipelines. And it will improve in scope and accuracy, and custom AIA models tuned for specific programming domains (e.g. quantum) already exist.[21] This is truly the death and rebirth of computer programming, as we have come to experience it. In 1975, IBM's Fred Brooks published the seminal book "The Mythical Man-Month" which described, among other things, the software team which one would want to wrap around a senior technical engineer - a team of as many as 10 specialized professionals to handle the documentation, testing, business communications, and more common technical tasks so your senior contributor ("the surgeon") can focus on great ideas and great architecture.[22] But however, in today's DevOps culture, where we expect our senior engineers to be "full stack", to do it all, to play all roles, the AI tooling brings back some sanity and reminds that there are tasks best left delegated. 2025It's that time in the calendar when everyone offers up their forecasts for the coming year. I'll not wade into that. My only prediction is this - that in 2025, the rate of change in key emerging and difficult to humanly understand (nearly black-box) technologies like AI and quantum computing will continue to accelerate, in many cases, beyond our ability to comprehend or predict. This is the so-called singularity, and it's evolving and emerging during a period also marked by political, military, and economic upheaval. Surf's up. Happy New Year. - andy References[0] Photo by Ben Wicks on Unsplash [1] SC24 schedule: https://sc24.conference-program.com/ [2] El Capitan hardware overview: https://hpc.llnl.gov/documentation/user-guides/using-el-capitan-systems/hardware-overview [3] "Breaking the Million-Electron and 1 EFLOP/s Barriers: Biomolecular-Scale Ab Initio Molecular Dynamics Using MP2 Potentials", https://dl.acm.org/doi/pdf/10.1109/SC41406.2024.00015 [4] NVIDIA SC24 superchip press release: https://www.datacenterdynamics.com/en/news/nvidia-announces-new-gb200-nvl4-superchip-at-sc24-but-says-theres-still-value-to-be-found-in-grace-hopper/ [5] Tracking CHIPS Act funding: https://www.semiconductors.org/chips-incentives-awards/ [6] NVIDIA Grace Hopper architecture: https://developer-blogs.nvidia.com/wp-content/uploads/2022/11/grace-hopper-overview.png [7] "Exploring quantum use cases for the aerospace industry", IBM white paper, https://www.ibm.com/thought-leadership/institute-business-value/en-us/report/quantum-aerospace [8] IonQ with NVIDIA SC24 press release: https://ionq.com/news/ionq-to-advance-hybrid-quantum-computing-with-new-chemistry-application-and [9] Microsoft and Atom quantum press releease: https://azure.microsoft.com/en-us/blog/quantum/2024/11/19/microsoft-and-atom-computing-offer-a-commercial-quantum-machine-with-the-largest-number-of-entangled-logical-qubits-on-record/ [10] Alice &amp; Bob logical qubit lib press release : https://alice-bob.com/newsroom/logical-qubit-emulator-felis-quantum-cloud-alice-bob/ [11] Quantinuum stack press release: https://www.quantinuum.com/blog/announcing-the-launch-of-quantinuum-nexus-our-all-in-one-quantum-computing-platform [12] RPI's experiments with HPC and quantum co-scheduling: https://www.ibm.com/quantum/blog/supercomputing-24 [13] "Combining quantum processors with real-time classical communication", Nature, Nov 2024, https://www.nature.com/articles/s41586-024-08178-2 [14] Algorithm Zoo: https://quantumalgorithmzoo.org/ [15] Pattern match example code: https://github.com/agallojr/research-notes/blob/02253900f33d784402f0cd0b3ed4d9d360544605/quantum/src/qiskit/pattern_match.py [16] "QC — Grover’s algorithm", J. Hui, https://jonathan-hui.medium.com/qc-grovers-algorithm-cd81e61cf248 [17] Codeium Windsurf IDE: https://codeium.com/windsurf [18] Cursor IDE: https://www.cursor.com/ [19] Dario Amodei, CEO Anthropic, on Lex Fridman podcast: https://www.youtube.com/watch?v=ugvHCXCOmm4&amp;t=20s&amp;pp=ygULbGV4IGZyaWRtYW4%3D [20] SWE-bench: https://www.swebench.com/ [21] IBM Qiskit Code Assistant: https://www.ibm.com/quantum/blog/qiskit-code-assistant [22] "The Mythical Man-Month", Fred Brooks, 1975: https://web.eecs.umich.edu/~weimerw/2018-481/readings/mythical-man-month.pdf]]></summary></entry><entry><title type="html">SC’24 recap</title><link href="https://hpc.social/personal-blog/2024/sc-24-recap/" rel="alternate" type="text/html" title="SC’24 recap" /><published>2024-12-02T07:30:00-07:00</published><updated>2024-12-02T07:30:00-07:00</updated><id>https://hpc.social/personal-blog/2024/sc-24-recap</id><content type="html" xml:base="https://hpc.social/personal-blog/2024/sc-24-recap/"><![CDATA[<p>The premiere annual conference of the high-performance computing community, SC24, was held in Atlanta last week, and
    it attracted a record-shattering number of attendees--<a href="https://www.hpcwire.com/2024/11/20/sc24-half-way-there/">nearly 18,000 registrants</a>, up 28% from last
    year! The conference <i>felt</i> big as well, and there seemed to be a lot more running between sessions, meetings,
    and the exhibition floor. Despite its objectively bigger size though, the content of the conference felt more diffuse this year, and I was left wondering if this reflected my own biases or was a real effect of the AI industry
    beginning to overflow into AI-adjacent technology conferences like SC.</p>
<div class="separator" style="clear: both; text-align: center;"><figure>
</figure></div>
<p></p>
<p>Of course, this isn't to say that SC24 was anything short of a great conference. Some exciting new technologies were
    announced, a new supercomputer beat out Frontier to become the fastest supercomputer on the Top500 list, and I got
    to catch up with a bunch of great people that I only get to see at shows like this. I'll touch on all of these
    things below. But this year felt different from previous SC conferences to me, and I'll try to talk about that too.</p>
<p>There's no great way to arrange all the things I jotted down in my notes, but I've tried to arrange them by what readers may be interested in. Here's the table of contents:</p>
<p></p>
<ol>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#approach">My approach to SC this year</a></li>
<li>
<a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#tech">New technology and announcements</a>
<ol>
<li>
<a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#tech-top500">Top500 and a new #1 system</a>
<ol>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#tech-top500-elcap">#1 - El Capitan</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#tech-top500-hpc6">#5 - Eni HPC6</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#tech-top500-softbank">#16 and #17 - SoftBank CHIE-2 and CHIE-3</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#tech-top500-jeti">#18 - Jülich's JUPITER Exascale Transition Instrument (JETI)</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#tech-top500-reindeer">#32 - Reindeer!</a></li>
</ol>
</li>
<li>
<a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#tech-expo">Technology on the exhibit floor</a>
<ol>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#tech-expo-gb200">GB200</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#tech-expo-ss400">Slingshot 400</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#tech-expo-gg">Grace-Grace for storage?</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#tech-expo-hbv5">Microsoft and AMD's new HBM CPU</a></li>
</ol>
</li>
</ol>
</li>
<li>
<a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#industry">The HPC industry overall</a>
<ol>
<li>
<a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#industry-attendee">What I learned about the average SC technical program attendee</a>
<ol>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#industry-attendee-sustainability">People think sustainability and energy efficiency are the same thing</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#industry-attendee-ai">AI sessions are really scientific computing sessions about AI</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#industry-attendee-ops">AI for operations is not yet real in scientific computing</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#industry-attendee-hyperscale">Some are beginning to realize that HPC exists outside of scientific computing</a></li>
</ol>
</li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#industry-nsf">NSF's broad front vs. DOE's big bets in HPC and AI</a></li>
<li>
<a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#industry-expo">Exhibitor trends</a>
<ol>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#industry-expo-booths">Booths by the numbers</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#industry-expo-gpuaas">Proliferation of GPU-as-a-Service providers</a></li>
</ol>
</li>
</ol>
</li>
<li>
<a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#community">Community and connections</a><ol>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#community-people">Getting to know people</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#community-career">Talking to early career people</a></li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#community-bsky">Shift in social media</a></li>
</ol>
</li>
<li><a href="https://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#conclusion">So what's the takeaway?</a></li>
</ol>
<p>Before getting into the details though, I should explain how my perspective shaped what I noticed (and missed) through the conference. And to be clear: <b><i><span style="color: #cc0000;">these are my own personal opinions and do not necessarily reflect those of my employer</span></i></b>. Although Microsoft covered the cost for me to attend SC, I wrote this blog post during my own free time over the Thanksgiving holiday, and nobody had any editorial control over what follows except me.</p>
<p></p>
<h2 id="approach">My approach to SC this year</h2>
<p>Although this is the eleventh SC conference I've attended, it was the first time that I:</p>
<p></p>
<ol>
<li>attended as a <a href="https://blog.glennklockwood.com/2024/08/how-has-life-after-leaving-labs-been.html#hpc-ai-development">practitioner
            of hyperscale AI</a> rather than traditional HPC and scientific computing</li>
<li>attended as a Microsoft engineer (I represented Microsoft as a <a href="https://blog.glennklockwood.com/2024/08/how-has-life-after-leaving-labs-been.html#storage-product-management">product manager</a> at
        SC22 and SC23)</li>
<li>did not attend SC as a designated storage person (since 2013)</li>
</ol>
<p>Because of these changes in my <b><span style="color: #990000;">identity</span></b> as an attendee, I approached the
    conference with a different set of <b><span style="color: #0b5394;">goals</span></b> in mind:</p>
<p>As a <b><span style="color: #990000;">hyperscale/AI person</span></b>, I felt that I should
    prioritize <b><span style="color: #0b5394;">attending all the cloud and AI sessions</span></b> whenever forced to choose between one session or another. I chose to focus on understanding the traditional HPC community's understanding of hyperscale and AI, which meant I had to spend less time in the workshops, panels and BOFs where I built my career.</p>
<p>As an <b><span style="color: #990000;">engineer</span></b> rather than a product manager,
    it wasn't my primary responsibility to run private briefings and gather HPC customers' requirements and feedback. Instead, I prioritized only those meetings where my first-hand
    knowledge of how massive-scale AI training works could have a meaningful impact. This meant I <b><span style="color: #0b5394;">focused on partners and practitioners who also operate in the realm of
            hyperscale</span></b>--think massive, AI-adjacent companies and the HPC centers who have historically
    dominated the very top of the Top500 list.
</p>
<p>One thing I didn't anticipate going into SC24 is that I've inherited a third identity: there are a new cohort of people in HPC who see me as a <b><span style="color: #990000;">long-time community
            member</span></b>. This resulted in a surprising amount of my time being spent <b><span style="color: #0b5394;">talking to students and early career practitioners</span></b> who were looking
    for advice.</p>
<p>These three identities and goals meant I don't many notes to share on the technical program, but I did capture more observations about broader trends in the HPC industry and community.</p>
<h2 id="tech">New technology and announcements</h2>
<div>HPC is all about cutting-edge technology, so that's a fine place to start talking about what was new.</div>
<h3 id="tech-top500">Top500 and a new #1 system</h3>
<p>A cornerstone of every SC conference is the release of the new Top500 list on Monday, and
    this is especially true on years when a new #1 supercomputer is announced. As was widely anticipated in the weeks
    leading up to SC24, El Capitan unseated Frontier as the new #1 supercomputer this year, posting an impressive <a href="https://www.top500.org/system/180307/">1.74 EFLOPS</a> of FP64. In addition though, Frontier grew a
    little (it added 400 nodes), there was a notable new #5 system (Eni's HPC6), and a number of smaller systems appeared that are worth calling
    out.</p>
<h4 id="tech-top500-elcap">#1 - El Capitan</h4>
<p>The highlight of the Top500 list was undoubtedly the debut of El Capitan, Lawrence
    Livermore National Laboratory's massive new MI300A-based exascale supercomputer. Its 1.74 EF score resulted from a
    105-minute HPL run that came in under 30 MW, and a bunch of technical details about the system were disclosed by
    Livermore Computing's CTO, Bronis de Supinski, during an invited talk during the Top500 BOF. Plenty of others
    summarize the system's speeds and feeds (e.g., see <a href="https://www.nextplatform.com/2024/11/18/el-capitan-supercomputer-blazes-the-trail-for-converged-cpu-gpu-compute/">The
        Next Platform's article on El Cap</a>), so I won't do that. However, I will comment on how unusual Bronis' talk
    was.</p>
<p>Foremost, the El Capitan talk seemed haphazard and last-minute. Considering the system took over half a decade of planning and cost at least half a
    billion dollars, El Capitan's unveiling was the most unenthusiastic description of a brand-new #1 supercomputer I've
    ever seen. I can understand that the Livermore folks have debuted plenty of novel #1 systems in their careers, but El
    Capitan is objectively a fascinating system, and running a full-system job for nearly two hours across first-of-a-kind APUs
    is an amazing feat. If community leaders don't get excited about their own groundbreaking achievements, what kind of message should the next generation of HPC professionals take home?</p>
<p>In sharp contrast to the blasé announcement of this new system was the leading slide that was presented to describe the speeds and feeds of El Capitan:</p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>I've never seen a speaker take the main stage and put <i>a photo of himself</i> literally in the center of the slide, in front of the supercomputer they're talking about. I don't know what the communications people at Livermore were trying to do with this graphic, but I don't think it
    was intended to be evocative of the first thing that came to my mind:</p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>The supercomputer is literally named "The Captain," and there's a photo of one dude (the boss of Livermore Computing,
    who is also standing on stage giving the talk) blocking the view of the machine. It wasn't a great look, and it left me feeling very uneasy about what I was witnessing and what message it was sending to the HPC community.</p>
<p>In case it needs to be said, HPC is a team sport. The unveiling of El Capitan (or any other #1 system
    before it) is always the product of dozens, if not hundreds, of people devoting years of their professional lives to
    ensuring it all comes together. It was a big miss, both to those who put in the work, and those who will have
    to put in the work on future systems, to suggest that a single, smiling face comes before the success of the system deployment.
</p>
<h4 id="tech-top500-hpc6">#5 - Eni HPC6</h4>
<p>The other notable entrant to the Top 10 list was HPC6, an industry system deployed by Eni (a major Italian energy
    company) built on MI250X. Oil and gas companies tend to be conservative in the systems they buy since the seismic
    imaging done on their large supercomputers informs hundred-million to billion-dollar investments in drilling a new
    well, and they have much less tolerance for weird architectures than federally funded leadership computing does.
    Thus, Eni's adoption of AMD GPUs in this #5 system is a strong endorsement of their capability in mission-critical
    commercial computing.</p>
<h4 id="tech-top500-softbank">#16 and #17 - SoftBank CHIE-2 and CHIE-3</h4>
<p>SoftBank, the Japanese investment conglomerate who, among other things, owns a significant stake in Arm, made its <a href="https://www.top500.org/site/51045/">Top500 debut with two identical 256-node DGX H100 SuperPODs</a>. While
    not technologically interesting (H100 is getting old), these systems represent significant investment in HPC by
    private industry in Japan and signals that SoftBank is following the lead of large <a href="https://www.nytimes.com/2023/08/16/technology/ai-gpu-chips-shortage.html">American investment groups in
        building private AI clusters for the AI startups in their portfolios</a>. In doing this, SoftBank's investments
    aren't dependent on third-party cloud providers to supply the GPUs to make these startups successful and reduces
    their overall risk.</p>
<p>Although I didn't hear anything about these SoftBank systems at the conference, NVIDIA issued a press statement
    during the NVIDIA AI Summit Japan during the week prior to SC24 that discussed <a href="https://nvidianews.nvidia.com/news/nvidia-and-softbank-accelerate-japans-journey-to-global-ai-powerhouse">SoftBank's
        investment in large NVIDIA supercomputers</a>. The press statement states that these systems will be used "for
    [SoftBank's] own generative AI development and AI-related business, as well as that of universities, research
    institutions and businesses throughout Japan." The release also suggests we can expect B200 and GB200 SuperPODs from
    SoftBank to appear as those technologies come online.</p>
<h4 id="tech-top500-jeti">#18 - Jülich's JUPITER Exascale Transition Instrument (JETI)</h4>
<p>Just below the SoftBank systems was the precursor system to Europe's first exascale system. I was hoping that
    JUPITER, the full exascale system being deployed at FRJ, would appear in the Top 10, but it seems like we'll have to
    wait for ISC25 for that. Still, the JETI system ran HPL across 480 nodes of BullSequana XH3000, the same node that
    will be used in JUPITER, and achieved 83 TFLOPS. By comparison, the full JUPITER system will be over 10x larger ("<a href="https://www.fz-juelich.de/en/ias/jsc/jupiter/tech">roughly 6000 compute nodes</a>" in the Booster), and
    projecting the JETI run (173 TF/node) out to this full JUPITER scale indicates that JUPITER should just squeak over
    the 1.0 EFLOPS line.</p>
<p>In preparation for JUPITER, Eviden had a couple of these BullSequana XH3000 nodes out on display this year:</p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>And if you're interested in more, I've been tracking the technical details of <a href="https://glennklockwood.com/garden/systems/jupiter">JUPITER in my digital garden</a>.</p>
<h4 id="tech-top500-reindeer">#32 - Reindeer!</h4>
<p>Waay down the list was Microsoft's sole new Top500 entry this cycle, an NVIDIA H200 system that ran HPL over 120 ND
    H200 v5 nodes in Azure. It was one of only two conventional (non-Grace) H200 clusters that appeared in the top 100,
    and it had a pretty good efficiency (Rmax/Rpeak &gt; 80%). Microsoft also had a Reindeer node on display at its
    booth:</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>An astute observer may note that this node looks an awful lot like the H100 node used in its Eagle supercomputer,
    which was <a href="https://blog.glennklockwood.com/2023/11/sc23-recap.html">on display at SC23 last year</a>. That's
    because it's the same chassis, just with an upgraded HGX baseboard.</p>
<p>Reindeer was not <i>super</i> exciting, and there were no press releases about it, but I mention it here for a couple
    reasons:</p>
<p></p>
<ul>
<li>One of my teammates did the HPL run and submission, and his group got to come up with the name of the system for
        the purposes of HPL. As it turns out, generating a public name for a Top500 submission involves a comical amount
        of legal and marketing process when it comes from a giant corporation like Microsoft. And as it turns out,
        naming a cluster "Reindeer" has a low probability of offending anyone.</li>
<li>Reindeer is pretty boring--it's a relatively small cluster with a bunch of GPUs. But when you're building out AI
        infrastructure at a pace of <a href="https://build.microsoft.com/en-US/sessions/984ca69a-ffca-4729-bf72-72ea0cd8a5db">5x Eagles (70,000
            GPUs!) per month</a>, you want the clusters that those GPUs go into to be as boring, predictable, and
        automatable as possible. Seeing as how Reindeer only used 960 GPUs but still got #32, it doesn't require much
        math to realize that the big hyperscalers could flood the Top500 list with these cookie-cutter GPU clusters and
        (in this case) make any ranking below #32 completely irrelevant. Heaven help the Top500 list if they ever
        publish an API for submitting new systems; cloud providers' build validation automation could tack a Top500
        submission on at the end of burn-in and permanently ruin the list.</li>
</ul>
<div>On a personal note, the supercomputer grant that gave me my first job in the HPC business <a href="https://www.top500.org/system/177455/">debuted at #48</a>. It's mind-boggling that I now work in a place
    where standing up a #32 system is just day-to-day business.</div>
<p></p>
<h3 id="tech-expo">Technology on the exhibit floor</h3>
<p>The exhibit floor had a few new pieces of HPC technology on display this year that are
    worthy of mention, but a lot of the most HPC-centric exciting stuff actually had a soft debut at <a href="https://blog.glennklockwood.com/2024/05/isc24-recap.html">ISC24 in May</a>. For example, even though SC24 was MI300A's big splash due to
    the El Capitan announcement, some MI300A nodes (such as the <a href="https://glennklockwood.com/garden/nodes/cray-ex255a">Cray EX255a</a>) were on display in Hamburg. However,
    Eviden had their MI300A node (branded XH3406-3) on display at SC24 which was new to me:</p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>I'm unaware of anyone who's actually committed to a large Eviden MI300A system, so I was
    surprised to see that Eviden already has a full blade design. But as with Eni's HPC6 supercomputer, perhaps this is
    a sign that AMD's GPUs (and now APUs) have graduated from being built-to-order science experiments to a technology
    ecosystem that people will want to buy off the rack.</p>
<p>There was also a ton of GH200 on the exhibit hall floor, but again, these node types were
    also on display at ISC24. This wasn't a surprise since a bunch of upcoming European systems have invested in GH200
    already; in addition to JUPITER's 6,000 GH200 nodes described above, <a href="https://www.cscs.ch/computers/alps">CSCS Alps</a> has 2,688 GH200 nodes, and <a href="https://glennklockwood.com/garden/systems/isambard-ai">Bristol's Isambard-AI</a> will have 1,362 GH200
    nodes. All of these systems will have a 1:1 CPU:GPU ratio and an NVL4 domain, suggesting this is the optimal way to
    configure GH200 for HPC workloads. I didn't hear a single mention of GH200 NVL32.</p>
<h4 id="tech-expo-gb200">GB200</h4>
<p>SC24 was the debut of NVIDIA's Blackwell GPU in the flesh, and a bunch of integrators had
    material on GB200 out at their booths. Interestingly, they all followed the same pattern as GH200 with an NVL4
    domain size, and just about every smaller HPC integrator followed a similar pattern where</p>
<p></p>
<ul>
<li>their booth had a standard "NVIDIA Partner" (or "Preferred Partner!") placard on their main desk</li>
<li>they had a bare NVIDIA GB200 baseboard (superchip) on display</li>
<li>there wasn't much other differentiation</li>
</ul>
<p>From this, I gather that not many companies have manufactured GB200 nodes yet, or if they
    have, there aren't enough GB200 boards available to waste them on display models. So, we had to settle for these
    bare NVIDIA-manufactured, 4-GPU + 2-CPU superchip boards:</p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>What struck me is that these are very large FRUs--if a single component (CPU, GPU, voltage
    regulator, DRAM chip, or anything else) goes bad, you have to yank and replace four GPUs and two CPUs. And because
    all the components are soldered down, someone's going to have to do a lot of work to remanufacture these boards to
    avoid throwing out a lot of very expensive, fully functional Blackwell GPUs.</p>
<p>There were a few companies who were further along their GB200 journey and had more
    integrated nodes on display. The HPE Cray booth had this GB200 NVL4 blade (the Cray EX154n) on display:</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>It looks remarkably sparse compared to the super-dense blades that normally slot into the
    Cray EX line, but even with a single NVL4 node per blade, the Cray EX cabinet only supports 56 of these blades,
    leaving 8 blade slots empty in the optimal configuration. I assume this is a limitation of power and cooling.</p>
<p>The booth collateral around this blade suggested its use case is "machine learning and
    sovereign AI" rather than traditional HPC, and that makes sense since each node has 768 GB of HBM3e which is enough
    to support training some pretty large sovereign models. However, the choice to force all I/O traffic on to the
    high-speed network by only leaving room for one piddly node-local NVMe drive (this blade only supports one SSD per
    blade) will make training on this platform very sensitive to the quality of the global storage subsystem. This is
    great if you bundle this blade with all-flash Lustre (like Cray ClusterStor) or DAOS (handy, since <a href="https://bsky.app/profile/adrianjhpc.bsky.social/post/3lba4yfg5fc2a">Intel divested the entire DAOS
        development team to HPE</a>). But it's not how I would build an AI-optimized system.</p>
<p>I suspect the cost-per-FLOP of this Cray GB200 solution is much lower than what a pure-play
    GB200 for LLM training would be. And since GB200 is actually a solid platform for FP64 (thanks to Dan Ernst for <a href="https://bsky.app/profile/ernstdj.bsky.social/post/3lb23ipwnvc26">challenging me on this</a> and sharing
    some <a href="https://arxiv.org/abs/2411.12090">great resources on the topic</a>), I expect to see this node do well
    in situations that are not training frontier LLMs, but rather fine-tuning LLMs, training smaller models, and mixing
    in traditional scientific computing on the same general-purpose HPC/AI system.</p>
<p>Speaking of pure-play LLM training platforms, though, I was glad that very few exhibitors
    were trying to talk up GB200 NVL72 this year. It may have been the case that vendors simply aren't ready to begin
    selling NVL72 yet, but I like to be optimistic and instead believe that the exhibitors who show up to SC24 know that
    the scientific computing community likely won't get enough value out of a 72-GPU coherence domain to justify the
    additional cost and complexity of NVL72. I didn't see a single vendor with a GB200 NVL36 or NVL72 rack on display
    (or a GH200 NVL32, for that matter), and not having to think about NVL72 for the week of SC24 was a nice break from
    my day job.</p>
<p>Perhaps the closest SC24 got to NVL72 was a joint announcement at the beginning of the week
    by Dell and CoreWeave, who announced that <a href="https://www.coreweave.com/blog/coreweave-pushes-boundaries-with-gb200-and-more">they have begun bringing
        GB200 NVL72 racks online</a>. Dell did have a massive, AI-focused booth on the exhibit floor, and they did talk
    up their high-powered, liquid-cooled rack infrastructure. But in addition to supporting GB200 with NVLink Switches,
    I'm sure that rack infrastructure would be equally good at supporting nodes geared more squarely at traditional HPC.
</p>
<h4 id="tech-expo-ss400">Slingshot 400</h4>
<p>HPE Cray also debuted a new 400G Slingshot switch, appropriately named Slingshot 400. I
    didn't get a chance to ask anyone any questions about it, but from the marketing material that came out right before
    the conference, it sounds like a serdes upgrade without any significant changes to Slingshot's L2 protocol.</p>
<p>There was a Slingshot 400 switch for the Cray EX rack on display at their booth, and it
    looked pretty amazing:</p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>It looks way more dense than the original 200G Rosetta switch, and it introduces
    liquid-cooled optics. If you look closely, you can also see a ton of flyover cables connecting the switch ASIC in
    the center to the transceivers near the top; similar flyover cables are showing up in all manner of
    ultra-high-performance networking equipment, likely reflecting the inability to maintain signal integrity across PCB
    traces.</p>
<p>The port density on Slingshot 400 remains the same as it was on 200G Slingshot, so there's
    still only 64 ports per switch, and the fabric scale limits don't increase. In addition, the media is saying that
    Slingshot 400 (and the GB200 blade that will launch with it) won't start appearing until "<a href="https://www.nextplatform.com/2024/11/26/hpe-upgrades-supercomputer-lineup-top-to-bottom-in-2025/">Fall
        2025</a>." Considering 64-port 800G switches (like <a href="https://nvidianews.nvidia.com/news/networking-switches-gpu-computing-ai">NVIDIA's SN5600</a> and <a href="https://www.arista.com/en/company/news/press-release/19493-arista-unveils-etherlink-ai-networking-platforms">Arista's
        7060X6</a>) will have already been on the market by then though, Slingshot 400 will be launching with HPE Cray
    on its back foot.</p>
<p>However, there was a curious statement on the placard accompanying this Slingshot 400
    switch:</p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>It reads, "Ultra Ethernet is the future, HPE Slingshot delivers today!"</p>
<p>Does this suggest that Slingshot 400 is just a stopgap until 800G Ultra Ethernet NICs begin
    appearing? If so, I look forward to seeing HPE Cray jam third-party 800G switch ASICs into the Cray EX liquid-cooled
    form factor at future SC conferences.</p>
<h4 id="tech-expo-gg">Grace-Grace for storage?</h4>
<p>One of the weirder things I saw on the exhibit floor was a scale-out storage server built
    on NVIDIA Grace CPUs that the good folks at WEKA had on display at their booth.</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>Manufactured by Supermicro, this "ARS-121L-NE316R" server (really rolls off the tongue)
    uses a two-socket Grace superchip and its LPDDR5X instead of conventional, socketed CPUs and DDR. The rest of it
    seems like a normal scale-out storage server, with sixteen E3.S SSD slots in the front and four 400G ConnectX-7 or
    BlueField-3 NICs in the back. No fancy dual-controller failover or anything like that; the presumption is that
    whatever storage system you'd install over this server would implement its own erasure coding across drives and
    servers.</p>
<p>At a glance, this might seem like a neat idea for a compute-intensive storage system like
    WEKA or DAOS. However, one thing that you typically want in a storage server is high reliability and repairability,
    features which weren't the optimal design point for these Grace superchips. Specifically,</p>
<p></p>
<ul>
<li>The Grace-Grace superchip turn both CPU sockets into a single FRU. This means that if one CPU goes bad, you're
        shipping the whole board back to NVIDIA rather than just doing a field-swap of a socket.</li>
<li>Grace uses LPDDR5X, whose ECC is not as robust as DDR5. I'm not an expert on memory architecture, but my
        understanding is that the ECC scheme on Grace does not provide ChipKill or row failures. And as with CPU
        failure, if a single DRAM chip goes back, you're throwing out two CPUs and all the DRAM.</li>
<li>There's no way to value-engineer the exact quantity of cores, clock, and DRAM to be optimal for the storage
        software installed on top of these servers.</li>
</ul>
<p>On the upside, though, there might be a cost advantage to using this Grace-Grace server
    over a beefier AMD- or Intel-based server with a bunch of traditional DIMMs. And if you really like NVIDIA products,
    this lets you do NVIDIA storage servers to go with your NVIDIA network and NVIDIA compute. As long as your storage
    software can work with the interrupt rates of such a server (e.g., it supports rebuild-on-read) and the 144 Neoverse
    V2 cores are a good fit for its computational requirements (e.g., calculating complex erasure codes), this server
    makes sense. But building a parallel storage system on LPDDR5X still gives me the willies.</p>
<p>I could also see this thing being useful for certain analytics workloads, especially those
    which may be upstream of LLM training. I look forward to hearing about where this turns up in the field.</p>
<p></p>
<h4 id="tech-expo-hbv5">Microsoft and AMD's new HBM CPU</h4>
<p>The last bit of new and exciting HPC technology that I noted came from my very own employer
    in the form of HBv5, a new, monster four-socket node featuring custom-designed AMD CPUs with HBM. STH wrote up <a href="https://www.servethehome.com/this-is-the-microsoft-azure-hbv5-and-amd-mi300c-nvidia/">an article with
        great photos of HBv5 and its speeds and feeds</a>, but in brief, this single node has:</p>
<p></p>
<ul>
<li>384 physical Zen 4 cores (352 accessible from within the VM) that clock up to 4 GHz</li>
<li>512 GB of HBM3 (up to 450 GB accessible from the VM) with up to 6.9 TB/s STREAM bandwidth</li>
<li>4x NDR InfiniBand NICs clocked at 200G per port</li>
<li>200G Azure Boost NIC (160G accessible from the VM)</li>
<li>8x 1.84 TB NVMe SSDs with up to 50 GB/s read and 30 GB/s write bandwidth</li>
</ul>
<p></p>
<p>The node itself looks kind of wacky as well, because there just isn't a lot on it:</p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>There are the obvious four sockets of AMD EPYC 9V64H, each with 96 physical cores and 128 GB of HBM3, and giant heat
    pipes on top of them since it's 100% air-cooled. But there's no DDR at all, no power converter board (the node is
    powered by a DC bus bar), and just a few flyover cables to connect the PCIe add-in-card cages. There is a separate
    fan board with just two pairs of power cables connecting to the motherboard, and that's really about it.</p>
<p>The front end of the node shows its I/O capabilities which are similarly uncomplicated:</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>There are four NDR InfiniBand cards (one localized to each socket) which are 400G-capable but cabled up at 200G,
    eight E1.S NVMe drives, and a brand-new dual-port Azure Boost 200G NIC. Here's a close-up of the right third of the
    node's front:</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p>This is the first time I've seen an Azure Boost NIC in a server, and it looks
much better integrated than the previous-generation 100G Azure SmartNIC that put the FPGA and hard NIC on separate
boards connected by a funny little pigtail. This older 100G SmartNIC with pigtail was also on display at the Microsoft
booth in an ND MI300X v5 node:</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>And finally, although I am no expert in this new node, I did hang around the people who are all week, and I
    repeatedly heard them answer the same few questions:</p>
<p></p>
<ul>
<li><b>Is this MI300C?</b> It is if you want it to be. You can call it Sally if you want; I don't think it will
        care. But Microsoft calls it HBv5, and the processor name will show up as AMD EPYC 9V64H in /proc/cpuinfo.</li>
<li><b>Is its InfiniBand 1x800 port, 2x400 ports, ...?</b> There are four NDR InfiniBand HCA cards, and each card
        has one full 400G NDR InfiniBand port. However, each port is only connected up to top-of-rack switching at 200G.
        Each InfiniBand HCA hangs off of a different EPYC 9V64H socket so that any memory address can get to
        InfiniBand without having to traverse Infinity Fabric. Running four ports of NDR InfiniBand at half speed is an
        unusual configuration, but that's what's going on here.</li>
<li><b>How can I buy this CPU?</b> EPYC 9V64H are "<a href="https://techcommunity.microsoft.com/blog/azurehighperformancecomputingblog/announcing-azure-hbv5-virtual-machines-a-breakthrough-in-memory-bandwidth-for-hp/4303504">custom
            AMD EPYC processors only available in Azure</a>." This means the only way to access it is by provisioning an
        HBv5 virtual machine in Azure.</li>
</ul>
<div>Amidst all the unrelenting news about new GPUs optimized for AI workloads, it was nice to see something new and
    unique launched squarely for the benefit of traditional scientific computing workloads.</div>
<p></p>
<p></p>
<h2 id="industry">The HPC industry overall</h2>
<div>
<p>New technology announcements are always exciting, but one of the main reasons I attend
        SC and ISC is to figure out the broader trends shaping the HPC industry. What concerns are top of mind for the
        community, and what blind spots remain open across all the conversations happening during the week? Answering
        these questions requires more than just walking the exhibit floor; it involves interpreting the subtext of the
        discussions happening at panels and BOF sessions. However, identifying where the industry needs more information
        or a clearer picture informs a lot of the public-facing talks and activities in which I participate throughout
        the year.</p>
</div>
<h3 id="industry-attendee">What I learned about the average SC technical program attendee</h3>
<p>The biggest realization that I confirmed this week is that <b>the SC conference is not an HPC
        conference; it is a scientific computing conference</b>. I sat in a few sessions where the phrase "HPC
    workflows" was clearly a stand-in for "scientific workflows," and "performance evaluation" still really means "MPI
    and OpenMP profiling." I found myself listening to ideas or hearing about tools that were <em>intellectually</em>
    interesting but ultimately not useful to me because they
    were so entrenched in the traditions of applying HPC to scientific computing. Let's talk about a few ways in which
    this manifested.</p>
<h4 id="industry-attendee-sustainability">People think sustainability and energy efficiency are the same thing</h4>
<p>Take, for example, the topic of sustainability. There were talks, panels, papers, and BOFs
    that touched on the environmental impact of HPC throughout the week, but the vast majority of them really weren't
    talking about sustainability at all; they were talking about energy efficiency. These talks often use the following
    narrative:</p>
<p></p>
<ol>
<li>Energy use from datacenters is predicted to reach some ridiculous number by 2030</li>
<li>We must create more energy-efficient algorithms, processors, and scheduling policies</li>
<li>Here is an idea we tested that reduced the energy consumption without impacting the performance of some
        application or workflow</li>
<li>Sustainability achieved! Success!</li>
</ol>
<p>The problem with this approach is that it declares victory when energy consumption is
    reduced. This is a great result if all you care about is spending less money on electricity for your supercomputer,
    but it completely misses the much greater issue that the electricity required to power an HPC job is often generated
    by burning fossil fuels, and that the carbon emissions that are directly attributable to HPC workloads are
    contributing to global climate change. This blind spot was exemplified by this slide, presented during a talk titled
    "Towards Sustainable Post-Exascale Leadership Computing" at the Sustainable Supercomputing workshop:</p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>I've <a href="https://blog.glennklockwood.com/2024/11/fasst-will-be-does-opportunity-to-adapt.html">written about
        this before</a> and I'll write about it again: FLOPS/Watt and PUE are not
    meaningful metrics by themselves when talking about sustainability. A PUE of 1.01 is not helpful if the datacenter
    that achieves it relies on burning coal for its power. Conversely, a PUE of 1.5 is not bad if all that electricity
    comes from a zero-carbon energy source. The biggest issue that I saw being reinforced at SC this year is that
    claims of "sustainable HPC" are accompanied by the subtext of "as long as I can keep doing everything else the way I
    always have."</p>
<p>There were glimmers of hope, though. Maciej Cytowski from Pawsey presented the opening talk
    at the Sustainable Supercomputing workshop, and he led with the right thing--he acknowledged that 60% of
    the fuel mix that powers Pawsey's supercomputers comes from burning fossil fuels:</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>Rather than patting himself on the back at his low PUE, Dr. Cytowski's described on how
    they built their datacenter atop a large aquifer from which they draw water at 21°C and return it at 30°C to avoid
    using energy-intensive chillers. To further reduce the carbon impact of this water loop, Pawsey also installed over
    200 kW of solar panels on its facility roof to power the water pumps. Given the fact that Pawsey cannot relocate to
    somewhere with a higher ratio of zero-carbon energy on account of its need to be physically near the Square
    Kilometer Array, Cytowski's talk felt like the most substantive discussion on sustainability in HPC that week.</p>
<p>Most other talks and panels on the topic really wanted to equate "sustainability" to "FLOPS
    per Watt" and pretend like where one deploys a supercomputer is not a part of the sustainability discussion. The
    reality is that, if the HPC industry wanted to take sustainability seriously, it would talk less about watts and
    more about tons of CO<sub>2</sub>. Seeing as how the average watt of electricity in Tennessee produces <a href="https://www.epa.gov/egrid/data-explorer">2.75x more carbon</a> than a watt of electricity in Washington,
    the actual environmental impact of fine-tuning Slurm scheduling or fiddling with CPU frequencies is meaningless when
    compared to the benefits that would be gained by deploying that supercomputer next to a hydroelectric dam instead of
    a coal-fired power plant.</p>
<p>I say all this because there are parts of the HPC industry (namely, the part in which I work)
    who <i>are</i> serious about sustainability. And those conversations go beyond simply building supercomputers in
    places where energy is low-carbon (thereby reducing <a href="https://www.epa.gov/climateleadership/scope-1-and-scope-2-inventory-guidance">Scope 2 emissions</a>). They
    include holding suppliers to high standards on reducing the carbon impact of transporting people and material to
    these data centers, reducing the carbon impact of all the excess packaging that accompanies components, and being
    accountable for the impact of everything in the data center after it reaches end of life (termed <a href="https://www.epa.gov/climateleadership/scope-3-inventory-guidance">Scope 3 emissions</a>).</p>
<p>The HPC community--or more precisely, the scientific computing community--is still married
    to the idea that the location of a supercomputer is non-negotiable, and "sustainability" is a nice-to-have secondary
    goal. I was
    hoping that the sessions I attended on sustainability would approach this topic at a level where the
    non-scientific HPC world has been living. Unfortunately, the discussion at SC24, which spanned workshops, BOFs, and
    Green 500, remains largely stuck on the idea that PUE and FLOPS/Watt are the end-all sustainability metrics. Those
    metrics are important, but there are global optimizations that have much greater effects on reducing the
    environmental impact of the HPC industry.</p>
<h4 id="industry-attendee-ai">AI sessions are really scientific computing sessions about AI</h4>
<p>Another area where "HPC" was revealed to really mean "scientific computing" was in the
    topic of AI. I sat in on a few BOFs and panels around AI topics to get a feel for where this community is in
    adopting AI for science, but again, I found the level of discourse to degrade to generic AI banter despite the best
    efforts of panelists and moderators. For example, I sat in the "Foundational Large Language Models for
    High-Performance Computing" BOF session, and Jeff Vetter very clearly defined what a "foundational large language
    model" was at the outset so we could have a productive discussion about their applicability in HPC (or, really,
    scientific computing):</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>The panelists did a good job of outlining their positions. On the upside, LLMs are good for
    performing source code conversion, documenting and validating code, and maximizing continuity in application codes
    that get passed around as graduate students come and go. On the downside, they have a difficult time creating
    efficient parallel code, and they struggle to debug parallel code. And that's probably where the BOF should have
    stopped, because LLMs, as defined at the outset of the session, don't actually have a ton of applicability in
    scientific computing. But as soon as the session opened up to audience questions, the session went off the rails.
</p>
<p>The first question was an extremely basic and nonspecific question: "Is AI a bubble?"</p>
<p>It's fun to ask provocative questions to a panel of experts. I get it. But the question had
    nothing to do with LLMs, any of the position statements presented by panelists, or even HPC or scientific computing.
    It turned a BOF on "LLMs for HPC" into a BOF that might as well have been titled "Let's just talk about AI!" A few
    panelists tried to get things back on track by talking about the successes of surrogate models to simulate physical
    processes, but this reduced the conversation to a point where "LLMs" really meant "any AI model" and "HPC" really
    meant "scientific simulations."</p>
<p>Perhaps the most productive statement to come out of that panel was when Rio Yokota
    asserted that "we" (the scientific community) should not train their own LLMs, because doing so would be
    "unproductive for science." But I, as well as anyone who understands the difference between LLMs and "AI," already
    knew that. And the people who don't understand the difference between an LLM and a surrogate model probably didn't
    pick up on Dr. Yokota's statement, so I suspect the meaning of his contribution was completely lost.</p>
<p>Walking out of that BOF (and, frankly, the other AI-themed BOFs and panels I attended), I
    was disappointed at how superficial the conversation was. This isn't to say these AI sessions were objectively
    <i>bad</i>; rather, I think it reflects the general state of understanding of AI amongst SC attendees. Or perhaps it
    reflects the demographic that is drawn to these sorts of sessions. If the SC community is not ready to have a
    meaningful discussion about AI in the context of HPC or scientific computing, attending BOFs with like-minded peers
    is probably a good place to begin getting immersed.
</p>
<p>But what became clear to me this past week is that SC BOFs and panels with "AI" in their
    title aren't really meant for practitioners of AI. They're meant for scientific computing people who are beginning
    to dabble in AI.</p>
<h4 id="industry-attendee-ops">AI for operations is not yet real in scientific computing</h4>
<p>I was invited to sit on a BOF panel called "Artificial Intelligence and Machine Learning
    for HPC Workload Analysis" following on a successful BOF in which I participated at ISC24. The broad intent was to
    have a discussion around the tools, methods, and neat ideas that HPC practitioners have been using to better
    understand workloads, and each of us panelists was tasked with talking about a project or idea we had in applying
    AI/ML to improve some aspect of workloads.</p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>What emerged from us speakers' lightning talks is that applying AI for operations--in this
    case, understanding user workloads--is nascent. Rather than talking about how we use AI to affect how we design or
    operate supercomputers, all of us seemed to focus more on how we are collecting data and beginning to analyze that
    data using ML techniques. And maybe that's OK, because AI won't ever do anything for workload characterization until
    you have a solid grasp of the telemetry you can capture about those workloads in the first place.</p>
<p>But when we opened the BOF up to discussion with all attendees, despite having a packed
    room, there was very little that the audience had. Our BOF lead, Kadidia Konaté, tried to pull discussion out of the
    room from a couple of different fronts by asking what tools people were using, what challenges they were facing, and
    things along those lines. However, it seemed to me that the majority of the audience was in that room as spectators;
    they didn't know where to start applying AI towards understanding the operations of supercomputers. Folks attended
    to find out the art of the possible, not talk about their own challenges.</p>
<p>As such, the conversation wound up bubbling back up to the safety of traditional topics in
    scientific computing--how is LDMS working out, how do you deal with data storage challenges of collecting telemetry,
    and all the usual things that monitoring and telemetry folks worry about. It's easy to talk about the topics you
    understand, and just as the LLM conversation reverted back to generic AI for science and the sustainability topic
    reverted back to FLOPS/Watt, this topic of AI for operations reverted back to standard telemetry collection.</p>
<h4 id="industry-attendee-hyperscale">Some are beginning to realize that HPC exists outside of scientific computing</h4>
<p>Despite the pervasive belief at SC24 that "HPC" and "scientific computing" are the same thing, there are early signs
    that the leaders in the community are coming to terms with the reality that there is now a significant amount of
    leadership HPC happening outside the scope of the conference. This was most prominent at the part of the Top500 BOF
    where Erich Strohmaier typically discusses trends based on the latest publication of the list.</p>
<p>In years past, Dr. Strohmaier's talk was full of statements that strongly implied that, if a supercomputer is not
    listed on Top500, it simply does not exist. This year was different though: he acknowledged that El Capitan,
    Frontier, and Aurora were "the three exascale systems <u style="font-style: italic;">we are aware of</u>," now being
    clear that there is room for exascale systems to exist that simply never ran HPL, or never submitted HPL results to
    Top500. He explicitly acknowledged again that China has stopped making any Top500 submissions, and although he
    didn't name them outright, he spent a few minutes dancing around "hyperscalers" who have been deploying exascale
    class systems such as <a href="https://glennklockwood.com/garden/systems/meta's-h100-clusters">Meta's H100
        clusters</a> (2x24K H100), <a href="https://glennklockwood.com/garden/systems/colossus">xAI's
        Colossus</a> (100K H100), and the full system behind <a href="https://glennklockwood.com/garden/systems/eagle">Microsoft's Eagle</a> (14K H100 is a "<a href="https://build.microsoft.com/en-US/sessions/984ca69a-ffca-4729-bf72-72ea0cd8a5db">tiny fraction</a>").</p>
<p>Strohmaier did an interesting analysis that estimated the total power of the Top500 list's supercomputers so he could
    compare it to industry buzz around hyperscalers building gigawatt-sized datacenters:</p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>It was a fun analysis where he concluded that there are between 500-600 megawatts of supercomputers on the Top500
    list, and after you factor in storage, PUE, and other ancillary power sources, the whole Top500 list sums up to what
    hyperscalers are talking about sticking into a single datacenter facility.</p>
<p>Although he didn't say it outright, I think the implication here is that the Top500 list is rapidly losing relevance
    in the broad HPC market, because a significant amount of the world's supercomputing capacity <i>and capability</i>
    are absent from the list. Although specific hyperscale supercomputers (like Meta's, xAI's, and Microsoft's) were not
    mentioned outright, their absence from the Top500 list suggests that this list might already be more incomplete than
    it is complete--the sum of the FLOPS or power on the Top500 supercomputers may be less than the sum of the giant
    supercomputers which are known but not listed. This will only get worse as the AI giants keep building systems every
    year while the government is stuck on its 3-5 year procurement cycles.</p>
<p>It follows that the meaning of the Top500 is sprinting towards a place where it is not representative of HPC so much
    as it is representative of <i>the slice of HPC that serves scientific computing</i>. Erich Strohmaier was clearly
    aware of this in his talk this year, and I look forward to seeing how the conversation around the Top500 list
    continues to morph as the years go on.</p>
<h3 id="industry-nsf">NSF's broad front vs. DOE's big bets in HPC and AI</h3>
<p>My career was started at an NSF HPC center and <a href="https://blog.glennklockwood.com/2022/05/life-and-leaving-nersc.html">built up over my years in the
        DOE</a>, so I feel like I owe a debt to the people who provided all the opportunities and mentorship that let me
    get to the place of privilege in the hyperscale/AI industry that I now enjoy. As a result, I find myself still
    spending a lot of my free time thinking about <a href="https://glennklockwood.com/garden/government's-role-in-ai">the role of governments in the changing face of
        HPC</a> (as evidenced by my critiques of <a href="https://blog.glennklockwood.com/2024/10/a-critique-of-call-for-public-ai.html">thinktank reports</a> and <a href="https://blog.glennklockwood.com/2024/11/fasst-will-be-does-opportunity-to-adapt.html">federal RFIs</a>...) and trying to bridge the gap
    in technical understanding between my old colleagues (in DOE, NSF, and European HPC organizations) and whatever they
    call what I work on now (hyperscale AI?).</p>
<p>To that end, I found myself doing quite a bit of <i>business development</i> (more on this later) with government
    types since I think that is where I can
    offer the most impact. I used to be government, and I closely follow the state of their thinking in HPC, but I also
    know what's going on inside the hyperscale and AI world. I also have enough context in both areas to draw a line
    through all the buzzy AI press releases to demonstrate how the momentum of private-sector investment in AI might
    affect the way national HPC
    efforts do business. So, I did a lot of talking to both my old colleagues in DOE and their industry partners in an
    attempt to help them understand how the hyperscale and AI industry thinks about infrastructure, and what they should
    expect in the next year.</p>
<p>More importantly though, I also sat in on a couple of NSF-themed BOFs to get a better understanding of where their
    thinking is, where NAIRR is going, how the NSF's strategy contrasts with DOE's strategy, and where the ambitions of
    the Office of Advanced Cyberinfrastructure might intersect with the trajectory of hyperscale AI.</p>
<p>What I learned was that NSF leadership is aware of everything that the community should be concerned about: the
    growth of data, the increasing need for specialized silicon, the incursion of AI into scientific computing, new
    business models and relationships with industry, and broadening the reach of HPC investments to be globally
    competitive. But beyond that, I struggled to see a cohesive vision for the future of NSF-funded
    supercomputing. </p>
<p>A BOF with a broad range of stakeholders probably isn't the best place to lay out a vision for the future of NSF's
    HPC efforts, and perhaps NSF's vision is best expressed through its funding opportunities and awards. Whichever the
    case may be, it seems like the NSF remains on a path to make incremental progress on a broad front of topics. Its
    Advanced Computing Systems and Services (ACSS) program will continue to fund the acquisition of newer
    supercomputers, and a smorgasbord of other research programs will continue funding efforts across public access to
    open science, cybersecurity, sustainable software, and other areas. My biggest concern is that peanut-buttering
    funding across such a broad portfolio will make net forward progress much slower than taking big bets. Perhaps big
    bets just aren't in the NSF's mission though.</p>
<p>NAIRR was also a topic that came up in every NSF-themed session I attended, but again, I didn't get a clear picture
    of the future. Most of the discussion that I heard was around socializing the resources that are available today
    through NAIRR, suggesting that the pilot's biggest issue is not a lack of HPC resources donated by industry, but
    awareness that NAIRR is a resource that researchers can use. This was reinforced by a survey whose results were
    presented in the NAIRR BOF:</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>It seems like the biggest challenges facing the NSF community relying on NAIRR (which has its own sample bias) is
    that they don't really know where to start even though they have AI resources (both GPUs and model API services) at
    their disposal. In a sense, this is a great position for the NSF since</p>
<p></p>
<ol>
<li>its users need intellectual help more than access to GPU resources, and the NSF has been great at promoting
        education, training, and workforce development.</li>
<li>its users are unlikely to demand the same cutting-edge GPUs that AI industry leaders are snapping up. For
        example, the largest pool of GPUs in NAIRR are A100 GPUs that NVIDIA donated via DGX Cloud; the big AI
        companies moved off of Ampere a year ago and are about to move off of Hopper.</li>
</ol>
<p></p>
<p>However, it also means that there's not a clear role for partnership with many industry players beyond donating
    resources to the NAIRR pilot today in the hopes of selling resources to the full NAIRR tomorrow. I asked what OAC
    leadership thought about moving beyond such a transactional relationship between NSF and industry at one of the BOFs
    I attended, and while the panelists were eager to explore specific answers to that question, I didn't hear any ideas
    that would approach some sort of truly equitable partnership where both parties contributed in-kind.</p>
<p>I also walked away from these NSF sessions struck by how different the NSF HPC community's culture is from that of
    the DOE. NSF BOF attendees seemed focused on getting answers and guidance from NSF leadership, unlike the typical
    DOE gathering, where discussions often revolve around attendees trying to shape priorities to align with their own
    agendas. A room full of DOE people tends to feel like everyone thinks they're the smartest person there, while NSF
    gatherings appear more diverse in the expertise and areas of depth of its constituents. Neither way is inherently
    better or worse, but it will make the full ambition of NAIRR (as an inter-agency collaboration) challenging to
    navigate. This is particularly relevant as DOE is now pursuing its own multi-billion-dollar AI infrastructure
    effort, FASST, that appears to sidestep NAIRR.</p>
<h3 id="industry-expo">Exhibitor trends</h3>
<p>There's no better way to figure out what's going on in the HPC industry than walking the
    exhibit floor each year, because booths cost money and reflect the priorities (and budgets) of all participants.
    This year's exhibit felt physically huge, and walking from one end to the other was an adventure. You can get a
    sense of the scale from this photo I took during the opening gala:</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>Despite having almost 18,000 registrants and the opening gala usually being a
crush of people, the gala this year felt and looked very sparse just because people and booths were more spread out.
There was also a perceptibly larger number of splashy vendors who have historically never attended before who were
promoting downstream HPC technologies like data center cooling and electrical distribution, and there was healthy
speculation online about whether the hugeness of the exhibit this year was due to these new power and cooling companies.</p>
<p></p>
<p>To put these questions to rest, I figured out how to yank down all the exhibitor metadata
    from the conference website so I could do some basic analysis on it.</p>
<h4 id="industry-expo-booths">Booths by the numbers</h4>
<p>The easiest way to find the biggest companies to appear this year was to compare the
    exhibitor list and booth sizes from SC23 to this year and see whose booth went from zero to some big square footage.
</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>I only took the top twenty new vendors, but they broadly fall into a couple of categories:</p>
<p></p>
<ul>
<li><b>Power and cooling</b>: Stulz, Delta, Airedale, Valvoline, Boundary Electric, Schneider Electric, Mara
        </li>
<li><b>Server manufacturing</b>: Wistron, AMI, Pegatron</li>
<li><b>Higher ed</b>: Tennessee Tech, SCRCC</li>
</ul>
<p>There were a couple other companies that must've just missed last SC but aren't new to
        the show (NetApp, Ansys, Samsung, Micron, Broadcom). And curiously, only one new GPU-as-a-Service provider
        (Nebius) showed up this year, suggesting last year was the year of the GPU Cloud.</p>
<p>But to confirm what others had speculated: yes, a significant amount of the new square
        footage of the exhibit floor can be attributed to companies focused on power and cooling. This is an interesting
        indicator that HPC is becoming mainstream, largely thanks to AI demanding ultra-high density of power and
        cooling. But it's also heartening to see a few new exhibitors in higher education making an appearance. Notably,
        SCRCC (South Carolina Research Computing Consortium) is a consortium between Clemon, University of South
        Carolina, and Savannah River National Laboratory that just formed last year, and I look forward to seeing what
        their combined forces can bring to bear.</p>
<p>We can also take a look at whose booths grew the most compared to SC23:</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>This distribution is much more interesting, since the top 20 exhibitors who grew their footprint comprise the
        majority of the growth in existing exhibitors. Cherry-picking a few interesting growers:</p>
<p></p>
<ul>
<li><b>Power and cooling</b>: USystems, Midas, Vertiv</li>
<li><b>Data center/GPUaaS</b>: iM, Iris Energy, and (arguably) Oracle</li>
<li><b>Software</b>: Arc Compute and CIQ</li>
<li><b>Companies facing serious financial or legal troubles</b>: I count at least three! Impressive that they
            are still pouring money into their SC booths.</li>
</ul>
<p>It's also interesting to see HLRS, the German national HPC center, grow so
        significantly. I'm not sure what prompted such a great expansion, but I take it to mean that things have been
        going well there.</p>
<p>Finally, Dell had a massive booth and showing this year. Not only did they grow the
        most since SC23, but they had the single largest booth on the exhibit floor at SC24. This was no doubt a result
        of their great successes in partnering with NVIDIA to land massive GPU buildout deals at places like <a href="https://qz.com/dell-super-micro-computer-stock-elon-musk-ai-nvidia-1851550428">xAI</a> and <a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/dell-reaches-milestone-with-industrys-first-enterprise-ready-nvidia-blackwell-poweredge-xe9712-server-racks">CoreWeave</a>.
        They also had "AI factory" messaging emblazoned all over their marketing material and debuted a nice 200 kW
        liquid-cooled rack that will be the basis for their GB200 NVL72 solution, clearly leaning into the idea that
        they are leaders in AI infrastructure. Despite this messaging being off-beat for the SC audience as I've
        described earlier, their booth was surprisingly full all the time, and I didn't actually get a chance to get in
        there to talk to anyone about what they've been doing.</p>
<p>Equally interesting are the vendors who reduced their footprint at SC24 relative to
        SC23:</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>Reading too much into any of these big shrinkers is pretty easy; while a reduction in
        booth size could suggest business hasn't been as good, it could equally mean that an exhibitor just went
        overboard at SC23 and downsized to correct this year. A few noteworthy exhibitors to call out:</p>
<p></p>
<ul>
<li>Penguin and the Korea Semiconductor Industry Association both cut way back from massive 50x50 booths to
            30x30. Their booths this year were both big, but they weren't massive. Viridien, formerly known as CGG, also
            shrunk from a massive booth to a less-massive 30x40.</li>
<li>Juniper still kept an independent booth, but it is in the <a href="https://www.hpe.com/us/en/newsroom/press-release/2024/01/hpe-to-acquire-juniper-networks-to-accelerate-ai-driven-innovation.html">process
                of being absorbed into HPE</a>. Shrinking makes sense.</li>
<li>Major cloud providers Google and AWS scaled back, but Microsoft did not.</li>
<li>GPU-as-a-Service cloud providers CoreWeave and Lambda both scaled back. Since these GPUaaS providers'
            business models typically rely on courting few big customers, it may make sense to cut back on booth volume.
        </li>
<li>Major AI storage companies DDN, VAST, and (to a lesser degree) Pure also scaled back, while WEKA did not. I
            know business for DDN and VAST has been great this past year, so these may just reflect having gone
            overboard last year.</li>
</ul>
<p>Overall, almost twice as many vendors grew their booths than scaled back, so I'd
        caution anyone against trying to interpret any of this as anything beyond exhibitors right-sizing their booths
        after going all-in last year.</p>
<p>Finally, there are a handful of vendors who disappeared outright after SC23:</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>It is critical to point out that the largest booths to vanish outright were all on the
        smaller size: SUSE, Tenstorrent, and Symbiosys Alliance all disappeared this year, but their booths last year
        were only 20x30. I was surprised to see that Tenstorrent and Arm didn't have booths, but the others are either
        companies I haven't heard of (suggesting the return on investment of showing at SC might've been low), are easy
        to rationalize as only being HPC-adjacent (such as SNIA and DigitalOcean), or simply went bankrupt in the last
        year.</p>
<p>As we say at the business factory, the net-net of the exhibit hall this year is that
        the square footage of booth space increased by 15,000 square feet, so it was in fact bigger, it did take longer
        to walk from one end to the other, and there definitely were a bunch of new power and cooling companies filling
        out the space. Some exhibitors shrank or vanished, but the industry as a whole appears to be moving in a healthy
        direction.</p>
<p>And if you're interested in analyzing this data more yourself, please have a look at <a href="https://github.com/glennklockwood/sc-exhibitors">the data and the Jupyter notebook I used to generate
            the above treemaps on GitHub</a>. If you discover anything interesting, please write about it and post it
        online!</p>
<p></p>
<p></p>
<h4 id="industry-expo-gpuaas">Proliferation of GPU-as-a-Service providers</h4>
<p>As an AI infrastructure person working for a major cloud provider, I kept an eye out for all the companies trying
        to get into the GPU-as-a-Service game. <a href="https://blog.glennklockwood.com/2023/11/sc23-recap.html">I described these players last year as
            "pure-play GPU clouds,"</a> and it seems like the number of options available to customers who want to go
        this route is growing. But I found it telling that a lot of them had booths that were completely
        indistinguishable from each other. Here's an example of one:</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>As best I can tell, these companies are all NVIDIA preferred partners with
    data centers and a willingness to deploy NVIDIA GPUs, NVIDIA SmartNICs, and NVIDIA cloud stack, and sell multi-year
    commitments to consume those GPUs. I tried to accost some of these companies' booth staff to ask them my favorite
    question ("What makes you different from everyone else?"), but most of these companies' booths were staffed by
    people more interested in talking to each other than me.</p>
<p>These GPUaaS providers tend to freak me out, because, as Microsoft's CEO recently stated, these companies are
        often "<a href="https://www.microsoft.com/en-us/Investor/events/FY-2025/earnings-fy-2025-q1">just a bunch of
            tech companies still using VC money to buy a bunch of GPUs</a>." I can't help but feel like this is where
        the AI hype will come back to bite companies who have chosen to build houses upon sand. Walking the SC24 exhibit
        floor is admittedly a very narrow view of this line of business, but it seemed like some of these companies were
        content to buy up huge booths, hang a pretty banner above it, and otherwise leave the booth empty of anything
        beyond a few chairs and some generic value propositions. I didn't feel a lot of hunger or enthusiasm from these
        companies despite the fact that a bunch of them have hundreds of millions of dollars of GPUs effectively sitting
        on credit cards that they are going to have to make payments on for the next five years.</p>
<p>That all said, not all the companies in the GPUaaS are kicking back and letting the money pour in. In particular,
        I spent a few minutes chatting up someone at the CoreWeave booth, and I was surprised to hear about how much
        innovation they're adding on top of their conventional GPUaaS offering. For example, they developed <a href="https://docs.coreweave.com/coreweave-machine-learning-and-ai/training/sunk">Slurm on Kubernetes
            (SUNK)</a> with one of their key customers to close the gap between the fact that CoreWeave exposes its GPU
        service through Kubernetes, but many AI customers have built their stack around Slurm, <a href="https://github.com/NVIDIA/pyxis">pyxis</a>, and <a href="https://github.com/NVIDIA/enroot">enroot</a>.
    </p>
<p>In a weird twist of fate, I later ran into an old acquaintance who turned out to be one of the key CoreWeave
        customers for whom SUNK was developed. He commented that SUNK is the real deal and does exactly what his users
        need which, given the high standards that this person has historically had, is a strong affirmation that SUNK is
        more than just toy software that was developed and thrown on to GitHub for an easy press release. CoreWeave is
        also developing some interesting high-performance object storage caching software, and all of these software
        services are provided at no cost above whatever customers are already paying for their GPU service.</p>
<p>I bring this up because it highlights an emerging distinction in the GPUaaS market, which used to be a homogenous
        sea of bitcoin-turned-AI providers. Of course, many companies still rely on that simple business model: holding
        the bill for rapidly depreciating GPUs that NVIDIA sells and AI startups consume. However, there are now GPUaaS
        providers moving up the value chain by taking on the automation and engineering challenges that model developers
        don't want to deal with. Investing in uncertain projects like new software or diverse technology stacks is
        certainly risky, especially since they may never result in enough revenue to pay for themselves. But having a
        strong point of view, taking a stance, and investing in projects that you feel are right deserves recognition.
        My hat is off to the GPUaaS providers who are willing to take these risks and raise the tide for all of us
        rather than simply sling NVIDIA GPUs to anyone with a bag of money.</p>
<h2 id="community">Community and connections</h2>
<p>As much as I enjoy <i>increasing shareholder value</i>, the part of SC that gives me the
    greatest joy is reconnecting with the HPC community. Knowing I'll get to chat with my favorite people in the
    industry (and meet some new favorite people!) makes the long plane rides, upper respiratory infections, and weird
    hotel rooms completely worth it.</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>I wound up averaging under six hours of sleep per night this year in large part because 9pm
    or 7am were often the only free times I had to meet with people I really wanted to see. I have this unhealthy
    mindset where every hour of every day, from the day I land to the day I leave, is too precious to waste, and it's
    far too easy for me to rationalize that spending an hour talking to someone interesting is worth losing an hour of
    sleep.</p>
<p>But like I said at the outset of this blog post, this year felt different for a few
    reasons, and a lot of them revolve around the fact that I think I'm getting old. Now, it's always fun to say "I'm
    getting old" in a mostly braggadocious way, but this feeling manifested in concrete ways that affected the way I
    experienced the conference:</p>
<p></p>
<ol>
<li>I hit my limit on Monday night and couldn't get home without spending 15 minutes sitting in an unlit playground
        across from the World of Coke. I've always gotten blisters and fatigue, but this was the first time I couldn't
        just cowboy up and muscle through it. To avoid a repeat of this, I wound up "wasting" (see above) a lot more
        time to just get off my feet this year.</li>
<li>This year, I reached the point where I need to start time-box how much time I spend chatting up the folks I
        bump into. I used to just let the good times roll if I ran into someone I knew, but this year I wound up
        spending as much time attending sessions as I did missing sessions because I got caught up in a conversation.
        This isn't a bad thing per se, but I did feel a little sour when I realized I'd made a bad bet on choosing to
        chat instead of attending a session or vice versa, and this bad feeling lingered in the back of my mind just
        about every day.</li>
<li>There weren't a lot of surprises for me at the conference this year, and I worry that I am at risk of losing
        touch with the technical aspects of the conference that get newer attendees excited. Instead of hearing about,
        say, the latest research in interconnects, more of my time was spent mucking it up with the sorts of people in
        the HPC community who I used to find intimidating. On the one hand, hooray me for making it into old boys'
        clubs. But on the other, I don't want to become some HPC greybeard whose last meaningful contribution to the
        industry was twenty years ago.</li>
<li>This is the first year where I've had people accost me <i>and ask me for advice</i>. I've long been accosted by
        strangers because of my online presence, but those interactions were always lighthearted exchanges of "I follow
        you on Twitter" and "Great to meet you. Have an @HPC_Guru pin." This year, I had people specifically ask me for
        advice on industry versus postdoc, AI versus HPC, and what my master plan was when I left NERSC. Even though I
        didn't have any sage advice, I still found it really hard to tell bright-eyed students to go kick rocks just so
        I wouldn't be late for yet another mushy panel on AI.</li>
</ol>
<p>If you read this all and think "boo hoo, poor Glenn is too popular and wise for his own
    good," yeah, I get it. There are worse problems to have. But this was the first year where I felt like what I put
    into the conference was greater than what I got out of it. Presenting at SC used to be at least as good for my
    career as it was useful for my audiences, but it just doesn't count for much given my current role and career stage.
    It felt like some of the magic was gone this year in a way I've never experienced before. </p>
<p></p>
<h3 id="community-people">Getting to know people</h3>
<p>As the years have gone on, I spend an increasing amount of my week having one-on-one
    conversations instead of wandering aimlessly. This year though, I came to SC without really having anything to buy
    or sell:</p>
<p></p>
<ul>
<li>I am not a researcher, so I don't need to pump up the work I'm doing to impress my fellow researchers.</li>
<li>I no longer own a product market segment, so I don't directly influence the customers or vendors with whom my
        employer works.</li>
<li>I don't have any bandwidth in my day job to support any new customers or partnerships, so I don't have a strong
        reason to sell people on partnering with me or my employer. </li>
</ul>
<p>Much to my surprise though, a bunch of my old vendor/partner colleagues still wanted to get
    together to chat this year. Reflecting back, I was surprised to realize that it was these conversations--not the
    ones about business--that were the most fulfilling this year.</p>
<p>I learned about people's hobbies, families, and their philosophies on life, and it was
    amazing to get to know some of the people behind the companies with whom I've long dealt. I was reminded that the
    person is rarely the same as the company, and even behind some of the most aggressive and blusterous tech companies
    are often normal people with the same concerns and moments of self-doubt that everyone else has. I was also reminded
    that good engineers appreciate good engineering regardless of whether it's coming from a competitor or not. The
    public persona of a tech exec may not openly admire a competitor's product, but that doesn't mean they don't know
    good work when they see it.</p>
<p>I also surprised a colleague whose career has been in the DOE labs with an anecdote that
    amounted to the following: even though two companies may be in fierce competition, the people who work for them
    don't have to be. The HPC community is small enough that almost everyone has got a pal at a competing company, and
    when there are deals to be made, people looove to gossip. If one salesperson hears a juicy rumor about a prospective
    customer, odds are that everyone else on the market will hear about it pretty quickly too. Of course, the boundaries
    of confidentiality and professionalism are respected when it matters, but the interpersonal relationships that are
    formed between coworkers and friends don't suddenly disappear when people change jobs.</p>
<p>And so, I guess it would make sense that people still want to talk to me even though I have
    nothing to buy or sell. I love trading gossip just as much as everyone else, and I really enjoyed this aspect of the
    week.</p>
<p></p>
<h3 id="community-career">Talking to early career people</h3>
<p>I also spent an atypically significant amount of my week talking to early career people in
    HPC who knew of me one way or another and wanted career advice. This is the first year I recall having the same
    career conversations with multiple people, and this new phase of my life was perhaps most apparent during the IEEE
    TCHPC/TCPP HPCSC career panel in which I was invited to speak this year.</p>
<p></p>
<div class="separator" style="clear: both; text-align: center;"><figure></figure></div>
<p></p>
<p>It was an honor to be asked to present on a career panel, but I didn't feel very qualified to give career advice to
    up-and-coming computer science graduate students who want to pursue HPC. I am neither a computer scientist nor a
    researcher, but fortunately for me, my distinguished co-panelists (Drs. Dewi Yokelson, Olga Pearce, YJ Ji, and
    Rabab Alomairy) had plenty of more relevant wisdom to share. And at the end of the panel, there were a few things we
    all seemed to agree on as good advice:</p>
<p></p>
<ol>
<li>Knowing stuff is good, but being able to learn things is better. Being eager to learn and naturally curious
        makes this much easier as well.</li>
<li>The life of a researcher sometimes requires more than working a standard nine-to-five, so it'll be hard to be
        really successful if your heart isn't in it.</li>
<li><a href="https://quoteinvestigator.com/2014/04/06/they-feel/">People will forget what you did or what you said,
            but they remember how you made them feel</a>. Don't be a jerk, because this community is small.</li>
</ol>
<p></p>
<p>In both this panel the one-on-one conversations I had with early career individuals, the best I could offer was the
    truth: I never had a master plan that got me to where I am; I just try out new things until I realize I don't like
    doing them anymore. I never knew what I wanted to be when I grew up, and I still don't really, so it now makes me
    nervous that people have started approaching me with the assumption that I've got it all figured out. Unless I
    torpedo my career and go live on a goat farm though, maybe I should prepare for this to be a significant part of my
    SC experiences going forward.</p>
<h3 id="community-bsky">Shift in social media</h3>
<p>One last, big change in the community aspect of SC this year was the mass-migration of a ton of HPC folks from
    Twitter to Bluesky during the week prior to the conference. I don't really understand what prompted it so suddenly;
    a few of us have been trying for years to get some kind of momentum on other social platforms like Mastodon, but the
    general lack of engagement meant that all the excitement around SC always wound up exclusively on Twitter. This year
    was different though, and Bluesky hit critical mass with the HPC community.</p>
<p>I personally have never experienced an SC conference without Twitter; my first SC was in 2013, and part of what made
    that first conference so exciting was being able to pull up my phone and see what other people were seeing,
    thinking, and doing across the entire convention center via Twitter. Having the social media component to the
    conference made me feel like I was a part of something that first year, and as the years went on, Twitter became an
    increasingly indispensable part of the complete SC experience for me.</p>
<p>This year, though, I decided to <a href="https://x.com/glennklockwood/status/1857571101028790498">try an
        experiment</a> and see what SC would be like if I set Twitter aside and invested my time into Bluesky instead.
</p>
<p>The verdict? <i>It was actually pretty nice.</i></p>
<p>It felt a lot like the SC13 days, where my day ended and began with me popping open Bluesky to see what new <a href="https://bsky.app/hashtag/sc24">#SC24</a> posts were made. And because many of the tech companies and HPC
    centers hadn't yet made it over, the hashtag wasn't clogged up by a bunch of prescheduled marketing blasts that
    buried the posts written by regular old conference attendees who were <a href="https://bsky.app/profile/walkingrandomly.bsky.social/post/3lbazofprgc2y">asking important questions</a>:</p>
<blockquote class="bluesky-embed"><p lang="en">Which booths at #sc24 have coffee? I noticed oracle do. Anyone else?</p>
— Mike Croucher (<a href="https://bsky.app/profile/did:plc:sd6xejkhcmyehbscxb5lz3uq?ref_src=embed">@walkingrandomly.bsky.social</a>) <a href="https://bsky.app/profile/did:plc:sd6xejkhcmyehbscxb5lz3uq/post/3lbazofprgc2y?ref_src=embed">November 18, 2024 at 3:02 PM</a></blockquote>
<p>Of course, I still clogged Bluesky up with my nonsense during the week, but there was an amazing amount of
    engagement by a diversity of thoughtful people--many who came from Twitter, but some whose names and handles I
    didn't recognize.</p>
<p>The volume of traffic on Bluesky during the week did feel a little lower than what it had been on Twitter in years
    past though. I also didn't see as many live posts of technical sessions as they happened, so I couldn't really tell
    whether I was missing something interesting in real time. This may have contributed to why I felt a little less
    connected to the pulse of the conference this year than I had in the past. It also could've been the fact that
    conference was physically smeared out across a massive space though; the sparsity of the convention center was at
    least on par with the sparsity on Bluesky.</p>
<p>At the end of the week, I didn't regret the experiment. In fact, I'll probably be putting more effort into my Bluesky
    account than my Twitter account going forward. To be clear though, this isn't a particularly political decision on
    my part, and I pass no judgment on anyone who wants to use one platform over the other. It's just that I like the
    way I feel when I scroll through my Bluesky feeds, and I don't get that same feeling when I use Twitter.</p>
<h2 id="conclusion">So what's the takeaway?</h2>
<p>SC this year was a great conference by almost every measure, as it always is, but it still felt a little different for me. I'm sure that some of that feeling is the result of my own growth, and my role with respect to the conference seems to be evolving from someone who gets a lot out of the conference to someone who is giving more to the conference. That's not to say that I don't get a lot out of it, though; I had no shortage of wonderful interactions with everyone from technology executives to rising stars who are early in their career, and I learned a lot about both them and me as whole people. But SC24, more than any SC before it, is when I realized this change was happening.</p>
<p>On the technological front, we saw the debut of a new #1 system (emblazoned with the smiling face of Bronis...) and a growing crop of massive, new clusters deployed for commercial applications. The exhibit floor was quantitatively bigger, in large part due to new power and cooling companies who are suddenly relevant to the HPC world thanks to the momentum of AI. At the same time, the SC technical program is clearly separating itself out as a conference focused on scientific computing; the level of discourse around AI remains largely superficial compared to true AI conferences, the role of hyperscalers in the HPC industry is still cast more as a threat than an opportunity.</p>
<p>For my part, I'm still trying to get a grasp on where government agencies like DOE and NSF want to take their AI ambitions so I can try to help build a better mutual understanding between the scientific computing community and the hyperscale AI community. However, it seems like the NSF is progressing slowly on a wide front, while the DOE is doing what DOE does and charging headfirst into a landscape that has changed more than I think they realize.</p>
<p>There's a lot of technical content that I know I missed on account of the increasing time I've been spending on the people and community aspect of the conference, and I'm coming to terms with the idea that this just may be the way SC is from now on. And I think I'm okay with that, since the support of the community is what helped me go from being a bored materials science student into someone whose HPC career advice is worth soliciting in the short span of eleven years. Despite any or all of the cynicism that may come out in the things I say about this conference, SC is always the highlight of my year. I always go into it with excitement, gladly burn the candle at both ends all week, and fly home feeling both grateful for and humbled by everything the HPC community has done and continues to do to keep getting me out of bed in the morning.</p>
<p></p>]]></content><author><name>Glenn K. Lockwood&apos;s Blog</name></author><category term="glennklockwood" /><summary type="html"><![CDATA[The premiere annual conference of the high-performance computing community, SC24, was held in Atlanta last week, and it attracted a record-shattering number of attendees--nearly 18,000 registrants, up 28% from last year! The conference felt big as well, and there seemed to be a lot more running between sessions, meetings, and the exhibition floor. Despite its objectively bigger size though, the content of the conference felt more diffuse this year, and I was left wondering if this reflected my own biases or was a real effect of the AI industry beginning to overflow into AI-adjacent technology conferences like SC. Of course, this isn't to say that SC24 was anything short of a great conference. Some exciting new technologies were announced, a new supercomputer beat out Frontier to become the fastest supercomputer on the Top500 list, and I got to catch up with a bunch of great people that I only get to see at shows like this. I'll touch on all of these things below. But this year felt different from previous SC conferences to me, and I'll try to talk about that too. There's no great way to arrange all the things I jotted down in my notes, but I've tried to arrange them by what readers may be interested in. Here's the table of contents: My approach to SC this year New technology and announcements Top500 and a new #1 system #1 - El Capitan #5 - Eni HPC6 #16 and #17 - SoftBank CHIE-2 and CHIE-3 #18 - Jülich's JUPITER Exascale Transition Instrument (JETI) #32 - Reindeer! Technology on the exhibit floor GB200 Slingshot 400 Grace-Grace for storage? Microsoft and AMD's new HBM CPU The HPC industry overall What I learned about the average SC technical program attendee People think sustainability and energy efficiency are the same thing AI sessions are really scientific computing sessions about AI AI for operations is not yet real in scientific computing Some are beginning to realize that HPC exists outside of scientific computing NSF's broad front vs. DOE's big bets in HPC and AI Exhibitor trends Booths by the numbers Proliferation of GPU-as-a-Service providers Community and connections Getting to know people Talking to early career people Shift in social media So what's the takeaway? Before getting into the details though, I should explain how my perspective shaped what I noticed (and missed) through the conference. And to be clear: these are my own personal opinions and do not necessarily reflect those of my employer. Although Microsoft covered the cost for me to attend SC, I wrote this blog post during my own free time over the Thanksgiving holiday, and nobody had any editorial control over what follows except me. My approach to SC this year Although this is the eleventh SC conference I've attended, it was the first time that I: attended as a practitioner of hyperscale AI rather than traditional HPC and scientific computing attended as a Microsoft engineer (I represented Microsoft as a product manager at SC22 and SC23) did not attend SC as a designated storage person (since 2013) Because of these changes in my identity as an attendee, I approached the conference with a different set of goals in mind: As a hyperscale/AI person, I felt that I should prioritize attending all the cloud and AI sessions whenever forced to choose between one session or another. I chose to focus on understanding the traditional HPC community's understanding of hyperscale and AI, which meant I had to spend less time in the workshops, panels and BOFs where I built my career. As an engineer rather than a product manager, it wasn't my primary responsibility to run private briefings and gather HPC customers' requirements and feedback. Instead, I prioritized only those meetings where my first-hand knowledge of how massive-scale AI training works could have a meaningful impact. This meant I focused on partners and practitioners who also operate in the realm of hyperscale--think massive, AI-adjacent companies and the HPC centers who have historically dominated the very top of the Top500 list. One thing I didn't anticipate going into SC24 is that I've inherited a third identity: there are a new cohort of people in HPC who see me as a long-time community member. This resulted in a surprising amount of my time being spent talking to students and early career practitioners who were looking for advice. These three identities and goals meant I don't many notes to share on the technical program, but I did capture more observations about broader trends in the HPC industry and community. New technology and announcements HPC is all about cutting-edge technology, so that's a fine place to start talking about what was new. Top500 and a new #1 system A cornerstone of every SC conference is the release of the new Top500 list on Monday, and this is especially true on years when a new #1 supercomputer is announced. As was widely anticipated in the weeks leading up to SC24, El Capitan unseated Frontier as the new #1 supercomputer this year, posting an impressive 1.74 EFLOPS of FP64. In addition though, Frontier grew a little (it added 400 nodes), there was a notable new #5 system (Eni's HPC6), and a number of smaller systems appeared that are worth calling out. #1 - El Capitan The highlight of the Top500 list was undoubtedly the debut of El Capitan, Lawrence Livermore National Laboratory's massive new MI300A-based exascale supercomputer. Its 1.74 EF score resulted from a 105-minute HPL run that came in under 30 MW, and a bunch of technical details about the system were disclosed by Livermore Computing's CTO, Bronis de Supinski, during an invited talk during the Top500 BOF. Plenty of others summarize the system's speeds and feeds (e.g., see The Next Platform's article on El Cap), so I won't do that. However, I will comment on how unusual Bronis' talk was. Foremost, the El Capitan talk seemed haphazard and last-minute. Considering the system took over half a decade of planning and cost at least half a billion dollars, El Capitan's unveiling was the most unenthusiastic description of a brand-new #1 supercomputer I've ever seen. I can understand that the Livermore folks have debuted plenty of novel #1 systems in their careers, but El Capitan is objectively a fascinating system, and running a full-system job for nearly two hours across first-of-a-kind APUs is an amazing feat. If community leaders don't get excited about their own groundbreaking achievements, what kind of message should the next generation of HPC professionals take home? In sharp contrast to the blasé announcement of this new system was the leading slide that was presented to describe the speeds and feeds of El Capitan: I've never seen a speaker take the main stage and put a photo of himself literally in the center of the slide, in front of the supercomputer they're talking about. I don't know what the communications people at Livermore were trying to do with this graphic, but I don't think it was intended to be evocative of the first thing that came to my mind: The supercomputer is literally named "The Captain," and there's a photo of one dude (the boss of Livermore Computing, who is also standing on stage giving the talk) blocking the view of the machine. It wasn't a great look, and it left me feeling very uneasy about what I was witnessing and what message it was sending to the HPC community. In case it needs to be said, HPC is a team sport. The unveiling of El Capitan (or any other #1 system before it) is always the product of dozens, if not hundreds, of people devoting years of their professional lives to ensuring it all comes together. It was a big miss, both to those who put in the work, and those who will have to put in the work on future systems, to suggest that a single, smiling face comes before the success of the system deployment. #5 - Eni HPC6 The other notable entrant to the Top 10 list was HPC6, an industry system deployed by Eni (a major Italian energy company) built on MI250X. Oil and gas companies tend to be conservative in the systems they buy since the seismic imaging done on their large supercomputers informs hundred-million to billion-dollar investments in drilling a new well, and they have much less tolerance for weird architectures than federally funded leadership computing does. Thus, Eni's adoption of AMD GPUs in this #5 system is a strong endorsement of their capability in mission-critical commercial computing. #16 and #17 - SoftBank CHIE-2 and CHIE-3 SoftBank, the Japanese investment conglomerate who, among other things, owns a significant stake in Arm, made its Top500 debut with two identical 256-node DGX H100 SuperPODs. While not technologically interesting (H100 is getting old), these systems represent significant investment in HPC by private industry in Japan and signals that SoftBank is following the lead of large American investment groups in building private AI clusters for the AI startups in their portfolios. In doing this, SoftBank's investments aren't dependent on third-party cloud providers to supply the GPUs to make these startups successful and reduces their overall risk. Although I didn't hear anything about these SoftBank systems at the conference, NVIDIA issued a press statement during the NVIDIA AI Summit Japan during the week prior to SC24 that discussed SoftBank's investment in large NVIDIA supercomputers. The press statement states that these systems will be used "for [SoftBank's] own generative AI development and AI-related business, as well as that of universities, research institutions and businesses throughout Japan." The release also suggests we can expect B200 and GB200 SuperPODs from SoftBank to appear as those technologies come online. #18 - Jülich's JUPITER Exascale Transition Instrument (JETI) Just below the SoftBank systems was the precursor system to Europe's first exascale system. I was hoping that JUPITER, the full exascale system being deployed at FRJ, would appear in the Top 10, but it seems like we'll have to wait for ISC25 for that. Still, the JETI system ran HPL across 480 nodes of BullSequana XH3000, the same node that will be used in JUPITER, and achieved 83 TFLOPS. By comparison, the full JUPITER system will be over 10x larger ("roughly 6000 compute nodes" in the Booster), and projecting the JETI run (173 TF/node) out to this full JUPITER scale indicates that JUPITER should just squeak over the 1.0 EFLOPS line. In preparation for JUPITER, Eviden had a couple of these BullSequana XH3000 nodes out on display this year: And if you're interested in more, I've been tracking the technical details of JUPITER in my digital garden. #32 - Reindeer! Waay down the list was Microsoft's sole new Top500 entry this cycle, an NVIDIA H200 system that ran HPL over 120 ND H200 v5 nodes in Azure. It was one of only two conventional (non-Grace) H200 clusters that appeared in the top 100, and it had a pretty good efficiency (Rmax/Rpeak &gt; 80%). Microsoft also had a Reindeer node on display at its booth: An astute observer may note that this node looks an awful lot like the H100 node used in its Eagle supercomputer, which was on display at SC23 last year. That's because it's the same chassis, just with an upgraded HGX baseboard. Reindeer was not super exciting, and there were no press releases about it, but I mention it here for a couple reasons: One of my teammates did the HPL run and submission, and his group got to come up with the name of the system for the purposes of HPL. As it turns out, generating a public name for a Top500 submission involves a comical amount of legal and marketing process when it comes from a giant corporation like Microsoft. And as it turns out, naming a cluster "Reindeer" has a low probability of offending anyone. Reindeer is pretty boring--it's a relatively small cluster with a bunch of GPUs. But when you're building out AI infrastructure at a pace of 5x Eagles (70,000 GPUs!) per month, you want the clusters that those GPUs go into to be as boring, predictable, and automatable as possible. Seeing as how Reindeer only used 960 GPUs but still got #32, it doesn't require much math to realize that the big hyperscalers could flood the Top500 list with these cookie-cutter GPU clusters and (in this case) make any ranking below #32 completely irrelevant. Heaven help the Top500 list if they ever publish an API for submitting new systems; cloud providers' build validation automation could tack a Top500 submission on at the end of burn-in and permanently ruin the list. On a personal note, the supercomputer grant that gave me my first job in the HPC business debuted at #48. It's mind-boggling that I now work in a place where standing up a #32 system is just day-to-day business. Technology on the exhibit floor The exhibit floor had a few new pieces of HPC technology on display this year that are worthy of mention, but a lot of the most HPC-centric exciting stuff actually had a soft debut at ISC24 in May. For example, even though SC24 was MI300A's big splash due to the El Capitan announcement, some MI300A nodes (such as the Cray EX255a) were on display in Hamburg. However, Eviden had their MI300A node (branded XH3406-3) on display at SC24 which was new to me: I'm unaware of anyone who's actually committed to a large Eviden MI300A system, so I was surprised to see that Eviden already has a full blade design. But as with Eni's HPC6 supercomputer, perhaps this is a sign that AMD's GPUs (and now APUs) have graduated from being built-to-order science experiments to a technology ecosystem that people will want to buy off the rack. There was also a ton of GH200 on the exhibit hall floor, but again, these node types were also on display at ISC24. This wasn't a surprise since a bunch of upcoming European systems have invested in GH200 already; in addition to JUPITER's 6,000 GH200 nodes described above, CSCS Alps has 2,688 GH200 nodes, and Bristol's Isambard-AI will have 1,362 GH200 nodes. All of these systems will have a 1:1 CPU:GPU ratio and an NVL4 domain, suggesting this is the optimal way to configure GH200 for HPC workloads. I didn't hear a single mention of GH200 NVL32. GB200 SC24 was the debut of NVIDIA's Blackwell GPU in the flesh, and a bunch of integrators had material on GB200 out at their booths. Interestingly, they all followed the same pattern as GH200 with an NVL4 domain size, and just about every smaller HPC integrator followed a similar pattern where their booth had a standard "NVIDIA Partner" (or "Preferred Partner!") placard on their main desk they had a bare NVIDIA GB200 baseboard (superchip) on display there wasn't much other differentiation From this, I gather that not many companies have manufactured GB200 nodes yet, or if they have, there aren't enough GB200 boards available to waste them on display models. So, we had to settle for these bare NVIDIA-manufactured, 4-GPU + 2-CPU superchip boards: What struck me is that these are very large FRUs--if a single component (CPU, GPU, voltage regulator, DRAM chip, or anything else) goes bad, you have to yank and replace four GPUs and two CPUs. And because all the components are soldered down, someone's going to have to do a lot of work to remanufacture these boards to avoid throwing out a lot of very expensive, fully functional Blackwell GPUs. There were a few companies who were further along their GB200 journey and had more integrated nodes on display. The HPE Cray booth had this GB200 NVL4 blade (the Cray EX154n) on display: It looks remarkably sparse compared to the super-dense blades that normally slot into the Cray EX line, but even with a single NVL4 node per blade, the Cray EX cabinet only supports 56 of these blades, leaving 8 blade slots empty in the optimal configuration. I assume this is a limitation of power and cooling. The booth collateral around this blade suggested its use case is "machine learning and sovereign AI" rather than traditional HPC, and that makes sense since each node has 768 GB of HBM3e which is enough to support training some pretty large sovereign models. However, the choice to force all I/O traffic on to the high-speed network by only leaving room for one piddly node-local NVMe drive (this blade only supports one SSD per blade) will make training on this platform very sensitive to the quality of the global storage subsystem. This is great if you bundle this blade with all-flash Lustre (like Cray ClusterStor) or DAOS (handy, since Intel divested the entire DAOS development team to HPE). But it's not how I would build an AI-optimized system. I suspect the cost-per-FLOP of this Cray GB200 solution is much lower than what a pure-play GB200 for LLM training would be. And since GB200 is actually a solid platform for FP64 (thanks to Dan Ernst for challenging me on this and sharing some great resources on the topic), I expect to see this node do well in situations that are not training frontier LLMs, but rather fine-tuning LLMs, training smaller models, and mixing in traditional scientific computing on the same general-purpose HPC/AI system. Speaking of pure-play LLM training platforms, though, I was glad that very few exhibitors were trying to talk up GB200 NVL72 this year. It may have been the case that vendors simply aren't ready to begin selling NVL72 yet, but I like to be optimistic and instead believe that the exhibitors who show up to SC24 know that the scientific computing community likely won't get enough value out of a 72-GPU coherence domain to justify the additional cost and complexity of NVL72. I didn't see a single vendor with a GB200 NVL36 or NVL72 rack on display (or a GH200 NVL32, for that matter), and not having to think about NVL72 for the week of SC24 was a nice break from my day job. Perhaps the closest SC24 got to NVL72 was a joint announcement at the beginning of the week by Dell and CoreWeave, who announced that they have begun bringing GB200 NVL72 racks online. Dell did have a massive, AI-focused booth on the exhibit floor, and they did talk up their high-powered, liquid-cooled rack infrastructure. But in addition to supporting GB200 with NVLink Switches, I'm sure that rack infrastructure would be equally good at supporting nodes geared more squarely at traditional HPC. Slingshot 400 HPE Cray also debuted a new 400G Slingshot switch, appropriately named Slingshot 400. I didn't get a chance to ask anyone any questions about it, but from the marketing material that came out right before the conference, it sounds like a serdes upgrade without any significant changes to Slingshot's L2 protocol. There was a Slingshot 400 switch for the Cray EX rack on display at their booth, and it looked pretty amazing: It looks way more dense than the original 200G Rosetta switch, and it introduces liquid-cooled optics. If you look closely, you can also see a ton of flyover cables connecting the switch ASIC in the center to the transceivers near the top; similar flyover cables are showing up in all manner of ultra-high-performance networking equipment, likely reflecting the inability to maintain signal integrity across PCB traces. The port density on Slingshot 400 remains the same as it was on 200G Slingshot, so there's still only 64 ports per switch, and the fabric scale limits don't increase. In addition, the media is saying that Slingshot 400 (and the GB200 blade that will launch with it) won't start appearing until "Fall 2025." Considering 64-port 800G switches (like NVIDIA's SN5600 and Arista's 7060X6) will have already been on the market by then though, Slingshot 400 will be launching with HPE Cray on its back foot. However, there was a curious statement on the placard accompanying this Slingshot 400 switch: It reads, "Ultra Ethernet is the future, HPE Slingshot delivers today!" Does this suggest that Slingshot 400 is just a stopgap until 800G Ultra Ethernet NICs begin appearing? If so, I look forward to seeing HPE Cray jam third-party 800G switch ASICs into the Cray EX liquid-cooled form factor at future SC conferences. Grace-Grace for storage? One of the weirder things I saw on the exhibit floor was a scale-out storage server built on NVIDIA Grace CPUs that the good folks at WEKA had on display at their booth. Manufactured by Supermicro, this "ARS-121L-NE316R" server (really rolls off the tongue) uses a two-socket Grace superchip and its LPDDR5X instead of conventional, socketed CPUs and DDR. The rest of it seems like a normal scale-out storage server, with sixteen E3.S SSD slots in the front and four 400G ConnectX-7 or BlueField-3 NICs in the back. No fancy dual-controller failover or anything like that; the presumption is that whatever storage system you'd install over this server would implement its own erasure coding across drives and servers. At a glance, this might seem like a neat idea for a compute-intensive storage system like WEKA or DAOS. However, one thing that you typically want in a storage server is high reliability and repairability, features which weren't the optimal design point for these Grace superchips. Specifically, The Grace-Grace superchip turn both CPU sockets into a single FRU. This means that if one CPU goes bad, you're shipping the whole board back to NVIDIA rather than just doing a field-swap of a socket. Grace uses LPDDR5X, whose ECC is not as robust as DDR5. I'm not an expert on memory architecture, but my understanding is that the ECC scheme on Grace does not provide ChipKill or row failures. And as with CPU failure, if a single DRAM chip goes back, you're throwing out two CPUs and all the DRAM. There's no way to value-engineer the exact quantity of cores, clock, and DRAM to be optimal for the storage software installed on top of these servers. On the upside, though, there might be a cost advantage to using this Grace-Grace server over a beefier AMD- or Intel-based server with a bunch of traditional DIMMs. And if you really like NVIDIA products, this lets you do NVIDIA storage servers to go with your NVIDIA network and NVIDIA compute. As long as your storage software can work with the interrupt rates of such a server (e.g., it supports rebuild-on-read) and the 144 Neoverse V2 cores are a good fit for its computational requirements (e.g., calculating complex erasure codes), this server makes sense. But building a parallel storage system on LPDDR5X still gives me the willies. I could also see this thing being useful for certain analytics workloads, especially those which may be upstream of LLM training. I look forward to hearing about where this turns up in the field. Microsoft and AMD's new HBM CPU The last bit of new and exciting HPC technology that I noted came from my very own employer in the form of HBv5, a new, monster four-socket node featuring custom-designed AMD CPUs with HBM. STH wrote up an article with great photos of HBv5 and its speeds and feeds, but in brief, this single node has: 384 physical Zen 4 cores (352 accessible from within the VM) that clock up to 4 GHz 512 GB of HBM3 (up to 450 GB accessible from the VM) with up to 6.9 TB/s STREAM bandwidth 4x NDR InfiniBand NICs clocked at 200G per port 200G Azure Boost NIC (160G accessible from the VM) 8x 1.84 TB NVMe SSDs with up to 50 GB/s read and 30 GB/s write bandwidth The node itself looks kind of wacky as well, because there just isn't a lot on it: There are the obvious four sockets of AMD EPYC 9V64H, each with 96 physical cores and 128 GB of HBM3, and giant heat pipes on top of them since it's 100% air-cooled. But there's no DDR at all, no power converter board (the node is powered by a DC bus bar), and just a few flyover cables to connect the PCIe add-in-card cages. There is a separate fan board with just two pairs of power cables connecting to the motherboard, and that's really about it. The front end of the node shows its I/O capabilities which are similarly uncomplicated: There are four NDR InfiniBand cards (one localized to each socket) which are 400G-capable but cabled up at 200G, eight E1.S NVMe drives, and a brand-new dual-port Azure Boost 200G NIC. Here's a close-up of the right third of the node's front: This is the first time I've seen an Azure Boost NIC in a server, and it looks much better integrated than the previous-generation 100G Azure SmartNIC that put the FPGA and hard NIC on separate boards connected by a funny little pigtail. This older 100G SmartNIC with pigtail was also on display at the Microsoft booth in an ND MI300X v5 node: And finally, although I am no expert in this new node, I did hang around the people who are all week, and I repeatedly heard them answer the same few questions: Is this MI300C? It is if you want it to be. You can call it Sally if you want; I don't think it will care. But Microsoft calls it HBv5, and the processor name will show up as AMD EPYC 9V64H in /proc/cpuinfo. Is its InfiniBand 1x800 port, 2x400 ports, ...? There are four NDR InfiniBand HCA cards, and each card has one full 400G NDR InfiniBand port. However, each port is only connected up to top-of-rack switching at 200G. Each InfiniBand HCA hangs off of a different EPYC 9V64H socket so that any memory address can get to InfiniBand without having to traverse Infinity Fabric. Running four ports of NDR InfiniBand at half speed is an unusual configuration, but that's what's going on here. How can I buy this CPU? EPYC 9V64H are "custom AMD EPYC processors only available in Azure." This means the only way to access it is by provisioning an HBv5 virtual machine in Azure. Amidst all the unrelenting news about new GPUs optimized for AI workloads, it was nice to see something new and unique launched squarely for the benefit of traditional scientific computing workloads. The HPC industry overall New technology announcements are always exciting, but one of the main reasons I attend SC and ISC is to figure out the broader trends shaping the HPC industry. What concerns are top of mind for the community, and what blind spots remain open across all the conversations happening during the week? Answering these questions requires more than just walking the exhibit floor; it involves interpreting the subtext of the discussions happening at panels and BOF sessions. However, identifying where the industry needs more information or a clearer picture informs a lot of the public-facing talks and activities in which I participate throughout the year. What I learned about the average SC technical program attendee The biggest realization that I confirmed this week is that the SC conference is not an HPC conference; it is a scientific computing conference. I sat in a few sessions where the phrase "HPC workflows" was clearly a stand-in for "scientific workflows," and "performance evaluation" still really means "MPI and OpenMP profiling." I found myself listening to ideas or hearing about tools that were intellectually interesting but ultimately not useful to me because they were so entrenched in the traditions of applying HPC to scientific computing. Let's talk about a few ways in which this manifested. People think sustainability and energy efficiency are the same thing Take, for example, the topic of sustainability. There were talks, panels, papers, and BOFs that touched on the environmental impact of HPC throughout the week, but the vast majority of them really weren't talking about sustainability at all; they were talking about energy efficiency. These talks often use the following narrative: Energy use from datacenters is predicted to reach some ridiculous number by 2030 We must create more energy-efficient algorithms, processors, and scheduling policies Here is an idea we tested that reduced the energy consumption without impacting the performance of some application or workflow Sustainability achieved! Success! The problem with this approach is that it declares victory when energy consumption is reduced. This is a great result if all you care about is spending less money on electricity for your supercomputer, but it completely misses the much greater issue that the electricity required to power an HPC job is often generated by burning fossil fuels, and that the carbon emissions that are directly attributable to HPC workloads are contributing to global climate change. This blind spot was exemplified by this slide, presented during a talk titled "Towards Sustainable Post-Exascale Leadership Computing" at the Sustainable Supercomputing workshop: I've written about this before and I'll write about it again: FLOPS/Watt and PUE are not meaningful metrics by themselves when talking about sustainability. A PUE of 1.01 is not helpful if the datacenter that achieves it relies on burning coal for its power. Conversely, a PUE of 1.5 is not bad if all that electricity comes from a zero-carbon energy source. The biggest issue that I saw being reinforced at SC this year is that claims of "sustainable HPC" are accompanied by the subtext of "as long as I can keep doing everything else the way I always have." There were glimmers of hope, though. Maciej Cytowski from Pawsey presented the opening talk at the Sustainable Supercomputing workshop, and he led with the right thing--he acknowledged that 60% of the fuel mix that powers Pawsey's supercomputers comes from burning fossil fuels: Rather than patting himself on the back at his low PUE, Dr. Cytowski's described on how they built their datacenter atop a large aquifer from which they draw water at 21°C and return it at 30°C to avoid using energy-intensive chillers. To further reduce the carbon impact of this water loop, Pawsey also installed over 200 kW of solar panels on its facility roof to power the water pumps. Given the fact that Pawsey cannot relocate to somewhere with a higher ratio of zero-carbon energy on account of its need to be physically near the Square Kilometer Array, Cytowski's talk felt like the most substantive discussion on sustainability in HPC that week. Most other talks and panels on the topic really wanted to equate "sustainability" to "FLOPS per Watt" and pretend like where one deploys a supercomputer is not a part of the sustainability discussion. The reality is that, if the HPC industry wanted to take sustainability seriously, it would talk less about watts and more about tons of CO2. Seeing as how the average watt of electricity in Tennessee produces 2.75x more carbon than a watt of electricity in Washington, the actual environmental impact of fine-tuning Slurm scheduling or fiddling with CPU frequencies is meaningless when compared to the benefits that would be gained by deploying that supercomputer next to a hydroelectric dam instead of a coal-fired power plant. I say all this because there are parts of the HPC industry (namely, the part in which I work) who are serious about sustainability. And those conversations go beyond simply building supercomputers in places where energy is low-carbon (thereby reducing Scope 2 emissions). They include holding suppliers to high standards on reducing the carbon impact of transporting people and material to these data centers, reducing the carbon impact of all the excess packaging that accompanies components, and being accountable for the impact of everything in the data center after it reaches end of life (termed Scope 3 emissions). The HPC community--or more precisely, the scientific computing community--is still married to the idea that the location of a supercomputer is non-negotiable, and "sustainability" is a nice-to-have secondary goal. I was hoping that the sessions I attended on sustainability would approach this topic at a level where the non-scientific HPC world has been living. Unfortunately, the discussion at SC24, which spanned workshops, BOFs, and Green 500, remains largely stuck on the idea that PUE and FLOPS/Watt are the end-all sustainability metrics. Those metrics are important, but there are global optimizations that have much greater effects on reducing the environmental impact of the HPC industry. AI sessions are really scientific computing sessions about AI Another area where "HPC" was revealed to really mean "scientific computing" was in the topic of AI. I sat in on a few BOFs and panels around AI topics to get a feel for where this community is in adopting AI for science, but again, I found the level of discourse to degrade to generic AI banter despite the best efforts of panelists and moderators. For example, I sat in the "Foundational Large Language Models for High-Performance Computing" BOF session, and Jeff Vetter very clearly defined what a "foundational large language model" was at the outset so we could have a productive discussion about their applicability in HPC (or, really, scientific computing): The panelists did a good job of outlining their positions. On the upside, LLMs are good for performing source code conversion, documenting and validating code, and maximizing continuity in application codes that get passed around as graduate students come and go. On the downside, they have a difficult time creating efficient parallel code, and they struggle to debug parallel code. And that's probably where the BOF should have stopped, because LLMs, as defined at the outset of the session, don't actually have a ton of applicability in scientific computing. But as soon as the session opened up to audience questions, the session went off the rails. The first question was an extremely basic and nonspecific question: "Is AI a bubble?" It's fun to ask provocative questions to a panel of experts. I get it. But the question had nothing to do with LLMs, any of the position statements presented by panelists, or even HPC or scientific computing. It turned a BOF on "LLMs for HPC" into a BOF that might as well have been titled "Let's just talk about AI!" A few panelists tried to get things back on track by talking about the successes of surrogate models to simulate physical processes, but this reduced the conversation to a point where "LLMs" really meant "any AI model" and "HPC" really meant "scientific simulations." Perhaps the most productive statement to come out of that panel was when Rio Yokota asserted that "we" (the scientific community) should not train their own LLMs, because doing so would be "unproductive for science." But I, as well as anyone who understands the difference between LLMs and "AI," already knew that. And the people who don't understand the difference between an LLM and a surrogate model probably didn't pick up on Dr. Yokota's statement, so I suspect the meaning of his contribution was completely lost. Walking out of that BOF (and, frankly, the other AI-themed BOFs and panels I attended), I was disappointed at how superficial the conversation was. This isn't to say these AI sessions were objectively bad; rather, I think it reflects the general state of understanding of AI amongst SC attendees. Or perhaps it reflects the demographic that is drawn to these sorts of sessions. If the SC community is not ready to have a meaningful discussion about AI in the context of HPC or scientific computing, attending BOFs with like-minded peers is probably a good place to begin getting immersed. But what became clear to me this past week is that SC BOFs and panels with "AI" in their title aren't really meant for practitioners of AI. They're meant for scientific computing people who are beginning to dabble in AI. AI for operations is not yet real in scientific computing I was invited to sit on a BOF panel called "Artificial Intelligence and Machine Learning for HPC Workload Analysis" following on a successful BOF in which I participated at ISC24. The broad intent was to have a discussion around the tools, methods, and neat ideas that HPC practitioners have been using to better understand workloads, and each of us panelists was tasked with talking about a project or idea we had in applying AI/ML to improve some aspect of workloads. What emerged from us speakers' lightning talks is that applying AI for operations--in this case, understanding user workloads--is nascent. Rather than talking about how we use AI to affect how we design or operate supercomputers, all of us seemed to focus more on how we are collecting data and beginning to analyze that data using ML techniques. And maybe that's OK, because AI won't ever do anything for workload characterization until you have a solid grasp of the telemetry you can capture about those workloads in the first place. But when we opened the BOF up to discussion with all attendees, despite having a packed room, there was very little that the audience had. Our BOF lead, Kadidia Konaté, tried to pull discussion out of the room from a couple of different fronts by asking what tools people were using, what challenges they were facing, and things along those lines. However, it seemed to me that the majority of the audience was in that room as spectators; they didn't know where to start applying AI towards understanding the operations of supercomputers. Folks attended to find out the art of the possible, not talk about their own challenges. As such, the conversation wound up bubbling back up to the safety of traditional topics in scientific computing--how is LDMS working out, how do you deal with data storage challenges of collecting telemetry, and all the usual things that monitoring and telemetry folks worry about. It's easy to talk about the topics you understand, and just as the LLM conversation reverted back to generic AI for science and the sustainability topic reverted back to FLOPS/Watt, this topic of AI for operations reverted back to standard telemetry collection. Some are beginning to realize that HPC exists outside of scientific computing Despite the pervasive belief at SC24 that "HPC" and "scientific computing" are the same thing, there are early signs that the leaders in the community are coming to terms with the reality that there is now a significant amount of leadership HPC happening outside the scope of the conference. This was most prominent at the part of the Top500 BOF where Erich Strohmaier typically discusses trends based on the latest publication of the list. In years past, Dr. Strohmaier's talk was full of statements that strongly implied that, if a supercomputer is not listed on Top500, it simply does not exist. This year was different though: he acknowledged that El Capitan, Frontier, and Aurora were "the three exascale systems we are aware of," now being clear that there is room for exascale systems to exist that simply never ran HPL, or never submitted HPL results to Top500. He explicitly acknowledged again that China has stopped making any Top500 submissions, and although he didn't name them outright, he spent a few minutes dancing around "hyperscalers" who have been deploying exascale class systems such as Meta's H100 clusters (2x24K H100), xAI's Colossus (100K H100), and the full system behind Microsoft's Eagle (14K H100 is a "tiny fraction"). Strohmaier did an interesting analysis that estimated the total power of the Top500 list's supercomputers so he could compare it to industry buzz around hyperscalers building gigawatt-sized datacenters: It was a fun analysis where he concluded that there are between 500-600 megawatts of supercomputers on the Top500 list, and after you factor in storage, PUE, and other ancillary power sources, the whole Top500 list sums up to what hyperscalers are talking about sticking into a single datacenter facility. Although he didn't say it outright, I think the implication here is that the Top500 list is rapidly losing relevance in the broad HPC market, because a significant amount of the world's supercomputing capacity and capability are absent from the list. Although specific hyperscale supercomputers (like Meta's, xAI's, and Microsoft's) were not mentioned outright, their absence from the Top500 list suggests that this list might already be more incomplete than it is complete--the sum of the FLOPS or power on the Top500 supercomputers may be less than the sum of the giant supercomputers which are known but not listed. This will only get worse as the AI giants keep building systems every year while the government is stuck on its 3-5 year procurement cycles. It follows that the meaning of the Top500 is sprinting towards a place where it is not representative of HPC so much as it is representative of the slice of HPC that serves scientific computing. Erich Strohmaier was clearly aware of this in his talk this year, and I look forward to seeing how the conversation around the Top500 list continues to morph as the years go on. NSF's broad front vs. DOE's big bets in HPC and AI My career was started at an NSF HPC center and built up over my years in the DOE, so I feel like I owe a debt to the people who provided all the opportunities and mentorship that let me get to the place of privilege in the hyperscale/AI industry that I now enjoy. As a result, I find myself still spending a lot of my free time thinking about the role of governments in the changing face of HPC (as evidenced by my critiques of thinktank reports and federal RFIs...) and trying to bridge the gap in technical understanding between my old colleagues (in DOE, NSF, and European HPC organizations) and whatever they call what I work on now (hyperscale AI?). To that end, I found myself doing quite a bit of business development (more on this later) with government types since I think that is where I can offer the most impact. I used to be government, and I closely follow the state of their thinking in HPC, but I also know what's going on inside the hyperscale and AI world. I also have enough context in both areas to draw a line through all the buzzy AI press releases to demonstrate how the momentum of private-sector investment in AI might affect the way national HPC efforts do business. So, I did a lot of talking to both my old colleagues in DOE and their industry partners in an attempt to help them understand how the hyperscale and AI industry thinks about infrastructure, and what they should expect in the next year. More importantly though, I also sat in on a couple of NSF-themed BOFs to get a better understanding of where their thinking is, where NAIRR is going, how the NSF's strategy contrasts with DOE's strategy, and where the ambitions of the Office of Advanced Cyberinfrastructure might intersect with the trajectory of hyperscale AI. What I learned was that NSF leadership is aware of everything that the community should be concerned about: the growth of data, the increasing need for specialized silicon, the incursion of AI into scientific computing, new business models and relationships with industry, and broadening the reach of HPC investments to be globally competitive. But beyond that, I struggled to see a cohesive vision for the future of NSF-funded supercomputing.  A BOF with a broad range of stakeholders probably isn't the best place to lay out a vision for the future of NSF's HPC efforts, and perhaps NSF's vision is best expressed through its funding opportunities and awards. Whichever the case may be, it seems like the NSF remains on a path to make incremental progress on a broad front of topics. Its Advanced Computing Systems and Services (ACSS) program will continue to fund the acquisition of newer supercomputers, and a smorgasbord of other research programs will continue funding efforts across public access to open science, cybersecurity, sustainable software, and other areas. My biggest concern is that peanut-buttering funding across such a broad portfolio will make net forward progress much slower than taking big bets. Perhaps big bets just aren't in the NSF's mission though. NAIRR was also a topic that came up in every NSF-themed session I attended, but again, I didn't get a clear picture of the future. Most of the discussion that I heard was around socializing the resources that are available today through NAIRR, suggesting that the pilot's biggest issue is not a lack of HPC resources donated by industry, but awareness that NAIRR is a resource that researchers can use. This was reinforced by a survey whose results were presented in the NAIRR BOF: It seems like the biggest challenges facing the NSF community relying on NAIRR (which has its own sample bias) is that they don't really know where to start even though they have AI resources (both GPUs and model API services) at their disposal. In a sense, this is a great position for the NSF since its users need intellectual help more than access to GPU resources, and the NSF has been great at promoting education, training, and workforce development. its users are unlikely to demand the same cutting-edge GPUs that AI industry leaders are snapping up. For example, the largest pool of GPUs in NAIRR are A100 GPUs that NVIDIA donated via DGX Cloud; the big AI companies moved off of Ampere a year ago and are about to move off of Hopper. However, it also means that there's not a clear role for partnership with many industry players beyond donating resources to the NAIRR pilot today in the hopes of selling resources to the full NAIRR tomorrow. I asked what OAC leadership thought about moving beyond such a transactional relationship between NSF and industry at one of the BOFs I attended, and while the panelists were eager to explore specific answers to that question, I didn't hear any ideas that would approach some sort of truly equitable partnership where both parties contributed in-kind. I also walked away from these NSF sessions struck by how different the NSF HPC community's culture is from that of the DOE. NSF BOF attendees seemed focused on getting answers and guidance from NSF leadership, unlike the typical DOE gathering, where discussions often revolve around attendees trying to shape priorities to align with their own agendas. A room full of DOE people tends to feel like everyone thinks they're the smartest person there, while NSF gatherings appear more diverse in the expertise and areas of depth of its constituents. Neither way is inherently better or worse, but it will make the full ambition of NAIRR (as an inter-agency collaboration) challenging to navigate. This is particularly relevant as DOE is now pursuing its own multi-billion-dollar AI infrastructure effort, FASST, that appears to sidestep NAIRR. Exhibitor trends There's no better way to figure out what's going on in the HPC industry than walking the exhibit floor each year, because booths cost money and reflect the priorities (and budgets) of all participants. This year's exhibit felt physically huge, and walking from one end to the other was an adventure. You can get a sense of the scale from this photo I took during the opening gala: Despite having almost 18,000 registrants and the opening gala usually being a crush of people, the gala this year felt and looked very sparse just because people and booths were more spread out. There was also a perceptibly larger number of splashy vendors who have historically never attended before who were promoting downstream HPC technologies like data center cooling and electrical distribution, and there was healthy speculation online about whether the hugeness of the exhibit this year was due to these new power and cooling companies. To put these questions to rest, I figured out how to yank down all the exhibitor metadata from the conference website so I could do some basic analysis on it. Booths by the numbers The easiest way to find the biggest companies to appear this year was to compare the exhibitor list and booth sizes from SC23 to this year and see whose booth went from zero to some big square footage. I only took the top twenty new vendors, but they broadly fall into a couple of categories: Power and cooling: Stulz, Delta, Airedale, Valvoline, Boundary Electric, Schneider Electric, Mara Server manufacturing: Wistron, AMI, Pegatron Higher ed: Tennessee Tech, SCRCC There were a couple other companies that must've just missed last SC but aren't new to the show (NetApp, Ansys, Samsung, Micron, Broadcom). And curiously, only one new GPU-as-a-Service provider (Nebius) showed up this year, suggesting last year was the year of the GPU Cloud. But to confirm what others had speculated: yes, a significant amount of the new square footage of the exhibit floor can be attributed to companies focused on power and cooling. This is an interesting indicator that HPC is becoming mainstream, largely thanks to AI demanding ultra-high density of power and cooling. But it's also heartening to see a few new exhibitors in higher education making an appearance. Notably, SCRCC (South Carolina Research Computing Consortium) is a consortium between Clemon, University of South Carolina, and Savannah River National Laboratory that just formed last year, and I look forward to seeing what their combined forces can bring to bear. We can also take a look at whose booths grew the most compared to SC23: This distribution is much more interesting, since the top 20 exhibitors who grew their footprint comprise the majority of the growth in existing exhibitors. Cherry-picking a few interesting growers: Power and cooling: USystems, Midas, Vertiv Data center/GPUaaS: iM, Iris Energy, and (arguably) Oracle Software: Arc Compute and CIQ Companies facing serious financial or legal troubles: I count at least three! Impressive that they are still pouring money into their SC booths. It's also interesting to see HLRS, the German national HPC center, grow so significantly. I'm not sure what prompted such a great expansion, but I take it to mean that things have been going well there. Finally, Dell had a massive booth and showing this year. Not only did they grow the most since SC23, but they had the single largest booth on the exhibit floor at SC24. This was no doubt a result of their great successes in partnering with NVIDIA to land massive GPU buildout deals at places like xAI and CoreWeave. They also had "AI factory" messaging emblazoned all over their marketing material and debuted a nice 200 kW liquid-cooled rack that will be the basis for their GB200 NVL72 solution, clearly leaning into the idea that they are leaders in AI infrastructure. Despite this messaging being off-beat for the SC audience as I've described earlier, their booth was surprisingly full all the time, and I didn't actually get a chance to get in there to talk to anyone about what they've been doing. Equally interesting are the vendors who reduced their footprint at SC24 relative to SC23: Reading too much into any of these big shrinkers is pretty easy; while a reduction in booth size could suggest business hasn't been as good, it could equally mean that an exhibitor just went overboard at SC23 and downsized to correct this year. A few noteworthy exhibitors to call out: Penguin and the Korea Semiconductor Industry Association both cut way back from massive 50x50 booths to 30x30. Their booths this year were both big, but they weren't massive. Viridien, formerly known as CGG, also shrunk from a massive booth to a less-massive 30x40. Juniper still kept an independent booth, but it is in the process of being absorbed into HPE. Shrinking makes sense. Major cloud providers Google and AWS scaled back, but Microsoft did not. GPU-as-a-Service cloud providers CoreWeave and Lambda both scaled back. Since these GPUaaS providers' business models typically rely on courting few big customers, it may make sense to cut back on booth volume. Major AI storage companies DDN, VAST, and (to a lesser degree) Pure also scaled back, while WEKA did not. I know business for DDN and VAST has been great this past year, so these may just reflect having gone overboard last year. Overall, almost twice as many vendors grew their booths than scaled back, so I'd caution anyone against trying to interpret any of this as anything beyond exhibitors right-sizing their booths after going all-in last year. Finally, there are a handful of vendors who disappeared outright after SC23: It is critical to point out that the largest booths to vanish outright were all on the smaller size: SUSE, Tenstorrent, and Symbiosys Alliance all disappeared this year, but their booths last year were only 20x30. I was surprised to see that Tenstorrent and Arm didn't have booths, but the others are either companies I haven't heard of (suggesting the return on investment of showing at SC might've been low), are easy to rationalize as only being HPC-adjacent (such as SNIA and DigitalOcean), or simply went bankrupt in the last year. As we say at the business factory, the net-net of the exhibit hall this year is that the square footage of booth space increased by 15,000 square feet, so it was in fact bigger, it did take longer to walk from one end to the other, and there definitely were a bunch of new power and cooling companies filling out the space. Some exhibitors shrank or vanished, but the industry as a whole appears to be moving in a healthy direction. And if you're interested in analyzing this data more yourself, please have a look at the data and the Jupyter notebook I used to generate the above treemaps on GitHub. If you discover anything interesting, please write about it and post it online! Proliferation of GPU-as-a-Service providers As an AI infrastructure person working for a major cloud provider, I kept an eye out for all the companies trying to get into the GPU-as-a-Service game. I described these players last year as "pure-play GPU clouds," and it seems like the number of options available to customers who want to go this route is growing. But I found it telling that a lot of them had booths that were completely indistinguishable from each other. Here's an example of one: As best I can tell, these companies are all NVIDIA preferred partners with data centers and a willingness to deploy NVIDIA GPUs, NVIDIA SmartNICs, and NVIDIA cloud stack, and sell multi-year commitments to consume those GPUs. I tried to accost some of these companies' booth staff to ask them my favorite question ("What makes you different from everyone else?"), but most of these companies' booths were staffed by people more interested in talking to each other than me. These GPUaaS providers tend to freak me out, because, as Microsoft's CEO recently stated, these companies are often "just a bunch of tech companies still using VC money to buy a bunch of GPUs." I can't help but feel like this is where the AI hype will come back to bite companies who have chosen to build houses upon sand. Walking the SC24 exhibit floor is admittedly a very narrow view of this line of business, but it seemed like some of these companies were content to buy up huge booths, hang a pretty banner above it, and otherwise leave the booth empty of anything beyond a few chairs and some generic value propositions. I didn't feel a lot of hunger or enthusiasm from these companies despite the fact that a bunch of them have hundreds of millions of dollars of GPUs effectively sitting on credit cards that they are going to have to make payments on for the next five years. That all said, not all the companies in the GPUaaS are kicking back and letting the money pour in. In particular, I spent a few minutes chatting up someone at the CoreWeave booth, and I was surprised to hear about how much innovation they're adding on top of their conventional GPUaaS offering. For example, they developed Slurm on Kubernetes (SUNK) with one of their key customers to close the gap between the fact that CoreWeave exposes its GPU service through Kubernetes, but many AI customers have built their stack around Slurm, pyxis, and enroot. In a weird twist of fate, I later ran into an old acquaintance who turned out to be one of the key CoreWeave customers for whom SUNK was developed. He commented that SUNK is the real deal and does exactly what his users need which, given the high standards that this person has historically had, is a strong affirmation that SUNK is more than just toy software that was developed and thrown on to GitHub for an easy press release. CoreWeave is also developing some interesting high-performance object storage caching software, and all of these software services are provided at no cost above whatever customers are already paying for their GPU service. I bring this up because it highlights an emerging distinction in the GPUaaS market, which used to be a homogenous sea of bitcoin-turned-AI providers. Of course, many companies still rely on that simple business model: holding the bill for rapidly depreciating GPUs that NVIDIA sells and AI startups consume. However, there are now GPUaaS providers moving up the value chain by taking on the automation and engineering challenges that model developers don't want to deal with. Investing in uncertain projects like new software or diverse technology stacks is certainly risky, especially since they may never result in enough revenue to pay for themselves. But having a strong point of view, taking a stance, and investing in projects that you feel are right deserves recognition. My hat is off to the GPUaaS providers who are willing to take these risks and raise the tide for all of us rather than simply sling NVIDIA GPUs to anyone with a bag of money. Community and connections As much as I enjoy increasing shareholder value, the part of SC that gives me the greatest joy is reconnecting with the HPC community. Knowing I'll get to chat with my favorite people in the industry (and meet some new favorite people!) makes the long plane rides, upper respiratory infections, and weird hotel rooms completely worth it. I wound up averaging under six hours of sleep per night this year in large part because 9pm or 7am were often the only free times I had to meet with people I really wanted to see. I have this unhealthy mindset where every hour of every day, from the day I land to the day I leave, is too precious to waste, and it's far too easy for me to rationalize that spending an hour talking to someone interesting is worth losing an hour of sleep. But like I said at the outset of this blog post, this year felt different for a few reasons, and a lot of them revolve around the fact that I think I'm getting old. Now, it's always fun to say "I'm getting old" in a mostly braggadocious way, but this feeling manifested in concrete ways that affected the way I experienced the conference: I hit my limit on Monday night and couldn't get home without spending 15 minutes sitting in an unlit playground across from the World of Coke. I've always gotten blisters and fatigue, but this was the first time I couldn't just cowboy up and muscle through it. To avoid a repeat of this, I wound up "wasting" (see above) a lot more time to just get off my feet this year. This year, I reached the point where I need to start time-box how much time I spend chatting up the folks I bump into. I used to just let the good times roll if I ran into someone I knew, but this year I wound up spending as much time attending sessions as I did missing sessions because I got caught up in a conversation. This isn't a bad thing per se, but I did feel a little sour when I realized I'd made a bad bet on choosing to chat instead of attending a session or vice versa, and this bad feeling lingered in the back of my mind just about every day. There weren't a lot of surprises for me at the conference this year, and I worry that I am at risk of losing touch with the technical aspects of the conference that get newer attendees excited. Instead of hearing about, say, the latest research in interconnects, more of my time was spent mucking it up with the sorts of people in the HPC community who I used to find intimidating. On the one hand, hooray me for making it into old boys' clubs. But on the other, I don't want to become some HPC greybeard whose last meaningful contribution to the industry was twenty years ago. This is the first year where I've had people accost me and ask me for advice. I've long been accosted by strangers because of my online presence, but those interactions were always lighthearted exchanges of "I follow you on Twitter" and "Great to meet you. Have an @HPC_Guru pin." This year, I had people specifically ask me for advice on industry versus postdoc, AI versus HPC, and what my master plan was when I left NERSC. Even though I didn't have any sage advice, I still found it really hard to tell bright-eyed students to go kick rocks just so I wouldn't be late for yet another mushy panel on AI. If you read this all and think "boo hoo, poor Glenn is too popular and wise for his own good," yeah, I get it. There are worse problems to have. But this was the first year where I felt like what I put into the conference was greater than what I got out of it. Presenting at SC used to be at least as good for my career as it was useful for my audiences, but it just doesn't count for much given my current role and career stage. It felt like some of the magic was gone this year in a way I've never experienced before.  Getting to know people As the years have gone on, I spend an increasing amount of my week having one-on-one conversations instead of wandering aimlessly. This year though, I came to SC without really having anything to buy or sell: I am not a researcher, so I don't need to pump up the work I'm doing to impress my fellow researchers. I no longer own a product market segment, so I don't directly influence the customers or vendors with whom my employer works. I don't have any bandwidth in my day job to support any new customers or partnerships, so I don't have a strong reason to sell people on partnering with me or my employer.  Much to my surprise though, a bunch of my old vendor/partner colleagues still wanted to get together to chat this year. Reflecting back, I was surprised to realize that it was these conversations--not the ones about business--that were the most fulfilling this year. I learned about people's hobbies, families, and their philosophies on life, and it was amazing to get to know some of the people behind the companies with whom I've long dealt. I was reminded that the person is rarely the same as the company, and even behind some of the most aggressive and blusterous tech companies are often normal people with the same concerns and moments of self-doubt that everyone else has. I was also reminded that good engineers appreciate good engineering regardless of whether it's coming from a competitor or not. The public persona of a tech exec may not openly admire a competitor's product, but that doesn't mean they don't know good work when they see it. I also surprised a colleague whose career has been in the DOE labs with an anecdote that amounted to the following: even though two companies may be in fierce competition, the people who work for them don't have to be. The HPC community is small enough that almost everyone has got a pal at a competing company, and when there are deals to be made, people looove to gossip. If one salesperson hears a juicy rumor about a prospective customer, odds are that everyone else on the market will hear about it pretty quickly too. Of course, the boundaries of confidentiality and professionalism are respected when it matters, but the interpersonal relationships that are formed between coworkers and friends don't suddenly disappear when people change jobs. And so, I guess it would make sense that people still want to talk to me even though I have nothing to buy or sell. I love trading gossip just as much as everyone else, and I really enjoyed this aspect of the week. Talking to early career people I also spent an atypically significant amount of my week talking to early career people in HPC who knew of me one way or another and wanted career advice. This is the first year I recall having the same career conversations with multiple people, and this new phase of my life was perhaps most apparent during the IEEE TCHPC/TCPP HPCSC career panel in which I was invited to speak this year. It was an honor to be asked to present on a career panel, but I didn't feel very qualified to give career advice to up-and-coming computer science graduate students who want to pursue HPC. I am neither a computer scientist nor a researcher, but fortunately for me, my distinguished co-panelists (Drs. Dewi Yokelson, Olga Pearce, YJ Ji, and Rabab Alomairy) had plenty of more relevant wisdom to share. And at the end of the panel, there were a few things we all seemed to agree on as good advice: Knowing stuff is good, but being able to learn things is better. Being eager to learn and naturally curious makes this much easier as well. The life of a researcher sometimes requires more than working a standard nine-to-five, so it'll be hard to be really successful if your heart isn't in it. People will forget what you did or what you said, but they remember how you made them feel. Don't be a jerk, because this community is small. In both this panel the one-on-one conversations I had with early career individuals, the best I could offer was the truth: I never had a master plan that got me to where I am; I just try out new things until I realize I don't like doing them anymore. I never knew what I wanted to be when I grew up, and I still don't really, so it now makes me nervous that people have started approaching me with the assumption that I've got it all figured out. Unless I torpedo my career and go live on a goat farm though, maybe I should prepare for this to be a significant part of my SC experiences going forward. Shift in social media One last, big change in the community aspect of SC this year was the mass-migration of a ton of HPC folks from Twitter to Bluesky during the week prior to the conference. I don't really understand what prompted it so suddenly; a few of us have been trying for years to get some kind of momentum on other social platforms like Mastodon, but the general lack of engagement meant that all the excitement around SC always wound up exclusively on Twitter. This year was different though, and Bluesky hit critical mass with the HPC community. I personally have never experienced an SC conference without Twitter; my first SC was in 2013, and part of what made that first conference so exciting was being able to pull up my phone and see what other people were seeing, thinking, and doing across the entire convention center via Twitter. Having the social media component to the conference made me feel like I was a part of something that first year, and as the years went on, Twitter became an increasingly indispensable part of the complete SC experience for me. This year, though, I decided to try an experiment and see what SC would be like if I set Twitter aside and invested my time into Bluesky instead. The verdict? It was actually pretty nice. It felt a lot like the SC13 days, where my day ended and began with me popping open Bluesky to see what new #SC24 posts were made. And because many of the tech companies and HPC centers hadn't yet made it over, the hashtag wasn't clogged up by a bunch of prescheduled marketing blasts that buried the posts written by regular old conference attendees who were asking important questions: Which booths at #sc24 have coffee? I noticed oracle do. Anyone else? — Mike Croucher (@walkingrandomly.bsky.social) November 18, 2024 at 3:02 PM Of course, I still clogged Bluesky up with my nonsense during the week, but there was an amazing amount of engagement by a diversity of thoughtful people--many who came from Twitter, but some whose names and handles I didn't recognize. The volume of traffic on Bluesky during the week did feel a little lower than what it had been on Twitter in years past though. I also didn't see as many live posts of technical sessions as they happened, so I couldn't really tell whether I was missing something interesting in real time. This may have contributed to why I felt a little less connected to the pulse of the conference this year than I had in the past. It also could've been the fact that conference was physically smeared out across a massive space though; the sparsity of the convention center was at least on par with the sparsity on Bluesky. At the end of the week, I didn't regret the experiment. In fact, I'll probably be putting more effort into my Bluesky account than my Twitter account going forward. To be clear though, this isn't a particularly political decision on my part, and I pass no judgment on anyone who wants to use one platform over the other. It's just that I like the way I feel when I scroll through my Bluesky feeds, and I don't get that same feeling when I use Twitter. So what's the takeaway? SC this year was a great conference by almost every measure, as it always is, but it still felt a little different for me. I'm sure that some of that feeling is the result of my own growth, and my role with respect to the conference seems to be evolving from someone who gets a lot out of the conference to someone who is giving more to the conference. That's not to say that I don't get a lot out of it, though; I had no shortage of wonderful interactions with everyone from technology executives to rising stars who are early in their career, and I learned a lot about both them and me as whole people. But SC24, more than any SC before it, is when I realized this change was happening. On the technological front, we saw the debut of a new #1 system (emblazoned with the smiling face of Bronis...) and a growing crop of massive, new clusters deployed for commercial applications. The exhibit floor was quantitatively bigger, in large part due to new power and cooling companies who are suddenly relevant to the HPC world thanks to the momentum of AI. At the same time, the SC technical program is clearly separating itself out as a conference focused on scientific computing; the level of discourse around AI remains largely superficial compared to true AI conferences, the role of hyperscalers in the HPC industry is still cast more as a threat than an opportunity. For my part, I'm still trying to get a grasp on where government agencies like DOE and NSF want to take their AI ambitions so I can try to help build a better mutual understanding between the scientific computing community and the hyperscale AI community. However, it seems like the NSF is progressing slowly on a wide front, while the DOE is doing what DOE does and charging headfirst into a landscape that has changed more than I think they realize. There's a lot of technical content that I know I missed on account of the increasing time I've been spending on the people and community aspect of the conference, and I'm coming to terms with the idea that this just may be the way SC is from now on. And I think I'm okay with that, since the support of the community is what helped me go from being a bored materials science student into someone whose HPC career advice is worth soliciting in the short span of eleven years. Despite any or all of the cynicism that may come out in the things I say about this conference, SC is always the highlight of my year. I always go into it with excitement, gladly burn the candle at both ends all week, and fly home feeling both grateful for and humbled by everything the HPC community has done and continues to do to keep getting me out of bed in the morning.]]></summary></entry><entry><title type="html">Surfing the Singularity - “the Workflow is the App”</title><link href="https://hpc.social/personal-blog/2024/surfing-the-singularity-the-workflow-is-the-app/" rel="alternate" type="text/html" title="Surfing the Singularity - “the Workflow is the App”" /><published>2024-11-12T17:00:00-07:00</published><updated>2024-11-12T17:00:00-07:00</updated><id>https://hpc.social/personal-blog/2024/surfing-the-singularity-the-workflow-is-the-app-</id><content type="html" xml:base="https://hpc.social/personal-blog/2024/surfing-the-singularity-the-workflow-is-the-app/"><![CDATA[<p class="ember-view reader-text-block__paragraph" id="ember2131"><span color="rgba(255, 255, 255, 0.9)" style="font-family: verdana;">Hello and happy fall holidays to you and yours.</span><span class="white-space-pre" color="rgba(255, 255, 255, 0.9)"> </span></p>
<p class="ember-view reader-text-block__paragraph" id="ember2131"><span style="font-family: verdana;">As I wrote about in the last blog post [<a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://www.linkedin.com/pulse/surfing-singularity-universe-computes-andy-gallo-6fgle" target="_self">1</a>], as quantum computing hardware matures over the next 5 to 10 years from an experimental toy through to utility and then perhaps advantage over classical (for some applications), it will be included into an already diverse and hybrid computing and applications landscape - on-prem computing, mobile and edge, cloud, and now novel types of computing devices which require new thinking and wholly new means of addressing them. How to deal with the burgeoning heterogeneity of the computing landscape - how to write and run apps which produce and consume data across a widening array of devices- is the topic of this post.<span class="white-space-pre"> </span></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember2131"><span style="font-family: verdana;"><span class="white-space-pre"><br /></span></span></p>
<h2><span style="font-family: verdana; font-size: x-large;">Language Landscape<span class="white-space-pre"> </span></span></h2>
<p class="ember-view reader-text-block__paragraph" id="ember2133"><span style="font-family: verdana;">The Java programming language, once touted in the glory days of "the World Wide Web" as being "write once, deploy anywhere", and in its heyday representing 25% of new application development, is now down below 10%. What's hot? Python (23%), and "the C's", a collection of C, C++, C# and their kin (&gt;24% in total) which are traditionally recompiled for specific hardware platforms. [2] And while Python provides portability, often for performance in math operations it depends on native libraries, built in, you guessed it, the C's. Into this mix wades the US government which has come out recently with a potentially disruptive statement against the use of the C's, citing security concerns due to their free-wheeling memory management, and in spite of efforts like Safe C++, the government is recommending movement to memory safe languages like Rust, currently with just 1% market share, but "with a bullet". [3] Whether it is better to port to Rust or just update to Safe C++ depends on many factors - for example, how good are your docs and test cases - and while there may exist conceptual impedance mismatches between languages, modern AI coding assistants will only increase in capability especially for more rote tasks like porting.</span></p>
<p class="ember-view reader-text-block__paragraph" id="ember2134"><span style="font-family: verdana;">Add to this mix the coding of Graphical Processing Units (GPUs) - originally intended for visualizations but now used in applications for almost anything involving matrix math (turns out, lots of stuff). GPUs today are mostly sold by NVIDIA and are programmed in the C's (sometimes with a Python interface) using the NVIDIA CUDA library. These pieces of the application, the "kernels", are hardware dependent, and while many attempts have been made to create hardware-portable frameworks for GPU programming (see SYCL for example [4]), nearly always the newest fastest GPU features are available in the native non-portable form first, leading to vendor lock. (This might be a good time to remember that<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://www.linkedin.com/company/nvidiausa/">NVIDIA</a><span class="white-space-pre"> </span>does not themselves manufacture chips - they design chips which others produce.)</span></p>
<p class="ember-view reader-text-block__paragraph" id="ember2135"><span style="font-family: verdana;">The manner in which we program GPUs is similar to the way we program quantum computers, i.e. QPUs - we delegate to them the portions of the application to which they are best suited, program them using device-specific instructions, and weave them back into the holistic solution. Rather than wielding the Java hammer where everything is a virtualized nail, we use the best tool for the job. In quantum computing, for example, "variational" hybrid algorithms are a common theme, where some part of the work and preparation are performed on classical hardware as a setup for a quantum step, and then post-processing the results back on classical hardware for potential iteration to an optimal solution.<span class="white-space-pre"> </span></span></p>
<div class="reader-image-block reader-image-block--full-width"><figure class="reader-image-block__figure"><div class="ivm-image-view-model"><div class="ivm-view-attr__img-wrapper"><img alt="" class="ivm-view-attr__img--centered reader-image-block__img evi-image lazy-image ember-view" id="ember2136" src="https://media.licdn.com/dms/image/v2/D4E12AQHHLPGy-FVQ7g/article-inline_image-shrink_1500_2232/article-inline_image-shrink_1500_2232/0/1731379488723?e=1740009600&amp;v=beta&amp;t=rBfADA-gBrGoIdiIzfxE-P90i-WNLV2EFP7uYQzam20" /></div>
</div>
<figcaption class="reader-image-block__figure-image-caption display-block full-width text-body-small-open t-sans text-align-center t-black--light"><span style="font-family: verdana;">Two of several emerging patterns for integrating quantum computing into an application solution. [5]</span></figcaption></figure></div>
<p class="ember-view reader-text-block__paragraph" id="ember2137"><span style="font-family: verdana;">This pattern is analogous to what is also common in classical high performance computing (HPC) for applications like weather modeling and other complex simulations - pre-process on commodity hardware, run an HPC job on the big box, and post-process the results. The introduction into the mix of steerage provided by AI models increases the heterogeneity of the complete solution.<span class="white-space-pre"> </span></span></p>
<div class="reader-image-block reader-image-block--full-width"><figure class="reader-image-block__figure"><div class="ivm-image-view-model"><div class="ivm-view-attr__img-wrapper"><img alt="" class="ivm-view-attr__img--centered reader-image-block__img evi-image lazy-image ember-view" id="ember2138" src="https://media.licdn.com/dms/image/v2/D4E12AQGVu8gKIzDRLg/article-inline_image-shrink_1000_1488/article-inline_image-shrink_1000_1488/0/1731379555676?e=1740009600&amp;v=beta&amp;t=Bt4u6m8pi0i6xVNmfWxkxGK4bXBGkWuP3_zxrab1R3M" /></div>
</div>
<figcaption class="reader-image-block__figure-image-caption display-block full-width text-body-small-open t-sans text-align-center t-black--light"><span style="font-family: verdana;">A blended computing landscape, enabling for example, quantum computing to produce highly precise data to train AI to steer a classical HPC simulation. [6]</span></figcaption></figure></div>
<p class="ember-view reader-text-block__paragraph" id="ember2139"><span style="font-family: verdana;">All these hardware-dependent application pieces for an ever widening array of hardware means that compilers are cool again, and compiler pipelines like LLVM are critical to application development and deployment. [7] Included in this class of development tools are circuit transpilers for quantum hardware which must take into consideration not only the architectural differences between QPUs (e.g. which gates are supported, what's the inter-qubit connectivity like, etc.), but also the changes which can occur in a quantum data center on a daily basis as these new, noisy, and fragile qubits simply fail and go offline, potentially altering the machine's topology. Just-in-time compilation is needed, and compiler optimization is therefore also cool again. Thank you, Frances Allen. [8]<span class="white-space-pre"> </span></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember2139"><span style="font-family: verdana;"><span class="white-space-pre"><br /></span></span></p>
<h3 class="ember-view reader-text-block__heading-3" id="ember2140"><span style="font-family: verdana; font-size: x-large;">Parts is Parts</span></h3>
<p class="ember-view reader-text-block__paragraph" id="ember2141"><span style="font-family: verdana;">What emerges from this landscape is not a singular executable running on one computer, but rather, multiple application piece parts, written in different languages, running on radically different hardware in sequence and simultaneously, being orchestrated into a complete solution.</span></p>
<p class="ember-view reader-text-block__paragraph" id="ember2142"><span style="font-family: verdana;">In other words, a workflow. Back in the day Java's Sun Microsystems (remember them?) asserted "the network is the computer". Now we assert "the workflow is the app".<span class="white-space-pre"> </span></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember2143"><span style="font-family: verdana;">Or more likely, a workflow of workflows. We like to think of these nested workflows in three types: [9]</span></p>
<p class="ember-view reader-text-block__paragraph" id="ember2144"></p>
<ol><li><span style="font-family: verdana;"><span>in-situ</span>: the workflow is running all on the same machine (e.g. a local process, an HPC job)</span></li><li><span style="font-family: verdana;"><span>intra-site</span>: the workflow is running on different machines within the same connected enterprise (e.g. within the same data center, virtual network, etc.)</span></li><li><span style="font-family: verdana;"><span>inter-site</span>: the workflow is running across different machines in different enterprises (e.g. hybrid on-prem and perhaps multi-vendor cloud)</span></li></ol>
<p></p>
<p class="ember-view reader-text-block__paragraph" id="ember2145"><span style="font-family: verdana;">With all these compute types, languages, and locations working together to realize the workflow and solution, loose coupling is key - components connected but not dependent - each part minding its own business. In other words, to paraphrase the poet, good interfaces make good neighbors. [10]</span></p>
<p class="ember-view reader-text-block__paragraph" id="ember2146"><span style="font-family: verdana;">We use the convenience term "Site" to mean a provider of secure compute and data services. What interfaces must a Site provide? The interface or API can include lots of things, but it must at least provide: 1) authentication and authorization, 2) a means to run components through their lifecycle, 3) a means to manage data being operated on and produced, perhaps being moved into and out of the Site, and 4) some way to get an inventory of the Site's service offerings and provision them for the purposes of running components or holding data. We call these by four functional nicknames: Auth, Run, Repo, and Spin.<span class="white-space-pre"> </span></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember2147"><span style="font-family: verdana;"><br /></span></p>
<div class="reader-image-block reader-image-block--resize"><figure class="reader-image-block__figure"><div class="ivm-image-view-model"><div class="ivm-view-attr__img-wrapper"><img alt="" class="ivm-view-attr__img--centered reader-image-block__img evi-image lazy-image ember-view" id="ember2148" src="https://media.licdn.com/dms/image/v2/D4E12AQGVO-K51OaPPw/article-inline_image-shrink_1500_2232/article-inline_image-shrink_1500_2232/0/1731379758535?e=1740009600&amp;v=beta&amp;t=t0p27qST2ZLG6ETfkhu-0OppATVBXdcfX7Mgnw1Qk2A" /></div>
</div>
<figcaption class="reader-image-block__figure-image-caption display-block full-width text-body-small-open t-sans text-align-center t-black--light"><span style="font-family: verdana;">Four functional pillars of an interoperable computing site.</span></figcaption></figure></div>
<p class="ember-view reader-text-block__paragraph" id="ember2149"><span style="font-family: verdana;">We can see in each of the three types of workflows the need for each of these four functional pillars, albeit some as a no-op or inherited from a higher order workflow. For example, in a "type 1" workflow of components running on a single machine or within an HPC allocation the Auth aspects may be implied to be already addressed - i.e. the user is already logged into the machine or authorized to run on the HPC cluster. But a workflow which utilizes compute resources both on-prem and in the cloud will have to interact at runtime with the "auth" aspects of the cloud provider prior to being able to "run" workloads, or put and get data to various "repos". Most cloud providers provide a means to list available computing resources, to "spin" them up and down. This provisioning itself can be part of an end-to-end workflow: authenticate, get an inventory of available services, spin some up, run jobs on them storing the results, and spin them down.</span></p>
<p class="ember-view reader-text-block__paragraph" id="ember2149"><span style="font-family: verdana;"><span class="white-space-pre"> </span></span></p>
<h3 class="ember-view reader-text-block__heading-3" id="ember2150"><span style="font-family: verdana; font-size: x-large;">Stuck in the Middle</span></h3>
<p class="ember-view reader-text-block__paragraph" id="ember2151"><span style="font-family: verdana;">Most cloud providers - from Amazon to IBM Quantum cloud - provide a callable API interface which can be viewed through the lens of Auth, Run, Repo, Spin. So do some of the supercomputers and cutting edge resources provided by the Federal government, most notably those provided by the<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://www.linkedin.com/company/national-energy-research-scientific-computing-center/">National Energy Research Scientific Computing Center (NERSC)</a>. [11]<span class="white-space-pre"> </span></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember2152"><span style="font-family: verdana;">As Sites, these providers expose their offerings to internal and external workflows, however, they do not themselves promote a means to author these cross-site workflows, to manage them, track them, or keep tabs on all that distributed data. What else is needed? First, since cloud and other service providers have no motivation to standardize their interfaces, a framework super-interface could exist with the ability to plug in drivers for specific service providers. This in theory is the Auth, Run, Repo, Spin interface. Second, since each provider defines their own service and runtime component lifecycle (loosely: start, run, and stop with success or fail end states) there needs to be a way to normalize the status terminology - a "fail" on one site is the same as an "error" on another, "success" means the same thing as "done". This permits the third aspect of a middleware framework - the ability to track running jobs on Sites and trigger other jobs on any Site to run accordingly - i.e. the control flow of the workflow.<span class="white-space-pre"> </span></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember2153"><span style="font-family: verdana;">What about the data? Commonly we need the ability to put data to a Site and get some back - this is the Repo interface of the Site. And while most (but not all) Sites provide some means to store and retrieve data, be it filesystem or S3 object store or database or something else, it would also be nice to be able to say something "meta" about the data - which Site did it come from, what job or application produced it, what other workflow steps on this Site or others consumed it? Some Sites provide storage with metadata (e.g. Amazon S3) but most don't. This metadata comprises the provenance of the data - like a Civil War sword on the Antiques Roadshow, its the paper trail showing where the item came from, proving the item is legit. In a workflow which perhaps produces many pieces of data, perhaps iteratively as it converges on a solution - keeping track of all the data pieces seems, well, important. The acronym FAIR - findable, accessible, interoperable, reusable - seems a good starting point. [12]</span></p>
<p class="ember-view reader-text-block__paragraph" id="ember2153"><span style="font-family: verdana;"><br /></span></p>
<h3 class="ember-view reader-text-block__heading-3" id="ember2154"><span style="font-family: verdana; font-size: x-large;">Open Says Me</span></h3>
<p class="ember-view reader-text-block__paragraph" id="ember2155"><span style="font-family: verdana;">Our open source project lwfm, the "local workflow manager", attempts to render these concepts as a reference implementation. [13] Its small with minimal Python lib dependencies and can be taken anywhere easily as a single runnable component, its provenancial metadata also easily portable and importable. A typical Site driver - a Python class which implements the Site interface - weighs in around 200 lines of code including the whitespace. Armed with a Site driver for a cloud service, you can author long-running workflows which utilize a mix of compute resources, storage, and data infrastructures, and automatically track the provenancial paper trail.<span class="white-space-pre"> </span></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember2156"><span style="font-family: verdana;">The lwfm middleware component provides some very recognizable services:</span></p>
<p class="ember-view reader-text-block__paragraph" id="ember2157"></p>
<ul><li><span style="font-family: verdana;">polling of remote job status<span class="white-space-pre"> </span></span></li><li><span style="font-family: verdana;">status normalization and persistence</span></li><li><span style="font-family: verdana;">system and user metadata persistence</span></li><li><span style="font-family: verdana;">event handling, control flow and data flow triggered</span></li></ul>
<p></p>
<p class="ember-view reader-text-block__paragraph" id="ember2158"><span style="font-family: verdana;">Should you use this tooling? I wouldn't recommend it. (Huh? Did I hear you correctly?) How many people are working maintaining it? (Two?) What about the community? (Next to none.) The software would fare poorly on a "spider web" analysis of its overall quality - you would not want to recommend it to your boss.</span></p>
<div class="reader-image-block reader-image-block--resize"><figure class="reader-image-block__figure"><div class="ivm-image-view-model"><div class="ivm-view-attr__img-wrapper"><img alt="" class="ivm-view-attr__img--centered reader-image-block__img evi-image lazy-image ember-view" id="ember2159" src="https://media.licdn.com/dms/image/v2/D4E12AQEs4kk5AG5gIA/article-inline_image-shrink_1500_2232/article-inline_image-shrink_1500_2232/0/1731379896216?e=1740009600&amp;v=beta&amp;t=AMlOWYzQl60i9SQsBL77teawfvRVlFW_7l_yHoT4Nnk" /></div>
</div>
<figcaption class="reader-image-block__figure-image-caption display-block full-width text-body-small-open t-sans text-align-center t-black--light"><span style="font-family: verdana;">A convenient multi-axis assessment framework for software model maturity. [14]</span></figcaption></figure></div>
<p class="ember-view reader-text-block__paragraph" id="ember2160"><span style="font-family: verdana;">The lwfm is a reference implementation of a workflow interop framework, at best. Are there alternatives? OMG are there alternatives! The workflow landscape is notoriously rich, fragmented, and super-niched. But portability and interoperability are often neglected as is data provenance. Government or university projects, while well meaning and sometimes directionally correct, quickly go stale when the funding elapses [15], and commercial solutions while often suffering some of the same deficiencies offer the added trap of vendor lock and can come with a hefty price tag.</span></p>
<h3 class="ember-view reader-text-block__heading-3" id="ember2161"><span style="font-family: verdana;">Order, Order</span></h3>
<p class="ember-view reader-text-block__paragraph" id="ember2162"><span style="font-family: verdana;">So its back to committee. [16] Next week the high performance computing community will be meeting again at the<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://www.linkedin.com/company/sc-conference/">SC Conference Series</a><span class="white-space-pre"> </span>Supercomputing 2024, this year in Atlanta. Hybrid workflows for scientific and engineering applications - involving classical HPC, AI-focused clusters, and now also quantum computers - will be among the very many topics discussed.[17] And we should expect some surprises - in the new rankings for example of top machines on the planet, at least, the ones they want us to know about. [18]</span></p>
<p class="ember-view reader-text-block__paragraph" id="ember2163"><span style="font-family: verdana;">Perhaps I'll report back on some of those returns in a future blog. Best regards. - andy<span class="white-space-pre"> </span></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember2164"><span style="font-family: verdana;"><br /></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember2165"><span><span style="font-family: verdana; font-size: x-large;">References &amp; Amusements<span class="white-space-pre"> </span></span></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember2166"><span style="font-family: verdana;">[0] Banner photo by Ben Wicks on Unsplash</span></p>
<p class="ember-view reader-text-block__paragraph" id="ember2167"><span style="font-family: verdana;">[1] "Surfing the Singularity: The Universe Computes", A. Gallo,<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://www.linkedin.com/pulse/surfing-singularity-universe-computes-andy-gallo-6fgle" target="_self">https://www.linkedin.com/pulse/surfing-singularity-universe-computes-andy-gallo-6fgle</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember2168"><span style="font-family: verdana;">[2] TIOBE ranking of programming language popularity:<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://www.tiobe.com/tiobe-index/" target="_self">https://www.tiobe.com/tiobe-index/</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember2169"><span style="font-family: verdana;">[3] Safe C++, with some chronology of the government statements:<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://safecpp.org/" target="_self">https://safecpp.org/</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember2170"><span style="font-family: verdana;">[4] SYCL:<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://www.khronos.org/sycl/" target="_self">https://www.khronos.org/sycl/</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember2171"><span style="font-family: verdana;">[5] "Post-variational quantum neural networks",<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://pennylane.ai/qml/demos/tutorial_post-variational_quantum_neural_networks" target="_self">https://pennylane.ai/qml/demos/tutorial_post-variational_quantum_neural_networks</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember2172"><span style="font-family: verdana;">[6] "Hope Versus Hype: Quantum, AI and the Path to Commercial Advantage", Matthias Troyer, presentation at IEEE Quantum Week, Montreal, September 2024.</span></p>
<p class="ember-view reader-text-block__paragraph" id="ember2173"><span style="font-family: verdana;">[7] LLVM:<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://llvm.org/" target="_self">https://llvm.org/</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember2174"><span style="font-family: verdana;">[8]<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://amturing.acm.org/award_winners/allen_1012327.cfm" target="_self">https://amturing.acm.org/award_winners/allen_1012327.cfm</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember2175"><span style="font-family: verdana;">[9] "Industrial Experience Deploying Heterogeneous Platforms for Use in Multi-Modal Power Systems Design Workflows", A. Gallo et al,<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://drive.google.com/file/d/1c3YEVmEAUjbI5urj4PiV2TtjzBUzLlws" target="_self">https://drive.google.com/file/d/1c3YEVmEAUjbI5urj4PiV2TtjzBUzLlws</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember2176"><span style="font-family: verdana;">[10] "Mending Wall, Robert Frost,<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://www.poetryfoundation.org/poems/44266/mending-wall" target="_self">https://www.poetryfoundation.org/poems/44266/mending-wall</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember2177"><span style="font-family: verdana;">[11] NERSC SuperFacility API:<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://docs.nersc.gov/services/sfapi/" target="_self">https://docs.nersc.gov/services/sfapi/</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember2178"><span style="font-family: verdana;">[12] "The FAIR Guiding</span></p>
<p class="ember-view reader-text-block__paragraph" id="ember2179"><span style="font-family: verdana;">Principles for scientific data management and stewardship", Mark D. Wilkinson et al.,<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC4792175/pdf/sdata201618.pdf" target="_self">https://pmc.ncbi.nlm.nih.gov/articles/PMC4792175/pdf/sdata201618.pdf</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember2180"><span style="font-family: verdana;">[13] lwfm,<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://github.com/lwfm-proj/lwfm" target="_self">https://github.com/lwfm-proj/lwfm</a><span class="white-space-pre"> </span></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember2181"><span style="font-family: verdana;">[14] "Model Maturity Web",<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://richardarthur.medium.com/co-design-web-6f37664ac1e1" target="_self">https://richardarthur.medium.com/co-design-web-6f37664ac1e1</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember2182"><span style="font-family: verdana;">[15] Them's fighting words, and I expect to be roasted for it. But it seems to me that even the most popular software tool kits (no names) which emerged from the massively government funded ExaScale Computing Project failed to gain traction outside of a narrow community, failed to provide sustainable maintenance in the face of the funded end of the ECP, and would thus fair similarly poorly on a spider web analysis of their sustainability, their recommendability.<span class="white-space-pre"> </span></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember2183"><span style="font-family: verdana;">[16] "Workflows Community Summit 2024: Future Trends and Challenges in Scientific Workflows", da Silva et al, "<a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://zenodo.org/records/13844759" target="_self">https://zenodo.org/records/13844759</a>. I participated in the event, as well as the prior in 2022, and you can compare to that report as well: "Workflows Community Summit 2022: A Roadmap Revolution", also da Silva et al,<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://zenodo.org/records/7750670" target="_self">https://zenodo.org/records/7750670</a>.</span></p>
<p class="ember-view reader-text-block__paragraph" id="ember2184"><span style="font-family: verdana;">[17] SC24,<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://sc24.conference-program.com/" target="_self">https://sc24.conference-program.com/</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember2185"><span style="font-family: verdana;">[18] TOP 500 supercomputers, June 2024,<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo" href="https://top500.org/lists/top500/list/2024/06/" target="_self">https://top500.org/lists/top500/list/2024/06/</a><span class="white-space-pre"> </span>- to be updated again before Thanksgiving.<span class="white-space-pre"> </span></span></p>]]></content><author><name>Surfing the Singularity</name></author><category term="surfthesing" /><summary type="html"><![CDATA[Hello and happy fall holidays to you and yours. As I wrote about in the last blog post [1], as quantum computing hardware matures over the next 5 to 10 years from an experimental toy through to utility and then perhaps advantage over classical (for some applications), it will be included into an already diverse and hybrid computing and applications landscape - on-prem computing, mobile and edge, cloud, and now novel types of computing devices which require new thinking and wholly new means of addressing them. How to deal with the burgeoning heterogeneity of the computing landscape - how to write and run apps which produce and consume data across a widening array of devices- is the topic of this post. Language Landscape The Java programming language, once touted in the glory days of "the World Wide Web" as being "write once, deploy anywhere", and in its heyday representing 25% of new application development, is now down below 10%. What's hot? Python (23%), and "the C's", a collection of C, C++, C# and their kin (&gt;24% in total) which are traditionally recompiled for specific hardware platforms. [2] And while Python provides portability, often for performance in math operations it depends on native libraries, built in, you guessed it, the C's. Into this mix wades the US government which has come out recently with a potentially disruptive statement against the use of the C's, citing security concerns due to their free-wheeling memory management, and in spite of efforts like Safe C++, the government is recommending movement to memory safe languages like Rust, currently with just 1% market share, but "with a bullet". [3] Whether it is better to port to Rust or just update to Safe C++ depends on many factors - for example, how good are your docs and test cases - and while there may exist conceptual impedance mismatches between languages, modern AI coding assistants will only increase in capability especially for more rote tasks like porting. Add to this mix the coding of Graphical Processing Units (GPUs) - originally intended for visualizations but now used in applications for almost anything involving matrix math (turns out, lots of stuff). GPUs today are mostly sold by NVIDIA and are programmed in the C's (sometimes with a Python interface) using the NVIDIA CUDA library. These pieces of the application, the "kernels", are hardware dependent, and while many attempts have been made to create hardware-portable frameworks for GPU programming (see SYCL for example [4]), nearly always the newest fastest GPU features are available in the native non-portable form first, leading to vendor lock. (This might be a good time to remember that NVIDIA does not themselves manufacture chips - they design chips which others produce.) The manner in which we program GPUs is similar to the way we program quantum computers, i.e. QPUs - we delegate to them the portions of the application to which they are best suited, program them using device-specific instructions, and weave them back into the holistic solution. Rather than wielding the Java hammer where everything is a virtualized nail, we use the best tool for the job. In quantum computing, for example, "variational" hybrid algorithms are a common theme, where some part of the work and preparation are performed on classical hardware as a setup for a quantum step, and then post-processing the results back on classical hardware for potential iteration to an optimal solution. Two of several emerging patterns for integrating quantum computing into an application solution. [5] This pattern is analogous to what is also common in classical high performance computing (HPC) for applications like weather modeling and other complex simulations - pre-process on commodity hardware, run an HPC job on the big box, and post-process the results. The introduction into the mix of steerage provided by AI models increases the heterogeneity of the complete solution. A blended computing landscape, enabling for example, quantum computing to produce highly precise data to train AI to steer a classical HPC simulation. [6] All these hardware-dependent application pieces for an ever widening array of hardware means that compilers are cool again, and compiler pipelines like LLVM are critical to application development and deployment. [7] Included in this class of development tools are circuit transpilers for quantum hardware which must take into consideration not only the architectural differences between QPUs (e.g. which gates are supported, what's the inter-qubit connectivity like, etc.), but also the changes which can occur in a quantum data center on a daily basis as these new, noisy, and fragile qubits simply fail and go offline, potentially altering the machine's topology. Just-in-time compilation is needed, and compiler optimization is therefore also cool again. Thank you, Frances Allen. [8] Parts is Parts What emerges from this landscape is not a singular executable running on one computer, but rather, multiple application piece parts, written in different languages, running on radically different hardware in sequence and simultaneously, being orchestrated into a complete solution. In other words, a workflow. Back in the day Java's Sun Microsystems (remember them?) asserted "the network is the computer". Now we assert "the workflow is the app". Or more likely, a workflow of workflows. We like to think of these nested workflows in three types: [9] in-situ: the workflow is running all on the same machine (e.g. a local process, an HPC job)intra-site: the workflow is running on different machines within the same connected enterprise (e.g. within the same data center, virtual network, etc.)inter-site: the workflow is running across different machines in different enterprises (e.g. hybrid on-prem and perhaps multi-vendor cloud) With all these compute types, languages, and locations working together to realize the workflow and solution, loose coupling is key - components connected but not dependent - each part minding its own business. In other words, to paraphrase the poet, good interfaces make good neighbors. [10] We use the convenience term "Site" to mean a provider of secure compute and data services. What interfaces must a Site provide? The interface or API can include lots of things, but it must at least provide: 1) authentication and authorization, 2) a means to run components through their lifecycle, 3) a means to manage data being operated on and produced, perhaps being moved into and out of the Site, and 4) some way to get an inventory of the Site's service offerings and provision them for the purposes of running components or holding data. We call these by four functional nicknames: Auth, Run, Repo, and Spin. Four functional pillars of an interoperable computing site. We can see in each of the three types of workflows the need for each of these four functional pillars, albeit some as a no-op or inherited from a higher order workflow. For example, in a "type 1" workflow of components running on a single machine or within an HPC allocation the Auth aspects may be implied to be already addressed - i.e. the user is already logged into the machine or authorized to run on the HPC cluster. But a workflow which utilizes compute resources both on-prem and in the cloud will have to interact at runtime with the "auth" aspects of the cloud provider prior to being able to "run" workloads, or put and get data to various "repos". Most cloud providers provide a means to list available computing resources, to "spin" them up and down. This provisioning itself can be part of an end-to-end workflow: authenticate, get an inventory of available services, spin some up, run jobs on them storing the results, and spin them down. Stuck in the Middle Most cloud providers - from Amazon to IBM Quantum cloud - provide a callable API interface which can be viewed through the lens of Auth, Run, Repo, Spin. So do some of the supercomputers and cutting edge resources provided by the Federal government, most notably those provided by the National Energy Research Scientific Computing Center (NERSC). [11] As Sites, these providers expose their offerings to internal and external workflows, however, they do not themselves promote a means to author these cross-site workflows, to manage them, track them, or keep tabs on all that distributed data. What else is needed? First, since cloud and other service providers have no motivation to standardize their interfaces, a framework super-interface could exist with the ability to plug in drivers for specific service providers. This in theory is the Auth, Run, Repo, Spin interface. Second, since each provider defines their own service and runtime component lifecycle (loosely: start, run, and stop with success or fail end states) there needs to be a way to normalize the status terminology - a "fail" on one site is the same as an "error" on another, "success" means the same thing as "done". This permits the third aspect of a middleware framework - the ability to track running jobs on Sites and trigger other jobs on any Site to run accordingly - i.e. the control flow of the workflow. What about the data? Commonly we need the ability to put data to a Site and get some back - this is the Repo interface of the Site. And while most (but not all) Sites provide some means to store and retrieve data, be it filesystem or S3 object store or database or something else, it would also be nice to be able to say something "meta" about the data - which Site did it come from, what job or application produced it, what other workflow steps on this Site or others consumed it? Some Sites provide storage with metadata (e.g. Amazon S3) but most don't. This metadata comprises the provenance of the data - like a Civil War sword on the Antiques Roadshow, its the paper trail showing where the item came from, proving the item is legit. In a workflow which perhaps produces many pieces of data, perhaps iteratively as it converges on a solution - keeping track of all the data pieces seems, well, important. The acronym FAIR - findable, accessible, interoperable, reusable - seems a good starting point. [12] Open Says Me Our open source project lwfm, the "local workflow manager", attempts to render these concepts as a reference implementation. [13] Its small with minimal Python lib dependencies and can be taken anywhere easily as a single runnable component, its provenancial metadata also easily portable and importable. A typical Site driver - a Python class which implements the Site interface - weighs in around 200 lines of code including the whitespace. Armed with a Site driver for a cloud service, you can author long-running workflows which utilize a mix of compute resources, storage, and data infrastructures, and automatically track the provenancial paper trail. The lwfm middleware component provides some very recognizable services: polling of remote job status status normalization and persistencesystem and user metadata persistenceevent handling, control flow and data flow triggered Should you use this tooling? I wouldn't recommend it. (Huh? Did I hear you correctly?) How many people are working maintaining it? (Two?) What about the community? (Next to none.) The software would fare poorly on a "spider web" analysis of its overall quality - you would not want to recommend it to your boss. A convenient multi-axis assessment framework for software model maturity. [14] The lwfm is a reference implementation of a workflow interop framework, at best. Are there alternatives? OMG are there alternatives! The workflow landscape is notoriously rich, fragmented, and super-niched. But portability and interoperability are often neglected as is data provenance. Government or university projects, while well meaning and sometimes directionally correct, quickly go stale when the funding elapses [15], and commercial solutions while often suffering some of the same deficiencies offer the added trap of vendor lock and can come with a hefty price tag. Order, Order So its back to committee. [16] Next week the high performance computing community will be meeting again at the SC Conference Series Supercomputing 2024, this year in Atlanta. Hybrid workflows for scientific and engineering applications - involving classical HPC, AI-focused clusters, and now also quantum computers - will be among the very many topics discussed.[17] And we should expect some surprises - in the new rankings for example of top machines on the planet, at least, the ones they want us to know about. [18] Perhaps I'll report back on some of those returns in a future blog. Best regards. - andy References &amp; Amusements [0] Banner photo by Ben Wicks on Unsplash [1] "Surfing the Singularity: The Universe Computes", A. Gallo, https://www.linkedin.com/pulse/surfing-singularity-universe-computes-andy-gallo-6fgle [2] TIOBE ranking of programming language popularity: https://www.tiobe.com/tiobe-index/ [3] Safe C++, with some chronology of the government statements: https://safecpp.org/ [4] SYCL: https://www.khronos.org/sycl/ [5] "Post-variational quantum neural networks", https://pennylane.ai/qml/demos/tutorial_post-variational_quantum_neural_networks [6] "Hope Versus Hype: Quantum, AI and the Path to Commercial Advantage", Matthias Troyer, presentation at IEEE Quantum Week, Montreal, September 2024. [7] LLVM: https://llvm.org/ [8] https://amturing.acm.org/award_winners/allen_1012327.cfm [9] "Industrial Experience Deploying Heterogeneous Platforms for Use in Multi-Modal Power Systems Design Workflows", A. Gallo et al, https://drive.google.com/file/d/1c3YEVmEAUjbI5urj4PiV2TtjzBUzLlws [10] "Mending Wall, Robert Frost, https://www.poetryfoundation.org/poems/44266/mending-wall [11] NERSC SuperFacility API: https://docs.nersc.gov/services/sfapi/ [12] "The FAIR Guiding Principles for scientific data management and stewardship", Mark D. Wilkinson et al., https://pmc.ncbi.nlm.nih.gov/articles/PMC4792175/pdf/sdata201618.pdf [13] lwfm, https://github.com/lwfm-proj/lwfm [14] "Model Maturity Web", https://richardarthur.medium.com/co-design-web-6f37664ac1e1 [15] Them's fighting words, and I expect to be roasted for it. But it seems to me that even the most popular software tool kits (no names) which emerged from the massively government funded ExaScale Computing Project failed to gain traction outside of a narrow community, failed to provide sustainable maintenance in the face of the funded end of the ECP, and would thus fair similarly poorly on a spider web analysis of their sustainability, their recommendability. [16] "Workflows Community Summit 2024: Future Trends and Challenges in Scientific Workflows", da Silva et al, "https://zenodo.org/records/13844759. I participated in the event, as well as the prior in 2022, and you can compare to that report as well: "Workflows Community Summit 2022: A Roadmap Revolution", also da Silva et al, https://zenodo.org/records/7750670. [17] SC24, https://sc24.conference-program.com/ [18] TOP 500 supercomputers, June 2024, https://top500.org/lists/top500/list/2024/06/ - to be updated again before Thanksgiving.]]></summary></entry><entry><title type="html">Surfing the Singularity - The Universe Computes</title><link href="https://hpc.social/personal-blog/2024/surfing-the-singularity-the-universe-computes/" rel="alternate" type="text/html" title="Surfing the Singularity - The Universe Computes" /><published>2024-09-30T16:00:00-06:00</published><updated>2024-09-30T16:00:00-06:00</updated><id>https://hpc.social/personal-blog/2024/surfing-the-singularity-the-universe-computes</id><content type="html" xml:base="https://hpc.social/personal-blog/2024/surfing-the-singularity-the-universe-computes/"><![CDATA[<p><span style="font-family: verdana;"><span>Just back from the</span><span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://www.linkedin.com/company/ieee-computer-society/" target="_self">IEEE Computer Society</a><span class="white-space-pre"> </span><span>Quantum Week in Montreal, and besides eating my weight in pastry and bagels [1], it was a great conference. The collective hardware roadmaps from the major players leaves us thinking the big wave in quantum computing is not here yet, but soon - perhaps in 5 years time for scientific applications, and within a decade for commercial utility. While the current software continues to be sparse and low-level, there are inklings of software engineers starting to build up a stack in anticipation of needing one. But besides that, and at the risk of sounding like the other hot topic - AI marketeer hype [2] - there is the sense with quantum of being present at a new phase in computing at least, if not something larger still.</span><span class="white-space-pre"> </span></span></p>
<blockquote class="ember-view reader-text-block__blockquote" id="ember3175"><span style="color: #04ff00; font-family: verdana;">The concept of the computing universe is still just a hypothesis; nothing has been proved. However, I am confident that this idea can help unveil the secrets of nature. - Konrad Zuse, 1969 [3]</span></blockquote>
<p class="ember-view reader-text-block__paragraph" id="ember3176"><span style="font-family: verdana;">It seems, at its core, that the universe computes. Conch shells grow in logarithmic spirals, bees and orb weavers understand structural geometry. Many animals - not including Mr. Ed or Clever Hans [4], but including primates, fish, and rats - have been shown to use simple arithmetic or approximations, and in other cases can show ability to order objects in a list. There are lizards and sea shells with surface patterns constructed by cellular automata processes - simple rules which can produce complex structures, like Conway's "Game of Life". Ducks, using an inherently quantum mechanical biological process, can see magnetic fields giving them a kind of "heads up" display when migrating. [5]<span class="white-space-pre"> </span></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3177"><span style="font-family: verdana;"><br /></span></p>
<div class="reader-image-block reader-image-block--full-width"><figure class="reader-image-block__figure"><div class="ivm-image-view-model   "><div class="ivm-view-attr__img-wrapper
        
        "><img alt="" class="ivm-view-attr__img--centered  reader-image-block__img evi-image lazy-image ember-view" id="ember3178" src="https://media.licdn.com/dms/image/v2/D4E12AQF2LHaealtiYA/article-inline_image-shrink_1000_1488/article-inline_image-shrink_1000_1488/0/1727727559513?e=1740009600&amp;v=beta&amp;t=vLl0WFGaEuH2xJfhffuvIBSRwDXRnMtGU8m-kADT2c4" /></div>
</div>
<figcaption class="reader-image-block__figure-image-caption display-block full-width text-body-small-open t-sans text-align-center t-black--light"><span style="font-family: verdana;">A variation on Conway's "Game of Life" [6]</span></figcaption></figure></div>
<p class="ember-view reader-text-block__paragraph" id="ember3179"><span style="font-family: verdana;">Our own eyes are themselves literally photon detectors, our retinas and optic nerves pre-processing the signals before they even get to the brain. DNA stores vast amounts of information in simple patterns of four "letters" (effectively two classical bits), recipes to manufacture from the raw material of the universe a wide array of proteins for all manner of biological purposes, including of course growing your own brain. Humans can themselves perform the manual calculations necessary to build bridges and other structures which can span and withstand the forces of nature for centuries. Its also not hard to see computation of a kind in plants and their systemic networks of roots. Is the universe computing, or is the universe doing the only thing it can based on the rules? Water flowing downhill. Lightning finding its own path of least resistance. Entangled electrons separated at distance flipping their spin in response to their partner, in real time. [7]</span></p>
<div class="separator" style="clear: both; text-align: center;"></div>
<p><br />&lt;div class="reader-embed-block__iframe-embed"&gt;<br />&lt;/div&gt;</p>
<div class="reader-embed-block__iframe-embed"><br /></div>
<div class="reader-embed-block__iframe-embed"></div>
<h3 class="ember-view reader-text-block__heading-3" id="ember3181"><span style="font-family: verdana; font-size: x-large;">An Entangled History</span></h3>
<blockquote class="ember-view reader-text-block__blockquote" id="ember3182"><span style="color: #04ff00; font-family: verdana;">Nature isn't classical, dammit, and if you want to make a simulation of nature, you'd better make it quantum mechanical. - Richard Feynman [8]<span class="white-space-pre"> </span></span></blockquote>
<blockquote class="ember-view reader-text-block__blockquote" id="ember3183"><span style="color: #04ff00; font-family: verdana;">I think I can safely say that nobody understands quantum mechanics. - Richard Feynman [9]</span></blockquote>
<p class="ember-view reader-text-block__paragraph" id="ember3184"><span style="font-family: verdana;">In the 1920s physicists like Heisenberg, Born, Pauli, and Schrödinger convinced their peers of the validity of a new formulation of the laws of physics they called (in German) "quantum mechanics", describing the behavior of nature and the universe even below the scale of atoms. This then led computing pioneer John Von Neumann in the 1930s to solidify some of the necessary maths to perform discrete quantum mechanical calculations. Von Neumann, a member of the Manhattan Project, would go on to formulate the hardware architecture for today's "classical" computers, the approach to computing being challenged by quantum computing today.<span class="white-space-pre"> </span></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3185"><span style="font-family: verdana;">Since then, mostly notably in the 1940s with the Manhattan Project, humans have shown an increasing ability to harness the basic quantum physics into a new range of applications. We learned how to make superconductors, and how to park and manipulate individual atoms like tinkertoys [10], and now we've learned how to use these skills to make computers, with most immediate applications in modeling quantum systems like atoms and molecules, as Nobel laureate and Manhattan Project member Feynman predicted more than 40 years ago. We learned how to make lasers and LEDs, and we can now also harness photons for computing. The scientists have had nearly a century to refine their theories, and are now handing off to the engineers to prove the depth of human understanding by building things in the real world, with the business people eagerly waiting on the sidelines in anticipation (think: MRI machines). It seems that the universe computes - we now endeavor to make use of that knowledge for our own human purposes.<span class="white-space-pre"> </span></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3186"><span style="font-family: verdana;"><br /></span></p>
<h3 class="ember-view reader-text-block__heading-3" id="ember3187"><span style="font-family: verdana; font-size: x-large;">Qubits, Gates, and Error Everywhere</span></h3>
<p class="ember-view reader-text-block__paragraph" id="ember3188"><span style="font-family: verdana;">What is a qubit? Like the early video game Q*bert which showed a simulated 3D world on a 2D screen back in 1982, a qubit is a little hard to visualize as it goes deeper than a classical binary bit - much deeper. While a bit can be either a zero or a one but nothing else, a qubit can model the<span class="white-space-pre"> </span><span>probability</span><span class="white-space-pre"> </span>of a zero or a one and probabilistically anything in between. It might be a zero, or it might be a one, with some probability of each. It might start off a zero, and then noise from the environment might cause it to drift, or it might move off the zero purposefully as a result of acting on the qubit with one of several kinds of single and multi-qubit gates. Like gates in classical computing, a quantum gate can flip the qubit, or unique to quantum just nudge it a little. As in classical computing, its the acting on the qubit by gates which results in the computing. A native quantum program is a circuit, a directed graph, composed of gates.<span class="white-space-pre"> </span></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3189"><span style="font-family: verdana;"><br /></span></p>
<div class="reader-image-block reader-image-block--full-width"><figure class="reader-image-block__figure"><div class="ivm-image-view-model   "><div class="ivm-view-attr__img-wrapper
        
        "><img alt="" class="ivm-view-attr__img--centered  reader-image-block__img evi-image lazy-image ember-view" id="ember3190" src="https://media.licdn.com/dms/image/v2/D4E12AQFAklbbJReWZA/article-inline_image-shrink_1500_2232/article-inline_image-shrink_1500_2232/0/1727726980638?e=1740009600&amp;v=beta&amp;t=vnbWE-YUgsGxlQf5M2FnYJ7DLtJl-P7NvK_b0j3YVw0" /></div>
</div>
<figcaption class="reader-image-block__figure-image-caption display-block full-width text-body-small-open t-sans text-align-center t-black--light"><span style="font-family: verdana;">Visualizing the effect of various quantum gates on a single qubit. [11]</span></figcaption></figure></div>
<p class="ember-view reader-text-block__paragraph" id="ember3191"><span class="white-space-pre"><span style="font-family: verdana;"> </span></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3192"><span style="font-family: verdana;">How fast can we flip a qubit? Heisenberg 100 years ago gave us a way to compute the lower bound on the time to flip a spin given an energy - in short, its fast. But this doesn't even tell the whole performance story because of superposition and the ability of wide circuits to act on multiple qubits simultaneously - the speed advantage of quantum over classical can be exponential, albeit application-specific, making a class of problems which would be classically uncomputable in any human lifetime now well within reach.<span class="white-space-pre"> </span></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3193"><span style="font-family: verdana;">The main challenge in realizing these potentials is the noise. Qubits are noisy, meaning they don't stay fixed where you think you last left them, and the seemingly magical entanglement also can show decoherence over time and distance. The gates necessary for computation can themselves introduce noisy error, as can the act of measuring the qubit to sample the solution result. Software algorithms can also just be estimates, and thus introduce their own error term relative to experiment. The prevalence of error in quantum computing means the algorithms themselves must be aware the results might be unpredictable, might need to be computed more then once to improve confidence, and might need to allocate and use a good number of precious qubits just to help mitigate the errors. We call the period we are in today the "NISQ era", meaning, noisy intermediate scale quantum computing.<span class="white-space-pre"> </span></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3194"><span style="font-family: verdana;">Beyond the noise, how do we quantify "scale" or other metrics for sizing up the capabilities of quantum computers, now, in future, and as compared to classical? One aspect of the problem is that when comparing quantum to classical we're not comparing apples to apples, and even within "quantum apples" as we have seen, there are different kinds. [12] In one simple measure we can count qubits, but we must also know their error rate, and we must notice something about their connectedness - some quantum hardware use an all-to-all grid, and others use other topologies - racetracks and the like. And qubits can fail. Because of the limitations of qubits in the NISQ era the connectedness matters, is necessary to be known at time of circuit transpilation, and may result in swaps or other strategies employed by the transpiler toolchain to minimize errors due to the physical layout. Qubit coherence in superposition can be measured and reported as a hardware spec. Quantum<span class="white-space-pre"> </span><span>volume</span><span class="white-space-pre"> </span>is a number which expresses the size of a circuit N qubits wide by d gates deep which can be executed on a given machine. Gate errors especially for 2-qubit gates can be reported by the vendor. CLOPS - circuit layer operations per second - is another proposed metric which takes into consideration the time to prepare the qubits, execute the gates, and take the measurement of the result. [13] The US government in the form of<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://www.linkedin.com/company/darpa/">Defense Advanced Research Projects Agency (DARPA)</a><span class="white-space-pre"> </span>has gotten into the game of studying this varied performance landscape, towards being able to help pick winners and losers and accelerate innovation with funding awards. [14]<span class="white-space-pre"> </span></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3195"><span style="font-family: verdana;"><br /></span></p>
<h3 class="ember-view reader-text-block__heading-3" id="ember3196"><span style="font-family: verdana; font-size: x-large;">Quantum Hardware Roadmap</span></h3>
<blockquote class="ember-view reader-text-block__blockquote" id="ember3197"><span style="color: #04ff00; font-family: verdana;">I build quantum computers that store information on individual atoms and then massage the normal interactions between atoms to make them compute. - Seth Lloyd [15]</span></blockquote>
<p class="ember-view reader-text-block__paragraph" id="ember3198"><span style="font-family: verdana;">This year's IEEE Quantum Week was an opportunity to see and hear from most of the major players in quantum computing R&amp;D - those focused on quantum processors, systems control, networking, and software. The software topic we'll leave as a topic of a future blog, but focusing on hardware, the vendors collectively represented multiple distinct technical mechanisms to making a quantum computing machine. There's superconducting qubits from US companies like<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://www.linkedin.com/showcase/ibm-quantum/">IBM Quantum</a><span class="white-space-pre"> </span>,<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://www.linkedin.com/company/google/">Google</a><span class="white-space-pre"> </span>, and<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://www.linkedin.com/company/rigetti-computing/">Rigetti Computing</a>, which refrigerate and maintain the qubit between a ground and an excited state. Trapped ion computers from companies like<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://www.linkedin.com/company/ionq-co/">IonQ</a><span class="white-space-pre"> </span>and<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://www.linkedin.com/company/quantinuumqc/">Quantinuum</a><span class="white-space-pre"> </span>, and neutral atom computers from<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://www.linkedin.com/company/quera-computing-inc/">QuEra Computing Inc.</a><span class="white-space-pre"> </span>use novel methods to again cool the system qubits to near absolute zero. But there are quantum computers which also operate quite differently. Quantum annealing, or algorithmically simulating an adiabatic process for slowly evolving a system to an optimal state, could be simulated on one of the above general quantum machines, or shown more directly on a specialized quantum machine from a Canadian company like<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://www.linkedin.com/company/d-wave-quantum/">D-Wave</a>.<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://www.linkedin.com/company/xanaduai/">Xanadu</a>, also based in Canada, performs its quantum tricks with photonics. And while Google may jump the gun on announcing successes from time to time, they and<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://www.linkedin.com/company/microsoft/">Microsoft</a><span class="white-space-pre"> </span>and others are working on a "topological qubit" based on previously only-hypothesized Majorana particles which provide the great advantage relative to other qubit implementations of being able to be controlled digitally. [16, 17]<span class="white-space-pre"> </span></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3199"><span style="font-family: verdana;">Staying within the NISQ era as the machines scale up, a good chunk of the available qubits will continue to be allocated to error correction schemes, a task which may later as these systems mature be allocated to a software layer. At 100s of useful error-corrected qubits we can start to gain real scientific utility from quantum computing - begin to do research<span class="white-space-pre"> </span><span>with</span><span class="white-space-pre"> </span>quantum rather than research<span class="white-space-pre"> </span><span>about</span><span class="white-space-pre"> </span>quantum. Vendors such as Quantinuum promise a fully connected machine of that size in 5 years. In 10 years, vendors expect to deliver machines with 1000s of QEC, which will usher in commercial utility, and the era of "cryptographically relevant" quantum computing (i.e. DARPA wants the US to get there first [18]).<span class="white-space-pre"> </span></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3200"><span style="font-family: verdana;">In the meantime, certain scientific domains, those which study things most closely associated with real quantum systems, will be early adopters of the technology. Molecular biology. Chemistry, for example, studying better ways to perform synthetic nitrogen fixation (think: energy-costly ammonia production for fertilizers).<span class="white-space-pre"> </span></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3201"><span style="font-family: verdana;"><br /></span></p>
<h3 class="ember-view reader-text-block__heading-3" id="ember3202"><span style="font-family: verdana; font-size: x-large;">Conclusion<span class="white-space-pre"> </span></span></h3>
<p class="ember-view reader-text-block__paragraph" id="ember3203"><span style="font-family: verdana;">The quantum hardware industry is in its infancy. From this gaggle of eager go-getters it’s reasonable to assume there will be technical and business winners and losers. For reasons of national security, governments will ramp up their involvement. But current machines are small, flaky, and limited in usefulness. It will be 5 to 10 years before there are quantum computers being used more commonly. A new but also a familiar approach to software will be needed - more on that in a future blog. Until utility some industries will be early leaders, ready to capitalize on an exponential increase in computing capability, one which promises to get us closer to harnessing the grand computing engine of the universe which is all around and within.</span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3204"><span style="font-family: verdana;"><br /></span></p>
<h3 class="ember-view reader-text-block__heading-3" id="ember3205"><span style="font-family: verdana; font-size: x-large;">References &amp; Trivia</span></h3>
<p class="ember-view reader-text-block__paragraph" id="ember3206"><span style="font-family: verdana;">[0] Photo by Ben Wicks on Unsplash</span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3207"><span style="font-family: verdana;">[1] Montreal bagels:<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://www.mtl.org/en/experience/the-famous-montreal-bagel" target="_self">https://www.mtl.org/en/experience/the-famous-montreal-bagel</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3208"><span style="font-family: verdana;">[2] Gartner AI Hype Cycle 2024 explained:<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://www.youtube.com/watch?v=qXKYOR3KqxQ" target="_self">https://www.youtube.com/watch?v=qXKYOR3KqxQ</a><span class="white-space-pre"> </span></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3209"><span style="font-family: verdana;">[3] "Calculating Space", Konrad Zuse, 1969,<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://philpapers.org/archive/ZUSRR.pdf" target="_self">https://philpapers.org/archive/ZUSRR.pdf</a><span class="white-space-pre"> </span>Its worth noting that while having worked for Ford Motor Co. in his early career, and like Von Neumann doing very important early work on computers, Zuse was a conscripted employee of the German Nazi government from 1939-1945.</span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3210"><span style="font-family: verdana;">[4] Clever Hans:<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://www.horsejournals.com/popular/history-heritage/clever-hans" target="_self">https://www.horsejournals.com/popular/history-heritage/clever-hans</a><span class="white-space-pre"> </span></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3211"><span style="font-family: verdana;">[5] "How Migrating Birds Use Quantum Effects to Navigate", Scientific American, April 2022,<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://www.scientificamerican.com/article/how-migrating-birds-use-quantum-effects-to-navigate/" target="_self">https://www.scientificamerican.com/article/how-migrating-birds-use-quantum-effects-to-navigate/</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3212"><span style="font-family: verdana;">[6] A variation on Conway's Game of Life,<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://stackoverflow.com/questions/70019538/simple-animation-for-conways-game-of-life-with-funcanimation" target="_self">https://stackoverflow.com/questions/70019538/simple-animation-for-conways-game-of-life-with-funcanimation</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3213"><span style="font-family: verdana;">[7] "Real-Time Imaging of Quantum Entanglement", 2013,<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://youtu.be/wGkx1MUw2TU?si=mnIExRs2ZOwv46Bh" target="_self">https://youtu.be/wGkx1MUw2TU?si=mnIExRs2ZOwv46Bh</a>, but not strangely enough "Entanglement between superconducting qubits and a tardigrade",<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://arxiv.org/pdf/2112.07978" target="_self">https://arxiv.org/pdf/2112.07978</a><span class="white-space-pre"> </span></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3214"><span style="font-family: verdana;">[8] "Simulating Physics with Computers", International Journal of Theoretical Physics vol 21, transcript of a talk at MIT by Richard Feynman, 1981,<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://s2.smu.edu/~mitch/class/5395/papers/feynman-quantum-1981.pdf" target="_self">https://s2.smu.edu/~mitch/class/5395/papers/feynman-quantum-1981.pdf</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3215"><span style="font-family: verdana;">[9] "The Character of Physical Law", transcript of lectures by Richard Feynman at Cornell U, 1967,<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://archive.org/details/characterofphysi0000feyn/page/12/mode/2up" target="_self">https://archive.org/details/characterofphysi0000feyn/page/12/mode/2up</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3216"><span style="font-family: verdana;">[10] "2 Researchers Spell 'I.B.M.' Atom by Atom", New York Times, April 5, 1990,<a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://timesmachine.nytimes.com/timesmachine/1990/04/05/356490.html?pageNumber=41" target="_self">https://timesmachine.nytimes.com/timesmachine/1990/04/05/356490.html?pageNumber=41</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3217"><span style="font-family: verdana;">[11] "Qubit Bloch Sphere Visualization", Casey Duckering,<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://raw.githubusercontent.com/cduck/bloch_sphere/master/examples/xyss_gate.gif" target="_self">https://raw.githubusercontent.com/cduck/bloch_sphere/master/examples/xyss_gate.gif</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3218"><span style="font-family: verdana;">[12] Its apple picking season here in New York:<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://www.applesfromny.com/varieties/" target="_self">https://www.applesfromny.com/varieties/</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3219"><span style="font-family: verdana;">[13] "Driving quantum performance: more qubits, higher Quantum Volume, and now a proper measure of speed",<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://www.ibm.com/quantum/blog/circuit-layer-operations-per-second" target="_self">https://www.ibm.com/quantum/blog/circuit-layer-operations-per-second</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3220"><span style="font-family: verdana;">[14] DARPA Quantum Benchmarking Initiative,<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://www.darpa.mil/work-with-us/quantum-benchmarking-initiative" target="_self">https://www.darpa.mil/work-with-us/quantum-benchmarking-initiative</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3221"><span style="font-family: verdana;">[15] "The Computational Universe", Seth Lloyd, 2002,<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://www.edge.org/conversation/seth_lloyd-the-computational-universe" target="_self">https://www.edge.org/conversation/seth_lloyd-the-computational-universe</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3222"><span style="font-family: verdana;">[16] "Google Claims To Achieve Quantum Supremacy — IBM Pushes Back",<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://www.npr.org/2019/10/23/772710977/google-claims-to-achieve-quantum-supremacy-ibm-pushes-back" target="_self">https://www.npr.org/2019/10/23/772710977/google-claims-to-achieve-quantum-supremacy-ibm-pushes-back</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3223"><span style="font-family: verdana;">[17] "A route to scalable Majorana qubits",<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://phys.org/news/2024-06-route-scalable-majorana-qubits.html" target="_self">https://phys.org/news/2024-06-route-scalable-majorana-qubits.html</a></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3224"><span style="font-family: verdana;">[18] "DARPA's quantum computing is powered by ... FOMO",<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://www.theregister.com/2023/02/02/darpa_quantum_microsoft/" target="_self">https://www.theregister.com/2023/02/02/darpa_quantum_microsoft/</a></span></p>
<div><br /></div>]]></content><author><name>Surfing the Singularity</name></author><category term="surfthesing" /><summary type="html"><![CDATA[Just back from the IEEE Computer Society Quantum Week in Montreal, and besides eating my weight in pastry and bagels [1], it was a great conference. The collective hardware roadmaps from the major players leaves us thinking the big wave in quantum computing is not here yet, but soon - perhaps in 5 years time for scientific applications, and within a decade for commercial utility. While the current software continues to be sparse and low-level, there are inklings of software engineers starting to build up a stack in anticipation of needing one. But besides that, and at the risk of sounding like the other hot topic - AI marketeer hype [2] - there is the sense with quantum of being present at a new phase in computing at least, if not something larger still. The concept of the computing universe is still just a hypothesis; nothing has been proved. However, I am confident that this idea can help unveil the secrets of nature. - Konrad Zuse, 1969 [3] It seems, at its core, that the universe computes. Conch shells grow in logarithmic spirals, bees and orb weavers understand structural geometry. Many animals - not including Mr. Ed or Clever Hans [4], but including primates, fish, and rats - have been shown to use simple arithmetic or approximations, and in other cases can show ability to order objects in a list. There are lizards and sea shells with surface patterns constructed by cellular automata processes - simple rules which can produce complex structures, like Conway's "Game of Life". Ducks, using an inherently quantum mechanical biological process, can see magnetic fields giving them a kind of "heads up" display when migrating. [5] A variation on Conway's "Game of Life" [6] Our own eyes are themselves literally photon detectors, our retinas and optic nerves pre-processing the signals before they even get to the brain. DNA stores vast amounts of information in simple patterns of four "letters" (effectively two classical bits), recipes to manufacture from the raw material of the universe a wide array of proteins for all manner of biological purposes, including of course growing your own brain. Humans can themselves perform the manual calculations necessary to build bridges and other structures which can span and withstand the forces of nature for centuries. Its also not hard to see computation of a kind in plants and their systemic networks of roots. Is the universe computing, or is the universe doing the only thing it can based on the rules? Water flowing downhill. Lightning finding its own path of least resistance. Entangled electrons separated at distance flipping their spin in response to their partner, in real time. [7] &lt;div class="reader-embed-block__iframe-embed"&gt;&lt;/div&gt; An Entangled History Nature isn't classical, dammit, and if you want to make a simulation of nature, you'd better make it quantum mechanical. - Richard Feynman [8] I think I can safely say that nobody understands quantum mechanics. - Richard Feynman [9] In the 1920s physicists like Heisenberg, Born, Pauli, and Schrödinger convinced their peers of the validity of a new formulation of the laws of physics they called (in German) "quantum mechanics", describing the behavior of nature and the universe even below the scale of atoms. This then led computing pioneer John Von Neumann in the 1930s to solidify some of the necessary maths to perform discrete quantum mechanical calculations. Von Neumann, a member of the Manhattan Project, would go on to formulate the hardware architecture for today's "classical" computers, the approach to computing being challenged by quantum computing today. Since then, mostly notably in the 1940s with the Manhattan Project, humans have shown an increasing ability to harness the basic quantum physics into a new range of applications. We learned how to make superconductors, and how to park and manipulate individual atoms like tinkertoys [10], and now we've learned how to use these skills to make computers, with most immediate applications in modeling quantum systems like atoms and molecules, as Nobel laureate and Manhattan Project member Feynman predicted more than 40 years ago. We learned how to make lasers and LEDs, and we can now also harness photons for computing. The scientists have had nearly a century to refine their theories, and are now handing off to the engineers to prove the depth of human understanding by building things in the real world, with the business people eagerly waiting on the sidelines in anticipation (think: MRI machines). It seems that the universe computes - we now endeavor to make use of that knowledge for our own human purposes. Qubits, Gates, and Error Everywhere What is a qubit? Like the early video game Q*bert which showed a simulated 3D world on a 2D screen back in 1982, a qubit is a little hard to visualize as it goes deeper than a classical binary bit - much deeper. While a bit can be either a zero or a one but nothing else, a qubit can model the probability of a zero or a one and probabilistically anything in between. It might be a zero, or it might be a one, with some probability of each. It might start off a zero, and then noise from the environment might cause it to drift, or it might move off the zero purposefully as a result of acting on the qubit with one of several kinds of single and multi-qubit gates. Like gates in classical computing, a quantum gate can flip the qubit, or unique to quantum just nudge it a little. As in classical computing, its the acting on the qubit by gates which results in the computing. A native quantum program is a circuit, a directed graph, composed of gates. Visualizing the effect of various quantum gates on a single qubit. [11] How fast can we flip a qubit? Heisenberg 100 years ago gave us a way to compute the lower bound on the time to flip a spin given an energy - in short, its fast. But this doesn't even tell the whole performance story because of superposition and the ability of wide circuits to act on multiple qubits simultaneously - the speed advantage of quantum over classical can be exponential, albeit application-specific, making a class of problems which would be classically uncomputable in any human lifetime now well within reach. The main challenge in realizing these potentials is the noise. Qubits are noisy, meaning they don't stay fixed where you think you last left them, and the seemingly magical entanglement also can show decoherence over time and distance. The gates necessary for computation can themselves introduce noisy error, as can the act of measuring the qubit to sample the solution result. Software algorithms can also just be estimates, and thus introduce their own error term relative to experiment. The prevalence of error in quantum computing means the algorithms themselves must be aware the results might be unpredictable, might need to be computed more then once to improve confidence, and might need to allocate and use a good number of precious qubits just to help mitigate the errors. We call the period we are in today the "NISQ era", meaning, noisy intermediate scale quantum computing. Beyond the noise, how do we quantify "scale" or other metrics for sizing up the capabilities of quantum computers, now, in future, and as compared to classical? One aspect of the problem is that when comparing quantum to classical we're not comparing apples to apples, and even within "quantum apples" as we have seen, there are different kinds. [12] In one simple measure we can count qubits, but we must also know their error rate, and we must notice something about their connectedness - some quantum hardware use an all-to-all grid, and others use other topologies - racetracks and the like. And qubits can fail. Because of the limitations of qubits in the NISQ era the connectedness matters, is necessary to be known at time of circuit transpilation, and may result in swaps or other strategies employed by the transpiler toolchain to minimize errors due to the physical layout. Qubit coherence in superposition can be measured and reported as a hardware spec. Quantum volume is a number which expresses the size of a circuit N qubits wide by d gates deep which can be executed on a given machine. Gate errors especially for 2-qubit gates can be reported by the vendor. CLOPS - circuit layer operations per second - is another proposed metric which takes into consideration the time to prepare the qubits, execute the gates, and take the measurement of the result. [13] The US government in the form of Defense Advanced Research Projects Agency (DARPA) has gotten into the game of studying this varied performance landscape, towards being able to help pick winners and losers and accelerate innovation with funding awards. [14] Quantum Hardware Roadmap I build quantum computers that store information on individual atoms and then massage the normal interactions between atoms to make them compute. - Seth Lloyd [15] This year's IEEE Quantum Week was an opportunity to see and hear from most of the major players in quantum computing R&amp;D - those focused on quantum processors, systems control, networking, and software. The software topic we'll leave as a topic of a future blog, but focusing on hardware, the vendors collectively represented multiple distinct technical mechanisms to making a quantum computing machine. There's superconducting qubits from US companies like IBM Quantum , Google , and Rigetti Computing, which refrigerate and maintain the qubit between a ground and an excited state. Trapped ion computers from companies like IonQ and Quantinuum , and neutral atom computers from QuEra Computing Inc. use novel methods to again cool the system qubits to near absolute zero. But there are quantum computers which also operate quite differently. Quantum annealing, or algorithmically simulating an adiabatic process for slowly evolving a system to an optimal state, could be simulated on one of the above general quantum machines, or shown more directly on a specialized quantum machine from a Canadian company like D-Wave. Xanadu, also based in Canada, performs its quantum tricks with photonics. And while Google may jump the gun on announcing successes from time to time, they and Microsoft and others are working on a "topological qubit" based on previously only-hypothesized Majorana particles which provide the great advantage relative to other qubit implementations of being able to be controlled digitally. [16, 17] Staying within the NISQ era as the machines scale up, a good chunk of the available qubits will continue to be allocated to error correction schemes, a task which may later as these systems mature be allocated to a software layer. At 100s of useful error-corrected qubits we can start to gain real scientific utility from quantum computing - begin to do research with quantum rather than research about quantum. Vendors such as Quantinuum promise a fully connected machine of that size in 5 years. In 10 years, vendors expect to deliver machines with 1000s of QEC, which will usher in commercial utility, and the era of "cryptographically relevant" quantum computing (i.e. DARPA wants the US to get there first [18]). In the meantime, certain scientific domains, those which study things most closely associated with real quantum systems, will be early adopters of the technology. Molecular biology. Chemistry, for example, studying better ways to perform synthetic nitrogen fixation (think: energy-costly ammonia production for fertilizers). Conclusion The quantum hardware industry is in its infancy. From this gaggle of eager go-getters it’s reasonable to assume there will be technical and business winners and losers. For reasons of national security, governments will ramp up their involvement. But current machines are small, flaky, and limited in usefulness. It will be 5 to 10 years before there are quantum computers being used more commonly. A new but also a familiar approach to software will be needed - more on that in a future blog. Until utility some industries will be early leaders, ready to capitalize on an exponential increase in computing capability, one which promises to get us closer to harnessing the grand computing engine of the universe which is all around and within. References &amp; Trivia [0] Photo by Ben Wicks on Unsplash [1] Montreal bagels: https://www.mtl.org/en/experience/the-famous-montreal-bagel [2] Gartner AI Hype Cycle 2024 explained: https://www.youtube.com/watch?v=qXKYOR3KqxQ [3] "Calculating Space", Konrad Zuse, 1969, https://philpapers.org/archive/ZUSRR.pdf Its worth noting that while having worked for Ford Motor Co. in his early career, and like Von Neumann doing very important early work on computers, Zuse was a conscripted employee of the German Nazi government from 1939-1945. [4] Clever Hans: https://www.horsejournals.com/popular/history-heritage/clever-hans [5] "How Migrating Birds Use Quantum Effects to Navigate", Scientific American, April 2022, https://www.scientificamerican.com/article/how-migrating-birds-use-quantum-effects-to-navigate/ [6] A variation on Conway's Game of Life, https://stackoverflow.com/questions/70019538/simple-animation-for-conways-game-of-life-with-funcanimation [7] "Real-Time Imaging of Quantum Entanglement", 2013, https://youtu.be/wGkx1MUw2TU?si=mnIExRs2ZOwv46Bh, but not strangely enough "Entanglement between superconducting qubits and a tardigrade", https://arxiv.org/pdf/2112.07978 [8] "Simulating Physics with Computers", International Journal of Theoretical Physics vol 21, transcript of a talk at MIT by Richard Feynman, 1981, https://s2.smu.edu/~mitch/class/5395/papers/feynman-quantum-1981.pdf [9] "The Character of Physical Law", transcript of lectures by Richard Feynman at Cornell U, 1967, https://archive.org/details/characterofphysi0000feyn/page/12/mode/2up [10] "2 Researchers Spell 'I.B.M.' Atom by Atom", New York Times, April 5, 1990,https://timesmachine.nytimes.com/timesmachine/1990/04/05/356490.html?pageNumber=41 [11] "Qubit Bloch Sphere Visualization", Casey Duckering, https://raw.githubusercontent.com/cduck/bloch_sphere/master/examples/xyss_gate.gif [12] Its apple picking season here in New York: https://www.applesfromny.com/varieties/ [13] "Driving quantum performance: more qubits, higher Quantum Volume, and now a proper measure of speed", https://www.ibm.com/quantum/blog/circuit-layer-operations-per-second [14] DARPA Quantum Benchmarking Initiative, https://www.darpa.mil/work-with-us/quantum-benchmarking-initiative [15] "The Computational Universe", Seth Lloyd, 2002, https://www.edge.org/conversation/seth_lloyd-the-computational-universe [16] "Google Claims To Achieve Quantum Supremacy — IBM Pushes Back", https://www.npr.org/2019/10/23/772710977/google-claims-to-achieve-quantum-supremacy-ibm-pushes-back [17] "A route to scalable Majorana qubits", https://phys.org/news/2024-06-route-scalable-majorana-qubits.html [18] "DARPA's quantum computing is powered by ... FOMO", https://www.theregister.com/2023/02/02/darpa_quantum_microsoft/]]></summary></entry><entry><title type="html">The HPC cluster as a reflection of values</title><link href="https://hpc.social/personal-blog/2024/the-hpc-cluster-as-a-reflection-of-values/" rel="alternate" type="text/html" title="The HPC cluster as a reflection of values" /><published>2024-09-29T22:22:51-06:00</published><updated>2024-09-29T22:22:51-06:00</updated><id>https://hpc.social/personal-blog/2024/the-hpc-cluster-as-a-reflection-of-values</id><content type="html" xml:base="https://hpc.social/personal-blog/2024/the-hpc-cluster-as-a-reflection-of-values/"><![CDATA[<p>Yesterday while I was cooking dinner, I happened to re-watch Bryan Cantrill&#8217;s talk on &#8220;<a href="https://www.youtube.com/watch?v=Xhx970_JKX4">Platform as a Reflection of Values</a>&#8220;. (I watch a lot tech talks while cooking or baking &#8212; I often have trouble focusing on a video unless I&#8217;m doing something with my hands, but if I know a recipe well I can often make it on autopilot.)</p>

<p>If you haven&#8217;t watched this talk before, I encourage checking it out. Cantrill gave it in part to talk about why the node.js community and Joyent didn&#8217;t work well together, but I thought he had some good insights into how values get built into a technical artifact itself, as well as how the community around those artifacts will prioritize certain values.</p>

<p>While I was watching the talk (and chopping some vegetables), I started thinking about what values are most important in the &#8220;HPC cluster platform&#8221;.</p>

<p><span id="more-339"></span></p>

<h2 class="wp-block-heading">Technical values</h2>

<p>This slide from the talk shows some examples of what Cantrill thinks of as platform values:</p>

<figure class="wp-block-image size-full"><img alt="A slide with the title &quot;Some platform values&quot;. The list includes approachability, availability, compatibility, composability, debuggability, expressiveness, extensibility, interoperability, integrity, maintainability, operability, performance, portability, resiliency, rigor, robustness, safety, security, simplicity, thoroughness, transparency, and velocity." class="wp-image-340" height="538" src="https://thinking.ajdecon.org/wp-content/uploads/2024/09/image.png" width="969" /></figure>

<p>A key point from the talk is that all of these are good things! Ideally you want to have <em>all</em> of these things when you build a new platform, whether that&#8217;s a programming language, a cloud platform, or whatever. But any given platform will choose to<em> </em>prioritize some set of values over others. You want them all, but when they come into tension, which ones will win?</p>

<p>One example that Cantrill gives in the talk is the original Unix out of Bell Labs, which prioritized simplicity, composability, and portability. Certainly Unix wanted other features, like performance and maintainability, but if forced into a choice like performance vs simplicity, it would generally choose simplicity. Similarly, he talked about how JavaScript and node.js are built around values like approachability, expressiveness, and velocity, and how that contrasted with values like robustness and debuggability that Joyent valued as a cloud provider.</p>

<h2 class="wp-block-heading">The HPC cluster platform</h2>

<p>When I saw &#8220;HPC cluster platform&#8221;, I&#8217;m loosely talking about the collection of hardware and software that is most often used to build high-performance computing clusters for workloads like scientific research or machine learning training.</p>

<p>This generic platform consists of a large collection of identical compute nodes, orchestrated by a batch scheduler like <a href="https://github.com/SchedMD/slurm">Slurm</a> or <a href="https://github.com/openpbs/openpbs">PBS</a>, and with one or more &#8220;login nodes&#8221; serving as a front-end where users SSH in to prepare and run jobs on the cluster. For multi-node jobs and high-speed storage access, the compute nodes are connected by a very high-speed network, like 100Gb Ethernet or InfiniBand, which needs specific libraries to use effectively. Users on the cluster have access to command-line editors and development tools like compilers and scientific libraries, but mostly interact with the platform in a purely command line environment.</p>

<p>See also, this really ugly Google Draw diagram:</p>

<figure class="wp-block-image size-large"><img alt="A simple diagram showing a login node, a set of compute nodes, and network storage. The login node is connected to compute nodes by a management network. The storage is connected to compute nodes by a high-speed network." class="wp-image-348" height="488" src="https://thinking.ajdecon.org/wp-content/uploads/2024/09/image-1-1024x488.png" width="1024" /></figure>

<p>What values does this platform prioritize? In general, I tend to think that HPC platforms prioritize <em>performance</em>, <em>portability</em>, and <em>approachability</em>.</p>

<p><strong>Performance: </strong>This might seem obvious given the name &#8220;HPC&#8221;, but it&#8217;s worth thinking a little more about. When faced with a choice between performance and some other value, HPC clusters <em>almost always</em> choose performance. <br /><br />Performance is generally performance above cost, with most clusters using expensive compute and networking hardware. It&#8217;s prioritized over observability (&#8220;measurability&#8221; on Cantrill&#8217;s slide?), with most HPC clusters I&#8217;m aware of disabling most active monitoring features if they have a performance cost. It&#8217;s even prioritized above security, often turning off security features if they lead to lower performance or even measurable performance <em>variability</em>.</p>

<p><strong>Portability: </strong>Mindful of the difficulty in writing high-performance, correct scientific code, the HPC platform works reasonably hard to maintain portability to new hardware and software over time. </p>

<p>A lot of this is due to a robust ecosystem of libraries and middleware. Most applications that scale across multiple nodes still use <a href="https://en.wikipedia.org/wiki/Message_Passing_Interface">MPI</a>; code doing linear algebra still depends on long-lived libraries like <a href="https://www.netlib.org/lapack/">LAPACK</a> and <a href="https://www.netlib.org/blas/">BLAS</a>; and platform tools like the scheduler tend to be remarkably stable over time. New hardware features are often abstracted by middleware, especially at the networking level where support is built into your MPI library of choice.</p>

<p>This story isn&#8217;t perfect &#8212; applications usually need recompilation on a new cluster, and still often need major changes to take advantages of new features. That&#8217;s why I chose &#8220;portability&#8221; instead of &#8220;compatibility&#8221;. But as a cluster admin, I&#8217;ve worked with many researchers who have maintained the same app on many different clusters for 10, 20, or even 30 years, which is a pretty impressive portability story.</p>

<p><strong>Approachability: </strong>This one may be controversial! The average HPC cluster can seem pretty arcane, especially for someone new to the platform. But I do think that HPC prioritizes a particular <em>kind</em> of approachability, which is that it is designed to onboard scientific researchers who are not themselves expert developers.</p>

<p>A new user onboarding to a research HPC cluster frequently needs to understand three main tools:</p>

<ul class="wp-block-list">
<li><strong>The Linux shell:</strong> Most HPC cluster environments are entirely command-line oriented (though <a href="https://openondemand.org/">Open OnDemand</a> is helping change this!). You log in with SSH; edit using nano, vim, or emacs; and interact with the system entirely using a shell.</li>



<li><strong>The cluster scheduler: </strong>When you have your application ready to go, you submit your job to a queue using a cluster scheduler like Slurm and wait for it to complete. Cluster schedulers have a lot of moving parts and a user can often find endless knobs to tune, but it&#8217;s easy to get started with just a few commands. (And interestingly, almost all HPC cluster schedulers define their jobs as&#8230; shell scripts! You&#8217;re back to needing to know the shell. Annoying, sure, but at least it ain&#8217;t YAML!)</li>



<li><a href="https://modules.readthedocs.io/en/latest/"><strong>Environment modules</strong></a>: This tool allows the cluster admins to provide a large library of libraries and tools, with specific versions, such that a cluster user just needs to type &#8220;module load openmpi/3&#8221;. While the tool munges the shell environment variables as needed to set up PATH, LD_LIBRARY_PATH, etc just so.</li>
</ul>

<p>Now if this doesn&#8217;t sound like a robust software engineering environment&#8230; it isn&#8217;t! There are endless things that can go wrong, especially with environment modules interacting with the user&#8217;s own shell rc files and who knows what else. And there&#8217;s very little in this environment to encourage best practices like linting, pinned library versions, or even version control at all!</p>

<p>But this environment is <em>approachable</em>&#8230; if you&#8217;re a graduate student in a field like physics or biology, running an existing application or writing your own simulation or data processing code. But who never got to take a class on software engineering, and where the code itself is not a first class deliverable. The deliverable is the published paper.</p>

<h2 class="wp-block-heading">But what about all those other values?</h2>

<p>They&#8217;re still important! But the point of this exercise is to think about which values are will &#8220;win&#8221; when they come into tension. And I do think that, if you look at HPC clusters in general, this is the set of values that will win.</p>

<p>Availability is important, but not if that work costs us (much) performance. Velocity is great, but we&#8217;ll de-prioritize it in the name of workload portability. Security is essential &#8212; but we don&#8217;t want to make it harder to onboard new grad students&#8230;</p>

<h2 class="wp-block-heading">You cluster is not the generic platform (and neither is mine)</h2>

<p>A last point I want to make is that there&#8217;s actually <em>no such thing</em> as the &#8220;generic HPC cluster platform&#8221;. Each individual cluster, at a university or company or government lab, is often configured in a unique way based on the hardware, performance goals, and whims of the person setting it up.</p>

<p>Because of this, each <em>individual</em> HPC cluster may prioritize different values. A cluster at a national lab may choose security at the expense of approachability; or a different cluster may choose to sacrifice portability in the name of velocity if they&#8217;re developing on a new hardware or software system.</p>

<p>(Also, the systems I build as part of my day job also make <em>very</em> different choices than the &#8220;generic&#8221; cluster would. To a first approximation, I think I&#8217;d say we choose performance/debuggability/portability/security&#8230; but we also make different choices depending on what we&#8217;re building!)</p>

<p>But I still think that <em>performance</em>, <em>portability</em>, and <em>approachability</em> represent the most common platform values I&#8217;ve seen in the HPC field as a whole. And I think the tools and practices we use bias towards those values.</p>

<p>However&#8230; all of that is what I thought about while making dinner! If you think a different set of values makes more sense, feel free to <a href="mailto:ajdecon@ajdecon.org">send me an email</a> and let me know. <img alt="😉" class="wp-smiley" src="https://s.w.org/images/core/emoji/15.0.3/72x72/1f609.png" style="height: 1em;" /></p>]]></content><author><name>Thinking Out Loud</name></author><category term="ajdecon" /><summary type="html"><![CDATA[Yesterday while I was cooking dinner, I happened to re-watch Bryan Cantrill&#8217;s talk on &#8220;Platform as a Reflection of Values&#8220;. (I watch a lot tech talks while cooking or baking &#8212; I often have trouble focusing on a video unless I&#8217;m doing something with my hands, but if I know a recipe well I can often make it on autopilot.)]]></summary></entry><entry><title type="html">Surfing the Singularity - Staying Relevant in a Time of Rapid Change</title><link href="https://hpc.social/personal-blog/2024/surfing-the-singularity-staying-relevant-in-a-time-of-rapid-change/" rel="alternate" type="text/html" title="Surfing the Singularity - Staying Relevant in a Time of Rapid Change" /><published>2024-09-10T16:00:00-06:00</published><updated>2024-09-10T16:00:00-06:00</updated><id>https://hpc.social/personal-blog/2024/surfing-the-singularity-staying-relevant-in-a-time-of-rapid-change</id><content type="html" xml:base="https://hpc.social/personal-blog/2024/surfing-the-singularity-staying-relevant-in-a-time-of-rapid-change/"><![CDATA[<p class="ember-view reader-text-block__paragraph" id="ember3428">The If you've been tracking the technology industry, and the software space in particular, for any amount of time you've witnessed the<span class="white-space-pre"> </span><span>accelerating</span><span class="white-space-pre"> </span>rate of technical change - it was always there, but now its become impossible to miss.<span class="white-space-pre"> </span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3428">The rate of technological change has seemed exponential for a while now, but recent advancements in AI have pushed this curve to new heights.<span class="white-space-pre"> </span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3429">An Accenture report released for Davos 2024 suggests that technical rate of change is seen by C-level leaders as the number one most impactful force on their business - more than financial or geopolitical matters - largely as a result of advances in various forms of AI tooling. [1] Of those surveyed, 88% see the rate of change increasing even further, and half say their organizations are not ready, even though 70% see it as a revenue opportunity.<span class="white-space-pre"> </span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3430"><br /></p>
<h3 class="ember-view reader-text-block__heading-3" id="ember3431"><span style="font-size: x-large;">Dinosaur Developers?<span class="white-space-pre"> </span></span></h3>
<p class="ember-view reader-text-block__paragraph" id="ember3432">Today staying alive in business, especially the business of software engineering, means surfing increasingly turbulent and potentially disruptive waters. Consider the leaked recent remarks of Amazon Web Services CEO Matt Garman, wherein he suggested that a mere 2 years from now most AWS programmers wouldn't be coding. [2] In their Q2 investor call, Amazon cited 4,500<span class="white-space-pre"> </span><span>person-years</span><span class="white-space-pre"> </span>of savings through the use of AI assistants on mostly mundane programming tasks like porting and hardening code with patterns of best practices. [3]<span class="white-space-pre"> </span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3433">While the International Monetary Fund suggests AI will impact 60% of jobs and increase wealth inequality, the jobs impacted are more likely to be skewed to higher income countries. [4] These remarks from influential leaders in the industry suggest that the impact of AI will be felt most acutely among software practitioners. Those of us who use integrated development environments (IDEs) to write code (and documents, like this one) with AI assist already are familiar with the benefits. For those unwilling to adapt, to retool and upscale their skills, the future might be bleak. Growing might mean zooming out from code to a more soup-to-nuts view of the software engineering process, especially specification and validation - the need to clearly state requirements and validate results without an immediate need to focus on implementation details. Notice that in the below diagram, taken from the Federal Aviation Administration which is increasingly interested in software engineering and model validation, traditional coding sits only at the bottom of the process rendered as a "V". [5]</p>
<p class="ember-view reader-text-block__paragraph" id="ember3434"><br /></p>
<div class="reader-image-block reader-image-block--full-width"><figure class="reader-image-block__figure"><div class="ivm-image-view-model   "><div class="ivm-view-attr__img-wrapper
        
        "><img alt="" class="ivm-view-attr__img--centered  reader-image-block__img evi-image lazy-image ember-view" id="ember3435" src="https://media.licdn.com/dms/image/v2/D4E12AQEOZqWZEle0-Q/article-inline_image-shrink_1500_2232/article-inline_image-shrink_1500_2232/0/1725982434333?e=1740009600&amp;v=beta&amp;t=IY_I5FqIuF9dAu5ikJgAtZat6UOQ67wPDShSYD2loeU" /></div>
</div>
<figcaption class="reader-image-block__figure-image-caption display-block full-width text-body-small-open t-sans text-align-center t-black--light">Development in the context of verification and validation, as seen by the FAA.</figcaption></figure></div>
<p class="ember-view reader-text-block__paragraph" id="ember3436">So how to stay relevant in a rapidly changing world, to stay one step ahead of AI and the algorithmic job reaper? A recent LinkedIn survey of technologists suggests the number one thing a person can do is to learn new technologies. [6]</p>
<p class="ember-view reader-text-block__paragraph" id="ember3437">A recent Gartner report [7] of the 30 most impactful technologies lists quantum computing as a weighty albeit distant critical enabler. Why? For starters, the existence of Shor's quantum-based numerical factoring algorithm means its a matter of when, not if, quantum computers will be used to crack existing military-grade encryption. In the hands of an adversary, especially when unknown as with the Enigma machine in WWII, the results could be catastrophic, and this is a good part of what is fueling the current government interest in quantum computing.<span class="white-space-pre"> </span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3438"><span class="white-space-pre"> </span></p>
<h3 class="ember-view reader-text-block__heading-3" id="ember3439"><span style="font-size: x-large;">Off to Quantum Summer School<span class="white-space-pre"> </span></span></h3>
<p class="ember-view reader-text-block__paragraph" id="ember3440">So for me, it was back to school. Summer school. First I hit the stacks, brushed my very stale self up on the fundamentals of the necessary calculus and linear algebra, the quantum mechanics to at least an undergraduate level of understanding, read several texts on the subject of quantum computing including the K&amp;R of quantum "Mike &amp; Ike", consumed mass quantities of videos from companies like IBM and Xanadu, and kicked the tires on their programming tool kits.<span class="white-space-pre"> </span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3441">Next I traveled a short distance from my home office to the<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://www.linkedin.com/company/griffiss-institute/">Griffiss Institute</a><span class="white-space-pre"> </span>in Rome NY at their now annual "Quantum 4 International" conference. This consisted of an impressive array of researchers and government administrators presenting their latest findings and laboratory results, often in a sort of national inventory of funded priority projects. The US Air Force, which maintains a research presence in Rome NY, is particularly interested in quantum computing and networking, for example, scaling up to a larger quantum computer by networking (entangling) a set of smaller ones. The Army and Navy are more focused on other non-computing aspects of quantum technology - sensing, magnetics, material defect identification, and as radio receivers. The Canadian delegation was focused on many of the same research topics, as well as a national emphasis on quantum technology education - to be impactful in quantum computing, one must be able to meld a variety of maths, physics, and programming skills with an unusual level of creativity to design novel and efficient algorithms which take advantage of the power of the quantum qubit - as a former college adjunct, this is no small educational challenge. Finally, researchers from the EU demonstrated new upper bounds on entanglement at a distance for wider area networking, and the use of novel estimation techniques to scale up quantum simulators in this "NISQ" era where real quantum computers are still small, noisy, fragile, and scarce. What was noticeably lacking was the demonstration of any current industrial utility for quantum computing applications, and the head of DARPA saw none emerging until we collectively move beyond the NISQ era.<span class="white-space-pre"> </span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3442"><br /></p>
<h3 class="ember-view reader-text-block__heading-3" id="ember3443"><span style="font-size: x-large;">Pack a Remote Lunch<span class="white-space-pre"> </span></span></h3>
<p class="ember-view reader-text-block__paragraph" id="ember3444">While some industrial domains like chemistry will likely gain utility first, the head of DARPA suggests that utility in my own current application area - computational fluid dynamics (CFD) - will not emerge until we move into the "cryptographically relevant" era. It was with this in mind that I remotely attended the<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://www.linkedin.com/company/vki-vonkarmaninstitute/">von Karman Institute for Fluid Dynamics</a><span class="white-space-pre"> </span>in Belgium for a week-long course called "Introduction to Quantum Computing in Fluid Dynamics" funded by NATO. Entirely civilian in nature, the training was aimed at CFD researchers who might take advantage of one of the quantum facilities being installed at national laboratories in the US and EU, often collocated with their existing high performance computing (HPC) clusters. Not being a physicist, for me much of the class was consumed for general domain literacy, and the "tl;dr" is the re-emergence of particle-based methods like Lattice Boltzmann as a focus of research over finite volume methods and solving the Navier-Stokes equations, as is currently dominant in HPC-based CFD.<span class="white-space-pre"> </span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3445">With mind fully blown by the Von Karman experience, I next took two weeks and attended the IBM Global Quantum Summer School, 2024 edition, consisting of 10 lectures on a variety of topics and 4 labs. The videos are now posted on YouTube [8] and while I personally enjoyed the lecture on Hamiltonian simulation, there was a distinct and unfortunately NISQ-era necessity to focus on error correction and compensating for noise, and on the inner workings of the IBM<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://www.linkedin.com/company/qiskit/">Qiskit</a><span class="white-space-pre"> </span>transpiler. In the latter case, because of the diverse nature of the emerging quantum hardware, because inter-qubit connectivity is often not N-way, and because at this stage things often break, it becomes common to mess with the compiler, and to adopt a toolchain with an eye to portability. Qiskit, a library and tool set for Python, is one of a couple frameworks (another being<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://www.linkedin.com/company/pennylaneai/">PennyLane</a>) which currently meet this need, and the labs went to length to expose the student to the various topological mapping, translation, and optimization stages which are present in the quantum programming toolchain. And we got to play with a Hamiltonian simulation up to 50 qubits on real hardware, as most classical machines would have a hard time managing the simultaneous behavior of 50 spins.</p>
<div class="reader-embed-block__iframe-embed"></div>
<div class="separator" style="clear: both; text-align: center;"></div>
<p class="ember-view reader-text-block__paragraph" id="ember3446"><br /></p>
<h3 class="ember-view reader-text-block__heading-3" id="ember3447"><span style="font-size: x-large;">Next Up: AI Assistants &amp; Hybrid Quantum Computing<span class="white-space-pre"> </span></span></h3>
<p class="ember-view reader-text-block__paragraph" id="ember3448">During the Qiskit labs, naturally I was using LLM assist in my IDE, at minimum for tedious or repetitive tasks. But it was remarkable how often the AI assistant was helpful, even for a seemingly niche programming task such as using a quantum computing framework. I intend to delve into this topic more in a future blog and share my experiences with the various emerging AI tools for code and document assist, as well as in the broader end-to-end software engineering context.<span class="white-space-pre"> </span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3449">In addition, I intend to share future blog installments as my quantum education in search of industrial utility continues through the fall conference season. As a software engineer, I'll be particularly on the lookout for frameworks, including those which leverage AI, which allow the programmer to rise above the level of 1950s-like qubits and gates to higher and portable constructs. I'll also be sharing learnings on the rise of classical-quantum hybrids, especially in HPC contexts, as today's quantum approaches such as variational algorithms which converge on solutions require it. Here is another place where toolchains will play a major role, and where heterogeneous workflows which utilize AI tools will likely be impactful.<span class="white-space-pre"> </span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3450">Until next time, enjoy these last few weeks of summer.</p>
<p class="ember-view reader-text-block__paragraph" id="ember3451">- andy<span class="white-space-pre"> </span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3452"><br /></p>
<p class="ember-view reader-text-block__paragraph" id="ember3453"><span style="font-size: x-large;">References:<span class="white-space-pre"> </span></span></p>
<p class="ember-view reader-text-block__paragraph" id="ember3454">0. Photo by<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://unsplash.com/@profwicks?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" target="_self">Ben Wicks</a><span class="white-space-pre"> </span>on<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://unsplash.com/photos/green-and-blue-light-bokeh-Ia-qPL-HQdA?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" target="_self">Unsplash</a></p>
<p class="ember-view reader-text-block__paragraph" id="ember3455">1.<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://www.accenture.com/us-en/about/company/pulse-of-change" target="_self">https://www.accenture.com/us-en/about/company/pulse-of-change</a></p>
<p class="ember-view reader-text-block__paragraph" id="ember3456">2.<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://www.businessinsider.com/aws-ceo-developers-stop-coding-ai-takes-over-2024-8" target="_self">https://www.businessinsider.com/aws-ceo-developers-stop-coding-ai-takes-over-2024-8</a></p>
<p class="ember-view reader-text-block__paragraph" id="ember3457">3.<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://accelerationeconomy.com/cloud-wars/amazon-genai-slashes-260-million-in-costs-saves-4500-years/" target="_self">https://accelerationeconomy.com/cloud-wars/amazon-genai-slashes-260-million-in-costs-saves-4500-years/</a></p>
<p class="ember-view reader-text-block__paragraph" id="ember3458">4.<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://nypost.com/2024/01/15/business/ai-will-affect-60-of-us-jobs-imf-warns/" target="_self">https://nypost.com/2024/01/15/business/ai-will-affect-60-of-us-jobs-imf-warns/</a></p>
<p class="ember-view reader-text-block__paragraph" id="ember3459">5.<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://www.faa.gov/sites/faa.gov/files/2024-07/d_VVFlow_2024Mar21.jpg" target="_self">https://www.faa.gov/sites/faa.gov/files/2024-07/d_VVFlow_2024Mar21.jpg</a></p>
<p class="ember-view reader-text-block__paragraph" id="ember3460">6.<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://www.linkedin.com/advice/0/how-can-you-stay-relevant-software-development-skills-it-services-kxeme" target="_self">https://www.linkedin.com/advice/0/how-can-you-stay-relevant-software-development-skills-it-services-kxeme</a></p>
<p class="ember-view reader-text-block__paragraph" id="ember3461">7.<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://www.gartner.com/en/articles/30-emerging-technologies-that-will-guide-your-business-decisions" target="_self">https://www.gartner.com/en/articles/30-emerging-technologies-that-will-guide-your-business-decisions</a></p>
<p class="ember-view reader-text-block__paragraph" id="ember3462">8.<span class="white-space-pre"> </span><a class="bpCpipVrrjRHIWtfjEjtbNsDescTJyo " href="https://www.youtube.com/playlist?list=PLOFEBzvs-Vvr-GzDWlZpAcDpki5jUqYJu" target="_self">https://www.youtube.com/playlist?list=PLOFEBzvs-Vvr-GzDWlZpAcDpki5jUqYJu</a></p>]]></content><author><name>Surfing the Singularity</name></author><category term="surfthesing" /><summary type="html"><![CDATA[The If you've been tracking the technology industry, and the software space in particular, for any amount of time you've witnessed the accelerating rate of technical change - it was always there, but now its become impossible to miss. The rate of technological change has seemed exponential for a while now, but recent advancements in AI have pushed this curve to new heights. An Accenture report released for Davos 2024 suggests that technical rate of change is seen by C-level leaders as the number one most impactful force on their business - more than financial or geopolitical matters - largely as a result of advances in various forms of AI tooling. [1] Of those surveyed, 88% see the rate of change increasing even further, and half say their organizations are not ready, even though 70% see it as a revenue opportunity. Dinosaur Developers? Today staying alive in business, especially the business of software engineering, means surfing increasingly turbulent and potentially disruptive waters. Consider the leaked recent remarks of Amazon Web Services CEO Matt Garman, wherein he suggested that a mere 2 years from now most AWS programmers wouldn't be coding. [2] In their Q2 investor call, Amazon cited 4,500 person-years of savings through the use of AI assistants on mostly mundane programming tasks like porting and hardening code with patterns of best practices. [3] While the International Monetary Fund suggests AI will impact 60% of jobs and increase wealth inequality, the jobs impacted are more likely to be skewed to higher income countries. [4] These remarks from influential leaders in the industry suggest that the impact of AI will be felt most acutely among software practitioners. Those of us who use integrated development environments (IDEs) to write code (and documents, like this one) with AI assist already are familiar with the benefits. For those unwilling to adapt, to retool and upscale their skills, the future might be bleak. Growing might mean zooming out from code to a more soup-to-nuts view of the software engineering process, especially specification and validation - the need to clearly state requirements and validate results without an immediate need to focus on implementation details. Notice that in the below diagram, taken from the Federal Aviation Administration which is increasingly interested in software engineering and model validation, traditional coding sits only at the bottom of the process rendered as a "V". [5] Development in the context of verification and validation, as seen by the FAA. So how to stay relevant in a rapidly changing world, to stay one step ahead of AI and the algorithmic job reaper? A recent LinkedIn survey of technologists suggests the number one thing a person can do is to learn new technologies. [6] A recent Gartner report [7] of the 30 most impactful technologies lists quantum computing as a weighty albeit distant critical enabler. Why? For starters, the existence of Shor's quantum-based numerical factoring algorithm means its a matter of when, not if, quantum computers will be used to crack existing military-grade encryption. In the hands of an adversary, especially when unknown as with the Enigma machine in WWII, the results could be catastrophic, and this is a good part of what is fueling the current government interest in quantum computing. Off to Quantum Summer School So for me, it was back to school. Summer school. First I hit the stacks, brushed my very stale self up on the fundamentals of the necessary calculus and linear algebra, the quantum mechanics to at least an undergraduate level of understanding, read several texts on the subject of quantum computing including the K&amp;R of quantum "Mike &amp; Ike", consumed mass quantities of videos from companies like IBM and Xanadu, and kicked the tires on their programming tool kits. Next I traveled a short distance from my home office to the Griffiss Institute in Rome NY at their now annual "Quantum 4 International" conference. This consisted of an impressive array of researchers and government administrators presenting their latest findings and laboratory results, often in a sort of national inventory of funded priority projects. The US Air Force, which maintains a research presence in Rome NY, is particularly interested in quantum computing and networking, for example, scaling up to a larger quantum computer by networking (entangling) a set of smaller ones. The Army and Navy are more focused on other non-computing aspects of quantum technology - sensing, magnetics, material defect identification, and as radio receivers. The Canadian delegation was focused on many of the same research topics, as well as a national emphasis on quantum technology education - to be impactful in quantum computing, one must be able to meld a variety of maths, physics, and programming skills with an unusual level of creativity to design novel and efficient algorithms which take advantage of the power of the quantum qubit - as a former college adjunct, this is no small educational challenge. Finally, researchers from the EU demonstrated new upper bounds on entanglement at a distance for wider area networking, and the use of novel estimation techniques to scale up quantum simulators in this "NISQ" era where real quantum computers are still small, noisy, fragile, and scarce. What was noticeably lacking was the demonstration of any current industrial utility for quantum computing applications, and the head of DARPA saw none emerging until we collectively move beyond the NISQ era. Pack a Remote Lunch While some industrial domains like chemistry will likely gain utility first, the head of DARPA suggests that utility in my own current application area - computational fluid dynamics (CFD) - will not emerge until we move into the "cryptographically relevant" era. It was with this in mind that I remotely attended the von Karman Institute for Fluid Dynamics in Belgium for a week-long course called "Introduction to Quantum Computing in Fluid Dynamics" funded by NATO. Entirely civilian in nature, the training was aimed at CFD researchers who might take advantage of one of the quantum facilities being installed at national laboratories in the US and EU, often collocated with their existing high performance computing (HPC) clusters. Not being a physicist, for me much of the class was consumed for general domain literacy, and the "tl;dr" is the re-emergence of particle-based methods like Lattice Boltzmann as a focus of research over finite volume methods and solving the Navier-Stokes equations, as is currently dominant in HPC-based CFD. With mind fully blown by the Von Karman experience, I next took two weeks and attended the IBM Global Quantum Summer School, 2024 edition, consisting of 10 lectures on a variety of topics and 4 labs. The videos are now posted on YouTube [8] and while I personally enjoyed the lecture on Hamiltonian simulation, there was a distinct and unfortunately NISQ-era necessity to focus on error correction and compensating for noise, and on the inner workings of the IBM Qiskit transpiler. In the latter case, because of the diverse nature of the emerging quantum hardware, because inter-qubit connectivity is often not N-way, and because at this stage things often break, it becomes common to mess with the compiler, and to adopt a toolchain with an eye to portability. Qiskit, a library and tool set for Python, is one of a couple frameworks (another being PennyLane) which currently meet this need, and the labs went to length to expose the student to the various topological mapping, translation, and optimization stages which are present in the quantum programming toolchain. And we got to play with a Hamiltonian simulation up to 50 qubits on real hardware, as most classical machines would have a hard time managing the simultaneous behavior of 50 spins. Next Up: AI Assistants &amp; Hybrid Quantum Computing During the Qiskit labs, naturally I was using LLM assist in my IDE, at minimum for tedious or repetitive tasks. But it was remarkable how often the AI assistant was helpful, even for a seemingly niche programming task such as using a quantum computing framework. I intend to delve into this topic more in a future blog and share my experiences with the various emerging AI tools for code and document assist, as well as in the broader end-to-end software engineering context. In addition, I intend to share future blog installments as my quantum education in search of industrial utility continues through the fall conference season. As a software engineer, I'll be particularly on the lookout for frameworks, including those which leverage AI, which allow the programmer to rise above the level of 1950s-like qubits and gates to higher and portable constructs. I'll also be sharing learnings on the rise of classical-quantum hybrids, especially in HPC contexts, as today's quantum approaches such as variational algorithms which converge on solutions require it. Here is another place where toolchains will play a major role, and where heterogeneous workflows which utilize AI tools will likely be impactful. Until next time, enjoy these last few weeks of summer. - andy References: 0. Photo by Ben Wicks on Unsplash 1. https://www.accenture.com/us-en/about/company/pulse-of-change 2. https://www.businessinsider.com/aws-ceo-developers-stop-coding-ai-takes-over-2024-8 3. https://accelerationeconomy.com/cloud-wars/amazon-genai-slashes-260-million-in-costs-saves-4500-years/ 4. https://nypost.com/2024/01/15/business/ai-will-affect-60-of-us-jobs-imf-warns/ 5. https://www.faa.gov/sites/faa.gov/files/2024-07/d_VVFlow_2024Mar21.jpg 6. https://www.linkedin.com/advice/0/how-can-you-stay-relevant-software-development-skills-it-services-kxeme 7. https://www.gartner.com/en/articles/30-emerging-technologies-that-will-guide-your-business-decisions 8. https://www.youtube.com/playlist?list=PLOFEBzvs-Vvr-GzDWlZpAcDpki5jUqYJu]]></summary></entry><entry><title type="html">How has life after leaving the Labs been going?</title><link href="https://hpc.social/personal-blog/2024/how-has-life-after-leaving-the-labs-been-going/" rel="alternate" type="text/html" title="How has life after leaving the Labs been going?" /><published>2024-08-04T20:21:00-06:00</published><updated>2024-08-04T20:21:00-06:00</updated><id>https://hpc.social/personal-blog/2024/how-has-life-after-leaving-the-labs-been-going-</id><content type="html" xml:base="https://hpc.social/personal-blog/2024/how-has-life-after-leaving-the-labs-been-going/"><![CDATA[<p>June 2024 marked two years since I <a href="http://blog.glennklockwood.com/2022/05/life-and-leaving-nersc.html">left my job at one of the world's most prestigious government HPC centers for a job in one of the world's largest technology corporations</a>. In that time, the world of HPC has changed
dramatically; just six months after I started, ChatGPT was released and triggered a gold rush in AI that is now overshadowing traditional scientific computing. This shift brought about massive HPC
deployments led by hyperscalers, challenging the long-held belief that only national governments could
deploy and operate world-leading supercomputers. <a href="http://blog.glennklockwood.com/2024/05/isc24-recap.html">My experiences at
ISC'24 this past summer</a> made clear to me that the traditional HPC community is now rethinking their role the industry, and some individuals who built their careers in public HPC are revisiting their
assumption that world-class HPC systems are limited to the public institutions that have
historically dominated the <a href="https://www.top500.org/lists/top500/list/2024/06/">top of the Top500 list</a>. I
had no idea things would unfold this way when I left my job at NERSC back in 2022, and I've been remarkably lucky to
now be a part of the largest forces driving this huge shift in HPC.</p>
<div class="separator" style="clear: both; text-align: center;">
<figure style="display: inline-block; margin-left: 1em; margin-right: 1em;">

<figcaption style="font-size: 14px; margin-top: 5px;">One of my new offices. Nicer than my old government office, and it has free food, but it's a ninety-minute drive each way.</figcaption>
</figure>
</div>
<p>In the spirit of openness and helping others who are facing similar career decisions, I thought I would follow up on my <a href="http://blog.glennklockwood.com/2022/05/life-and-leaving-nersc.html">Life and leaving
NERSC</a> post by sharing how my professional journey from DOE HPC into cloud HPC has been going. I'll first explain the path I've traveled over these past two years, then answer some of the most
common questions I've been asked about this transition.</p>
<p>As a forewarning, this is not a typical technology-focused post, and most of this might be obvious to people who already work in Big Tech. Here are the questions on which I reflected:</p>
<ol>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#first-two-years">What happened during my first two years in Corporate America?</a></li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#what-i-do">So what do I actually do?</a>
<ol>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#storage-product-management">Storage product management</a></li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#hpc-ai-development">HPC/AI development</a></li>
</ol>
</li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#am-i-happy">Am I happy with my decision and the new job?</a>
<ol>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#am-i-happy-broadly-yes">Broadly, yes</a></li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#long-time-no">But for a long time, no</a></li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#finally-yes">Finally, yes</a></li>
</ol>
</li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#industry-does-better">What does industry do better than the Labs?</a>
<ol>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#accountability">Accountability</a></li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#pace-and-decision-making">Pace and decision making</a></li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#relevance">Relevance</a></li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#technically-security">Technically: security</a></li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#pay-good">But the pay is good, right?</a></li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#work-life-balance">How's work-life balance?</a></li>
</ol>
</li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#what-i-miss">Do you miss anything about working at the lab?</a>
<ol>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#freedom-to-have-an-off-day">Freedom to have an off day</a></li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#travel">Travel</a></li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#openness">Openness</a></li>
</ol>
</li>
<li><a href="http://blog.glennklockwood.com/feeds/posts/default/-/hpc?alt=rss#regret-decision">Would you still have left NERSC knowing what you know now?</a></li>
</ol>
<h2 id="first-two-years">What happened during my first two years in Corporate America?</h2>
<p>I published my <a href="http://blog.glennklockwood.com/2022/05/life-and-leaving-nersc.html">Life and leaving NERSC</a> blog post on a
Thursday, which was my last day working at NERSC. The following Monday was my first day at the new job, and being
hired as 100% remote, it didn't feel that different; I was just booting up a Lenovo laptop (yuck) instead of a
MacBook, using Teams and Outlook instead of Slack, GSuite, and Zoom, and that sort of thing.</p>
<p>However, the job was undeniably different; whereas I used to be an engineer at NERSC, I was hired to be a "Principal Product
Manager" within the cloud storage organization which was responsible for all object, disk, and file storage
services. Although my title was "product manager," I wasn't a people manager, and
I didn't manage any specific storage products. Rather, my responsibility was to act as an HPC-focused overlay
across all cloud storage services, and my job was to represent the interests of HPC users to all the people who did manage specific storage products. I didn't define product or feature roadmaps myself, but I could help those responsible for each product or service understand how to shape their roadmaps to benefit HPC workloads.
</p>
<p>I struggled in this position for a variety of reasons, so after I gave the new role
an honest six to nine months, I decided that being a storage product manager just wasn't a good fit for me.
Unfortunately, I reached this decision after the <a href="https://www.nytimes.com/2022/07/21/business/yield-curve-inversion.html">yield curve inverted</a> and
<a href="https://www.theverge.com/2023/1/18/23560315/microsoft-job-cuts-layoffs-2023-tech">mass-layoffs and hiring
freezes</a> were implemented, so there weren't a lot of places to go other than back to a government lab.
Although I wasn't thriving as a storage product manager, I did have allies that helped me navigate my
day-to-day struggles, and I decided to wait until more opportunities opened up and learn as much about
product management as I could in the meantime.</p>
<div class="separator" style="clear: both; text-align: center;">
<figure style="display: inline-block; margin-left: 1em; margin-right: 1em;">

<figcaption style="font-size: 14px; margin-top: 5px;">The yield curve inverted a month after I started my new job. Not great timing.</figcaption>
</figure>
</div>
<p>After a little over a year as a storage product manager, a new engineering role opened up within a sister team in our
HPC/AI infrastructure organization. After discussing the needs and nature of the work with the hiring manager, I applied for the job, went through the
interview process, and was eventually given a verbal offer to join his team in June 2023. Unfortunately, the global
economic outlook was still uncertain, and I wound up sitting in a holding pattern (as a storage product
manager) from June 2023 to November 2023. It wasn't until the week of SC'23 that I finally got the written offer
letter, and I spent December wrapping up loose ends within the storage organization.</p>
<p>On January 2, 2024, I began my new (and current) role within the company. The move was completely lateral, but I changed job titles from "Product Manager" to "Software Engineer," and I
changed organizations from storage to specialized compute.</p>
<p>I say all this because my experiences in making the professional transition from government HPC to cloud HPC are colored by the fact that I really changed jobs twice. I've had both product management and engineering/development roles, and I've been in both storage and HPC
organizations.</p>
<h2 id="what-i-do">So what do I actually do?</h2>
<p>I've had two very different roles within the same orbit of HPC/AI infrastructure, so I'll
describe them separately to give you a sense of the breadth of HPC roles possible.</p>
<h3 id="storage-product-management">Storage product management</h3>
<p>As a <b>storage product manager</b> (PM), I was an inch deep but a mile wide on every storage service, every commercial HPC workload, and
all the ways in which those two could touch each other. I'd guess that only 25% of my day-to-day work required deep
expertise in HPC; the remainder was either business-centric or required only understanding HPC in broad strokes. This
was quite unlike the things I'd done earlier in my career in the public sector, since there's not an equivalent to
what a product manager does within the DOE Labs.
</p>
<p>For example, I spent a lot of my time as a storage PM explaining the basics of HPC I/O to different teams within the
company. When most cloud people think "storage," they are really thinking about either enterprise storage (things
like virtual disks for virtual machines) or content distribution (think serving up content for web apps). The
concept of hundreds or thousands of VMs all writing to the same place at the same time is standard practice in the
HPC world, but in the cloud world, this is a <a href="https://www.microsoft.com/en-us/security/business/security-101/what-is-a-ddos-attack?msockid=2008901357a56c4518b3840856e96dad">DDoS
attack</a>. Since my organization was responsible for all storage, not just HPC storage, there were a lot of
people who simply never had to think about the challenges that HPC people take for granted, and it could be challenging (as the new guy) to convince seasoned cloud storage PMs that some workloads legitimately need hundreds of gigabytes per second of bandwidth.</p>
<p>As a PM, I also wound up doing a fair amount of business reporting. For example, object storage is used by all manner of cloud
customers, so prioritizing features that specifically help HPC customers required understanding how many HPC
customers actually used it. How do you define whether a workload is really an HPC workloads or not? In
DOE, we'd waste hours debating stuff like this for no real purpose, but when I became a product manager, I had to
define this to make the business case that we needed to develop a certain feature that would only be used by HPC workloads.</p>
<p>Finally, I did a fair amount of actual product and project management work. Get on the phone with a customer,
write down what they do, and turn those into requirements. Do that a bunch of times, then synthesize a more general
requirements document. Review it with leadership. Get approval to assign developers to work on the features to meet
those requirements. Ask other teams to develop features you need for your feature. Negotiate with everyone on
development priorities in the next six months. Track progress of the development team. Produce demos to show that
progress is being made. Present progress to leadership. That sort of thing. It's similar to being a PI on a research
grant, except I had customers, dependencies, and ultimate accountability.</p>
<p>As far as technical work, a lot of it revolved around meeting customers and internal partner teams where they were in
terms of their knowledge of HPC. I did a fair amount of technical marketing; I would come up with
the ways people should think about combining storage services together in their HPC workflows, then figure out
how to communicate that to audiences with vastly different levels of technical understanding. For example, I didn't
own our <a href="https://learn.microsoft.com/en-us/azure/azure-managed-lustre/amlfs-overview">Lustre product</a>, <a href="https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction">object storage
product</a>, or <a href="https://learn.microsoft.com/en-us/azure/virtual-machines/sizes/high-performance-compute/hb-family">HPC CPU
node product</a>, but I owned <a href="https://techcommunity.microsoft.com/t5/azure-high-performance-computing/azure-managed-lustre-not-your-grandparents-parallel-file-system/ba-p/3889946">the
story around how we envisioned all three services worked well together</a>. This meant I would create slides and
narratives around this, then present them to anyone from our sales teams (who often had limited HPC-specific experience) to
the world's leading HPC centers.</p>
<p>I also sometimes helped development teams accurately test their storage systems against HPC workloads. For example,
when ChatGPT exploded, everyone wanted to know how well their storage service worked for training large language
models. I would talk to the engineers who trained LLMs, infer what their I/O patterns would be based on
their description of how they did training, then design a benchmark that our developers could follow to
emulate that LLM training workflow. Since I understood both the workload and the storage technology, it was often faster for me to translate between AI engineers and storage engineers rather than have them speak directly.</p>
<h3 id="hpc-ai-development">HPC/AI development</h3>
<p>As an <b>HPC/AI engineer</b>, my work is a lot more technical and focused. I'm on a "white-glove support team"
that works directly with large, strategic customers in HPC and AI, so rather than working with dozens of customers and connect them to dozens of storage technologies, I work with one or two customers and the specific technologies on which they build their HPC or AI clusters. Because of this, I'd wager 95% of my day-to-day work is
technical.</p>
<p>I don't spend much time in a terminal by virtue of my relative seniority. Instead, I sit in on a lot of internal meetings and represent the perspective of our strategic HPC and AI customers. For example, if we are trying to decide which CPU to
include in our next HPC-optimized CPU node, I might work with our benchmarking engineers to develop a representative
benchmark and then interpret the results with the node's product managers. I'm not the person running the benchmark
myself; instead, I might ask hard questions that the customer might ask, help decide the next experiments to run,
and backstop our engineers if the customer starts poking too many holes in the work.</p>
<p>I also function as a system architect at times; if a customer shows up with unusually large or complex
HPC system requirements, I'll help translate the customer requirement (e.g., "We need 10 TB/s of storage bandwidth)
for individual product teams (e.g., "they will be using N compute nodes and accessing storage via a network with
this topology and tapering, likely running an application that has this pattern, ..."). This often requires understanding what the compute, network, <i>and</i> storage product teams are doing and being able to explain it all in
whatever terms each team understands. I also wind up sitting in on customer meetings and asking critical questions
so that we can make informed design tradeoffs.</p>
<p>I do write code, but no more than I did when I was a system architect at NERSC. For example, I might pull PDU
telemetry from across a data center to help determine if oversubscribing the power for a future cluster would
impact workloads. The code itself is pretty straightforward statistical analysis, but interpreting it requires an
understanding of a bunch of things ranging from the workload running on the nodes to how nodes are distributed
across PDUs, racks, rows, halls, and buildings.</p>
<p>The remaining 5% of my work is not very technical and involves things I opt into because it's interesting or
the right thing to do. This might be spending time providing historical context for a business strategy document or showing up at a meeting to help explain the customer perspective to a finance or sales team.</p>
<h2 id="am-i-happy">Am I happy with my decision and the new job?</h2>
<p>Yes, no, and yes.</p>
<h3 id="am-i-happy-broadly-yes">Broadly, yes</h3>
<p>I am glad I made the decision to leave NERSC and take on a job in Big Tech for a couple of
high-level reasons.</p>
<p>As a product manager, I learned a lot about how businesses and corporations work to a
degree that I never did when I worked at a startup and I never would have if I stayed with the government. Not only
do I now know what the difference between gross and operating margin is, but I <i>get</i> it because I've had to
build <a href="https://www.investopedia.com/terms/c/cogs.asp">COGS</a> and pricing models that could sustain and grow a new product. I know exactly how to
price cloud services (or any product or service, really) and where that money goes. I now pay much more attention to
quarterly earnings reports, and I have a more confident opinion on what different elements of these reports say
about a technology company's trajectory. This has equipped me with what feels like a much more complete
understanding of the HPC industry overall.</p>
<p>I'm also glad to work at a company that generally tries to do the right things. It
is investing heavily towards being <a href="https://blogs.microsoft.com/blog/2020/01/16/microsoft-will-be-carbon-negative-by-2030/">carbon
negative</a> (rather than just buying carbon offsets) while others are <a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/elon-musks-new-worlds-fastest-ai-data-center-is-powered-by-massive-portable-power-generators-to-sidestep-electricity-supply-constraints">burning gas inefficiently</a> in a race to be #1. It also <a href="https://givebutter.com/blog/companies-that-match-donations">matches
every donation I make to 501(c)3 nonprofits</a> which is a huge benefit that matches up with the ways in
which I try to share my good fortune with others. And it beats employees over the heads with a strong, positive <a href="https://careers.microsoft.com/v2/global/en/culture">corporate culture</a> which holds managers and leaders
accountable for the wellness of their employees. These sorts of things don't meaningfully exist in government, and
there are a lot of big corporations out prioritize short-term profits over the longer-term benefits that come from investing in sustainability and philanthropy.</p>
<h3 id="long-time-no">But for a long time, no</h3>
<p>However, I was unhappy for my first eighteen months.</p>
<p>I took a gamble on storage product management being as interesting and fulfilling as
engineering when I decided to step into this new job, and I lost that bet. I quickly came to realize that there's a big difference between
being a <u>storage person in an HPC organization</u> and being an <u>HPC person in a storage organization</u>.</p>
<p>When I worked in an HPC organization like NERSC, I was used to being the odd man out
because parallel storage is a complicated topic that most HPC folks don't <i>really</i> understand. Despite that, everyone is still generally like-minded and appreciates the same things; everyone knows what MPI and InfiniBand
are, and everybody knows what a checkpoint and restart might look like.</p>
<p>Conversely, when I worked in a storage organization, I was an odd man out because nobody
really understood HPC. The average engineer only had a vague notion of what MPI or
InfiniBand accomplished. If you don't understand that MPI is what lets hundreds of servers all work on the same
distributed problem at once, it's easy to forget that an MPI application will also cause hundreds of servers to
all write data at once. And if you've never used an MPI barrier, it's hard to internalize the fact that the whole
application stops until the slowest process finishes writing.</p>
<p>Instead of worrying about tightly coupled applications, I realized that storage people
worry about data availability and durability above all else. After all, storage's #1 job is to not lose data. In
contrast, it's not unusual for an HPC user to have hundreds of terabytes of data vanish because they forgot to copy
it off of scratch before it got purged. This sharp difference in priorities--data durability versus
performance--causes friction, because at the end of the day, what's good for HPC (high bandwidth and low latency) is
usually bad for storage (high durability and availability).</p>
<div class="separator" style="clear: both; text-align: center;">
<figure style="display: inline-block; margin-left: 1em; margin-right: 1em;">

<figcaption style="font-size: 14px; margin-top: 5px;">The landscape of storage for HPC and storage for enterprises as I see it. If you care about one but work with people who care about the other, expect friction.</figcaption>
</figure>
</div>
<p>These are technological differences, but they result in a persistent, elevated level of latent stress that never goes away. People tend to worry about the things they understand, and people tend to ask for help
about the things that worry them. What this meant for me is that I spent a lot of time focusing on things that
everyone understood (like market trends, revenue, and general indicators of performance) instead of hard problems
unique to large-scale HPC. And because I was never solving the hard problems, I never got the gratification of feeling like I accomplished something that, as I learned, is an important motivator to me.</p>
<p>To be clear, I realize that I made the decision to focus on problems that other people brought me
rather than carve out a space for me to work on the problems I felt were important. I'm sure that someone who was
more tenacious and unafraid to pursue challenges that nobody else understood would have a very different experience as a PM. But after about a year, I realized that what I value and enjoy doing just isn't
aligned with what a successful storage PM needs to be successful. I realized I didn't want to keep doing what I was doing
for another five years, so I decided to stop.</p>
<h3 id="finally-yes">Finally, yes</h3>
<p>I quite enjoy my role in HPC/AI engineering and development now, as it's similar to
what I used to do in the DOE. I have to learn about how different hardware, software,
and systems work, and I have a lot of room to focus on challenges that play to my strengths and interests.
For example, I love engaging with the HPC community, and my job still allows me to go out to the big HPC conferences to do that. At the same time, I also like getting into the guts of system behavior, and I still get to spend at least an hour or two a week doing something quantitative.</p>
<p>My day-to-day is also steeped in that familiar feel of working in an HPC organization.
Every cluster has a name that gets bandied about in meetings, and they have the same familiar challenges--fabric disruptions, firmware
upgrades, flaky nodes, and the like. The standard responsibilities are also all there; some teams perform system
administration, others support users, and some of us focus on future system designs. But the cluster names aren't nearly as creative as those in the public sector (<a href="https://www.top500.org/system/180236/">Eagle's</a> real name sounds like a serial number). And they look pretty boring too; there are no fancy rack graphics.</p>
<div class="separator" style="clear: both; text-align: center;">
<figure style="display: inline-block; margin-left: 1em; margin-right: 1em;">

<figcaption style="font-size: 14px; margin-top: 5px;">Five racks of a cloud GPU cluster that runs ND H100 v5-series VMs. <a href="https://www.youtube.com/watch?v=ntKZ5CibuIQ">Source</a></figcaption></figure></div>
<p>There are also teams that have no analogue in the traditional HPC world, like those who
are responsible for things ranging from the <a href="https://techcommunity.microsoft.com/t5/azure-infrastructure-blog/announcing-the-general-availability-of-azure-boost/ba-p/3981384">smart NICs</a> and software-defined networks to profits and losses. This is
what keeps things interesting; I can just as easily spend an hour reviewing benchmark results from the latest GPU
with my teammates as I can learning how the control systems for <a href="https://news.microsoft.com/source/features/ai/in-house-chips-silicon-to-service-to-meet-ai-demand/">liquid
heat exchangers</a> affect system reliability or <a href="https://www.osha.gov/noise/standards">data center
safety</a>. When things are quiet and no fires are burning, going to work can sometimes feel like going to a big
playground full of HPC and HPC-adjacent technology.</p>
<p>Don't get me wrong; it's still a job, and there are still unpleasant tasks and
uncomfortable situations. Working at a cloud provider means a lot of processes are designed to be slow and steady,
and some teams struggle to understand why anyone would want to reboot every node in a cluster at once--such an event would be a massive outage in general-purpose cloud! But working in an HPC organization means that when these
situations arise, I'm no longer the <i>odd HPC guy</i>--I'm on the <i>odd HPC team</i>.</p>
<h2 id="industry-does-better">What does industry do better than the Labs?</h2>
<h3 id="accountability">Accountability</h3>
<p>Organizational planning happens <a href="https://devblogs.microsoft.com/azure-sdk/planning-2021/">twice a year</a>,
and this planning is the time when teams all get on the same page about what
work to prioritize in the next six months (a <i>semester</i>). Teams coordinate dependent work with each other,
trades horses on what the priority of each request is, and at the end of planning, have committed agreements about what work
will be done in the next semester. The progress on that work is tracked throughout the semester, delays and
interrupts are accounted, and there's an escalation path up through the ranks of management and leadership if
priorities cannot be agreed upon by individual teams.</p>
<p>The DOE Labs operate much more loosely in my experience. There, people tend
to work on whatever pet projects they want until they lose interest. If a project is funded by a research grant, there are loose
deliverables and timelines (write X papers per year), but at the end of the day, nothing really bad happens if the
work progresses slowly or its quality is poor. There's no penalty if a research grant results in a piece of software
that nobody uses or a paper that nobody reads. The value of the work is largely intellectual, and as a result, it's perfectly
possible to have a long career at a DOE lab, churning out papers and software, that lacks any
lasting impact.</p>
<p>Tying money to the value of work can make accountability much more black and white. If you pay a team of engineers a
million dollars a year to develop a new service that only increases revenue by a million dollars a year, that
service is going to be scrutinized every time prioritization happens. Is there a way to increase its revenue through
better features or better positioning? It'll be a product manager's job to go figure that out. If the answer comes
back as "no," then that service might be put on a shelf and its engineering team reassigned to work on something
that has a greater impact. Those engineers don't get to decide that they keep wanting to work on the service that
has limited demonstrable value.</p>
<p>At the same time, managers are accountable for the wellbeing of their team and the teams underneath them. All
employees fill out regular, semi-anonymized surveys on different aspects of job satisfaction, and the results of
these surveys roll up all the way to the top of the company. If employees are disgruntled, their managers know it,
and those managers' managers know it, and everyone up the chain is accountable for improving those scores. Sometimes that results
in increased hiring so engineers don't feel overworked. Other times it means reorganizing people and teams to align
them with the work they are good at performing. And if nothing works and a team's morale keeps declining, maybe it's because of
the manager--and the manager gets replaced.</p>
<h3 id="pace-and-decision-making">Pace and decision making</h3>
<p>Because managers and leaders are accountable, I've also found them to be much more empowered to just do what they
feel is the right thing to do. Whereas no big decision in the DOE Labs can be made without reviews, panels,
strategic offsites, more reviews, and presentations to headquarters--all of which could add months or
years to a project--the direction can move on a dime because all it takes is one executive to sign off and accept
full responsibility for the consequences of their decision. Getting the approval to staff up and pursue a good idea
often requires only winning over one or two key people, not an army of feds in Germantown or an anonymous review
panel who isn't conversant in what you're proposing.</p>
<p>And again, sometimes money makes decisions much easier to make. For example, a few people at ISC'24 asked me why we
didn't re-do the <a href="https://www.top500.org/system/180236/">Top500 run for Eagle</a> to beat Aurora since the
SC'23 scoring was so close. The decision process can be as simple as this:</p>
<p></p>
<ul>
<li>According to the <a href="https://top500.org/lists/top500/2024/06/download/TOP500_202406.xlsx">Top500 list's raw
data</a>, Eagle achieved 561,200 TFlop/s using an Nmax of 11,796,480.</li>
<li>Knowing that HPL's walltime is (flop count / Rmax) and HPL's flop count is (2/3 * Nmax^3), you can calculate
that the HPL walltime for this run was 1,950 seconds or 0.512 hours.</li>
<li>The <a href="https://azure.microsoft.com/en-us/pricing/calculator/">public list price</a> for an Eagle
node (ND96isr H100 v5) is something like $60 an hour.</li>
<li>The HPL run used 1,800 such nodes.</li>
</ul>
<p>Give the above, during the half hour it would take to run HPL, those same nodes could be
running a production workload which would have resulted in $58,000 in revenue. That is, the <i>opportunity cost</i>
of re-running HPL is at least $58,000 in lost revenue. In reality, it would take time to boot up and configure the
cluster of virtual machines and do a few scale-up runs which would tie up the nodes for a couple hours, making
this opportunity cost closer to a couple hundred thousand dollars.</p>
<p>Is getting a marginally higher Top500 score worth a couple hundred thousand dollars if your
machine is already listed and had its day in the sun? I don't need an executive to answer that question. But in the
public HPC space, who's to say what the opportunity cost is? If HPL wasn't running twice a year on Frontier, are the
dozen or so lattice QCD jobs that would be running instead worth a couple hundred thousand dollars?</p>
<p></p>
<h3 id="relevance">Relevance</h3>
<p>I might be more vain than I thought when I worked for the government, because I really enjoy being able to talk about the work that I do
with the general public now. When people ask, "What work do you do?" and I respond with, "Have you ever heard of Copilot or
ChatGPT?" there is almost always a conversation that follows. People may not really understand how artificial intelligence and
large language models work, but they've played with those technologies and have opinions and questions. Sometimes the conversation is about big-picture stuff like "will AI take over the world?" At other times it's specific like "what do you think about
AI's effect on global climate change?" Because I am steeped in all aspects of AI in my day-to-day work, I can
usually speak intelligently about any dimension of the AI industry when my neighbors ask.</p>
<div class="separator" style="clear: both; text-align: center;">
<figure style="display: inline-block; margin-left: 1em; margin-right: 1em;">

<figcaption style="font-size: 14px; margin-top: 5px;">Every blog post these days needs at least one AI-generated picture, so here is a picture generated by DALL-E that "captures the essence of explaining AI concepts to neighbors in a friendly, approachable setting." But more poignantly, my team directly supports the supercomputers that trained the model that generates these pictures.</figcaption>
</figure>
</div>
<p>This was a much bigger challenge when I worked in the public sector. When I told people that I worked at Lawrence Berkeley
National Lab, nobody knew what I was talking about half of the time. The other half of the time, people would think I worked on
nuclear weapons because Lawrence Livermore National Lab has a confusingly similar name and geography. And if the
conversation ever got as far as what people did on the supercomputers I supported, it would rapidly tail off once
all parties (including me) realized that cosmological hydrodynamics and quantum Monte Carlo don't really make for great conversation since they don't touch people's everyday lives.</p>
<p>This isn't to say that the work done at the Labs isn't important. But the general public doesn't understand it, and
to a large degree, doesn't really care about it. I realize that being able to impress your neighbors with what you
do isn't at the top of the list of most people's job requirements, but I get a lot of satisfaction out of it.</p>
<h3 id="technically-security">Technically: security</h3>
<p>HPC doesn't really worry about cybersecurity. Every HPC center has a security group and does scans and threat
modeling, but at the end of the day, the security practices on all the largest supercomputers in the public sector
are roughly the same as they were twenty years ago. Users ssh into a login node, and once you're inside, you have
access to everything. You can see everyone else who's logged in, you can see everyone who chmodded their home
directory to be +777, and the only thing separating you from everyone else is the Linux kernel. Passwordless ssh is
everywhere, and often times, passwordless ssh for the root user is everywhere.</p>
<p>This does not fly with paying commercial HPC and AI customers in the cloud who use supercomputing to develop better
products faster than their competitors. For example, both <a href="https://www.synopsys.com/blogs/chip-design/eda-in-the-cloud-snug-2023.html">Arm and AMD have publicly
stated that they perform a lot of their silicon design simulations using HPC in the cloud</a>. What would happen
if both AMD and Arm used the same cluster and one accidentally made their project directory world-readable? Should
domain scientists' understanding of how POSIX file permissions work really be the last line of defense against a
next-generation CPU or GPU's specs being leaked to the competition?</p>
<p>I had to quickly learn about modern security practices when I started doing HPC in the commercial cloud out of
necessity. I'm still nowhere close to being a security expert, but two years has been long enough for me to now
cringe when I talk to my colleagues in the traditional HPC community about how they protect against threats. It's
not really their fault that most of the HPC community hasn't adopted modern practices, because the tools and
practices required to do it right aren't easy to set up, automate, and maintain from scratch.</p>
<p>For example, basic LDAP is a short path to allowing users to log into a cluster's nodes, but if those users also need
to authenticate themselves to REST services that support an HPC workflow across multiple clusters, you have to start building a Rube Goldberg machine of software on top of LDAP. Similarly, sticking every user on their own overlay network is great to limit the blast radius of a
compromised account. However, automating the configuration of VXLAN tunnel endpoints as nodes get allocated and deallocated to
jobs requires a lot of fancy orchestration that is either very complicated to build and maintain yourself or very
expensive to buy and maintain. As a result, HPC just accepts the risk. Cloud has
figured all this out though, and the price of providing this security infrastructure is included in the cost of
cloud-based supercomputers.</p>
<h3 id="pay-good">But the pay is good, right?</h3>
<p>Like I said before I left the public sector, <a href="http://blog.glennklockwood.com/2022/05/life-and-leaving-nersc.html">my base salary is
comparable to what I got at the lab</a>. It's actually gotten less competitive because <a href="https://www.theregister.com/2023/05/11/microsoft_pay_freeze/">all salaries were frozen</a> when I was first eligible for a raise. So, after considering the effects of inflation, my paycheck is a little lower than what it was in the government two years ago.</p>
<p>What's different is the bonus structure which simply does not exist in the government or university world. For those
who aren't familiar with how bonuses work in the tech industry, I'll share how it works for me:</p>
<p></p>
<ul>
<li>In the first year, I was awarded two signing bonuses: one in cash, one in stock. Half of the cash bonus was paid
out up-front, and the other half was paid out after I had been there a year. The stock grant cannot be touched
during the first year because it had a one-year "cliff."</li>
<li>On my one-year anniversary, I got the second half of my cash signing bonus, and my signing stock grant began
"vesting."</li>
<li>After a year, I was also eligible for an annual performance-based raise, cash bonus, and stock bonus.</li>
<ul>
<li>Because of the economy, my annual raise was zero.</li>
<li>The cash bonus was paid out in a lump sum, similar to my cash signing bonus.</li>
<li>The stock bonus was awarded all at once but follows a multi-year "vesting schedule" which means I am only
actually given fractions of the total award over time. However, these bonuses don't have a "cliff" and begin
vesting immediately.</li>
</ul>
<li>Every year thereafter, I am eligible for an annual raise, cash bonus, and another stock bonus.</li>
</ul>
<p>The way stock bonuses work was the least intuitive part to me, but since it's such a significant part of total compensation, it's worth spelling
out for anyone who's considering an offer that includes this:</p>
<p></p>
<ul>
<li>Stock bonuses are defined in terms of dollar values. For example, let's say I got a signing stock bonus of $1000
with a one-year cliff that vests quarterly (every three months) over five years.</li>
<li>On the day that stock bonus is awarded, my employer converts that $1000 value into company stock based on the
market value that day. If stocks are $50 per share, I am awarded 20 shares. My employer hangs on to those shares
on my behalf, so I can't actually do anything with them yet.</li>
<li>Since I have a five-year vesting schedule and the award vests quarterly, my shares will vest twenty times (four
quarters, five years). Coincidentally, since I have 20 shares, I will get one share per quarter.</li>
<li>However, because I have a one-year cliff, I get all four quarters of my first year at my one-year anniversary.
So, four shares should appear in my brokerage account on my one-year anniversary. Once a share is in my
brokerage account, I can do whatever I want with it (like sell it immediately!)</li>
<li>Every quarter thereafter, one more share vests and appears in my brokerage account.</li>
</ul>
<p>Assuming I get a stock bonus as part of my overall annual bonus, this means that stock
awards pile up and vest every year. This is tricky for two reasons:</p>
<p></p>
<ol>
<li>Although my initial stock award was $1,000 in the above example, that amount was converted to stock the day it
was awarded. <i>Assuming I am doing a good job and increasing the value of my employer's stock</i>, the value of
those shares will increase while they're vesting. This means by the time the first four shares of my award
vested at my one-year anniversary, they were worth more than the $50 per share they represented when they were
awarded. More broadly, the value of a stock bonus tends to increase over time, making the true cash value of a
$1000 stock bonus worth a lot more than $1000 by the time it completely vests.</li>
<li>Every year's stock award comes with its own multi-year vesting period, which means at any given time, I have
multiple years' bonuses all vesting at once. This also means that at any given time, I have a bunch of unvested
stock that's worth a lot of money that I can't yet spend. If I quit my job though, all these unvested shares
vanish into thin air.</li>
</ol>
<p>These two factors make up the golden handcuffs that people often talk about in industry.
The longer I stick around, the more unvested stock I have hanging over my head, and it usually becomes increasingly
valuable (yet inaccessible!) over time. The reality is that if you've put in a few years in Big Tech, you might have
years' worth of base salary tied up in unvested stock that all goes away if you quit.</p>
<p>The end result is that although base salary is competitive with what you can make in a government HPC facility, there's a significant cash bonus that falls out of the sky once a year, and
an appreciable amount of stock appears in your brokerage account every couple of months which you can turn around
and sell for more cash. Depending on seniority and performance, these bonuses can add up to a significant fraction
of base salary.</p>
<p>Finally, the above is consistent with what I've seen firsthand at two companies in Big Tech but may be different based on the role and the company. For example, field-facing roles in sales and support may be completely different beasts, and private companies and startups load things differently due to the value of equity.</p>
<h3 id="work-life-balance">How's work-life balance?</h3>
<p>It hasn't been different than working in the government. Just like at a lab or university, some people
work around the clock while others stick pretty close to the standard workday. There may be a higher concentration
of Type A personalities who put in a lot of time in Big Tech, and this may pressure others to keep up and also put
in long hours, but there's rarely been an occasion where a manager expects staff to routinely work nights and
weekends. Doing so would probably result in negative employee satisfaction scores which would roll up and eventually
have to be addressed.</p>
<p>Of course, there are cases where working odd hours is required to get the job done. Because
I work for a global organization, I've had to get up early to meet with teams or customers in Europe. I've also had
to stay up late to meet with teams or customers in Asia. And in some particularly annoying days, I've had to do both
and wind up working from 5am to 8pm. But I never felt that I had no choice in the matter; I pulled these hours
because it was the right thing to do at the time. And I don't see this as being too different from the days when I'd
work sixteen-hour days, seven days a week, for the entire month of March to put together a paper for SC. Or days
when I'm at SC and am preparing talks, meeting with partners, and otherwise hustling from 8am to 1am for five days
straight.</p>
<p>One big difference is the fact that my employer offers discretionary time off ("unlimited vacation"). This is a divisive topic in industry, but I see it as a positive for work-life balance
because it underscores an emphasis on <i>outcomes</i> rather than <i>output</i>. I can take an afternoon
off or enjoy a long weekend with little fanfare, because <i>productivity</i> is infinitely more valuable that <i>presence</i>. As
long as I do what needs to get done, I don't have to worry about timing vacations to ensure I am banking enough time
off in between.</p>
<h2 id="what-i-miss">Do you miss anything about working at the lab?</h2>
<div>Absolutely. There are a bunch of appealing things about working in a DOE lab (or an NSF center)
that I've had to give up since coming to industry.</div>
<h3 id="freedom-to-have-an-off-day">Freedom to have an off day</h3>
<p>Right before I finished graduate school, I had
a conversation with <a href="https://engineering.lehigh.edu/faculty/edmund-webb-iii">Professor Edmund Webb</a> soon
after he became a professor after a decade-long career at Sandia National Labs about life at the Labs. He said that,
after becoming a professor, he lost the ability to just close the door to his office and focus on
something he needed to get done for a day. I didn't really grasp what this meant at the time, but I totally get it now. The
DOE might be one of the few places where you can take a day--maybe even a week--and just close your door to
everything else that's going on around you to focus on what you want to do. In the case of professorship, there's always students requiring attention; in industry, it's customers and partners.</p>
<p>I think this difference results from two factors: very few things in public
HPC are very urgent, and the Labs are stocked full of independent, free-thinking Ph.D. types. There's rarely a
penalty if something is late by a day (or two years! Remember when <a href="https://insidehpc.com/2020/10/doe-under-secretary-for-science-dabbars-exascale-update-frontier-to-be-first-aurora-to-be-monitored/">Aurora
was called "A21?"</a>), but there can be huge payoff in prestige if one of your wacky side projects turns out to be
something useful (this is how <a href="https://docs.nersc.gov/development/containers/shifter/">Shifter</a> came to be). By comparison, working at a giant corporation often means there are a bunch of interdependencies
on others, and the odds of any one of your 200,000 coworkers sending you a Teams message asking for help is just a lot higher than it is at a 70-person supercomputer center. The culture is much more team-oriented, and being a one-person army isn't incentivized as much.</p>
<h3 id="travel">Travel</h3>
<p>Part of my job within the DOE complex was to go around the country (and the world) and be smart, and secondarily,
show that my lab hired smart people and did smart things. If headquarters wanted to make sure that the supercomputer
they were about to spend $500M on was technically sound, I'd sometimes get invited to go sit in on a review and try
to poke holes in the design. If a European HPC project wanted to ensure they were including a global perspective on
some dimension of future HPC strategy, I'd sometimes get invited to give a talk about how I view the world of data.
And if these reviews and workshops happened to be in awesome places around the world--oh well!</p>
<p>I feel a lot more self-conscious about requesting approval to attend these sorts of boondoggles as an engineer now
because the first question I have to answer is, "Is this trip business critical?" If there's a direct line of sight
between me giving a talk at a workshop and a specific business strategy, I can say "yes" with a straight face. But it's
hard to accept an invitation to fly off to Switzerland to give a 30-minute talk when I know that my attendance isn't
going to move any needles.</p>
<h3 id="openness">Openness</h3>
<p>Just like it's no longer my job to travel the world and just be smart, it's not my job to write about the work that I
(or my team) does. I miss writing papers and giving technical talks, because the process of putting together
coherent thoughts around a technical topic is one of the ways I really come to understand it. There's also a lot of
really wild ideas that we're pursuing at scale that the scientific computing
community has never considered, but there are two factors that work against being open about these things:</p>
<p></p>
<ol>
<li>In terms of prioritization, my time is always better spent solving problems, or at least documenting them for
internal audiences who fully grasp the context around them, than writing about them in a way that the rest of
the world can understand. It's hard to justify the time to write a retrospective or a study unless there's a
strategic advantage behind it.</li>
<li>The customers I support typically do not want the world knowing what they're doing. There is an AI arms race
happening right now, and having the technical sophistication to utilize massive-scale supercomputers effectively
is a competitive advantage. In the traditional HPC community, only national security is comparable to the level
of secrecy involved, and none of the intelligence agencies are openly contributing to the state of the art in
HPC either.</li>
</ol>
<div>So instead of making conference papers and presentations, these days I make more internal papers and presentations.
I'm trying to figure out ways to publish interesting technical anecdotes on my website (for example, I maintain <a href="https://www.glennklockwood.com/ai/ai-requirements.html">a collection of LLM training requirements as I am exposed to them</a>), but it's a lot of extra work to disentangle the proprietary bits from my work notes to do this.</div>
<p></p>
<p>Related to openness is also freedom to speak my mind in public forums. I had the most latitude to blast my
opinions out on to the Internet when I was still early in my career and nobody listened to me, but I've had to get
progressively less opinionated over the years. At this point, I abide by a written corporate social media policy
which, although very reasonable in what it requests (don't slander competitors, always be transparent about who employs you), it stops me from commenting on news as much as I used to since so many tech
companies qualify as competitors in some dimension.</p>
<h2 id="regret-decision">Would you still have left knowing what you know now?</h2>
<p>Yes. I still stand by just about everything I wrote in my <a href="http://blog.glennklockwood.com/2022/05/life-and-leaving-nersc.html">original blog post</a>; at the time, I just needed a change, and I
found the change that I was looking for. Without immersing myself in the world of cloud, I would have
never learned about virtualization, physical infrastructure, or modern security to the degree that I have. And the fact that I
stumbled into what has become one of the leading companies in AI at the dawn of generative AI was an extremely lucky
coincidence.</p>
<p>However, this doesn't mean that I now turn my nose up at doing HPC in the public sector.
There are many unique aspects to working at a DOE lab or NSF center that have no parallel in industry. I also believe that I am
the sum of the experiences that led me to where I work today, and I would never have gotten the opportunity to write
this retrospective if I didn't learn everything I did working in the DOE and NSF.</p>
<p>And perhaps above all else, there is something attractive about public service that I haven't been
able to shake in the last two years. I still dial in to <a href="https://science.osti.gov/ascr/ascac">ASCAC
meetings</a> to see what the world of public HPC and scientific computing is thinking and doing, and I still try
to contribute time and attention to working groups like <a href="https://www.nitrd.gov/coordination-areas/lsn/magic/">NITRD's MAGIC</a>. I write lengthy blog posts in a <a href="http://blog.glennklockwood.com/2024/05/isc24-recap.html">futile attempt to caution the leaders in public-sector HPC</a> against
rejecting AI workloads in commercial clouds as HPC. And every time I learn some slick way we deal with hard technological or sociological issues at work, I still file it away in the "good ideas for when I go
back" folder in the back of my mind.</p>
<p>I don't have any near-term plans on going anywhere though. Like I said before, there are
still plenty of days when dialing into work is like going to the playground. Amazing things are happening in the
world of HPC infrastructure at scale now that the world is pouring money into AI, and the rate of scale and
innovation is no longer constrained to <a href="https://www.llnl.gov/article/48101/powering-llnl-prepares-exascale-massive-energy-water-upgrade">40 MW</a>
and <a href="https://www.olcf.ornl.gov/wp-content/uploads/OLCF-6-RFP-Cover-Letter-07-19-2024.pdf">$500M</a> per
supercomputer like it was when public-sector HPC was setting the bar for leadership. There is a whole new exciting world of challenges and possibilities when you start thinking about building supercomputers that consume <a href="https://www.datacenterdynamics.com/en/news/aws-acquires-talens-nuclear-data-center-campus-in-pennsylvania/">hundreds of megawatts of power</a>.</p>
<p>Like I wrote two years ago, I don't think any government has the appetite to build data centers for scientific computing that are larger than today's 50 MW exascale facilities. This means that government HPC centers will never have a reason to explore the exciting world of 100+ MW supercomputers or work on the wacky problems that arise at that scale. Consequently, the biggest and most challenging problems in HPC--at least in terms of infrastructure and systems design at scale--are becoming unique to industry, not public HPC.</p>
<p>I got into HPC because I enjoy working on large, complex systems. Considering where I am at this stage of my life, what I want to accomplish in the rest of my career, and what gets me out of bed in the morning, I feel like I wound up in the right place for now. I have no regrets.</p>]]></content><author><name>Glenn K. Lockwood&apos;s Blog</name></author><category term="glennklockwood" /><summary type="html"><![CDATA[June 2024 marked two years since I left my job at one of the world's most prestigious government HPC centers for a job in one of the world's largest technology corporations. In that time, the world of HPC has changed dramatically; just six months after I started, ChatGPT was released and triggered a gold rush in AI that is now overshadowing traditional scientific computing. This shift brought about massive HPC deployments led by hyperscalers, challenging the long-held belief that only national governments could deploy and operate world-leading supercomputers. My experiences at ISC'24 this past summer made clear to me that the traditional HPC community is now rethinking their role the industry, and some individuals who built their careers in public HPC are revisiting their assumption that world-class HPC systems are limited to the public institutions that have historically dominated the top of the Top500 list. I had no idea things would unfold this way when I left my job at NERSC back in 2022, and I've been remarkably lucky to now be a part of the largest forces driving this huge shift in HPC.]]></summary></entry></feed>